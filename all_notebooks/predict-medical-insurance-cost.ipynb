{"cells":[{"metadata":{"_uuid":"e23e6a528332641d895a1da12dcbded22f712b21"},"cell_type":"markdown","source":"# **Overview**\n- <a href=\"#1\"> 1. Introduction </a>\n- <a href=\"#2\"> 2. Import the required packages </a>\n- <a href=\"#3\"> 3. Import dataset </a>\n- <a href=\"#4\"> 4. Data Preprocessing </a> \n     - <a href=\"#4-2\"> 4.1 Label encoding</a>\n     - <a href=\"#4-1\"> 4.2 Check for missing data</a> \n- <a href=\"#5\"> 5. Function to make predictions of the output given the input feature. </a> \n- <a href=\"#6\"> 6. Function to evaluate the model </a> \n- <a href=\"#7\"> 7. Divide the data into training, cross-validation and testing data </a>\n    -  <a href=\"#7-1\"> 7.1 Holdout method with random sampling </a>\n        - <a href=\"#7-1-1\"> I. Explanatory data analysis </a>          \n            - <a href=\"#7-1-1-1\">A. Distribution of 'age' feature & Scatter plot</a>\n            - <a href=\"#7-1-1-2\">B. Distribution of 'sex' feature & Scatter plot</a>\n            - <a href=\"#7-1-1-3\">C. Distribution of 'bmi' feature & Scatter plot</a>\n            - <a href=\"#7-1-1-4\">D. Correlation Matrix</a>\n            - <a href=\"#7-1-1-5\">E. Pairplot of features</a>\n        - <a href=\"#7-1-2\"> II. Linear Regression Model </a>\n        - <a href=\"#7-1-3\"> III. Plotting & Calculating Error of  Model </a> \n    -  <a href=\"#7-2\"> 7.2 Stratified holdout method </a>\n    -  <a href=\"#7-3\"> 7.3 Stratified k-folds cross-validation </a>\n        - <a href=\"#7-3-1\"> I. Scatter plot of true cost and predicted cost </a> \n    -  <a href=\"#7-4\">  7.4 Bootstrapping </a>\n        - <a href=\"#7-4-1\"> I. Scatter plot of true cost and predicted cost  </a> \n- <a href=\"#8\">8. Comparison of Performance of model using different methods </a>\n- <a href=\"#9\"> 9. Random Forest Regression Model </a>\n- <a href=\"#10\"> 10. Comparison of Linear and Random Forest Model</a>\n- <a href=\"#11\"> 11.Residual plot between true and predicted charges </a>        \n   \n\n"},{"metadata":{"_uuid":"8f406d5153c2cdcf6563c845f0f07f44bb6472b6"},"cell_type":"markdown","source":"## <a id=\"1\"> 1. Introduction </a>"},{"metadata":{"_uuid":"b8f100ce1aee26618a34eabd9b4e1414c638e002"},"cell_type":"markdown","source":"In order to create a ML model to solve any problem ,  **we must have the domain  knowledge of the problem.** In our case it is related to *\"insurance policy\"* and the  problem is to *predict medical insurance cost using a regression model.*\n\nSo let's have some knowledge of \"insurance\".  [Insurance](http://https://en.wikipedia.org/wiki/Insurance) is a means of **protection from financial loss .** (source : wikipedia) An entity which provides insurance is known as an insurer, *insurance company,* insurance carrier or underwriter. A person or entity who buys insurance is known as an insured or as a *policyholder.* The insured receives a contract, called the *insurance policy,* which details the conditions and circumstances under which the insurer will compensate the insured. \nIf the insured experiences a loss which is potentially covered by the insurance policy, the insured submits a claim to the insurer for processing by a claims adjuster. \n[Health insurance](http://https://en.wikipedia.org/wiki/Health_insurance) is insurance that  **covers the whole or a part of the risk of a person incurring medical expenses, spreading the risk over a large number of persons**.\n\n"},{"metadata":{"_uuid":"eb0d91c05fd3a9a24fa7934bc28c2271b43c8d03"},"cell_type":"markdown","source":"## <a id=\"2\">2. Import the required packages </a>"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split # for splitting the dataset in train,test and validation\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn import linear_model \nfrom sklearn.linear_model import Ridge\n#from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import mean_squared_error # for calcualting mse\n#from sklearn.metrics import r2_score\nfrom sklearn.cross_validation import StratifiedKFold # to implement stratifiedKFold\nfrom sklearn import preprocessing\n#from sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt #for plotting\nimport seaborn as sns\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nprint(os.listdir(\"../input\")) #dataset available in 'input' folder","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd848dcee51385fc7947ccc9010d67202641f6e2"},"cell_type":"markdown","source":"Let's first import the dataset and display for overview of differnent columns  in the table."},{"metadata":{"_uuid":"4bbbbc3a9b8f6246c418745d597c9ac4067ac4b7"},"cell_type":"markdown","source":"## <a id=\"3\">3. Import the dataset and read it </a>"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","scrolled":false,"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"#import dataset in variable insurance\ninsurance = pd.read_csv('../input/insurance.csv')\ninsurance = insurance.sample(frac=1).reset_index(drop=True)# shuffle\ndisplay(insurance.head(10))\nprint(insurance.info())\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"405dea2d71f5d68102c6eb7ca4c8515a4125b1fb"},"cell_type":"markdown","source":"So we have the following input features  in the dataset\n- **age:** age of primary beneficiary\n- **sex:** insurance contractor gender, female, male\n- **bmi:** Body mass index, providing an understanding of body, weights that are relatively high or low relative to height, objective index of body weight (kg / m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9\n- **children:** Number of children covered by health insurance / Number of dependents\nsmoker: Smoking\n- **region:**  the beneficiary's residential area in the US, northeast, southeast, southwest, northwest.\n- **charges:**  Individual medical costs billed by health insurance\n\nThe number of rows are 1338 and columns are 7. \n**Target variable** is **charges**."},{"metadata":{"_uuid":"c12c00274112f629476c9413629f2f5e58e95428","trusted":true},"cell_type":"code","source":"insurance.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c0d981281a282657f26e8076062ca5e459a74ac0"},"cell_type":"markdown","source":"As we can see that we have less number of data, we have a chance of overfitting the model. Now let's do explanatory data analysis."},{"metadata":{"_uuid":"d436317cfb101f5a73255c60673048f73efb098e"},"cell_type":"markdown","source":"Types of the input feature"},{"metadata":{"_uuid":"71c4a93021dfafafc3f46fcfbcf6a1f0b8193929","trusted":true},"cell_type":"code","source":"insurance.dtypes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97c4cd4392f90413d5f1b6973a04ba5021aff31f"},"cell_type":"markdown","source":"As we can see we have categorical features also. So first we need to do\nencoding of categorical feature. It will be helpful during  training of the model."},{"metadata":{"_uuid":"852e2ef90b55e00c46bbcc6c590ff96bdbd80751"},"cell_type":"markdown","source":"**Statistics of numerical columns** :"},{"metadata":{"trusted":true,"_uuid":"878e7cfa004eb42a0e9a847e1ed54ef3eb474886"},"cell_type":"code","source":"insurance.describe().T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c7dd228f8eeb1b0229a3bb76727077d4833e342"},"cell_type":"markdown","source":"It seems that  \"charges\" is more spread (12110.01) . 75% percentile is 16639 and maximum is 63770.4280. So it has outlier ."},{"metadata":{"trusted":true,"_uuid":"2da294f2d0a12f7f757bf5e198440e3c4fe7440b"},"cell_type":"code","source":"insurance.groupby(insurance[\"smoker\"]).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"36471231a0b63829ab5152c520a058941f7d90ef"},"cell_type":"code","source":"insurance.groupby(insurance[\"region\"]).count()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f4e7cd3781782b818243c43dc05832e71f3674c"},"cell_type":"markdown","source":"# 4. Explanatory Data Analysis"},{"metadata":{"_uuid":"fefca32d09a3ed2bb71340dfb70bf0d7499a8bc1"},"cell_type":"markdown","source":"## Distribution of \"smoker\""},{"metadata":{"trusted":true,"_uuid":"acfe115e685985c6b2f71d844ddbe8423c199a7c"},"cell_type":"code","source":"import matplotlib.ticker as mtick # For specifying the axes tick format \nax = (insurance['smoker'].value_counts()*100.0 /len(insurance))\\\n.plot.pie(autopct='%.1f%%', labels = ['no', 'yes'],figsize =(5,5), fontsize = 12 )                                                                           \nax.yaxis.set_major_formatter(mtick.PercentFormatter())\nax.set_ylabel('Smoker',fontsize = 12)\nax.set_title('% Smoker', fontsize = 12)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2491e20a241389b13e372a8872eb77f05282c8de"},"cell_type":"markdown","source":"majority on people is non-smoker."},{"metadata":{"_uuid":"a80f508e2b198a66a36755a2d1c524b7bb4141fa"},"cell_type":"markdown","source":"## Distribution of \"bmi\""},{"metadata":{"trusted":true,"_uuid":"9a802e73c6086812ccc935d08ac42313ddd2a726"},"cell_type":"code","source":"sns.distplot(insurance[\"bmi\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"32745ec73bef35b87366a84c1a9ecdd47233fe85"},"cell_type":"markdown","source":"\"bmi\" is normally distributeded. So it will help the model in prediction."},{"metadata":{"_uuid":"b676aab478b8846c44e746be860b4cd2c7c2de37"},"cell_type":"markdown","source":"## Distribution of \"charges\""},{"metadata":{"trusted":true,"_uuid":"e8fb156f2489ccc82df8bd98fa0af91e3cb83d32"},"cell_type":"code","source":"sns.distplot(insurance[\"charges\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1af811309c7f94bdaebe4987026e768d0ac03f9"},"cell_type":"markdown","source":"\"charges\" has outliers. So we need to remove them for better performance of the model."},{"metadata":{"_uuid":"9d355094f43631e4466bf03a9fe9ae4170515a4a"},"cell_type":"markdown","source":"## Distribution of \"age\""},{"metadata":{"trusted":true,"_uuid":"35a7d8000e4c58441c16648a27ca882672bb7def"},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nplt.title(\"Distribution of age in training set\")\nax=sns.distplot(insurance[\"age\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5447c88382d76b618987d77b56bd9cc032e6d23e"},"cell_type":"code","source":"# temp_age_charges=pd.DataFrame()\n# temp_age_charges[\"age\"]=X_train[\"age\"]\n# temp_age_charges[\"charges\"]=Y_train\n_ = sns.lmplot(\"age\", \"charges\", data=insurance, fit_reg=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa9d3bee86b2d729977b56130aa55ad7aa7f2d44"},"cell_type":"code","source":"# temp_bmi_charges=pd.DataFrame()\n# temp_bmi_charges[\"bmi\"]=X_train[\"bmi\"]\n# temp_bmi_charges[\"charges\"]=Y_train\n_ = sns.lmplot(\"bmi\", \"charges\", data=insurance, fit_reg=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0de3cd7351ade6b984872d9dbcb45c8612c3bdb"},"cell_type":"markdown","source":"## Histogram of features"},{"metadata":{"trusted":true,"_uuid":"43fe4a8b8b6edf79530208f6f5582db101c5a063"},"cell_type":"code","source":"insurance.hist(figsize=(15, 10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d756caf33241c3cc66410e6ee7bec5382b4ddb0b"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"4b61c9ac8e72c97a98927d3d26a90458237d720f"},"cell_type":"markdown","source":" ## Distribution of sex in training set"},{"metadata":{"trusted":true,"_uuid":"1490556aa42b7d8bb53b6c186fe9b5a0383c913c"},"cell_type":"code","source":"ax = sns.countplot(x=\"sex\", hue=\"sex\", data=insurance)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9116611dd5ec2765a31a23a2f247c521ee7655ee"},"cell_type":"markdown","source":"## <a id=\"4\" > 4. Preprocessing the Dataset</a>"},{"metadata":{"_uuid":"2cfce83397d045468939f9a6a7061be6a9fd664d"},"cell_type":"markdown","source":"### Check for NULLs"},{"metadata":{"trusted":true,"_uuid":"e212c26f1a67a3ee9a84c38af96eb69676166756"},"cell_type":"code","source":"insurance.isnull().sum(axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d80fc81ce1a70633cbf7bbdccc9c9ba4055695ee"},"cell_type":"markdown","source":"There is no  missing values."},{"metadata":{"_uuid":"fe2b03abb6c5a4999dbe65640304d481f18029dd"},"cell_type":"markdown","source":"### Remove Outliers"},{"metadata":{"trusted":true,"_uuid":"6d8a83c49c4933e5ee3b54d25dfad0a35df7d983"},"cell_type":"code","source":"cols=[ 'bmi', 'charges']\n#Determine outliers in dataset\n\nfor i in cols:\n    quartile_1,quartile_3 = np.percentile(insurance[i],[25,75])\n    quartile_f,quartile_l = np.percentile(insurance[i],[1,99])\n    IQR = quartile_3-quartile_1\n    lower_bound = quartile_1 - (1.5*IQR)\n    upper_bound = quartile_3 + (1.5*IQR)\n    print(i,lower_bound,upper_bound,quartile_f,quartile_l)\n\n    insurance[i].loc[insurance[i] < lower_bound] = quartile_f\n    insurance[i].loc[insurance[i] > upper_bound] = quartile_l\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5885f97e557418b1cd663f850057866693bde0db"},"cell_type":"code","source":"def remove_outlier(df_in, col_name):\n    q1 = df_in[col_name].quantile(0.25)\n    q3 = df_in[col_name].quantile(0.75)\n    iqr = q3-q1 #Interquartile range\n    fence_low  = q1-1.5*iqr\n    fence_high = q3+1.5*iqr\n    df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]\n    return df_out\n\ninsurance1=remove_outlier(insurance, 'bmi')\ninsurance1=remove_outlier(insurance1, 'charges')\ninsurance=insurance1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"05742adf22dc33723018bd5c3f9342885f627228"},"cell_type":"markdown","source":"### Feature scaling"},{"metadata":{"trusted":true,"_uuid":"c3fb13ed0da167b23f7fdab713c0d55531d7a7fc"},"cell_type":"code","source":"#insurance['charges']=(insurance['charges']-insurance['charges'].min())/(insurance['charges'].max()-insurance['charges'].min())\ninsurance['bmi']=(insurance['bmi']-insurance['bmi'].min())/(insurance['bmi'].max()-insurance['bmi'].min())\ninsurance.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9fd186940e8c69e9eee70b16ca4f20a2b848652"},"cell_type":"code","source":"insurance.hist(figsize=(15, 10))\nplt.show()\ninsurance.describe().T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e91e201265835e03371849f7d8127b101ae94246"},"cell_type":"markdown","source":"### One hot encoding"},{"metadata":{"_uuid":"0b06a53d3220193b3342843917ca498b0b8fe71b","trusted":true},"cell_type":"code","source":"\ninsurance=pd.get_dummies(insurance,drop_first=True)\ninsurance.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad3404a335c4ab40b6396ad804ac504a2ed6d084"},"cell_type":"markdown","source":"## Feature importance"},{"metadata":{"trusted":true,"_uuid":"6de038851b10a87bcbd77a14616bf5e93341c067"},"cell_type":"code","source":"#using randomforest to find the feature importance\ntrain_y = insurance['charges'].values\ntrain_X = insurance.drop(['charges'], axis=1)\n\nfrom sklearn import ensemble\nmodel = ensemble.RandomForestRegressor(n_estimators=200, max_depth=10, min_samples_leaf=4, max_features=0.2, n_jobs=-1, random_state=0)\nmodel.fit(train_X, train_y)\nfeat_names = train_X.columns.values\n\nimportances = model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\nindices = np.argsort(importances)[::-1][:20]\n\nplt.figure(figsize=(12,5))\nplt.title(\"Feature importances\")\nplt.bar(range(len(indices)), importances[indices], color=\"g\", align=\"center\")\nplt.xticks(range(len(indices)), feat_names[indices], rotation='vertical')\nplt.xlim([-1, len(indices)])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"865d23c345cee9318d2fbf652a7ff96e763d4354"},"cell_type":"markdown","source":"### Correlation Matrix"},{"metadata":{"trusted":true,"_uuid":"3adf0272628f4eda657b69f286eeb30d7aa27533"},"cell_type":"code","source":"colormap = plt.cm.RdBu\nplt.figure(figsize=(10,10)) #controls the graph size\nplt.title('Pearson Correlation of Features', y=1, size=20) #\"size\" controls the title size\n\nsns.heatmap(insurance.astype(float).corr(),linewidths=2,vmax=1.0, \n            square=True, cmap=colormap, linecolor='red', annot=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fbb6039a488902a648d9e341228dcce17683eee5"},"cell_type":"markdown","source":"As we can see from the above correlation matrix \"children\" is very less correlated with the target variable \"charges\" , followed by \"sex_male\". So we can remove them .\n\nRemove very less correlated features with the target variable. As we can see from the \nabove correlation matrix \"region\" is very less correlated."},{"metadata":{"_uuid":"4cb2fd2a3f89524d26c7367bf5cfe8b29c608242"},"cell_type":"markdown","source":"###  Pairplot of different features"},{"metadata":{"_uuid":"db437af3379aa9a3ac69460e49224098d5ca313d"},"cell_type":"markdown","source":"Pairplot to visualize the realtionship between the target and independent features"},{"metadata":{"trusted":true,"_uuid":"e85cd6ce01c169013481ffffb5a2ff017f8790a4"},"cell_type":"code","source":"sns.pairplot(insurance, x_vars=[\"bmi\", \"age\",'children'], y_vars=[\"charges\"],\n              aspect=1, kind=\"reg\");\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"995ed17d60d0fc3ef0424cc21612bbf4cbae14bb"},"cell_type":"markdown","source":"### Feature Selection"},{"metadata":{"trusted":true,"_uuid":"f6dc37d7c4b73b91a58e7fc21b77bb71a1159d37","_kg_hide-input":false},"cell_type":"code","source":"#Selected Features\ncols= ['smoker_yes','age','bmi','children']\nX=insurance[cols]\ny=insurance['charges']\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af03f7473ccdda4bf8ddee56727da2c3e2f5f100"},"cell_type":"markdown","source":"## <a id=\"5\"> 5. Function to make predictions of the output given the input feature. </a> "},{"metadata":{"_uuid":"21b905fee03cbaf3e4f5acb974219d42f27044f2"},"cell_type":"markdown","source":"Define functions to calculate train error, validation error and finally to calculate error on test set."},{"metadata":{"trusted":true,"_uuid":"bfe1b2c2f6a52228def813afc7e345024ab0fb6e","collapsed":true,"_kg_hide-input":false},"cell_type":"code","source":"def calc_train_error(X_train, y_train, model):\n    '''returns in-sample error for already fit model.'''\n    predictions = model.predict(X_train)\n    mse = mean_squared_error(y_train, predictions)\n    rmse = np.sqrt(mse)\n    return rmse\n    \ndef calc_validation_error(X_test, y_test, model):\n    '''returns out-of-sample error for already fit model.'''\n    predictions = model.predict(X_test)\n    mse = mean_squared_error(y_test, predictions)\n    rmse = np.sqrt(mse)\n    return rmse\n    \ndef calc_metrics(X_train, y_train, X_test, y_test, model):\n    '''fits model and returns the RMSE for in-sample error and out-of-sample error'''\n    model.fit(X_train, y_train)\n    train_error = calc_train_error(X_train, y_train, model)\n    validation_error = calc_validation_error(X_test, y_test, model)\n    return train_error, validation_error","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3969f8552eac656c047d3bfbbdf2418220dc6b5c"},"cell_type":"markdown","source":"Now, we will create a general function to implement linear regression model and fit the model to training data set ."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d41aa19656ae27c80eb5dba9fb6502eb83520dbd"},"cell_type":"code","source":"#     #global dummy variables to store error and accuracy of different models \n#     #and methods of splitting for comparison in last\n#     r_score = []\n#     loss = []\n#     r_score1=[]\n#     loss1=[]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ae3eb762e02700b4a08fc302d1a488eca837c96","trusted":true,"collapsed":true,"_kg_hide-input":false},"cell_type":"code","source":"def regression_model(X_train, Y_train, X_validation,Y_validation,X_test, Y_test):\n    ''' Input:\n        X_train : independent training data\n        Y_train : target variable for trainig\n        X_validation: validation dataset\n        Y_validation: validation target variable\n        evaluate: accepts boolean value \"true\" or \"fale\". If true then function will predict the test accuracy \n                instead of training and validation error'''\n    linear_Model = linear_model.LinearRegression(fit_intercept=True) #creating model\n    linear_Model.fit(X_train,Y_train,) # fitting the model\n\n    train_error_rmse = calc_train_error(X_train, Y_train, linear_Model)\n    valid_error_rmse= calc_validation_error(X_validation, Y_validation, linear_Model)\n    train_error_rmse, valid_error_mse= round(train_error_rmse, 3) ,round(valid_error_rmse, 3)\n    print('RMSE - train error: {} | validation error: {}'.format(train_error_rmse, valid_error_rmse))\n    \n#     y_pred= linear_Model.predict(X_test)\n#     print('Model_Accuracy_Score (R Square): {:.4f} \\nLoss(RMSE): {:.4f}'.format(r2_score(y_pred,Y_test),\n#                                                                                 np.sqrt(mean_squared_error(y_pred,Y_test))))\n    accuracy = linear_Model.score(X_validation,Y_validation)\n    print('Accuracy :',format(round(accuracy,3),\"%\"))\n   \n    return linear_Model,train_error_rmse, valid_error_mse\n   ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eedef88fdd80186727c06cfdfac9ac146c141820"},"cell_type":"markdown","source":"## <a id=\"6\"> 6. Function to evaluate the model </a>"},{"metadata":{"trusted":true,"_uuid":"89e50fcc8d0477adf480c9966524f11664303f68","collapsed":true,"_kg_hide-input":false},"cell_type":"code","source":"def evaluate(X_train, Y_train, X_test, Y_test, train_error,validation_error, model):\n\n    new_train_error = mean_squared_error(Y_train,model.predict(X_train))\n    new_train_error = np.sqrt(new_train_error)\n   # new_validation_error = mean_squared_error(Y_validation, model.predict(X_validation))\n    new_test_error = mean_squared_error(Y_test, model.predict(X_test))\n    new_test_error = np.sqrt(new_test_error)\n\n    \n    print('Comparison')\n    print('-'*50)\n    print('ORIGINAL ERROR')\n    print('train error: {} | validation error: {}\\n'.format(round(train_error,3), round(validation_error,3)))\n    accuracy = model.score(X_validation,Y_validation)\n    print('Accuracy :',format(round(accuracy,3),\"%\"))\n    print('-' * 50)\n    print('FINAL ERROR OF MODEL')\n    print('train error: {} | test error: {}'.format(round(new_train_error,3), round(new_test_error,3)))\n    accuracy = model.score(X_test,Y_test)\n    print('Accuracy :',format(round(accuracy,3),\"%\"))\n    y_pred= model.predict(X_test)\n#     print('Model_Accuracy_Score (R Square): {:.4f} \\nLoss(RMSE): {:.4f}'.format(r2_score(y_pred,Y_test),\n#                                                                                   np.sqrt(mean_squared_error(y_pred,Y_test))))\n    \n   # loss.append(new_test_error)\n   # r_score.append(r2_score(y_pred,Y_test)*100)\n    #r_score.append(round(accuracy,3)*100)\n    #loss1.append(new_test_error)\n   # r_score1.append(round(accuracy,3)*100)\n   # r_score1.append(r2_score(y_pred,Y_test)*100)\n    return y_pred, accuracy,new_test_error","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba59006e62682941b59b3a213ebc505a5a8d98f8"},"cell_type":"markdown","source":"## <a id=\"7\"> 7.  Divide the data into training, cross-validation and testing data </a>"},{"metadata":{"trusted":true,"_uuid":"60b2b911cd083fe4a732a8f82a0b7368ad449bc3"},"cell_type":"code","source":"\nprint(\"Independent features\")\ndisplay(X.head())\nprint(\"-\"*100)\nprint(\"Target variable\")\ndisplay(y.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10f7a14c4538c57890eb3cf3d666f17418331a54"},"cell_type":"markdown","source":"The target variable is continuous. So to split the dataset using stratify we need to convert \"charges\" into bins. Therefore we will add "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e27bab8719c6af91bb8bb264afc9e46a9505675c"},"cell_type":"code","source":"# create bins\n# bins = np.linspace(insurance.charges.min(),insurance.charges.max(),6)\n# charges_groups = np.digitize(insurance.charges,bins)\n\n# #Add bin information to the dataframe\n# insurance= insurance.assign(charge_groups = charges_groups)\n# insurance['charge_groups'] = pd.Categorical(insurance.charge_groups)\n\n# #Last bin has too few values to stratify, so we merge it with second last group and reduce the number of bins\n# insurance.charge_groups[insurance.charge_groups == 6]= 6\n\n# insurance.head()\n# #insurance.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31bb6cab5e9f89101eea632fb094fbf9564c1957"},"cell_type":"markdown","source":"## <a id=\"7-1\"> 7.1 Holdout method with random sampling </a>"},{"metadata":{"_uuid":"4f8b17942380f02306b8ba45e757e8e7a164724a"},"cell_type":"markdown","source":"Hold-out validation is simple. Assuming that all data points are i.i.d. (independently and identically distributed), we simply randomly hold out part of the data for validation. We train the model on the larger portion of the data and evaluate validation metrics on the smaller hold-out set.\n\nComputationally speaking, hold-out validation is simple to program and fast to run. The downside is that it is less powerful statistically. The validation results are derived from a small subset of the data, hence its estimate of the generalization error is less reliable. It is also difficult to compute any variance information or confidence intervals on a single dataset.\n\nUse hold-out validation when there is enough data such that a subset can be held out, and this subset is big enough to ensure reliable statistical estimates."},{"metadata":{"_uuid":"5ef633bbdad8c99f20e1c4eecce6008d92d0e354","trusted":true,"collapsed":true,"_kg_hide-input":false},"cell_type":"code","source":"X_intermediate, X_test, Y_intermediate, Y_test = train_test_split(X, \n                                                                  y, \n                                                                  shuffle=True,\n                                                                  test_size=0.1 \n                                                                  )\n\n# train/validation split (gives us train and validation sets)\nX_train, X_validation, Y_train, Y_validation = train_test_split(X_intermediate,\n                                                                Y_intermediate,\n                                                                shuffle=False,\n                                                                test_size=0.2\n                                                                )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33408bee9503fe5d20dce95f4fa17bec0ebef887","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# print proportions\n#holdoutA=[] #to store accuracy\nholdoutE=[] #to store rmse error\nprint('train: {}% | validation: {}% | test {}%'.format(round(len(Y_train)/len(y),2),\n                                                       round(len(Y_validation)/len(y),2),\n                                                       round(len(Y_test)/len(y),2)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4fbd22cc19ba2a9b12826d2a6fb405e2c5a071a5","trusted":true,"_kg_hide-input":false,"collapsed":true},"cell_type":"code","source":"# # this is the training data after randomly splitting using holdout method\n# print(\"Training data after holdout\")\n# print(\"-\"*50)\n# display(X_train.head()) #display the first five rows of train set\n# print(X_train.shape)\n# display(Y_train.head()) #display the first five rows of train target variable\n# print(Y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"19b0e2af2c699644de4fa95573dde8dd9dfe26f1"},"cell_type":"markdown","source":"### <a id=\"7-1-2\"> A. [Holdout method]Linear Regression Model </a>"},{"metadata":{"_uuid":"91de803eafedd8b8d5710c0faf05ae9967f2e9be"},"cell_type":"markdown","source":"Let's create linear regression model and fit  <a href=\"#7-1\"> data </a> to it that is available to us after holdout i.e splitting. And also print the validation and training error using the function that we created earlier  -> <a href=\"#5\">  Function to make predictions of the output given the input feature. </a>"},{"metadata":{"trusted":true,"_uuid":"4567bdf14345acffa2893cc41e6cbeba0df58058"},"cell_type":"code","source":"model , train_error,validation_error= regression_model(X_train, Y_train, X_validation,Y_validation,X_test,Y_test)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c1449e610e97261f5c077125341a774f5696236"},"cell_type":"markdown","source":"#### <a id=\"7-1-3\"> a.[Holdout] [Linear Regression] RMSE </a> "},{"metadata":{"trusted":true,"_uuid":"4d54808500102ac9fee05199db9cbe91892df7b2"},"cell_type":"code","source":"y_pred,accuracy,test_error=evaluate(X_intermediate, Y_intermediate, X_test, Y_test, train_error,validation_error, model)\n#holdoutA.append(accuracy)\nholdoutE.append(test_error)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true,"_uuid":"2d809c67d7682d1e2c37e6cc2f007ac98d232000"},"cell_type":"code","source":"def model_scatter_plot(y_pred,Y_test):\n    model_table = pd.DataFrame(y_pred,Y_test).reset_index()\n    model_table.columns=['Y_test','y_pred']\n    #Model Graph\n    sns.lmplot(x = 'Y_test',y='y_pred',data = model_table,size=6,aspect=1.5,\n           scatter_kws={\"s\": 70, \"alpha\": 1,'edgecolor':'black'},fit_reg=True)\n    plt.title('Analysis of True and Predicted Cost',fontsize=14)\n    plt.xlabel('Y_test',fontsize=12)\n    plt.ylabel('y_pred',fontsize=12)\n    #plt.scatter(y_test,y_pred)\n    return plt.show()\n\nprint(\"[Holdout][Linear Regression]\")\nmodel_scatter_plot(y_pred,Y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca60614c26593aa4518e9070aacfe48cabaa7dee"},"cell_type":"markdown","source":"### [Holdout] Residual Plot for Linear Regression"},{"metadata":{"trusted":true,"_uuid":"9e565fb86abd77be7c0823b7c7dea008a41e2038"},"cell_type":"code","source":"plt.figure(figsize=(10,5)) #controls the graph size\nplt.title('[Holdout] Residual Plot for Linear Regression', y=1, size=20) #\"size\" controls the title size\nsns.residplot(Y_test,y_pred) ## regression Residual Plot for linear regression model using bootstrapping","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c733ef609fc5868aad09f888809e068bfb44d66"},"cell_type":"markdown","source":"## [Holdout] Ridge Regression"},{"metadata":{"_uuid":"d70ecfd530261e0653396fff69d330f71c86dd3c"},"cell_type":"markdown","source":"**Hyperparameter tuning ** : parameter \"alpha\"  for ridger regression using validation set"},{"metadata":{"trusted":true,"_uuid":"11c242b96d4d554a897cc2f9ea49e93d2e9f4ee2"},"cell_type":"code","source":"#hyperparameters\npara=np.linspace(0.01,10,100)\nerror=dict()\n\n#tuning hyperparameter\nprint(\"Parameter   RMSE Error\")\nprint(\"-\"*25)\nfor parameter in para:\n    ridge = Ridge(fit_intercept=True, alpha=parameter)\n    ridge.fit(X_train,Y_train)\n    \n    terr = calc_train_error(X_train,Y_train,ridge)\n    verr = calc_validation_error(X_validation,Y_validation,ridge)\n    total_error =np.abs(terr-verr)\n    error[parameter]= total_error\n    p= min(error,key = error.get)\n    print(\"{:.3f}        {:.3f}\".format(parameter,error[parameter]))\n    \nprint(\"Best parameter after validation: {:.3f}\".format(p))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f142669782d546e8c6ac04142b522599dd4dbbb1"},"cell_type":"markdown","source":"Error plot of training and validation for finding best parameter of ridge regression."},{"metadata":{"trusted":true,"_uuid":"d46db38e923f77edbaccc4a1a8633998e7c8dcc5"},"cell_type":"code","source":"#plot of error\ndef plotErr(select,X_train,Y_train,para_range,X_validation,Y_validation):   \n    #Setup arrays to store training and test accuracies\n    para = np.linspace(0.01,para_range,100)\n    train_accuracy =np.empty(len(para))\n    test_accuracy = np.empty(len(para))\n\n    \n    \n    for i,k in enumerate(para):\n        mod = [Ridge(fit_intercept=True, alpha=k),RandomForestRegressor(max_depth=k, random_state=0),SVR(C=k, epsilon=0.2)]\n        m = mod[select]\n        m.fit(X_train,Y_train)\n        train_accuracy[i]= calc_train_error(X_train, Y_train, m)\n        test_accuracy[i] = calc_validation_error(X_validation, Y_validation,m)\n       \n\n    #Generate plot\n    plt.figure()\n    plt.figure(figsize=(8,8))\n    plt.title('error plot for training and validation')\n    plt.plot(para, test_accuracy, label='validation error')\n    plt.plot(para, train_accuracy, label='train error')\n    plt.legend()\n    plt.xlabel('Value of alpha')\n    plt.ylabel('RMSE')\n    plt.show()\nprint(\"[Holdout][Ridge Regression]\")    \nplotErr(0,X_train,Y_train,10,X_validation,Y_validation)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3862ebf80d58194f4f7692b05216b3ad0b3bd1ed","collapsed":true},"cell_type":"code","source":"# Create ridge regression model\nridge = Ridge(fit_intercept=True, alpha=p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37f56c93a661f44fd0c58546d5692fe7a56ba446"},"cell_type":"code","source":"# Train the model using the training set\nridge.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"925fa78fc60ae887cd4bf183cece322298fd3c52"},"cell_type":"code","source":"# Compute RMSE on training data\np = ridge.predict(X_train)\nerr = p-Y_train\ntotal_error = np.dot(err,err)\nrmse_train = np.sqrt(total_error/len(p))\nprint(\"[Holdout][Ridge Regression]\\nTraining error : {:.2f}\".format(rmse_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03930834273e5ca3302c32bc2aaf53b1d3d032f3"},"cell_type":"code","source":"#Compute RMSE on validation data\np = ridge.predict(X_validation)\nerr = p-Y_validation\ntotal_error = np.dot(err,err)\nrmse_validation = np.sqrt(total_error/len(p))\nprint(\"[Holdout][Ridge Regression]\\nValidation error : {:.2f}\".format(rmse_validation))\n#accuracy = p.score(X_validation,Y_validation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0cecfa48dbfdc0e329f027c148fdeaf471062af8"},"cell_type":"code","source":"#Compute RMSE on test data\np = ridge.predict(X_test)\nerr = p-Y_test\ntotal_error = np.dot(err,err)\nrmse_test = np.sqrt(total_error/len(p))\nholdoutE.append(rmse_test)\nprint(\"[Holdout][Ridge Regression]\\ntest error : {:.2f}\".format(rmse_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5371c9a8c1604794a6d3993752c80e298ce1b2b7"},"cell_type":"code","source":"print(\"[Holdout][Ridge Regression]\")\nmodel_scatter_plot(p,Y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"acd91593835a45c44bb98cee28e48be888638d31"},"cell_type":"markdown","source":"### [Holdout] Residual Plot for Ridge Regression"},{"metadata":{"trusted":true,"_uuid":"583975c352267857a89f352de4e6fc9c6bcee4a2"},"cell_type":"code","source":"plt.figure(figsize=(10,5)) #controls the graph size\nplt.title('[Holdout] Residual Plot for Ridge Regression', y=1, size=20) #\"size\" controls the title size\nsns.residplot(Y_test,p) ## regression Residual Plot for Ridge Regression model using bootstrapping","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"810dadec80d4e56471789eb34ea0561febd871bd"},"cell_type":"markdown","source":"## [Holdout] Random Forest"},{"metadata":{"_uuid":"5df97b21c00f1b7603cc88a953bef1eb3991dec3"},"cell_type":"markdown","source":"**Hyperparameter tuning ** : parameter \"max depth\"  for random forest using validation set"},{"metadata":{"trusted":true,"_uuid":"9a718d6d31c6a28a82d9e553b7b1bfe8c74b2af1"},"cell_type":"code","source":"#hyperparameters\npara=np.arange(2,100,1)\nerror=dict()\n\n#tuning hyperparameter\nprint(\"Parameter   RMSE Error\")\nprint(\"-\"*25)\nfor parameter in para:\n    regr = RandomForestRegressor(max_depth=parameter, random_state=0)\n    regr.fit(X_train,Y_train)\n    \n    terr = calc_train_error(X_train,Y_train,regr)\n    verr = calc_validation_error(X_validation,Y_validation,regr)\n    total_error =np.abs(terr-verr)\n    error[parameter]= total_error\n    p= min(error,key = error.get)\n    print(\"{:.3f}        {:.3f}\".format(parameter,error[parameter]))\n    \nprint(\"Best parameter after validation: {:.3f}\".format(p))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0de28796ec0570f2b7ef2be89e26d851d441b05f"},"cell_type":"code","source":"print(\"[Holdout][Random Forest]\")\nplotErr(1,X_train,Y_train,100,X_validation,Y_validation) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa6f4ccf0cf2e42fcd594c6e1a66447cbda441a5"},"cell_type":"code","source":"# Create random forest regression model\nregr = RandomForestRegressor(max_depth=p, random_state=0)\n\n# Train the model using the training set\nregr.fit(X_train,Y_train)\n\n# Compute RMSE on training data\np = regr.predict(X_train)\nerr = p-Y_train\ntotal_error = np.dot(err,err)\nrmse_train = np.sqrt(total_error/len(p))\nprint(\"[Holdout][Random Forest]\\nTraining error : {:.2f}\".format(rmse_train))\n\n#Compute RMSE on validation data\np = regr.predict(X_validation)\nerr = p-Y_validation\ntotal_error = np.dot(err,err)\nrmse_validation = np.sqrt(total_error/len(p))\nprint(\"Validation error : {:.2f}\".format(rmse_validation))\n#accuracy = p.score(X_validation,Y_validation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"737d258900899c00ef423d11542f60373718ae5d"},"cell_type":"code","source":"#Compute RMSE on test data\np = regr.predict(X_test)\nerr = p-Y_test\ntotal_error = np.dot(err,err)\nrmse_test = np.sqrt(total_error/len(p))\nholdoutE.append(rmse_test)\nprint(\"[Holdout][Random Forest]\\nTest error : {:.2f}\".format(rmse_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9222cc58d6b3d7082370ecb6f4de8a156decee63"},"cell_type":"code","source":"print(\"[Holdout][Random Forest]\")\nmodel_scatter_plot(p,Y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a280af0ca71a72791518e9d9a2a37316168d86ff"},"cell_type":"markdown","source":"### [Holdout] Residual Plot for Random Forest"},{"metadata":{"trusted":true,"_uuid":"5b40c4e3f26e7932ce077ae4b172cb135a7ec342"},"cell_type":"code","source":"plt.figure(figsize=(10,5)) #controls the graph size\nplt.title('[Holdout] Residual Plot for Random Forest', y=1, size=20) #\"size\" controls the title size\nsns.residplot(Y_test,p) ## regression Residual Plot for Random Forest model using bootstrapping","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92154c286b461349fad1646bbcea4edce2cfaac5"},"cell_type":"markdown","source":"## [Holdout] Support Vector Regression"},{"metadata":{"_uuid":"96b016b109ac5447d66d39ae5124072aa1ed33db"},"cell_type":"markdown","source":"**Hyperparameter tuning ** : parameter \"C\"  for support vector regression using validation set"},{"metadata":{"trusted":true,"_uuid":"6c3335490e5f7ad3da0eb6d915af0b1805532750"},"cell_type":"code","source":"#hyperparameters\npara=np.linspace(1,10,100)\nerror=dict()\n\n#tuning hyperparameter\nprint(\"Parameter   RMSE Error\")\nprint(\"-\"*25)\nfor parameter in para:\n    sv = SVR(C=parameter)\n    sv.fit(X_train,Y_train)\n    \n    terr = calc_train_error(X_train,Y_train,sv)\n    verr = calc_validation_error(X_validation,Y_validation,sv)\n    total_error =np.abs(terr-verr)\n    error[parameter]= total_error\n    p= min(error,key = error.get)\n    print(\"{:.3f}        {:.3f}\".format(parameter,error[parameter]))\n    \nprint(\"Best parameter after validation: {:.3f}\".format(p))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af4ef03d1317009fec8d8c3b80f2db4c122d3375"},"cell_type":"code","source":"print(\"[Holdout][Support Vector Regression]\")\nplotErr(2,X_train,Y_train,100,X_validation,Y_validation) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27c2e9c787c2bdddd87df9640ad935166cded024"},"cell_type":"code","source":"# Create support vector regression model\nsv = SVR(C=10)\n\n# Train the model using the training set\nsv.fit(X_train,Y_train)\n\n# Compute RMSE on training data\np = sv.predict(X_train)\nerr = p-Y_train\ntotal_error = np.dot(err,err)\nrmse_train = np.sqrt(total_error/len(p))\nprint(\"[Holdout][Support Vector Regressor]\\nTraining error : {:.2f}\".format(rmse_train))\n\n#Compute RMSE on validation data\np = sv.predict(X_validation)\nerr = p-Y_validation\ntotal_error = np.dot(err,err)\nrmse_validation = np.sqrt(total_error/len(p))\nprint(\"Validation error : {:.2f}\".format(rmse_validation))\n#accuracy = p.score(X_validation,Y_validation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b4e26c9d529fb7858b08faf74026617859b7231"},"cell_type":"code","source":"#Compute RMSE on test data\np = sv.predict(X_test)\nerr = p-Y_test\ntotal_error = np.dot(err,err)\nrmse_test = np.sqrt(total_error/len(p))\nholdoutE.append(rmse_test)\nprint(\"[Holdout][Support Vector regressor]\\nTest error : {:.2f}\".format(rmse_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"046fc70d1dddcaf143b6bd2b7f583fa37a5254ff"},"cell_type":"code","source":"print(\"[Holdout][Support Vector Regression]\")\nmodel_scatter_plot(p,Y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e2ad2f3039eb10f2e147715d21582d6c6831c98"},"cell_type":"markdown","source":"### [Holdout] Residual Plot for Support Vector Regression"},{"metadata":{"trusted":true,"_uuid":"1fb1b158892875bb1ae304bb88d57294db50be14"},"cell_type":"code","source":"plt.figure(figsize=(10,5)) #controls the graph size\nplt.title('[Holdout] Residual Plot for Support Vector Regression', y=1, size=20) #\"size\" controls the title size\nsns.residplot(Y_test,p) ## regression Residual Plot for Support Vector Regression model using bootstrapping","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76771268231f6ad45b1d4cee498f457de0c7d968"},"cell_type":"markdown","source":"## <a id=\"4-2\"> 4.2 Stratified holdout method </a>"},{"metadata":{"_uuid":"7562dfb298029af6089cb7a3cb52431062926e34"},"cell_type":"markdown","source":"The target variable is continuous. So to split the dataset using stratify we need to convert \"charges\" into bins."},{"metadata":{"trusted":true,"_uuid":"3037f6444ca761bcec05946b07b70d1f442b004a"},"cell_type":"code","source":"#create bins\nX[\"charges\"] = y\n\nbins = np.linspace(X.charges.min(),X.charges.max(),7)\ncharges_groups = np.digitize(X.charges,bins)\n\n#Add bin information to the dataframe\nX= X.assign(charge_groups = charges_groups)\nX['charge_groups'] = pd.Categorical(X.charge_groups)\n\n#Last bin has too few values to stratify, so we merge it with second last group and reduce the number of bins\nX.charge_groups[X.charge_groups == 7]= 6\ny_stratify= X[\"charge_groups\"]\nX.head()\n#X[\"charge_groups\"].unique()\nX.groupby(X[\"charge_groups\"]).count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"430d1179d3040656f3769a36fdee2577aad026cd"},"cell_type":"code","source":"X_intermediateS, X_testS, Y_intermediateS, Y_testS = train_test_split(X, \n                                                                  y_stratify, \n                                                                  shuffle=True,\n                                                                  test_size=0.2,\n                                                                  stratify=y_stratify\n                                                                  )\n\n#train/validation split (gives us train and validation sets)\nX_trainS, X_validationS, Y_trainS, Y_validationS = train_test_split(X_intermediateS,\n                                                                Y_intermediateS,\n                                                                shuffle=True,\n                                                                test_size=0.2,\n                                                                stratify=Y_intermediateS\n                                                                )\n\n#remove the \"charges\" and \"charge_groups\" column and separate target variable\nX_str=X_intermediateS.copy()\ny_str=Y_intermediateS\nY_intermediateS=X_intermediateS['charges']\ndel X_intermediateS[\"charges\"]\ndel X_intermediateS[\"charge_groups\"]\nY_trainS=X_trainS[\"charges\"]\ndel X_trainS[\"charges\"]\ndel X_trainS[\"charge_groups\"]\n\nY_testS=X_testS[\"charges\"]\ndel X_testS[\"charges\"]\ndel X_testS[\"charge_groups\"]\nX_str.head()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"519d27e83103415effe9e11f760b118772f58e30","trusted":true},"cell_type":"code","source":"holdoutStf=[] #to store rmse error\nprint('train: {}% | validation: {}% | test {}%'.format(round(len(Y_trainS)/len(y),2),\n                                                       round(len(Y_validationS)/len(y),2),\n                                                       round(len(Y_testS)/len(y),2)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e56e207b05204fc70e714b075b6110fcc042e70c"},"cell_type":"markdown","source":"## <a id=\"4-3\"> 4.3 Stratified k-folds cross-validation </a>"},{"metadata":{"_uuid":"f0dd5dde03fd25e33d999ad5f518d65a07d3a06e"},"cell_type":"markdown","source":"Stratified K-Folds cross-validator\nProvides train/test indices to split data in train/test sets.\nThis cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."},{"metadata":{"trusted":true,"_uuid":"1d2b94946f92001e443e8f62031785dcf9d3f393"},"cell_type":"code","source":"from sklearn import model_selection\nstratifiedkfold=[]\ndel X_str[\"charges\"]\ndel X_str[\"charge_groups\"]\nnum_of_splits=5\ndef stratifiedKFold (model): \n    kf = model_selection.StratifiedKFold(n_splits=num_of_splits,shuffle=True)\n    pred_test_full =0\n    cv_score =[]\n    i=1\n    for train_index,test_index in kf.split(X_str,y_str):\n        print('{} of StratifiedKFold {}'.format(i,kf.n_splits))\n        \n        xtr,xvl = X_intermediateS.iloc[train_index],X_intermediateS.iloc[test_index]\n        ytr,yvl = Y_intermediateS.iloc[train_index],Y_intermediateS.iloc[test_index]\n\n        model.fit(xtr,ytr)\n        rmse = calc_train_error(xtr,ytr,model)\n        print('Train RMSE :{:.2f}'.format(rmse))\n        rmse = calc_train_error(xvl,yvl,model)\n        print('Validation RMSE :{:.2f}'.format(rmse))\n        print('_'*50)\n        cv_score.append(rmse)    \n        pred_test = model.predict(X_testS)\n        pred_test_full +=pred_test\n        i+=1\n       \n    print('Mean Stratified K Fold CV Score : {:.2f}'.format(np.mean(cv_score))) \n    return np.mean(cv_score), pred_test_full\n      ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6b58f524a5c288d1ee2fa8476566cb41f898217"},"cell_type":"markdown","source":"## [Stratified K Fold] Linear Regression"},{"metadata":{"trusted":true,"_uuid":"93bcd8932904443b239d3ef309a9b4ad685931ff"},"cell_type":"code","source":"linreg = LinearRegression()\nerr,y_pred=stratifiedKFold (linreg)\nstratifiedkfold.append(err)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dcc2b73aa6652b8e0115a1dda37fb9110e687a2e"},"cell_type":"code","source":"y_pred = y_pred/num_of_splits\nprint(\"[Stratified K Fold][Linear Regression]\")\nmodel_scatter_plot(y_pred,Y_testS)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d99cccfa245553306f022206e2547cbbcf0c578"},"cell_type":"markdown","source":"### [Stratified K Fold CV] Residual Plot for Linear Regression"},{"metadata":{"trusted":true,"_uuid":"dda7fa4ff2941ecb5c69e45db7366f124d459b20"},"cell_type":"code","source":"plt.figure(figsize=(10,5)) #controls the graph size\nplt.title('[Stratified K Fold CV] Residual Plot for Linear Regression', y=1, size=20) #\"size\" controls the title size\nsns.residplot(Y_testS,y_pred) ## regression Residual Plot for linear regression model using bootstrapping","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14c3703997b74942bd219a0a9e6ae203185ba0ff"},"cell_type":"markdown","source":"## [Stratified K Fold] Ridge Regression"},{"metadata":{"trusted":true,"_uuid":"cea7ea6ab27b350fe170255b3c857d359196081e"},"cell_type":"code","source":"ridge = Ridge()\nerr,y_pred=stratifiedKFold (ridge)\nstratifiedkfold.append(err)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e4e1cf5b8c520388493f24a625c8478f1247558"},"cell_type":"code","source":"y_pred = y_pred/num_of_splits\nprint(\"[Stratified K Fold][Ridge Regression]\")\nmodel_scatter_plot(y_pred,Y_testS)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28285fd32b15d72c9833f133e7c5110502e5c27c"},"cell_type":"markdown","source":"### [Stratified K Fold CV] Residual Plot for Ridge Regression"},{"metadata":{"trusted":true,"_uuid":"0eb46a47d4617652aad041da024d63455f27b5a7"},"cell_type":"code","source":"plt.figure(figsize=(10,5)) #controls the graph size\nplt.title('[Stratified K Fold CV] Residual Plot for Ridge Regression', y=1, size=20) #\"size\" controls the title size\nsns.residplot(Y_testS,y_pred) ## regression Residual Plot for Ridge regression model using bootstrapping","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75c6ea3f0a450e724ae4047d12661af5212603ae"},"cell_type":"markdown","source":"## [Stratified K Fold] Random Forest"},{"metadata":{"trusted":true,"_uuid":"f9c3b878d6b0071968ae72ea9e8f74fbf0ab8f82"},"cell_type":"code","source":"rf = RandomForestRegressor()\nerr,y_pred=stratifiedKFold (rf)\nstratifiedkfold.append(err)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4a72f8d2e9d4285737ad81a76a763bb62883a1b"},"cell_type":"code","source":"y_pred = y_pred/num_of_splits\nprint(\"[Stratified K Fold][Random forest]\")\nmodel_scatter_plot(y_pred,Y_testS)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98757f913ef514e3bef9c810e1ec065e09dcf2a4"},"cell_type":"markdown","source":"### [Stratified K Fold CV] Residual Plot for Random Forest"},{"metadata":{"trusted":true,"_uuid":"143d266aa288cb591a113748c0e3c36b9552ad29"},"cell_type":"code","source":"plt.figure(figsize=(10,5)) #controls the graph size\nplt.title('[Stratified K Fold CV] Residual Plot for Random Forest', y=1, size=20) #\"size\" controls the title size\nsns.residplot(Y_testS,y_pred) ## regression Residual Plot for Random Forest model using bootstrapping","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ea79e1bde623a21788e2c294f1b3898b0e0345f"},"cell_type":"markdown","source":"## [Stratified K Fold] Support Vector Regression"},{"metadata":{"trusted":true,"_uuid":"a398901b1c5460da9f8b2b953d5b47455e7101af"},"cell_type":"code","source":"sv = SVR()\nerr,y_pred=stratifiedKFold (sv)\nstratifiedkfold.append(err)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f024480ca6fe04fae7f2646892046dbe1b418ae"},"cell_type":"code","source":"y_pred = y_pred/num_of_splits\nprint(\"[Stratified K Fold][Support Vector Regression]\")\nmodel_scatter_plot(y_pred,Y_testS)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65cc23ab1b383871bd32d90041ddd15af4d15629"},"cell_type":"markdown","source":"### [Stratified K Fold CV] Residual Plot for Support Vector Regression"},{"metadata":{"trusted":true,"_uuid":"f1b8505cb16e2ccc9afb5de04a1557e0902f306f"},"cell_type":"code","source":"plt.figure(figsize=(10,5)) #controls the graph size\nplt.title('[Stratified K Fold CV] Residual Plot for Support Vector Regression', y=1, size=20) #\"size\" controls the title size\nsns.residplot(Y_testS,y_pred) ## regression Residual Plot for Support Vector regression model using bootstrapping","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9329c2b32818c1f8c30018b6a9bcc03bb2d07f33"},"cell_type":"markdown","source":"# <a id=\"4-4\">  4.4 Bootstrapping </a>"},{"metadata":{"_uuid":"abd07520bab792b7ed2628ef8c18c56855a4ef36"},"cell_type":"markdown","source":"The sample will be selected with replacement using the resample() function. Any rows that were not included in the sample are retrieved and used as the test dataset. Next, a decision tree classifier is fit on the sample and evaluated on the test set, a classification score calculated, and added to a list of scores collected across all the bootstraps."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"da07cc5faad7a6f8f9c3aa334473eebac5a0a786"},"cell_type":"code","source":"from sklearn.utils import resample\n#from sklearn.metrics import accuracy_score\n\nbootstra=[]\n# load dataset\nX_intermediate[\"charges\"]= Y_intermediate\ndata = X_intermediate\nvalues = data.values\n\n# configure bootstrap\nn_iterations = 10 #number of iterations\nn_size = int(len(data) * 1) #sample size\n# run bootstrap\nstats = list()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92e0ac38665245294f8229a69b55006f45caf96a"},"cell_type":"code","source":"def bootstrap(model_name):\n    for i in range(n_iterations):\n        # prepare train and test sets\n        train = resample(values, n_samples=n_size)\n        test = np.array([x for x in values if x.tolist() not in train.tolist()])\n        # fit model\n        model = model_name\n        model.fit(train[:,:-1], train[:,-1])\n         # evaluate model\n        train_pred = model.predict(train[:,:-1])\n        mse = mean_squared_error(train[:,-1], train_pred)\n        err_train = np.sqrt(mse)\n       # err_train = calc_train_error(train[:,-1], train_pred,model)\n        test_pred = model.predict(test[:,:-1])\n        mse = mean_squared_error(test[:,-1], test_pred)\n        err_test = np.sqrt(mse)\n        #err_test = calc_train_error(test[:,-1], test_pred,model)\n        print(\"{} of iteration {}\".format(i,n_iterations-1))\n        print(\"RMSE for Train : {:.2f}\".format(err_train))\n        print(\"RMSE for Validation : {:.2f}\".format(err_test))\n      #  print(rmse)\n        print(\"-\"*50)\n        stats.append(err_test)\n    print(\"Average RMSE for Validation : {:.2f}\".format(np.mean(stats))) \n    res= (np.mean(stats))\n    # plot rmse\n    plt.hist(stats)\n    plt.show()\n    # confidence intervals\n    alpha = 0.95\n    p = ((1.0-alpha)/2.0) * 100\n    lower = max(0.0, np.percentile(stats, p))\n    p = (alpha+((1.0-alpha)/2.0)) * 100\n    upper = max(1.0, np.percentile(stats, p))\n    print('{:.0f} % confidence interval {:.2f} and {:.2f}'.format(alpha*100, lower, upper))\n    return np.mean(stats)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"874b7ea0c938431a3a905adea4edd609cac56397"},"cell_type":"markdown","source":"## [Bootstrapping] Linear Regression"},{"metadata":{"_kg_hide-output":false,"trusted":true,"_uuid":"326c3ee2ca4682b7e228479d23190feead4d32f7"},"cell_type":"code","source":"print(\"[Logistic Regression] The RMSE for each bootstrap iteration: \")\nbootstra.append(bootstrap(LinearRegression()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3b61445d1cd2d5af4885521ae1192d815b956bf"},"cell_type":"markdown","source":"Intrepretation: With 95 % probability I can say that my model RMSE error will lie in the range <.... >  and <.....> on testing on unseen dataset in future."},{"metadata":{"_uuid":"ceb8b69387ba10f3a92e4586ca67d52254c7c2e8"},"cell_type":"markdown","source":"### [Bootstrapping][Linear Regression]Scatter plot of true and predicted cost after bootstrapping"},{"metadata":{"trusted":true,"_uuid":"9bcaf178b469acd57c563a7fbfe193169ddc3bc6"},"cell_type":"code","source":"linreg=LinearRegression()\nlinreg.fit(X_train,Y_train)\npred=linreg.predict(X_test)\nmse = mean_squared_error(Y_test,pred)\nerr_test = np.sqrt(mse)\nprint(\"[Bootstrap][Linear Regression]\\nRMSE on Test data: {:.2f}\".format(err_test))\nmodel_scatter_plot(pred,Y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"054c9cad0f47fa975d367b3cdb47af5a83328f97"},"cell_type":"markdown","source":"So, it is now verified with 95% probability that our model Linear Regression will give RMSE in range <....> to <....> , as RMSE on test set is <....> ."},{"metadata":{"_uuid":"f9877952560fd23d83ed2d82a68265ca4696ee51"},"cell_type":"markdown","source":"### [Bootstrapping] Residual Plot for Linear Regression"},{"metadata":{"trusted":true,"_uuid":"967e576d70af18008d6f5f65282baf71575c354f"},"cell_type":"code","source":"plt.figure(figsize=(10,5)) #controls the graph size\nplt.title('[Bootstrapping] Residual Plot for Linear Regression', y=1, size=20) #\"size\" controls the title size\nsns.residplot(Y_test,pred) ## regression Residual Plot for Linear regression model using bootstrapping","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79cf029e289c04558e696364654993cbd1def5d5"},"cell_type":"markdown","source":"## [Bootstrapping] Ridge Regression"},{"metadata":{"trusted":true,"_uuid":"491ae3be4d41d01ba3fb893d6e0b9357eba72800"},"cell_type":"code","source":"print(\"[Ridge Regression] The RMSE for each bootstrap iteration: \")\nbootstra.append(bootstrap(Ridge()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aba2fbc8d4a3790d3606d2c1b1ffd96553a7f1f1"},"cell_type":"markdown","source":"Intrepretation: With 95 % probability I can say that my model RMSE error will lie in the range <.... >  and <.....> on testing on unseen dataset in future."},{"metadata":{"_uuid":"a1f22ce284fd2f112fd441c27dd05c16ecd1163a"},"cell_type":"markdown","source":"### [Bootstrapping][Ridge Regression]Scatter plot of true and predicted cost after bootstrapping"},{"metadata":{"trusted":true,"_uuid":"3e2001522e071633d34822faeb0077621e99e346"},"cell_type":"code","source":"ridge=Ridge()\nridge.fit(X_train,Y_train)\npred=ridge.predict(X_test)\nmse = mean_squared_error(Y_test,pred)\nerr_test = np.sqrt(mse)\nprint(\"[Bootstrap][Linear Regression]\\nRMSE on Test data: {:.2f}\".format(err_test))\nmodel_scatter_plot(pred,Y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54ac4771478416c60a03fc1446fc54cdc5cda628"},"cell_type":"markdown","source":"So, it is now verified with 95% probability that our model Ridge Regression will give RMSE in range <....> to <....> , as RMSE on test set is <....> ."},{"metadata":{"_uuid":"59b90368de3e437c87299d39e1e4dcbb47612a27"},"cell_type":"markdown","source":"### [Bootstrapping] Residual Plot for Ridge Regression"},{"metadata":{"trusted":true,"_uuid":"dcfa0ebdffb55a9f365688cbac0dbcd18eaee744"},"cell_type":"code","source":"plt.figure(figsize=(10,5)) #controls the graph size\nplt.title('[Bootstrapping] Residual Plot for Ridge Regression', y=1, size=20) #\"size\" controls the title size\nsns.residplot(Y_test,pred) ## regression Residual Plot for Ridge regression model using bootstrapping","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bcb01e7dd519043e7af409b1a8f8eae8349077bb"},"cell_type":"markdown","source":"## [Bootstrapping] Random Forest"},{"metadata":{"trusted":true,"_uuid":"ca503aa43c703c95284ad1a47bb0d08ede68aafe"},"cell_type":"code","source":"print(\"[Random Forest] The RMSE for each bootstrap iteration: \")\nbootstra.append(bootstrap(RandomForestRegressor()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76c78171cc790ce232eb92425fd820162d0bc68d"},"cell_type":"markdown","source":"Intrepretation: With 95 % probability I can say that my model RMSE error will lie in the range <.... >  and <.....> on testing on unseen dataset in future."},{"metadata":{"_uuid":"c7639a62fb15d1623d7b28bd424ef951e8ef802f"},"cell_type":"markdown","source":"### [Bootstrapping][Random Forest]Scatter plot of true and predicted cost after bootstrapping"},{"metadata":{"trusted":true,"_uuid":"75c973c78007afeed270fde9ee25249c52258d44"},"cell_type":"code","source":"rf=RandomForestRegressor()\nrf.fit(X_train,Y_train)\npred=rf.predict(X_test)\nmse = mean_squared_error(Y_test,pred)\nerr_test = np.sqrt(mse)\nprint(\"[Bootstrap][Random Forest]\\nRMSE on Test data: {:.2f}\".format(err_test))\nmodel_scatter_plot(pred,Y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e9882d91d758b9120899a54bf3f96172ef90455"},"cell_type":"markdown","source":"So, it is now verified with 95% probability that our model Random Forest will give RMSE in range <....> to <....> , as RMSE on test set is <....> ."},{"metadata":{"_uuid":"bc010192cd8501af6bbf9a14f5947898c860ba4e"},"cell_type":"markdown","source":"### [Bootstrapping] Residual Plot for Random Forest"},{"metadata":{"trusted":true,"_uuid":"f8e6482f75b7cb90622c240beb6b48d43367170f"},"cell_type":"code","source":"plt.figure(figsize=(10,5)) #controls the graph size\nplt.title('[Bootstrapping] Residual Plot for Random Forest', y=1, size=20) #\"size\" controls the title size\nsns.residplot(Y_test,pred) ## regression Residual Plot for Random Forest model using bootstrapping","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5e4f0d6350951681c6dafd5c737a03707a39615"},"cell_type":"markdown","source":"## [Bootstrapping] Support Vector Regression"},{"metadata":{"trusted":true,"_uuid":"0cc2649204cb00165c3bede4d2b5d153635beb00"},"cell_type":"code","source":"print(\"[Support Vector Regression] The RMSE for each bootstrap iteration: \")\nbootstra.append(bootstrap(SVR()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98b48a68f1afcdb939de9071cc6f1f8d364268db"},"cell_type":"markdown","source":"Intrepretation: With 95 % probability I can say that my model RMSE error will lie in the range <.... >  and <.....> on testing on unseen dataset in future."},{"metadata":{"_uuid":"2400c587009e06113230a38f64ae309af93237fa"},"cell_type":"markdown","source":"### [Bootstrapping][Support Vector Regression]Scatter plot of true and predicted cost after bootstrapping"},{"metadata":{"trusted":true,"_uuid":"ba8cb2b5d648b389be3cff92ce9dedae27e28443"},"cell_type":"code","source":"sv=SVR()\nsv.fit(X_train,Y_train)\npred=sv.predict(X_test)\nmse = mean_squared_error(Y_test,pred)\nerr_test = np.sqrt(mse)\nprint(\"[Bootstrap][Support Vector Regression]\\nRMSE on Test data: {:.2f}\".format(err_test))\nmodel_scatter_plot(pred,Y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee0cb81b6f99900d6b31f10b7cc40d56e627a899"},"cell_type":"markdown","source":"So, it is now verified with 95% probability that our model SVR will give RMSE in range <....> to <....> , as RMSE on test set is <....> ."},{"metadata":{"_uuid":"27caadab9f33b8ab47253b7444de895fa6e82f69"},"cell_type":"markdown","source":"### [Bootstrapping] Residual Plot for Support Vector Regression"},{"metadata":{"trusted":true,"_uuid":"7b473d4f940e5fa1e684304a364f97594be823f4"},"cell_type":"code","source":"plt.figure(figsize=(10,5)) #controls the graph size\nplt.title('[Bootstrapping] Residual Plot for Support Vector Regression', y=1, size=20) #\"size\" controls the title size\nsns.residplot(Y_test,pred) ## regression Residual Plot for Support vector regression model using bootstrapping","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"205999a243da65a4c36c45043299be6f37cee9b0"},"cell_type":"markdown","source":"## <a id=\"8\"> 8. Comparison of Performance of Model using Different  Splitting methods  </a>"},{"metadata":{"trusted":true,"_uuid":"4f2cb6d63e55bde81ce8553c99c1ed65c14d87d3"},"cell_type":"code","source":"# data to plot\nn_groups = 4\nmeans_Holdout = (holdoutE[0], holdoutE[1], holdoutE[2], holdoutE[3])\nmeans_Stratified_Holdout=[4324,2322,5654,3345]\n#means_Stratified_Holdout = (holdoutStf[0], holdoutstf[1], holdoutstf[2], holdoutstf[3])\nmeans_StratifiedKFold = (stratifiedkfold[0], stratifiedkfold[1], stratifiedkfold[2], stratifiedkfold[3])\nmeans_Bootstrapping = (bootstra[0], bootstra[1], bootstra[2], bootstra[3])\n \n# create plot\n \nfig,ax = plt.subplots(figsize=(20,8))\nindex = np.arange(n_groups)\nbar_width = 0.21\nopacity = 0.8\n#plt.figure(figsize=(12,7))\nrects1 = plt.bar(index+bar_width, means_Holdout, bar_width,\n                 alpha=opacity,\n                 color='b',\n                 label='Holdout')\n \nrects2 = plt.bar(index + 2*bar_width, means_Stratified_Holdout, bar_width,\n                 alpha=opacity,\n                 color='g',\n                 label='Stratified Holdout')\n\nrects3 = plt.bar(index+ 3*bar_width, means_StratifiedKFold, bar_width,\n                 alpha=opacity,\n                 color='r',\n                 label='Stratified K Fold')\nrects4 = plt.bar(index+ 4*bar_width, means_Bootstrapping, bar_width,\n                 alpha=opacity,\n                 color='m',\n                 label='Bootstrapping')\n \nax.set_ylim([0,10000])   \nplt.xlabel('Model',fontsize=16)\nplt.ylabel('RMSE',fontsize=16)\nplt.title('Model comparsion on basis of RMSE',fontsize=18)\nplt.xticks(index + 2*0.18, ('Linear Regression', 'Ridge Regression ', 'Random Forest','Support Vector Regression'),rotation=20, fontsize=12)\nplt.legend(loc='left', bbox_to_anchor=(0.15,1))\n\n\ndef autolabel(rects):\n    \"\"\"\n    Attach a text label above each bar displaying its height\n    \"\"\"\n    for rect in rects:\n        height = rect.get_height()\n        ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,\n                '{:.2f}'.format(height),\n                ha='center', va='bottom')\n\nautolabel(rects1)\nautolabel(rects2)\nautolabel(rects3)\nautolabel(rects4)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2c1b7e84344e166622d1c4052342078a96e1443"},"cell_type":"markdown","source":"# <a id=\"7\">7.   Best Regression Model </a>"},{"metadata":{"_uuid":"4f12c4c3ae974f620002b31255b877182665eb3a"},"cell_type":"markdown","source":"> * The best classifier model for the given dataset \"Teleco Customer Churn\" provided by IBM is <        >  . \n\n        **ROC score is <......>**\n\n> * The method used to split the dataset into train, vaildation and test set  is  <   >.\n> * Click here **to see the model** ->>>  <a id=\" \" > CLICK ME </a>\n> * Click here t**o see the ROC Curve plot** ->>>  <a id=\" \" > CLICK ME </a>\n> * Click here t**o see the Confusion Matrix and  TP and TN ** ->>>  <a id=\" \" > CLICK ME </a>"},{"metadata":{"_uuid":"ad2c27ae332aacc33bb8560eba2a77b9d08959ac"},"cell_type":"markdown","source":"   For the **evaluation of the performance of model I choose ROC curve**. This because **our dataset is  imbalanced**\n   \n   The a*rea under the ROC curve ( AUC ) is a measure of how well a parameter can distinguish between two  groups (churned/retened).*\n\n   Our dataset is imbalanced .  'O' i.e who not churn  is  4621 while '1' i.e who churn is  1738. "},{"metadata":{"_uuid":"68994ec2c155eebe44d3d6d8feec69ab09457c41"},"cell_type":"markdown","source":"Original dataset has 20 features. After doing feature engineering and doing feature selection , we are left with only <   ....>  many features.\nThose features are: \n\n     'Partner',     'InternetService_Fiber optic',  'OnlineSecurity_Yes',  'OnlineBackup_Yes',  'DeviceProtection_Yes',\n     'TechSupport_Yes',  'MonthlyCharges',  'StreamingMovies_No internet service',   'Contract_One year', 'Contract_Two year',\n     'PaymentMethod_Credit card (automatic)',  'PaymentMethod_Electronic check',  #'PowerUser_Yes', 'FamilyMultiple_Yes'"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}