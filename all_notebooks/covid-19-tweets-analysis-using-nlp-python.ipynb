{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\nIn this Notebook, I am going to discuss a practical guide of`Natural Language Processing(NLP) using Python`.\n\nBefore we move further, I will just take a look at the concept of Corona Virus namely CoVid-19.\n\n**CoVid-19:** Coronavirus disease (CoVid-19) is an infectious disease that is caused by a newly discovered coronavirus. Most of the people who have been infected with the CoVid-19 virus will experience mild to adequate respiratory illness and some will recover without requiring any special treatment. Older or aged people and those with intrinsic medical problems like cardiovascular disease(heart diseases), diabetes, chronic respiratory disease, and cancer are more likely to create serious illnesses.\n![26604co.jpg](https://editor.analyticsvidhya.com/uploads/26604co.jpg)\n\nThe COVID-19 virus can spread through droplets of saliva or release from the nose when an infected person coughs or sneezes.\n\nNow we will see how to perform CoVid-19 tweets analysis. Let’s get started…\n\n# Dataset\nHere I have used a dataset of `coronavirus tweets NLP`.\n\nTake a look at the description of the data:\n\nThe tweets have been taken from Twitter. Whatever the names and usernames have been given codes is just to avoid privacy concerns.\n\n**Columns:**\n1) Location- Location of user\n\n2) Tweet At- Date of a tweet\n\n3) Original Tweet- actual tweet text\n\n4) Sentiment- sentiments(we can say emotions) like positive, negative, neutral, etc\n\n# Implementation\n\n**1)** Here we need to import the necessary libraries that be required for our model. In the above code, we have imported libraries such as pandas to deal with data frames/datasets, re for regular expression, nltk is a natural language tool kit and from that, we have imported module – stopwords which are nothing but ‘dictionary’. \n\nAs shown below:","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-06-23T08:02:14.939321Z","iopub.execute_input":"2021-06-23T08:02:14.939726Z","iopub.status.idle":"2021-06-23T08:02:16.916264Z","shell.execute_reply.started":"2021-06-23T08:02:14.939646Z","shell.execute_reply":"2021-06-23T08:02:16.915357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2)** Here we have read the file named “Corona_NLP_train” in CSV(comma-separated value) format. And have checked for the top 5 values in the dataset using head()","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_train.csv\",encoding='latin1')\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T08:13:57.397234Z","iopub.execute_input":"2021-06-23T08:13:57.39761Z","iopub.status.idle":"2021-06-23T08:13:57.569762Z","shell.execute_reply.started":"2021-06-23T08:13:57.397579Z","shell.execute_reply":"2021-06-23T08:13:57.568728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(data)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T08:14:32.52547Z","iopub.execute_input":"2021-06-23T08:14:32.525843Z","iopub.status.idle":"2021-06-23T08:14:32.53047Z","shell.execute_reply.started":"2021-06-23T08:14:32.52581Z","shell.execute_reply":"2021-06-23T08:14:32.529422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T08:14:33.761896Z","iopub.execute_input":"2021-06-23T08:14:33.76254Z","iopub.status.idle":"2021-06-23T08:14:33.776464Z","shell.execute_reply.started":"2021-06-23T08:14:33.762503Z","shell.execute_reply":"2021-06-23T08:14:33.775309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3)** Further, I have performed some data visualizations using matplotlib and seaborn libraries which are really the best visualization libraries in Python. I have plotted only one graph, you can plot more graphs to see how your data is!","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.countplot(x='Sentiment', data=df, order=['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive'])\nplt.title(\"Sentiment\")\nplt.ylabel(\"Count\", fontsize = 12)\nplt.xlabel(\"Sentiments\",fontsize = 12)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T08:14:42.752885Z","iopub.execute_input":"2021-06-23T08:14:42.753293Z","iopub.status.idle":"2021-06-23T08:14:42.938609Z","shell.execute_reply.started":"2021-06-23T08:14:42.753255Z","shell.execute_reply":"2021-06-23T08:14:42.937493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**4)** In this step, we are able to see how the summary of our data like No. of columns with their data types.","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T08:14:48.02731Z","iopub.execute_input":"2021-06-23T08:14:48.027707Z","iopub.status.idle":"2021-06-23T08:14:48.058126Z","shell.execute_reply.started":"2021-06-23T08:14:48.027674Z","shell.execute_reply":"2021-06-23T08:14:48.056986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**5)** Here we will perform a regular expression function to remove any symbols and special characters, etc to get pure data.","metadata":{}},{"cell_type":"code","source":"reg = re.compile(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)|([^0-9A-Za-z t])|(w+://S+)\")\ntweet = []\nfor i in df[\"OriginalTweet\"]:\n    tweet.append(reg.sub(\" \", i))\ndf = pd.concat([df, pd.DataFrame(tweet, columns=[\"CleanedTweet\"])], axis=1, sort=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T08:15:06.498204Z","iopub.execute_input":"2021-06-23T08:15:06.498581Z","iopub.status.idle":"2021-06-23T08:15:08.083086Z","shell.execute_reply.started":"2021-06-23T08:15:06.498547Z","shell.execute_reply":"2021-06-23T08:15:08.082115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**6)** Now we can see cleaned data obtained from the above code.","metadata":{}},{"cell_type":"code","source":"df.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T08:15:11.037957Z","iopub.execute_input":"2021-06-23T08:15:11.038305Z","iopub.status.idle":"2021-06-23T08:15:11.055384Z","shell.execute_reply.started":"2021-06-23T08:15:11.038273Z","shell.execute_reply":"2021-06-23T08:15:11.054391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**7)** now convert text into the matrix of tokens, we have to import the following library and perform code.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nstop_words = set(stopwords.words('english'))     # make a set of stopwords\nvectoriser = TfidfVectorizer(stop_words=None)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T08:16:44.117831Z","iopub.execute_input":"2021-06-23T08:16:44.118214Z","iopub.status.idle":"2021-06-23T08:16:44.126607Z","shell.execute_reply.started":"2021-06-23T08:16:44.118177Z","shell.execute_reply":"2021-06-23T08:16:44.125345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**8)** LabelEncoder is used here for transforming categorical values into numerical values.","metadata":{}},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"X_train = vectoriser.fit_transform(df[\"CleanedTweet\"])\n# Encoding the classes in numerical values\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ny_train = encoder.fit_transform(df['Sentiment'])\nfrom sklearn.naive_bayes import MultinomialNB\nclassifier = MultinomialNB()\nclassifier.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T08:17:37.261541Z","iopub.execute_input":"2021-06-23T08:17:37.262327Z","iopub.status.idle":"2021-06-23T08:17:39.172835Z","shell.execute_reply.started":"2021-06-23T08:17:37.262278Z","shell.execute_reply":"2021-06-23T08:17:39.171891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**9)** Let’s do all operations for test data also.","metadata":{}},{"cell_type":"code","source":"# importing the Test dataset for prediction and testing purposes\ntest_data = pd.read_csv(\"/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_test.csv\",encoding='latin1')\ntest_df = pd.DataFrame(test_data)\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T08:18:32.190051Z","iopub.execute_input":"2021-06-23T08:18:32.190512Z","iopub.status.idle":"2021-06-23T08:18:32.237001Z","shell.execute_reply.started":"2021-06-23T08:18:32.190472Z","shell.execute_reply":"2021-06-23T08:18:32.235879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**10)** Here we will perform a regular expression function to remove any symbols and special character, etc to get pure test data.","metadata":{}},{"cell_type":"code","source":"reg1 = re.compile(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)|([^0-9A-Za-z t])|(w+://S+)\")\ntweet = []\nfor i in test_df[\"OriginalTweet\"]:\n    tweet.append(reg1.sub(\" \", i))\ntest_df = pd.concat([test_df, pd.DataFrame(tweet, columns=[\"CleanedTweet\"])], axis=1, sort=False)\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T08:19:38.897925Z","iopub.execute_input":"2021-06-23T08:19:38.898271Z","iopub.status.idle":"2021-06-23T08:19:39.072693Z","shell.execute_reply.started":"2021-06-23T08:19:38.898233Z","shell.execute_reply":"2021-06-23T08:19:39.071801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**11)** By using vectorization, we have performed normalization of test data and stored it into x_test & y_test. We have also predicted actual and predicted values.","metadata":{}},{"cell_type":"code","source":"X_test = vectoriser.transform(test_df[\"CleanedTweet\"])\ny_test = encoder.transform(test_df[\"Sentiment\"])\n# Prediction\ny_pred = classifier.predict(X_test)\npred_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\npred_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T08:20:22.233019Z","iopub.execute_input":"2021-06-23T08:20:22.233387Z","iopub.status.idle":"2021-06-23T08:20:22.411797Z","shell.execute_reply.started":"2021-06-23T08:20:22.23334Z","shell.execute_reply":"2021-06-23T08:20:22.410786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**12)** So, at last, we have performed the accuracy of our model in the form of an AUC curve plotted using the matplotlib library.","metadata":{}},{"cell_type":"code","source":"from sklearn import metrics\n# Generate the roc curve using scikit-learn.\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=1)\nplt.plot(fpr, tpr)\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.show()\n# Measure the area under the curve. The closer to 1, the \"better\" the predictions.\nprint(\"AUC of the predictions: {0}\".format(metrics.auc(fpr, tpr)))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T08:21:28.258157Z","iopub.execute_input":"2021-06-23T08:21:28.258529Z","iopub.status.idle":"2021-06-23T08:21:28.406898Z","shell.execute_reply.started":"2021-06-23T08:21:28.258498Z","shell.execute_reply":"2021-06-23T08:21:28.405821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we got a score of `AUC – 0.64` for the classifier (Naive Byes), we can say that the classifier (Naive Bayes) is not that so good but can acceptable. Since the more nearer to 1 AUC score, the classifier will be better.\n\nIn the same way, we can perform any sentimental analysis of “tweets”.","metadata":{}},{"cell_type":"markdown","source":"# Conclusion\n\nI hope you liked my notebook. Please do share with your friends, colleagues & upvoted. Thank You!\n","metadata":{}}]}