{"cells":[{"metadata":{"collapsed":true},"cell_type":"markdown","source":"#### Import Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Read Dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Read Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"OriginalData=pd.read_csv('../input/WA_Fn-UseC_-HR-Employee-Attrition.csv');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"OriginalData.head(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"OriginalData.columns.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Exploratory Data Analysis(EDA)"},{"metadata":{"trusted":true},"cell_type":"code","source":"OriginalData.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"OriginalData.info(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"OriginalData.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Pandas Profiling","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Generates profile reports from a pandas DataFrame. The pandas df.describe() function is great but a little basic for serious exploratory data analysis.<br>\npandas_profiling extends the pandas DataFrame with df.profile_report() for quick data analysis.\n\nFor each column the following statistics - if relevant for the column type - are presented in an interactive HTML report:\n\nEssentials: type, unique values, missing values\nQuantile statistics like minimum value, Q1, median, Q3, maximum, range, interquartile range\nDescriptive statistics like mean, mode, standard deviation, sum, median absolute deviation, coefficient of variation, kurtosis, skewness\nMost frequent values\nHistogram\nCorrelations highlighting of highly correlated variables, Spearman, Pearson and Kendall matrices\nMissing values matrix, count, heatmap and dendrogram of missing values\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install pandas-profiling","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import pandas_profiling","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#OriginalData.profile_report()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Cleaning of dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"OriginalData.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"OriginalData.isna().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Import libraries for visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Attrition: Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(OriginalData['Attrition'], palette=\"Set2\",saturation=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check for outliers"},{"metadata":{},"cell_type":"markdown","source":"#### Visualization : Histograms"},{"metadata":{"trusted":true},"cell_type":"code","source":"numericalData=OriginalData.copy().drop(columns=['BusinessTravel','Department','EducationField','Gender','JobRole','MaritalStatus','OverTime','Attrition','Over18'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numericalData.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (20,20))\nax = fig.gca()\nnumericalData.hist(bins=30,ax=ax);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Handle Attrition Categorical Value"},{"metadata":{},"cell_type":"markdown","source":"#### Attrition : Convert \"No\"->0 and \"Yes\"->1\n'''\nEmployee leaving the company (0=No, 1=Yes)\n'''"},{"metadata":{"trusted":true},"cell_type":"code","source":"OriginalData['Attrition'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### create of copy of original dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset=OriginalData.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"attritionStatus={'No':0,'Yes':1}\ndataset['Attrition']=dataset['Attrition'].map(attritionStatus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(dataset['Attrition'].value_counts()).T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Baseline Model Accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Employee not leaving the company : ',round((1233/1470)*100,2),'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Employee leaving the company : ',round((237/1470)*100,2),'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feature Selection Using Correlation\n'''\nThe correlation coefficient has values between -1 to 1 <br>\n - A value closer to 0 implies weaker correlation (exact 0 implying no correlation) <br>\n - A value closer to 1 implies stronger positive correlation <br>\n - A value closer to -1 implies stronger negative correlation <br>\n '''"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data looks clean with no potential outliers.\n#We can drop Employee Count and StanardHours features since they are constant and does not contribute to the model.\ncorrData=dataset.copy().drop(columns=['StandardHours','EmployeeCount'])\ncorrData.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using Pearson Correlation\nplt.figure(figsize=(20,20))\ncor = corrData.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Conclusion from correlation\n'''\nStatistical relationship between two variables is referred to as their correlation. The performance of some algorithms can deteriorate if two or more variables are tightly related, called multicollinearity.This is of special importance in Regression. From the above correlation matrix , we find most of the features are uncorrelated.But, there is a high correlation (0.95) between Monthly Income and Job Level.\n'''"},{"metadata":{},"cell_type":"markdown","source":"#### Removing Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Employee number will be used for display purpose\nempNo=dataset['EmployeeNumber']\n#Target/Respone Variable\nresponse=dataset['Attrition']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Some variable whose value is not changing, So standard deviation of that variable is Zero. So It is not Significant for analysis.\n# Those variable are Employee count, Over18, StandardHours.\ndataset=dataset.drop(columns=['EmployeeNumber','Attrition','StandardHours','EmployeeCount','Over18'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Converting Categorical Features to numerical: One Hot Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset=pd.get_dummies(dataset,columns=['BusinessTravel','Department','EducationField','Gender','JobRole','MaritalStatus','OverTime'],drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.columns.size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feature Selection using SelectKbest-chi2\n'''\nAdvantage:<br>\n· Reduces Overfitting: Less redundant data means less opportunity to make decisions based on noise.<br>\n· Improves Accuracy: Less misleading data means modeling accuracy improves.<br>\n· Reduces Training Time: fewer data points reduce algorithm complexity and algorithms train faster.<br>\n'''"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#apply SelectKBest class to extract top 34 best features\nk=35\nselect_feature = SelectKBest(score_func=chi2,k=k)\nselect_feature.fit(dataset,response)\n\ndfscores=pd.DataFrame(select_feature.scores_)\ndfcolumns=pd.DataFrame(dataset.columns)\n\n#concat two dataframes for better visualization\nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\n#naming the dataframe columns\nfeatureScores.columns=['Features','Score']\n\n#print 15 best features\nprint(featureScores.nlargest(k,'Score'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.columns[select_feature.get_support()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"impFeatures=dataset[['Age', 'DailyRate', 'DistanceFromHome', 'EnvironmentSatisfaction',\n       'JobInvolvement', 'JobLevel', 'JobSatisfaction', 'MonthlyIncome',\n       'MonthlyRate', 'NumCompaniesWorked', 'RelationshipSatisfaction',\n       'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear',\n       'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole',\n       'YearsSinceLastPromotion', 'YearsWithCurrManager',\n       'BusinessTravel_Travel_Frequently', 'BusinessTravel_Travel_Rarely',\n       'Department_Research & Development', 'Department_Sales',\n       'EducationField_Marketing', 'EducationField_Medical',\n       'EducationField_Technical Degree', 'JobRole_Human Resources',\n       'JobRole_Laboratory Technician', 'JobRole_Manager',\n       'JobRole_Manufacturing Director', 'JobRole_Research Director',\n       'JobRole_Sales Representative', 'MaritalStatus_Married',\n       'MaritalStatus_Single', 'OverTime_Yes']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"impFeatures.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Splitting into Train and Test Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test=train_test_split(impFeatures,response,test_size=0.3,random_state=0)\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feature Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n'''\nThe main idea is to normalize/standardize (mean = 0 and standard deviation = 1) your features before applying machine learning techniques.\nStandardScaler performs the task of Standardization. Usually a dataset contains variables that are different in scale. \nFor e.g. an Employee dataset will contain AGE column with values on scale 20-70 and SALARY column with values on scale 10000-80000.\nAs these two columns are different in scale, they are Standardized to have common scale while building machine learning model.\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc_X=StandardScaler()\n\n#Standard scalar removes columns values and indexs after normalization so we have to provide columns values and indexes again.\nX_train2=pd.DataFrame(sc_X.fit_transform(X_train))\nX_test2=pd.DataFrame(sc_X.transform(X_test))\n\nX_train2.columns=X_train.columns.values\nX_test2.columns=X_test.columns.values\n\nX_train2.index=X_train.index.values\nX_test2.index=X_test.index.values\n\nX_train=X_train2\nX_test=X_test2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model Building"},{"metadata":{},"cell_type":"markdown","source":"#### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nmodel_Reg=LogisticRegression()\nmodel_Reg.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Test Data Accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_reg = model_Reg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model Performance Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n\nacc=accuracy_score(y_test,y_pred_reg)\nprec=precision_score(y_test,y_pred_reg)\nrec=recall_score(y_test,y_pred_reg)\nf1=f1_score(y_test,y_pred_reg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_test,y_pred_reg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the Confusion Matrix\nrf_cm = confusion_matrix(y_test,y_pred_reg)\n\n# building a graph to show the confusion matrix results\nrf_cm_plot = pd.DataFrame(rf_cm, index = [i for i in {\"Attrition\", \"No Attrition\"}],\n                  columns = [i for i in {\"No attrition\", \"Attrition\"}])\nplt.figure(figsize = (6,5))\nsns.heatmap(rf_cm_plot, annot=True, vmin=5, vmax=90.5, cbar=False, fmt='g')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results=pd.DataFrame([['Logistic Regression',acc,prec,rec,f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall','F1 Score'])\nresults","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python36","display_name":"Python 3.6","language":"python"},"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","name":"python","pygments_lexer":"ipython3","version":"3.6.6","file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"}}},"nbformat":4,"nbformat_minor":1}