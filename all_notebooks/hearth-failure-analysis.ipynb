{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Introduction:\n\nCardiovascular diseases (CVDs) are thenumber 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worlwide.\nHeart failure is a common event caused by CVDs and this dataset contains 12 features that can be used to predict mortality by heart failure."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#1\ndf_hearth_failure = pd.read_csv('/kaggle/input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv')\ndf_hearth_failure.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2) We are only going to use the feature columns \n“DEATH_EVENT, time, age, high_blood_pressure, ejection_fraction, serum_creatinine, serum_sodium”.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_hf = df_hearth_failure[['DEATH_EVENT', 'time', 'age', 'high_blood_pressure', 'ejection_fraction', 'serum_creatinine', 'serum_sodium']].copy()\ndf_hf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3) Creating a new column called “age_text”, where the value is “low” for age<=56, “high” for age>= 73 and “medium” for ages between."},{"metadata":{"trusted":true},"cell_type":"code","source":"age_text = lambda x: 'low' if(x<=56) else ('high' if(x>=73) else 'medium')\ndf_hf['age_text'] = df_hf['age'].apply(age_text)\ndf_hf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3) remove the age column"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_hf = df_hf.drop(['age'], axis=1)\ndf_hf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4) Using the “from sklearn.preprocessing import OneHotEncoder” and doing a one hotencoding of “age_text” \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nimport numpy as np\n\nencoder = OneHotEncoder(handle_unknown='ignore')\n\nencoder.fit(np.c_[df_hf['age_text']])\nencoder.categories","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transformed = encoder.transform(np.c_[df_hf['age_text']])\ndf_oh = pd.DataFrame(transformed.toarray())\ndf_oh.columns = ['high', 'low', 'medium']\ndf_oh.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n4-b) Adding the three columns to the dataframe with the names “[age_text0, age_text1 and age_text2]"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_hf[['age_text0', 'age_text1', 'age_text2']] = df_oh[['high', 'low', 'medium']]\ndf_hf = df_hf.drop(['age_text'], axis=1)\ndf_hf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5) Using “from sklearn.preprocessing import StandardScaler” and scaling the columns “ejection_fraction, serum_creatinine, serum_sodium”.\n\n\nAdding the columns back to the dataframe with the column names “ejection_fraction_sc, serum_creatinine_sc, serum_sodium_sc”, and removing the columns “ejection_fraction, serum_creatinine, serum_sodium”."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_hf[['ejection_fraction', 'serum_creatinine', 'serum_sodium']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(df_hf[['ejection_fraction', 'serum_creatinine', 'serum_sodium']])\nprint(scaler.mean_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_hf[['ejection_fraction_sc', 'serum_creatinine_sc', 'serum_sodium_sc']] = scaler.transform(df_hf[['ejection_fraction', 'serum_creatinine', 'serum_sodium']])\ndf_hf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"askbjd = df_hf[['ejection_fraction', 'serum_creatinine', 'serum_sodium']]#saving just in case\ndf_hf = df_hf.drop(['ejection_fraction', 'serum_creatinine', 'serum_sodium'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"6) Log scaling for any column where that seams desirable. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_hf['serum_creatinine_sc_log'] = askbjd['serum_creatinine'].apply(np.log)\ndf_hf['serum_creatinine_sc_log'].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_hf['serum_creatinine_sc'].hist()\naskbjd['serum_creatinine_sc'] = df_hf['serum_creatinine_sc']\ndf_hf = df_hf.drop(['serum_creatinine_sc'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_hf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1> Classification"},{"metadata":{},"cell_type":"markdown","source":"From the prepped dataset above, extracting the column “DEATH_EVENT” as y, and “age_text0, age_text1, age_text2, high_blood_pressure, ejection_fraction_sc, serum_creatinine_sc, serum_sodium_sc” as X."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_hf.head()\ny = df_hf['DEATH_EVENT']\nX = np.c_[df_hf[['age_text0', 'age_text1', 'age_text2', 'high_blood_pressure', 'ejection_fraction_sc', 'serum_creatinine_sc_log', 'serum_sodium_sc']]]\nprint('success')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the “from sklearn.model_selection import train_test_split” and split X and y into X_train, X_test, y_train and y_test. Using a 20% test data size."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint('success')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train a logistic regression algorithm on the training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression(random_state=0)\nlog_reg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Make prediction using the training and test data, name the prediction variables y_train_pred and y_test_pred. \n\nCreate a y_test_pred_naive which has the same shape as y_test, but only predicts the most common label."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = log_reg.predict(X_train)\ny_test_pred = log_reg.predict(X_test)\ny_test_pred_naive = pd.Series([int(round(y.sum()/len(y))) for _ in range(len(y_test))])\nprint('predicted')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(y_test_pred_naive) == len(y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1> 5\n    "},{"metadata":{},"cell_type":"markdown","source":"\n<p>Using “from sklearn.metricsimport accuracy_score” and calculate the train, test, and test_naive accuracy.****"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nprint('TRAIN SCORE')\nprint(accuracy_score(y_train ,y_train_pred))\n\nprint('TEST SCORE')\nprint(accuracy_score(y_test ,y_test_pred))\n\nprint('NAIVE SCORE')\nprint(accuracy_score(y_test ,y_test_pred_naive))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using “fromsklearn.metricsimportconfustion_matrix” and print the train, test, and test_naive confusion matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nprint('Format')\nprint('TN, FP')\nprint('FN, TP')\nprint('TRAIN CM SCORE')\nprint(confusion_matrix(y_train ,y_train_pred))\nprint('TEST CM SCORE')\nprint(confusion_matrix(y_test ,y_test_pred))\n\nprint('NAIVE CM SCORE')\nprint(confusion_matrix(y_test ,y_test_pred_naive))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The naive model got 35 correct predictions while our model got 41.\nSo our model is better.\n"},{"metadata":{},"cell_type":"markdown","source":"Testing XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\nX_trainxg, X_valid, y_trainxg, y_valid = train_test_split(X_train, y_train, test_size=0.05, random_state=42)\n\n\nmodel=xgb.XGBClassifier(learning_rate=0.05, n_estimators=1000)\nmodel.fit(X_trainxg, y_trainxg, eval_set=[(X_valid, y_valid)], early_stopping_rounds=2500, verbose=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xg_pred = model.predict(X_test)\nprint(accuracy_score(xg_pred ,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Little bit better :)"},{"metadata":{},"cell_type":"markdown","source":"<h1> Regression\n   "},{"metadata":{},"cell_type":"markdown","source":"From the prepped dataset above extract all columns, but only the rows where “DEATH_EVENT” is True (or 1)."},{"metadata":{"trusted":true},"cell_type":"code","source":"i_see_dead_people= df_hf[df_hf['DEATH_EVENT'] == 1].copy()\ni_see_dead_people.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the data with only death instances, extract the column “time” as y, \nand “age_text0, age_text1, age_text2, high_blood_pressure, ejection_fraction_sc, serum_creatinine_sc, serum_sodium_sc” as X."},{"metadata":{"trusted":true},"cell_type":"code","source":"y = i_see_dead_people['time']\nX = i_see_dead_people[['age_text0', 'age_text1', 'age_text2', 'high_blood_pressure', 'ejection_fraction_sc', 'serum_creatinine_sc_log', 'serum_sodium_sc']]\nX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use the “np.c_[ ]” to get X and y in numpy format ready for training."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.c_[X]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use the “from sklearn.model_selection import train_test_split” and split X and y into X_train_r, X_test_r, y_train_r and y_test_r. \n\nUsing a 20% test data size."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X, y, test_size=0.2, random_state=42)\nprint('success')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train a linearregression algorithm on the training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlr_model = LinearRegression()\n\nlr_model.fit(X_train_r, y_train_r)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Make prediction using the training and test data, name the prediction variables y_train_pred_r and y_test_pred_r. \n\n\nCreate a y_test_pred_naive_r which has the same shape as y_test_r, but only predicts the average time for the patients who died."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_r = lr_model.predict(X_train_r)\ny_test_pred_r = lr_model.predict(X_test_r)\ny_test_pred_naive_r = pd.Series([y_train_r.sum()/len(y_train_r) for _ in range(len(y_test_pred_r))])\nprint('Done')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use “from sklearn.metrics import mean_square_error” and “from sklearn.metrics import mean_absolute_error” and calculate the train, test, and test_naive scores.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nprint('TRAIN MSE')\nprint(mean_squared_error(y_train_r, y_train_pred_r))\nprint('TEST MSE')\nprint(mean_squared_error(y_test_r, y_test_pred_r))\nprint('NAIVE MSE')\nprint(mean_squared_error(y_test_r, y_test_pred_naive_r))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nprint('TRAIN MAE')\nprint(mean_absolute_error(y_train_r, y_train_pred_r))\nprint('TEST MAE')\nprint(mean_absolute_error(y_test_r, y_test_pred_r))\nprint('NAIVE MAE')\nprint(mean_absolute_error(y_test_r, y_test_pred_naive_r))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conclusion: Our model is crap.\n"},{"metadata":{},"cell_type":"markdown","source":"Try other models example k-nearest-neighbor, SVM. DecisionTrees, Xgboost, try hyperparameter tuning them. \n\nHow good an MSE and MAE are you able to get on the test data?"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nNN = KNeighborsClassifier(n_neighbors=2, algorithm='ball_tree')\nNN.fit(X_train_r, y_train_r)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nny_train_pred_r = NN.predict(X_train_r)\nnny_test_pred_r = NN.predict(X_test_r)\nprint('Done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nprint('TRAIN MAE')\nprint(mean_absolute_error(y_train_r, nny_train_pred_r))\nprint('TEST MAE')\nprint(mean_absolute_error(y_test_r, nny_test_pred_r))\nprint('NAIVE MAE')\nprint(mean_absolute_error(y_test_r, y_test_pred_naive_r))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A little bit better."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}