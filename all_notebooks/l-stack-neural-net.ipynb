{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv(\"../input/sign_mnist_train.csv\")\ntest=pd.read_csv(\"../input/sign_mnist_test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()             #datayı incele","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()                     #datayı incele","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()                     #datayı incele","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[train[\"label\"]==1].append(train[train[\"label\"]==2]).append(train[train[\"label\"]==3]).append(train[train[\"label\"]==4]).append(train[train[\"label\"]==5])\ntest = test[test[\"label\"]==1].append(test[test[\"label\"]==2]).append(test[test[\"label\"]==3]).append(test[test[\"label\"]==4]).append(test[test[\"label\"]==5])                                                                                                                                        \nx_train = train.drop([\"label\"], axis=1).values.T\ny_train = train[[\"label\"]].values.reshape(-1,1).T\nx_test =  test.drop([\"label\"], axis=1).values.T    # LABELLARI AYIR ARRAYE DÖNÜŞTÜR \ny_test =  test[[\"label\"]].values.reshape(-1,1).T\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_to_one_hot(Y, C):\n    Y = np.eye(C)[Y.reshape(-1)].T\n    return Y\ny_train = convert_to_one_hot(y_train, 6)\ny_test = convert_to_one_hot(y_test, 6)\nx_train = x_train/255                           # NORMALIZE DATA\nx_test = x_test/255   \n           ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of x_train\",x_train.shape)\nprint(\"Shape of y_train\",y_train.shape)\nprint(\"Shape of x_test\",x_test.shape)\nprint(\"Shape of y_test\",y_test.shape)              #Shape of DATA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GRADED FUNCTION: initialize_parameters_he\nlayer_dims= [x_train.shape[0],50,25,12,6]  \ndef initialize_params(layer_dims):\n    \"\"\"\n    Arguments:\n    layer_dims -- python array (list) containing the size of each layer.\n    \n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n                    b1 -- bias vector of shape (layers_dims[1], 1)\n                    ...\n                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n                    bL -- bias vector of shape (layers_dims[L], 1)\n    \"\"\"\n    \n    \n    parameters = {}\n    L = len(layer_dims) - 1 # integer representing the number of layers\n     \n    for l in range(1, L + 1):\n        ### START CODE HERE ### (≈ 2 lines of code)\n        parameters['w' + str(l)] = np.random.randn(layer_dims [l],layer_dims[l-1]) * np.sqrt(2 / layer_dims[l - 1])\n        parameters['b' + str(l)] = np.zeros([layer_dims[l],1])\n        ### END CODE HERE ###\n        \n    return parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Forward propagation\ndef relu(z):\n    a = np.maximum(0,z)\n    return a\ndef softmax(z):\n    t = np.exp(z)\n    summ=np.sum(t,axis=0)\n    t=t/summ\n    return t\n\ndef forward_Z(a_prev,w,b):         # Z DEĞERLERİNİ HESAPLAMA\n    z = np.dot(w,a_prev) + b\n    return z\n\ndef forward_activate(x, parameters):      # aktivasyon\n    z_a_deposu = {}\n    z_a_deposu[\"A0\"] = x\n    for l in range(1,int(len(parameters)/2)):\n        z_a_deposu[\"z\" + str(l)] = forward_Z(z_a_deposu[\"A\" + str(l-1)],parameters[\"w\" + str(l)] , parameters[\"b\" + str(l)])\n        z_a_deposu[\"A\" + str(l)] = np.tanh(z_a_deposu[\"z\" + str(l)]) \n    L = int(len(parameters)/2)\n    z_a_deposu[\"z\"+str(L)] = forward_Z(z_a_deposu[\"A\" + str(L-1)],parameters[\"w\" + str(L)] , parameters[\"b\" + str(L)])\n    z_a_deposu[\"A\"+str(L)  ] = softmax(z_a_deposu[\"z\" + str(L)])\n    AL = z_a_deposu[\"A\"+str(L)]\n    return AL , z_a_deposu\n\ndef cost(AL,y):\n    cost = -np.sum(y*np.log(AL))/y.shape[1]\n    return cost\n\ndef forward_prop(x, y, parameters):\n    AL, z_a_deposu = forward_activate(x,parameters)\n    Cost = cost(AL,y)\n    return AL, Cost , z_a_deposu        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# BACKWARD PROPAGATIN\n\ndef relu_derrivative(a):\n    da= a>0\n    return da\ndef tanh_derrivative(A):\n    dA = 1-A**2\n    return dA\ndef softmax_derrivative(A):\n    dA = A*(1-A)\n    return dA\ndef backward(x, y, parameters):\n    AL, cost , z_a_deposu = forward_prop(x, y, parameters)\n    m = y.shape[1]            # NUMBER OF EXAMPLES\n    türev_deposu = {}\n    türev_deposu[\"dA\" + str(int(len(parameters)/2))] = -np.divide(-y,AL)     #dJ/dAL \n    türev_deposu[\"dz\" + str(int(len(parameters)/2))] = AL - y\n    for l in reversed(range(1,int(len(parameters)/2) + 1)):\n        türev_deposu[\"dw\" + str(l)] = np.dot(türev_deposu[\"dz\" + str(l)],z_a_deposu[\"A\" + str(l-1)].T)/m\n        türev_deposu[\"db\" + str(l)] = np.sum(türev_deposu[\"dz\" + str(l)], axis=1, keepdims=True)/m\n        türev_deposu[\"dA\" + str(l-1)] = np.dot(parameters[\"w\" + str(l)].T , türev_deposu[\"dz\" + str(l)])/m\n        türev_deposu[\"dz\" + str(l-1)] = türev_deposu[\"dA\" + str(l-1)] * tanh_derrivative(z_a_deposu[\"A\" + str(l-1)])\n    return türev_deposu","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def update(X, parameters, z_a_deposu, türev_deposu, learning_rate):\n    for l in range(1,len(layer_dims)):\n        parameters[\"w\" + str(l)] = parameters[\"w\" + str(l)] - learning_rate * türev_deposu[\"dw\" + str(l)]\n        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * türev_deposu[\"db\" + str(l)]\n    return parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"layer_dims= [x_train.shape[0],50,25,12,6]     \ndef l_layer_model(x, y,x_test,y_test, layer_dims, learning_rate, num_of_iterations):\n    parameters = initialize_params(layer_dims)\n    AL, cost , z_a_deposu = forward_prop(x, y, parameters)\n    türev_deposu =  backward(x, y, parameters)\n    Cost1 = []\n    Cost2 = []\n    index = []\n    for i in range(num_of_iterations):\n        AL, cost , z_a_deposu = forward_prop(x, y, parameters)\n        türev_deposu =  backward(x, y, parameters)\n        parameters = update(x, parameters, z_a_deposu, türev_deposu, learning_rate)\n        Cost1.append(cost)\n        if i%250 ==0:\n            Cost2.append(cost)\n            index.append(i)\n            print(\"cost after iter\",i , \"is\" , cost)\n    plt.plot(index,Cost2)\n    plt.plot(index,Cost2)\n\n    predicted_train = AL\n    predicted_test = forward_prop(x_test, y_test, parameters)[0]\n    train_accuracy = np.sum(np.equal(np.argmax(AL,axis=0), np.argmax(y_train,axis=0)))/y_train.shape[1]\n    test_accuracy = np.sum(np.equal(np.argmax(predicted_test,axis=0), np.argmax(y_test,axis=0)))/y_test.shape[1]\n\n    print(\"Train Accuracy:\", train_accuracy)\n    print(\"Test Accuracy:\", test_accuracy)\n    return AL","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l_layer_model(x_train, y_train,x_test,y_test, layer_dims, learning_rate=1, num_of_iterations=2500)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* > ***AS SEEN ACCURACY IS NOT ENOUGH HIGH.\n* > LETS USE ADAM OPTIMIZER*****","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_adam(parameters) :\n  \n    \n    L = len(parameters) // 2 # number of layers in the neural networks\n    v = {}\n    s = {}\n    \n    # Initialize v, s.\n    for l in range(L):\n        v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"w\" + str(l + 1)])\n        v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n\n        s[\"dW\" + str(l+1)] = np.zeros_like(parameters[\"w\" + str(l + 1)])\n        s[\"db\" + str(l+1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n    \n    return v, s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def update_parameters_with_adam(parameters, grads, v, s, t=2, learning_rate=0.01,\n                                beta1=0.9, beta2=0.999, epsilon=1e-8):\n\n    L = len(parameters) // 2                 # number of layers in the neural networks\n    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n    \n    for l in range(L):\n        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n\n        v[\"dW\" + str(l + 1)] = beta1 * v[\"dW\" + str(l + 1)] + (1 - beta1) * grads['dw' + str(l + 1)]\n        v[\"db\" + str(l + 1)] = beta1 * v[\"db\" + str(l + 1)] + (1 - beta1) * grads['db' + str(l + 1)]\n\n\n        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n\n        v_corrected[\"dW\" + str(l + 1)] = v[\"dW\" + str(l + 1)] / (1 - np.power(beta1, t))\n        v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)] / (1 - np.power(beta1, t))\n\n\n        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n\n        s[\"dW\" + str(l + 1)] = beta2 * s[\"dW\" + str(l + 1)] + (1 - beta2) * np.power(grads['dw' + str(l + 1)], 2)\n        s[\"db\" + str(l + 1)] = beta2 * s[\"db\" + str(l + 1)] + (1 - beta2) * np.power(grads['db' + str(l + 1)], 2)\n\n        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n\n        s_corrected[\"dW\" + str(l + 1)] = s[\"dW\" + str(l + 1)] / (1 - np.power(beta2, t))\n        s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)] / (1 - np.power(beta2, t))\n\n        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n        parameters[\"w\" + str(l + 1)] = parameters[\"w\" + str(l + 1)] - learning_rate * v_corrected[\"dW\" + str(l + 1)] / np.sqrt(s_corrected[\"dW\" + str(l + 1)] + epsilon)\n        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v_corrected[\"db\" + str(l + 1)] / np.sqrt(s_corrected[\"db\" + str(l + 1)] + epsilon)\n\n\n    return parameters, v, s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"layer_dims= [x_train.shape[0],50,25,12,6]     \ndef l_layer_model(x, y,x_test,y_test, layer_dims, learning_rate, num_of_iterations):\n    parameters = initialize_params(layer_dims)\n    v , s = initialize_adam(parameters)\n    AL, cost , z_a_deposu = forward_prop(x, y, parameters)\n    türev_deposu =  backward(x, y, parameters)\n    Cost1 = []\n    Cost2 = []\n    index = []\n    for i in range(num_of_iterations):\n        AL, cost , z_a_deposu = forward_prop(x, y, parameters)\n        türev_deposu =  backward(x, y, parameters)\n        parameters , v , s  =update_parameters_with_adam(parameters,türev_deposu,v,s)\n        Cost1.append(cost)\n        if i%250 ==0:\n            print(\"cost after iter\",i , \"is\" , cost)\n        Cost2.append(cost)\n        index.append(i)\n            \n    plt.plot(index,Cost2)\n\n    predicted_train = AL\n    predicted_test = forward_prop(x_test, y_test, parameters)[0]\n    train_accuracy = np.sum(np.equal(np.argmax(AL,axis=0), np.argmax(y_train,axis=0)))/y_train.shape[1]\n    test_accuracy = np.sum(np.equal(np.argmax(predicted_test,axis=0), np.argmax(y_test,axis=0)))/y_test.shape[1]\n \n    print(\"Train Accuracy:\", train_accuracy)\n    print(\"Test Accuracy:\", test_accuracy)\n    return AL","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" l_layer_model(x_train, y_train,x_test,y_test, layer_dims, learning_rate=1, num_of_iterations=3000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}