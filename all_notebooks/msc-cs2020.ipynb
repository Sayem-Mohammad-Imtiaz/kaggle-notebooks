{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom pandas.plotting import scatter_matrix\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\ndataset = pd.read_csv(\"../input/iris-flower-dataset/IRIS.csv\")\ndataset.head(12)\nprint(dataset.shape)\nprint(dataset.describe())\n#class distribution\nprint(dataset.groupby('species').size())\n# box and whisker plots\ndataset.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)\n#pyplot.show()\n# histograms\ndataset.hist()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scatter plot matrix\nscatter_matrix(dataset)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split-out validation dataset\narray = dataset.values\nX = array[:,0:4]\nY= array[:,4]\nX_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=0.20, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n\n# Create Decision Tree classifer object\nclf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n\n# Train Decision Tree Classifer\nclf = clf.fit(X_train,Y_train)\n\n#Predict the response for test dataset\ny_pred = clf.predict(X_validation)\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(Y_validation, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from IPython.display import Image  \nfrom sklearn import tree\ntree.plot_tree(clf)  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" #now we try mlp classifier\nfrom sklearn.neural_network import MLPClassifier\nclassifier = MLPClassifier(hidden_layer_sizes=(3,3), max_iter=3000,activation = 'relu',solver='adam',random_state=1)\nclassifier=classifier.fit(X_train, Y_train)\n#Using the trained network to predict\n\n#Predicting y for X_val\nY_pred = classifier.predict(X_validation)\n\n#Calculating the accuracy of predictions\n#Importing Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\n#Comparing the predictions against the actual observations in y_val\ncm = confusion_matrix(Y_pred, Y_validation)\n\n#Printing the accuracy\nprint(\"Accuracy of MLPClassifier : \",metrics.accuracy_score(Y_validation, Y_pred))\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n #Import svm model\nfrom sklearn import svm\n\n#Create a svm Classifier\nclf = svm.SVC(kernel='linear') # Linear Kernel\n\n#Train the model using the training sets\nclf.fit(X_train, Y_train)\n\n#Predict the response for test dataset\nY_pred = clf.predict(X_validation)\ncm = confusion_matrix(Y_pred, Y_validation)\n\n#Printing the accuracy\nprint(\"Accuracy of SVM Classifier : \",metrics.accuracy_score(Y_validation, Y_pred))\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig, axes = plt.subplots(4, 4)\n# use global min / max to ensure all weights are shown on the same scale\nvmin, vmax = classifier.coefs_[0].min(), classifier.coefs_[0].max()\nfor coef, ax in zip(classifier.coefs_[0].T, axes.ravel()):\n    ax.matshow(coef.reshape(2, 2), cmap=plt.cm.gray, vmin=.5 * vmin,\n               vmax=.5 * vmax)\n    ax.set_xticks(())\n    ax.set_yticks(())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"# importing the required module \nimport matplotlib.pyplot as plt \n\n ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nplt.rcParams['figure.figsize'] = (15, 10)\nMC = pd.read_csv(\"../input/customer-segmentation-tutorial-in-python/Mall_Customers.csv\")\nMC.head()\n#plotting \nX=MC['Age']\nY=MC.iloc[:,3]\nZ=MC.iloc[:,4]\nplt.xlabel('AGE') \n# naming the y axis \nplt.ylabel('INCOME') \n\n#plt.scatter(X, Y)\nplt.ylabel('Spending')\nplt.scatter(X, Z)\n#X\n\nMC.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.xlabel('INCOME') \n# naming the y axis \nplt.ylabel('Spending')\nplt.scatter(Y, Z)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = MC.iloc[:, [3, 4]].values\n\n# let's check the shape of x\nprint(x.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\n\nwcss = []\nfor i in range(1, 11):\n    km = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n    km.fit(x)\n    wcss.append(km.inertia_)\n    \nplt.plot(range(1, 11), wcss)\nplt.title('The Elbow Method', fontsize = 20)\nplt.xlabel('No. of Clusters')\nplt.ylabel('wcss')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"km = KMeans(n_clusters = 5, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\ny_means = km.fit_predict(x)\n\nplt.scatter(x[y_means == 0, 0], x[y_means == 0, 1], s = 100, c = 'pink', label = 'miser')\nplt.scatter(x[y_means == 1, 0], x[y_means == 1, 1], s = 100, c = 'yellow', label = 'general')\nplt.scatter(x[y_means == 2, 0], x[y_means == 2, 1], s = 100, c = 'cyan', label = 'target')\nplt.scatter(x[y_means == 3, 0], x[y_means == 3, 1], s = 100, c = 'magenta', label = 'spendthrift')\nplt.scatter(x[y_means == 4, 0], x[y_means == 4, 1], s = 100, c = 'orange', label = 'careful')\nplt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:, 1], s = 50, c = 'blue' , label = 'centeroid')\n\nplt.style.use('fivethirtyeight')\nplt.title('K Means Clustering', fontsize = 20)\nplt.xlabel('Annual Income')\nplt.ylabel('Spending Score')\n#plt.legend()\nplt.grid()\n\nplt.legend()\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.cluster.hierarchy as sch\n\ndendrogram = sch.dendrogram(sch.linkage(x, method = 'ward'))\nplt.title('Dendrogam', fontsize = 20)\nplt.xlabel('Customers')\nplt.ylabel('Ecuclidean Distance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**To determine the optimal number of clusters by visualizing the data, imagine all the horizontal lines as being completely horizontal and then after calculating the maximum distance between any two horizontal lines, draw a horizontal line in the maximum distance calculated******\n\nFor more details please read https://www.analyticsvidhya.com/blog/2019/05/beginners-guide-hierarchical-clustering/"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\n\nhc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'ward')\ny_hc = hc.fit_predict(x)\n#hc.labels_=['A',\"B\",\"C\",\"D\",\"E\"]\nplt.scatter(x[:,0],x[:,1], c=hc.labels_, cmap='rainbow')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\n\nhc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'ward')\ny_hc = hc.fit_predict(x)\n#plt.scatter(x[0],x[1], c=cluster.labels_)\nplt.scatter(x[y_hc == 0, 0], x[y_hc == 0, 1], s = 100, c = 'pink', label = 'miser')\nplt.scatter(x[y_hc == 1, 0], x[y_hc == 1, 1], s = 100, c = 'yellow', label = 'general')\nplt.scatter(x[y_hc == 2, 0], x[y_hc == 2, 1], s = 100, c = 'cyan', label = 'target')\nplt.scatter(x[y_hc == 3, 0], x[y_hc == 3, 1], s = 100, c = 'magenta', label = 'spendthrift')\nplt.scatter(x[y_hc == 4, 0], x[y_hc == 4, 1], s = 100, c = 'orange', label = 'careful')\nplt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:, 1], s = 50, c = 'blue' , label = 'centeroid')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import DBSCAN\nclustering = DBSCAN(eps=3, min_samples=2).fit(x)\nclustering.labels_\n\nclustering\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(context=\"notebook\", palette=\"Spectral\", style = 'darkgrid' ,font_scale = 1.5, color_codes=True)\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import Birch\nbrc = Birch(branching_factor=500, n_clusters=5, threshold=1.5)\nbrc.fit(x)\n#We use the predict method to obtain a list of points and their respective cluster.\nlabels = brc.predict(x)\nplt.scatter(x[:,0], x[:,1], c=labels, cmap='rainbow')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To read more about the BIRCH algorithm \nread\nhttps://towardsdatascience.com/machine-learning-birch-clustering-algorithm-clearly-explained-fb9838cbeed9"},{"metadata":{},"cell_type":"markdown","source":"To see How A k medoids is implemented,See https://towardsdatascience.com/k-medoids-clustering-on-iris-data-set-1931bf781e05"},{"metadata":{},"cell_type":"markdown","source":"how to create datasets https://machinelearningmastery.com/generate-test-datasets-python-scikit-learn/"},{"metadata":{"trusted":true},"cell_type":"code","source":" import matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_moons\nfrom matplotlib import pyplot\nfrom pandas import DataFrame\n# generate 2d classification dataset\nX, y = make_moons(n_samples=1000, noise=0.1)\n# scatter plot, dots colored by class value\ndf = DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\ncolors = {0:'red', 1:'blue'}\nfig, ax = pyplot.subplots()\ngrouped = df.groupby('label')\nfor key, group in grouped:\n    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.cluster import OPTICS, cluster_optics_dbscan \nfrom sklearn.preprocessing import normalize, StandardScaler \nMC = pd.read_csv(\"../input/customer-segmentation-tutorial-in-python/Mall_Customers.csv\")\nMC.head()\nx = MC.iloc[:, [2, 3]].values\n\n# let's check the shape of x\nprint(x.shape)\noptics_model = OPTICS(min_samples = 10, xi = 0.05, min_cluster_size = 0.05)\noptics_model.fit(x) \n\nlabels = optics_model.labels_[optics_model.ordering_]\nplt.scatter(x[:,0], x[:,1], c=labels, cmap='rainbow')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import load_digits\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\ndigits = load_digits()\nprint(digits.data.shape)\nprint(digits.keys())\n# set up the figure\nfig = plt.figure(figsize=(6, 6))  # figure size in inches\nfig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n\n# plot the digits: each image is 8x8 pixels\nfor i in range(64):\n    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')\n    \n    # label the image with the target value\n    ax.text(0, 7, str(digits.target[i]))\nX_train, X_validation, Y_train, Y_validation = train_test_split(digits.data, digits.target,\n                                                random_state=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\n\n#Create a svm Classifier\nclf = svm.SVC(kernel='linear') # Linear Kernel\n\n#Train the model using the training sets\nclf.fit(X_train, Y_train)\n\n#Predict the response for test dataset\nY_pred = clf.predict(X_validation)\ncm = confusion_matrix(Y_pred, Y_validation)\n\n#Printing the accuracy\nprint(\"Accuracy of SVM Classifier : \",accuracy_score(Y_validation, Y_pred))\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" #now we try mlp classifier\nfrom sklearn.neural_network import MLPClassifier\nclassifier = MLPClassifier(hidden_layer_sizes=(3,3), max_iter=3000,activation = 'relu',solver='adam',random_state=1)\nclassifier=classifier.fit(X_train, Y_train)\n#Using the trained network to predict\n\n#Predicting y for X_val\nY_pred = classifier.predict(X_validation)\nprint(\"Accuracy of MLP Classifier : \",accuracy_score(Y_validation, Y_pred))\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nclassifier = MLPClassifier(hidden_layer_sizes=(100,100), max_iter=3000,activation = 'relu',solver='adam',random_state=1)\nclassifier=classifier.fit(X_train, Y_train)\n#Using the trained network to predict\n\n#Predicting y for X_val\nY_pred = classifier.predict(X_validation)\nprint(\"Accuracy of MLP Classifier : \",accuracy_score(Y_validation, Y_pred))\n#print(cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nmat = confusion_matrix(Y_pred, Y_validation)\nsns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\nplt.xlabel('true label')\nplt.ylabel('predicted label');","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}