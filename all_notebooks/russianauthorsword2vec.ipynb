{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.DataFrame()\ndf = pd.read_csv('../input/russian-authors/russian_authors_12.csv', encoding='utf-8')\ndf.head(3)","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"                                            sentence   author\n0   А живет он второе лето в этом вонючем городиш...   Chehov\n1   Вдруг она услыхала ровный и спокойный носовой...  Tolstoy\n2                              \"Послушайте, матушка.    Gogol","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>author</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>А живет он второе лето в этом вонючем городиш...</td>\n      <td>Chehov</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Вдруг она услыхала ровный и спокойный носовой...</td>\n      <td>Tolstoy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\"Послушайте, матушка.</td>\n      <td>Gogol</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\nreview_lines = list()\nlines = df['sentence'].values.tolist()\n\nfor line in lines:   \n    tokens = word_tokenize(line)\n    # convert to lower case\n    tokens = [w.lower() for w in tokens]\n    # remove punctuation from each word    \n    table = str.maketrans('', '', string.punctuation)\n    stripped = [w.translate(table) for w in tokens]\n    # remove remaining tokens that are not alphabetic\n    words = [word for word in stripped if word.isalpha()]\n    # filter out stop words    \n    stop_words = set(stopwords.words('russian'))\n    words = [w for w in words if not w in stop_words]\n    review_lines.append(words)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(review_lines)","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"22500"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim \n\nEMBEDDING_DIM = 300\n# train word2vec model\nmodel = gensim.models.Word2Vec(sentences=review_lines, size=EMBEDDING_DIM, window=1, workers=4, min_count=1)\n# vocab size\nwords = list(model.wv.vocab)\nprint('Vocabulary size: %d' % len(words))","execution_count":18,"outputs":[{"output_type":"stream","text":"Vocabulary size: 47967\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save model in ASCII (word2vec) format\nfilename = 'russian_embedding_word2vec.txt'\nmodel.wv.save_word2vec_format(filename, binary=False)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\nembeddings_index = {}\nf = open(os.path.join('', 'russian_embedding_word2vec.txt'),  encoding = \"utf-8\")\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:])\n    embeddings_index[word] = coefs\nf.close()","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_reviews = df['sentence'].values\nmax_length = max([len(s.split()) for s in total_reviews])","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(max_length)","execution_count":22,"outputs":[{"output_type":"stream","text":"199\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\nVALIDATION_SPLIT = 0.2\n\n# vectorize the text samples into a 2D integer tensor\ntokenizer_obj = Tokenizer()\ntokenizer_obj.fit_on_texts(review_lines)\nsequences = tokenizer_obj.texts_to_sequences(review_lines)\n\n# pad sequences\nword_index = tokenizer_obj.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\nreview_pad = pad_sequences(sequences, maxlen=max_length)\nlabelencoder_X = LabelEncoder()\nauthor = df['author'].values\nauthor = pd.DataFrame(labelencoder_X.fit_transform(df['author']))\nonehotencoder = OneHotEncoder(categorical_features = [0])\nauthor = pd.DataFrame(onehotencoder.fit_transform(author).toarray())\nauthor = np.array(author)\nauthor = author.astype(dtype = 'int32')\n\nprint('Shape of review tensor:', review_pad.shape)\nprint('Shape of sentiment tensor:', author.shape)\n\n","execution_count":23,"outputs":[{"output_type":"stream","text":"Found 47967 unique tokens.\nShape of review tensor: (22500, 199)\nShape of sentiment tensor: (22500, 5)\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\nIf you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\nIn case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n  warnings.warn(msg, FutureWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:392: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n  \"use the ColumnTransformer instead.\", DeprecationWarning)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split the data into a training set and a validation set\nindices = np.arange(review_pad.shape[0])\nnp.random.shuffle(indices)\nreview_pad = review_pad[indices]\nauthor = author[indices]\nnum_validation_samples = int(VALIDATION_SPLIT * review_pad.shape[0])\n\nX_train_pad = review_pad[:-num_validation_samples]\ny_train = author[:-num_validation_samples]\nX_test_pad = review_pad[-num_validation_samples:]\ny_test = author[-num_validation_samples:]","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of X_train_pad tensor:', X_train_pad.shape)\nprint('Shape of y_train tensor:', y_train.shape)\n\nprint('Shape of X_test_pad tensor:', X_test_pad.shape)\nprint('Shape of y_test tensor:', y_test.shape)","execution_count":25,"outputs":[{"output_type":"stream","text":"Shape of X_train_pad tensor: (18000, 199)\nShape of y_train tensor: (18000, 5)\nShape of X_test_pad tensor: (4500, 199)\nShape of y_test tensor: (4500, 5)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_DIM = 300\nnum_words = len(word_index) + 1\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n\nfor word, i in word_index.items():\n    if i > num_words:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(num_words)","execution_count":27,"outputs":[{"output_type":"stream","text":"47968\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Embedding, Flatten\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.initializers import Constant\n\n# define model\nmodel = Sequential()\n# load pre-trained word embeddings into an Embedding layer\n# note that we set trainable = False so as to keep the embeddings fixed\nembedding_layer = Embedding(num_words,\n                            EMBEDDING_DIM,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=max_length,\n                            trainable=False)\n\nmodel.add(embedding_layer)\nmodel.add(Conv1D(filters=600, kernel_size=1, activation='tanh'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Flatten())\nmodel.add(Dense(5, activation='softmax'))\nprint(model.summary())\n\n# compile network\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# fit the model\nmodel.fit(X_train_pad, y_train, batch_size=128, epochs=50, validation_data=(X_test_pad, y_test), verbose=2)","execution_count":30,"outputs":[{"output_type":"stream","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_4 (Embedding)      (None, 199, 300)          14390400  \n_________________________________________________________________\nconv1d_4 (Conv1D)            (None, 199, 600)          180600    \n_________________________________________________________________\nmax_pooling1d_4 (MaxPooling1 (None, 99, 600)           0         \n_________________________________________________________________\nflatten_4 (Flatten)          (None, 59400)             0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 5)                 297005    \n=================================================================\nTotal params: 14,868,005\nTrainable params: 477,605\nNon-trainable params: 14,390,400\n_________________________________________________________________\nNone\nTrain on 18000 samples, validate on 4500 samples\nEpoch 1/50\n - 3s - loss: 1.5952 - acc: 0.2211 - val_loss: 1.5856 - val_acc: 0.2318\nEpoch 2/50\n - 2s - loss: 1.5761 - acc: 0.2485 - val_loss: 1.5761 - val_acc: 0.2529\nEpoch 3/50\n - 2s - loss: 1.5626 - acc: 0.2697 - val_loss: 1.5737 - val_acc: 0.2442\nEpoch 4/50\n - 2s - loss: 1.5415 - acc: 0.2888 - val_loss: 1.5425 - val_acc: 0.2940\nEpoch 5/50\n - 2s - loss: 1.5156 - acc: 0.3141 - val_loss: 1.5241 - val_acc: 0.3060\nEpoch 6/50\n - 2s - loss: 1.4915 - acc: 0.3311 - val_loss: 1.5052 - val_acc: 0.3182\nEpoch 7/50\n - 2s - loss: 1.4687 - acc: 0.3460 - val_loss: 1.4922 - val_acc: 0.3211\nEpoch 8/50\n - 2s - loss: 1.4513 - acc: 0.3568 - val_loss: 1.4732 - val_acc: 0.3373\nEpoch 9/50\n - 2s - loss: 1.4338 - acc: 0.3659 - val_loss: 1.4645 - val_acc: 0.3444\nEpoch 10/50\n - 2s - loss: 1.4195 - acc: 0.3773 - val_loss: 1.4518 - val_acc: 0.3547\nEpoch 11/50\n - 2s - loss: 1.4050 - acc: 0.3878 - val_loss: 1.4421 - val_acc: 0.3607\nEpoch 12/50\n - 2s - loss: 1.3934 - acc: 0.3959 - val_loss: 1.4357 - val_acc: 0.3589\nEpoch 13/50\n - 2s - loss: 1.3814 - acc: 0.4011 - val_loss: 1.4291 - val_acc: 0.3667\nEpoch 14/50\n - 2s - loss: 1.3706 - acc: 0.4095 - val_loss: 1.4217 - val_acc: 0.3724\nEpoch 15/50\n - 2s - loss: 1.3601 - acc: 0.4166 - val_loss: 1.4266 - val_acc: 0.3676\nEpoch 16/50\n - 2s - loss: 1.3517 - acc: 0.4205 - val_loss: 1.4151 - val_acc: 0.3747\nEpoch 17/50\n - 2s - loss: 1.3435 - acc: 0.4247 - val_loss: 1.4099 - val_acc: 0.3838\nEpoch 18/50\n - 2s - loss: 1.3326 - acc: 0.4298 - val_loss: 1.4057 - val_acc: 0.3807\nEpoch 19/50\n - 2s - loss: 1.3247 - acc: 0.4359 - val_loss: 1.4018 - val_acc: 0.3853\nEpoch 20/50\n - 2s - loss: 1.3184 - acc: 0.4367 - val_loss: 1.4077 - val_acc: 0.3793\nEpoch 21/50\n - 2s - loss: 1.3115 - acc: 0.4417 - val_loss: 1.3999 - val_acc: 0.3829\nEpoch 22/50\n - 2s - loss: 1.3033 - acc: 0.4449 - val_loss: 1.3954 - val_acc: 0.3900\nEpoch 23/50\n - 2s - loss: 1.2958 - acc: 0.4536 - val_loss: 1.3928 - val_acc: 0.3891\nEpoch 24/50\n - 2s - loss: 1.2894 - acc: 0.4573 - val_loss: 1.3897 - val_acc: 0.3902\nEpoch 25/50\n - 2s - loss: 1.2835 - acc: 0.4561 - val_loss: 1.3958 - val_acc: 0.3804\nEpoch 26/50\n - 2s - loss: 1.2780 - acc: 0.4617 - val_loss: 1.3895 - val_acc: 0.3869\nEpoch 27/50\n - 2s - loss: 1.2721 - acc: 0.4626 - val_loss: 1.3866 - val_acc: 0.3960\nEpoch 28/50\n - 2s - loss: 1.2649 - acc: 0.4687 - val_loss: 1.3867 - val_acc: 0.3860\nEpoch 29/50\n - 2s - loss: 1.2573 - acc: 0.4731 - val_loss: 1.3851 - val_acc: 0.3904\nEpoch 30/50\n - 2s - loss: 1.2511 - acc: 0.4761 - val_loss: 1.3820 - val_acc: 0.3958\nEpoch 31/50\n - 2s - loss: 1.2455 - acc: 0.4791 - val_loss: 1.3870 - val_acc: 0.3947\nEpoch 32/50\n - 2s - loss: 1.2402 - acc: 0.4824 - val_loss: 1.3806 - val_acc: 0.3978\nEpoch 33/50\n - 2s - loss: 1.2354 - acc: 0.4822 - val_loss: 1.3797 - val_acc: 0.4000\nEpoch 34/50\n - 2s - loss: 1.2291 - acc: 0.4848 - val_loss: 1.3812 - val_acc: 0.4011\nEpoch 35/50\n - 2s - loss: 1.2225 - acc: 0.4863 - val_loss: 1.3871 - val_acc: 0.3913\nEpoch 36/50\n - 2s - loss: 1.2162 - acc: 0.4942 - val_loss: 1.3789 - val_acc: 0.4044\nEpoch 37/50\n - 2s - loss: 1.2107 - acc: 0.4988 - val_loss: 1.3847 - val_acc: 0.3987\nEpoch 38/50\n - 2s - loss: 1.2053 - acc: 0.5019 - val_loss: 1.3794 - val_acc: 0.4022\nEpoch 39/50\n - 2s - loss: 1.2007 - acc: 0.5023 - val_loss: 1.3778 - val_acc: 0.4020\nEpoch 40/50\n - 2s - loss: 1.1943 - acc: 0.5052 - val_loss: 1.3741 - val_acc: 0.4051\nEpoch 41/50\n - 2s - loss: 1.1887 - acc: 0.5096 - val_loss: 1.3848 - val_acc: 0.4009\nEpoch 42/50\n - 2s - loss: 1.1842 - acc: 0.5132 - val_loss: 1.3841 - val_acc: 0.4027\nEpoch 43/50\n - 2s - loss: 1.1774 - acc: 0.5162 - val_loss: 1.3793 - val_acc: 0.4053\nEpoch 44/50\n - 2s - loss: 1.1726 - acc: 0.5158 - val_loss: 1.3823 - val_acc: 0.4002\nEpoch 45/50\n - 2s - loss: 1.1679 - acc: 0.5188 - val_loss: 1.3822 - val_acc: 0.4049\nEpoch 46/50\n - 2s - loss: 1.1610 - acc: 0.5243 - val_loss: 1.3749 - val_acc: 0.4069\nEpoch 47/50\n - 2s - loss: 1.1570 - acc: 0.5263 - val_loss: 1.3791 - val_acc: 0.4042\nEpoch 48/50\n - 2s - loss: 1.1500 - acc: 0.5286 - val_loss: 1.3875 - val_acc: 0.3998\nEpoch 49/50\n - 2s - loss: 1.1444 - acc: 0.5316 - val_loss: 1.3826 - val_acc: 0.4056\nEpoch 50/50\n - 2s - loss: 1.1397 - acc: 0.5334 - val_loss: 1.3886 - val_acc: 0.4007\n","name":"stdout"},{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"<keras.callbacks.History at 0x7f9060f204e0>"},"metadata":{}}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}