{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Author: Pierre Jeanne\n# Project Name: Hole Deviation Prediction \n# Date Created: 11 April 2021\n# from: https://www.kaggle.com/cboychinedu/starter-hole-deviation-prediction-eda","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## INTRODUCTION\n\nHole Deviation is the unintentional departure of the drill bit from a preselected borehole trajactory. Whether it involves drilling a straight or curved-hole section. The tendency of the drill bit to walk away from the desired path can lead to drilling problems such as higher drilling costs and also lease-boundary legal problems.\n\n### Causes of hole deviation:\nIt is not exactly known what causes a drill bit to deviate from its uninteded path. it is generally agreed that one or a combination of the following factors may be responsible for deviation.\n\n1- Heterogeneous nature of formation and dip angle\n\n2- Drill string characteristics, specifically the bottomehole assemble makeup (BHA)\n\n3- Applied weight on bit (WOB)\n\n4- Stabilizers (location, number, clearances)\n\n5- Hole-inclination angle from the vertical\n\n6- Hydraulics at the bit\n\n7- Improper hole cleaning\n\nN/B; It is known that some resultand force acting on a drill bit causes hole deviation to occur. The machanics of this resultand force is complex and it is governed mainly by the mechanics of the bottomhole assemble makeup (BHA).","metadata":{}},{"cell_type":"code","source":"# Importing the necessary packages\nimport numpy as np\nimport pandas as pd\n# data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns;","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"top\"></a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#7ca4cd; border:0' role=\"tab\" aria-controls=\"home\"><center>1- load the dataset</center></h3>","metadata":{}},{"cell_type":"code","source":"# import dataset\ndf = pd.read_csv(r\"../input/hole-deviation/well_log.csv\")\n# drop unwanted columns\ndf = df.drop(columns=['Unnamed: 0'])\n# set 'Depth' as index\ndf = df.set_index('Depth')\n# show first 3 columns\ndf.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The provided well log dataset contains parameters for drilling which are:\n- Gamma-ray = natural radioactivity of the formation along the borehole.\n- Resistivity = ability to impede the flow of electric current.\n- Density = density along the length of a borehole (bulk density)\n- Density_Calculated = This logging measures the calculated density.\n- Classification: 0 not deviate from its pre selected trajectory, 1 deviated from its pre selected trajectory.","metadata":{}},{"cell_type":"code","source":"# show number of rows and columns\nprint('Shape of the file')\nprint('-'*30)\nprint(df.shape)\nprint('')\n# show if missing values\nprint('Number of nan values')\nprint('-'*30)\nprint(df.isnull().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the data\ndf.plot(subplots=True, figsize=(15,35))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# can drop columns with constant number\ndf = df.drop('Neuron_Porosity',axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"top\"></a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#7ca4cd; border:0' role=\"tab\" aria-controls=\"home\"><center>2- feature engineering</center></h3>","metadata":{}},{"cell_type":"code","source":"# calculate various mechanical properties\ndf['ratio_vp/vs'] = df['Vp']/df['Vs']\ndf['Rigidity'] = df['Density']*df['Vs']**2\ndf[\"Young’s modulus\"] = df['Rigidity']* ((3*(df['Vp']**2)-(4*(df['Vs']**2)))/(df['Vp']**2-df['Vs']**2))\ndf[\"Bulk's modulus\"] = df[\"Young’s modulus\"]/(3*(1-2*df['Possions_Ratio']))\ndf['Lame'] = df[\"Bulk's modulus\"]-2*df['Rigidity']/3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot mechanical parameters\ndf1 = df[['ratio_vp/vs','Rigidity',\"Young’s modulus\",\"Bulk's modulus\",'Lame']] \ndf1.plot(subplots=True, figsize=(15,15))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"top\"></a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#7ca4cd; border:0' role=\"tab\" aria-controls=\"home\"><center>3- correlations</center></h3>","metadata":{}},{"cell_type":"code","source":"# Get correlation matrix \ncorr_df = df.corr(method='pearson')\n# heatmap\nfig, axes = plt.subplots(1,figsize=(15,15))\nsns.heatmap(corr_df,annot=True,linewidths=.5, annot_kws={\"size\": 14},vmin=-1.0, vmax=1.0, square=True,cbar=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we removed variables that are highly correlated between them...\ndf = df.drop(columns=['Gamma-ray','Delta T','Density_Calculated','Vp','Vs',\n                     'ratio_vp/vs','Rigidity',\"Bulk's modulus\",'Lame','Density_Porosity'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get correlation matrix \ncorr_df = df.corr(method='pearson')\n# heatmap\nfig, axes = plt.subplots(1,figsize=(6,6))\nsns.heatmap(corr_df,annot=True,linewidths=.5, annot_kws={\"size\": 14},vmin=-1.0, vmax=1.0, square=True,cbar=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the correlations between variables and their relations with 'classification'\nfig, axes = plt.subplots(2,2,figsize=(10,10))\nfig.subplots_adjust(hspace=0.3)\nax0, ax1, ax2, ax3 = axes.flatten() \nsns.scatterplot(data=df,x='Shale_Volume',y='Restivity',hue='Classification',ax=ax0)\nsns.scatterplot(data=df,x='Density',y='Restivity',hue='Classification',ax=ax1)\nsns.scatterplot(data=df,x='Possions_Ratio',y=\"Young’s modulus\",hue='Classification',ax=ax2)\nsns.scatterplot(data=df,x='Density',y=\"Young’s modulus\",hue='Classification',ax=ax3)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Displaying the count for non Deviated hole \nmajority_class = df.loc[df['Classification'] == 0].count()[0]\n\n# Showing the count for Deviated hole \nminority_class = df.loc[df['Classification'] == 1].count()[0]\n\n# Printing the classes for the deviated and non-deviated class \nprint('Non Deviated Class (Classification = 0): {}'.format(majority_class))\nprint('Deviated Class (Classification = 1) : {}'.format(minority_class))\n\n\nsns.countplot(x=\"Classification\", data=df)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The target data is imbalanced \n\nThe challenge of working with imbalanced datasets is that most machine learning techniques will ignore, and in turn have poor performance on, the minority class, although typically it is performance on the minority class that is most important.\n\nOne approach to addressing imbalanced datasets is to use **SMOTE**.\n\nSMOTE is an oversampling method. It works by creating synthetic samples from the minor class instead of creating copies. The algorithm selects two or more similar instances (using a distance measure) and perturbing an instance one attribute at a time by a random amount within the difference to the neighboring instances. This is a type of data augmentation for the minority class and is referred to as the Synthetic Minority Oversampling Technique, or **SMOTE** for short.","metadata":{}},{"cell_type":"code","source":"# pip install imbalanced-learn\nfrom imblearn.over_sampling import SMOTE\nX = df.drop('Classification',axis=1).values\ny = df[['Classification']].values.ravel()\n# Using SMOTE to Balance the imbalanced data \nX_resampled, y_resampled = SMOTE().fit_resample(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert y_resampled to df\ndf_y_resampled = pd.DataFrame(y_resampled,columns=['Classification'])\n\n# showing a plot of the Balanced dataset \nmajority_class = df_y_resampled.loc[df_y_resampled['Classification'] == 0].count()[0]\n\n# Showing the count for Non Hole Deviation \nminority_class = df_y_resampled.loc[df_y_resampled['Classification'] == 1].count()[0]\n\n# Printing the classes for the deviated and non-deviated class \nprint('Non Deviated Class (Classification = 0): {}'.format(majority_class))\nprint('Deviated Class (Classification = 1) : {}'.format(minority_class))\n\n\nsns.countplot(x=\"Classification\", data=df_y_resampled)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"top\"></a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#7ca4cd; border:0' role=\"tab\" aria-controls=\"home\"><center>4- Machine learning</center></h3>","metadata":{}},{"cell_type":"code","source":"# slip the data\nfrom sklearn.model_selection import train_test_split\n# scale the data\nfrom sklearn.preprocessing import StandardScaler\n# cross validation\nfrom sklearn.model_selection import cross_val_score\n# classification model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\n# hyperparameter tunning\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\n# model evaluation\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.1 Preprocessing\n### 4.1.1: Split the data","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled,test_size = .3, random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.1.2: Scale the data","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\n# fit and transform \"x_train\"\nX_train = scaler.fit_transform(X_train)\n# transform \"x_test\"\nX_test = scaler.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2: Model selection\n### 4.2.1: Classification with logistic regression\nBinary logistic regression requires the dependent variable to be binary.\n- Only the meaningful variables should be included.\n- The independent variables should be independent of each other. That is, the model should have little or no multicollinearity.\n- The independent variables are linearly related to the log odds.\n- Logistic regression requires quite large sample sizes.","metadata":{}},{"cell_type":"code","source":"clf_lr = LogisticRegression()\n\n# Compute accuracy on the training set with 5-fold cross-validation\ncv_scores_train = cross_val_score(clf_lr,X_train, y_train,cv=5)\n# Compute accuracy on the training set with 5-fold cross-validation\ncv_scores_test = cross_val_score(clf_lr,X_test, y_test,cv=5)\n\nprint(cv_scores_train)\nprint(cv_scores_test)\n\nclf_lr_mean_train = np.mean(cv_scores_train)\nclf_lr_mean_test = np.mean(cv_scores_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2.2: Classification with k-Nearest Neighbors","metadata":{}},{"cell_type":"code","source":"clf_knn = KNeighborsClassifier(n_neighbors=2)\n\n# Compute accuracy on the training set with 5-fold cross-validation\ncv_scores_train = cross_val_score(clf_knn,X_train, y_train,cv=5)\n# Compute accuracy on the training set with 5-fold cross-validation\ncv_scores_test = cross_val_score(clf_knn,X_test, y_test,cv=5)\n\nprint(cv_scores_train)\nprint(cv_scores_test)\n\nclf_knn_mean_train = np.mean(cv_scores_train)\nclf_knn_mean_test = np.mean(cv_scores_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2.3: Classification with Support Vector Machines","metadata":{}},{"cell_type":"code","source":"clf_svm = svm.SVC()\n\n# Compute accuracy on the training set with 5-fold cross-validation\ncv_scores_train = cross_val_score(clf_svm,X_train, y_train,cv=5)\n# Compute accuracy on the training set with 5-fold cross-validation\ncv_scores_test = cross_val_score(clf_svm,X_test, y_test,cv=5)\n\nprint(cv_scores_train)\nprint(cv_scores_test)\n\nclf_svm_mean_train = np.mean(cv_scores_train)\nclf_svm_mean_test = np.mean(cv_scores_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2.4: Classification with random Forest classifier\n#### random search","metadata":{}},{"cell_type":"code","source":"# Setup the parameters and distributions to sample from: param_dist\nparameters = {'bootstrap': [True, False],\n 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n 'max_features': ['auto', 'sqrt'],\n 'min_samples_leaf': [1, 2, 4],\n 'min_samples_split': [2, 5, 10],\n 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n\n# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = parameters, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# view the best parameters from fitting the random search:\nrf_random.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Grid Search with Cross Validation\nRandom search allowed us to narrow down the range for each hyperparameter. Now that we know where to concentrate our search, we can explicitly specify every combination of settings to try. We do this with GridSearchCV, a method that, instead of sampling randomly from a distribution, evaluates all combinations we define. To use Grid Search, we make another grid based on the best values provided by random search:","metadata":{}},{"cell_type":"code","source":"# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [25,27,29,30,32,34],\n    'max_features': ['sqrt'],\n    'min_samples_leaf': [1],\n    'min_samples_split': [2,3],\n    'n_estimators': [170.185,200,215,230]\n}\n# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the grid search to the data\ngrid_search.fit(X_train, y_train)\ngrid_search.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### best model","metadata":{}},{"cell_type":"code","source":"clf_rf = RandomForestClassifier(bootstrap ='True',max_depth = 25,max_features = 'sqrt',\n                                min_samples_leaf = 1, min_samples_split = 2, n_estimators = 200)\n\n# Compute accuracy on the training set with 5-fold cross-validation\ncv_scores_train = cross_val_score(clf_rf,X_train, y_train,cv=5)\n# Compute accuracy on the training set with 5-fold cross-validation\ncv_scores_test = cross_val_score(clf_rf,X_test, y_test,cv=5)\n\nprint(cv_scores_train)\nprint(cv_scores_test)\n\nclf_rf_mean_train = np.mean(cv_scores_train)\nclf_rf_mean_test = np.mean(cv_scores_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.1.6: Classification with Neutral network","metadata":{}},{"cell_type":"code","source":"# Import necessary modules\nimport keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.callbacks import History \nfrom keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import KFold\nfrom keras.wrappers.scikit_learn import KerasClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### find the best model architecture","metadata":{}},{"cell_type":"code","source":"def create_model(unit1,unit2):\n    # creating the layers of the NN\n    model2 = Sequential()\n    model2.add(Dense(units=unit1, activation='relu'))\n    model2.add(Dense(units=unit2, activation='relu'))\n    model2.add(Dense(units=1, activation='sigmoid'))\n    model2.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return model2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Import KerasClassifier from keras scikit learn wrappers\n# from sklearn.model_selection import KFold\n\n# # Create a KerasClassifier\n# model = KerasClassifier(build_fn = create_model)\n\n# # Define the parameters to try out\n# params={'batch_size':[5, 15, 20],\n#         'unit1':[40,50,60],\n#         'unit2':[20,30,40,50]\n#         }\n# gs=GridSearchCV(estimator=model, param_grid=params, cv=10)\n# # now fit the dataset to the GridSearchCV object. \n# gs = gs.fit(X_train, y_train)\n# early_stopping_monitor = EarlyStopping(monitor='accuracy', patience=3)\n# gs = gs.fit(X_train, y_train,epochs=500, callbacks = [early_stopping_monitor], verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print results\n# print(\"Best: {} using {}\".format(gs.best_score_,gs.best_params_))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### best model","metadata":{}},{"cell_type":"code","source":"# Wrap Keras model so it can be used by scikit-learn\nclf_nn = KerasClassifier(build_fn=create_model, \n                                 unit1=60, \n                                 unit2=40,\n                                 batch_size=5)\n\n# Compute accuracy on the training set with 5-fold cross-validation\ncv_scores_train = cross_val_score(clf_nn,X_train, y_train,cv=5)\n\n# Compute accuracy on the training set with 5-fold cross-validation\ncv_scores_test = cross_val_score(clf_nn,X_test, y_test,cv=5)\n\nprint(cv_scores_train)\nprint(cv_scores_test)\n\nclf_nn_mean_train = np.mean(cv_scores_train)\nclf_nn_mean_test = np.mean(cv_scores_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### add other hidden layer","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## summary","metadata":{}},{"cell_type":"code","source":"# create list with mean model score on training and testing dataset\nscore_train = []\nscore_train.extend((clf_lr_mean_train, clf_knn_mean_train, clf_svm_mean_train,clf_rf_mean_train,clf_nn_mean_train))\nscore_test = []\nscore_test.extend((clf_lr_mean_test, clf_knn_mean_test, clf_svm_mean_test,clf_rf_mean_test,clf_nn_mean_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create dataframe\nlist_regressors = ['Log reg','knn','svm','rf','nn']\n\ndic_score = {'model': list_regressors,\n            'score_train':score_train,\n            'score_test':score_test}\n\ndic_score = pd.DataFrame(dic_score)\ndic_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the predictions for each model\nfig, axes = plt.subplots(1,figsize=(15,5))\nax = plt.subplot(1,1,1)\n\nax = sns.pointplot(x = \"model\", y = \"score_train\", data = dic_score,label = 'accuracy on train data') \nax = sns.pointplot(x = \"model\", y = \"score_test\", data = dic_score,color='red',label = 'accuracy on test data') \nax.legend()\nax.set_ylabel('Score (accuracy)', size=20, labelpad=12.5)\nax.set_xlabel('Model', size=20, labelpad=12.5)\nax.tick_params(labelsize=14)\n# ax.set_xticklabels(ax.get_xticklabels(), size=14) \n# ax.text(i, dic_score[0] + 0.002, '{:.6f}'.format(dic_score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')\n\n# add annotations one by one with a loop\nfor ind in dic_score.index: \n    ax.text(ind,dic_score['score_train'][ind]+0.01,'{:.5f}'.format(dic_score['score_train'][ind]),\n             horizontalalignment='left', size='medium', color='blue', weight='semibold',fontsize=12)\n    ax.text(ind,dic_score['score_test'][ind]-0.01,'{:.5f}'.format(dic_score['score_test'][ind]),\n             horizontalalignment='left', size='medium', color='red', weight='semibold',fontsize=12)\n        \nplt.title('Scores of Models', size=20)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### see how best model is performing on the original data set","metadata":{}},{"cell_type":"code","source":"clf_rf = RandomForestClassifier(bootstrap ='True',max_depth = 25,max_features = 'sqrt',\n                                min_samples_leaf = 1, min_samples_split = 2, n_estimators = 200)\nclf_rf.fit(X_train,y_train)\ny_pred = clf_rf.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate the confusion matrix and \nprint('confusion matrix')\nprint('-'*30)\nprint(confusion_matrix(y_test, y_pred))\nprint('')\n# print classification report\nprint('classification report')\nprint('-'*30)\nprint(classification_report(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}