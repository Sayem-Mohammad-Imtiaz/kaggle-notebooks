{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import Model, regularizers\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Activation, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/mnist-digit-recognizer/train.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_data = df.iloc[:, 1:].to_numpy()\ny_data = df.iloc[:, 0].to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (type(x_data))\nprint (type(y_data))\nprint (x_data.shape)\nprint (y_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nor_x_data = x_data.astype('float32') / 255.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(nor_x_data, y_data, test_size=0.2)\nprint (x_train.shape, y_train.shape)\nprint (x_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(x_train[0].reshape(28, 28))\nprint (y_train[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# [AutoEncoder][1] 解析\n\n### 目的：擁有如同PCA一樣的「降維」功能，從input擷取重要特徵，代表全體。當輸入新的測試資料，和所擷取的代表特徵進行比對，判斷是否有異常。\n\n### AutoEncoder（AE）為一【非監督式學習】的神經網路，中間的 Internal Representation（或稱 Bottleneck）可解釋為對輸入的資料做壓縮（維降）或是加入雜訊。\n\n### AutoEncoder（AE）稱為自動編碼器，可以幫助資料分類、視覺化、儲存。\n### 架構中可細分為 Encoder（編碼器）和 Decoder（解碼器）兩部分，分別做【壓縮】與【解壓縮】的動作，讓輸出值（output）和輸入值（input）表示相同意義。\n### 編碼器會建立一個隱藏層（或多個隱藏層）對所輸入的資料進行「降維」。\n### 解碼器會將通過隱藏層的低維向量進行重建（Reconstruct），以低維向量呈現輸入值（input）。\n\n[1]: https://medium.com/%E7%A8%8B%E5%BC%8F%E5%B7%A5%E4%BD%9C%E7%B4%A1/autoencoder-%E4%B8%80-%E8%AA%8D%E8%AD%98%E8%88%87%E7%90%86%E8%A7%A3-725854ab25e8\n![autoencoder architecture](https://miro.medium.com/proxy/1*77dA0nWbQoGGxz7bSf5Qow.png)\n![autoencoder architecture](https://miro.medium.com/max/1000/0*oTxwcZWFgM7ReT5j.png)\n![autoencoder architecture](https://miro.medium.com/max/700/0*Y1AAl7zNstpSOD3S.jpg)"},{"metadata":{},"cell_type":"markdown","source":"# 單層 AutoEncoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_layer = Input(shape=(x_train.shape[1], ))  # 輸入資料維度 dim=784\n\nencoded = Dense(units=64,\n                activation='relu')(input_layer)  # 建立編碼器（Encoder）\n\ndecoded = Dense(units=x_train.shape[1],\n                activation='sigmoid')(encoded)   # 建立解碼器（Decoder），輸出層與輸入層同維度 dim=784 >> 重建（Reconstruct）資料\n\n#-- 建立 auto_encdoer 模型 --#\nauto_encoder = Model(input_layer, decoded)\n\n#-- 建立 Encoder 模型 --#\nencoder = Model(input_layer, encoded)\n\n#-- 建立 Decoder 模型 --#\nencoded_input = Input(shape=(64, ))\ndecoder_layer = auto_encoder.layers[-1](encoded_input)\ndecoder = Model(encoded_input, decoder_layer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"auto_encoder.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks = EarlyStopping(monitor='val_loss',\n                          min_delta=0.0001,\n                          patience=5,\n                          restore_best_weights=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"auto_encoder.compile(optimizer='adam', loss='binary_crossentropy')\n\nhistory = auto_encoder.fit(x_train, x_train,\n                           epochs=100, batch_size=256,\n                           validation_data=(x_test, x_test),\n                           shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'], label='loss')\nplt.plot(history.history['val_loss'], label='val_loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded = encoder.predict(x_test)\ndecoded = decoder.predict(encoded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(16, 8))\n\nn_plots = 10\nn_rows = int(n_plots / 2)\n\nfor j in range(n_plots):\n    #-- 原始資料 --#\n    fig.add_subplot(n_rows, 6, 3*j+1)\n    plot_tmp = plt.imshow(x_test[j].reshape(28, 28))\n    plt.xticks([])\n    plt.yticks([])\n    \n    #-- 低維陣列 --#\n    fig.add_subplot(n_rows, 6, 3*j+2)\n    plot_tmp = plt.imshow(encoded[j].reshape(1, 64))\n    plt.xticks([])\n    plt.yticks([])\n    \n    #-- Decoder 重建輸出值 --#\n    fig.add_subplot(n_rows, 6, 3*j+3)\n    plot_tmp = plt.imshow(decoded[j].reshape(28, 28))\n    plt.xticks([])\n    plt.yticks([])\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_layer = Input(shape=(x_train.shape[1], ))  # dim=784\n\nencoded = Dense(units=32,\n                activation='relu',\n                activity_regularizer=regularizers.l1(10e-5))(input_layer)  # 增加 regularizer\n                                                                           # e 是底數，代表 10\n                                                                           # 「＋」代表正 N 次方；「－」代表負 N 次方\n                                                                           # 1.e+5  = 10的5次方 = 100,000\n                                                                           # 2.e-6 =  2x 10的負6次方 = 0.000002\n                                                                           # 3.14e-4 = 3.14x10的負4次方 = 0.000314\n\ndecoded = Dense(units=x_train.shape[1],\n                activation='sigmoid')(encoded)  # dim=784\n\n#-- 建立 AutoEncoder --#\nauto_encoder = Model(input_layer, decoded)\n\n#-- 建立 Encoder --#\nencoder = Model(input_layer, encoded)\n\n#-- 建立 Decoder --#\nencoded_input = Input(shape=(32, ))\ndecoded_layer = auto_encoder.layers[-1](encoded_input)\ndecoder = Model(encoded_input, decoded_layer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"auto_encoder.compile(optimizer='adam', loss='binary_crossentropy')\n\nhistory = auto_encoder.fit(x_train, x_train,\n                           epochs=100, batch_size=256,\n                           validation_data=(x_test, x_test),\n                           shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'], label='loss')\nplt.plot(history.history['val_loss'], label='val_loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded = encoder.predict(x_test)\ndecoded = decoder.predict(encoded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(16, 8))\n\nn_plots = 10\nn_rows = int(n_plots / 2)\n\nfor j in range(n_plots):\n    #-- 原始資料 --#\n    fig.add_subplot(n_rows, 6, 3*j+1)\n    plot_tmp = plt.imshow(x_test[j].reshape(28, 28))\n    plt.xticks([])\n    plt.yticks([])\n    \n    #-- 低維陣列 --#\n    fig.add_subplot(n_rows, 6, 3*j+2)\n    plot_tmp = plt.imshow(encoded[j].reshape(1, 32))\n    plt.xticks([])\n    plt.yticks([])\n    \n    #-- Decoder 重建輸出值 --#\n    fig.add_subplot(n_rows, 6, 3*j+3)\n    plot_tmp = plt.imshow(decoded[j].reshape(28, 28))\n    plt.xticks([])\n    plt.yticks([])\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Deep AutoEncoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_layer = Input(shape=(x_train.shape[1],))  # input_layer dim=784\n\nencoded = Dense(units=128,\n                activation='relu')(input_layer)\nencoded = Dense(units=64,\n                activation='relu')(encoded)        # lower dimension representation\nencoded = Dense(units=32,\n                activation='relu')(encoded)\n\ndecoded = Dense(units=64,\n          activation='relu')(encoded)\ndecoded = Dense(units=128,\n                activation='relu')(decoded)\ndecoded = Dense(units=x_train.shape[1],\n                activation='sigmoid')(decoded)   # output_layer dim=784\n\n#-- 建立 AutoEncoder --#\nauto_encoder = Model(input_layer, decoded)\n\n#-- 建立 Encoder --#\nencoder = Model(input_layer, encoded)\n\n#-- 建立 Decoder --#\nencoded_input = Input(shape=(32, ))\ndecoder_layer_1 = auto_encoder.layers[-3](encoded_input)\ndecoder_layer_2 = auto_encoder.layers[-2](decoder_layer_1)\ndecoder_layer_3 = auto_encoder.layers[-1](decoder_layer_2)\ndecoder = Model(encoded_input, decoder_layer_3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"auto_encoder.compile(optimizer='adam',\n                     loss='binary_crossentropy')\n\ncallbacks = EarlyStopping(monitor='val_loss',\n                          min_delta=0.0001,\n                          patience=5,\n                          restore_best_weights=True)\n\nhistory = auto_encoder.fit(x_train, x_train,\n                           epochs=100, batch_size=256,\n                           validation_data=(x_test, x_test),\n                           shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'], label='loss')\nplt.plot(history.history['val_loss'], label='val_loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded = np.array(encoder.predict(x_test))\ndecoded = np.array(decoder.predict(encoded))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (encoded.shape)\nprint (decoded.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(16, 8))\n\nn_plots = 10\nn_rows = int(n_plots / 2)\n\nfor j in range(n_plots):\n    #-- 原始資料 --#\n    fig.add_subplot(n_rows, 6, 3*j+1)\n    plot_tmp = plt.imshow(x_test[j].reshape(28, 28))\n    plt.xticks([])\n    plt.yticks([])\n    \n    #-- 低維陣列 --#\n    fig.add_subplot(n_rows, 6, 3*j+2)\n    plot_tmp = plt.imshow(encoded[j].reshape(1, 32))\n    plt.xticks([])\n    plt.yticks([])\n    \n    #-- Decoder 輸出值 --#\n    fig.add_subplot(n_rows, 6, 3*j+3)\n    plot_tmp = plt.imshow(decoded[j].reshape(28, 28))\n    plt.xticks([])\n    plt.yticks([])\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 不管是「Encoder」還是「Decoder」，\n## 他們的權重是可以調整的，\n## 所以建立AutoEncoder（Encoder+Decoder）的結構後，\n## 搭配Input當作Output的目標答案（x 與 y 相同），\n## 在Training的過程中，AutoEncoder會試著找出最好的權重，使得資訊可以盡量完整還原回去。\n\n![1](https://miro.medium.com/max/1000/0*l5pTmpzrTagcw3GC)\n\n# [Convolutional AutoEncoder][1]\n[1]:https://www.kaggle.com/br19920702/convolutional-autoencoder/"},{"metadata":{},"cell_type":"markdown","source":"# Band Marketing\n\n## 預測客戶是否會簽訂 短/中/長期的 存款合約\n## 影響客戶簽訂合約的重要因素（特徵工程）"},{"metadata":{"trusted":true},"cell_type":"code","source":"b_df = pd.read_csv('/kaggle/input/bank-marketing-analysis/bank-additional-full.csv', sep=';')\nb_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"b_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = (b_df.y == 'yes') + 0\nprint (type(y))\nprint (y.iloc[:20])\n\n# condition: False=0; True=1\n# False + 0 = 0 >> 不會簽訂合約\n# True + 0 = 1 >> 會簽訂合約","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"b_df.drop(['duration', 'y'], axis=1, inplace=True)\nb_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.get_dummies(b_df)\nprint (type(X))\n\nx_data = X.to_numpy()\narr_cols = X.columns\n\nprint (x_data.shape)\nprint (arr_cols.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mms = MinMaxScaler()\nx_data_sc = mms.fit_transform(x_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.inspection import permutation_importance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=500,\n                            max_depth=2,\n                            n_jobs=8,\n                            random_state=42)\n\ndict_performance = cross_validate(estimator=rf,\n                                  X=x_data_sc, y=y,\n                                  cv=10, n_jobs=4,\n                                  return_train_score=True,\n                                  scoring=['balanced_accuracy',\n                                           'f1_weighted',\n                                           'roc_auc',\n                                           'average_precision'])\n\ndf_performance = pd.DataFrame({'Original': [np.mean(dict_performance[k]) for k in dict_performance.keys()]},  # 將 Cross_validate 10 次的結果平均\n                              index=dict_performance.keys())\n\ndf_performance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=500,\n                            max_depth=2,\n                            n_jobs=8,\n                            random_state=42)\n\nrf.fit(x_data_sc, y)\n\nfi = permutation_importance(estimator=rf,\n                            X=x_data_sc, y=y,\n                            n_repeats=10, n_jobs=8,\n                            random_state=42).importances_mean\n# 以 原輸入值（x_data_sc）\n\nfig = plt.figure(figsize=(16,8))\nplt.barh(y=range(10, 0, -1),\n         width=sorted(fi, reverse=True)[:10],  # sorted() 升冪排序（小→大），reverse=True 轉為降冪排序（大→小）\n         alpha=0.9)                            # alpha：透明度（0~1），值越大越不透明。\nplt.yticks(range(10, 0, -1),\n           arr_cols[fi.argsort()[::-1][:10]])  # np.argsort() 升冪排序（小→大）； [::-1] 轉換成降冪排序（大→小）；返回索引值（index）\nplt.xlabel('Importance')\nplt.ylabel('Feature')\nplt.title('Original features importance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_layer = Input(shape=x_data_sc.shape[1],)\nx = Activation('relu')(input_layer)\nx = Dense(units=int(x_data_sc.shape[1]*3/4))(x)\nx = Activation('relu')(x)\nbottleneck = Dense(units=int(x_data_sc.shape[1]/2))(x)\n\nx = Activation('relu')(bottleneck)\nx = Dense(units=int(x_data_sc.shape[1]*3/4))(x)\nx = Activation('relu')(x)\n\noutput_layer = Dense(units=x_data_sc.shape[1])(x)\n\nencoder = Model(input_layer, bottleneck)\nauto_encoder = Model(input_layer, output_layer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callback = EarlyStopping(monitor='val_loss',\n                         min_delta=0.0001,\n                         patience=5,\n                         restore_best_weights=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"auto_encoder.compile(optimizer='adam',\n                     loss='mse')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = auto_encoder.fit(x_data_sc, x_data_sc,\n                           epochs=100, batch_size=256,\n                           validation_split=0.3,\n                           shuffle=True,\n                           callbacks=[callback])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded = np.array(encoder(x_data_sc))\n\n# 用 enocoder 所產出較低維的向量重建原始資料","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=500,\n                            max_depth=2,\n                            n_jobs=8,\n                            random_state=42)\n\ndict_performance = cross_validate(estimator=rf,\n                                  X=encoded, y=y,                # 以 encoder 重建的資料來進行模型訓練，\n                                  cv=10, n_jobs=4,               # 與原始資料訓練出來的模型表現比較，\n                                  return_train_score=True,\n                                  scoring=['balanced_accuracy',\n                                           'f1_weighted',\n                                           'roc_auc',\n                                           'average_precision'])\n\ndf_performance['Encoded'] = [np.mean(dict_performance[k]) for k in dict_performance.keys()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_performance\n\n# 以原始資料訓練的模型表現與；\n# 以 encoder 重建的資料訓練的模型表現\n# 差異不大\n# 代表透過 encoder 重建的原始資料與實際原始資料相似","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_pers = ['age', 'marital_divorced', 'marital_married', 'marital_single', 'marital_unknown', 'education_basic.4y', 'education_basic.6y', 'education_basic.9y',\n             'education_high.school', 'education_illiterate', 'education_professional.course', 'education_university.degree', 'education_unknown','job_admin.',\n             'job_blue-collar', 'job_entrepreneur', 'job_housemaid', 'job_management', 'job_retired', 'job_self-employed', 'job_services', 'job_student',\n             'job_technician', 'job_unemployed', 'job_unknown']\n\nfeat_fina = ['default_no', 'default_unknown', 'default_yes', 'housing_no', 'housing_unknown', 'housing_yes', 'loan_no', 'loan_unknown', 'loan_yes']\n\nfeat_camp = ['campaign', 'pdays', 'previous', 'contact_cellular', 'contact_telephone', 'month_apr', 'month_aug', 'month_dec', 'month_jul', 'month_jun', 'month_mar',\n             'month_may', 'month_nov', 'month_oct', 'month_sep', 'day_of_week_fri', 'day_of_week_mon', 'day_of_week_thu', 'day_of_week_tue', 'day_of_week_wed',\n             'poutcome_failure', 'poutcome_nonexistent', 'poutcome_success']\n\nfeat_econ = ['emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_groups = [feat_pers, feat_fina, feat_camp, feat_econ]\n\nencoded = []\n\nfor f in feat_groups:\n    dim_input_layer = len(f)\n    #dim_layer_1 = np.max((int(dim_input_layer*3/4), 1))\n    #dim_layer_2 = np.max((int(dim_input_layer/2), 1))\n    dim_layer_1 = 64\n    dim_layer_2 = 16\n    \n    input_layer = Input(shape=(dim_input_layer,))\n    x = Activation('relu')(input_layer)\n    x = Dense(units=dim_layer_1)(x)\n    x = Activation('relu')(x)\n    \n    bottleneck = Dense(units=dim_layer_2)(x)\n    \n    x = Activation('relu')(bottleneck)\n    x = Dense(units=dim_layer_1)(x)\n    x = Activation('relu')(x)\n    \n    output_layer = Dense(units=dim_input_layer)(x)\n    \n    encoder = Model(input_layer, bottleneck)\n    auto_encoder = Model(input_layer, output_layer)\n    \n    auto_encoder.compile(optimizer='adam',\n                         loss='mse')\n    \n    x_tmp = x_data_sc[:, arr_cols.isin(f)]\n    \n    auto_encoder.fit(x_tmp, x_tmp,\n                     epochs=100, batch_size=256,\n                     shuffle=True,\n                     validation_split=0.3,\n                     callbacks=[callbacks])\n    \n    encoded.append(np.array(encoder(x_tmp)))\n    \nx_encoded = np.hstack(encoded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_name = np.array(['pers_'+str(j) for j in range(encoded[0].shape[1])] + \\\n                     ['fina_'+str(j) for j in range(encoded[1].shape[1])] + \\\n                     ['camp_'+str(j) for j in range(encoded[2].shape[1])] + \\\n                     ['econ_'+str(j) for j in range(encoded[3].shape[1])])\n\ncols_name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded_df = pd.DataFrame(x_encoded, columns=cols_name)\nencoded_df.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=500,\n                            max_depth=2,\n                            n_jobs=8,\n                            random_state=42)\n\ndict_performance = cross_validate(estimator=rf,\n                                  X=x_encoded, y=y,\n                                  cv=10, n_jobs=4,\n                                  return_train_score=True,\n                                  scoring=['balanced_accuracy',\n                                           'f1_weighted',\n                                           'roc_auc',\n                                           'average_precision'])\n\ndf_performance['group_encoded'] = [np.mean(dict_performance[k]) for k in dict_performance.keys()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_performance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=500,\n                            max_depth=2,\n                            n_jobs=8,\n                            random_state=42)\n\nrf.fit(x_encoded, y)\n\nfi = permutation_importance(estimator=rf,\n                            X=x_encoded, y=y,\n                            n_repeats=10,\n                            n_jobs=8,\n                            random_state=42).importances_mean\n\nfig = plt.figure(figsize=(16, 8))\nplt.barh(y=range(10, 0, -1),\n         width=sorted(fi, reverse=True)[:10],\n         alpha=0.9)\nplt.yticks(range(10, 0, -1),\n           cols_name[fi.argsort()[::-1][:10]])\nplt.ylabel('Feature')\nplt.xlabel('Importance')\nplt.title('Group-encoded features importance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}