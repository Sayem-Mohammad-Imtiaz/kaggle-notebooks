{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 01-Importing Libraries:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport os\nimport nltk\n#nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 02-Load Data:"},{"metadata":{},"cell_type":"markdown","source":"![](http://)Loading Data "},{"metadata":{"trusted":true},"cell_type":"code","source":"messages = pd.read_csv('../input/sms-spam-collection-dataset/spam.csv',encoding='latin-1')\nmessages.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems like there are some columns, who dont have any value in the data set.we can drop them.\nHii Friends\" Do you guys Love Cricket\",Let me know in the comment section. I am from India and I love Cricket.Let me Know about You."},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"messages.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above line of code drops the unnessary columns,Now lets make Our DataFrame more Interesting by giving some meaningful column names."},{"metadata":{"trusted":true},"cell_type":"code","source":"messages.rename(columns={'v1':'Label','v2':'Message'},inplace = True)\nmessages.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here some information about our created dataframe, It seems like there is no NaN or null values present,Let me know if you gus find some. We can clean them .Right..."},{"metadata":{},"cell_type":"markdown","source":"## 03- Visualization:"},{"metadata":{},"cell_type":"markdown","source":"Ok now Let's Visualize Our Data."},{"metadata":{},"cell_type":"markdown","source":"As I am a Big Fan \"Twilight\" , I am using this palette style ,You can use as your wish , I have given a set of Palette styles down ,Play aound"},{"metadata":{},"cell_type":"markdown","source":"Possible Palettes are: Accent, Accent_r, Blues, Blues_r, BrBG, BrBG_r, BuGn, BuGn_r, BuPu, BuPu_r, CMRmap, CMRmap_r, Dark2, Dark2_r, GnBu, GnBu_r, Greens, Greens_r, Greys, Greys_r, OrRd, OrRd_r, Oranges, Oranges_r, PRGn, PRGn_r, Paired, Paired_r, Pastel1, Pastel1_r, Pastel2, Pastel2_r, PiYG, PiYG_r, PuBu, PuBuGn, PuBuGn_r, PuBu_r, PuOr, PuOr_r, PuRd, PuRd_r, Purples, Purples_r, RdBu, RdBu_r, RdGy, RdGy_r, RdPu, RdPu_r, RdYlBu, RdYlBu_r, RdYlGn, RdYlGn_r, Reds, Reds_r, Set1, Set1_r, Set2, Set2_r, Set3, Set3_r, Spectral, Spectral_r, Wistia, Wistia_r, YlGn, YlGnBu, YlGnBu_r, YlGn_r, YlOrBr, YlOrBr_r, YlOrRd, YlOrRd_r, afmhot, afmhot_r, autumn, autumn_r, binary, binary_r, bone, bone_r, brg, brg_r, bwr, bwr_r, cividis, cividis_r, cool, cool_r, coolwarm, coolwarm_r, copper, copper_r, cubehelix, cubehelix_r, flag, flag_r, gist_earth, gist_earth_r, gist_gray, gist_gray_r, gist_heat, gist_heat_r, gist_ncar, gist_ncar_r, gist_rainbow, gist_rainbow_r, gist_stern, gist_stern_r, gist_yarg, gist_yarg_r, gnuplot, gnuplot2, gnuplot2_r, gnuplot_r, gray, gray_r, hot, hot_r, hsv, hsv_r, icefire, icefire_r, inferno, inferno_r, jet, jet_r, magma, magma_r, mako, mako_r, nipy_spectral, nipy_spectral_r, ocean, ocean_r, pink, pink_r, plasma, plasma_r, prism, prism_r, rainbow, rainbow_r, rocket, rocket_r, seismic, seismic_r, spring, spring_r, summer, summer_r, tab10, tab10_r, tab20, tab20_r, tab20b, tab20b_r, tab20c, tab20c_r, terrain, terrain_r, twilight, twilight_r, twilight_shifted, twilight_shifted_r, viridis, viridis_r, vlag, vlag_r, winter, winter_r"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(messages.Label,palette=\"twilight\")\nplt.xlabel('Label')\nplt.title('Number of ham and spam messages')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets add some percentages to more understanding."},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.countplot(y=\"Label\", data=messages,palette=\"twilight\")\nplt.xlabel('Count')\nplt.title('Number of ham and spam messages')\n\ntotal = len(messages['Label'])\nfor p in ax.patches:\n        percentage = '{:.1f}%'.format(100 * p.get_width()/total)\n        x = p.get_x() + p.get_width() + 0.02\n        y = p.get_y() + p.get_height()/2\n        ax.annotate(percentage, (x, y))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 04-Data Cleaning and Preprocessing:"},{"metadata":{},"cell_type":"markdown","source":"I will try to make you understand evry line,Please read every comments written after every line of code "},{"metadata":{"trusted":true},"cell_type":"code","source":"ps = PorterStemmer # You can use stemming but here I am using Lemmatization.\nlmr = WordNetLemmatizer\ncorpus = []\nfor i in range( 0, len(messages)):\n    review = re.sub('[^a-zA-Z]', ' ', messages['Message'][i]) # Cleaning all special characters and numbers, keeping words only\n    review = review.lower() # Making all the words to lower case.\n    review = review.split()\n    \n    review = [lmr.lemmatize('word',word) for word in review if word not in set(stopwords.words('english'))]\n    # This is list compreshession adding the words which are not avaibale in stopwords.\n    review = ' '.join(review)\n    corpus.append(review) # adding all words to Corpus","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Stemming->\"Stemming is the process of reducing infected or derived words to there word stem, base or root form\"-Wikipedia\nBut the problem in stemming is some of stemming words dont have any actual meaning.(for example- \"intelligence,intelligent,intelligently\" becomes \"intelgen\" which dont have any meaning.\nin otherhand Lemmatization make it \"intelligent\" which have an actual meaning."},{"metadata":{},"cell_type":"markdown","source":"Ok it's becoming little intense ,,, Let's have some fun.... Anyone can tell me \"Pandora\" is a name used for one Planate in a very famous Science Friction Movie ,What is the name of the movie ? # Answer it in the comment section"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a Bag Of Words model:\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features=5000) # Taking the 5000 important words.\nX = cv.fit_transform(corpus).toarray() # Fitting the model to Corpus and converting it to array.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets create a Bag Of words using the messages and consider the 5000 important words."},{"metadata":{},"cell_type":"markdown","source":"Lets's Create Dummi Values for Ham and Spam"},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = pd.get_dummies(messages['Label'])\n# No need to specify teo catregorical column here we can just define one culumn (ie:if 0-> Ham or if ->1 Spam):\nY = Y.iloc[:,1].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets see Our X, Is it looking Handsome?\nAs it is the independent Variable I am taking it as Hero of Our Movie.(Don't aks me if there are more than one one independent variables, they might be side Heros or Fraiends of Hero)- Got it You Dirty Mind"},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it time for Our Heroine, Y"},{"metadata":{"trusted":true},"cell_type":"code","source":"Y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Woew She looks Beautyful...#bornsexy"},{"metadata":{},"cell_type":"markdown","source":"## 05-Spliting Data into train and test:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Spliting the Data into train and test:\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test = train_test_split(X, Y, test_size= 0.2,random_state = 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test_size = 0.2 mean I am taking 20% of all data to test our model and random_state means that 20% data will be cloose randomly from the main dataset."},{"metadata":{},"cell_type":"markdown","source":"## 06-Training Model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the model with Naive Bayes Classifier:\nfrom sklearn.naive_bayes import MultinomialNB\nspam_detection_model=MultinomialNB().fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The predected output of our model\nY_pred = spam_detection_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Confusion matrix- It show the True +ve,False +ve,True -ve and False -ve values. It mainly helps to find the accuracy of the Classificaton model."},{"metadata":{},"cell_type":"markdown","source":"Okk answer me onething guys ---Which one is more Dengerious ? (False +ve or False -ve or Depends on the situation) If you can explane your ans!!!"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconfusion_m = confusion_matrix(Y_test,Y_pred) #It shows the total Right Predictions (960+140) and Wrong Predictions(10+5).\nprint(confusion_m)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 07-Accuracy:"},{"metadata":{},"cell_type":"markdown","source":"Finally Accuracy of Our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy = accuracy_score(Y_test,Y_pred) # It shows the accuracy of the model.\nprint (accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like we got a good accuracy just using a simple Classifier, we an accuracy approximate to 98 %.OKKK it's GOOD ."},{"metadata":{},"cell_type":"markdown","source":"Thank You Guys.ALL the very Best for your Data Science Career."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}