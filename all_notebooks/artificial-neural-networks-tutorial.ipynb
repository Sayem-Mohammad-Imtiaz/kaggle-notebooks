{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Artificial Neural Networks Tutorial\n- In this tutorial, We will write the artificial neural network algorithm.\n- In fact, Artificial Neural Networks is similar with Logistic Regression.\n- "},{"metadata":{},"cell_type":"markdown","source":"## 2 Layer Neural Network\n- First, we will see 2 layer neural network with stages\n- We will use 1 hidden layer and 3 artificial neurons\n\n### Stages:\n- Initialize Parameters\n- Forward Propagation\n- Compute Loss and Cost Function.\n- Backward Propagation\n- Update weight and bias\n- Repeat n times"},{"metadata":{},"cell_type":"markdown","source":"### Imports"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # Linear algebra.\nimport pandas as pd # Data processing.\nimport matplotlib.pyplot as plt # Visualize\n\nfrom sklearn.model_selection import train_test_split # For data split.\nfrom sklearn.model_selection import cross_val_score # For find accuracy.\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prepare Dataset"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/data.csv\")\ndata.drop([\"id\"],axis = 1,inplace = True)\n\ndata.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis] # M = 1, B = 0\n\nx_data = data.drop([\"diagnosis\",\"Unnamed: 32\"],axis = 1)\n\nx = (x_data - np.min(x_data))/(np.max(x_data)-np.min(x_data)).values # Normalize data\ny = data.diagnosis","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train - Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(x, y, test_size = 0.15, random_state = 42) # 85% Train, 15% Test\n\nx_train = x_train.values.T\nx_test = x_test.values.T\ny_test = y_test.values.reshape(1,y_test.shape[0])\ny_train = y_train.values.reshape(1,y_train.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Initialize Parameters\n- To start the loop we need to give the initial values ​​in an intuitive manner."},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_weights_and_bias(dimension):\n    w = np.full((dimension,1),0.01)\n    b = 0\n    return w, b","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Activation Function\n- We will use sigmoid function\n- In fact, it is not logical to use sigmoid function in artificial neural networks except between hidden layer and output.\n- We ignore it because our current goal is to learn"},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(z):\n    y_head = 1/(1+np.exp(-z)) # It is the formule of sigmoid function\n    return y_head","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Forward - Backward Propagation\n- First we will estimate with bias and weight values ​​then we will go back and update our weights and bias if the result is wrong."},{"metadata":{"trusted":true},"cell_type":"code","source":"def forward_backward_propagation(w,b,x_train,y_train):\n    y_head = sigmoid(np.dot(w.T,x_train) + b) # We multiply features with our weight values, add bias and send it to sigmoid function\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head) # It is the formule of loss function\n    cost = (np.sum(loss))/x_train.shape[1] # Calculate cost function\n    \n    derivative_weight = (np.dot(x_train, ((y_head-y_train).T)))/x_train.shape[1] # Calculate derivative of weights\n    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1] # Calculate derivative of bias\n    \n    gradients = {\"derivative_weight\":derivative_weight,\"derivative_bias\":derivative_bias}\n    return cost,gradients","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Update Parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"def update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n\n    for i in range(number_of_iterarion):\n        # Start learning\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train) # Make forward and backward propagation and calculate cost and derivatives\n        cost_list.append(cost)\n        # Updating weight and bias\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 250 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n\n    parameters = {\"w\": w,\"b\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"def prediction(w,b,x_test):\n    z = sigmoid(np.dot(w.T,x_test)+b) # z -> Estimates of our model\n    y_prediction = np.zeros((1,x_test.shape[1])) # We create an array, we will set the array.\n    \n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            y_prediction[0,i] = 0\n        else:\n            y_prediction[0,i] = 1\n\n    return y_prediction","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def artificial_neural_networks(x_train,x_test,y_train,y_test,learning_rate,number_of_iteration):\n    dimension = x_train.shape[0]\n    w,b = initialize_weights_and_bias(dimension) # Initialize Parameters\n    parameters,gradients, cost_list = update(w,b,x_train,y_train,learning_rate,number_of_iteration) # Update parameters\n    \n    train_prediction = prediction(parameters[\"w\"],parameters[\"b\"],x_train) # Estimates of our model\n    test_prediction = prediction(parameters[\"w\"],parameters[\"b\"],x_test) # Estimates of our model\n    \n    print(\"Train accuracy: {} %\".format(100 - np.mean(np.abs(train_prediction - y_train)) * 100))\n    print(\"Test accuracy: {} %\".format(100 - np.mean(np.abs(test_prediction - y_test)) * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate = 1\nnumber_of_iteration = 5000\nann = artificial_neural_networks(x_train,x_test,y_train,y_test,learning_rate,number_of_iteration)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## L Layer Neural Network\n\n- We will use 3 hidden layer.\n- We will use keras. Because It's a easy way to create ANN model\n- In this model, We will calculate accuracy with cross validation score"},{"metadata":{},"cell_type":"markdown","source":"### Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Dense","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Terminology\n- units -> Number of Neuron\n- kernel_initializer (uniform) -> initialize values\n- activation -> Our activation function. We use tanh\n- input_dim -> Number of Feature\n- optimizer (adam) -> adaptive moment estimation. (You can search in Google)\n- loss -> Our loss function\n- metrics -> Accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_classifier():\n    classifier = Sequential()\n    classifier.add(Dense(units = 8, kernel_initializer = \"uniform\", activation = \"tanh\", input_dim = x_train.shape[1]))\n    classifier.add(Dense(units = 4, kernel_initializer = \"uniform\", activation = \"tanh\"))\n    classifier.add(Dense(units = 2, kernel_initializer = \"uniform\", activation = \"tanh\"))\n    classifier.add(Dense(units = 1, kernel_initializer = \"uniform\", activation = \"sigmoid\"))\n    classifier.compile(optimizer = \"adam\", loss = \"binary_crossentropy\",metrics = [\"accuracy\"])\n    return classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = x_train.T\ny_train = y_train.T\n\nclassifier = KerasClassifier(build_fn = build_classifier, epochs = 100) # epoch -> Number of Iteration\naccuracies = cross_val_score(estimator = classifier, X = x_train, y = y_train, cv = 3) # Cross validation score\nmean = accuracies.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracies)\nprint(\"Accuracy mean :\",mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}