{"cells":[{"metadata":{},"cell_type":"markdown","source":"code from sklearn \"Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation\" example"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n!pip install  -U pbr\n!pip install -U lda \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nimport lda\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport seaborn as sns\nimport os\nimport glob\nimport random\nprint(os.listdir(\"../input\"))\nimport seaborn as sns\n\n\n# Any results you write to the current directory are saved as output.\nimport nltk\nfrom nltk import ngrams\n\nimport spacy\nimport sklearn\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer,TfidfTransformer\nfrom sklearn.decomposition import  PCA,NMF, LatentDirichletAllocation\nfrom dask.distributed import Client\nimport joblib\n#nlp_fr = spacy.load(\"fr_core_news_sm\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntry:\n#     client = Client('127.0.0.1:8786',timeout=5,set_as_default=True)\n    client =Client(timeout=5,set_as_default=True,processes=False,n_workers=3,memory_limit='auto', silence_logs='error')\nexcept ValueError as e:\n    client =Client(timeout=5,set_as_default=True,processes=False, silence_logs='error')\nexcept OSError as e:\n#     print(e)\n    client =Client(set_as_default=True,n_workers=2,threads_per_worker=4,dashboard_address=None,processes=False, silence_logs='error')\nclient","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import spacy.lang.fr\nfrom stop_words import get_stop_words\nstopwords_fr_set=set(nltk.corpus.stopwords.words('french'))\nstopwords_fr_set.update(get_stop_words('fr'))\nstopwords_fr_set.update(spacy.lang.fr.stop_words.STOP_WORDS)\nstopwords_fr_set.update([\"c'est\",\"j'ai\",\"n'est\",\"n'ait\",\"ca\",\"Ã§a\",\"sais\",\"jamais\",\"chose\",\"ex\",\"'quelqu'\",'quelqu',\"quelqu'\",\"faut\",\"faudrait\"])\nstopwords_fr_set=list(stopwords_fr_set)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_samples = 1024*1024\nn_features = 3000\nn_components = 160\nn_top_words = 15\nngram_range=(1,3)\nmin_df=16\nmax_df=0.25","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_vectorizer = TfidfVectorizer(max_df=max_df, min_df=min_df,\n                                   max_features=n_features,\n                                   stop_words=stopwords_fr_set,\n                                   ngram_range=ngram_range\n                                   )\ntf_vectorizer = CountVectorizer(max_df=max_df, min_df=min_df,\n                                max_features=n_features,\n                                stop_words=stopwords_fr_set,\n                                ngram_range=ngram_range)\ntfidf_transformer=TfidfTransformer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"texts=[]\ntexts_byfiles=dict()\nfilelist=glob.glob(\"../input/**/*.csv*\", recursive=True)\nrandom.shuffle(filelist)\nfor f in filelist:\n    print (f)\n    df=pd.read_csv(f,low_memory=False)\n    dftext=[]\n    texts_byfiles[\"f\"]=dftext\n    for n,s in df.items():\n        for e in s:\n            if isinstance(e,str):\n                if len(e.split())>2 :\n                    dftext.append(e)\n    texts+=dftext\n\ntexts=list(set(texts))\n\nrandom.shuffle(texts)\ntexts_all=texts.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(texts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if n_samples<len(texts):\n    texts=random.sample(texts,n_samples)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with joblib.parallel_backend('dask'):\n    tf = tf_vectorizer.fit_transform(texts)\ntf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf_vectorizer.stop_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with joblib.parallel_backend('dask'):\n    tfidf=tfidf_transformer.fit_transform(tf)\ntfidf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf_vectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf_vectorizer.stop_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nrr = dict(zip(tf_vectorizer.get_feature_names(),  tfidf_transformer.idf_))\n\ntoken_weight = pd.DataFrame.from_dict(rr, orient='index').reset_index()\ndel rr\n\ntoken_weight.columns=('token','weight')\ntoken_weight = token_weight.sort_values(by='weight', ascending=False)\ntoken_weight.reset_index(drop=True,inplace=True) \ntoken_weight[\"freq\"]=1/token_weight.weight\n\nsns.barplot(x='token', y='weight', data=token_weight.iloc[:60], )            \nplt.title(\"Inverse Document Frequency(idf) per token\")\nfig=plt.gcf()\nfig.set_size_inches(25,15)\nax=fig.axes[0]\nax.tick_params(axis='x',labelrotation=90 )\nplt.show()\n\n\ntoken_weight","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x='token', y='freq', data=token_weight.iloc[:60], )            \nplt.title(\"Inverse Document Frequency(idf) per token\")\nfig=plt.gcf()\nfig.set_size_inches(25,15)\nax=fig.axes[0]\nax.tick_params(axis='x',labelrotation=90 )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token_weight.tail(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token_weight.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nwith joblib.parallel_backend('dask'):\n    nmf = NMF(n_components=n_components,\n              alpha=.1, \n              l1_ratio=.5,\n    #           tol=1e-3,\n              verbose=True,\n              max_iter=20\n             ).fit(tfidf)\nnmf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef print_top_words(model, feature_names, n_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        message = \"Topic #%d: \" % topic_idx\n        message += \" \".join([feature_names[i]\n                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n        print(message)\n    print()\ndef get_top_words_list(model, feature_names, n_top_words):\n    topwords=[]\n    for topic_idx, topic in enumerate(model.components_):\n       \n        topwords.append([feature_names[i]\n                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n    return topwords\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_feature_names = tf_vectorizer.get_feature_names()\nprint_top_words(nmf, tfidf_feature_names, n_top_words)\nnmf_top_words_list=get_top_words_list(nmf, tfidf_feature_names, n_top_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with joblib.parallel_backend('dask'):\n    tfidf_nmf=nmf.transform(tfidf)\ntfidf_nmf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ind_text=np.random.choice(n_samples,5)\n\nfor t,topics in zip([texts[i] for i in ind_text ],\n                    \n        tfidf_nmf[ind_text]):\n    print(t)\n    top_topics=np.argsort(topics)[-3:]\n    for n in top_topics:\n        print(f\"topic {n}: {', '.join(nmf_top_words_list[n][:6])}\")\n    print()\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with joblib.parallel_backend('dask'):\n    pca_nmf=sklearn.decomposition.KernelPCA(n_components=2,eigen_solver= \"arpack\")\n    pca_nmf.fit(tfidf_nmf[np.random.choice(n_samples,9000)])\n    pca_nmf.fit(tfidf_nmf[np.random.choice(n_samples,9000)])\n    tfidf_nmf_pca=pca_nmf.fit_transform(tfidf_nmf[np.random.choice(n_samples,9000)])\n\nsns.scatterplot(x=\"pca1\",y=\"pca2\",data=pd.DataFrame(tfidf_nmf_pca,columns=[\"pca1\",\"pca2\"]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with joblib.parallel_backend('dask'):\n    lda = lda.LDA(n_topics=n_components,\n                                    n_iter =5\n                                   )\n\n    tfidf_lda=lda.fit_transform(tf)\n\n\nprint(\"\\nTopics in LDA model:\")\ntf_feature_names = tf_vectorizer.get_feature_names()\nlda_top_words_list=get_top_words_list(lda, tf_feature_names, n_top_words)\nprint_top_words(lda, tf_feature_names, n_top_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda_best_topics=np.argsort(tfidf_lda.mean(axis=0))[-10:]\nfor n in range(6):\n    print(f\"topic {lda_best_topics[-n]}: {', '.join(lda_top_words_list[lda_best_topics[-n]][:15])}\")\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topic_texts=np.argsort(tfidf_lda,axis=0)[:300]\nfor t in range(tfidf_lda.shape[1]):\n    for i in range(6):\n        nmax=topic_texts[-i,t]\n        print(tfidf_lda[nmax,t])\n        print(texts[nmax][:300])    \n    print(f\"topic {t}: {', '.join(lda_top_words_list[t][:15])}\")\n    print(\"***\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ind_text=np.random.choice(n_samples,5)\n\nfor t,topics in zip([texts[i] for i in ind_text ],\n                    \n        tfidf_lda[ind_text]):\n    print(t)\n    top_topics=np.argsort(topics)[-3:]\n    for n in top_topics:\n        print(f\"topic {n}: {', '.join(nmf_top_words_list[n][:6])}\")\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nprint(\"\\nTopics in LDA model:\")\ntf_feature_names = tf_vectorizer_n.get_feature_names()\nlda_top_words_list=get_top_words_list(lda, tf_feature_names, n_top_words)\nprint_top_words(lda, tf_feature_names, n_top_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}