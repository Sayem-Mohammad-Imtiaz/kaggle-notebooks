{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data  Analysis\n"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"#importing important libraries\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 2 csv files in the current version of the dataset:\n"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n#I renamed the file as train and test files to keep it simple.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next hidden code cells define functions for plotting data. Click on the \"Code\" button in the published kernel to reveal the hidden code."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Distribution graphs (histogram/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow # To fit the graphs with in the area of notebook properly.\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n    filename = df.dataframeName\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's check 1st file: /kaggle/input/test.csv"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"import pandas as pd\nnRowsRead = 1000 # specify 'None' if want to read whole file\n# test.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows\ndf1 = pd.read_csv('/kaggle/input/test.csv', delimiter=',', nrows = nRowsRead)\ndf1.dataframeName = 'test.csv'\nnRow, nCol = df1.shape\nprint(f'There are {nRow} rows and {nCol} columns')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a quick look at what the data looks like:"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"df1.head(5)#printing the first five rows of the test dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Distribution graphs (histogram/bar graph) of sampled columns:"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"plotPerColumnDistribution(df1, 10, 5) \n#calling the plotting function to plot different graphs to visualize relations between different features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation matrix:"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"#plotting the correlation matrix colors changing from dark to light show an increase in correlation \n#the darker colors suggest negative correlation\nplotCorrelationMatrix(df1, 8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" **It can be observed in the above correlation matrix there is a clear positive correlation between (land_area & graduates)  , (physicians & income) , (physicians and labors) and negative correlation between (percent_senior & percent_city)**"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"Scatter and density plots:"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"#plotting correlation by making pairs between different features to figure out which pairs are more correlated.\nplotScatterMatrix(df1, 20, 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's check 2nd file: /kaggle/input/train.csv"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"nRowsRead = 1000 # specify 'None' if want to read whole file\n# train.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows\ndf2 = pd.read_csv('/kaggle/input/train.csv', delimiter=',', nrows = nRowsRead)\ndf2.dataframeName = 'train.csv'\nnRow, nCol = df2.shape\nprint(f'There are {nRow} rows and {nCol} columns')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a quick look at what the data looks like:"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"df2.head(5) #printing first five values of train dataset.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Distribution graphs (histogram/bar graph) of sampled columns:"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"#plotting the number of counts of each region which vary from 1-4\nplotPerColumnDistribution(df2, 10, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation matrix:"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"#plotting the correlation matrix of test dataset\n#both test and train correlation matrix show similarities in between these pairs (physicians,hospital_beds),(hospital_beds,income) as must \n#be clear from the fact that more physicians are required if no. of hospitals increase.\nplotCorrelationMatrix(df2, 8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scatter and density plots:"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"plotScatterMatrix(df2, 20, 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now lets plot graph between features which are highly correlated**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nax = sns.scatterplot(x=df2['physicians'] ,y= df2['income'], data=df2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.scatterplot(x=df2['percent_senior'],y=df2['percent_city'],data=df2)\n#Negative correlation can be clearly seen in the graph","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.scatterplot(x=df2['physicians'] ,y= df2['labor'], data=df2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.scatterplot(x=df2['graduates'] ,y= df2['percent_senior'], data=df2)\n#There is a clear correlation between graduates and percent senior","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = df2.drop('crime_rate',axis=1)\nY_train = df2[['crime_rate']]\nX_test = df1.dropna(axis=1)\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Note : "},{"metadata":{},"cell_type":"markdown","source":"**Since the actual labels have not been provided in order to find the best model for prediction all operations have to be done merely on train dataset to find the best model, I will use Mean Squared Error Metrics for model evaluation **"},{"metadata":{},"cell_type":"markdown","source":"# Using different Models to generate Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets split the data into train and test 1:3 split\nfrom sklearn.model_selection import train_test_split\nX = df2.drop('crime_rate', axis=1)\nY = df2[['crime_rate']]\ntrain_x, test_x, train_y, test_y = train_test_split(X,Y\n            ,test_size=0.33,random_state=0)\ntrain_x.shape[0], test_x.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Using Linear Model from Sk-Learn for prediction**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#using Linear Model for Predictions\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error \nlin   = linear_model.LinearRegression()\nlin.fit(train_x,train_y)\npreds = lin.predict(test_x)\nmean_squared_error(test_y,preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state= 0)\nregressor.fit(train_x, train_y)\npreds = regressor.predict(test_x)\nmean_squared_error(test_y,preds)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\nmodel = svm.SVR()\nmodel.fit(train_x,train_y)\npreds = model.predict(test_x)\nmean_squared_error(test_y,preds)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"            **Above Model Analysis indicates the best model is Linear Regression with minimum mean squared error**\n                    "},{"metadata":{},"cell_type":"markdown","source":"                            **DecisionTreeRegressor(237.62)<SVM(168.504)<LinearModel(147.556)**"},{"metadata":{},"cell_type":"markdown","source":"**Using Statsmodel library to generate statistical features of the data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as ssm\n\ntrain_x = ssm.add_constant(train_x)\nmodel = ssm.OLS(train_y,train_x).fit()\npreds = model.summary()\npreds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now Let's generate predictions for our test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Linear Model from sklearn library for predictions\nfrom sklearn import linear_model\nfrom sklearn import metrics\nlin   = linear_model.LinearRegression()\nlin.fit(X_train, Y_train)\npreds = lin.predict(X_test)\nindexes = len(list(preds)) #generating index\n\n\ncrime_rate_predictions = pd.DataFrame(data = preds, index=range(1,indexes+1),columns=['crime_rate'])\nprint(crime_rate_predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"                          **Creating CSV File for Prediction**"},{"metadata":{"trusted":true},"cell_type":"code","source":"crime_rate_predictions.to_csv('CrimeRatePredictions.csv', index=True)","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}