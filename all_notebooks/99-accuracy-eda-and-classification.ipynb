{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Jun 10 08:59:51 2020","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"@author: jaket\n\"\"\"\n#j2p\n### Exercise pattern classification","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This data was collected from activity tracking device(s) (unspecified). There is a training data set with a known class ranging<br>\nFrom A-E, which should be preticted in the test data set. Many of the features are not defined, limiting the domain-specific<br>\npreprocessing and EDA we can do.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Set up<br>\nImport, set, read, initial exploration","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Import packages","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nimport pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport warnings \nimport seaborn as sns\nimport sklearn\nfrom datetime import datetime\nimport calendar\n%matplotlib inline\npd.set_option('display.max_rows', 1000)\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Set wd and read in data","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/exercisepatternpredict/pml-training.csv', error_bad_lines=False, index_col=False).drop('Unnamed: 0', axis=1)\ntest_df = pd.read_csv('../input/exercisepatternpredict/pml-testing.csv', error_bad_lines=False, index_col=False).drop('Unnamed: 0', axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Randomise the rows, it is currently very structured and improve training predictability","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df = train_df.sample(frac=1).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Explore","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"print(train_df.columns.values)\nprint(train_df.isna().sum()) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.heatmap(train_df.isnull(), cbar=False) # Heatmap to visualise NAs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's clear we have a large amount of missing data in some columns. It's likely that where this data is available is specific to<br>\na certain exercise type/class. Due to the amount, imputation will not be effective. In all other columns there is no missing data<br>\nIt could be appropriate to create a seperate df in the cases which have this additional data.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA<br>\nFirst, it would make sense to run some pairplots using class as the Hue, so we can begin to determine which variables are related<br>\nTo the target. We can only do this in the training data set. Lets start by examining the frequency of classes and participants","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df['classe']=train_df['classe'].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"freq_plot1=train_df.filter(items=['user_name', 'classe'])\nfreq_plot1=freq_plot1.groupby(['user_name'])['classe'].agg(counts='value_counts').reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.barplot(data = freq_plot1, x = 'counts', y = 'user_name', hue = 'classe', ci = None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Its clear that the frequency of class A is much greater than each other class. The classes are not divided equally between users.<br>\nFor example, Jeremy has around 2x as many A's as other classes. This is important as user may be an important predictive<br>\nVariable when predicting the test data, and we will later need to OH Encode user.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now lets plot some pairplots...","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"            \npairplot1=train_df.filter(items=['num_window', 'roll_belt', 'pitch_belt', 'yaw_belt', 'total_accel_belt', 'classe'])\nsns.pairplot(pairplot1, hue='classe',  plot_kws = {'alpha': 0.6, 'edgecolor': 'k'},size = 4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There aren't any large differences in these relationships regarding class. However, the distribution does seem to be slightly different<br>\nAcross different classes. Lets look at the X and Y axes of the gyros, accel and magnet belt.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"pairplot2=train_df.filter(items=['num_window', 'gyros_belt_x', 'gyros_belt_y', 'accel_belt_x', 'accel_belt_y',  'magnet_belt_x','magnet_belt_y', 'classe'])\nsns.pairplot(pairplot2, hue='classe',  plot_kws = {'alpha': 0.6,  'edgecolor': 'k'},size = 4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again the data is closely grouped, though we can see that movement D has some determinable features not shared by the others on<br>\nThese axes. Can the Z-axes provide any more information?","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"pairplot3=train_df.filter(items=['num_window', 'gyros_belt_z', 'accel_belt_z', 'magnet_belt_z', 'classe'])\nsns.pairplot(pairplot3, hue='classe',  plot_kws = {'alpha': 0.6, 'edgecolor': 'k'},size = 4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we see some extremely distinctive features of class D on the Z axis. This means these variables will be important in classification.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"pairplot4=train_df.filter(items=['roll_arm', 'pitch_arm', 'yaw_arm', 'total_accel_arm', 'classe'])\nsns.pairplot(pairplot4, hue='classe',  plot_kws = {'alpha': 0.6, 'edgecolor': 'k'},size = 4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The arm data shows significantly different patterns from the belt data. However, there is not a considerable difference between classes.<br>\nAgain, lets look at them on the x/y and then the Z axis.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"pairplot5=train_df.filter(items=['num_window', 'gyros_arm_x', 'gyros_arm_y', 'accel_arm_x', 'accel_arm_y',  'magnet_arm_x','magnet_arm_y', 'classe'])\nsns.pairplot(pairplot5, hue='classe',  plot_kws = {'alpha': 0.6,  'edgecolor': 'k'},size = 4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And on the Z:","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"pairplot6=train_df.filter(items=['num_window', 'gyros_arm_z', 'accel_arm_z', 'magnet_arm_z', 'classe'])\nsns.pairplot(pairplot6, hue='classe',  plot_kws = {'alpha': 0.6, 'edgecolor': 'k'},size = 4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The differences here are again subtle. The distribution of A seems to take a distinctly different pattern from the others.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We can do similar plots for the forearm variables if we wanted but i'll skip it for now. Lastly, lets take a look at some of the<br>\nVariables where nearly all data is missing. If this is uninformative, it makes sense for us to remove it, however it may give<br>\naway a certain class clearly. Since there are many of these I shall just pick a few out.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"pairplot7=train_df.filter(items=['skewness_roll_belt', 'max_roll_belt', 'max_picth_belt', \n                                 'var_total_accel_belt', 'stdev_roll_belt',\n                                 'avg_yaw_belt', 'classe'])\nsns.pairplot(pairplot7, hue='classe',  plot_kws = {'alpha': 0.6, 'edgecolor': 'k'},size = 4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These dont show any clear associations with class. Given the small (<1%) fraction of the data available, it makes sense that we remove these.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Before we move to modelling we should consider feature removal, feature engineering and categorical encoding amongst other things.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Feature Removal","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First, drop cols with high % NA","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"print(train_df.isna().sum()) \ntrain_df = train_df.loc[:, train_df.isnull().mean() < .8] #remove cols with <80% completeness.\ntest_df = test_df.loc[:, test_df.isnull().mean() < .8] #remove cols with <80% completeness.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The timestamps are not going to be useful for predicting on the other data sets. Also, they would not be informative in real-life<br>\nactivity prediction. It make be that correctly used these timestampts could give away the entire answer if they align with the<br>\nTest data, which will be the case if the test data is a random sample of train. We'll lose these for now. to make it more realistic.<br>\nFor the same reasons, it makes sense to also lose num window and new window","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df = train_df.drop(['raw_timestamp_part_1', 'raw_timestamp_part_2' ,'cvtd_timestamp', 'new_window','num_window'], axis=1)\ntest_df = test_df.drop(['raw_timestamp_part_1', 'raw_timestamp_part_2' ,'cvtd_timestamp', 'new_window','num_window', 'problem_id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"As we dont have an excessive number of features, nor a considerable amount of categorical variables to expand our X-cols, we<br>\nCan consider generating some interaction features. For example, lets combine all x y and z","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Generate a fn to turn 0s into 1s as it not ruin the interaction variables","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def zeros_to_ones(x):\n    x = np.where(x==0, 1, x)\n    return(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Np.prod will give us the product (multiple) of all columns for a given row, creating an interaction variable on the axis.    ","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def feat_eng (df):\n    df['x_axis_feat']=df[df.columns[df.columns.to_series().str.contains('_x')]].apply(zeros_to_ones).apply(np.prod, axis=1)\n    df['y_axis_feat']=df[df.columns[df.columns.to_series().str.contains('_y')]].apply(zeros_to_ones).apply(np.prod, axis=1)\n    df['z_axis_feat']=df[df.columns[df.columns.to_series().str.contains('_z')]].apply(zeros_to_ones).apply(np.prod, axis=1)\n    \n    # Lets interact all belt, arm, dumbell and forearm variables\n    \n    df['belt_feat']=df[df.columns[df.columns.to_series().str.contains('_belt')]].apply(zeros_to_ones).apply(np.prod, axis=1)\n    df['arm_feat']=df[df.columns[df.columns.to_series().str.contains('_arm')]].apply(zeros_to_ones).apply(np.prod, axis=1)\n    df['forearm_feat']=df[df.columns[df.columns.to_series().str.contains('_forearm')]].apply(zeros_to_ones).apply(np.prod, axis=1)\n    \n    # Let's interact all magnet, accel and gyros variables\n    \n    df['accel_feat']=df[df.columns[df.columns.to_series().str.contains('accel_')]].apply(zeros_to_ones).apply(np.prod, axis=1)\n    df['magnet_feat']=df[df.columns[df.columns.to_series().str.contains('magnet_')]].apply(zeros_to_ones).apply(np.prod, axis=1)\n    df['gyros_feat']=df[df.columns[df.columns.to_series().str.contains('gyros_')]].apply(zeros_to_ones).apply(np.prod, axis=1)\n    \n    return(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df=feat_eng(train_df)\ntest_df=feat_eng(test_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We could continue to generate more features by interacting newly engineered features, or in new combinations, and this may give<br>\nus some additional model performance however due to time and computation restraints we'll leave it here.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Encoding","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Only 2 encoding processes need to be done. (1) to one hot encode the user and (2) to label encode the outcome.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def Encode_fn(df):\n    users=pd.get_dummies(df['user_name']) #OneHot encode username\n    df=pd.concat([df, users], axis=1).reset_index(drop=True) #Join to modelling df\n    df=df.drop('user_name', axis=1) #Drop original username var\n    return(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df=Encode_fn(train_df)\ntest_df=Encode_fn(test_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Label encode target","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"train_df['classe']=train_df['classe'].astype('category') # Ensure the target is cat\ntrain_df['target']=train_df['classe'].cat.codes # Label encoding\ntrain_df['target']=train_df['target'].astype('category') # Ensure the target is cat\ntrain_df=train_df.drop('classe', axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define features and labels","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"X=train_df.drop('target', axis=1).reset_index(drop=True)\ny=train_df['target']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"efine train and test","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Initial classification testing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Load packages","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis,  QuadraticDiscriminantAnalysis\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, log_loss, precision_score, recall_score, f1_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Select classification algos","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"classifiers = [\n    KNeighborsClassifier(3),\n    SVC(kernel=\"rbf\", C=0.025, probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Log results for performance vis","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\nlog = pd.DataFrame(columns=log_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run algo loop","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"for clf in classifiers:\n    clf.fit(X_train, y_train)\n    name = clf.__class__.__name__\n    \n    print(\"=\"*30)\n    print(name)\n    \n    print('****Results****')\n    train_predictions = clf.predict(X_test)\n    acc = accuracy_score(y_test, train_predictions)\n    \n    # calculate score\n    precision = precision_score(y_test, train_predictions, average = 'macro') \n    recall = recall_score(y_test, train_predictions, average = 'macro') \n    f_score = f1_score(y_test, train_predictions, average = 'macro')\n    \n    \n    print(\"Precision: {:.4%}\".format(precision))\n    print(\"Recall: {:.4%}\".format(recall))\n    print(\"F-score: {:.4%}\".format(recall))\n    print(\"Accuracy: {:.4%}\".format(acc))\n    \n    train_predictions = clf.predict_proba(X_test)\n    ll = log_loss(y_test, train_predictions)\n    print(\"Log Loss: {}\".format(ll))\n    \n    log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols)\n    log = log.append(log_entry)\n    \nprint(\"=\"*30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plot results of algo testing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Accuracy","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Log Loss.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.set_color_codes(\"muted\")\nsns.barplot(x='Log Loss', y='Classifier', data=log, color=\"g\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is clear that random forest does an extremely good job of classifying these. Usually I would opt to tune multiple algos but based<br>\nOn the accuracy of the RF i'll just do some brief tuning.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First, lets consider variable importance","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=500, random_state = 42)\nrf.fit(X_train, y_train);\nfeat_importances = pd.Series(rf.feature_importances_, index=X_train.columns)\nfeat_importances.nlargest(25).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No engineered features seem that important according to the RF, likely because theyre interactions (/derivatives). It would make sense<br>\nto go back and remove these to test the RF accuracy without engineering, but i'll leave that for now.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Parameter tuning<br>\nWe'll tune the RF using a random search grid. We could use a grid search but it is computationally expensive and given that we're<br>\nOn >99% accuracy and only need to predict a data set size of 20, I think we can manage without.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Run randomized search","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Number of trees in random forest","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"n_estimators = [int(x) for x in np.linspace(start = 10, stop = 20, num = 10)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Number of features to consider at every split","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"max_features = ['auto', 'sqrt']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Maximum number of levels in tree","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"max_depth = [int(x) for x in np.linspace(10, 1000, num = 10)]\nmax_depth.append(None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Minimum number of samples required to split a node","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"min_samples_split = [2, 5, 10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Minimum number of samples required at each leaf node","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"min_samples_leaf = [2, 4, 10, 100]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Method of selecting samples for training each tree","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"bootstrap = [True, False]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create the random grid","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"random_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use the random grid to search for best hyperparameters<br>\nRandom search of parameters, using 3 fold cross validation, <br>\nSearch across 100 different combinations, and use all available cores","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_random = RandomizedSearchCV(estimator = rf, \n                               param_distributions = random_grid, \n                               n_iter = 100, cv = 3, verbose=2, \n                               random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(rf_random.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fit the tuned model","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"best_params_rf = rf_random.best_estimator_\nbest_params_rf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Predict test data","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred_rf = best_params_rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Evaluate","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"precision = precision_score(y_test, y_pred_rf, average = 'macro') \nrecall = recall_score(y_test, y_pred_rf, average = 'macro') \nf_score = f1_score(y_test, y_pred_rf, average = 'macro')\n    \n    \nprint(\"Precision: {:.4%}\".format(precision))\nprint(\"Recall: {:.4%}\".format(recall))\nprint(\"F-score: {:.4%}\".format(recall))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final Predictions","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"final_predictions = best_params_rf.predict(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(final_predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Convert Py to Notebook","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"! p2j Exercise classification.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":" ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}