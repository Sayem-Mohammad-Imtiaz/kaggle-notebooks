{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Breast Cancer Analysis - EDA and Machine Learning - Predicting diagnosis"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/data.csv') #reading the dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape #Number of rows and columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The ID isn't useful to predict diagnosis - It'll be removed "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['id','Unnamed: 32'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking counts of diagnosis of tumor"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(11,6))\nsns.set()\nplt.title('Distribution of pokemon types')\nk = sns.countplot(x = 'diagnosis',data=df)\nk.set_xticklabels(k.get_xticklabels(), rotation=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(11,6))\nplt.title('Distribution of radius mean in tumor')\nsns.distplot(df['radius_mean'],color='r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(11,6))\nplt.title('Distribution of area mean in tumor')\nsns.distplot(df['area_mean'],color='g')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's convert the variables M in number 0 and B in number 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert(x):\n    if x == 'M':\n        return 0\n    if x == 'B':\n        return 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['diagnosis'] = df['diagnosis'].apply(lambda x: convert(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's compare predictions with no modified data, normalized data and with PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Independent variables\nX = df.iloc[:,1:].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dependent variables\ny = df.iloc[:,0].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Choosing attributes\npca = PCA(n_components = 3)\n# Normalizing data\nscaler = MinMaxScaler(feature_range = (0, 1))\nXi = scaler.fit_transform(X)\nfit = pca.fit(Xi)\n\n\nprint(\"Variance: %s\" % fit.explained_variance_ratio_)\nprint(np.sum(fit.explained_variance_ratio_))\np = []\nx = []\nfor i in range(1,25):\n    pca = PCA(n_components = i)\n    fit = pca.fit(Xi)\n    x.append(i)\n    p.append(np.sum(fit.explained_variance_ratio_))\nx_pca = pca.transform(Xi)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.grid(True)\nplt.scatter(x,p)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Folds\nnum_folds = 10\nseed = 7\n\n# Number of trees\nnum_trees = 100\n\n# Folds in data\nkfold = KFold(num_folds, True, random_state = seed)\n\n# model\nmodelo = GradientBoostingClassifier(n_estimators = num_trees, random_state = seed)# Cross Validation\nresultado = cross_val_score(modelo, X, y, cv = kfold)\nresultado1 = cross_val_score(modelo, Xi, y, cv = kfold)\nresultado2 = cross_val_score(modelo, x_pca, y, cv = kfold)\n\n# Printing result without modification\nprint(\"Accuracy without modification: %.3f\" % (resultado.mean() * 100))\n# Printing result with normalization\nprint(\"Accuracy with normalization: %.3f\" % (resultado1.mean() * 100))\n# Printing result with PCA\nprint(\"Accuracy with PCA: %.3f\" % (resultado2.mean() * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"# model\nmodelo = XGBClassifier(n_estimators = num_trees, random_state = seed)# Cross Validation\nresultado = cross_val_score(modelo, X, y, cv = kfold)\nresultado1 = cross_val_score(modelo, Xi, y, cv = kfold)\nresultado2 = cross_val_score(modelo, x_pca, y, cv = kfold)\n\n# Printing result without modification\nprint(\"Accuracy without modification: %.3f\" % (resultado.mean() * 100))\n# Printing result with normalization\nprint(\"Accuracy with normalization: %.3f\" % (resultado1.mean() * 100))\n# Printing result with PCA\nprint(\"Accuracy with PCA: %.3f\" % (resultado2.mean() * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"modelo = RandomForestClassifier(n_estimators = num_trees, random_state = seed)# Cross Validation\nresultado = cross_val_score(modelo, X, y, cv = kfold)\nresultado1 = cross_val_score(modelo, Xi, y, cv = kfold)\nresultado2 = cross_val_score(modelo, x_pca, y, cv = kfold)\n\n# Printing result without modification\nprint(\"Accuracy without modification: %.3f\" % (resultado.mean() * 100))\n# Printing result with normalization\nprint(\"Accuracy with normalization: %.3f\" % (resultado1.mean() * 100))\n# Printing result with PCA\nprint(\"Accuracy with PCA: %.3f\" % (resultado2.mean() * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Neural Networks"},{"metadata":{"trusted":true},"cell_type":"code","source":"modelo = MLPClassifier(hidden_layer_sizes=500,max_iter=1000,tol=1e-5,solver='adam')\nresultado = cross_val_score(modelo, X, y, cv = kfold)\nresultado1 = cross_val_score(modelo, Xi, y, cv = kfold)\nresultado2 = cross_val_score(modelo, x_pca, y, cv = kfold)\n\n# Printing result without modification\nprint(\"Accuracy without modification: %.3f\" % (resultado.mean() * 100))\n# Printing result with normalization\nprint(\"Accuracy with normalization: %.3f\" % (resultado1.mean() * 100))\n# Printing result with PCA\nprint(\"Accuracy with PCA: %.3f\" % (resultado2.mean() * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"AdaBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"# model\nmodelo = AdaBoostClassifier(n_estimators = num_trees, random_state = seed)# Cross Validation\nresultado = cross_val_score(modelo, X, y, cv = kfold)\nresultado1 = cross_val_score(modelo, Xi, y, cv = kfold)\nresultado2 = cross_val_score(modelo, x_pca, y, cv = kfold)\n\n# Printing result without modification\nprint(\"Accuracy without modification: %.3f\" % (resultado.mean() * 100))\n# Printing result with normalization\nprint(\"Accuracy with normalization: %.3f\" % (resultado1.mean() * 100))\n# Printing result with PCA\nprint(\"Accuracy with PCA: %.3f\" % (resultado2.mean() * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"modelo = SVC()\nresultado = cross_val_score(modelo, X, y, cv = kfold)\nresultado1 = cross_val_score(modelo, Xi, y, cv = kfold)\nresultado2 = cross_val_score(modelo, x_pca, y, cv = kfold)\n\n# Printing result without modification\nprint(\"Accuracy without modification: %.3f\" % (resultado.mean() * 100))\n# Printing result with normalization\nprint(\"Accuracy with normalization: %.3f\" % (resultado1.mean() * 100))\n# Printing result with PCA\nprint(\"Accuracy with PCA: %.3f\" % (resultado2.mean() * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking accuracy of algorithms, with mean, std dev and a boxplot.\nHigh std dev may indicate overfitting.\nHigh median and mean indicates a good algorithm in this situation. We must considerate time to process the algorithm.\nThe first one evaluate the variables without modification. The second one evaluate the normalized variables.\nThe third with PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparing models\nmodelos = []\nmodelos.append(('LR', LogisticRegression()))\nmodelos.append(('XGBoost', XGBClassifier(n_estimators = num_trees, random_state = seed)))\nmodelos.append(('AdaBoost', AdaBoostClassifier(n_estimators = num_trees, random_state = seed)))\nmodelos.append(('KNN', KNeighborsClassifier()))\nmodelos.append(('GradientBoosting', GradientBoostingClassifier(n_estimators = num_trees, random_state = seed)))\nmodelos.append(('SVM', SVC()))\nmodelos.append(('R.Forest',RandomForestClassifier(n_estimators = num_trees, random_state = seed)))\nmodelos.append(('Neural',MLPClassifier(hidden_layer_sizes=500,max_iter=1000,tol=1e-5,solver='adam')))\n\n# Each model will be evaluated by loop\nresultados = []\nnomes = []\n\nfor nome, modelo in modelos:\n    kfold = KFold(n_splits = num_folds, random_state = seed)\n    cv_results = cross_val_score(modelo, X, y, cv = kfold, scoring = 'accuracy')\n    resultados.append(cv_results)\n    nomes.append(nome)\n    msg = \"%s: %f (%f)\" % (nome, cv_results.mean(), cv_results.std())\n    print(msg)\n\n# Boxplot to compare algorithms\nfig = plt.figure(figsize=(13,6))\nfig.suptitle('Comparing algorithms')\nax = fig.add_subplot(111)\nplt.boxplot(resultados)\nax.set_xticklabels(nomes)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparing models\nmodelos = []\nmodelos.append(('LR', LogisticRegression()))\nmodelos.append(('XGBoost', XGBClassifier(n_estimators = num_trees, random_state = seed)))\nmodelos.append(('AdaBoost', AdaBoostClassifier(n_estimators = num_trees, random_state = seed)))\nmodelos.append(('KNN', KNeighborsClassifier()))\nmodelos.append(('GradientBoosting', GradientBoostingClassifier(n_estimators = num_trees, random_state = seed)))\nmodelos.append(('SVM', SVC()))\nmodelos.append(('R.Forest',RandomForestClassifier(n_estimators = num_trees, random_state = seed)))\nmodelos.append(('Neural',MLPClassifier(hidden_layer_sizes=500,max_iter=1000,tol=1e-5,solver='adam')))\n\n# Each model will be evaluated by loop\nresultados = []\nnomes = []\n\nfor nome, modelo in modelos:\n    kfold = KFold(n_splits = num_folds, random_state = seed)\n    cv_results = cross_val_score(modelo, Xi, y, cv = kfold, scoring = 'accuracy')\n    resultados.append(cv_results)\n    nomes.append(nome)\n    msg = \"%s: %f (%f)\" % (nome, cv_results.mean(), cv_results.std())\n    print(msg)\n\n# Boxplot to compare algorithms\nfig = plt.figure(figsize=(13,6))\nfig.suptitle('Comparing algorithms')\nax = fig.add_subplot(111)\nplt.boxplot(resultados)\nax.set_xticklabels(nomes)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparing models\nmodelos = []\nmodelos.append(('LR', LogisticRegression()))\nmodelos.append(('XGBoost', XGBClassifier(n_estimators = num_trees, random_state = seed)))\nmodelos.append(('AdaBoost', AdaBoostClassifier(n_estimators = num_trees, random_state = seed)))\nmodelos.append(('KNN', KNeighborsClassifier()))\nmodelos.append(('GradientBoosting', GradientBoostingClassifier(n_estimators = num_trees, random_state = seed)))\nmodelos.append(('SVM', SVC()))\nmodelos.append(('R.Forest',RandomForestClassifier(n_estimators = num_trees, random_state = seed)))\nmodelos.append(('Neural',MLPClassifier(hidden_layer_sizes=500,max_iter=1000,tol=1e-5,solver='adam')))\n\n# Each model will be evaluated by loop\nresultados = []\nnomes = []\n\nfor nome, modelo in modelos:\n    kfold = KFold(n_splits = num_folds, random_state = seed)\n    cv_results = cross_val_score(modelo, x_pca, y, cv = kfold, scoring = 'accuracy')\n    resultados.append(cv_results)\n    nomes.append(nome)\n    msg = \"%s: %f (%f)\" % (nome, cv_results.mean(), cv_results.std())\n    print(msg)\n\n# Boxplot to compare algorithms\nfig = plt.figure(figsize=(13,6))\nfig.suptitle('Comparing algorithms')\nax = fig.add_subplot(111)\nplt.boxplot(resultados)\nax.set_xticklabels(nomes)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.16"}},"nbformat":4,"nbformat_minor":1}