{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the libraries\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.offline as pyoff\nimport plotly.graph_objs as go\nimport nltk\nfrom collections import Counter\n\nfrom plotly import graph_objs as go\nfrom sklearn import preprocessing \nfrom sklearn.preprocessing import LabelBinarizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom bs4 import BeautifulSoup\nimport re,string,unicodedata\nfrom keras.preprocessing import text, sequence\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom string import punctuation\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\nfrom bs4 import BeautifulSoup\nfrom tqdm import tqdm\nimport re\nimport nltk\nimport gensim\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import classification_report\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Embedding,LSTM,Dropout, Bidirectional, Conv2D\nfrom keras.callbacks import ReduceLROnPlateau\nimport tensorflow as tf\nimport transformers\nfrom tokenizers import BertWordPieceTokenizer\nfrom keras.layers import LSTM,Dense,Bidirectional,Input\nfrom keras.models import Model\nimport torch\nimport transformers","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_train.csv', encoding='latin-1')\n#test = pd.read_csv('/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_test.csv')\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Basic EDA and Data cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df['Sentiment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Location'].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"location = df['Location'].value_counts().nlargest(n=15)\n\nfig = px.bar(y=location.values,\n       x=location.index,\n       orientation='v',\n       color=location.index,\n       text=location.values,\n       color_discrete_sequence= px.colors.qualitative.Bold)\n\nfig.update_traces(texttemplate='%{text:.2s}', \n                  textposition='outside', \n                  marker_line_color='rgb(8,48,107)', \n                  marker_line_width=1.5, \n                  opacity=0.7)\n\nfig.update_layout(width=1000, \n                  showlegend=False, \n                  xaxis_title=\"Location\",\n                  yaxis_title=\"Count\",\n                  title=\"Top 15 Locations with tweet count\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get all hashtags\n\ndef extract_hash_tags(s):\n    hashes = re.findall(r\"#(\\w+)\", s)\n    return \" \".join(hashes)\ndf['hashtags'] = df['OriginalTweet'].apply(lambda x : extract_hash_tags(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"allHashTags = list(df[(df['hashtags'] != None) & (df['hashtags'] != \"\")]['hashtags'])\nallHashTags = [tag.lower() for tag in allHashTags]\nhash_df = dict(Counter(allHashTags))\ntop_hash_df = pd.DataFrame(list(hash_df.items()),columns = ['word','count']).reset_index(drop=True).sort_values('count',ascending=False)[:20]\ntop_hash_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(x=top_hash_df['word'],y=top_hash_df['count'],\n       orientation='v',\n       color=top_hash_df['word'],\n       text=top_hash_df['count'],\n       color_discrete_sequence= px.colors.qualitative.Bold)\n\nfig.update_traces(texttemplate='%{text:.2s}', \n                  textposition='outside', \n                  marker_line_color='rgb(8,48,107)', \n                  marker_line_width=1.5, \n                  opacity=0.7)\n\nfig.update_layout(width=1000, \n                  showlegend=False, \n                  xaxis_title=\"Word\",\n                  yaxis_title=\"Count\",\n                  title=\"Top #hashtags in Covid19 Tweets\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get all mentions\n\ndef get_mentions(s):\n    mentions = re.findall(\"(?<![@\\w])@(\\w{1,25})\", s)\n    return \" \".join(mentions)\ndf['mentions'] = df['OriginalTweet'].apply(lambda x : get_mentions(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['OriginalTweet'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['mentions'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"allMentions = list(df[(df['mentions'] != None) & (df['mentions'] != \"\")]['mentions'])\nallMentions = [tag.lower() for tag in allMentions]\nmentions_df = dict(Counter(allMentions))\ntop_mentions_df = pd.DataFrame(list(mentions_df.items()),columns = ['word','count']).reset_index(drop=True).sort_values('count',ascending=False)[:20]\ntop_mentions_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(x=top_mentions_df['word'],y=top_mentions_df['count'],\n       orientation='v',\n       color=top_mentions_df['word'],\n       text=top_mentions_df['count'],\n       color_discrete_sequence= px.colors.qualitative.Bold)\n\nfig.update_traces(texttemplate='%{text:.2s}', \n                  textposition='outside', \n                  marker_line_color='rgb(8,48,107)', \n                  marker_line_width=1.5, \n                  opacity=0.7)\n\nfig.update_layout(width=1000, \n                  showlegend=False, \n                  xaxis_title=\"Word\",\n                  yaxis_title=\"Count\",\n                  title=\"Top #hashtags in Covid19 Tweets\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Cleaning\nstop = set(stopwords.words('english'))\n\ndef cleaner(phrase):\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can't\", 'can not', phrase)\n  \n  # general\n    phrase = re.sub(r\"n\\'t\",\" not\", phrase)\n    phrase = re.sub(r\"\\'re'\",\" are\", phrase)\n    phrase = re.sub(r\"\\'s\",\" is\", phrase)\n    phrase = re.sub(r\"\\'ll\",\" will\", phrase)\n    phrase = re.sub(r\"\\'d\",\" would\", phrase)\n    phrase = re.sub(r\"\\'t\",\" not\", phrase)\n    phrase = re.sub(r\"\\'ve\",\" have\", phrase)\n    phrase = re.sub(r\"\\'m\",\" am\", phrase)\n    \n    return phrase\n\ncleaned_title = []\n\nfor sentance in tqdm(df['OriginalTweet'].values):\n    sentance = str(sentance)\n    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n    sentance = cleaner(sentance)\n    sentance = re.sub(r'[?|!|\\'|\"|#|+]', r'', sentance)\n    sentance = re.sub('<.*?>','',sentance)\n    sentance = re.sub(r'@\\w+','',sentance)\n    sentance = re.sub(r'#\\w+','',sentance)\n    sentance = re.sub(r'[0-9]+','',sentance)\n    sentance = re.sub(r'[0-9]+','',sentance)\n    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stop)\n    cleaned_title.append(sentance.strip())\n    \ndf['text'] = cleaned_title\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# WordClouds\n# Text that is displaying a positive sentiment\nplt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.Sentiment == 'Positive'].text))\nplt.imshow(wc , interpolation = 'bilinear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Text that is displaying a negative sentiment\n\nplt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.Sentiment == 'Negative'].text))\nplt.imshow(wc , interpolation = 'bilinear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Text that is displaying a neutral sentiment\n\nplt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.Sentiment == 'Neutral'].text))\nplt.imshow(wc , interpolation = 'bilinear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Continuing with some n-gram analysis\n\ndef basic_clean(text):\n  \"\"\"\n  A simple function to clean up the data. All the words that\n  are not designated as a stop word is then lemmatized after\n  encoding and basic regex parsing are performed.\n  \"\"\"\n  wnl = nltk.stem.WordNetLemmatizer()\n  stopwords = nltk.corpus.stopwords.words('english')\n  text = (unicodedata.normalize('NFKD', text)\n    .encode('ascii', 'ignore')\n    .decode('utf-8', 'ignore')\n    .lower())\n  words = re.sub(r'[^\\w\\s]', '', text).split()\n  return [wnl.lemmatize(word) for word in words if word not in stopwords]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Uni-grams for Tweets\n\nHQ_words = basic_clean(''.join(str(df['text'].tolist())))\nunigram_HQ = (pd.Series(nltk.ngrams(HQ_words, 1)).value_counts())[:20]\nunigram_HQ = pd.DataFrame(unigram_HQ)\nunigram_HQ['idx'] = unigram_HQ.index\nunigram_HQ['idx'] = unigram_HQ.apply(lambda x: '('+x['idx'][0]+')',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data = [\n    go.Bar(\n        x=unigram_HQ['idx'],\n        y=unigram_HQ[0],\n        marker = dict(\n            color = 'Blue'\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 20 uni-grams from Covid-19 Tweets',\n        yaxis_title='Count',\n        xaxis_title='Uni-gram',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bi-grams for Tweets\n\nHQ_words = basic_clean(''.join(str(df[df['Sentiment']=='Negative']['text'].tolist())))\nbigram_HQ = (pd.Series(nltk.ngrams(HQ_words, 2)).value_counts())[:20]\nbigram_HQ = pd.DataFrame(bigram_HQ)\nbigram_HQ['idx'] = bigram_HQ.index\nbigram_HQ['idx'] = bigram_HQ.apply(lambda x: '('+x['idx'][0]+', '+x['idx'][1]+')',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data = [\n    go.Bar(\n        x=bigram_HQ['idx'],\n        y=bigram_HQ[0],\n        marker = dict(\n            color = 'Red'\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 20 bi-grams from Covid 19 Tweets',\n        yaxis_title='Count',\n        xaxis_title='bi-gram',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tri-grams for Tweets\n\nHQ_words = basic_clean(''.join(str(df['text'].tolist())))\ntrigram_HQ = (pd.Series(nltk.ngrams(HQ_words, 3)).value_counts())[:20]\ntrigram_HQ = pd.DataFrame(trigram_HQ)\ntrigram_HQ['idx'] = trigram_HQ.index\ntrigram_HQ['idx'] = trigram_HQ.apply(lambda x: '('+x['idx'][0]+', '+x['idx'][1]+', '+x['idx'][2]+')',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data = [\n    go.Bar(\n        x=trigram_HQ['idx'],\n        y=trigram_HQ[0],\n        marker = dict(\n            color = 'Green'\n        )\n    )\n]\nplot_layout = go.Layout(\n        title='Top 20 Tri-grams from Covid 19 Tweets',\n        yaxis_title='Count',\n        xaxis_title='Tri-gram',\n        plot_bgcolor='rgba(0,0,0,0)'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Model Building"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/covid-19-nlp-text-classification/Corona_NLP_test.csv', encoding='latin-1')\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_title = []\n\nfor sentance in tqdm(test['OriginalTweet'].values):\n    sentance = str(sentance)\n    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n    sentance = cleaner(sentance)\n    sentance = re.sub(r'[?|!|\\'|\"|#|+]', r'', sentance)\n    sentance = re.sub('<.*?>','',sentance)\n    sentance = re.sub(r'@\\w+','',sentance)\n    sentance = re.sub(r'#\\w+','',sentance)\n    sentance = re.sub(r'[0-9]+','',sentance)\n    sentance = re.sub(r'[0-9]+','',sentance)\n    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stop)\n    cleaned_title.append(sentance.strip())\n    \ntest['text'] = cleaned_title\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'].replace('', np.nan, inplace=True)\ndf.dropna(subset=['text'], inplace=True)\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = df.copy()\ntrain.drop(['UserName', 'ScreenName', 'Location', 'TweetAt', 'OriginalTweet', 'hashtags', 'mentions'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def target(label):\n    if label == 'Neutral': \n        return 0\n    if label == 'Positive' or label=='Extremely Positive':\n        return 1\n    else:\n        return -1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['label'] = train['Sentiment'].apply(target)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train['text']\ny = train['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_Train, X_test, y_Train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify = y)\n\nX_train, X_cross, y_train, y_cross = train_test_split(X_Train, y_Train, test_size=0.1, random_state=42, stratify = y_Train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntf_idf=TfidfVectorizer(use_idf=True,ngram_range=(1,2))\n\ntf_idf.fit(X_train)\nTrain_TFIDF = tf_idf.transform(X_train)\nCrossVal_TFIDF = tf_idf.transform(X_cross)\nTest_TFIDF= tf_idf.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\n\nc=[0.0001,0.001,0.01,0.1,1,10,100,1000]\nTrain_AUC_TFIDF = []\nCrossVal_AUC_TFIDF = []\nfor i in c:\n  logreg = LogisticRegression(C=i,penalty='l2')\n  logreg.fit(Train_TFIDF, y_train)\n  Train_y_pred =  logreg.predict_proba(Train_TFIDF)[0:,]\n  Train_AUC_TFIDF.append(roc_auc_score(y_train ,Train_y_pred, multi_class='ovr'))\n  CrossVal_y_pred =  logreg.predict_proba(CrossVal_TFIDF)[0:,]\n  CrossVal_AUC_TFIDF.append(roc_auc_score(y_cross,CrossVal_y_pred, multi_class='ovr'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"C=[]\nfor i in range(len(c)):\n  C.append(np.math.log(c[i]))\n\nplt.plot(C, Train_AUC_TFIDF, label='Train AUC')\nplt.scatter(C, Train_AUC_TFIDF)\nplt.plot(C, CrossVal_AUC_TFIDF, label='CrossVal AUC')\nplt.scatter(C, CrossVal_AUC_TFIDF)\nplt.legend()\nplt.xlabel(\"lambda : hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimal_inverse_lambda=c[CrossVal_AUC_TFIDF.index(max(CrossVal_AUC_TFIDF))]\nprint(pow(optimal_inverse_lambda,-1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Classifier=LogisticRegression(C=optimal_inverse_lambda,penalty='l2')\nClassifier.fit(Train_TFIDF, y_train)\n\nauc_train_tfidf = roc_auc_score(y_train,Classifier.predict_proba(Train_TFIDF)[0:,], multi_class='ovr')\nprint (\"AUC for Train set\", auc_train_tfidf)\n\nauc_test_tfidf = roc_auc_score(y_test,Classifier.predict_proba(Test_TFIDF)[0:,], multi_class='ovr')\nprint (\"AUC for Test set\",auc_test_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = Classifier.predict(Test_TFIDF)\nprint('Confusion Matrix of Test Data')\nTest_mat=confusion_matrix(y_test, y_pred)\nprint (Test_mat)\n\nprint('Accuracy Score on test: ', accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nprint(metrics.classification_report(y_test, y_pred, target_names = ['Negative', 'Neutral', 'Positive']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LSTM","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\nnumpy.random.seed(7)\nfrom keras.layers import SpatialDropout1D\nfrom keras.callbacks import EarlyStopping\nfrom keras.preprocessing import text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_NB_WORDS = 50000\nMAX_SEQUENCE_LENGTH = 300\nEMBEDDING_DIM = 100\ntokenizer = text.Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(X.values)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = tokenizer.texts_to_sequences(X.values)\nx = sequence.pad_sequences(x, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', x.shape)\n\nX_train, X_test, Y_train, Y_test = train_test_split(x,y, test_size=0.3,stratify=y)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\nY_train=np.array(Y_train)\n\nlabel_encoder = LabelEncoder()\ninteger_encoded = label_encoder.fit_transform(Y_train)\nprint(Y_train)\n\nonehot_encoder = OneHotEncoder(sparse=False)\ninteger_encoded = integer_encoded.reshape(len(integer_encoded), 1)\nonehot_encoded = onehot_encoder.fit_transform(integer_encoded)\nprint(onehot_encoded)\nY_train=onehot_encoded","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_test=np.array(Y_test)\n\nlabel_encoder = LabelEncoder()\ninteger_encoded = label_encoder.fit_transform(Y_test)\n#print(Y_test)\n\nonehot_encoder = OneHotEncoder(sparse=False)\ninteger_encoded = integer_encoded.reshape(len(integer_encoded), 1)\nonehot_encoded = onehot_encoder.fit_transform(integer_encoded)\nprint(onehot_encoded)\nY_test=onehot_encoded","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=x.shape[1]))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(3, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nepochs = 10\nbatch_size = 64\n\nhistory = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accr = model.evaluate(X_test,Y_test)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}