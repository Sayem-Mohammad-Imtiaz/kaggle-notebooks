{"cells":[{"metadata":{"id":"Dc8SXFdLzNq3","colab_type":"text"},"cell_type":"markdown","source":"# Breast cancer Prediction\n# -[Rishit Dagli](rishitdagli.ml)"},{"metadata":{"id":"R41_EiK410L8","colab_type":"text"},"cell_type":"markdown","source":"![](https://drive.google.com/uc?id=16c6UtqGFDrJNordq9lIursSR0Ks5W8k6)"},{"metadata":{"id":"iGyReTu1z8Rf","colab_type":"text"},"cell_type":"markdown","source":"I have used the Wisconsin Breast cancer dataset from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml) availaible [here](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)).\n<br>\nI have used Artificial Neural Networks for this problem and found out the best hyper parameters using cross validation.<br>For more details read my research paper [here](https://iarjset.com/papers/machine-learning-as-a-decision-aid-for-breast-cancer-diagnosis/).\n<br><br>\n![](https://drive.google.com/uc?id=1ETVCulfECkSBOcZXtXLnaDUIjnpoZMu5)\n<br>\n<br>\n![](https://drive.google.com/uc?id=1mIKCJ6wyvSMrx-oa4IFRGG5FUR8pPOKN)\n\n---\n\n\n<font color=\"red\">**Data Set Information:**</font>\n\nFeatures are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. A few of the images can be found [here](www.cs.wisc.edu/~street/images/)\n\n\n\n---\n\n\n\n<font color=\"red\">**Attribute Information:**</font>\n\n1) ID number<br>\n2) Diagnosis (M = malignant, B = benign)<br>\n3-32)<br>\n<br>\n<font color=\"light green\">*Ten real-valued features are computed for each cell nucleus:*</font><br>\n\na) radius (mean of distances from center to points on the perimeter)<br>\nb) texture (standard deviation of gray-scale values)<br>\nc) perimeter<br>\nd) area<br>\ne) smoothness (local variation in radius lengths)<br>\nf) compactness ($\\frac{perimeter^2}{area} - 1.0$)<br>\ng) concavity (severity of concave portions of the contour)<br>\nh) concave points (number of concave portions of the contour)<br>\ni) symmetry<br>\nj) fractal dimension (\"coastline approximation\" - 1)<br>\n"},{"metadata":{"id":"6wHSACTGReSw","colab_type":"text"},"cell_type":"markdown","source":"### Import required libraries"},{"metadata":{"id":"jtTrpsZsq1ZK","colab_type":"text"},"cell_type":"markdown","source":"We use `matplotlib` and `seaborn` to create some wonderful visualizations of our data.<br>\n`pandas` to read our data and know some insights about the data, efficiently. With `pandas` by our side we can do many things with just simple functions which we will take a look at in the later part.<br>\n`sklearn` to<br>\n- Select the model with best hyper parameters\n- Encode the labels i.e. M and B\n- Print a confusion matrix with test data results\n- Make a train / test split easily\n- Scale values\n<br><br>\n`tensorflow` and `keras` to create our model (ANN) and make some plots of it"},{"metadata":{"id":"hGO8dSVa9LFq","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.utils.vis_utils import plot_model\nfrom IPython.display import SVG\nfrom keras.utils import model_to_dot\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\nimport seaborn as sns\n\ndataset = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"3VoCB7FeSKeU","colab_type":"text"},"cell_type":"markdown","source":"### Analyze the dataset"},{"metadata":{"id":"pLsan0CesTk8","colab_type":"text"},"cell_type":"markdown","source":"We will now analyze our data :\n\n\n1.Print the features of dataset which are also mentioned above<br>\n2.View how many samples and missing values there are for each feature and display them accordingly<br>\nWe here see the missing samples and values:\n```\nRangeIndex: 568 entries, 0 to 567\nData columns (total 32 columns):\n842302      568 non-null int64\nM           568 non-null object\n17.99       568 non-null float64\n10.38       568 non-null float64\n122.8       568 non-null float64\n1001        568 non-null float64\n0.1184      568 non-null float64\n0.2776      568 non-null float64\n...\n...\n0.4601      568 non-null float64\n0.1189      568 non-null float64\ndtypes: float64(30), int64(1), object(1)\nmemory usage: 142.1+ KB\nNone\n```\n\n3.View numerical features of data set we will majorly focus on `mean`, `count`, `std`, `min value`, `max value`, `upper quartile`, `inter quartile` and `lower quartile`<br>\nFor this all we need to do is `dataset.describe()` So this justifies our use of `pandas` library.<br>\n4.We will now take a look at the label features specially `count`, `unique`, `top` and `frequency`. The `count` parameter just tells us the number of entries, the `unique` parameter is important.\nHere we receive -\n```\nunique: 2\n```\nWhich tells us to perform2 class classification.<br>\nThe `top` parameter is often used to check biases in the data set itself.\n\n\n"},{"metadata":{"id":"M4BhDMzs-b8d","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def analyze(data):\n    \n  # View features in data set\n  print(\"Dataset Features\")\n  print(data.columns.values)\n  print(\"=\" * 30)\n    \n  # View How many samples and how many missing values for each feature\n  print(\"Dataset Features Details\")\n  print(data.info())\n  print(\"=\" * 30)\n    \n  # view distribution of numerical features across the data set\n  print(\"Dataset Numerical Features\")\n  print(data.describe())\n  print(\"=\" * 30)\n    \n  # view distribution of categorical features across the data set\n  print(\"Dataset Categorical Features\")\n  print(data.describe(include=['O']))\n  print(\"=\" * 30)","execution_count":null,"outputs":[]},{"metadata":{"id":"cpkhmW80k9fC","colab_type":"code","outputId":"a4f41a96-efd4-4c67-ea6d-2de4bd84c5f8","colab":{"base_uri":"https://localhost:8080/","height":1000},"trusted":true},"cell_type":"code","source":"analyze(dataset)","execution_count":null,"outputs":[]},{"metadata":{"id":"3lzH5OR2SQgq","colab_type":"text"},"cell_type":"markdown","source":"### Make a feature pairplot"},{"metadata":{"id":"T4iFH1Azvl9Z","colab_type":"text"},"cell_type":"markdown","source":"We will now make a feature wise pairplot meaning we will plot labels $x_1$, $x_2$, $...$ and or label $y$ with each other. Where $x$ and $y$ have their usual meaning. We will use `seaborn` to help us with this. A pair plot allows us to see both distribution of single variables and relationships between two variables . Pair plots are a great method to identify trends for follow-up analysis. So this again becomes an important step for us."},{"metadata":{"id":"ypk9cvCclAbt","colab_type":"code","outputId":"08f75052-ba24-479c-eb50-32b5884a4efa","colab":{"base_uri":"https://localhost:8080/","height":1000},"trusted":true},"cell_type":"code","source":"sns.pairplot(dataset, hue=\"diagnosis\", size= 2.5)","execution_count":null,"outputs":[]},{"metadata":{"id":"orvYoDDtWw-X","colab_type":"text"},"cell_type":"markdown","source":"### Seperate the the features and laabels"},{"metadata":{"id":"MhvPW5WvwY3p","colab_type":"text"},"cell_type":"markdown","source":"This is just a simple code which stores $X$ or the features and $y$ or the labels in different variables"},{"metadata":{"id":"rwEAxRkg-chd","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"X = dataset.iloc[:,2:32] \ny = dataset.iloc[:,1] ","execution_count":null,"outputs":[]},{"metadata":{"id":"4sJPywrqXN-9","colab_type":"text"},"cell_type":"markdown","source":"### Encode the **labels** to 1, 0"},{"metadata":{"id":"9ggch_1Vwj7L","colab_type":"text"},"cell_type":"markdown","source":"We had our labels as `M` and `B` depicting malignant and benign respectively. As these are `strings` or `char` as you might say and we are only concerned with numbers so we encode such that <br>\n\n\n*   All `M = 1`\n*   All `B = 0`\n\nFor this we use the `LabelEncoder` class.\n"},{"metadata":{"id":"0RFKAVdM-xRN","colab_type":"code","outputId":"0384465a-4af7-42e8-b5e3-d74f6c681292","colab":{"base_uri":"https://localhost:8080/","height":293},"trusted":true},"cell_type":"code","source":"print(\"Earlier: \")\nprint(y[100:110])\n\nlabelencoder_Y = LabelEncoder()\ny = labelencoder_Y.fit_transform(y)\n\nprint()\nprint(\"After: \")\nprint(y[100:110])","execution_count":null,"outputs":[]},{"metadata":{"id":"blomafC1YOVW","colab_type":"text"},"cell_type":"markdown","source":"### Make a 80/ 20 train, test split"},{"metadata":{"id":"RX2Uv7fnxZHq","colab_type":"text"},"cell_type":"markdown","source":"Making a train test split is important for any AI problem without which we do not know how our model would perform to unseen values and also not overfit the data"},{"metadata":{"id":"VRkQTuqI-2dY","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"id":"T1wXC0D8Z9XJ","colab_type":"text"},"cell_type":"markdown","source":"Scaling the values is not actually compulsory but I would recommend one to do it for a faster convergence, so we use `sklearn` to help us in this"},{"metadata":{"id":"QB_uu3Ag-7NB","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"# Scale values from faster convergence\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"yfGtr7hWYYjm","colab_type":"text"},"cell_type":"markdown","source":"### Build a classifier"},{"metadata":{"id":"vm6MFly7x5j0","colab_type":"text"},"cell_type":"markdown","source":"We finally build a `tensorflow`, `keras` classifier"},{"metadata":{"id":"8no_R0NFhvJH","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def build_classifier(optimizer):\n  classifier = Sequential()\n  classifier.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu', input_dim = 30))\n  classifier.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu'))\n  classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n  classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n  classifier.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n  return classifier","execution_count":null,"outputs":[]},{"metadata":{"id":"dGVFqhMmicmY","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"classifier = KerasClassifier(build_fn = build_classifier)","execution_count":null,"outputs":[]},{"metadata":{"id":"N4XsUj5tYiZT","colab_type":"text"},"cell_type":"markdown","source":"### Experiment with various hyper parameters"},{"metadata":{"id":"-8xIDP16yEwq","colab_type":"text"},"cell_type":"markdown","source":"Before tuning our hyper parameters we surely want to test the data with various of them and choose the best one so define few options for the model"},{"metadata":{"id":"dRtF-PTsicjC","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"parameters = {'batch_size': [1, 5],\n               'epochs': [100, 120],\n               'optimizer': ['adam', 'rmsprop']}","execution_count":null,"outputs":[]},{"metadata":{"id":"BFWhIGU7Ynk2","colab_type":"text"},"cell_type":"markdown","source":"### Use Cross Vaildation to obtain best model"},{"metadata":{"id":"olm8Xoo6yTzD","colab_type":"text"},"cell_type":"markdown","source":"Cross Vaalidation is a wonderful technique which often comes to our rescue while selecting the best model so we use a 10 fold CV here.<br>\nWe could also have used `AIC`, `BIC` or even Mallows $C_p$ if the CV does not give us a good result but that's not the case here"},{"metadata":{"id":"3XcH3WHoicee","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"# Cross validation\ngrid_search = GridSearchCV(estimator = classifier,\n                            param_grid = parameters,\n                            scoring = 'accuracy',\n                            cv = 10)","execution_count":null,"outputs":[]},{"metadata":{"id":"ZblOY8NbZ0Te","colab_type":"text"},"cell_type":"markdown","source":"Search for the best model in the complete matrix"},{"metadata":{"id":"Qz8jYl0XipLc","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"# Get best model\n# Note: this may take some time\ngrid_search = grid_search.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"I9jYJc4NYxaW","colab_type":"text"},"cell_type":"markdown","source":"### Finally build model according to above obtained results"},{"metadata":{"id":"BxFOmd25y4MD","colab_type":"text"},"cell_type":"markdown","source":"In the above step we already obtained the best model for this problem so now we are almost done all we need to do is build a classifier according to obtained results"},{"metadata":{"id":"OGTLF9x3jt7D","colab_type":"code","outputId":"2b6237ee-b9b4-4270-f4f5-a7a8cb38f28e","colab":{"base_uri":"https://localhost:8080/","height":74},"trusted":true},"cell_type":"code","source":"classifier = Sequential()","execution_count":null,"outputs":[]},{"metadata":{"id":"fm6o2x6ZzKGx","colab_type":"text"},"cell_type":"markdown","source":"So we make an ANN now with\n\n* Input Layer - **16** neurons and `ReLu` activator\n* Hidden Layer 1 - **8** neurons and `ReLu` activator\n* Hidden Layer 2 - **6** neurons and `ReLu` activator\n* Output Layer - **1** neuron (ok that was obvious !) and `sigmoid` activator\n\n"},{"metadata":{"id":"YYCO96O9j5Ps","colab_type":"code","outputId":"0da4339f-4212-470c-8f38-18aa14f9adbe","colab":{"base_uri":"https://localhost:8080/","height":110},"trusted":true},"cell_type":"code","source":"# Make the best classifier as we received earlier\nclassifier.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu', input_dim = 30))\nclassifier.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"id":"mIm0022qZ5DS","colab_type":"text"},"cell_type":"markdown","source":"Complete the classifier"},{"metadata":{"id":"AhKE4Tbsz87c","colab_type":"text"},"cell_type":"markdown","source":"We use `BCE` or binary cross entropy which is suited best to sigmoid.\n<br> $BCE=CE_1+CE_2$\n<br> $BCE=-y log \\hat y - (1-y) log (1 - \\hat y)$"},{"metadata":{"id":"y9YA5Qm3j8Hb","colab_type":"code","outputId":"a1fec766-28dd-4cea-b713-a2771a5de6e9","colab":{"base_uri":"https://localhost:8080/","height":166},"trusted":true},"cell_type":"code","source":"classifier.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"id":"OOe4ysiCY5sC","colab_type":"text"},"cell_type":"markdown","source":"### Fit the classifier to the data"},{"metadata":{"id":"N8XR5DAT0Z1E","colab_type":"text"},"cell_type":"markdown","source":"Finally fit the classifier to training data"},{"metadata":{"id":"Nvoe9OXCkDk6","colab_type":"code","outputId":"d6a594c5-1941-4488-ccca-e3484e705803","colab":{"base_uri":"https://localhost:8080/","height":1000},"trusted":true},"cell_type":"code","source":"classifier.fit(X_train, y_train, batch_size = 1, epochs = 100, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"9qfI7K8zY9Tu","colab_type":"text"},"cell_type":"markdown","source":"### Evaluate model"},{"metadata":{"id":"dF6VDasU0f67","colab_type":"text"},"cell_type":"markdown","source":"We cannot complete any AI algorithm or model without assesing it so we now measure the accuracy of our model.<br>\nThis happens to be a classification model so we can simply use the accuracy formula:<br>\n$Accuracy=\\frac{True Positive + True Negative}{Total}$"},{"metadata":{"id":"J4nrqUo4ZBT5","colab_type":"text"},"cell_type":"markdown","source":"First make a y predictions list for all entries in x test list as we have the probabilities and not 1 or 0 corresponding to `M` and `B`\n<br>\nWe now receive the probabilities, This is the pseudo code used:\n* if the `prob >= 0.5`\n* we classify it as `1`\n* else `0`\n"},{"metadata":{"id":"R6asup-o5kQg","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"y_pred = classifier.predict(X_test)\n# If probab is >= 0.5 classify as 1 or 0\ny_pred = [ 1 if y>=0.5 else 0 for y in y_pred ]","execution_count":null,"outputs":[]},{"metadata":{"id":"dzsMwIeWZYfj","colab_type":"text"},"cell_type":"markdown","source":"Now we build the confuson matrix for easy interpretation of model accuracy. A confusion matrix is very helpful in interpreting our model results in this manner.<br><br>\n![](https://drive.google.com/uc?id=1SaflBpLkDz753uijkjzGK70KpMCmp520)"},{"metadata":{"colab_type":"code","outputId":"d05f0a53-5d79-4254-bee8-fb979b005dcf","id":"fyomYz6K5XCl","colab":{"base_uri":"https://localhost:8080/","height":54},"trusted":true},"cell_type":"code","source":"# Finally use scikit-learn to build a confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"id":"kPBQClADSKzi","colab_type":"text"},"cell_type":"markdown","source":"Create a heat map of the confusion matrix"},{"metadata":{"id":"4Zt4FjfqR3wT","colab_type":"code","outputId":"eff59b01-d496-4759-bb28-b6234f6e9cd8","colab":{"base_uri":"https://localhost:8080/","height":283},"trusted":true},"cell_type":"code","source":"sns.heatmap(cm,annot=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"MYiEYr2xZgde","colab_type":"text"},"cell_type":"markdown","source":"Now print out the accuracy<br> $Accuracy=\\frac{True Positive + True Negative}{Total}$"},{"metadata":{"id":"GT1NqSzA5G7o","colab_type":"code","outputId":"f60c34a1-fa2a-4746-a22c-a3f6ee1a9fd5","colab":{"base_uri":"https://localhost:8080/","height":35},"trusted":true},"cell_type":"code","source":"# (True positive + True Negative)/Total\naccuracy = (cm[0][0]+cm[1][1])/(cm[0][0]+cm[0][1]+cm[1][0]+cm[1][1])\nprint(\"Accuracy: \"+ str(accuracy*100)+\"%\")","execution_count":null,"outputs":[]},{"metadata":{"id":"5gSiFXApSXxD","colab_type":"text"},"cell_type":"markdown","source":"Create a visualization of our ANN to see its layers at a glance"},{"metadata":{"id":"_stG8kH92ZAD","colab_type":"code","outputId":"62673108-c8d1-46b1-8fe8-d912116e7a61","colab":{"base_uri":"https://localhost:8080/","height":533},"trusted":true},"cell_type":"code","source":"plot_model(classifier, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"vKA94mxtMqqh","colab_type":"code","colab":{}},"cell_type":"markdown","source":"So, now you can see a png file of the model architecture created as `model_plot.png`"}],"metadata":{"colab":{"name":"Breast Cancer TF GCI.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":1}