{"cells":[{"metadata":{},"cell_type":"markdown","source":"* **Purpose**: Explore topic modelling (LDA) for COVID Tweets\n* **Ideal outcome**: topic modelling to cluster the many thousands of tweets into a few buckets, allowing me to infer topic focus and political bias of the accounts that created the tweets.\n* **Method**: forked \"Topic Modelling (LDA) on Elon Tweets\" and run on English language tweets in USA with a few tweeks. Saved the tweet clusters and manually inspected them.\n* **Actual outcome**: Even though the measures such as \"Intertopic Distance Map\" showed clear separation of topics, I did not discover obverious the clusters when manually inspecting the clustered tweets. \n* **Analysis**: LDA in its current form take words individually and do not factor in the context. Recent NLP technology such as BERT is not integrated into LDA yet. Thus, for novice ML user like me, it may not be easy to extract topics in nuanced discussions. To put this in perspective, using LDA, it would probably be easy to distinguish COVID tweets from govenment election or black-lives-matter protests. But it would be harder to extract topics within the COVID such as open up economy vs vaccine, as both may use similar vocabulary. Also, it would be hard to extract topics related to different news events such as \"New York COVID cases spike\", \"Florida open economy plan delayed\".","execution_count":null},{"metadata":{"_cell_guid":"5a7c334d-3afa-4508-874f-3366da0a9c2c","_uuid":"1139f2793448fc6a647e9a0c8e8679963d9f206d","trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly as py\nimport plotly.graph_objs as go\nimport gensim\nfrom gensim import corpora, models, similarities\nimport logging\nimport tempfile\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nfrom collections import OrderedDict\nimport seaborn as sns\nimport pyLDAvis.gensim\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ninit_notebook_mode(connected=True) #do not miss this line\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7cdec987-42d7-495d-8aed-d904f430707a","_uuid":"2ab794b9bdd2400c1eb349493361f325fef51eee","trusted":true},"cell_type":"code","source":"datafile = '/kaggle/input/coronavirus-covid19-tweets-late-april/2020-04-16 Coronavirus Tweets.CSV'","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"52abb44e-881a-4fa5-aaad-cca178c148b4","_uuid":"3247c5f08df034b7afe4be36e52a99baa625a5ba","trusted":true},"cell_type":"code","source":"# read tweets, filter to USA and English, sample to 1000\n\ntweets1 = pd.read_csv(datafile, encoding='latin1')\ntweets1 =tweets1[(tweets1.country_code == \"US\") & (tweets1.lang == \"en\")].reset_index(drop = True)\ntweets2 = tweets1[['text']].copy()\ntweets2 = tweets2.rename(columns={'text': 'Tweet'})\ntweets = tweets2.sample(1000).reset_index()\n\nprint(\"Number of tweets: \",len(tweets['Tweet']))\ntweets.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b06b4ca0-f1ab-45a1-9550-1492a30c3c74","_uuid":"ffb58f64bee55b852c07d0bc87be430d1258d976","trusted":true},"cell_type":"code","source":"# Preparing a corpus for analysis and checking first 10 entries\n\ncorpus=[]\na=[]\nfor i in range(len(tweets['Tweet'])):\n        a=tweets['Tweet'][i]\n        corpus.append(a)\n        \ncorpus[0:10]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d117ab5e-3395-4430-8dad-0750a520c313","_uuid":"e6f1dd9faefafd8045c94fc71a033c201c5443ff","trusted":true},"cell_type":"code","source":"TEMP_FOLDER = tempfile.gettempdir()\nprint('Folder \"{}\" will be used to save temporary dictionary and corpus.'.format(TEMP_FOLDER))\n\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"068cfe59-dbc2-4481-a20f-6cd321cbc4bb","_uuid":"df0a72a7a17efcf0da96a51753855ec2039cffb8","trusted":true},"cell_type":"code","source":"# removing common words and tokenizing. Also removing Covid hash tags, as it appeared in all LDA topics equally. \n\nlist1 = ['RT','rt','#coronavirus','#covid19','#covid_19']\nstoplist = stopwords.words('english') + list(punctuation) + list1\n\ntexts = [[word for word in str(document).lower().split() if word not in stoplist] for document in corpus]\n\ndictionary = corpora.Dictionary(texts)\ndictionary.save(os.path.join(TEMP_FOLDER, 'elon.dict'))  # store the dictionary, for future reference","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b80d4a62-6126-48e5-aba8-8de557c5d00d","_uuid":"040914ec58061f9f66a39df81c8c1ae4cb9dbeb5","trusted":true},"cell_type":"code","source":"corpus = [dictionary.doc2bow(text) for text in texts]\ncorpora.MmCorpus.serialize(os.path.join(TEMP_FOLDER, 'elon.mm'), corpus)  # store to disk, for later use","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"30731758-2dd0-482c-8ad8-995ebf37b686","_uuid":"547b528b1af3eac74eed647c6013c67304a44f42"},"cell_type":"markdown","source":"In the previous cells, I created a corpus of documents represented as a stream of vectors. To continue, lets use that corpus, with the help of Gensim.","execution_count":null},{"metadata":{"_cell_guid":"8c2fae3e-3c57-4a1b-b302-7f9bfe2fb5a0","_uuid":"516974a573c993c655cec2ea8ccda4a3e4e3d899"},"cell_type":"markdown","source":"### Creating a transformation","execution_count":null},{"metadata":{"_cell_guid":"c8cc6eba-dfde-4078-93aa-49ec66b79483","_uuid":"95d9a030b86689a097ff84cf988ce6bfee42b6d6"},"cell_type":"markdown","source":"\nThe transformations are standard Python objects, typically initialized by means of a training corpus:\n\nDifferent transformations may require different initialization parameters; in case of TfIdf, the “training” consists simply of\ngoing through the supplied corpus once and computing document frequencies of all its features.\nTraining other models, such as Latent Semantic Analysis or Latent Dirichlet Allocation, is much more involved and,\nconsequently, takes much more time.","execution_count":null},{"metadata":{"_cell_guid":"a7966606-dc6a-44a8-8fe5-d1a79929e203","_uuid":"91cd194bf43c84b564c693c6e0ea109d28724abc","trusted":true},"cell_type":"code","source":"tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"47a7ad12-9917-4a74-b190-b6f039e87baf","_uuid":"902cd861d68967c9d6d78f7d074fe6e4d31e4547"},"cell_type":"markdown","source":"### Note\nTransformations always convert between two specific vector spaces. The same vector space (= the same set of feature ids) must be used for training as well as for subsequent vector transformations. Failure to use the same input feature space, such as applying a different string preprocessing, using different feature ids, or using bag-of-words input vectors where TfIdf vectors are expected, will result in feature mismatch during transformation calls and consequently in either garbage output and/or runtime exceptions.","execution_count":null},{"metadata":{"_cell_guid":"31035482-743c-4143-a016-af6e7aafa8b2","_uuid":"12084cb081ffae38fd076b271f748faa6e3ff5af"},"cell_type":"markdown","source":"From now on, tfidf is treated as a read-only object that can be used to apply a transformation to a whole corpus:","execution_count":null},{"metadata":{"_cell_guid":"8d3211bd-a12d-4518-b822-eb92df9a564d","_uuid":"6ccc9986f35d02916c62139410d0cbc6455e3149","trusted":true},"cell_type":"code","source":"corpus_tfidf = tfidf[corpus]  # step 2 -- use the model to transform vectors","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6096b0ef-bb2b-40d1-aac4-f538dae0f1e3","_uuid":"39f769bd5e8fea11c952a7c33bf32b329618f837"},"cell_type":"markdown","source":"### LDA:\nhttps://en.wikipedia.org/wiki/Latent_Dirichlet_allocation","execution_count":null},{"metadata":{"_cell_guid":"7c793905-48bb-4526-9e78-1d5d1a9ed70b","_uuid":"8aca0810b66308f947046dd9689409fc56067b1a"},"cell_type":"markdown","source":"Latent Dirichlet Allocation, LDA is yet another transformation from bag-of-words counts into a topic space of lower dimensionality. LDA is a probabilistic extension of LSA (also called multinomial PCA), so LDA’s topics can be interpreted as probability distributions over words. These distributions are, just like with LSA, inferred automatically from a training corpus. Documents are in turn interpreted as a (soft) mixture of these topics (again, just like with LSA).","execution_count":null},{"metadata":{"_cell_guid":"8fb9df90-fdc2-491b-a2aa-02c0eca4c29a","_uuid":"db447043098a7546237bbaf4c7d1145103209160","trusted":true},"cell_type":"code","source":"total_topics = 8\n\nlda = models.LdaModel(corpus, id2word=dictionary, num_topics=total_topics)\ncorpus_lda = lda[corpus_tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dfbb2723-ec7e-490e-8abd-fb646335bc55","_uuid":"53e59fb13b5f55abb2e8158cdd0b244bcb055b3f","trusted":true},"cell_type":"code","source":"#Show first n important word in the topics:\nlda.show_topics(total_topics,10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for topic_id in range(lda.num_topics):\n    topk = lda.show_topic(topic_id, 30)\n    topk_words = [ w for w, _ in topk ]\n    print('{}: {}'.format(topic_id, ' '.join(topk_words)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Add top topic and score for each document\n\ntweets['topic1'] = np.nan\ntweets['score1'] = np.nan\nfor i in range(len(tweets['Tweet'])):\n    topic = lda.get_document_topics(lda.id2word.doc2bow(tweets['Tweet'][i].split()))[0]\n    tweets['topic1'][i] = topic[0]\n    tweets['score1'][i] = topic[1]\n\n# Save to file to be downloaded\n\ndfTopic = tweets.sort_values(by=['score1'], ascending = False).head(500)\ndfTopic = dfTopic.sort_values(by=['topic1'])\ndfTopic.to_csv('/kaggle/working/topics.csv',index=False)\ndfTopic.head(50)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5e31a267-03c5-4a4e-8a73-c72f7abd4644","_uuid":"25d66c5df87a0c365dcd0725e779dc8724ab63af","scrolled":true,"trusted":true},"cell_type":"code","source":"data_lda = {i: OrderedDict(lda.show_topic(i,10)) for i in range(total_topics)}","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"409fed91-48dd-4355-9587-a6299b63f181","_uuid":"edeb3eedc9a10cc135c7bbef50fda042f330482d","trusted":true},"cell_type":"code","source":"df_lda = pd.DataFrame(data_lda)\ndf_lda = df_lda.fillna(0).T\nprint(df_lda.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6410b413-a0d4-49be-a2c4-6844cfc8cb0c","_uuid":"f81dd0ed3ef908b520c2416cf626473a4dd8acd6","trusted":true},"cell_type":"code","source":"df_lda","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"19a51407-5488-44d3-bdd2-9a0ec19ecf35","_uuid":"5206e6ec92bc2678c20b427efd788c0be67adcbd","trusted":true},"cell_type":"code","source":"g=sns.clustermap(df_lda.corr(), center=0, standard_scale=1, cmap=\"RdBu\", metric='cosine', linewidths=.75, figsize=(15, 15))\nplt.setp(g.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\nplt.show()\n#plt.setp(ax_heatmap.get_yticklabels(), rotation=0)  # For y axis","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bc653426-6a63-4b41-887d-92d270dc1168","_uuid":"415d0d14427e94f9900776f3126d5bf8c2391beb","trusted":true},"cell_type":"code","source":"pyLDAvis.enable_notebook()\npanel = pyLDAvis.gensim.prepare(lda, corpus_lda, dictionary, mds='tsne')\npanel","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}