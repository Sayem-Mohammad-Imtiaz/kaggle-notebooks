{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"I have lived and worked near Heathrow airport for most of my life. I was born within a mile of the main runway on the day that QE2 officially opened terminal 1. I lived within half a mile of the main runway for 6 years and worked at the airport for 18 years. I have mild- moderate COPD. A lung disease.... I have never EVER smoked. However I have wondered why I have COPD. Did my lung development gets stifled by asbestos or possibly local air pollution contribute. I wonder how many people who live and work near Airports have lung diseases?  \n\nMy father did work with Asbestos during the 1960's and 1970's and possibly carried fibres in his clothing - which he wore home. But air pollution is a possibility . Thus I am starting this analysis of sites around Heathrow Airport and drawer on other sites to compare pollution away from the airport. Is pollution markedly different near the Airport? and  did pollution drop during Covid ?\n\n\nThis is quite a messy data analysis worksheet. My methodolgy is not set in stone and I am trying out lots of different ideas here - presenting the information in different ways. \n\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import date\nfrom datetime import time\nfrom datetime import datetime as dt\nfrom dateutil.parser import parse\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-30T09:42:46.678789Z","iopub.execute_input":"2021-06-30T09:42:46.679302Z","iopub.status.idle":"2021-06-30T09:42:47.665505Z","shell.execute_reply.started":"2021-06-30T09:42:46.679195Z","shell.execute_reply":"2021-06-30T09:42:47.664465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I tried adding the New Forest Hourly data as an afterthought . However it would not work  with the other data . Removed\n","metadata":{}},{"cell_type":"code","source":"keats_hourly = pd.read_csv('/kaggle/input/d/justinnailard/hillingdon-pollution-data-20182020/KEATS WAY POLLUTION HOURLY DATA.csv')\nhillingdon_data = pd.read_csv('/kaggle/input/d/justinnailard/hillingdon-pollution-data-20182020/hillingdon pollution data 2018-2020.csv')\nharlington_hourly = pd.read_csv('/kaggle/input/d/justinnailard/hillingdon-pollution-data-20182020/HARLINGTON POLLUTION HOURLY DATA.csv')\nharlington_daily = pd.read_csv('/kaggle/input/d/justinnailard/hillingdon-pollution-data-20182020/HARLINGTON DAILY AVERAGE POLLUTION DATA.csv')\nkeats_daily = pd.read_csv('/kaggle/input/d/justinnailard/hillingdon-pollution-data-20182020/KEATS WAY DAILY MEAN POLLUTION DATA.csv')\nreading_daily = pd.read_csv('/kaggle/input/d/justinnailard/hillingdon-pollution-data-20182020/READING NEW TOWN DAILY MEAN POLLTION.csv')\n#newforest_hourly = pd.read_csv('/kaggle/input/d/justinnailard/hillingdon-pollution-data-20182020/NEW FOREST HOURLY.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:42:47.666998Z","iopub.execute_input":"2021-06-30T09:42:47.667334Z","iopub.status.idle":"2021-06-30T09:42:47.959215Z","shell.execute_reply.started":"2021-06-30T09:42:47.667272Z","shell.execute_reply":"2021-06-30T09:42:47.958211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keats_hourly.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:42:47.96111Z","iopub.execute_input":"2021-06-30T09:42:47.961425Z","iopub.status.idle":"2021-06-30T09:42:47.990071Z","shell.execute_reply.started":"2021-06-30T09:42:47.961395Z","shell.execute_reply":"2021-06-30T09:42:47.988937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hillingdon_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:42:47.99156Z","iopub.execute_input":"2021-06-30T09:42:47.991968Z","iopub.status.idle":"2021-06-30T09:42:48.005838Z","shell.execute_reply.started":"2021-06-30T09:42:47.991936Z","shell.execute_reply":"2021-06-30T09:42:48.00479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"harlington_hourly.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:42:48.007253Z","iopub.execute_input":"2021-06-30T09:42:48.007621Z","iopub.status.idle":"2021-06-30T09:42:48.026496Z","shell.execute_reply.started":"2021-06-30T09:42:48.007579Z","shell.execute_reply":"2021-06-30T09:42:48.025325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"harlington_daily.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:42:48.028428Z","iopub.execute_input":"2021-06-30T09:42:48.028788Z","iopub.status.idle":"2021-06-30T09:42:48.052213Z","shell.execute_reply.started":"2021-06-30T09:42:48.028753Z","shell.execute_reply":"2021-06-30T09:42:48.051031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keats_daily.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:42:48.053752Z","iopub.execute_input":"2021-06-30T09:42:48.054057Z","iopub.status.idle":"2021-06-30T09:42:48.073155Z","shell.execute_reply.started":"2021-06-30T09:42:48.054028Z","shell.execute_reply":"2021-06-30T09:42:48.072204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reading_daily.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:42:48.075018Z","iopub.execute_input":"2021-06-30T09:42:48.075395Z","iopub.status.idle":"2021-06-30T09:42:48.087885Z","shell.execute_reply.started":"2021-06-30T09:42:48.075364Z","shell.execute_reply":"2021-06-30T09:42:48.087113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Will add colum head location so it's clear\n","metadata":{}},{"cell_type":"code","source":"keats_hourly['location']='keatshourly'\nhillingdon_data['location']= 'hillingdonhourly'\nharlington_hourly['location']= 'harlingtonhourly'\nharlington_daily['location']='harlingtondaily'\nkeats_daily['location']='keatsdaily'\nreading_daily['location']='readingdaily'\n","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:42:48.089131Z","iopub.execute_input":"2021-06-30T09:42:48.089602Z","iopub.status.idle":"2021-06-30T09:42:48.107088Z","shell.execute_reply.started":"2021-06-30T09:42:48.089552Z","shell.execute_reply":"2021-06-30T09:42:48.105716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_original=keats_hourly.append([hillingdon_data, harlington_hourly, harlington_daily, keats_daily, reading_daily],ignore_index=False, verify_integrity=False, sort=False)\ndf1=keats_hourly.append([harlington_hourly],ignore_index=False, verify_integrity=False, sort=False)\ndf2=harlington_daily.append([keats_daily,reading_daily], ignore_index=False, verify_integrity=False, sort=False)\ndf3=hillingdon_data.append([hillingdon_data], ignore_index=False, verify_integrity=False, sort=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:42:48.22416Z","iopub.execute_input":"2021-06-30T09:42:48.224511Z","iopub.status.idle":"2021-06-30T09:42:48.33353Z","shell.execute_reply.started":"2021-06-30T09:42:48.224479Z","shell.execute_reply":"2021-06-30T09:42:48.3324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:42:48.335319Z","iopub.execute_input":"2021-06-30T09:42:48.335735Z","iopub.status.idle":"2021-06-30T09:42:48.358288Z","shell.execute_reply.started":"2021-06-30T09:42:48.33569Z","shell.execute_reply":"2021-06-30T09:42:48.357177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:42:48.691531Z","iopub.execute_input":"2021-06-30T09:42:48.692122Z","iopub.status.idle":"2021-06-30T09:42:48.712631Z","shell.execute_reply.started":"2021-06-30T09:42:48.692088Z","shell.execute_reply":"2021-06-30T09:42:48.711733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df3","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:42:48.714246Z","iopub.execute_input":"2021-06-30T09:42:48.714918Z","iopub.status.idle":"2021-06-30T09:42:48.738448Z","shell.execute_reply.started":"2021-06-30T09:42:48.714882Z","shell.execute_reply":"2021-06-30T09:42:48.737649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#To tidy up the code. first check how many rows missing \nno_missing = df1.isnull().sum()\ntotal_missing=no_missing[0:15]\nprint(total_missing)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:42:49.044516Z","iopub.execute_input":"2021-06-30T09:42:49.045193Z","iopub.status.idle":"2021-06-30T09:42:49.117637Z","shell.execute_reply.started":"2021-06-30T09:42:49.045154Z","shell.execute_reply":"2021-06-30T09:42:49.116665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#removed unwanted columns - can't see any to immediately remove\n#remove df1 rows (found 58k with missing value)\ndf1.dropna(inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:42:49.142789Z","iopub.execute_input":"2021-06-30T09:42:49.143149Z","iopub.status.idle":"2021-06-30T09:42:49.283273Z","shell.execute_reply.started":"2021-06-30T09:42:49.143118Z","shell.execute_reply":"2021-06-30T09:42:49.282168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:42:49.284688Z","iopub.execute_input":"2021-06-30T09:42:49.284979Z","iopub.status.idle":"2021-06-30T09:42:49.306255Z","shell.execute_reply.started":"2021-06-30T09:42:49.28495Z","shell.execute_reply":"2021-06-30T09:42:49.305195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#remove Nan in Value column","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:42:49.469654Z","iopub.execute_input":"2021-06-30T09:42:49.470194Z","iopub.status.idle":"2021-06-30T09:42:49.474334Z","shell.execute_reply.started":"2021-06-30T09:42:49.470144Z","shell.execute_reply":"2021-06-30T09:42:49.473228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.info()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:42:49.60795Z","iopub.execute_input":"2021-06-30T09:42:49.608411Z","iopub.status.idle":"2021-06-30T09:42:49.660295Z","shell.execute_reply.started":"2021-06-30T09:42:49.608379Z","shell.execute_reply":"2021-06-30T09:42:49.659322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.plot('ReadingDateTime','Value')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:42:49.815114Z","iopub.execute_input":"2021-06-30T09:42:49.815473Z","iopub.status.idle":"2021-06-30T09:42:50.152666Z","shell.execute_reply.started":"2021-06-30T09:42:49.815442Z","shell.execute_reply":"2021-06-30T09:42:50.151424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This graph does not tell us much. The dates at the bottom are scrunched up, The \"value\" figures relate to 2 different sites and incorporate hourly data. This hourly data maybe useful at some point as we may see an regular daily trend through different parts of the day. For now by aim is to group by day, month and separate the 2 sites of data into separate lines. \n\nHowever I have now decided to look at the df1 data (this is the hourly data). ","metadata":{}},{"cell_type":"code","source":"#messing about stuff\nplt.figure(figsize=(8,8))\ndf1plot1= sns.scatterplot(x=df1['ReadingDateTime'],y=df1['Value'], hue=df1['Species'],style=df1['location'])\n#df1[\"Value\"].asfreq('M').plot() # asfreq method is used to convert a time series to a specified frequency. Here it is monthly frequency.\n#plt.title('can add a title')\n#plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:42:50.154323Z","iopub.execute_input":"2021-06-30T09:42:50.154668Z","iopub.status.idle":"2021-06-30T09:46:16.268657Z","shell.execute_reply.started":"2021-06-30T09:42:50.154627Z","shell.execute_reply":"2021-06-30T09:46:16.267456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"THis graph is a bit too busy . so how to modify ?? I think I shall try to amend to create graphs for each species (i.e. each type of pollutant) and then each site can be analysed in relation to another site....... now how best to do that. Thinking Groupby !","metadata":{}},{"cell_type":"markdown","source":"This looks a quite enough complicated plot. Would suggest individual species (i.e NO2,  PM2.5)","metadata":{}},{"cell_type":"code","source":"df1['date']=pd.to_datetime(df1['ReadingDateTime']).dt.date\ndf1['time']=pd.to_datetime(df1['ReadingDateTime']).dt.time\ndf1['month']=pd.to_datetime(df1['ReadingDateTime']).dt.month\ndf1['time']=pd.to_datetime(df1['ReadingDateTime']).dt.time\ndf1","metadata":{"execution":{"iopub.status.busy":"2021-06-30T10:03:24.361365Z","iopub.execute_input":"2021-06-30T10:03:24.361812Z","iopub.status.idle":"2021-06-30T10:03:42.008212Z","shell.execute_reply.started":"2021-06-30T10:03:24.361773Z","shell.execute_reply":"2021-06-30T10:03:42.00717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grpdf1 = df1.groupby(by=[\"Species\"])\ntestdf1=grpdf1.get_group(\"NO2\")\ntestdf1.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:46:16.270363Z","iopub.execute_input":"2021-06-30T09:46:16.270675Z","iopub.status.idle":"2021-06-30T09:46:16.297717Z","shell.execute_reply.started":"2021-06-30T09:46:16.270644Z","shell.execute_reply":"2021-06-30T09:46:16.296707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1plot2= sns.scatterplot(x=testdf1['ReadingDateTime'],y=testdf1['Value'], hue=df1['location'])\n\n# this same as df1plot 1\n#df1plot2=sns.scatterplot(x=df1['ReadingDateTime'],y=df1['Value'], hue=df1['Species'],style=df1['location'])","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:46:16.299544Z","iopub.execute_input":"2021-06-30T09:46:16.299977Z","iopub.status.idle":"2021-06-30T09:46:16.97517Z","shell.execute_reply.started":"2021-06-30T09:46:16.299934Z","shell.execute_reply":"2021-06-30T09:46:16.971871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First of all howeverI am going to look at the hourly data in df2 and try to separate this out by a better set of criteria---- looking at each site first","metadata":{}},{"cell_type":"code","source":"#df2 first tidy up code\n#To tidy up the code. first check how many rows missing \nno_missing = df2.isnull().sum()\ntotal_missing=no_missing[0:15]\nprint(total_missing)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:46:16.97616Z","iopub.status.idle":"2021-06-30T09:46:16.97662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3733 \"Value\" rows are missing at this stage from 7000 + in the original row . We remove these values to give ","metadata":{}},{"cell_type":"code","source":"#remove df2 rows (found 3k with missing value)\ndf2.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:46:16.977646Z","iopub.status.idle":"2021-06-30T09:46:16.978079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:46:16.978993Z","iopub.status.idle":"2021-06-30T09:46:16.979407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:46:16.980278Z","iopub.status.idle":"2021-06-30T09:46:16.98071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df2.plot('ReadingDateTime','Value')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:46:16.981664Z","iopub.status.idle":"2021-06-30T09:46:16.982094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add date from date time column\ndf2['date']= pd.to_datetime(df2['ReadingDateTime']).dt.date\n","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:46:16.984634Z","iopub.status.idle":"2021-06-30T09:46:16.985088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filter1=[\"location\"]==\"harlingtondaily\"\nfilter2=[\"location\"]==\"keatsdaily\"\nfilter3=[\"location\"]==\"readingdaily\"","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:46:16.986313Z","iopub.status.idle":"2021-06-30T09:46:16.986806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2\n","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:46:16.987894Z","iopub.status.idle":"2021-06-30T09:46:16.988344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,8))\ndf2plot1= sns.scatterplot(x=df2['ReadingDateTime'],y=df2['Value'], hue=df2['Species'],style=df2['location'])","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:46:16.989244Z","iopub.status.idle":"2021-06-30T09:46:16.989675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,8))\ndf2plot2= sns.lineplot(x=df2['ReadingDateTime'],y=df2['Value'], hue=df2['Species'],style=df2['location'])","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:46:16.990559Z","iopub.status.idle":"2021-06-30T09:46:16.991002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#striplot\nplt.figure(figsize=(8,8))\ndf2plot3= sns.stripplot(x=df2['Species'],y=df2['Value'], hue=df2['location'])\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:46:16.991912Z","iopub.status.idle":"2021-06-30T09:46:16.992315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I think a good approach now would be to group data by weekly average","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trying to groupby by species by mean value per day\nspecies_df2= df2.groupby(by=[\"Species\",\"location\",\"date\"]).aggregate({\"Value\":\"mean\"})\nspecies_df2","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:46:16.993148Z","iopub.status.idle":"2021-06-30T09:46:16.993538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#trying out a few different approaches so WARNING there are errors . plot 4 does work however\nplt.figure(figsize=(10,10))\nplot4_species_df2 = sns.swarmplot(x=df2['Species'], y=df2['Value'], marker='o')\n#plot5_species_df2= sns.swarmplot(x=df2['Species'], y=df2['location'].aggregate({'Value':\"mean\"}))\n# y = df2.groupby(by=[\"Species\",\"location\"]).aggregate({\"Value\":\"mean\"}))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:46:16.994471Z","iopub.status.idle":"2021-06-30T09:46:16.994924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot3_species_df2 = species_df2.plot()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:46:16.995786Z","iopub.status.idle":"2021-06-30T09:46:16.99619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#second attempt\nplot2_species_df2 = sns.lineplot(x=df2['date'], y = df2.groupby(by=[\"Species\",\"location\"]).aggregate({\"Value\":\"mean\"}))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:46:16.997169Z","iopub.status.idle":"2021-06-30T09:46:16.9976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#this needs to be worked on # this is first attempt \nplot_species_df2= sns.lineplot(x= species_df2['date'], y= df2['Value'], hue=df2['Species'],style=df2['location'])","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:46:16.998758Z","iopub.status.idle":"2021-06-30T09:46:16.999162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nplot_df2= sns.lineplot(data=df2.where(filter1),inplace =True ['Value'])","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:46:16.999993Z","iopub.status.idle":"2021-06-30T09:46:17.000401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sns.lineplot(Value in reading_d)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:46:17.001416Z","iopub.status.idle":"2021-06-30T09:46:17.00186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"harlington_d= ['Value' for harlington_daily in 'location']\nkeats_d = ['Value' for keats_daily in 'location']\nreading_d =['Value' for reading_daily in 'location']\nsns.lineplot(data= \"keats_d\")","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:46:17.002764Z","iopub.status.idle":"2021-06-30T09:46:17.003176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lineplot(data= keats_d)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:46:17.004038Z","iopub.status.idle":"2021-06-30T09:46:17.00444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndateTime= pd.to_datetime(df1['ReadingDateTime'], format='%d/%m/%Y %H:%M')\n#dateTime= time(\"Day: %d, Month: %m, Year: %Y, Hour: %H, Minute: %M\")","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:46:17.005292Z","iopub.status.idle":"2021-06-30T09:46:17.005743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1['day']= dateTime('%d')\ndf1['month']=dateTime('%m')\ndf1['year']=dateTime('%Y')\ndf1['hour']=dateTime('%H')\ndf1['minutes']=dateTime('%M')\ndf1['Date']= dateTime('%d/%m/%Y')\ndf1['Time']=dateTime(%H:%M)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-30T09:46:17.006512Z","iopub.status.idle":"2021-06-30T09:46:17.006948Z"},"trusted":true},"execution_count":null,"outputs":[]}]}