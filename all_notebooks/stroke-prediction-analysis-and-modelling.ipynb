{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np    \nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline  \nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import svm\nfrom sklearn.metrics import plot_confusion_matrix,classification_report\nfrom sklearn.metrics import plot_precision_recall_curve\nfrom sklearn.metrics import f1_score, accuracy_score, matthews_corrcoef\nfrom sklearn import metrics","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:01.637664Z","iopub.execute_input":"2021-07-25T17:41:01.638294Z","iopub.status.idle":"2021-07-25T17:41:02.985776Z","shell.execute_reply.started":"2021-07-25T17:41:01.638166Z","shell.execute_reply":"2021-07-25T17:41:02.984795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:02.987477Z","iopub.execute_input":"2021-07-25T17:41:02.987789Z","iopub.status.idle":"2021-07-25T17:41:03.052925Z","shell.execute_reply.started":"2021-07-25T17:41:02.98776Z","shell.execute_reply":"2021-07-25T17:41:03.051886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let's see a concise summary of the dataframe.","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:03.054916Z","iopub.execute_input":"2021-07-25T17:41:03.055381Z","iopub.status.idle":"2021-07-25T17:41:03.07822Z","shell.execute_reply.started":"2021-07-25T17:41:03.055336Z","shell.execute_reply":"2021-07-25T17:41:03.077056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The column containing Id numbers of persons is not much of use here, so dropping the 'id' column.","metadata":{}},{"cell_type":"code","source":"df.drop(columns ='id', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:03.079661Z","iopub.execute_input":"2021-07-25T17:41:03.079992Z","iopub.status.idle":"2021-07-25T17:41:03.089816Z","shell.execute_reply.started":"2021-07-25T17:41:03.079961Z","shell.execute_reply":"2021-07-25T17:41:03.088989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The column named 'bmi' has some missing values. Let us check it.","metadata":{}},{"cell_type":"code","source":"df['bmi'].isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:03.091523Z","iopub.execute_input":"2021-07-25T17:41:03.092041Z","iopub.status.idle":"2021-07-25T17:41:03.103Z","shell.execute_reply.started":"2021-07-25T17:41:03.092009Z","shell.execute_reply":"2021-07-25T17:41:03.102187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The number of missing values in the column 'bmi' is 201. Replacing the missing data points with mean of the column.","metadata":{}},{"cell_type":"code","source":"bmi_mean=df['bmi'].mean()\ndf['bmi'].replace(np.nan, bmi_mean, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:03.104137Z","iopub.execute_input":"2021-07-25T17:41:03.104543Z","iopub.status.idle":"2021-07-25T17:41:03.114176Z","shell.execute_reply.started":"2021-07-25T17:41:03.104514Z","shell.execute_reply":"2021-07-25T17:41:03.113384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Basic statistical summary of the data.","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:03.325366Z","iopub.execute_input":"2021-07-25T17:41:03.325911Z","iopub.status.idle":"2021-07-25T17:41:03.358929Z","shell.execute_reply.started":"2021-07-25T17:41:03.325881Z","shell.execute_reply":"2021-07-25T17:41:03.357857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# number of entries for each categories in 'stroke' column\ndf['stroke'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:03.780076Z","iopub.execute_input":"2021-07-25T17:41:03.780442Z","iopub.status.idle":"2021-07-25T17:41:03.788733Z","shell.execute_reply.started":"2021-07-25T17:41:03.780412Z","shell.execute_reply":"2021-07-25T17:41:03.78738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So the entries for 'stroke' = 1 is just 249 out of 5110 rows (which is about 5%). So out of 5110 samples, persons who had stroke is 5%.","metadata":{}},{"cell_type":"markdown","source":"## A- Visualising the data\n### 1. Age and Gender ","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(3,2,figsize=(8,10))\nsns.set_theme(style='whitegrid')\nsns.boxplot(x=df.hypertension, y=df.age,hue=df.gender, data = df, ax =axs[0,0])\nsns.stripplot(x=df.hypertension, y=df.age,hue=df.gender, data = df, ax =axs[0,1])\n\nsns.boxplot(x=df.heart_disease, y=df.age,hue=df.gender, data = df, ax = axs[1,0])\nsns.stripplot(x=df.heart_disease, y=df.age,hue=df.gender, data = df, ax = axs[1,1])\n\nsns.boxplot(x=df.stroke, y=df.age,hue=df.gender, data = df, ax = axs[2,0])\nsns.stripplot(x=df.stroke, y=df.age,hue=df.gender, data = df, ax = axs[2,1])","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:04.100123Z","iopub.execute_input":"2021-07-25T17:41:04.100486Z","iopub.status.idle":"2021-07-25T17:41:06.277447Z","shell.execute_reply.started":"2021-07-25T17:41:04.100455Z","shell.execute_reply":"2021-07-25T17:41:06.276515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We want to know the average age for patients of hypertension, heart_disease and stroke for males and females.","metadata":{}},{"cell_type":"code","source":"a =df.groupby(['hypertension', 'gender'])\nprint(\"Average 'age' for hypertension: \",'\\n',a['age'].aggregate('mean'),'\\n')\n\nb =df.groupby(['heart_disease','gender'])\nprint(\"Average 'age' for Heart-disease : \",'\\n' ,b['age'].aggregate('mean'),'\\n')\n\nc=df.groupby(['stroke','gender'])\nprint(\"Average 'age' for stroke : \",'\\n' ,c['age'].aggregate('mean'))\n","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:06.279611Z","iopub.execute_input":"2021-07-25T17:41:06.279987Z","iopub.status.idle":"2021-07-25T17:41:06.30124Z","shell.execute_reply.started":"2021-07-25T17:41:06.279953Z","shell.execute_reply":"2021-07-25T17:41:06.299917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**So from the above analysis we can say**\n\n **1. The average age for Hypertension is about 63 years in Females and 61 year in Males. It is slightly more in case female.**\n\n**2. The plot for occurrence of heart-disease vs age very clearly depicts that for people below age of 40 years have very rare cases of Heart-disease. On an average for people of age of about 70 years it is most common. There is not much difference in average age for Male and Female for heart-disease.**\n\n**3. The average age of Stroke for Males and Females are not very different (about 67 years in Female and 69 years in Males).**\n\n**4. The occurrence of any of the above three diseases is very rare for people of age below 40 years and high for people of age 60 years and above for all.**","metadata":{}},{"cell_type":"markdown","source":"### 2. Hypertension ","metadata":{}},{"cell_type":"code","source":"plt.plot(figsize=(8,10))\nsns.set_theme(style='whitegrid')\nsns.countplot(data = df,  x='stroke', hue='hypertension')\n\nprint('\\n','Stroke vs Hypertension Frequency table : ','\\n')\npd.crosstab(df['stroke'], \n                   df['hypertension'],  normalize='all', margins = True)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:06.30446Z","iopub.execute_input":"2021-07-25T17:41:06.304785Z","iopub.status.idle":"2021-07-25T17:41:06.569483Z","shell.execute_reply.started":"2021-07-25T17:41:06.304754Z","shell.execute_reply":"2021-07-25T17:41:06.568666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Among 90% of persons who are not having hypertension about 87% are those who did not get stroke as well.**","metadata":{}},{"cell_type":"markdown","source":"### 3. Heart-disease","metadata":{}},{"cell_type":"code","source":"plt.plot(figsize=(8,10))\nsns.set_theme(style='whitegrid')\nsns.countplot(data = df, x='stroke', hue='heart_disease')\n\nprint('\\n','Stroke vs Heart-disease Frequency table: ', '\\n')\npd.crosstab(df['stroke'], \n                  df['heart_disease'], normalize='all', margins = True)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:06.57082Z","iopub.execute_input":"2021-07-25T17:41:06.571118Z","iopub.status.idle":"2021-07-25T17:41:06.778255Z","shell.execute_reply.started":"2021-07-25T17:41:06.571088Z","shell.execute_reply":"2021-07-25T17:41:06.777213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The data shows 90% of persons who do not have heart-disease don't get stroke.**\n","metadata":{}},{"cell_type":"markdown","source":"### 4. Marital-status","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(3, figsize=(8,10))\nsns.set_theme(style='whitegrid')\nsns.countplot(data = df, x='ever_married', hue='hypertension', ax=axs[0])\n\nsns.countplot(data = df, x='ever_married', hue='stroke', ax=axs[1])\n\nsns.countplot(data = df, x='ever_married', hue='heart_disease', ax= axs[2])\n","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:06.781146Z","iopub.execute_input":"2021-07-25T17:41:06.781434Z","iopub.status.idle":"2021-07-25T17:41:07.245207Z","shell.execute_reply.started":"2021-07-25T17:41:06.781407Z","shell.execute_reply":"2021-07-25T17:41:07.244105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.crosstab([df['hypertension'], df['heart_disease'],df['stroke']], \n            df['ever_married'])","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:07.247917Z","iopub.execute_input":"2021-07-25T17:41:07.248224Z","iopub.status.idle":"2021-07-25T17:41:07.275707Z","shell.execute_reply.started":"2021-07-25T17:41:07.248196Z","shell.execute_reply":"2021-07-25T17:41:07.274669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Those who were ever-married have more cases of hypertension, heart-disease and stroke than those who were not.**","metadata":{}},{"cell_type":"markdown","source":"### 5.Work-type\nLet us see if there is any trend in the type of work a person does and disease\n","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(3, figsize=(8,10))\nsns.set_theme(style='whitegrid')\nsns.countplot(data = df, x='work_type', hue='hypertension', ax=axs[0])\n\nsns.countplot(data = df, x='work_type', hue='stroke', ax=axs[1])\n\nsns.countplot(data = df, x='work_type', hue='heart_disease', ax= axs[2])","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:07.277188Z","iopub.execute_input":"2021-07-25T17:41:07.277596Z","iopub.status.idle":"2021-07-25T17:41:07.984547Z","shell.execute_reply.started":"2021-07-25T17:41:07.277554Z","shell.execute_reply":"2021-07-25T17:41:07.983506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The people in private-sector are at higher risk to develop any of these diseases. People who never-worked are at minimal risk.**","metadata":{}},{"cell_type":"markdown","source":"### 6. Residence-type\nLet us now find out trend between Residence-type and diseases.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(3, figsize=(8,10))\nsns.set_theme(style='whitegrid')\nsns.countplot(data = df, x='Residence_type', hue='hypertension', ax=axs[0])\n\nsns.countplot(data = df, x='Residence_type', hue='stroke', ax=axs[1])\n\nsns.countplot(data = df, x='Residence_type', hue='heart_disease', ax= axs[2]) ","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:07.985874Z","iopub.execute_input":"2021-07-25T17:41:07.986147Z","iopub.status.idle":"2021-07-25T17:41:08.440198Z","shell.execute_reply.started":"2021-07-25T17:41:07.98612Z","shell.execute_reply":"2021-07-25T17:41:08.439059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The countplot for hypertension, heart-disease and stroke for both (urban and rural) categories of Residence-type does not show major difference between each residents for their diseases. So we can say residence location does not has any significant role in causing stroke.**","metadata":{}},{"cell_type":"markdown","source":"### 7. Smoking_status","metadata":{}},{"cell_type":"code","source":"#     smoking_status    \n\nfig, axs = plt.subplots(3, figsize=(8,10))\nsns.set_theme(style='whitegrid')\nsns.countplot(data = df, x='smoking_status', hue='hypertension', ax=axs[0])\n\nsns.countplot(data = df, x='smoking_status', hue='stroke', ax=axs[1])\n\nsns.countplot(data = df, x='smoking_status', hue='heart_disease', ax= axs[2]) ","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:08.441508Z","iopub.execute_input":"2021-07-25T17:41:08.441852Z","iopub.status.idle":"2021-07-25T17:41:08.973176Z","shell.execute_reply.started":"2021-07-25T17:41:08.441805Z","shell.execute_reply":"2021-07-25T17:41:08.971938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.crosstab(df['smoking_status'], df['hypertension'], normalize = 'columns')","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:08.974492Z","iopub.execute_input":"2021-07-25T17:41:08.974819Z","iopub.status.idle":"2021-07-25T17:41:08.999236Z","shell.execute_reply.started":"2021-07-25T17:41:08.974789Z","shell.execute_reply":"2021-07-25T17:41:08.99827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**From all the patients of Hypertension, highest 46% are from never-smoked group while 24% are from 'formerly-smoked' and and 18% 'smokes' group. For 10% patients of Hypertension patients the smoking-status is unkonwn.**\n\n**Although for many(52) of the patients their smoking-status is not-known. But we can make inference based on fact that a big percentage (46% which in numbers is 234 out of 498) of stroke-patients never-smoked.** ","metadata":{}},{"cell_type":"markdown","source":"### 8. Body-Mass-Index (bmi)\nLet us find out if there is any relation among these disease data and bmi.","metadata":{}},{"cell_type":"code","source":"sns.catplot(x=\"hypertension\", y=\"bmi\",\n                hue=\"heart_disease\", col=\"stroke\",\n                data=df, kind=\"bar\",\n                height=5);","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:09.000411Z","iopub.execute_input":"2021-07-25T17:41:09.000701Z","iopub.status.idle":"2021-07-25T17:41:09.82556Z","shell.execute_reply.started":"2021-07-25T17:41:09.000675Z","shell.execute_reply":"2021-07-25T17:41:09.824633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 9. Average Glucose-level","metadata":{}},{"cell_type":"code","source":"sns.catplot(x=\"hypertension\", y=\"avg_glucose_level\",\n                hue=\"heart_disease\", col=\"stroke\",\n                data=df, kind=\"bar\",\n                height=5);","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:09.827324Z","iopub.execute_input":"2021-07-25T17:41:09.827736Z","iopub.status.idle":"2021-07-25T17:41:10.679169Z","shell.execute_reply.started":"2021-07-25T17:41:09.827694Z","shell.execute_reply":"2021-07-25T17:41:10.678089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b =df.groupby(['hypertension', 'gender'])\nprint(\"Average 'avg_glucose_level' for hypertension: \",'\\n'*2 ,b['avg_glucose_level'].aggregate('mean'),'\\n')\n\nc =df.groupby(['heart_disease','gender'])\nprint(\"Average 'avg_glucose_level' for Heart-disease : \",'\\n'*2 ,c['avg_glucose_level'].aggregate('mean'),'\\n')\n\na=df.groupby(['stroke','gender'])\nprint(\"Average 'avg_glucose_level' for stroke : \",'\\n' *2 ,a['avg_glucose_level'].aggregate('mean'))","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:10.681205Z","iopub.execute_input":"2021-07-25T17:41:10.681567Z","iopub.status.idle":"2021-07-25T17:41:10.705364Z","shell.execute_reply.started":"2021-07-25T17:41:10.681534Z","shell.execute_reply":"2021-07-25T17:41:10.704497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Mean glucose-level among hypertension patients is slightly higher for males(132) than females(128)**\n\n**The mean value of glucose-level in Heart-disease patients are higher for females(about 143) than that for males(about 132)**\n\n**The stroke patients who have mean-glucose level of about 124 for females and 143 for males. Here females show lower mean value of glucose level that for males**\n\nOverall, we can say that high glucose level is common among all these patients.","metadata":{}},{"cell_type":"markdown","source":"## B- Preprocessing\nLet us find all the columns that contain string type categorical data. \nAnd using Scikit-learn library change the string type categorical variable columns to numerical type.","metadata":{}},{"cell_type":"code","source":"cat_col = df.select_dtypes(['object']).columns\nprint(cat_col)\nlabel_encode = LabelEncoder()    #initializing an object of class LabelEncoder\nfor i in df.columns:\n    df[i] = label_encode.fit_transform(df[i])\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:10.706797Z","iopub.execute_input":"2021-07-25T17:41:10.707088Z","iopub.status.idle":"2021-07-25T17:41:10.742414Z","shell.execute_reply.started":"2021-07-25T17:41:10.70706Z","shell.execute_reply":"2021-07-25T17:41:10.741613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us see correlation table and Heat-map.","metadata":{}},{"cell_type":"code","source":"df.corr()","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:10.743334Z","iopub.execute_input":"2021-07-25T17:41:10.743597Z","iopub.status.idle":"2021-07-25T17:41:10.767498Z","shell.execute_reply.started":"2021-07-25T17:41:10.743571Z","shell.execute_reply":"2021-07-25T17:41:10.766481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(figsize=(15,15))\nsns.heatmap(df.corr())","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:10.769113Z","iopub.execute_input":"2021-07-25T17:41:10.769423Z","iopub.status.idle":"2021-07-25T17:41:11.157156Z","shell.execute_reply.started":"2021-07-25T17:41:10.769394Z","shell.execute_reply":"2021-07-25T17:41:11.156394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  Feature-selection\n**Feature selection technique permits researchers to choose measures that are maximally predictive of relevant outcomes, even when there are interactions or nonlinearities. These techniques facilitate decisions about which measures may be dropped from a study while maintaining efficiency of prediction**\n\n**Let us choose best features, using SelectKBest method, which is a univariate feature selection method, using f_classif as scoring function. For classification, scoring functions based on F-test estimate the degree of linear dependency between two random variables.**","metadata":{}},{"cell_type":"code","source":"#drop target columns\ndrop_col = ['stroke']\n\n#dataset containing the features columns\nx_feat = df.iloc[:,0:10]     #independent variable (predictors)\ny_tar = df['stroke']      # target feature column\n\nbest_feat = SelectKBest(score_func = f_classif, k='all')\nfeatures_fitted = best_feat.fit(x_feat, y_tar)\ndf_scores = pd.DataFrame(features_fitted.scores_)\ndf_columns = pd.DataFrame(x_feat.columns)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:11.158279Z","iopub.execute_input":"2021-07-25T17:41:11.158727Z","iopub.status.idle":"2021-07-25T17:41:11.174452Z","shell.execute_reply.started":"2021-07-25T17:41:11.158684Z","shell.execute_reply":"2021-07-25T17:41:11.173026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# concatenate dataframes\nfeature_scores = pd.concat([df_columns , df_scores], axis =1)\nfeature_scores.columns = ['Features','Score']\n\n#sorting the feature dataframe based on score values\nfeature_scores.sort_values(by='Score',ascending=False)\nfeature_scores","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:11.175724Z","iopub.execute_input":"2021-07-25T17:41:11.176009Z","iopub.status.idle":"2021-07-25T17:41:11.189468Z","shell.execute_reply.started":"2021-07-25T17:41:11.17598Z","shell.execute_reply":"2021-07-25T17:41:11.188454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Selecting features with scores greater than 60","metadata":{}},{"cell_type":"code","source":"cols = feature_scores[feature_scores['Score']>60]['Features']\nprint('Choosen features : ','\\n', cols)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:11.190879Z","iopub.execute_input":"2021-07-25T17:41:11.19128Z","iopub.status.idle":"2021-07-25T17:41:11.202149Z","shell.execute_reply.started":"2021-07-25T17:41:11.19125Z","shell.execute_reply":"2021-07-25T17:41:11.200946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Normalizing the dataset.","metadata":{}},{"cell_type":"code","source":"x=df[cols].values\ny=df['stroke'].values\nfrom sklearn import preprocessing\nx = preprocessing.StandardScaler().fit_transform(x)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:11.203367Z","iopub.execute_input":"2021-07-25T17:41:11.203699Z","iopub.status.idle":"2021-07-25T17:41:11.215504Z","shell.execute_reply.started":"2021-07-25T17:41:11.203662Z","shell.execute_reply":"2021-07-25T17:41:11.214633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting the dataset into training and testing sets.","metadata":{}},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x,y,random_state = 50, test_size =0.25)\nx_train.shape,x_test.shape,y_train.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:11.216752Z","iopub.execute_input":"2021-07-25T17:41:11.217046Z","iopub.status.idle":"2021-07-25T17:41:11.229273Z","shell.execute_reply.started":"2021-07-25T17:41:11.217019Z","shell.execute_reply":"2021-07-25T17:41:11.228212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Modelling**\n\n### Let us now implement Logistic regression algorithm.\n\nLet's build our model using LogisticRegression from the Scikit-learn package. This function implements logistic regression and can use different numerical optimizers to find parameters, including ‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’ solvers.  C parameter indicates inverse of regularization strength which must be a positive float. Smaller values specify stronger regularization.\n\n","metadata":{}},{"cell_type":"code","source":"#from sklearn.linear_model import LogisticRegression\n\nLR = LogisticRegression(C=0.1, solver='liblinear').fit(x_train, y_train)\n\nyp_lr = LR.predict(x_test)\n\n#f1_score:\nyf = f1_score(y_test,yp_lr )\n\nya = metrics.accuracy_score(y_test,yp_lr)     \n\nprint('F1 Score for Logistic regression model : %.2f'%yf)\nprint('Accuracy : %.2f'%ya)\n\nplot_confusion_matrix(LR, x_test, y_test,cmap='YlGn')\nplt.title('Cofusion matrix for model')\n\nplt.show()  ","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:11.230907Z","iopub.execute_input":"2021-07-25T17:41:11.231368Z","iopub.status.idle":"2021-07-25T17:41:11.447338Z","shell.execute_reply.started":"2021-07-25T17:41:11.231337Z","shell.execute_reply":"2021-07-25T17:41:11.446222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**As shown by confusion matrix the Logistic regression classifier gives zero true-positives so precision will be zero.**\n\n**The zero value of F1-score shows that the model is failure.**","metadata":{}},{"cell_type":"markdown","source":"Now let us try to model using three more algorithms namely: K-Nearest Neighbor, Decision Trees and Support Vector Machine. Then we will compare their results using different evaluation matrices.","metadata":{}},{"cell_type":"code","source":"# declaring three list variables to strore various evaluation matrices\nacc_val = []\nf1_val = []\nmacc_val = []","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:11.448687Z","iopub.execute_input":"2021-07-25T17:41:11.44899Z","iopub.status.idle":"2021-07-25T17:41:11.452971Z","shell.execute_reply.started":"2021-07-25T17:41:11.448961Z","shell.execute_reply":"2021-07-25T17:41:11.452013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Implementing KNearest neighbour","metadata":{}},{"cell_type":"code","source":"# Selecting best K-value(number of neighbours) based upon the accuracy-score\nks = 10\nmean_acc = np.zeros((ks-1))\nstd_acc = np.zeros((ks-1))\nfor n in range (1,ks):\n    neg = KNeighborsClassifier(n_neighbors=n).fit(x_train, y_train)\n    ypred_knn = neg.predict(x_test)\n    mean_acc[n-1]= metrics.accuracy_score(y_test, ypred_knn)\n    std_acc[n-1]= np.std(ypred_knn==y_test)/np.sqrt(ypred_knn.shape[0])\nprint(' Best accuracy of %.3f '%mean_acc.max(), 'was with k = ', mean_acc.argmax())","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:11.454198Z","iopub.execute_input":"2021-07-25T17:41:11.454554Z","iopub.status.idle":"2021-07-25T17:41:12.088477Z","shell.execute_reply.started":"2021-07-25T17:41:11.454525Z","shell.execute_reply":"2021-07-25T17:41:12.087363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.plot(range(1,ks),mean_acc,'g')\nplt.fill_between(range(1,ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\nplt.fill_between(range(1,ks),mean_acc - 3 * std_acc,mean_acc + 3 * std_acc, alpha=0.10,color=\"green\")\nplt.legend(('Accuracy ', '+/- 1xstd','+/- 3xstd'))\nplt.ylabel('Accuracy ')\nplt.xlabel('Number of Neighbors (K)')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:12.08975Z","iopub.execute_input":"2021-07-25T17:41:12.090056Z","iopub.status.idle":"2021-07-25T17:41:12.414007Z","shell.execute_reply.started":"2021-07-25T17:41:12.090028Z","shell.execute_reply":"2021-07-25T17:41:12.412984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The plot above depicts the changing values of accuracy with K-values.**","metadata":{}},{"cell_type":"code","source":"#Finding Training and testing set accuracy for best k\nknn = KNeighborsClassifier(n_neighbors=7).fit(x_train, y_train)\n\ntrain_scores= metrics.accuracy_score(y_train, knn.predict(x_train))\ntest_scores= metrics.accuracy_score(y_test, knn.predict(x_test))\nprint(' Train-set accuracy with k=7 is: %.3f'%train_scores)\nprint(' Test-set accuracy with k=7 is: %.3f'%test_scores)\nk_acc = metrics.accuracy_score(y_test, ypred_knn)\nacc_val.append(k_acc)\n#print(' Accuracy-score : %.3f'%k_acc)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:12.415133Z","iopub.execute_input":"2021-07-25T17:41:12.415421Z","iopub.status.idle":"2021-07-25T17:41:12.658324Z","shell.execute_reply.started":"2021-07-25T17:41:12.415394Z","shell.execute_reply":"2021-07-25T17:41:12.657096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Implementing Decision tree","metadata":{}},{"cell_type":"code","source":"d_tree = DecisionTreeClassifier(criterion='entropy')\nd_tree.fit(x_train, y_train)\nypred_tree = d_tree.predict(x_test)\n#Evaluating the model\nd_acc =  metrics.accuracy_score(y_test, ypred_tree)\nacc_val.append(d_acc)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:12.659588Z","iopub.execute_input":"2021-07-25T17:41:12.659943Z","iopub.status.idle":"2021-07-25T17:41:12.668332Z","shell.execute_reply.started":"2021-07-25T17:41:12.659913Z","shell.execute_reply":"2021-07-25T17:41:12.667189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Implementing Support Vector Machine","metadata":{}},{"cell_type":"markdown","source":"**SVM algorithms use a set of mathematical functions that are defined as the kernel. The function of kernel is to take data as input and transform it into the required form. Different SVM algorithms use different types of kernel functions. These functions can be different types. For example linear, nonlinear, polynomial, radial basis function (RBF), and sigmoid.**\n**The most used type of kernel function is RBF. Because it has localized and finite response along the entire x-axis.**","metadata":{}},{"cell_type":"code","source":"clf = svm.SVC(kernel='rbf')\nsv = clf.fit(x_train, y_train)\nypred_svm = clf.predict(x_test)\ns_acc = accuracy_score(y_test, ypred_svm)\nacc_val.append(s_acc)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:12.669999Z","iopub.execute_input":"2021-07-25T17:41:12.670517Z","iopub.status.idle":"2021-07-25T17:41:12.796772Z","shell.execute_reply.started":"2021-07-25T17:41:12.670419Z","shell.execute_reply":"2021-07-25T17:41:12.795831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Evaluating the model**\n\n### Accuracy-score\n**Let us compare the accuracy of the three algorithms for given dataset**","metadata":{}},{"cell_type":"code","source":"print('KNN model accuracy : %.3f' %k_acc)\nprint('Decision tree model accuracy : %.3f'%d_acc)\nprint('SVM model accuracy : %.3f' %s_acc)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:12.798339Z","iopub.execute_input":"2021-07-25T17:41:12.798719Z","iopub.status.idle":"2021-07-25T17:41:12.804309Z","shell.execute_reply.started":"2021-07-25T17:41:12.798672Z","shell.execute_reply":"2021-07-25T17:41:12.803359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**So accuracy scores for all of te chosen models are equal.**","metadata":{}},{"cell_type":"markdown","source":"### Computing the Confusion Matrix","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(nrows=3, figsize=(10,15), constrained_layout = True)\nplt.rcParams['font.size'] = '16'\nplot_confusion_matrix(d_tree, x_test, y_test,ax=axs[0],cmap='YlGn')\naxs[0].set_title('Cofusion matrix for Decision tree model')\n\nplot_confusion_matrix(sv, x_test, y_test,ax = axs[1],cmap='YlGn')\naxs[1].set_title('Cofusion matrix for SVM model')\n\nplot_confusion_matrix(knn, x_test, y_test, ax = axs[2],cmap='YlGn')\naxs[2].set_title('Cofusion matrix for KNN model')\nplt.show()  ","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:12.805688Z","iopub.execute_input":"2021-07-25T17:41:12.80609Z","iopub.status.idle":"2021-07-25T17:41:13.700978Z","shell.execute_reply.started":"2021-07-25T17:41:12.806058Z","shell.execute_reply":"2021-07-25T17:41:13.700178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### As seen from confusion matrices, \n**1. The True-Negatives for support vector machine model highest in numbers.**\n\n**2. The decision tree model gives highest True-Positives followed by KNN model while SVM model fails to give any True-Positives.**\n\nHere we are trying to build a model to predict of a person has stroke or not, so we want to capture the disease i.e as many positives as possible. Decision tree classifiers performs best among the three models in capture highest number of True-Positives (as desirable)","metadata":{}},{"cell_type":"markdown","source":"### Classification Report","metadata":{}},{"cell_type":"code","source":"np.set_printoptions(precision=2)\nt_n = ['Stroke = 0', 'Stroke = 1']\nprint('\\n',\"Classification report for KNN classifier\")\nprint(classification_report(y_test, ypred_knn, labels=[0,1], target_names=t_n))\nprint('\\n',\"Classification report for Decision tree classifier\")\nprint(classification_report(y_test, ypred_tree, labels=[0,1], target_names=t_n))\nprint('\\n',\"Classification report for SVM classifier\")\nprint(classification_report(y_test, ypred_svm, labels=[0,1],target_names=t_n))","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:13.70223Z","iopub.execute_input":"2021-07-25T17:41:13.702776Z","iopub.status.idle":"2021-07-25T17:41:13.728665Z","shell.execute_reply.started":"2021-07-25T17:41:13.702733Z","shell.execute_reply":"2021-07-25T17:41:13.727819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The average Precision and Recall are highest for Decision tree**","metadata":{}},{"cell_type":"markdown","source":"**The F1 score**","metadata":{}},{"cell_type":"code","source":"print('F1-score for the KNN model is : %.3f '  %f1_score(y_test, knn.predict(x_test)))\nprint('F1-score for the decision tree model is : %.3f'%f1_score(y_test, ypred_tree))\nprint('F1-score for the support vector machine model : %.3f' %f1_score(y_test,ypred_svm))\nf1_val.append(f1_score(y_test, knn.predict(x_test)))\nf1_val.append(f1_score(y_test, ypred_tree))\nf1_val.append(f1_score(y_test,ypred_svm))","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:13.732189Z","iopub.execute_input":"2021-07-25T17:41:13.732678Z","iopub.status.idle":"2021-07-25T17:41:13.862455Z","shell.execute_reply.started":"2021-07-25T17:41:13.732624Z","shell.execute_reply":"2021-07-25T17:41:13.861597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The F1-score for decision tree classifier is higher than the other two classifiers.**","metadata":{}},{"cell_type":"markdown","source":"### Matthews correlation coefficient\n#### The Matthews correlation coefficient is used in machine learning as a measure of the quality of binary (two-class) classifications. The MCC is in essence a correlation coefficient value between -1 and +1. A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction. The statistic is also known as the phi coefficient.”\n#### The Matthews correlation coefficient (MCC), is a more reliable statistical rate which produces a high score only if the prediction obtained good results in all of the four confusion matrix categories (true positives, false negatives, true negatives, and false positives), proportionally both to the size of positive elements and the size of negative elements in the dataset.","metadata":{}},{"cell_type":"code","source":"print(\"The Matthews correlation coefficient for Decision tree model : %.3f\" %matthews_corrcoef(y_test, ypred_tree))\nprint(\"The Matthews correlation coefficient for K nearest neighbour model : %.3f\" %matthews_corrcoef(y_test, knn.predict(x_test)))\nprint(\"The Matthews correlation coefficient for SVM model : %.3f\" %matthews_corrcoef(y_test, ypred_svm))\nmacc_val.append(matthews_corrcoef(y_test, knn.predict(x_test)))\nmacc_val.append(matthews_corrcoef(y_test, ypred_tree))\nmacc_val.append(matthews_corrcoef(y_test, ypred_svm))","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:13.863921Z","iopub.execute_input":"2021-07-25T17:41:13.864224Z","iopub.status.idle":"2021-07-25T17:41:14.008321Z","shell.execute_reply.started":"2021-07-25T17:41:13.864194Z","shell.execute_reply":"2021-07-25T17:41:14.007454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Decision Tree model has highest matthews correlation coefficient among the chosen models.","metadata":{}},{"cell_type":"code","source":"eval_mat = pd.DataFrame(list(zip(acc_val, f1_val, macc_val)), \n                        columns=['Accuracy','F1_score','Matthews_cor_coef'],\n                       index = ['KNN', 'Decision_tree', 'SVM'])\neval_mat","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:14.010045Z","iopub.execute_input":"2021-07-25T17:41:14.010471Z","iopub.status.idle":"2021-07-25T17:41:14.024782Z","shell.execute_reply.started":"2021-07-25T17:41:14.010426Z","shell.execute_reply":"2021-07-25T17:41:14.023657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Analyzing the various evaluation matrices for our models we clearly see that the Decision tree classifier performs best among the three classifiers.","metadata":{}},{"cell_type":"markdown","source":"### Precision-Recall curves for the three classifiers","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(figsize=(8,8))\nplot_precision_recall_curve(knn, x_test, y_test, name = 'K-Nearest Neighbour',ax=axs)\nplot_precision_recall_curve(d_tree, x_test, y_test, name = 'Decision tree',ax=axs)\nplot_precision_recall_curve(clf, x_test, y_test, name = 'Support Vector Machine', ax=axs)\nplt.legend(loc=(0, -.30), prop=dict(size=14))\nplt.title('Precision-Recall curve for different models')","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:41:14.026541Z","iopub.execute_input":"2021-07-25T17:41:14.027041Z","iopub.status.idle":"2021-07-25T17:41:14.480993Z","shell.execute_reply.started":"2021-07-25T17:41:14.026997Z","shell.execute_reply":"2021-07-25T17:41:14.479863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion\nThe precision-recall curve for decision matrix has higher area under the curve, thus confirming that it is better than the other two models, This is in agreement with our conclusion that decision tree classifier is best when compared to K-nearest neighbour and Support Vector Machine.\n","metadata":{}}]}