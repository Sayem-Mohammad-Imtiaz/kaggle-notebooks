{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nsns.set(style='white', palette='deep')\nwidth = 0.35\nfontsize = 10\n%matplotlib inline\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Function\ndef autolabel_without_pct(rects,ax): #autolabel\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate('{}'.format(height),\n                    xy = (rect.get_x() + rect.get_width()/2, height),\n                    xytext= (0,3),\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\ndef autolabel_horizontal(rects,ax):\n    \"\"\"\n    Attach a text label above each bar displaying its height\n    \"\"\"\n    for rect in rects:\n        width = rect.get_width()\n        ax.text(rect.get_x() + rect.get_width() / 2, rect.get_y() + rect.get_height()/2.,\n                '%.2f' % width,\n                ha='center', va='center', color='white')    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing dataset\ndf = pd.read_csv('/kaggle/input/2006-2010-brazilian-congressman-election/vote.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Looking for null values\nnull_values = (df.isnull().sum()/len(df))*100\nnull_values = pd.DataFrame(null_values, columns=['% of null values'])\nnull_values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting dataset by year\ndf_2006 = df[df['ano'] == 2006]\ndf_2010 = df[df['ano'] == 2010]\nprint('In 2006 had {} candidates and in 2010 had {} candidates'.format(len(df_2006),len(df_2010)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#In 2006 and 2010, which was the political party with more candidates elected?\nfrom collections import Counter\ndf_2006_win = df_2006[df_2006['situacao'] == np.unique(df['situacao'])[0]]\ndf_2010_win = df_2010[df_2010['situacao'] == np.unique(df['situacao'])[0]]\n\nparty_win_2006 = Counter(df_2006_win['partido'])\nparty_win_2010 = Counter(df_2010_win['partido'])\n\nparty_win_2006 = {k: v for k, v in sorted(party_win_2006.items(), key=lambda item: item[1], reverse=True)}\nparty_win_2010 = {k: v for k, v in sorted(party_win_2010.items(), key=lambda item: item[1], reverse=True)}\n\nparty_win_2006_labels = list(party_win_2006.keys())\nparty_win_2006_values = list(party_win_2006.values())\nparty_win_2010_labels = list(party_win_2010.keys())\nparty_win_2010_values = list(party_win_2010.values())\n\nind_2006 = np.arange(len(party_win_2006_labels))\nind_2010 = np.arange(len(party_win_2010_labels))\nfig = plt.figure(figsize=(10,10))\nfig.suptitle('Top 5 Political Party in 2006 and 2010 in Brazil', fontsize=15) \nax1 = fig.add_subplot(1,2,1)\nax2 = fig.add_subplot(1,2,2)\nfor i in np.arange(0,5):\n    rects1 = ax1.bar(party_win_2006_labels[i],party_win_2006_values[i], width=width, edgecolor='black')\n    rects2 = ax2.bar(party_win_2010_labels[i],party_win_2010_values[i], width=width, edgecolor='black')\n    autolabel_without_pct(rects1, ax1)\n    autolabel_without_pct(rects2, ax2)\n   \nax1.set_xlabel('Political Party', fontsize=fontsize)\nax2.set_xlabel('Political Party', fontsize=fontsize)\nax1.set_xticks(ind_2006[:5])\nax2.set_xticks(ind_2010[:5])\nax1.set_xticklabels(party_win_2006_labels, fontsize=fontsize)\nax2.set_xticklabels(party_win_2010_labels, fontsize=fontsize)\nax1.set_ylabel('Number of Candidate Elected', fontsize=fontsize)\nax2.set_yticklabels([])\nax1.set_ylim(0,95)\nax2.set_ylim(0,95)\nax1.set_title('Top 5 Political Party in 2006', fontsize=fontsize)\nax2.set_title('Top 5 Political Party in 2010', fontsize=fontsize)\nax1.grid(b=True, which= 'major', linestyle='--' )\nax2.grid(b=True,which='major', linestyle='--')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#In 2006 and 2010, which was the top 10 candidate that collected more resources and was elected?\ndf_2006_win.columns\nfor x,y,z in [df_2006_win[['nome', 'uf', 'partido']]]:\n    print(df_2006_win[x].values + ' ' + '('+ df_2006_win[y].values+')' + ' ' + '('+df_2006_win[z].values+')')\n\ndf_2006_win['nome-uf-partido'] = df_2006_win['nome'].values + ' ' + '('+ df_2006_win['uf'].values+')' + ' ' + '('+df_2006_win['partido'].values+')'\ndf_2010_win['nome-uf-partido'] = df_2010_win['nome'].values + ' ' + '('+ df_2010_win['uf'].values+')' + ' ' + '('+df_2010_win['partido'].values+')'\n\ndf_2006_win_receita = df_2006_win[['nome-uf-partido','total_receita','total_despesa' ]].sort_values(by='total_receita', ascending=False)[:10]\ndf_2010_win_receita = df_2010_win[['nome-uf-partido','total_receita','total_despesa' ]].sort_values(by='total_receita', ascending=False)[:10]\n\nind=np.arange(len(df_2006_win_receita))\nfig = plt.figure(figsize=(15,15))\nax1 = fig.add_subplot(2,1,1)\nax2 = fig.add_subplot(2,1,2)\nrects1 = ax1.barh(ind+width/2,df_2006_win_receita['total_receita'].values, width, edgecolor= 'black', label='Total Resources', align='center')\nrects2 = ax1.barh(ind-width/2,df_2006_win_receita['total_despesa'].values, width, edgecolor='black', label='Total Expenses', align='center')\nax1.set_title('The top 10 candidate that collected more resources and was elected in 2006')\nax1.set_yticks(ind)\nax1.set_yticklabels(df_2006_win_receita['nome-uf-partido'].values)\nax1.legend(loc='best', frameon=False)\nax1.grid(b=True, which='major', linestyle='--')\nax1.set_ylabel('Candidate / State / Political Party')\nax1.set_xlabel('Amount of Money (R$) - Brazilian Currency')\nax1.tick_params(axis='y', labelsize=10, labelcolor='k', labelrotation=0)\nautolabel_horizontal(rects1,ax1)\nautolabel_horizontal(rects2,ax1)\n\nrects3 = ax2.barh(ind+width/2,df_2010_win_receita['total_receita'].values, width, edgecolor= 'black', label='Total Resources', align='center')\nrects4 = ax2.barh(ind-width/2,df_2010_win_receita['total_despesa'].values, width, edgecolor='black', label='Total Expenses', align='center')\nax2.set_title('The top 10 candidate that collected more resources and was elected in 2010')\nax2.set_yticks(ind)\nax2.set_yticklabels(df_2010_win_receita['nome-uf-partido'].values)\nax2.legend(loc='best', frameon=False)\nax2.grid(b=True, which='major', linestyle='--')\nax2.set_ylabel('Candidate / State / Political Party')\nax2.set_xlabel('Amount of Money (R$) - Brazilian Currency')\nax2.tick_params(axis='y', labelsize=10, labelcolor='k', labelrotation=0)\nautolabel_horizontal(rects3,ax2)\nautolabel_horizontal(rects4,ax2)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#In 2006 and 2010, what were the numbers of men and women elected?\ndf_2006_win.columns\nnp.unique(df_2006_win['sexo'])\nmen_2006 = np.sum([df_2006_win['sexo'].values[i] == np.unique(df_2006_win['sexo'])[1] for i in np.arange(len(df_2006_win))])\nwomen_2006 = np.sum([df_2006_win['sexo'].values[i] == np.unique(df_2006_win['sexo'])[0] for i in np.arange(len(df_2006_win))])\n\nnp.unique(df_2010_win['sexo'])\nmen_2010 = np.sum([df_2010_win['sexo'].values[i] == np.unique(df_2010_win['sexo'])[1] for i in np.arange(len(df_2006_win))])\nwomen_2010 = np.sum([df_2010_win['sexo'].values[i] == np.unique(df_2010_win['sexo'])[0] for i in np.arange(len(df_2006_win))])\n\nind = np.arange(len(np.unique(df_2006_win['sexo'])))\nfig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(1,1,1)\nrects1 = ax.bar(ind[0]+width/2, men_2006,width=width, edgecolor='black', color='black', label='Men')\nrects2 = ax.bar(ind[0]-width/2, women_2006,width=width, edgecolor='black', color='gray', label='Women')\nrects3 = ax.bar(ind[1]+width/2, men_2010,width=width, edgecolor='black',color='black')\nrects4 = ax.bar(ind[1]-width/2, women_2010,width=width, edgecolor='black', color='gray' )\nax.set_xticks(ind)\nax.legend(loc='best', title='Genre')\nax.set_xticklabels(['2006','2010'])\nax.set_xlabel('Years')\nax.set_ylabel('Amount')\nax.set_title('Numbers of men and women elected')\nax.grid(b=True, which='major', linestyle='--')\nautolabel_without_pct(rects1,ax)\nautolabel_without_pct(rects2,ax)\nautolabel_without_pct(rects3,ax)\nautolabel_without_pct(rects4,ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#In the Brazilian election, when the number of donations is not equal to the number collected resource, it can be a fraud signal.\n#In 2006 and 2010, what were the candidates that the number of donations it did not equal the collected resource?\ndf_2006_win.columns\nfor x,y in [df_2006_win[['quantidade_doacoes', 'quantidade_doadores']]]:\n    df_2006_win['fraud']= df_2006_win[x] - df_2006_win[y]\ndf_2006_win.sort_values(by='fraud', ascending=False)[['nome-uf-partido', 'fraud']]  \n    \nfor x,y in [df_2010_win[['quantidade_doacoes', 'quantidade_doadores']]]:\n    df_2010_win['fraud']= df_2010_win[x] - df_2010_win[y]\ndf_2010_win.sort_values(by='fraud', ascending=False)[['nome-uf-partido', 'fraud']]  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#In 2006 and 2010, what were the educational level of candidates ellected?\ndf_2006_win.columns\ngrouped_2006 = df_2006_win.groupby('grau')['grau'].count().sort_values(ascending=False)\ngrouped_2010 = df_2010_win.groupby('grau')['grau'].count().sort_values(ascending=False)\ngrouped_2006, grouped_2010","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#In 2006 and 2010, what were the civil status of candidates ellected?\ndf_2006_win.columns\ngrouped_2006 = df_2006_win.groupby('estado_civil')['estado_civil'].count().sort_values(ascending=False)\ngrouped_2010 =df_2010_win.groupby('estado_civil')['estado_civil'].count().sort_values(ascending=False)\ngrouped_2006, grouped_2010","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature engineering\ndf_feature = df.copy()\ndf_feature['fraud'] = df_feature['quantidade_doacoes']-df_feature['quantidade_doadores'] \ndf_feature['situacao'] = df_feature['situacao'].apply(lambda x: 1 if x==np.unique(df_feature['situacao'].values)[0] else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Getting dummies features\ndf_feature = pd.get_dummies(df_feature.drop(['partido','nome','uf','cargo', 'ocupacao'], axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Avoiding dummies trap\ndf_feature.columns\ndf_feature = df_feature.drop(['sexo_FEMININO','grau_LÊ E ESCREVE','estado_civil_VIÚVO(A)'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Correlation with independent Variable (Note: Models like RF are not linear like these)\ndf2 = df_feature.drop(['situacao'], axis=1)\ndf2.corrwith(df_feature.situacao).plot.bar(\n        figsize = (10, 10), title = \"Correlation with Situacao\", fontsize = 15,\n        rot = 90, grid = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting dataset into X and y\ndf_feature.columns\nX = df_feature.drop(['ano', 'sequencial_candidato', 'situacao'],axis=1)\ny = df_feature['situacao'] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting the Dataset into the training set and test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)\nX_train.shape\nX_test.shape\ny_train.shape\ny_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature scaling\nfrom sklearn.preprocessing import StandardScaler\nsc_x = StandardScaler()\nX_train = pd.DataFrame(sc_x.fit_transform(X_train), columns=X.columns.values)\nX_test = pd.DataFrame(sc_x.transform(X_test), columns=X.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Model Building ####\n### Comparing Models\n\n## Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlr_classifier = LogisticRegression(random_state = 0, penalty = 'l2')\nlr_classifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = lr_classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nresults = pd.DataFrame([['Logistic Regression (Lasso)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## K-Nearest Neighbors (K-NN)\n#Choosing the K value\nerror_rate= []\nfor i in range(1,40):\n    from sklearn.neighbors import KNeighborsClassifier\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))\nplt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')\nprint(np.mean(error_rate))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nkn_classifier = KNeighborsClassifier(n_neighbors=25, metric='minkowski', p= 2)\nkn_classifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = kn_classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['K-Nearest Neighbors (minkowski)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## SVM (Linear)\nfrom sklearn.svm import SVC\nsvm_linear_classifier = SVC(random_state = 0, kernel = 'linear', probability= True)\nsvm_linear_classifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = svm_linear_classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['SVM (Linear)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## SVM (rbf)\nfrom sklearn.svm import SVC\nsvm_rbf_classifier = SVC(random_state = 0, kernel = 'rbf', probability= True)\nsvm_rbf_classifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = svm_rbf_classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['SVM (RBF)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\ngb_classifier = GaussianNB()\ngb_classifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = gb_classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Naive Bayes (Gaussian)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\ndt_classifier = DecisionTreeClassifier(criterion='entropy', random_state=0)\ndt_classifier.fit(X_train, y_train)\n\n#Predicting the best set result\ny_pred = dt_classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Decision Tree', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrf_classifier = RandomForestClassifier(random_state = 0, n_estimators = 200,\n                                    criterion = 'gini')\nrf_classifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = rf_classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Random Forest Gini (n=200)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Ada Boosting\nfrom sklearn.ensemble import AdaBoostClassifier\nad_classifier = AdaBoostClassifier()\nad_classifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = ad_classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Ada Boosting', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Gradient Boosting\nfrom sklearn.ensemble import GradientBoostingClassifier\ngr_classifier = GradientBoostingClassifier()\ngr_classifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = gr_classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Gradient Boosting', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Xg Boosting\nfrom xgboost import XGBClassifier\nxg_classifier = XGBClassifier()\nxg_classifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = xg_classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Xg Boosting', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Ensemble Voting Classifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import accuracy_score\nvoting_classifier = VotingClassifier(estimators= [('lr', lr_classifier),\n                                                  ('kn', kn_classifier),\n                                                  ('svc_linear', svm_linear_classifier),\n                                                  ('svc_rbf', svm_rbf_classifier),\n                                                  ('gb', gb_classifier),\n                                                  ('dt', dt_classifier),\n                                                  ('rf', rf_classifier),\n                                                  ('ad', ad_classifier),\n                                                  ('gr', gr_classifier),\n                                                  ('xg', xg_classifier),],\nvoting='soft')\n\nfor clf in (lr_classifier,kn_classifier,svm_linear_classifier,svm_rbf_classifier,\n            gb_classifier, dt_classifier,rf_classifier, ad_classifier, gr_classifier, xg_classifier,\n            voting_classifier):\n    clf.fit(X_train,y_train)\n    y_pred = clf.predict(X_test)\n    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n\n# Predicting Test Set\ny_pred = voting_classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Ensemble Voting', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The Best Classifier\nprint('The best classifier is:')\nprint('{}'.format(results.sort_values(by='Accuracy',ascending=False).head(5)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying K-fold validation\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator=rf_classifier, X=X_train, y=y_train,cv=10)\naccuracies.mean()\naccuracies.std()\nprint(\"Randon Forest Gini (n=200) Accuracy: %0.3f (+/- %0.3f)\" % (accuracies.mean(), accuracies.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting Cumulative Accuracy Profile (CAP)\ny_pred_proba = rf_classifier.predict_proba(X=X_test)\nimport matplotlib.pyplot as plt\nfrom scipy import integrate\ndef capcurve(y_values, y_preds_proba):\n    num_pos_obs = np.sum(y_values)\n    num_count = len(y_values)\n    rate_pos_obs = float(num_pos_obs) / float(num_count)\n    ideal = pd.DataFrame({'x':[0,rate_pos_obs,1],'y':[0,1,1]})\n    xx = np.arange(num_count) / float(num_count - 1)\n    \n    y_cap = np.c_[y_values,y_preds_proba]\n    y_cap_df_s = pd.DataFrame(data=y_cap)\n    y_cap_df_s = y_cap_df_s.sort_values([1], ascending=False).reset_index(level = y_cap_df_s.index.names, drop=True)\n    \n    print(y_cap_df_s.head(20))\n    \n    yy = np.cumsum(y_cap_df_s[0]) / float(num_pos_obs)\n    yy = np.append([0], yy[0:num_count-1]) #add the first curve point (0,0) : for xx=0 we have yy=0\n    \n    percent = 0.5\n    row_index = int(np.trunc(num_count * percent))\n    \n    val_y1 = yy[row_index]\n    val_y2 = yy[row_index+1]\n    if val_y1 == val_y2:\n        val = val_y1*1.0\n    else:\n        val_x1 = xx[row_index]\n        val_x2 = xx[row_index+1]\n        val = val_y1 + ((val_x2 - percent)/(val_x2 - val_x1))*(val_y2 - val_y1)\n    \n    sigma_ideal = 1 * xx[num_pos_obs - 1 ] / 2 + (xx[num_count - 1] - xx[num_pos_obs]) * 1\n    sigma_model = integrate.simps(yy,xx)\n    sigma_random = integrate.simps(xx,xx)\n    \n    ar_value = (sigma_model - sigma_random) / (sigma_ideal - sigma_random)\n    \n    fig, ax = plt.subplots(nrows = 1, ncols = 1)\n    ax.plot(ideal['x'],ideal['y'], color='grey', label='Perfect Model')\n    ax.plot(xx,yy, color='red', label='User Model')\n    ax.plot(xx,xx, color='blue', label='Random Model')\n    ax.plot([percent, percent], [0.0, val], color='green', linestyle='--', linewidth=1)\n    ax.plot([0, percent], [val, val], color='green', linestyle='--', linewidth=1, label=str(val*100)+'% of positive obs at '+str(percent*100)+'%')\n    \n    plt.xlim(0, 1.02)\n    plt.ylim(0, 1.25)\n    plt.title(\"CAP Curve - a_r value =\"+str(ar_value))\n    plt.xlabel('% of the data')\n    plt.ylabel('% of positive obs')\n    plt.legend()\n    \n\ncapcurve(y_test,y_pred_proba[:,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## EXTRA: Confusion Matrix\ncm = confusion_matrix(y_test, y_pred) # rows = truth, cols = prediction\ndf_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))\nplt.figure(figsize = (10,7))\nsns.set(font_scale=1.4)\nsns.heatmap(df_cm, annot=True, fmt='g')\nprint(\"Test Data Accuracy: %0.4f\" % accuracy_score(y_test, y_pred)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Feature Selection\n# Recursive Feature Elimination\nfrom sklearn.feature_selection import RFE\n\n# Model to Test\nclassifier = LogisticRegression(random_state=0)\n\n# Select Best X Features\nrfe = RFE(classifier, n_features_to_select=None)\nrfe = rfe.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize the selection of the attributes\nprint(rfe.support_)\nprint(rfe.ranking_)\nX_train.columns[rfe.support_]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting Model to the Training Set\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state=0)\nclassifier.fit(X_train[X_train.columns[rfe.support_]], y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting Test Set\ny_pred = classifier.predict(X_test[X_train.columns[rfe.support_]])\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Logistic Regression (RFE)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluating Results\n#Making the confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test,y_pred)\nsns.heatmap(data=cm, annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making the classification report\nfrom sklearn.metrics import classification_report\ncr = classification_report(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying k-Fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier,\n                             X = X_train[X_train.columns[rfe.support_]],\n                             y = y_train, cv = 10)\nprint(\"Logistic Regression (RFE) Accuracy: %0.3f (+/- %0.3f)\" % (accuracies.mean(), accuracies.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The Best Classifier\nprint('The best classifier is:')\nprint('{}'.format(results.sort_values(by='Accuracy',ascending=False)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}