{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as  np\nimport matplotlib.pyplot as  plt\nimport seaborn as sns\npd.set_option('display.max_columns',None)\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df= pd.read_csv(r'/kaggle/input/heart-attack-analysis-prediction-dataset/heart.csv',header=0)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# understanding the data \nprint(df.shape)\nprint(df.dtypes)\n\nprint(df.info())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Understanding the categorical variables\na=['sex','cp','fbs','restecg','exng','slp','caa','thall']\nfor i in a:\n    print(df[i].value_counts())\n    sns.countplot(df[i])\n    plt.title([i])\n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Handling Missing  Values","metadata":{}},{"cell_type":"code","source":"# as per the data set  description the variable 'Thal  has to have  only  3  values 1,2 & 3 .  the  value  0  is  given  are  null- values.\ndf.thall=df.thall.replace(0,np.nan)\n\nprint(df.thall.mode())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.thall.fillna(df.thall.mode()[0],inplace=True)\nprint(df.thall.value_counts())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking for  other  mising values \ndf.isnull().sum().sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking for correlation between variables \nX=df[[\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]] \ncorr_df=X.corr(method=\"pearson\")\nprint(corr_df)\n\nsns.heatmap(corr_df,vmax=1.0,vmin=-1.0,annot=True)\nplt.savefig(\"heatmap.jpg\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Relationship between Sex, age and  output \nsns.barplot(x='sex',y='age',data=df,hue='output',palette='rocket')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Different chestpains  and coletrol and  output\nsns.barplot(x='cp',y='chol',data=df,hue='output')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check biasness in data \nsns.countplot(df.output)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Insight- Data is  not biased ","metadata":{}},{"cell_type":"code","source":"\nsns.pairplot(x_vars=[\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"],y_vars='age',data=df,hue='output')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splitting the Variables","metadata":{}},{"cell_type":"code","source":"X=df.values[:,0:-1]\nY=df.values[:,-1]\nY=Y.astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X.shape)\nprint(Y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing steps","metadata":{}},{"cell_type":"code","source":"# Scaling the data\nfrom sklearn.preprocessing  import StandardScaler\nscaler=StandardScaler()\nscaler.fit(X)\nX=scaler.transform(X)\nprint(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splitting the data","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.3,random_state=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlm=LogisticRegression()\nlm.fit(X_train,Y_train)\nY_predict=lm.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(list(zip(Y_test,Y_predict)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n\ncfm=confusion_matrix(Y_test,Y_predict) \nprint(cfm)\n\nprint(\"Classification report: \")\n\nprint(classification_report(Y_test,Y_predict))\n\nacc=accuracy_score(Y_test, Y_predict)\nprint(\"Accuracy of the model: \",acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tuning Logistic Regression ","metadata":{}},{"cell_type":"code","source":"y_pred_prob = lm.predict_proba(X_test)\nprint(y_pred_prob)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#the accuracy has  slightly increased  to  ","metadata":{}},{"cell_type":"code","source":"for i in np.arange(0.4,0.61,0.01):\n    predict_new=np.where(y_pred_prob[:,1]>i,1,0)\n    cfm=confusion_matrix(Y_test,predict_new)\n    total_err=cfm[0,1]+cfm[1,0]\n    print('errors at threshold:',i,':',total_err,'Type_1:',cfm[0,1],'type2_error:',cfm[1,0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#As its a sensitive data set and we want  to keep the type 2 error  minimal, we choose  0.4  ","metadata":{}},{"cell_type":"code","source":"y_pred_class=[]\nfor i in y_pred_prob[:,1]:\n    if i>0.40:\n        y_pred_class.append(1)\n    else:\n        y_pred_class.append(0)\nprint(y_pred_class)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics  import confusion_matrix, accuracy_score,classification_report\naccuracy_score= accuracy_score(Y_test,y_pred_class)\nprint('accuracy:',accuracy_score)\ncfm=confusion_matrix(Y_test,y_pred_class)\nprint('confusion_matrix',cfm)\nprint(classification_report(Y_test,y_pred_class))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"estimators=[]\nsvc_model=SVC(kernel='rbf',C=30.0,gamma=0.01)\nestimators.append(('SVM',svc_model))\nmodel_RF=RandomForestClassifier(n_estimators=35,random_state=10,max_depth=10,min_samples_leaf=5,min_samples_split=6)\nestimators.append(('RF',model_RF))\nmodel_LR=LogisticRegression()\nestimators.append(('LR',model_LR))\nmodel_knn=model_knn=KNeighborsClassifier(n_neighbors=int(np.sqrt(len(X_train))),metric='manhattan')\nestimators.append(('KNN',model_knn))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ensemble=VotingClassifier(estimators)\nensemble.fit(X_train,Y_train)\nY_pred=ensemble.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(list(zip(Y_test,Y_pred)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics  import confusion_matrix, accuracy_score,classification_report\naccuracy_score= accuracy_score(Y_test,Y_pred)\nprint('accuracy:',accuracy_score)\ncfm=confusion_matrix(Y_test,Y_pred)\nprint('confusion_matrix',cfm)\nprint(classification_report(Y_test,Y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# USING THE  MODELS  GIVING  BETTER RECALL VALUE  FOR  CLASS1 ","metadata":{}},{"cell_type":"code","source":"estimators=[]\nsvc_model=SVC(kernel='rbf',C=30.0,gamma=0.01)\nestimators.append(('SVM',svc_model))\nmodel_RF=RandomForestClassifier(n_estimators=35,random_state=10,max_depth=10,min_samples_leaf=5,min_samples_split=6)\nestimators.append(('RF',model_RF))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ensemble=VotingClassifier(estimators)\nensemble.fit(X_train,Y_train)\nY_pred=ensemble.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(list(zip(Y_test,Y_pred)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics  import confusion_matrix, accuracy_score,classification_report\naccuracy_score= accuracy_score(Y_test,Y_pred)\nprint('accuracy:',accuracy_score)\ncfm=confusion_matrix(Y_test,Y_pred)\nprint('confusion_matrix',cfm)\nprint(classification_report(Y_test,Y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using Ensemble  model  with SVM  and  Random forest Classifiers\nsns.heatmap(cfm,annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}