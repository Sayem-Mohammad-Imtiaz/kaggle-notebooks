{"cells":[{"metadata":{},"cell_type":"markdown","source":"I'm still learning all the in-and-outs of ML and creating/sharing notebooks. All constructive feedback would be greatly appreciated.\n\n# Import the data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\ndata_filepath = '../input/jobs-on-naukricom/home/sdf/marketing_sample_for_naukri_com-jobs__20190701_20190830__30k_data.csv'\ndf = pd.read_csv(data_filepath)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Analyse the dataset using pandas profiling","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Initial data analysis\n## Pandas profiling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas_profiling\npandas_profile = pandas_profiling.ProfileReport(df, progress_bar=False)\npandas_profile.to_widgets()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pandas profiling results\n### Categorial data\nAlthough many columns seem to be categorical, we can see they still show a high cardinality. For example \"Key skills\" has 26909 distinct values for 30000 data entries.\nLooking at the data this seems to have two main reasons:\n1. Categories are not standardized\n2. Multi-selection of categories is group together as comma-seperated into the same column\n\n### Missing values\nWe have missing values for several columns. Usually around 2% per column, with a couple of outliers. \"Key Skills\" with 4.2% and \"Role Category\" with 7.7% are most notable.\n\n### Correlations/Interactions\nDue to the nature of the data (non-numerical), Pandas profiling doesn't perform any correlation analysis\n\n# Specific column analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's look at the freshness of the data using the crawl timestamp\n\nimport plotly.express as px\ndf['Crawl Timestamp_dt'] = pd.to_datetime(df['Crawl Timestamp']) #Convert to Pandas DateTime\npx.box(df, y=\"Crawl Timestamp_dt\", points=\"all\", hover_data=[\"Uniq Id\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Crawling seems to have occured during two clusters: July 4 2019-July 8 2019 and Aug 4 2019-Aug 7 2019","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's now look at categorical columns such as Functional Area\n\nfunc_area_df = (df['Functional Area'].str.split(' , ', expand=True) #Multiple categories are combined in the column as comma-separated so we first need to split this\n     .stack() # We then stack them again as they'd otherwise be represented as separate columns\n     .value_counts() # As last transformation we count all the values\n    )\n\n# We only want to show the top values, and combine the smaller values together\ntop_func_area_df = pd.concat([func_area_df[:20], pd.Series(func_area_df[20:].sum(), index=[\"Others\"])])\n\n\nfig = px.pie(top_func_area_df, values=top_func_area_df.values, names=top_func_area_df.index)\nfig.update_traces(textposition='inside', textinfo='percent+label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's do the same with Industry and Key Skills\n\nindustry_df = (df['Industry'].str.split(', ', expand=True) #Multiple categories are combined in the column as comma-separated so we first need to split this\n     .stack() # We then stack them again as they'd otherwise be represented as separate columns\n     .value_counts() # As last transformation we count all the values\n    )\n\n# We only want to show the top values, and combine the smaller values together\ntop_industry_df = pd.concat([industry_df[:30], pd.Series(industry_df[30:].sum(), index=[\"Others\"])])\npx.bar(top_industry_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We notice there is a large amount of IT and Software jobs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's do the same with Industry and Key Skills\n\nkey_skills_df = (df['Key Skills'].str.split('\\| ', expand=True) #Multiple categories are combined in the column as comma-separated so we first need to split this\n     .stack() # We then stack them again as they'd otherwise be represented as separate columns\n     .value_counts() # As last transformation we count all the values\n    )\n\n# We only want to show the top values, and combine the smaller values together\ntop_key_skills_df = pd.concat([key_skills_df[:30], pd.Series(key_skills_df[30:].sum(), index=[\"Others\"])])\nfig = px.bar(top_key_skills_df)\nfig.update_layout(yaxis_type=\"log\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We notice a large amount of different skills required, without any real outlier.\nAs already noticed when analyzing Industry, we notice a large amount of programming skills in the top","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\nwc = WordCloud(max_words=30, background_color='white', width = 2400, height = 800, min_font_size = 10)\n\nplt.imshow(wc.generate_from_frequencies(df['Role'].astype(str).value_counts()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Suggested further work\nA more detailed look at job experience and job salary would be useful, especially converting the string value to a range and analyzing this would be useful","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}