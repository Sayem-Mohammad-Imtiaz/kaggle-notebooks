{"cells":[{"metadata":{},"cell_type":"markdown","source":"# What is PyTorch?\n* Its a machine learning library used for NLP, computer vision,etc.\n* Provides Tensor computing with GPU support.\n\n# Modules in Pytorch\n*  **Autograd module:**\nPyTorch uses a method called automatic differentiation. A recorder records what operations have performed, and then it replays it backward to compute the gradients. This method is especially powerful when building neural networks to save time on one epoch by calculating differentiation of the parameters at the forward pass.\n\n* **Optim module:**\ntorch.optim is a module that implements various optimization algorithms used for building neural networks. \n\n* **nn module:**\nPyTorch autograd makes it easy to define computational graphs and take gradients, but raw autograd can be a bit too low-level for defining complex neural networks. This is where the nn module can help.\n\n# Topics covered in this notebook\n* Handwritten Digits Classification (Numerical Data)-**Digit MNIST**\n* Objects Image Classification (Image Data, CNN)-**Sign Language MNIST**\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Handwritten Digits Classification (Numerical Data)-Digit MNIST","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Importing lib","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport os\nimport math\n%matplotlib inline\nimport time\n\n#pytorch utility imports\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torchvision.utils import make_grid\n\n#neural net imports\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\nstart = torch.cuda.Event(enable_timing=True) #time measure during cuda training\nend = torch.cuda.Event(enable_timing=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importing dataset ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('../input/mnist-in-csv/mnist_test.csv')\ntrain_df = pd.read_csv('../input/mnist-in-csv/mnist_train.csv')\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Separating labels and features (pixel)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = train_df['label'].values # converting to numpy also\n\ntest_labels=test_df['label'].values\ntrain_images = (train_df.iloc[:,1:].values).astype('float32')\ntest_images = (test_df.iloc[:,1:].values).astype('float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"train images shape\",train_images.shape)\nprint(\"train labels shape\",train_labels.shape)\nprint(\"test images shape\",test_images.shape)\nprint(\"test labels shape\",test_labels.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reshape features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images = train_images.reshape(train_images.shape[0], 28, 28)\ntest_images = test_images.reshape(test_images.shape[0], 28, 28)\nprint(train_images.shape)\nprint(test_images.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualize some training images","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#train samples\nfor i in range(6, 9):\n    plt.subplot(330 + (i+1))\n    plt.imshow(train_images[i].squeeze(), cmap=plt.get_cmap('gray'))\n    plt.title(train_labels[i])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Convert numpy data into tensor for PyTorch\n* torch.tensor default uses float\n* feature are divided by 255 to change them into suitable range","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images_tensor = torch.tensor(train_images)/255.0 #default torch.FloatTensor\ntrain_labels_tensor = torch.tensor(train_labels)\ntrain_tensor = TensorDataset(train_images_tensor, train_labels_tensor)\n\ntest_images_tensor = torch.tensor(test_images)/255.0\ntest_labels_tensor = torch.tensor(test_labels)\ntest_tensor = TensorDataset(test_images_tensor, test_labels_tensor)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DataLoader for train and test \n* Batch size 16","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loader = DataLoader(train_tensor, batch_size=16, num_workers=2, shuffle=True)\ntest_loader = DataLoader(test_images_tensor, batch_size=16, num_workers=2, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model important terms\n\n* nn.Linear : Applies a linear transformation (in_features, out_features)\n* nn.BatchNorm1d : By normalizing the inputs we are able to bring all the inputs features to the same scale\n* nn.functional.relu : Applies the rectified linear unit function element-wise\n* nn.functioncal.dropout : Dropout and offers a very computationally cheap and remarkably effective regularization method to reduce overfitting and improve generalization error in deep neural networks of all kinds.\n\n### **Model used**\n\nModel(\n\n  (fc1): Linear(in_features=784, out_features=548, bias=True)\n  \n  (bc1): BatchNorm1d(548, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  \n  (fc2): Linear(in_features=548, out_features=252, bias=True)\n  \n  (bc2): BatchNorm1d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  \n  (fc3): Linear(in_features=252, out_features=10, bias=True)\n  \n)\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        \n        self.fc1 = nn.Linear(784, 548)\n        self.bc1 = nn.BatchNorm1d(548)\n        \n        self.fc2 = nn.Linear(548, 252)\n        self.bc2 = nn.BatchNorm1d(252)\n        \n        self.fc3 = nn.Linear(252, 10)\n        \n        \n    def forward(self, x):\n        x = x.view((-1, 784))\n        h = self.fc1(x)\n        h = self.bc1(h)\n        h = F.relu(h)\n        h = F.dropout(h, p=0.5, training=self.training)\n        \n        h = self.fc2(h)\n        h = self.bc2(h)\n        h = F.relu(h)\n        h = F.dropout(h, p=0.2, training=self.training)\n        \n        h = self.fc3(h)\n        out = F.log_softmax(h)\n        return out\n\nmodel = Model()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking for cuda availability","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\nif (device.type=='cuda'):\n    model.cuda() # convert model to cuda model\n\n    \noptimizer = optim.Adam(model.parameters(), lr=0.001) #adam optimizer from optim module","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if (device.type=='cuda'):\n    start.record() #timer start\n\nmodel.train()\n\n\nlosses = []\nfor epoch in range(20):\n    for batch_idx, (data, target) in enumerate(train_loader):\n        # Get Samples\n        if (device.type=='cuda'):\n            data, target = Variable(data.cuda()), Variable(target.cuda())\n        else:\n            data, target = Variable(data), Variable(target) # making group of 16\n            \n        \n        # Init\n        optimizer.zero_grad() #making gradient zero for new mini-batch. \n\n        # Predict\n        y_pred = model(data) \n         \n        \n        # Calculate loss\n        loss = F.cross_entropy(y_pred, target)\n        losses.append(loss.data)\n        \n        # Backpropagation\n        loss.backward()  #It computes gradient of loss w.r.t all the parameters and store them in (parameter.grad) attribute.\n        optimizer.step() #optimizer.step() updates all the parameters based on (parameter.grad)\n        \n        \n        # Display\n        #if batch_idx % 100 == 1:\n        print('\\r Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format( epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.data), end='')\n            \n    print()\n    \nif (device.type=='cuda'):\n    end.record()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predicting the output and accuracy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"if (device.type=='cuda'):\n    evaluate_x=test_images_tensor.cuda()\n    evaluate_y=test_labels_tensor.cuda()\nelse:\n    evaluate_x=test_images_tensor\n    evaluate_y=test_labels_tensor\n    \n\noutput = model(evaluate_x)\n\npred = output.data.max(1)[1]\nd = pred.eq(evaluate_y.data).cpu()\na=(d.sum().data.cpu().numpy())\nb=d.size()\nb=torch.tensor(b)\nb=(b.sum().data.cpu().numpy())\naccuracy = a/b\n\nprint('Accuracy:', accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### Time used for training if cuda is used","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"if (device.type=='cuda'):\n    torch.cuda.synchronize()\n    print(start.elapsed_time(end)/1000,\"sec\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Objects Image Classification (Image Data, CNN)-Sign Language MNIST","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Loading dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('../input/sign-language-mnist/sign_mnist_test/sign_mnist_test.csv')\ntrain_df = pd.read_csv('../input/sign-language-mnist/sign_mnist_train/sign_mnist_train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print((train_df['label'].unique()).shape )# There are 24 possible labels, 9=J and 25=Z require motion so they are absent.\nprint(np.sort(train_df['label'].unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Separating labels and features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = train_df['label'].values\ntest_labels=test_df['label'].values\ntrain_images = (train_df.iloc[:,1:].values).astype('float32')\ntest_images = (test_df.iloc[:,1:].values).astype('float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"train images shape\",train_images.shape)\nprint(\"train labels shape\",train_labels.shape)\nprint(\"test images shape\",test_images.shape)\nprint(\"test labels shape\",test_labels.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reshape features\n*Note: For images reshape will be in 4D*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images = train_images.reshape(train_images.shape[0],1, 28, 28)\ntest_images = test_images.reshape(test_images.shape[0],1, 28, 28)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_images.shape)\nprint(test_images.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Changing to Tensor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images_tensor = torch.tensor(train_images)/255.0 #default torch.FloatTensor\ntrain_labels_tensor = torch.tensor(train_labels)\ntrain_tensor = TensorDataset(train_images_tensor, train_labels_tensor)\n\ntest_images_tensor = torch.tensor(test_images)/255.0\ntest_labels_tensor = torch.tensor(test_labels)\ntest_tensor = TensorDataset(test_images_tensor, test_labels_tensor)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DataLoader","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loader = DataLoader(train_tensor, batch_size=16, num_workers=2, shuffle=True)\ntest_loader = DataLoader(test_images_tensor, batch_size=16, num_workers=2, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model\nNet(\n\n  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n  \n  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n  \n  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n  \n  (fc1): Linear(in_features=128, out_features=512, bias=True)\n  \n  (fc2): Linear(in_features=512, out_features=26, bias=True)\n  \n)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn.functional as F\nfrom torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\nfrom torch.optim import Adam, SGD\n\nclass Net(nn.Module):                                           # class Net inherits from predefined Module class in torch.nn\n    def __init__(self):                                         # calling constructor of  parent class\n        super().__init__()                                     \n        \n        \n        self.conv1 = nn.Conv2d(1,32,3)              # 2d convolution layer : (input : 1 image , output : 32 channels , kernel size : 3*3)\n        self.conv2 = nn.Conv2d(32,64,3)\n        self.conv3 = nn.Conv2d(64,128,3)\n        \n        self.linear_in = None                      # used to calculate input of first linear layer by passing fake data through 2d layers\n        x = torch.rand(28,28).view(-1,1,28,28)     # using convs function\n        self.convs(x)\n    \n        self.fc1 = nn.Linear(self.linear_in,512)\n        self.fc2 = nn.Linear(512,26)\n        \n    def convs(self,x):\n        x = F.max_pool2d(F.relu(self.conv1(x)) , (2,2) )      # relu used for activation function \n        x = F.max_pool2d(F.relu(self.conv2(x)) , (2,2) )      # max_pool2d for max pooling results of each kernel with window size 2*2\n        x = F.max_pool2d(F.relu(self.conv3(x)) , (2,2) )\n        \n        if self.linear_in == None:\n            self.linear_in = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]  # input of first linear layer is multiplication of dimensions of ouput \n        return x                                                        # tensor of the 2d layers\n    \n    def forward(self,x):                                    # forward pass function uses the convs function to pass through 2d layers\n        x = self.convs(x)\n        x = x.view(-1,self.linear_in)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        x = F.log_softmax(x ,dim = -1)                     # log_softmax for finding output neuron with highest value\n        return x\n    \nnet = Net()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(net)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\nif (device.type=='cuda'):\n    model.cuda() # CUDA\n\nnet.to(device)\n\n\nimport torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(net.parameters(), lr=0.001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if (device.type=='cuda'):\n    start.record() \n    \nloss_log = []\nfor epoch in range(20): # loop over dataset multiple times\n    running_loss = 0.0\n    for i, (data,target) in enumerate(train_loader):\n\n        \n        if (device.type=='cuda'):\n            inputs,labels= Variable(data.cuda()), Variable(target.cuda())\n        else:\n            inputs,labels= Variable(data), Variable(target)\n       \n        \n        \n        # zero parameter gradients\n        optimizer.zero_grad()\n        \n        # forward + backward + optimize\n        outputs = net(inputs)\n\n        loss =  F.cross_entropy(outputs, labels)\n        #print(loss)\n        \n   \n        \n        loss.backward()\n        optimizer.step()\n        \n      \n        #if i % 100 == 1:\n        print('\\r Train Epoch: {} [{}/{} ({:.0f}%)] \\tLoss: {:.6f}'.format( epoch, i * len(data), len(train_loader.dataset),\n                                                                           100. * i / len(train_loader), loss.data), end='')\n        \n    print(\"\")\n                \nprint('Finished Training')\nif (device.type=='cuda'):\n    end.record()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if (device.type=='cuda'):\n    evaluate_x=test_images_tensor.cuda()\n    evaluate_y=test_labels_tensor.cuda()\nelse:\n    evaluate_x=test_images_tensor\n    evaluate_y=test_labels_tensor\n    \n\noutput = net(evaluate_x)\n\npred = output.data.max(1)[1]\nd = pred.eq(evaluate_y.data).cpu()\na=(d.sum().data.cpu().numpy())\nb=d.size()\nb=torch.tensor(b)\nb=(b.sum().data.cpu().numpy())\naccuracy = a/b\n\nprint('Accuracy:', accuracy*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if (device.type=='cuda'):\n    torch.cuda.synchronize()\n    print(start.elapsed_time(end)/1000,\"sec\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calculating the F1 score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\nprint(\"f1 score =\",f1_score(test_labels, pred.cpu().numpy(), average='macro'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Other use case to be covered in upcoming notebook\n\n* R-CNN\n* Sentiment Text Classification (Text Data, RNN)\n* Image Style Transfer (Transfer Learning)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\nLets get connected on [Linkedin](https://www.linkedin.com/in/manzoor-bin-mahmood/)\n\nVisit my [website](https://manzoormahmood.github.io/) \n\n### Please **upvote** my work if you like it.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}