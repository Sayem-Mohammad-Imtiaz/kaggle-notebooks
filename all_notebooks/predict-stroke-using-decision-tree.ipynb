{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Our  goal is to predict whether a given person had a stroke or not\n## We can see from the description of data that we have a lot of cases when a given person didn't have a stroke \n### So our main goal will be to build machine learning algorithm that will in some cases predict that a given person had a stroke even if we get a little bit lower prediction success rate (we can get a very high prediction rate if we predict that everyone didn't have a stroke)"},{"metadata":{},"cell_type":"markdown","source":"## Reading data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# There are some missing values that we need to take care for\n# Let's start by dropping the id column because it doesn't give any info related to strokes\ndf.drop('id', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['stroke'].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# So we mostly have cases that didn't result in a stroke","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fill the missing bmi values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['bmi'].hist(bins=40)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Most people have bmi ranging from 15 to 40\n# I think that the best way to fill bmi is by using the average value of bmi for every age and gender\nmean_bmi = df.groupby(['age', 'gender']).mean()['bmi']\nmean_bmi.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\ndef fill_bmi(df, mean_bmi):\n    if math.isnan(df['bmi']): \n        return mean_bmi[df['age']][df['gender']]\n    else:\n        return df['bmi']\n    \ndf['bmi'] = df.apply(fill_bmi, axis=1, args=(mean_bmi, ))\ndf.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# There is one missing value. We need to check why we still have one missing value\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['bmi'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(str(mean_bmi[0.48]['Male']) + '\\n')\nprint(mean_bmi[0.48])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can see that this male is the only male at a given age so the mean in his age group is equal to Nan\n# We will manually assign the value for this male by using female bmi mean for this age group\ndf.loc[2030, 'bmi'] = mean_bmi[0.48]['Female']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['bmi'].hist(bins=40)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory data analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df.corr())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['gender'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Time to verify what this other gender means\ndf[df['gender'] == 'Other']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The best thing we can do here is to get drop this row\ndf.drop([3116], inplace=True)\ndf['gender'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(font_scale=1)\nplt.figure(figsize=(12, 6))\nax = sns.countplot(data=df, x='stroke', hue='gender')\nplt.title('Distribution of strokes based on gender')\nplt.xlabel('Stroke')\nplt.ylabel('How many people had a stroke')\nfor column in ax.patches:\n    ax.annotate(column.get_height(), (column.get_x() + 0.15, column.get_height() + 50))\nplt.ylim(0, 3100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df.groupby(['gender', 'stroke']).count()['age']\nprint('Percentage of female that had a stroke: ', (data['Female'][1]/(data['Female'][0] + data['Female'][1])) * 100)\nprint('Percentage of male that had a stroke: ', (data['Male'][1]/(data['Male'][0] + data['Male'][1])) * 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that men have higher probability to get a stroke than women but not by much"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['age'].hist(bins=40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"number_of_strokes = df.groupby('age').sum()\nwhole_population = df.groupby('age').count()\npercentage_had_a_stroke = (number_of_strokes['stroke']/whole_population['stroke']) * 100\n\nplt.figure(figsize=(12, 8))\nplt.plot(percentage_had_a_stroke.index, percentage_had_a_stroke.values)\nplt.title('Distribution of strokes based on age', fontsize=25)\nplt.xlabel('Age', fontsize=15)\nplt.ylabel('Percentage of people that had a stroke', fontsize=15)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the older we get the more likely we are to have a stroke"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"def get_ilnesses_of_given_patient(df):\n    if df['heart_disease'] == 1 and df['hypertension'] == 1:\n        return 'Heart disease and hypertension'\n    elif df['heart_disease'] == 1:\n        return 'Heart disease'\n    elif df['hypertension'] == 1:\n        return 'Hypertension'\n    else:\n        return 'No ilness'\n\nsns.set(font_scale=2)\nfig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(18, 24))\nax1.set_ylim(0, 5000)\nax2.set_ylim(0, 5000)\nax3.set_ylim(0, 5000)\nfig.suptitle('Distribution of strokes based on other ilnesses', fontsize=36)\n\nsns.countplot(data=df, x='hypertension', hue='stroke', ax=ax1)\nsns.countplot(data=df, x='heart_disease', hue='stroke', ax=ax2)\n\ndata = df[['heart_disease', 'hypertension', 'stroke']] \ndata = data.assign(ilness = data.apply(get_ilnesses_of_given_patient, axis=1))\nsns.countplot(data=data, x='ilness', hue='stroke', ax=ax3)\n\nfor column in ax1.patches:\n    ax1.annotate(column.get_height(), (column.get_x() + 0.15, column.get_height() + 50))\n    \nfor column in ax2.patches:\n    ax2.annotate(column.get_height(), (column.get_x() + 0.15, column.get_height() + 50))\n    \nfor column in ax3.patches:\n    ax3.annotate(column.get_height(), (column.get_x() + 0.15, column.get_height() + 50))\n\nfig.tight_layout()\nfig.subplots_adjust(top=0.94)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that people that had ilnesses are much more likely to have a stroke. 1/4 of people that have both hypertension and heart disease had a stroke, 1/6 of people that had heart disease had a stroke and 1/8 of people that had hypertension had a stroke. This is a lot compared to around 3,5% of people that had a stroke but were not ill beforehand"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reseting the font scale for feature\nsns.set(font_scale=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['ever_married'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('ever_married').describe()['age']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can see that this data also include kids that cannot get married\n# To see whether being married is correlated to strokes we will analyse this data but with relation to age\nplt.figure(figsize=(12, 8))\nsns.lineplot(data=df, x='age', y='stroke', hue='ever_married')\nplt.ylabel('Chance of having a stroke')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that people that got married are less likely to have a stroke than people than people that didn't get married"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df['work_type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.countplot(data=df, x='work_type', hue='stroke')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('work_type').sum()['stroke']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['stroke'] == 0].groupby('work_type').count()['gender']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(df.groupby('work_type').sum()['stroke'] / df[df['stroke'] == 0].groupby('work_type').count()['gender']) * 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that children almost never have a stroke (the same goes for never worked but this may be because we have a very low amount of people in this data). We see that self-employed people are most likely to have a stroke compared to other work_types"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Residence_type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It's good that the amount of people in residence_type is well balanced.\nsns.countplot(data=df, x='Residence_type', hue='stroke')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can clearly see that residence type itself doesn't tell correlate to the strokes"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['avg_glucose_level'].hist(bins=40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['bmi'].hist(bins=40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('stroke').mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that people with higher average glucose level or bmi are more likely to have a stroke"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nax = sns.scatterplot(data=df[df['stroke'] == 0], x='avg_glucose_level', y='bmi', alpha=0.3, label='No Stroke')\nsns.scatterplot(data=df[df['stroke'] == 1], x='avg_glucose_level', y='bmi', alpha=1, ax=ax, label='Stroke')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that if we have a high glucose level than we are more likely to have a stroke."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['smoking_status'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Quick check why there are many Unknown features in smoking_status\ndf[df['smoking_status'] == 'Unknown'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# So Unknown is just a mix of people that for which we don't have any knowledge about their smoking status\nsns.countplot(data=df, x='smoking_status', hue='stroke')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(df.groupby('smoking_status').sum()['stroke'] / df[df['stroke'] == 0].groupby('smoking_status').count()['gender']) * 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the people that used to smoke have the highest chance of having a stroke.\nSuprisingly people that don't have smoking status have the lowest chance of having a stroke"},{"metadata":{},"cell_type":"markdown","source":"# Preparing data for training"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Time to create some dummy variables\ndata = pd.get_dummies(df['gender'], drop_first=True)\ndf = pd.concat([df, data], axis=1)\n\ndata = pd.get_dummies(df['ever_married'], drop_first=True)\ndf = pd.concat([df, data], axis=1)\n\ndata = pd.get_dummies(df['work_type'], drop_first=True)\ndf = pd.concat([df, data], axis=1)\n\ndata = pd.get_dummies(df['Residence_type'], drop_first=True)\ndf = pd.concat([df, data], axis=1)\n\ndata = pd.get_dummies(df['smoking_status'], drop_first=True)\ndf = pd.concat([df, data], axis=1)\n\ndf.drop(['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Time to do some feature scaling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\nX = df.drop('stroke', axis=1)\ny = df['stroke']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nnorm = MinMaxScaler()\nnorm.fit(X_train)\nX_train = norm.transform(X_train)\nX_test = norm.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that our data got scaled properly"},{"metadata":{},"cell_type":"markdown","source":"# Creating machine learning algorithm"},{"metadata":{},"cell_type":"markdown","source":"## Importing needed libraries for all machine learning algorithms"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report, f1_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lg = LogisticRegression()\nlg.fit(X_train, y_train)\npredictions = lg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test, predictions))\nprint()\nprint(classification_report(y_test, predictions))\nprint()\nprint('F1 score: ', f1_score(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that logistic regression always predicted that a given person didn't have a stroke.\nSo it's a terrible machine learning algorithm for this data"},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = DecisionTreeClassifier(random_state=101)\ndt.fit(X_train, y_train)\npredictions = dt.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test, predictions))\nprint()\nprint(classification_report(y_test, predictions))\nprint()\nprint('F1 score: ', f1_score(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Decision tree didn't do a very good job here but at least it predicted that some people had a strok"},{"metadata":{},"cell_type":"markdown","source":"# Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rt = RandomForestClassifier(random_state=101)\nrt.fit(X_train, y_train)\npredictions = rt.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test, predictions))\nprint()\nprint(classification_report(y_test, predictions))\nprint()\nprint('F1 score: ', f1_score(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest did very similaras the logistic regression"},{"metadata":{},"cell_type":"markdown","source":"# Neural networks"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(12, input_dim=len(df.columns)-1, activation='relu'))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train, epochs=150, batch_size=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(X_test).round()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test, predictions))\nprint()\nprint(classification_report(y_test, predictions))\nprint()\nprint('F1 score: ', f1_score(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Neural networks did a little bit better job than random forest but still not good enough "},{"metadata":{},"cell_type":"markdown","source":"# Improving machine learning algorithm"},{"metadata":{},"cell_type":"markdown","source":"Decision tree worked the best so we will try to improve it"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = dt.predict(X_train)\npredictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_train, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_train, predictions))\nprint()\nprint(classification_report(y_train, predictions))\nprint()\nprint('F1 score: ', f1_score(y_train, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Decision tree perfectly fitted our training data but did not do such a good job at the test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = DecisionTreeClassifier(splitter='random', random_state=101)\ndt.fit(X_train, y_train)\npredictions = dt.predict(X_test)\nprint(confusion_matrix(y_test, predictions))\nprint()\nprint(classification_report(y_test, predictions))\nprint('F1 score: ', f1_score(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = dt.predict(X_train)\nprint(confusion_matrix(y_train, predictions))\nprint()\nprint(classification_report(y_train, predictions))\nprint()\nprint('F1 score: ', f1_score(y_train, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Better fit on the test data but we can still improve it"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Time to set max_depth to the optimal value\ndef max_depths_accuracy():\n    max_depth_train = {}\n    max_depth_test = {}\n    for i in range(1, 100):\n        dt = DecisionTreeClassifier(splitter='random', max_depth=i, random_state=101)\n        dt.fit(X_train, y_train)\n        \n        predictions = dt.predict(X_train)\n        f1 = f1_score(y_train, predictions)\n        max_depth_train[i] = f1\n        \n        predictions = dt.predict(X_test)\n        f1 = f1_score(y_test, predictions)\n        max_depth_test[i] = f1\n    \n    return max_depth_train, max_depth_test\n    \nmax_depth_train, max_depth_test = max_depths_accuracy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.lineplot(x=max_depth_train.keys(), y=max_depth_train.values())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.lineplot(x=max_depth_test.keys(), y=max_depth_test.values())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_depth_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_depth_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This won't really solve our problem but we will keep max_depth at 22 for future improvements"},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = DecisionTreeClassifier(splitter='random', max_depth=22, random_state=101)\ndt.fit(X_train, y_train)\npredictions = dt.predict(X_test)\nprint(confusion_matrix(y_test, predictions))\nprint()\nprint(classification_report(y_test, predictions))\nprint('F1 score: ', f1_score(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Time to set min_samples_split to the optimal value\ndef min_samples_split_accuracy():\n    min_samples_split_train = {}\n    min_samples_split_test = {}\n    for i in range(2, 50):\n        dt = DecisionTreeClassifier(splitter='random', max_depth=22, min_samples_split=i, random_state=101)\n        dt.fit(X_train, y_train)\n        \n        predictions = dt.predict(X_train)\n        f1 = f1_score(y_train, predictions)\n        min_samples_split_train[i] = f1\n        \n        predictions = dt.predict(X_test)\n        f1 = f1_score(y_test, predictions)\n        min_samples_split_test[i] = f1\n    \n    return min_samples_split_train, min_samples_split_test\n\nmin_samples_split_train, min_samples_split_test = min_samples_split_accuracy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.lineplot(x=min_samples_split_train.keys(), y=min_samples_split_train.values())","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.lineplot(x=min_samples_split_test.keys(), y=min_samples_split_test.values())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"min_samples_split_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"min_samples_split_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again no improvement"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Maybe min_samples_leaf will improve the algorithm\ndef min_samples_leaf_accuracy():\n    min_samples_leaf_train = {}\n    min_samples_leaf_test = {}\n    for i in range(1, 30):\n        dt = DecisionTreeClassifier(splitter='random', max_depth=22, min_samples_split=2, min_samples_leaf=i, random_state=101)\n        dt.fit(X_train, y_train)\n        \n        predictions = dt.predict(X_train)\n        f1 = f1_score(y_train, predictions)\n        min_samples_leaf_train[i] = f1\n        \n        predictions = dt.predict(X_test)\n        f1 = f1_score(y_test, predictions)\n        min_samples_leaf_test[i] = f1\n    \n    return min_samples_leaf_train, min_samples_leaf_test\n\nmin_samples_leaf_train, min_samples_leaf_test = min_samples_leaf_accuracy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.lineplot(x=min_samples_leaf_train.keys(), y=min_samples_leaf_train.values())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.lineplot(x=min_samples_leaf_test.keys(), y=min_samples_leaf_test.values())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"min_samples_leaf_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"min_samples_leaf_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Maybe criterion will change something\ndt = DecisionTreeClassifier(criterion='entropy', splitter='random',  max_depth=22, min_samples_split=2, \n                            min_samples_leaf=1, random_state=101)\ndt.fit(X_train, y_train)\npredictions = dt.predict(X_test)\nprint(confusion_matrix(y_test, predictions))\nprint()\nprint(classification_report(y_test, predictions))\nprint('F1 score: ', f1_score(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We stay at gini criterion\ndt = DecisionTreeClassifier(criterion='gini', splitter='random',  max_depth=22, min_samples_split=2, \n                            min_samples_leaf=1, random_state=101)\ndt.fit(X_train, y_train)\npredictions = dt.predict(X_test)\nprint(confusion_matrix(y_test, predictions))\nprint()\nprint(classification_report(y_test, predictions))\nprint('F1 score: ', f1_score(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training our improved machine learning algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = DecisionTreeClassifier(criterion='gini', splitter='random',  max_depth=22, min_samples_split=2, \n                                min_samples_leaf=1, random_state=101)\ndt.fit(X_train, y_train)\npredictions = dt.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test, predictions))\nprint()\nprint(classification_report(y_test, predictions))\nprint('F1 Score: ', f1_score(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We've got around 0.22 F1 score. That's a pretty good score for the dataset where we mostly had people that didn't have a stroke"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}