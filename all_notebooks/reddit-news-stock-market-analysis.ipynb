{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-25T00:20:00.316841Z","iopub.execute_input":"2021-05-25T00:20:00.317169Z","iopub.status.idle":"2021-05-25T00:20:00.336507Z","shell.execute_reply.started":"2021-05-25T00:20:00.31714Z","shell.execute_reply":"2021-05-25T00:20:00.3356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime as dt\nimport os \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport altair as alt\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\n#nltk.download('punkt')\n#nltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.corpus import stopwords\n#nltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:20:00.396582Z","iopub.execute_input":"2021-05-25T00:20:00.396958Z","iopub.status.idle":"2021-05-25T00:20:02.253082Z","shell.execute_reply.started":"2021-05-25T00:20:00.396925Z","shell.execute_reply":"2021-05-25T00:20:02.25205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read the datasets into dataframes","metadata":{}},{"cell_type":"code","source":"reddit_df = pd.read_csv(\"/kaggle/input/reddit-news/rednewswscpt2.csv\")\nassert reddit_df.created.min() == \"2019-11-14 01:43:54\"\nassert reddit_df.created.max() == \"2021-04-29 23:58:58\"\nassert reddit_df.shape == (1059314, 6)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:20:02.254722Z","iopub.execute_input":"2021-05-25T00:20:02.255288Z","iopub.status.idle":"2021-05-25T00:20:13.787909Z","shell.execute_reply.started":"2021-05-25T00:20:02.255243Z","shell.execute_reply":"2021-05-25T00:20:13.787142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reddit_df.sample(2)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:20:13.789712Z","iopub.execute_input":"2021-05-25T00:20:13.790247Z","iopub.status.idle":"2021-05-25T00:20:13.841829Z","shell.execute_reply.started":"2021-05-25T00:20:13.790205Z","shell.execute_reply":"2021-05-25T00:20:13.84086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"using_original_sp500 = True\nstock_df = pd.read_csv(\"/kaggle/input/original-sp-500/sp500.csv\", index_col=0).rename({\"index\":\"date\"}, axis=1)\nassert stock_df.shape==(359073, 8)\nassert len(stock_df.groupby(\"stock\").count().reset_index().stock.unique()) == 429","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:20:13.843505Z","iopub.execute_input":"2021-05-25T00:20:13.844106Z","iopub.status.idle":"2021-05-25T00:20:14.786081Z","shell.execute_reply.started":"2021-05-25T00:20:13.844062Z","shell.execute_reply":"2021-05-25T00:20:14.785111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Add change (field+\"_chg\") and percent-change (field+\"_per_chg\") fields to the Stock Dataset\n\nThe following takes quite a while (over 90 minutes on Kaggle) to complete, and can be circumvented by using the already-modified dataset included. Uncomment the next cell to skip the manipulation.","metadata":{}},{"cell_type":"code","source":"### Uncomment the below line to skip this manipulation cell and load the preprocessed data from a file\nusing_original_sp500 = False\n\nif using_original_sp500:\n    # set a list of the fields to mess with in the dataframe \n    fields = ['low', 'open', 'vol', 'high', 'close', 'adj_close']\n\n    stocks = stock_df.stock.unique()\n\n    for stock in stocks:\n\n        start_index = stock_df.where(stock_df.stock == stock).first_valid_index()\n        last_index = stock_df.where(stock_df.stock == stock).last_valid_index()\n\n        # iterate through each field\n        for field in fields:\n            # for each field, set the \"change\" fields for the first row of each stock\n            #   to zero, as there has been no change from the prior row (since I don't)\n            #   have the prior row\n            stock_df[stock_df['stock']==stock].loc[start_index, field+\"_chg\"] = 0\n            stock_df[stock_df['stock']==stock].loc[start_index, field+\"_per_chg\"] = 0\n\n\n        # iterate through each row after the first\n        for i in range (start_index+1, len(stock_df)):\n            # iterate through each base field\n            for field in fields:\n                # set the field's \"change\" value to be the current value minus the \n                #   previous day's value\n                stock_df.loc[i, field+\"_chg\"] = stock_df.loc[i, field] - stock_df.loc[i-1, field]\n                # check if the previous day's value is zero, so we don't divide by zero\n                if stock_df.loc[i-1, field] == 0:\n                    # if it is, just set it to zero\n                    stock_df.loc[i, field+\"_per_chg\"] = 0\n                else:\n                    # otherwise, let's set the \"percent change\" value to the current\n                    #  change divided by the previous day's value, *100 as a percent.\n                    stock_df.loc[i, field+\"_per_chg\"] = stock_df.loc[i, field+\"_chg\"] / stock_df.loc[i-1, field] * 100\nelse:\n    # Skip the processing and load the pre-processed file\n    stock_df = pd.read_csv(\"../input/sp-500-v2-processed/sp500_v2.csv\")\nstock_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:20:14.787474Z","iopub.execute_input":"2021-05-25T00:20:14.787883Z","iopub.status.idle":"2021-05-25T00:20:16.52625Z","shell.execute_reply.started":"2021-05-25T00:20:14.787843Z","shell.execute_reply":"2021-05-25T00:20:16.525388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# assertions\nassert stock_df.shape == (357357, 21)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:20:16.527403Z","iopub.execute_input":"2021-05-25T00:20:16.5277Z","iopub.status.idle":"2021-05-25T00:20:16.531298Z","shell.execute_reply.started":"2021-05-25T00:20:16.527654Z","shell.execute_reply":"2021-05-25T00:20:16.530441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Clean the Reddit Dataframe","metadata":{}},{"cell_type":"code","source":"#Separate date and time and take only necessary columns \nreddit_df['created']= pd.to_datetime(reddit_df['created'])\nreddit_df['date'] = [d.date() for d in reddit_df['created']]\nreddit_df['time'] = [d.time() for d in reddit_df['created']]\nreddit_df = reddit_df.loc[:,[\"link\",\"title\",\"date\",\"time\", \"score\"]]\nreddit_df['title'] = reddit_df.title.apply(str.lower)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:20:16.532551Z","iopub.execute_input":"2021-05-25T00:20:16.532837Z","iopub.status.idle":"2021-05-25T00:20:23.781134Z","shell.execute_reply.started":"2021-05-25T00:20:16.53281Z","shell.execute_reply":"2021-05-25T00:20:23.780136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#extract website names from links\nreddit_df['link'] = reddit_df['link'].str.extract('.*\\://(?:www.)?([^\\/]+)', expand=True)\nreddit_df.sample(2)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:20:23.783632Z","iopub.execute_input":"2021-05-25T00:20:23.783957Z","iopub.status.idle":"2021-05-25T00:20:27.502605Z","shell.execute_reply.started":"2021-05-25T00:20:23.78393Z","shell.execute_reply":"2021-05-25T00:20:27.501611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_covid_posts(df):\n    # set keywords that identify a post as covid related, borrowed from https://www.henryford.com/blog/2020/04/covid19-key-terms-to-know \n    keywords = [\"covid\",\"virus\",\"corona\",\"flu\",\"vaccine\",\"mask\",\"symptom\",\"ventilator\",\"PPE\",\"social distancing\",\"quarantine\",\"super spreader\",\n                \"flatten the curve\",\"antibody\",\"antibodies\",\"epidemic\",\"pandemic\",\"outbreak\",\"n95\",\"herd immunity\"]\n    # set a mask to only return covid-related posts.\n    mask = df.title.apply(lambda x: any(item for item in keywords if item in x))\n    return df[mask]\n\ncovid_df = get_covid_posts(reddit_df)\ncovid_df","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:20:27.504242Z","iopub.execute_input":"2021-05-25T00:20:27.504508Z","iopub.status.idle":"2021-05-25T00:20:30.603126Z","shell.execute_reply.started":"2021-05-25T00:20:27.504481Z","shell.execute_reply":"2021-05-25T00:20:30.601981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Get only English entries\ncovid_df_scores = covid_df[covid_df['title'].map(lambda x: x.isascii())]\n\n#Only reliable news sources\n#Keep only entries where the source link ocurs more than 200 times. This gets rid of \"spam\" entries\n#that can be harmful to the analysis\nlinks = covid_df_scores.loc[:,['link']]\nlinks['count'] = 1\nlinks_gr = links.groupby(\"link\").sum().reset_index()\nlinkss = links_gr[links_gr[\"count\"] > 200]\n","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:20:30.60433Z","iopub.execute_input":"2021-05-25T00:20:30.604646Z","iopub.status.idle":"2021-05-25T00:20:30.749138Z","shell.execute_reply.started":"2021-05-25T00:20:30.604614Z","shell.execute_reply":"2021-05-25T00:20:30.748164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sentiment Analysis","metadata":{}},{"cell_type":"code","source":"#Lets start our sentiment analysis\nvader = SentimentIntensityAnalyzer()\nscores = covid_df_scores['title'].apply(vader.polarity_scores).tolist()","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:20:30.750386Z","iopub.execute_input":"2021-05-25T00:20:30.750695Z","iopub.status.idle":"2021-05-25T00:20:54.149415Z","shell.execute_reply.started":"2021-05-25T00:20:30.750642Z","shell.execute_reply":"2021-05-25T00:20:54.148504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores_df = pd.DataFrame(scores)\ncovid_df_scores.reset_index(drop=True, inplace=True)\nreddit_df_scores = covid_df_scores.join(scores_df, rsuffix='_right')\nreddit_df_scores[\"count\"] = 1\nreddit_df_scores","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:20:54.150376Z","iopub.execute_input":"2021-05-25T00:20:54.150637Z","iopub.status.idle":"2021-05-25T00:20:54.349166Z","shell.execute_reply.started":"2021-05-25T00:20:54.150611Z","shell.execute_reply":"2021-05-25T00:20:54.348222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_scores = reddit_df_scores.groupby(['date']).agg({\"compound\":\"sum\", \"count\":\"sum\"}).reset_index()\nmean_scores.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:20:54.350349Z","iopub.execute_input":"2021-05-25T00:20:54.350618Z","iopub.status.idle":"2021-05-25T00:20:54.401746Z","shell.execute_reply.started":"2021-05-25T00:20:54.350592Z","shell.execute_reply":"2021-05-25T00:20:54.400763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_scores.sort_values(by=\"count\", ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:20:54.403014Z","iopub.execute_input":"2021-05-25T00:20:54.403285Z","iopub.status.idle":"2021-05-25T00:20:54.421765Z","shell.execute_reply.started":"2021-05-25T00:20:54.403258Z","shell.execute_reply":"2021-05-25T00:20:54.42071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"date_selection = alt.selection_single(on=\"mouseover\", encodings=[\"color\"])\n\nmean_scores['date']= mean_scores['date'].apply(str)\n\ncompound_scores_chart = alt.Chart(mean_scores).mark_bar(size = 1).encode(\n    x = alt.X(\"date:T\"),\n    y = alt.Y(\"compound:Q\", title =\"sentiment score sum\"), \n).properties(\n    title=\"Sentiment Score by Day\"\n).add_selection(date_selection).encode(\n        tooltip=['date:T'],\n)\n\ncount_chart = alt.Chart(mean_scores).mark_line(size = 1).encode(\n    x = alt.X(\"date:T\"),\n    y = alt.Y(\"count:Q\")\n).properties(\n    title=\"Number of News Stories per day\"\n).add_selection(date_selection).encode(\n        tooltip=['date:T'],\n) \n\ncompound_scores_chart | count_chart","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:20:54.422935Z","iopub.execute_input":"2021-05-25T00:20:54.423206Z","iopub.status.idle":"2021-05-25T00:20:54.480458Z","shell.execute_reply.started":"2021-05-25T00:20:54.42318Z","shell.execute_reply":"2021-05-25T00:20:54.479509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_chart = alt.Chart(mean_scores).mark_line(color=\"orange\",size = 1).encode(\n    x = alt.X(\"date:T\"),\n    y = alt.Y(\"count:Q\")\n)\nsentiment_and_volume = alt.layer(compound_scores_chart, count_chart).resolve_scale(\n    y = 'independent'\n).configure_axisRight(\n  labelColor='orange',\n  titleColor='orange'\n).properties(\n    title=\"Sentiment and Volume of News Stories\"\n)\n\n\nsentiment_and_volume","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:20:54.481978Z","iopub.execute_input":"2021-05-25T00:20:54.482377Z","iopub.status.idle":"2021-05-25T00:20:54.515694Z","shell.execute_reply.started":"2021-05-25T00:20:54.482325Z","shell.execute_reply":"2021-05-25T00:20:54.514626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"March 11th, and October 2nd have the highest sentiment scores. ","metadata":{}},{"cell_type":"code","source":"import datetime as dt\nreddit_df[reddit_df['date']==dt.date(2020,10,2)].sample(10)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:20:54.517407Z","iopub.execute_input":"2021-05-25T00:20:54.517805Z","iopub.status.idle":"2021-05-25T00:20:54.682807Z","shell.execute_reply.started":"2021-05-25T00:20:54.517766Z","shell.execute_reply":"2021-05-25T00:20:54.681838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"October 2, 2020 was the date Trump tested positive for COVID-19.","metadata":{}},{"cell_type":"code","source":"# find the max and min scores for normalization\nscore_max_val = reddit_df_scores.score.max()\nscore_min_val = reddit_df_scores.score.min()\n\n# normalize the scores between 0 and 1\nreddit_df_scores['score_norm'] = reddit_df_scores.score.apply(lambda x: (x-score_min_val)/(score_max_val-score_min_val))","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:20:54.683976Z","iopub.execute_input":"2021-05-25T00:20:54.684247Z","iopub.status.idle":"2021-05-25T00:20:54.842411Z","shell.execute_reply.started":"2021-05-25T00:20:54.684222Z","shell.execute_reply":"2021-05-25T00:20:54.841488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets take only the top 5000 stories, according to score. \ntop_scores = reddit_df_scores.sort_values(\"score_norm\", ascending=False).head(5000)\ntop_scores","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:20:54.843575Z","iopub.execute_input":"2021-05-25T00:20:54.843907Z","iopub.status.idle":"2021-05-25T00:20:54.894488Z","shell.execute_reply.started":"2021-05-25T00:20:54.843878Z","shell.execute_reply":"2021-05-25T00:20:54.893441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Map score over time to see which news stories have recieved the most upvotes, or if there are any trends\nselection = alt.selection_single(on=\"mouseover\", encodings=[\"color\"])\n\ntop_scores['date']= top_scores['date'].apply(str)\ntop_scores['time']= top_scores['time'].apply(str)\nalt.Chart(top_scores).mark_circle().encode(\n    x = alt.X(\"date:T\"),\n    y = alt.Y(\"score\")\n).properties(\n    width = 1500\n).add_selection(selection).encode(\n        tooltip=['title:N',\"score:Q\", \"date:T\"],\n)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:20:54.895533Z","iopub.execute_input":"2021-05-25T00:20:54.895784Z","iopub.status.idle":"2021-05-25T00:20:55.238523Z","shell.execute_reply.started":"2021-05-25T00:20:54.895758Z","shell.execute_reply":"2021-05-25T00:20:55.237428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot only the top stories of each day. \ndaily_top_news = top_scores.groupby(by=\"date\").max().reset_index()\n\nalt.Chart(daily_top_news).mark_circle().encode(\n    x = alt.X(\"date:T\"),\n    y = alt.Y(\"score_norm\")\n).properties(\n    width = 1500\n).add_selection(selection).encode(\n        tooltip=['title:N',\"score:Q\", \"date:T\"],\n)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:20:55.239807Z","iopub.execute_input":"2021-05-25T00:20:55.240091Z","iopub.status.idle":"2021-05-25T00:20:55.483396Z","shell.execute_reply.started":"2021-05-25T00:20:55.240061Z","shell.execute_reply":"2021-05-25T00:20:55.482722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stock Data Cleaning","metadata":{}},{"cell_type":"code","source":"stock_df","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:20:55.484553Z","iopub.execute_input":"2021-05-25T00:20:55.484813Z","iopub.status.idle":"2021-05-25T00:20:55.630349Z","shell.execute_reply.started":"2021-05-25T00:20:55.484787Z","shell.execute_reply":"2021-05-25T00:20:55.629324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's ensure the date field is a datetime date\nstock_df.date = pd.to_datetime(stock_df.date)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:20:55.635335Z","iopub.execute_input":"2021-05-25T00:20:55.635625Z","iopub.status.idle":"2021-05-25T00:21:20.575345Z","shell.execute_reply.started":"2021-05-25T00:20:55.635596Z","shell.execute_reply":"2021-05-25T00:21:20.574291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stock_df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:21:20.576817Z","iopub.execute_input":"2021-05-25T00:21:20.577068Z","iopub.status.idle":"2021-05-25T00:21:20.9366Z","shell.execute_reply.started":"2021-05-25T00:21:20.577042Z","shell.execute_reply":"2021-05-25T00:21:20.935517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's combine the stock values to get an overall trend. \n#.  Eventually, we'll want to look at individual stocks, but that's beyond the scope of this project\nstock_df_compiled = stock_df.groupby(by=\"date\").sum().reset_index()\nstock_df_compiled","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:21:20.937884Z","iopub.execute_input":"2021-05-25T00:21:20.938257Z","iopub.status.idle":"2021-05-25T00:21:21.023695Z","shell.execute_reply.started":"2021-05-25T00:21:20.938223Z","shell.execute_reply":"2021-05-25T00:21:21.022865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Combining the two data sets ","metadata":{}},{"cell_type":"code","source":"selection = alt.selection_single(on=\"mouseover\", encodings=[\"color\"])\n\nstock_compiled_chart = alt.Chart(stock_df_compiled).mark_line().encode(\n    x = alt.X(\"date:T\"),\n    y = alt.Y(\"close:Q\", title=\"Dollars (in thousands\")\n).properties(\n    width = 500, \n    title = \"S&P 500 Close overlapped with Sentiment Scores of Reddit Data\"\n).add_selection(selection).encode(\n        tooltip=['vol:N'],\n)\n\nalt.layer(stock_compiled_chart, compound_scores_chart).resolve_scale(\n    y = 'independent'\n)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:21:21.024828Z","iopub.execute_input":"2021-05-25T00:21:21.025089Z","iopub.status.idle":"2021-05-25T00:21:21.182689Z","shell.execute_reply.started":"2021-05-25T00:21:21.025062Z","shell.execute_reply":"2021-05-25T00:21:21.181967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the %change of volume, summed for all stocks by date\nvol_chg_by_date = stock_df.groupby('date').sum().vol_per_chg.reset_index()\n\n# set the date field to datetime\nvol_chg_by_date = stock_df_compiled.copy()\nvol_chg_by_date.date = pd.to_datetime(vol_chg_by_date.date)\n\n# pick only stock values after 2020-01-01\n#vol_chg_by_date = vol_chg_by_date[vol_chg_by_date.date>=\"2019-11-01\"]\n\n# get the max amount of volume %change for all days. This can be either positive\n#   (for positive volume change) or negative (for reduced volume), so find the one\n#    that is the greatest absolute val\nvol_max_val = max(vol_chg_by_date.vol_per_chg.max(), vol_chg_by_date.vol_per_chg.min()*-1)\n\n# normalize the volume change between -1 and +1, with -1 being the most reduction in\n#   volume, and +1 being the most increase in volume day to day\nvol_chg_by_date[\"norm_chg\"] = vol_chg_by_date.vol_per_chg.apply(lambda x: (x)/(vol_max_val))\n\nvol_chg_by_date.sample(5)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:21:21.183725Z","iopub.execute_input":"2021-05-25T00:21:21.184112Z","iopub.status.idle":"2021-05-25T00:21:21.260396Z","shell.execute_reply.started":"2021-05-25T00:21:21.18407Z","shell.execute_reply":"2021-05-25T00:21:21.259418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_scores.sample(5)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:21:21.261583Z","iopub.execute_input":"2021-05-25T00:21:21.261869Z","iopub.status.idle":"2021-05-25T00:21:21.27258Z","shell.execute_reply.started":"2021-05-25T00:21:21.261842Z","shell.execute_reply":"2021-05-25T00:21:21.271643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nmean_scores.date = pd.to_datetime(mean_scores.date)\n\nsentiment_stock = pd.merge(vol_chg_by_date, mean_scores, how='inner', on=\"date\")\n\nsns.lineplot(x=sentiment_stock[\"compound\"],y=sentiment_stock[\"norm_chg\"])","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:21:21.274026Z","iopub.execute_input":"2021-05-25T00:21:21.274581Z","iopub.status.idle":"2021-05-25T00:21:21.521386Z","shell.execute_reply.started":"2021-05-25T00:21:21.274538Z","shell.execute_reply":"2021-05-25T00:21:21.520373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lineplot(x=vol_chg_by_date[\"date\"],y=vol_chg_by_date[\"norm_chg\"])\nplt.xticks(rotation=75);","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:21:21.522698Z","iopub.execute_input":"2021-05-25T00:21:21.523002Z","iopub.status.idle":"2021-05-25T00:21:21.7907Z","shell.execute_reply.started":"2021-05-25T00:21:21.52297Z","shell.execute_reply":"2021-05-25T00:21:21.789699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"daily_top_news.date = pd.to_datetime(daily_top_news.date)\nsentiment_stock_final = pd.merge(sentiment_stock, daily_top_news, how='inner', on=\"date\")\nsentiment_stock_final.head(5)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:21:21.791829Z","iopub.execute_input":"2021-05-25T00:21:21.792124Z","iopub.status.idle":"2021-05-25T00:21:21.833584Z","shell.execute_reply.started":"2021-05-25T00:21:21.792094Z","shell.execute_reply":"2021-05-25T00:21:21.832539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's plot the scores to see what we're dealing with\n\nsns.lineplot(x=sentiment_stock_final.score_norm, y=sentiment_stock_final.norm_chg)\nplt.xticks(rotation=75);","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:21:21.834736Z","iopub.execute_input":"2021-05-25T00:21:21.834993Z","iopub.status.idle":"2021-05-25T00:21:22.219095Z","shell.execute_reply.started":"2021-05-25T00:21:21.834967Z","shell.execute_reply":"2021-05-25T00:21:22.218112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we see a large spike (400k) around 2020-10 (this will be around 2020-10-02 or \n2020-10-03, as determined later), as well as a spike (300k) just before \n2020-12 (again, we'll later see this is 2020-11-09). So these were busy days\nwith lots of post activity and upvotes.  ","metadata":{}},{"cell_type":"code","source":"# let's do the same thing for the percent of volume change per day.\n\n# get the %change of volume, summed for all stocks by date\nvol_chg_by_date = stock_df.groupby('date').sum().vol_per_chg.reset_index()\n\n# set the date field to datetime\nvol_chg_by_date.date = pd.to_datetime(vol_chg_by_date.date)\n\n# pick only stock values after 2020-01-01\nvol_chg_by_date = vol_chg_by_date[vol_chg_by_date.date>=\"2020-01-01\"]\n\n# get the max amount of volumne %change for all days. This can be either positive\n#   (for positive volume change) or negative (for reduced volume), so find the one\n#    that is the greatest absolute val\nvol_max_val = max(vol_chg_by_date.vol_per_chg.max(), vol_chg_by_date.vol_per_chg.min()*-1)\n#vol_min_val = vol_max_val * -1\n\n# normalize the volume change between -1 and +1, with -1 being the most reduction in\n#   volume, and +1 being the most increase in volume day to day\nvol_chg_by_date[\"norm_chg\"] = vol_chg_by_date.vol_per_chg.apply(lambda x: (x)/(vol_max_val))\n\nvol_chg_by_date","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:21:22.220635Z","iopub.execute_input":"2021-05-25T00:21:22.221003Z","iopub.status.idle":"2021-05-25T00:21:22.290117Z","shell.execute_reply.started":"2021-05-25T00:21:22.22097Z","shell.execute_reply":"2021-05-25T00:21:22.289233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's plot the volume %change day to day and see what we're working with\n\nsns.lineplot(x=vol_chg_by_date.date ,y=vol_chg_by_date.vol_per_chg)\nplt.xticks(rotation=75)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:21:22.291334Z","iopub.execute_input":"2021-05-25T00:21:22.291623Z","iopub.status.idle":"2021-05-25T00:21:22.541475Z","shell.execute_reply.started":"2021-05-25T00:21:22.291595Z","shell.execute_reply":"2021-05-25T00:21:22.54061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again we see spikes in volume change (increased stock trading) around 2020-11 \nor 2020-12, with some additional hotspots around 2020-06/07, 2020-10, \n2020-12/2021-01, and 2021-04","metadata":{}},{"cell_type":"code","source":"stock_df_compiled.date = pd.to_datetime(stock_df_compiled.date)\ncompiled_stock_news = pd.merge(stock_df_compiled, daily_top_news, how=\"inner\")","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:21:22.543743Z","iopub.execute_input":"2021-05-25T00:21:22.544168Z","iopub.status.idle":"2021-05-25T00:21:22.557925Z","shell.execute_reply.started":"2021-05-25T00:21:22.544122Z","shell.execute_reply":"2021-05-25T00:21:22.556846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a selection that chooses the nearest point & selects based on x-value\nnearest = alt.selection(type='single', nearest=True, on='mouseover',\n                        fields=['date'], empty='none')\n\n# The basic line\ndaily_news_stock_chart = alt.Chart(sentiment_stock_final).mark_line().encode(\n    x='date:T',\n    y='close:Q',\n    #color='stock:N'  ##used when looking at non-compiled data \n).properties(\n    width = 1000\n).add_selection(nearest).encode(\n        tooltip=['title:N','date:T', 'link:N'],\n)\n\n\n\nalt.layer(daily_news_stock_chart, compound_scores_chart).resolve_scale(\n    y = 'independent'\n)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:21:22.559696Z","iopub.execute_input":"2021-05-25T00:21:22.560114Z","iopub.status.idle":"2021-05-25T00:21:22.744591Z","shell.execute_reply.started":"2021-05-25T00:21:22.560071Z","shell.execute_reply":"2021-05-25T00:21:22.743645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The basic line\ndaily_news_stock_chart = alt.Chart(sentiment_stock_final).mark_line().encode(\n    x='date:T',\n    y='close:Q',\n    #color='stock:N'  ##used when looking at non-compiled data \n).properties(\n    width = 1000\n)\n\nmark1 = alt.Chart(sentiment_stock_final[sentiment_stock_final.date==\"2020-01-30\"]).mark_circle(size=50, color=\"black\"\n).encode(\n    x=\"date:T\",\n    y=\"close:Q\",\n    text=\"title:N\"\n)\nlabel1 = alt.Chart(sentiment_stock_final[sentiment_stock_final.date==\"2020-01-30\"]).mark_text(    \n    fontWeight=500, \n    align='center',\n    lineBreak=\"is\",\n    baseline='line-top',\n    dy=-40\n).encode(\n    x=\"date:T\",\n    y=\"close:Q\",\n    text=\"title:N\"\n)\n\nmark2 = alt.Chart(sentiment_stock_final[sentiment_stock_final.date==\"2020-02-24\"]).mark_circle(size=50, color=\"black\"\n).encode(\n    x=\"date:T\",\n    y=\"close:Q\",\n    text=\"title:N\"\n)\nlabel2 = alt.Chart(sentiment_stock_final[sentiment_stock_final.date==\"2020-02-24\"]).mark_text(    \n    fontWeight=500,\n    align='left',\n    baseline='line-top',\n    dy=-15\n).encode(\n    x=\"date:T\",\n    y=\"close:Q\",\n    text=\"title:N\"\n)\n\nmark3 = alt.Chart(sentiment_stock_final[sentiment_stock_final.date==\"2020-03-11\"]).mark_circle(size=50, color=\"black\"\n).encode(\n    x=\"date:T\",\n    y=\"close:Q\",\n    text=\"title:N\"\n)\nlabel3 = alt.Chart(sentiment_stock_final[sentiment_stock_final.date==\"2020-03-11\"]).mark_text(    \n    fontWeight=500,\n    align='right',\n    lineBreak = \"the\",\n    baseline='middle',\n    dy=0, \n    dx=-15\n).encode(\n    x=\"date:T\",\n    y=\"close:Q\",\n    text=\"title:N\"\n)\n\nmark4 = alt.Chart(sentiment_stock_final[sentiment_stock_final.date==\"2020-03-23\"]).mark_circle(size=50, color=\"black\"\n).encode(\n    x=\"date:T\",\n    y=\"close:Q\",\n    text=\"title:N\"\n)\nlabel4 = alt.Chart(sentiment_stock_final[sentiment_stock_final.date==\"2020-03-23\"]).mark_text(    \n    fontWeight=500,\n    align='left',\n    lineBreak = \"'s\",\n    baseline='middle',\n    dy=15, \n    dx=0\n).encode(\n    x=\"date:T\",\n    y=\"close:Q\",\n    text=\"title:N\"\n)\n\nmark5 = alt.Chart(sentiment_stock_final[sentiment_stock_final.date==\"2020-04-30\"]).mark_circle(size=50, color=\"black\"\n).encode(\n    x=\"date:T\",\n    y=\"close:Q\",\n    text=\"title:N\"\n)\nlabel5 = alt.Chart(sentiment_stock_final[sentiment_stock_final.date==\"2020-04-30\"]).mark_text(    \n    fontWeight=500,\n    align='left',\n    lineBreak = \"'s\",\n    baseline='middle',\n    dy=30, \n    dx=0\n).encode(\n    x=\"date:T\",\n    y=\"close:Q\",\n    text=\"title:N\"\n)\nmark6 = alt.Chart(sentiment_stock_final[sentiment_stock_final.date==\"2020-10-02\"]).mark_circle(size=50, color=\"black\"\n).encode(\n    x=\"date:T\",\n    y=\"close:Q\",\n    text=\"title:N\"\n)\nlabel6 = alt.Chart(sentiment_stock_final[sentiment_stock_final.date==\"2020-10-02\"]).mark_text(    \n    fontWeight=500,\n    align='left',\n    lineBreak = \",\",\n    baseline='middle',\n    dy=15, \n    dx=0\n).encode(\n    x=\"date:T\",\n    y=\"close:Q\",\n    text=\"title:N\"\n)\n\nfinal_chart = daily_news_stock_chart + label1 + mark1 +label2+mark2+label3+mark3+label4+mark4+label6+mark6 \n\nfinal = alt.layer(final_chart, compound_scores_chart).resolve_scale(\n    y = 'independent'\n)\n\nfinal","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:21:22.745994Z","iopub.execute_input":"2021-05-25T00:21:22.746283Z","iopub.status.idle":"2021-05-25T00:21:23.50561Z","shell.execute_reply.started":"2021-05-25T00:21:22.746253Z","shell.execute_reply":"2021-05-25T00:21:23.504862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alt.layer(final, count_chart).resolve_scale(\n    y = 'independent'\n)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T00:21:23.506708Z","iopub.execute_input":"2021-05-25T00:21:23.507142Z","iopub.status.idle":"2021-05-25T00:21:24.065315Z","shell.execute_reply.started":"2021-05-25T00:21:23.507111Z","shell.execute_reply":"2021-05-25T00:21:24.064321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}