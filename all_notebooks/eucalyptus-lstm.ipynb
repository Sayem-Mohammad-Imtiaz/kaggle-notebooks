{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](https://images.westend61.de/0001055733pw/australia-queensland-koala-eating-eucalyptus-leaves-GEMF02425.jpg)https://www.westend61.de/en/imageView/GEMF02425/australia-queensland-koala-eating-eucalyptus-leaves","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/eucalyptus/dataset_194_eucalyptus.csv\")\nprint(df.shape)\ndf.head().style.set_properties(**{'background-color':'Aquamarine',\n                                     'color': 'purple'})","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#importing ploting libraries\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Lucas Abrah√£o https://www.kaggle.com/lucasabrahao/trabalho-manufatura-an-lise-de-dados-no-brasil\n\ndf[\"Utility\"].value_counts().plot.barh(color=['blue', 'red','lime','purple','teal','cyan'], title='Utility');","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Abbrev\"].value_counts().plot.barh(title='Abbreviation');","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Locality\"].value_counts().plot.barh(color='g', title='Locality');","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Sp\"].value_counts().plot.barh(color='purple', title='Species Code');","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Code by Madhan Chandrasekharan https://www.kaggle.com/gcmadhan/amazon-stock-prediction-99-accuracy","metadata":{}},{"cell_type":"code","source":"#Code by Madhan Chandrasekharan https://www.kaggle.com/gcmadhan/amazon-stock-prediction-99-accuracy\n\ncolors = ['#FF7F50','#228B22']\nsns.set(palette=colors, font='Serif', style='white', rc={'axes.facecolor':'whitesmoke', 'figure.facecolor':'whitesmoke'})\nsns.palplot(colors, size=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Madhan Chandrasekharan https://www.kaggle.com/gcmadhan/amazon-stock-prediction-99-accuracy\n\nfig=plt.figure(figsize=(20,8))\nax=sns.lineplot(data=df, x='Year',y='Rainfall')\nax=sns.lineplot(data=df, x='Year',y='Frosts', color=colors[1]);\nfor s in ['left','right','top','bottom']:\n    ax.spines[s].set_visible(False)\n\nplt.title(\"Rainfall & Frosts 1980-86\", size=20, weight='bold');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Madhan Chandrasekharan https://www.kaggle.com/gcmadhan/amazon-stock-prediction-99-accuracy\n\nfig=plt.figure(figsize=(20,8))\nax=sns.lineplot(data=df, x='Year',y='Altitude')\nax=sns.lineplot(data=df, x='Year',y='Rep', color=colors[1]);\nfor s in ['left','right','top','bottom']:\n    ax.spines[s].set_visible(False)\nplt.title(\"Altitude Influence 1980-86\", size=20, weight='bold');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Univariated Analysis","metadata":{}},{"cell_type":"code","source":"#Code by Madhan Chandrasekharan https://www.kaggle.com/gcmadhan/amazon-stock-prediction-99-accuracy\n\n#integer columns\nfig=plt.figure(figsize=(20,8), tight_layout=True)\nplt.suptitle(\"Analysing the Numeric variables\", size=20, weight='bold')\nax=fig.subplot_mosaic(\"\"\"AB\n                         CC\n                         DE\"\"\")\nsns.kdeplot(df['Rep'], ax=ax['A'], color=colors[0], fill=True, linewidth=2)\nsns.kdeplot(df['Altitude'], ax=ax['B'], color=colors[1],fill=True, linewidth=2)\nsns.kdeplot(df['Rainfall'], ax=ax['C'], color=colors[0],fill=True, linewidth=2)\nsns.kdeplot(df['Frosts'], ax=ax['D'], color=colors[1],fill=True, linewidth=2)\nsns.kdeplot(df['Year'], ax=ax['E'], color=colors[0],fill=True, linewidth=2)\nax['B'].yaxis.set_visible(False)\nax['E'].yaxis.set_visible(False)\nax['A'].yaxis.label.set_alpha(0.5)\nax['C'].yaxis.label.set_alpha(0.5)\nax['A'].yaxis.label.set_alpha(0.5)\nax['C'].yaxis.label.set_alpha(0.5)\nax['D'].yaxis.label.set_alpha(0.5)\nfor s in ['left','right','top','bottom']:\n    ax['A'].spines[s].set_visible(False)\n    ax['B'].spines[s].set_visible(False)\n    ax['C'].spines[s].set_visible(False)\n    ax['D'].spines[s].set_visible(False)\n    ax['E'].spines[s].set_visible(False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Madhan Chandrasekharan https://www.kaggle.com/gcmadhan/amazon-stock-prediction-99-accuracy\n\n#integer columns\nfig=plt.figure(figsize=(20,8), tight_layout=True)\nplt.suptitle(\"Boxplot the Numeric variables\", size=20, weight='bold')\nax=fig.subplot_mosaic(\"\"\"AB\n                         CC\n                         DE\"\"\")\nsns.boxplot(df['Rep'], ax=ax['A'], color=colors[0])\nsns.boxplot(df['Altitude'], ax=ax['B'], color=colors[1])\nsns.boxplot(df['Rainfall'], ax=ax['C'], color=colors[0])\nsns.boxplot(df['Frosts'], ax=ax['D'], color=colors[1])\nsns.boxplot(df['Year'], ax=ax['E'], color=colors[1])\nax['B'].yaxis.set_visible(False)\nax['E'].yaxis.set_visible(False)\nax['A'].yaxis.label.set_alpha(0.5)\nax['C'].yaxis.label.set_alpha(0.5)\nax['A'].yaxis.label.set_alpha(0.5)\nax['C'].yaxis.label.set_alpha(0.5)\nax['D'].yaxis.label.set_alpha(0.5)\nfor s in ['left','right','top','bottom']:\n    ax['A'].spines[s].set_visible(False)\n    ax['B'].spines[s].set_visible(False)\n    ax['C'].spines[s].set_visible(False)\n    ax['D'].spines[s].set_visible(False)\n    ax['E'].spines[s].set_visible(False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Bivariated & Multivariated Analysis","metadata":{}},{"cell_type":"code","source":"sns.pairplot(df,corner=True);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.corr()['Rep']","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Hypothesis tested to find the Normality in the Dataset","metadata":{}},{"cell_type":"code","source":"#Code by Madhan Chandrasekharan https://www.kaggle.com/gcmadhan/amazon-stock-prediction-99-accuracy\n\nfrom scipy.stats import levene, shapiro\nint_cols=df.select_dtypes(exclude='object').columns.to_list()\n\nfor i in int_cols:\n    _, p_value=shapiro(df[i])\n    if p_value<0.05:\n        print(\"Feature {} is normaly distributed\".format(i))\n    else:\n        print(\"Feature {} is not normaly distributed\".format(i))\n        \n    print(\"Normalitiy test p_value for featue -  {} is {}\".format(i,np.round(p_value,3)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Correlation","metadata":{}},{"cell_type":"code","source":"fig=plt.figure(figsize=(15,8))\nsns.heatmap(df.corr(), annot=True, cmap=[colors[0],colors[1]], linecolor='white', linewidth=2 );","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Label Encoding","metadata":{}},{"cell_type":"code","source":"#Code by Bizen https://www.kaggle.com/hiro5299834/tps-apr-2021-deebtables/notebook\n\nTARGET = 'Rep' #Target could Not be float otherwise will result in valueError: Unknown label type: 'continuous'. Even after the encoding.\n\nlabel_cols = ['Abbrev', 'Locality', 'Map_Ref', 'Latitude', 'Sp', 'PMCno', 'DBH', 'Ht', 'Surv', 'Vig', 'Ins_res', 'Stem_Fm', 'Crown_Fm', 'Brnch_Fm', 'Utility']\nnumerical_cols = ['Altitude', 'Rainfall', 'Frosts', 'Year']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Bizen https://www.kaggle.com/hiro5299834/tps-apr-2021-deebtables/notebook\n\nfrom sklearn.preprocessing import LabelEncoder\n\ndef label_encoder(c):\n    le = LabelEncoder()\n    return le.fit_transform(c)\n\nlabel_encoded_df = df[label_cols].apply(label_encoder)\nnumerical_df = df[numerical_cols]\ntarget_df = df[TARGET]\n\ndf = pd.concat([numerical_df, label_encoded_df, target_df], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=df[['Year','Rainfall']]\ny=df['Rainfall']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Train Test Split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, shuffle=False, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Normalizing the values","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test =scaler.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Model Creation\n\nBasic Linear regression model","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\nfrom sklearn import set_config\nmodel = LinearRegression()\nmodel.fit(X_train,y_train)\nset_config(display='diagram')\npred=model.predict(X_test)\nsc=np.round(model.score(X_test, y_test),2) * 100\nr2=np.round(r2_score(y_test,pred),2)\nmse=np.round(mean_squared_error(y_test,pred),2)\nmae=np.round(mean_squared_error(y_test,pred),2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Figure size 1080x576 Original Plot text was x=800000, y=600 resulted in error\n#x=1000 the figure is small,,, x= 1500 it gets smaller The more I reduced x value increased the figure size\n#x=600 figure reached  a normal size. Though MAE is still zero.\n#Whenever I changed y values I could read the legends for Accuracy/R2/MSE/MAE\n#The original y values were y= 600, 580,560,540\n\nfig=plt.figure(figsize=(15,8))\np=pd.Series(pred, index=y_test.index)\nplt.plot(y_test)\nplt.plot(p)\nplt.legend(['y_test','predicted'])\nplt.title(\"Compare test and predicted values\", size=20, weight='bold')\nplt.text(x=600, y=500,s='Accuracy score : {} %'.format(sc))\nplt.text(x=600, y=570,s='R2 Score : {}'.format(r2))\nplt.text(x=600, y=300,s='Mean Squared error : {}'.format(mse))\nplt.text(x=600, y=400,s='Mean Absolute error : {}'.format(mae));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#LSTM model","metadata":{}},{"cell_type":"code","source":"X=df[['Year','Altitude','Rep']]\ny=df['Rainfall']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, shuffle=False, random_state=42)\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test =scaler.transform(X_test)\n\nprint(X_train.shape)\nX_train=X_train.reshape(X_train.shape[0],X_train.shape[1],1)\nX_test=X_test.reshape(X_test.shape[0],X_test.shape[1],1)\nprint(X_train.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_features=X_train.shape[1]\nlength=30\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Flatten,Dropout\nfrom tensorflow.keras.optimizers import Adam\nmodel = Sequential()\nmodel.add(LSTM(100, input_shape=(n_features,1), return_sequences=True))\n#model.add(Dropout(0.2))\nmodel.add(LSTM(100, return_sequences=False))\n#model.add(LSTM(100))\n\nmodel.add(Dense(64,activation='relu'))    \n#model.add(Dense(32,activation='relu', kernel_initializer='uniform'))  \n#model.add(Dense(5,activation='relu', kernel_initializer='uniform'))  \nmodel.add(Dense(1,activation='relu'))\n#model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])\nmodel.compile(optimizer = 'adam', loss='mse')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\nearlystop = EarlyStopping(monitor='val_loss', patience =3)\n#validation_generation = TimeseriesGenerator(X_test, X_test,length=length, batch_size=batch_size)\nmodel.fit(X_train,y_train, validation_split=0.2, epochs=200, callbacks=[earlystop], verbose=1)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss = pd.DataFrame(model.history.history)\nfig=plt.figure(figsize=(15,8))\nplt.title(\"Validation loss Vs TrainInverse_transforms\", size=20, weight='bold')\nplt.plot(loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred=model.predict(X_test)\ntest=pd.DataFrame(columns=['test','pred'])\ntest['test']=y_test\ntest['pred']=pred.flatten()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig=plt.figure(figsize=(15,8))\nplt.title(\"Test vs Predicted value\", size=20, weight='bold')\nplt.plot(test);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I didn't get what this chart is showing. Though the author didn't comment his too.","metadata":{}}]}