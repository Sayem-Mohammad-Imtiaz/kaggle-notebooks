{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Classifiying the tweets using BERT"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport re\nimport string\nimport tqdm\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nfrom tqdm.notebook import tqdm\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\nimport tokenization\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"../input/tweets-with-sarcasm-and-irony/train.csv\")\ntest_data = pd.read_csv(\"../input/tweets-with-sarcasm-and-irony/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remove recurring tweets to prevent ambiguity"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tweets=train_data['tweets'].tolist()\ntest_tweets=test_data['tweets'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def keep_uniques(array, df):\n    dels=[]\n    for i in array:\n        if array.count(i)>1:\n            dels.append(i)\n    dels=list(set(dels))\n    for i in dels:\n        df.drop( df[ df['tweets'] == i ].index, inplace=True)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data=keep_uniques(train_tweets, train_data)\ntest_data=keep_uniques(test_tweets, test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_data['tweets'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(test_data['tweets'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dataset details"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_data.sample(frac = 1)\ntest_data = test_data.sample(frac = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we see that the `regular` class has 18k tweets, which causes our dataset to be imbalanced. So we shall delete some tweets from this class"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp=train_data.loc[train_data['class'] == 'regular']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lis=temp['tweets'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nreg_del=[]\nvisited=set()\nfor _ in range(3600):\n    n=random.randint(0,18556)\n    if n not in visited:\n        reg_del.append(lis[n])\n        \n        \nfor i in reg_del:\n    train_data.drop( train_data[ train_data['tweets'] == i ].index, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data['class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_URL(text):\n    text=str(text)\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'', text)\n\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef remove_mentions(text):\n    ment = re.compile(r\"(@[A-Za-z0-9]+)\")\n    return ment.sub(r'', text)\n\n\ndef remove_html(text):\n    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n    return re.sub(html, '', text)\n\n\ndef remove_punct(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['clean_text'] = train_data['tweets'].apply(lambda x: remove_URL(x))\ntrain_data['clean_text'] = train_data['clean_text'].apply(lambda x: remove_emoji(x))\ntrain_data['clean_text'] = train_data['clean_text'].apply(lambda x: remove_html(x))\ntrain_data['clean_text'] = train_data['clean_text'].apply(lambda x: remove_mentions(x))\ntrain_data['clean_text'] = train_data['clean_text'].apply(lambda x: remove_punct(x))\ntrain_data['clean_text'] = train_data['clean_text'].apply(\n    lambda x: x.lower())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned = train_data['clean_text'].tolist()\n\nfor i,text in enumerate(cleaned):\n    splits = text.split()\n    splits = [word for word in splits if word not in set(nltk.corpus.stopwords.words('english'))]\n    cleaned[i]=' '.join(splits)\n    \ntrain_data['clean_text']=cleaned\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data['clean_text'] = test_data['tweets'].apply(lambda x: remove_URL(x))\ntest_data['clean_text'] = test_data['clean_text'].apply(lambda x: remove_emoji(x))\ntest_data['clean_text'] = test_data['clean_text'].apply(lambda x: remove_html(x))\ntest_data['clean_text'] = test_data['clean_text'].apply(lambda x: remove_mentions(x))\ntest_data['clean_text'] = test_data['clean_text'].apply(lambda x: remove_punct(x))\ntest_data['clean_text'] = test_data['clean_text'].apply(\n    lambda x: x.lower())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned = test_data['clean_text'].tolist()\n\nfor i,text in enumerate(cleaned):\n    splits = text.split()\n    splits = [word for word in splits if word not in set(nltk.corpus.stopwords.words('english'))]\n    cleaned[i]=' '.join(splits)\n    \ntest_data['clean_text']=cleaned","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = test_data.dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Basic EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(10,10)})\nsns.countplot(train_data['class'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Word cloud for regular class"},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nstopwords = nltk.corpus.stopwords.words('english')\n\nplt.figure(figsize=(12,6))\ntext = ' '.join(train_data.clean_text[train_data['class']=='regular'])\nwc = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Word cloud for irony class"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\ntext = ' '.join(train_data.clean_text[train_data['class']=='irony'])\nwc = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Word cloud for sarcasm class"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\ntext = ' '.join(train_data.clean_text[train_data['class']=='sarcasm'])\nwc = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Word cloud for figurative class"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\ntext = ' '.join(train_data.clean_text[train_data['class']=='figurative'])\nwc = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## BERT Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens, all_masks, all_segments = [], [], []\n    \n    for text in tqdm(texts):\n        # Tokenize the current text\n        text = tokenizer.tokenize(text)\n        # Select text only till \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Get BERT Model from TFHub"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nurl = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\nbert_layer = hub.KerasLayer(url, trainable=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get tokenizer\nvocab_fl = bert_layer.resolved_object.vocab_file.asset_path.numpy()\nlower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_fl, lower_case)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encoding the texts"},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_target(t_class):\n    t_class=str(t_class)\n    class_dict = {\n        'irony':0,\n        'sarcasm':1,\n        'regular':2,\n        'figurative':3\n    }\n    return class_dict[t_class]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[\"target\"] = train_data['class'].apply(lambda x: encode_target(x))\ntest_data[\"target\"] = test_data['class'].apply(lambda x: encode_target(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_input = bert_encode(train_data['clean_text'].values, tokenizer, max_len=160)\ntest_input = bert_encode(test_data['clean_text'].values, tokenizer, max_len=160)\ntrain_labels = train_data['target'].values\ntest_labels = test_data['target'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fine tuned model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(transformer, max_len=512):\n\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name='input_word_ids')\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name='input_mask')\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name='segment_ids')\n    # Get the sequence output\n    _, seq_op = transformer([input_word_ids, input_mask, segment_ids])\n    # Get the respective class token from that sequence output\n    class_tkn = seq_op[:, 0, :]\n    # Final Neuron (for Classification)\n    op = Dense(4, activation='softmax')(class_tkn)\n    # Bind the inputs and outputs together into a Model\n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=op)\n    \n    model.compile(optimizer=Adam(1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model(bert_layer, max_len=160)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.07,\n    epochs=4,\n    callbacks=[checkpoint],\n    batch_size=16\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = model.evaluate(test_input, test_labels, verbose=0)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}