{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport nltk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata = pd.read_csv(\"/kaggle/input/CORD-19-research-challenge/metadata.csv\",\n                        na_values=[], keep_default_na=False)\nmetadata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords \nfrom string import punctuation\n\nstop_words = set(stopwords.words('english'))\np_chars = set([char for char in punctuation])\n\ndef extract(items):\n    records = []\n    for x in items:\n        if(isinstance(x, str)):\n            records.append(x)\n    return records\n\ndef tokenize(items):\n    records = []\n    for x in items:\n        for word in nltk.word_tokenize(x):\n            word = word.lower()\n            if not word in (stop_words|p_chars):\n                records.append(word)\n    return records\n\ndef ShowCloud(text, title):\n    # build\n    wc = WordCloud(max_font_size=50, background_color=\"white\", \n                   collocations=False,\n                   max_words=100, stopwords=STOPWORDS)\n    wc.generate(\" \".join(text))\n    # plot\n    plt.figure(figsize=(20,10))\n    plt.axis(\"off\")\n    plt.title(title, fontsize=20)\n    plt.imshow(wc, interpolation=\"bilinear\")\n    plt.show()    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titles = extract(metadata.title)\ntitle_tokens = tokenize(titles)\nprint(f\"total words in title: {len(title_tokens)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ShowCloud(title_tokens, \"Frequent words in titles\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abstracts = extract(metadata.abstract)\nabstract_tokens = tokenize(abstracts)\nprint(f\"total words in abstract: {len(abstract_tokens)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ShowCloud(abstract_tokens, \"Frequent words in abstract\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.probability import FreqDist\n\ndef builtFreq(data):\n    porter = nltk.PorterStemmer()\n    lemma = nltk.WordNetLemmatizer()\n    stems = [porter.stem(t) for t in data]\n    words = [lemma.lemmatize(t) for t in stems]\n    return FreqDist(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title_dist = builtFreq(title_tokens)\ntitle_dist.most_common()[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plotFreq(dist, count, title):\n    plt.figure(figsize=(20, 8))\n    plt.title(title, size = 40)\n    plt.xticks(size = 20)\n    plt.yticks(size = 20)\n    dist.plot(count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotFreq(title_dist, 50, \"word frequency in title\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abstract_dist = builtFreq(abstract_tokens)\nabstract_dist.most_common()[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotFreq(abstract_dist, 50, \"word frequency in abstract\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Processing data functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\n\nclass FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.abstract = []\n            self.body_text = []\n            # Abstract\n            for entry in content['abstract']:\n                self.abstract.append(entry['text'])\n            # Body text\n            for entry in content['body_text']:\n                self.body_text.append(entry['text'])\n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n    def __repr__(self):\n        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\n\ndef get_breaks(content, length):\n    data = \"\"\n    words = content.split(' ')\n    total_chars = 0\n\n    # add break every length characters\n    for i in range(len(words)):\n        total_chars += len(words[i])\n        if total_chars > length:\n            data = data + \"<br>\" + words[i]\n            total_chars = 0\n        else:\n            data = data + \" \" + words[i]\n    return data    \n    \ndef process(files, meta_df):\n    dic = {'paper_id': [], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [], 'abstract_summary': []}\n    for idx, entry in enumerate(files):\n        if idx % (len(files) // 10) == 0:\n            print(f'Processing index: {idx} of {len(files)}')\n\n        content = FileReader(entry)\n\n        # get metadata information\n        meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n        # no metadata, skip this paper\n        if len(meta_data) == 0:\n            continue\n\n        dic['paper_id'].append(content.paper_id)\n        dic['abstract'].append(content.abstract)\n        dic['body_text'].append(content.body_text)\n\n        # also create a column for the summary of abstract to be used in a plot\n        if len(content.abstract) == 0: \n            # no abstract provided\n            dic['abstract_summary'].append(\"Not provided.\")\n        elif len(content.abstract.split(' ')) > 100:\n            # abstract provided is too long for plot, take first 300 words append with ...\n            info = content.abstract.split(' ')[:100]\n            summary = get_breaks(' '.join(info), 40)\n            dic['abstract_summary'].append(summary + \"...\")\n        else:\n            # abstract is short enough\n            summary = get_breaks(content.abstract, 40)\n            dic['abstract_summary'].append(summary)\n\n        # get metadata information\n        meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n\n        try:\n            # if more than one author\n            authors = meta_data['authors'].values[0].split(';')\n            if len(authors) > 2:\n                # more than 2 authors, may be problem when plotting, so take first 2 append with ...\n                dic['authors'].append(\". \".join(authors[:2]) + \"...\")\n            else:\n                # authors will fit in plot\n                dic['authors'].append(\". \".join(authors))\n        except Exception as e:\n            # if only one author - or Null valie\n            dic['authors'].append(meta_data['authors'].values[0])\n\n        # add the title information, add breaks when needed\n        try:\n            title = get_breaks(meta_data['title'].values[0], 40)\n            dic['title'].append(title)\n        # if title was not provided\n        except Exception as e:\n            dic['title'].append(meta_data['title'].values[0])\n\n        # add the journal information\n        dic['journal'].append(meta_data['journal'].values[0])\n    return dic","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import glob\n\nall_json = glob.glob('/kaggle/input/CORD-19-research-challenge/**/*.json', recursive=True)\nlen(all_json)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get sample\nfirst_row = FileReader(all_json[0])\nprint(first_row)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_ = process(all_json, metadata)\ndf_covid = pd.DataFrame(dict_)\ndf_covid.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}