{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nimport numpy as np\nimport geopandas as gpd\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom kmodes.kprototypes import KPrototypes\nfrom sklearn.preprocessing import scale\nfrom sklearn import metrics\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Idea of the notebook\n\nMaximilian Lang (friend & collaborator https://www.kaggle.com/maximilianlang) and I wanted to understand how we could handle categorical data in a clustering problem. In order to do so and work on our DS skills we made this notebook. PS: The visualisation anf general pandas analysis is short on purpose, I had the feeling the notebook is quite full as it is."},{"metadata":{},"cell_type":"markdown","source":"# Overview\n\n**Importing E-commerce business data & first analysis**\n    \n\n**Address Analysis**\n\n\n**Creating dummy variables**\n\n\n**Visualisation**\n\n\n**DBSCAN**\n\n\n**K-Prototypes**\n\n\n**Validation**"},{"metadata":{},"cell_type":"markdown","source":"# **Importing E-commerce business data & first analysis**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/ecommerce-customers/Ecommerce Customers.csv\")\ndisplay(data.head())\n\ndata_num = data[[\"Avg. Session Length\",\"Time on App\",\"Time on Website\",\"Length of Membership\",\"Yearly Amount Spent\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Address analysis**"},{"metadata":{},"cell_type":"markdown","source":"The original dataframe had three categorical variables: Email, Address and Avatar\n\nI tried making clusters consisting of E-Mail domains like yahoo.com/ gmail.com and unique ones. I thought unique domains could potentially be from companies and they might have higher values. Unfortunately half of the rows had unique domain names and those were not from companies. I guess the data is handmade and was created for practise purposes. Furthermore I couldn't retrieve any sensible information from \"Avatar\". The variables \"Email\" and \"Avatar\" were no longer part of my project. \n\nAnyway I still used the variable \"Address\", because I wanted to have some categorical data and try out geopandas ;) After all this is practise, so please feel free to comment below if you have any remarks! "},{"metadata":{},"cell_type":"markdown","source":"The idea behind the analysis of the variable \"Address\" is to only look at the state the customer lives in. "},{"metadata":{"trusted":true},"cell_type":"code","source":"list_shortcut_states = [\"AK\",\"AL\",\"AR\",\"AZ\",\"CA\",\"CO\",\"CT\",\"DE\",\"FL\",\"GA\",\"HI\",\"IA\",\"ID\",\"IL\",\"IN\",\"KS\",\"KY\",\"LA\",\"MA\",\"MD\",\"ME\",\"MI\",\"MN\",\"MO\",\"MS\",\"MT\",\"NC\",\"ND\",\"NE\",\"NH\",\"NJ\",\"NM\",\"NV\",\"NY\",\"OH\",\"OK\",\"OR\",\"PA\",\"RI\",\"SC\",\"SD\",\"TN\",\"TX\",\"UT\",\"VA\",\"VT\",\"WA\",\"WI\",\"WV\",\"WY\"]\nstate_list = []\n\nfor i in data[\"Address\"]:\n    if \"Box\" in i:\n        index_to_drop = data[data[\"Address\"] == i].index.values \n        data.drop(index_to_drop, inplace = True)\n    else:\n        state = i.split(\",\")[-1].split()[0]\n\n        if state in list_shortcut_states:\n            state_list.append(state)\n        else:\n            index_to_drop = data[data[\"Address\"] == i].index.values \n            data.drop(index_to_drop, inplace = True)\n\n\ndata.drop([\"Email\",\"Avatar\",\"Address\"], inplace = True, axis = 1)\ndata.insert(5, \"State\", state_list)\n\ndata_mixed = data\ndata_mixed.index = range(len(data_mixed.index))\n\n#print(data_mixed.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"states_and_customers_list = data_mixed.State.value_counts()\n\nstates = gpd.read_file(\"../input/usa-states-geopandas/cb_2016_us_state_5m/cb_2016_us_state_5m.shp\")\n#print(states.head(10))\n\ngeos_to_drop = [26,29,51,52,53,54]\nstates.drop(geos_to_drop, inplace=True)\n\n\nstates_50 = states\nstates_50.index = range(len(states_50.index))\n\nstates_50.sort_values('STUSPS', inplace= True)\n\nstates_50.loc[28,\"STUSPS\"] = \"DE\"\n#print(states_50)\n\nstates_50 = states_50\nstates_50.index = range(len(states_50.index))\n\n\ngeo_list = []\n\nfor i in list_shortcut_states:\n    #print(i,\" \",states_and_customers_list[i])\n    geo_list.append(states_and_customers_list[i])\n   \nVALUES = gpd.GeoDataFrame(geo_list)\n\n#print(states_50)\n\n\nusa = pd.concat([states_50, VALUES], axis=1)\nusa.columns = [\"STATEFP\",\"STATENS\",\"AFFGEOID\",\"GEOID\",\"STUSPS\",\"NAME\",\"LSAD\",\"ALAND\",\"AWATER\",\"geometry\",\"Customers\"]\n\nusa = usa\n\n#indexes_to_drop = [0]\n# Alaska got dropped for visualisation\n#usa.drop(indexes_to_drop, inplace=True)\n\nfig = plt.figure(1, figsize=(10,10))\nax = fig.add_subplot()\nplt.xlim(-130, -60)\nplt.ylim(20,55)\n\nusa.plot(column=\"Customers\",ax=ax, legend= True, cmap=\"YlOrBr\", legend_kwds = {\"label\": \"Customers in states\", \"orientation\":\"horizontal\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"states_and_spent_list = []\n\nfor i in list_shortcut_states:\n    state_rows = data_mixed.loc[data_mixed[\"State\"] == i]\n\n    state_spent = state_rows[\"Yearly Amount Spent\"].sum()\n\n    states_and_spent_list.append(state_spent)\n\n#print(states_and_spent_list)\n#print(usa.head())\n\n\nstates_and_spent_list = gpd.GeoDataFrame(states_and_spent_list)\n\n#print(states_and_spent_list.head())\n\nusa_spent = pd.concat([usa, states_and_spent_list], axis=1)\nusa_spent.columns = [\"STATEFP\",\"STATENS\",\"AFFGEOID\",\"GEOID\",\"STUSPS\",\"NAME\",\"LSAD\",\"ALAND\",\"AWATER\",\"geometry\",\"Customers\",\"Avg. Amount Spent\"]\n#print(usa_spent.head())\n\nfig = plt.figure(1, figsize=(10,10))\nax = fig.add_subplot()\nplt.xlim(-130, -60)\nplt.ylim(20,55)\nusa_spent.plot(column=\"Avg. Amount Spent\",ax=ax, legend= True, cmap=\"YlGn\", legend_kwds = {\"label\": \"Avg. amount spent in states\",\"orientation\":\"horizontal\"})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Creating dummy variables**"},{"metadata":{"trusted":true},"cell_type":"code","source":"state_group_list = []\n\nfor i in data_mixed[\"State\"]:\n    # 12 - 10\n    if i in ([\"MO\",\"DE\",\"SC\",\"OR\",\"VT\",\"FL\",\"MS\",\"MN\",\"KS\",\"NJ\",\"NC\"]):\n        state_group_list.append(\"HIGH\")\n    # 9 - 8\n    if i in ([\"AZ\",\"HI\",\"AL\",\"MI\",\"WV\",\"ME\",\"ND\",\"NY\",\"IL\",\"TX\",\"PA\",\"GA\",\"KY\",\"MT\"]):\n        state_group_list.append(\"MEDIUM\")\n    # 7 - 0\n    if i in ([\"MA\",\"OK\",\"WY\",\"IN\",\"IA\",\"SD\",\"AK\",\"NH\",\"RI\",\"CA\",\"NV\",\"NE\",\"VA\",\"LA\",\"NM\",\"AR\",\"WI\",\"OH\",\"CT\",\"MD\",\"CO\",\"TN\",\"UT\",\"WA\",\"ID\"]):\n        state_group_list.append(\"LOW\")\n\nstate_group_series_new = pd.Series(state_group_list)\n\ndata_mixed_new = pd.concat([data_mixed, state_group_series_new], axis=1)\n\ndata_mixed_new.columns = [\"Avg. Session Length\",\"Time on App\",\"Time on Website\",\"Length of Membership\",\"Yearly Amount Spent\",\"State\",\"State group\"]\n\n#print(data_mixed_new.head(5))\n\nhot_states = pd.get_dummies(data_mixed_new[\"State group\"])\n#print(hot_states.head(5))\n\ndata_hot_clustering = pd.concat([data_mixed_new,hot_states], axis=1)\ndisplay(data_hot_clustering.head(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Visualisation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_hot_clustering_num = data_hot_clustering[[\"Avg. Session Length\",\"Time on App\",\"Time on Website\",\"Length of Membership\",\"Yearly Amount Spent\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data_hot_clustering_num)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ****DBSCAN****"},{"metadata":{},"cell_type":"markdown","source":"**Standardizing numerical data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dbscan = data_hot_clustering[[\"Avg. Session Length\",\"Time on App\",\"Time on Website\",\"Length of Membership\",\"Yearly Amount Spent\",\"HIGH\",\"MEDIUM\",\"LOW\"]]\n\nscale = StandardScaler()\n\ndata_dbscan.iloc[:,[0,1,2,3,4]] = scale.fit_transform(data_dbscan.iloc[:,[0,1,2,3,4]])\n\ndata_dbscan.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Finding optimal values for eps**"},{"metadata":{"trusted":true},"cell_type":"code","source":"neigh = NearestNeighbors(n_neighbors=3)\nnbrs = neigh.fit(data_dbscan)\ndistances, indices = nbrs.kneighbors(data_dbscan)\n\ndistances = np.sort(distances, axis=0)\n\ndistances = distances[:,1]\n\nplt.plot(distances)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- eps = 1.6\n- min_samples >= dimensinality + 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"dbscan = DBSCAN(eps=1.6, min_samples=10)\nclusters_dbscan = dbscan.fit_predict(data_dbscan)\n\nclusters_dbscan","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2D eps"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = data_dbscan.iloc[:,[0,4]]\n\nneigh = NearestNeighbors(n_neighbors=3)\nnbrs = neigh.fit(x)\ndistances, indices = nbrs.kneighbors(x)\n\ndistances = np.sort(distances, axis=0)\n\ndistances = distances[:,1]\n\nplt.plot(distances)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data_dbscan.iloc[:,[0,4]].values\n\ndbscan = DBSCAN(eps=0.4, min_samples=8)\nclusters = dbscan.fit_predict(X)\nlabels = dbscan.labels_\n\nsns.scatterplot(X[:,0], X[:,1], hue=[\"cluster: {}\".format(i) for i in labels])\nplt.xlabel(\"Avg. Session Length\")\nplt.ylabel(\"Yearly Amount Spent\")\nplt.title('DBSCAN clustering 2D' )\nplt.legend(fancybox=False, fontsize='small')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# K-Prototypes"},{"metadata":{},"cell_type":"markdown","source":"Choosing the correct data \n   * K-Prototypes can handle categorical variables, so we do not utilize one-hot encoding\n   * We do need to standardize the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_kproto = data_hot_clustering[[\"Avg. Session Length\",\"Time on App\",\"Time on Website\",\"Length of Membership\",\"Yearly Amount Spent\",\"State group\"]]\n\nscale = StandardScaler()\n\ndata_kproto.iloc[:,[0,1,2,3,4]] = scale.fit_transform(data_kproto.iloc[:,[0,1,2,3,4]])\n\ndata_kproto.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Turning data_kproto into numpy array --> K-Prototypes takes a numpy array as input"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_kproto_array = data_kproto.values\ndata_kproto_array","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The numerical data has to be of type float --> it already is in this case\n\n    *data_kproto_array[:,0] = data_kproto_array[:,0].astype(float)\n    *data_kproto_array[:,1] = data_kproto_array[:,1].astype(float)\n    *data_kproto_array[:,2] = data_kproto_array[:,2].astype(float)\n    *data_kproto_array[:,3] = data_kproto_array[:,3].astype(float)\n    *data_kproto_array[:,4] = data_kproto_array[:,4].astype(float)\n    \nwould be a possible way of turning int to float"},{"metadata":{},"cell_type":"markdown","source":"Choosing optimal number of clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"cost = []\nfor num_clusters in list(range(1,8)):\n    kproto = KPrototypes(n_clusters=num_clusters, init='Cao')\n    kproto.fit_predict(data_kproto_array, categorical=[5])\n    cost.append(kproto.cost_)\n\nplt.plot(cost)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The diagram above shows the optimal number of clusters. The kink in the function hints the optimal number. The diagram suggests one cluster is optimal. We chose three clusters, because after one the slope of the function is still quite steep."},{"metadata":{"trusted":true},"cell_type":"code","source":"kproto = KPrototypes(n_clusters=3, max_iter=20)\nclusters_kproto = kproto.fit_predict(data_kproto_array, categorical=[5])\nclusters_kproto","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us take a deeper look into the cluster values!"},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_list = []\nfor i in clusters_kproto:\n    cluster_list.append(i)\n\n#data_kproto[\"Clusters\"].loc[cluster_list]\ndata_kproto[\"Clusters\"] = cluster_list\ndata_kproto.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cluster 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_0 = data_kproto[data_kproto[\"Clusters\"] == 0]\n\nindices_cluster_0 = cluster_0.index\n\ncluster_0_df = []\n\nfor i in indices_cluster_0:\n    cluster_0_df.append(data_hot_clustering.iloc[i])\n\ncluster_0_df = pd.DataFrame(cluster_0_df)\n\ndisplay(cluster_0_df.iloc[:,[0,1,2,3,4]].describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Cluster 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_1 = data_kproto[data_kproto[\"Clusters\"] == 1]\n\nindices_cluster_1 = cluster_1.index\n\ncluster_1_df = []\n\nfor i in indices_cluster_1:\n    cluster_1_df.append(data_hot_clustering.iloc[i])\n\ncluster_1_df = pd.DataFrame(cluster_1_df)\n\ndisplay(cluster_1_df.iloc[:,[0,1,2,3,4]].describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cluster 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_2 = data_kproto[data_kproto[\"Clusters\"] == 2]\n\nindices_cluster_2 = cluster_2.index\n\ncluster_2_df = []\n\nfor i in indices_cluster_2:\n    cluster_2_df.append(data_hot_clustering.iloc[i])\n\ncluster_2_df = pd.DataFrame(cluster_2_df)\n\ndisplay(cluster_2_df.iloc[:,[0,1,2,3,4]].describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Key takeaways**\n    \n\"Avg. Session Length\" & \"Time on App\" & \"Time on Website\" are very similar throughout the clusters\n\nCluster 0 has the lowest mean values: \"Length of Membership\" and \"Yearly Amount Spent\" (~ 2.8 years and ~ 433$) & 150 people in cluster 0\n\nCluster 1 is the middleground: \"Length of Membership\" and \"Yearly Amount Spent\" (~ 3.9 years and ~ 510$) & 130 people in cluster 1\n\nCluster 2 has the highest mean values: \"Length of Membership\" and \"Yearly Amount Spent\" (~ 4.3 years and ~ 585$) & 113 people in cluster 2\n    \n\n\n--> it appears that long-standing and new customers like to surf the website for a shorter period of time as well as spending more time on the website (when they are buying merchandise?)\n--> as one would expect with increasing \"Average Amount Spent\" and \"Length of Memebership\" the actual number of people in the clusters decrease\n    "},{"metadata":{},"cell_type":"markdown","source":"2D graph of \"Yearly Amount Spent\" and \"Average Session Length\""},{"metadata":{"trusted":true},"cell_type":"code","source":"x = data_kproto.iloc[:,[0,4,5]].values\n\nkproto = KPrototypes(n_clusters=3)\nclusters = kproto.fit_predict(x, categorical=[2])\n\nplt.scatter(x[clusters == 0, 0], x[clusters == 0, 1], s = 25, c = 'orange', label=\"cluster 0\")\nplt.scatter(x[clusters == 1, 0], x[clusters == 1, 1], s = 25, c = \"blue\", label=\"cluster 1\")\nplt.scatter(x[clusters == 2, 0], x[clusters == 2, 1], s = 25, c = 'cyan', label=\"cluster 2\")\nplt.xlabel(\"Average Session Length\")\nplt.ylabel(\"Yearly Amount Spent\")\nplt.title('K-Prototypes clustering 2D' )\nplt.legend(fancybox=False, fontsize='small')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Interpretation**\n\nCluster 0 spend an above average amount of money each year & and tend to have a high \"Average Session Length\"\n\nCluster 1 has a relatively long \"Average Session Length\". Interestingly enough their \"Yearly Amount Spent\" values are lesser compared to Cluster 0 and 2. It seems like these customers like to browse the website but not actually buy. Maybe they are hesitant because of certain insecurities. These could potentially be quality and/ or price. \n\nCluster 2: The customers purchase quickly of the website/ app and spend low to medium funds on merchandise\n\n\n--> The question arises what makes some customers hesitate to buy? "},{"metadata":{},"cell_type":"markdown","source":"# Validation"},{"metadata":{},"cell_type":"markdown","source":"For the validation of our cluster we will be using the silhouette coefficient\n\nInterpretation:\n\n-1: bad heterogeneous clustering\n\n0: overlapping clusters\n\n1: ideal homogeneous clustering"},{"metadata":{},"cell_type":"markdown","source":"**DBSCAN**"},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.silhouette_score(X= data_dbscan, labels=clusters_dbscan,metric='euclidean')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**K-Prototype**"},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.silhouette_score(X= data_dbscan, labels=clusters_kproto,metric='euclidean')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The k-prototypes silhouette score was calculated using X=data_dbscan (one-hot encoded), because of the categorical nature of the data. I am not sure how else to handle this situation."},{"metadata":{},"cell_type":"markdown","source":"# Thank you very much for your time! We hope you enjoyed it :) "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}