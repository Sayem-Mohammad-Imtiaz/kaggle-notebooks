{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Titanic dataset\n\n**compétition Kaggle - Cédric LEBOCQ<br>**\n\n### 1) Analyse des données\n\n### 2) Nettoyage des données / Feature Engineering \n\n### 3) Preprocessing des données\n\n### 4) Utilisation de modéle basiques\n\n### 5) Tuning des modéles\n\n### 6) Resultats ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\n# data visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dstrain = pd.read_csv(\"../input/titanic/train.csv\")\ndstrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dstest = pd.read_csv(\"../input/titanic/test.csv\")\ndstest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combinaisons des deux dataset en un seul pour ce faciliter le traitement des données\nds = pd.concat([dstrain, dstest], sort=False, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1) Analyse des données","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ds.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Vérifier les valeurs nulles et afficher leur pourcentage\ntotal = ds.isna().sum().sort_values(ascending=False)\npercent_1 = ds.isna().sum()/ds.isna().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\nmissing_data = missing_data[missing_data['%']!=0]\nmissing_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Récupérer les valeurs uniques des différentes colonnes\ncolsWithUniqueValues =  ['Survived', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']\nfor col in colsWithUniqueValues:\n    try:\n        print(f\"{col:<10} {sorted(ds[col].dropna().unique())}\")\n    except:\n        print(f\"{col:<10} {ds[col].unique()}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# On peut voir ci-dessus que 38% du dataset a survecu\n# Normalement Ticket, PassengerId ne devraient pas avoir d'impact sur le taux de survie\nds.drop(['PassengerId','Ticket'],axis=1,inplace = True)\nds.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2) Nettoyage des données / Feature Engineering ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Remplacement des valeur nan dans la colonne Age\n\nOn va utiliser la même distribution que les Age présents pour l'instant","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# les ages ne suivent pas une distribution 'normale'\nfrom scipy.stats import normaltest\n# test de normalité\nstat, p = normaltest(ds['Age'].dropna())\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\n# interpretation\nalpha = 0.05\nif p > alpha:\n    print(\"l'échantillon semble Gaussien (impossible de rejeter l'hypothése nulle H0)\")\nelse:\n    print(\"l'échantillon ne semble pas Gaussien (rejet de l'hypothése nulle H0)\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Renvoit un age au hasard pour la colonne Age qui suit la distribution des valeurs de la colonne\n# utilisation de sample (Pandas)\nds['Age'].dropna().sample(ds['Age'].isnull().sum(),random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def impute_nan(df,variable):\n    # on duplique la colonne dans un premier temps (y compris les valeurs nulles qu'elle contient)\n    df[variable+\"_rnd\"]=df[variable]\n    # on génére une série de la taille des Age qui sont null et on sample dedans avec la distrib de la colonne Age\n    random_sample=df[variable].dropna().sample(df[variable].isnull().sum(),random_state=0)\n    # Pour réaliser la fusion, on conserve les index des Age null\n    random_sample.index=df[df[variable].isnull()].index\n    df.loc[df[variable].isnull(),variable+'_rnd']=random_sample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# on remplace dans le dataset les valeurs nulles dans la colonne Age\nimpute_nan(ds,\"Age\")\nds.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# On voit ci-dessous qu'en remplissant les 263 valeurs age manquantes que l'on n'a pas (peu) modifié la distribution\nfig = plt.figure()\nax = fig.add_subplot(111)\nds['Age'].plot(kind='kde', ax=ax)\nds.Age_rnd.plot(kind='kde', ax=ax, color='green')\nlines, labels = ax.get_legend_handles_labels()\nax.legend(lines, labels, loc='best')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# La colonne Age ne sert plus, elle contient des valeurs nulles, on utilisera Age_rnd que l'on vient de créer\nds.drop(['Age'],axis=1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Port d'embarquement absent","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Le port d'embarquement le plus fréquent est 'S'\ndstrain['Embarked'].describe() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"common_value = 'S'\nds['Embarked'] = ds['Embarked'].fillna(common_value)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calcul du nombre de personnes accompagnantes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calcul du nombre de personnes accompagnantes\nds['Relatives'] = ds['SibSp'] + ds['Parch']\n\n# création d'une colonne not alone qui va permettre d'avoir un booléen permettant de savoir\n# si le voyageur est seul ou accompagné\nds.loc[ds['Relatives'] > 0, 'Not_alone'] = 1\nds.loc[ds['Relatives'] == 0, 'Not_alone'] = 0\nds['Not_alone'] = ds['Not_alone'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Taux de survie en fonction du nombre de personnes accompagnantes\naxes = sns.catplot('Relatives','Survived', data=ds, aspect = 2.5,kind='point')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# il semblerait que l'on survive plus en voyageant avec 1 à 3 personnes\n# nous allons créer trois classes pour représenter ce que l'on peut apprécier visuellement sur le graphe\ndef family_cat(size):\n    if (size >= 1) & (size < 4):\n        return 0\n    elif ((size >= 4) & (size < 7)) | (size == 0):\n        return 1\n    elif (size >= 7):\n        return 2\n    \nds['Famcat'] = ds['Relatives'].apply(family_cat)\nds['Famcat'] = ds['Famcat'].astype(int)\nds.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Graphe du taux de survie en fonction de la catégorie de famille et du sexe\nplt.figure(figsize=(8, 8))\nsns.barplot(x=\"Famcat\", y=\"Survived\", hue=\"Sex\", data=ds, palette='Blues_d')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### calcul du Fare par personne","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ds['Fare_Per_Person'] = ds['Fare'].fillna(0)/(ds['Relatives']+1)\nds.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dans le dataset on a un Fare inconnu. Mais on sait que le voyageur était seul\nds[ds['Fare'].isna()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# on va donc utiliser la moyenne de la classe 3 pour son Fare et Fare_Per_Person\nfare = ds[ds['Pclass']==3]['Fare_Per_Person'].mean()\nfare","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds.loc[ds['Fare'].isna(),'Fare_Per_Person'] = fare\nds.loc[ds['Fare'].isna(),'Fare'] = fare","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# vérification\nds.iloc[1043,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Taux de survie en fonction du Fare_Per_Person et du sexe\ndstemp = ds.copy()\nplt.figure(figsize=(8, 8))\n# on crée des tranches de 'Fare' équilibrées avec qcut pour le graphe\ndstemp['Fare_Per_Person'] = pd.qcut(dstemp['Fare_Per_Person'], 5)\nsns.barplot(x=\"Fare_Per_Person\", y=\"Survived\", data=dstemp, hue =\"Sex\", palette='Blues_d')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Création d'une feature Titre, d'aprés le contenu du nom","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating 'Title' column\nds['Title'] = ds['Name'].str.extract(' ([A-Za-z]+)\\.', expand = False)\nds['Title'].unique().tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pourcentage de passagers en fonction du titre\nds['Title'].value_counts(normalize=True)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Taux de survie en fonction du Titre\nds.groupby(['Title'])['Survived'].mean().sort_values(ascending=False) * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pas de null dans la colonne crée\nds['Title'].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Création de catégories pour regrouper les Titre qui ont un taux de survie équivalent\n# Dona est surement un titre du dataset de test, mais c'est un titre de noblesse, on le met en catégorie Top\n\nds['Title'] = ds['Title'].replace(['Sir', 'Countess', 'Mme', 'Mlle', 'Dona' , 'Lady'], 'Top')\nds['Title'] = ds['Title'].replace(['Mrs', 'Miss'], 'High')\nds['Title'] = ds['Title'].replace(['Master', 'Dr', 'Col', 'Major', 'Ms'], 'Mid')\nds['Title'] = ds['Title'].replace(['Mr'], 'Low')\nds['Title'] = ds['Title'].replace(['Jonkheer', 'Rev', 'Don', 'Capt'], 'Bottom')\n\nds['Title'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Taux de survie en fonction des catégories et du sexe \nplt.figure(figsize=(8, 8))\nsns.barplot(x=\"Title\", y=\"Survived\", data=ds, order = ['Bottom','Low','Mid','High','Top'], hue =\"Sex\", palette='Blues_d')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analyse de la feature Cabine et création d'une feature Deck","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# beaucoup de valeurs nulles dans la feature Cabin, mais pour le cabines renseignées\n# on peut voir qu'elles commencent toutes par une lettre\nds[ds['Cabin'].isna() == False].head(5) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds['Cabin'] = ds['Cabin'].fillna('Unknown')\nds['Deck']=ds['Cabin'].str.get(0)\n\nds[ds['Cabin']!='Unknown'].head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted(ds['Deck'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualisation du taux de survie par pont (Deck) et par sexe\nplt.figure(figsize=(8, 8))\nsns.barplot(x='Deck', y='Survived', data=ds, hue = 'Sex' ,palette='ocean', order = sorted(ds['Deck'].unique()))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3) Preprocessing des données","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Remplacement des valeurs catégoriques ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ds['Pclass'] = ds['Pclass'].astype(str)\nds = pd.get_dummies(ds, columns=['Pclass','Embarked','Famcat','Title','Sex','Deck'],drop_first=False)\nds.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Suppression des colonnes inutiles","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop des colonnes qui ne servent plus\nds.drop(['Name','SibSp','Parch','Cabin'],axis=1,inplace = True)\nds.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualisations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Séparation des données Train / Test\ntrain = ds[:len(dstrain)]\n\n# on récupére la matrice de corrélation générale\ncorr = train.corr()\n# on l'affiche avec sns\nplt.figure(figsize=(15,15))\nsns.heatmap(corr,annot = True, fmt='.1g',vmin=-1, vmax=1, center= 0, square = True, cbar = None, cmap= 'coolwarm')\n\n# on peut voir ci dessous que le sexe est la feature la plus fortement correlée à la survie avec le titre\n# mais les autres (Fare_Per_Person, Fare, Embarked, Pclass, Famcat, Title, Not_alone) le sont aussi dans une moindre mesure","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4) Utilisation de modéle basiques","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Normalisation des données numériques","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalisation des données numériques\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nds[['Age_rnd', 'Fare', 'Fare_Per_Person', 'Relatives']] = scaler.fit_transform(ds[['Age_rnd', 'Fare', 'Fare_Per_Person', 'Relatives']])\nds.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Séparation des données Train / Test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Séparation des données Train / Test\ntrain = ds[:len(dstrain)]\n\n# Splitting dataset into test\ntest = ds[len(dstrain):]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Premier tests avec RandomForestClassifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Premier test rapide avec RandomForestClassifier   \nfrom sklearn.ensemble import RandomForestClassifier\nX_train = train.drop(\"Survived\", axis=1)\nY_train = train[\"Survived\"]\n\nrandom_forest = RandomForestClassifier(n_estimators=100,random_state=0)\nrandom_forest.fit(X_train, Y_train)\n\nrandom_forest.score(X_train, Y_train)\nprint(\"Train score : \",round(random_forest.score(X_train, Y_train) * 100, 2))\n\n# le score nous montre qu'on est probablement en overfitting...","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# On peut récuperer les features qui ont été importante pour ce modéle\nimportances = pd.DataFrame({'feature':X_train.columns,'importance':np.round(random_forest.feature_importances_,3)})\nimportances = importances.nlargest(20,'importance').set_index('feature')\nimportances","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Avec ExtraTreesClassifier\n\nIl est probable qu'avec de la cross validation, le résultat ne sera plus du tout le même pour les TreeClassifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\nmodel=ExtraTreesClassifier()\nmodel.fit(X_train,Y_train)\nranked_features=pd.Series(model.feature_importances_,index=X_train.columns)\nplt.figure(figsize=(8, 8))\nranked_features.nlargest(len(X_train.columns)).sort_values(ascending=True).plot(kind='barh')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.score(X_train, Y_train)\nprint(\"Train score : \",round(model.score(X_train, Y_train) * 100, 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DecisionTree","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier,plot_tree\nmodel=DecisionTreeClassifier()\nmodel.fit(X_train,Y_train)\nranked_features=pd.Series(model.feature_importances_,index=X_train.columns)\nplt.figure(figsize=(8, 8))\nranked_features.nlargest(len(X_train.columns)).sort_values(ascending=True).plot(kind='barh')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.score(X_train, Y_train)\nprint(\"Train score : \",round(model.score(X_train, Y_train) * 100, 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Avec un DecisionTree (CART), on peut afficher l'abre de décision qui a été crée\n'''\nplt.figure(figsize=(100,100))\nplot_tree(model,feature_names=X_train.columns,class_names=\"Survived\", filled=True,fontsize=6)\nplt.savefig(\"dt.jpg\",dpi = 100)\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SelectKBest pour obtenir l'importance des features par un test d'indépendance chi2\n\nLes best features n'ont rien à voir avec les classifiers de type ensemble","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SelectKBest Algorithm\nordered_rank_features=SelectKBest(score_func=chi2,k=20)\nordered_feature=ordered_rank_features.fit(X_train,Y_train)\ndfscores=pd.DataFrame(ordered_feature.scores_,columns=[\"Score\"])\ndfcolumns=pd.DataFrame(X_train.columns)\nfeatures_rank=pd.concat([dfcolumns,dfscores],axis=1)\nfeatures_rank.columns=['Features','Score']\nfeatures_rank = features_rank.sort_values('Score',ascending=False).set_index('Features')\nfeatures_rank\n\n# les features les plus liées au taux de survie, sont le sexe, le titre, la classe, la catégorie de famille (telle qu'on l'a crée).\n# on peut voit notamment que l'age et le prix du billet ne sont pas si important","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ranked_features=pd.Series(ordered_feature.scores_,index=X_train.columns)\nplt.figure(figsize=(8, 8))\nranked_features.nlargest(len(X_train.columns)).sort_values(ascending=True).plot(kind='barh')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comparaisons de plusieurs modéles avec cross validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC,LinearSVC\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n\nseed = 47\n# preparation des modéles\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('SGD', SGDClassifier()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier(3)))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('EXT', ExtraTreesClassifier()))\nmodels.append(('RF', RandomForestClassifier()))\nmodels.append(('ADAB', AdaBoostClassifier()))\nmodels.append(('GDB', GradientBoostingClassifier()))\nmodels.append(('SVM', SVC()))\nmodels.append(('LSVC', LinearSVC()))\nmodels.append(('XGB', XGBClassifier()))\n# évaluation des modéles\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=5, random_state=seed, shuffle=True)\n    cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring,n_jobs=-1)\n    results.append(cv_results)\n    names.append(name)\n    msg = f\"{name:<5}: Mean={cv_results.mean():-<10.3f}Median={np.median(cv_results):-<10.3f}std={cv_results.std():.4f}\"\n    print(msg)\n# graphe de comparaison en boxplot \nfig = plt.figure()\nfig.suptitle('Comparaison des Algorithmes')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5) Tuning des modéles choisis\n\nNous allons conserver les modéles suivants :<BR>\n\n<code>LR   : Mean=0.827-----Median=0.831-----std=0.0283\nLDA  : Mean=0.826-----Median=0.837-----std=0.0254\nRF   : Mean=0.796-----Median=0.809-----std=0.0369\nGDB  : Mean=0.827-----Median=0.837-----std=0.0241\nSVM  : Mean=0.819-----Median=0.820-----std=0.0163\nLSVC : Mean=0.829-----Median=0.843-----std=0.0285\nXGB  : Mean=0.804-----Median=0.809-----std=0.0400</code>    \n<BR>\nOn commencera par RandomizedSearchCV pour trouver les meilleurs réglages, puis GridSearchCV pour affiner","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### RandomizedSearchCV\n\nva nous permettre de trouver rapidement quels sont les meilleurs Hyperparamétres qu'il faudra ensuite régler plus finement","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, log_loss, confusion_matrix\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n\n#Modéles que l'on va essayer\nclf_lr = LogisticRegression()\nclf_lda = LinearDiscriminantAnalysis()\nclf_rf = RandomForestClassifier()\nclf_gdb = GradientBoostingClassifier()\nclf_svm = SVC()\nclf_lsvc = LinearSVC()\nclf_xgb = XGBClassifier()\n\nclassifiers = [clf_lr, clf_lda, clf_rf, clf_gdb,clf_svm,clf_lsvc,clf_xgb]\n\n### paramétres de départ pour RandomizedSearchCV\n\n### ------------------\n### LogisticRegression\n### ------------------\nparam_lr = {\"penalty\" :          [\"l1\",\"l2\"],\n            \"tol\" :              [0.0001,0.0002,0.0003],\n            \"max_iter\":          [100,300,500,800,1000],\n            \"C\" :                [0.01, 0.1, 1, 10, 100],\n            \"intercept_scaling\": [1, 2, 3, 4],\n            \"solver\":['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n\n### --------------------------\n### LinearDiscriminantAnalysis\n### --------------------------\nparam_lda = {\"solver\" : ['svd', 'lsqr', 'eigen']}\n\n### ----------------------\n### RandomForestClassifier\n### ----------------------\n# Nombre d'abre dans RandomForest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Nombre de features à considérer à chaque split\nmax_features = ['auto', 'sqrt']\n# Nombre maximum de niveau dans les abres\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Combien d'echantillon au minimum pour séparer un noeud\nmin_samples_split = [2, 5, 10]\n# Nombre minimum d'échantillons dans chaque feuille\nmin_samples_leaf = [1, 2, 4]\n# Méthode de séléction des échantillons\nbootstrap = [True, False]\n\nparam_rf = {'n_estimators':      n_estimators,\n            'criterion':         ['entropy', 'gini'],\n            'max_features':      max_features,\n            'max_depth':         max_depth,\n            'min_samples_split': min_samples_split,\n            'min_samples_leaf':  min_samples_leaf,\n            'bootstrap':         bootstrap}\n\n### --------------------------\n### GradientBoostingClassifier\n### --------------------------\n\nparam_gdb = {\n    \"loss\":              [\"deviance\"],\n    \"learning_rate\":     [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n    \"min_samples_split\": np.linspace(0.1, 0.5, 12),\n    \"min_samples_leaf\":  np.linspace(0.1, 0.5, 12),\n    \"max_depth\":         [3,5,8],\n    \"max_features\":      [\"log2\",\"sqrt\"],\n    \"criterion\":         [\"friedman_mse\",  \"mae\"],\n    \"subsample\":         [0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n    \"n_estimators\":      [10,100,500]}\n\n\n### -------\n### XGBoost\n### -------\n\nparam_xgb = {\n    'n_estimators':     [10,100,500],\n    'colsample_bytree': [0.75,0.8,0.85],\n    'max_depth':        [10,50,100,None],\n    'reg_alpha':        [1],\n    'reg_lambda':       [2, 5, 10],\n    'subsample':        [0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n    'learning_rate':    [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n    'gamma':            [.5,1,2],\n    'min_child_weight': [0.01],\n    'sampling_method':  ['uniform']}\n\n\n### ---------------\n### SVC \n### ---------------\n\nparam_svc = {'gamma':  [1e-3, 1e-2,0.1, 1, 10,100],\n             'C':      [1e-2,0.1,1, 10, 100, 1000],\n             'kernel': ['linear', 'rbf'],\n             'degree': [1,2,3]}\n\n### ---------------\n### LinearSVC \n### ---------------\n\nparam_lsvc = {'C':        [1e-2,0.1,1, 10, 100, 1000],\n              'tol' :     [0.0001,0.0002,0.0003],\n              'max_iter': [100,300,500,800,1000]}\n\n\nparameters = [param_lr, param_lda, param_rf, param_gdb, param_svc, param_lsvc, param_xgb]\n\n\nclf_best_acc = []\nclf_best_params = []\nclf_best_estimator = []\nrnd_searchs = [] \n\n#Recherche sur tous les classifiers\nfor i in range(len(classifiers)):\n    rnd_searchs.append(RandomizedSearchCV(estimator = classifiers[i],\n                                 param_distributions = parameters[i],\n                                 scoring = 'accuracy',\n                                 n_iter = 40,\n                                 cv = 5,\n                                 verbose=0,\n                                 random_state=47,\n                                 n_jobs = -1))\n    \n    print(classifiers[i].__class__.__name__)\n    underline=['-']*len(classifiers[i].__class__.__name__)\n    print(''.join(underline))\n    rnd_searchs[i].fit(X_train, Y_train)\n    print(\"Meilleurs paramétres :\",rnd_searchs[i].best_params_)\n    print(\"Score : \",rnd_searchs[i].best_score_)\n    clf_best_acc.append(rnd_searchs[i].best_score_)\n    clf_best_params.append(rnd_searchs[i].best_params_)\n    clf_best_estimator.append(rnd_searchs[i].best_estimator_)\n\nprint(\"\")    \nprint(\"RandomSearchCV Terminé...\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelNames=[]\nfor model in classifiers:\n    modelNames.append(model.__class__.__name__)\nrndtunedScores=pd.Series(clf_best_acc,index=modelNames)\nrndtunedScores=rndtunedScores-0.8\nplt.figure(figsize=(8, 8))\nrndtunedScores.sort_values(ascending=True).plot(kind='barh',left = 0.8)\nplt.show()\n# comme on peut le voir tous les modéles sont dans un mouchoir de poche...","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### GridSearchCV pour régler finement les paramétres\n\n**LogisticRegression:<br>**\n{'tol': 0.0003, 'solver': 'sag', 'penalty': 'l2', 'max_iter': 800, 'intercept_scaling': 2, 'C': 0.1}<br>\n0.824913690289373<br>\n<br>\n**LinearDiscriminantAnalysis:<br>**\n{'solver': 'svd'}<br>\n0.8282719226664993<br>\n<br>\n**RandomForestClassifier :<br>**\n{'n_estimators': 1400, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 30, 'criterion': 'entropy', 'bootstrap': False}<br>\n0.8327600276191074<br>\n<br>\n**GradientBoostingClassifier:<br>**\n{'subsample': 0.9, 'n_estimators': 500, 'min_samples_split': 0.24545454545454548, 'min_samples_leaf': 0.13636363636363638, 'max_features': 'sqrt', 'max_depth': 5, 'loss': 'deviance', 'learning_rate': 0.075, 'criterion': 'friedman_mse'}<br>\n0.8350072186303434<br>\n<br>\n**SVC:<br>**\n{'kernel': 'rbf', 'gamma': 0.1, 'degree': 1, 'C': 1}<br>\n0.8316238779737617<br>\n<br>\n**LinearSVC:<br>**\n{'tol': 0.0003, 'max_iter': 100, 'C': 0.1}<br>\n0.8305191136777352<br>\n<br>\n**XGBClassifier:<br>**\n{'subsample': 0.95, 'sampling_method': 'uniform', 'reg_lambda': 10, 'reg_alpha': 1, 'n_estimators': 100, 'min_child_weight': 0.01, 'max_depth': 100, 'learning_rate': 0.05, 'gamma': 1, 'colsample_bytree': 0.85}<br>\n0.8361559224154165<br>\n<br>\n - On ne fera pas mieux avec LDA que 0.825, car aucun autre HyperParamétre à régler\n - On peut probablement encore gagner un peu en accuracy en réglant finement les autres modéles\n - Essayons avec LogisticRegression, RandomForest et XGBoost\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import learning_curve\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LogisticRegression fine tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### ------------------\n### LogisticRegression\n### ------------------\nparam_lr = {  \"penalty\" : [\"l2\"],\n              \"tol\" : [0.0001,0.00015,0.0002],\n              \"max_iter\": [1,2,5,10,100,200,600,800],\n              \"C\" :[0.01, 0.1, 1],\n              \"intercept_scaling\": [2,3,4],\n              \"solver\":['sag']}\n\nclf_lr = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CV=5 / petite amélioration \nbest_clf_lr = GridSearchCV(clf_lr, param_grid = param_lr, cv = 5, verbose = False, n_jobs = -1).fit(X_train,Y_train)\nprint(best_clf_lr.best_params_)\nprint(best_clf_lr.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CV = 10 améliore encore le résultat\nbest_clf_lr = GridSearchCV(clf_lr, param_grid = param_lr, cv = 10, verbose = False, n_jobs = -1).fit(X_train,Y_train)\nprint(best_clf_lr.best_params_)\nprint(best_clf_lr.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotLearningCurve(model):\n    N, train_score, val_score = learning_curve(model, X_train, Y_train,\n                                                  train_sizes=np.linspace(0.1, 1, 10), cv=10)\n\n    # N contient le nombre d'éléments retenus pour faire l'entrainement\n    print(N)\n    # train_score contient pour toutes les itérations de train_sizes (10 ici), les résultat pour chaque cv sur le jeu de train\n    # val_score contient pour toutes les itérations de train_sizes (10 ici), les résultat pour chaque cv sur le jeu de validation\n    plt.plot(N, train_score.mean(axis=1), label='train')\n    plt.plot(N, val_score.mean(axis=1), label='validation')\n    plt.xlabel('train_sizes')\n    plt.legend() \n\n\n# on récupére le meilleur modéle trouvé grâce à GridSearchCV \nmodel = best_clf_lr.best_estimator_ \nplotLearningCurve(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# précédemment on a vu grace à learning curves, que la meilleur taille du dataset pour l'entrainement était la taille de 720\n# on réinjecte les paramétres trouvés avec GridSearchCV dans notre modéle, avec 10% de test_size\n\nmodel_lr = LogisticRegression(C=1,\n                           intercept_scaling=4,\n                           max_iter=10,\n                           penalty='l2',\n                           solver='sag',\n                           tol=0.00015,\n                           random_state=47)                          \nX_tr, X_te, y_tr, y_te = train_test_split(X_train, Y_train, test_size=0.1)\nmodel_lr.fit(X_tr, y_tr)\nprint(\"train : \",model_lr.score(X_tr, y_tr))\nprint(\"test  : \",model_lr.score(X_te, y_te))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RandomForestClassifier fine tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### ----------------------\n### RandomForestClassifier\n### ----------------------\nn_estimators = [int(x) for x in np.linspace(start = 1300, stop = 1500, num = 10)]\nmax_features = ['sqrt']\nmax_depth = [int(x) for x in np.linspace(20, 40, num = 10)]\nmin_samples_split = [4,5,6]\nmin_samples_leaf = [3,4,5]\nbootstrap = [False]\ncriterion = ['entropy']\n# Create the random grid\nparam_rf = {'n_estimators': n_estimators,\n            'criterion': criterion,\n            'max_features': max_features,\n            'max_depth': max_depth,\n            'min_samples_split': min_samples_split,\n            'min_samples_leaf': min_samples_leaf,\n            'bootstrap': bootstrap}","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"# désactivé car prends trop de temps sur Kaggle\n'''\nclf_rf = RandomForestClassifier()\nbest_clf_rf = GridSearchCV(clf_rf, param_grid = param_rf, cv = 5, verbose = True, n_jobs = -1).fit(X_train,Y_train)\nprint(best_clf_rf.best_params_)\nprint(best_clf_rf.best_score_)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n# on récupére le meilleur modéle trouvé grâce à GridSearchCV \nmodel = best_clf_rf.best_estimator_ \nplotLearningCurve(model)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_rf = RandomForestClassifier(bootstrap=False,\n                                  criterion='entropy',\n                                  max_depth=28,\n                                  max_features='sqrt',\n                                  min_samples_leaf=3,\n                                  min_samples_split=5,\n                                  n_estimators= 1477,\n                                  random_state=47)\nX_tr, X_te, y_tr, y_te = train_test_split(X_train, Y_train, test_size=0.3)\nmodel_rf.fit(X_tr, y_tr)\nprint(\"train : \",model_rf.score(X_tr, y_tr))\nprint(\"test  : \",model_rf.score(X_te, y_te))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost fine tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### -------\n### XGBoost\n### -------\n\nparam_xgb = {\n    'n_estimators':     [100],\n    'colsample_bytree': [0.85],\n    'max_depth':        [80,90],\n    'reg_alpha':        [1],\n    'reg_lambda':       [9,10,11],\n    'subsample':        [0.95],\n    'learning_rate':    [0.05],\n    'gamma':            [.75,1],\n    'min_child_weight': [0.01],\n    'sampling_method':  ['uniform']}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_xgb = XGBClassifier()\nbest_clf_xgb = GridSearchCV(clf_xgb, param_grid = param_xgb, cv = 5, verbose = False, n_jobs = -1).fit(X_train,Y_train)\nprint(best_clf_xgb.best_params_)\nprint(best_clf_xgb.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_clf_xgb = GridSearchCV(clf_xgb, param_grid = param_xgb, cv = 10, verbose = False, n_jobs = -1).fit(X_train,Y_train)\nprint(best_clf_xgb.best_params_)\nprint(best_clf_xgb.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# on récupére le meilleur modéle trouvé grâce à GridSearchCV \nmodel = best_clf_xgb.best_estimator_ \nplotLearningCurve(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_xgb = XGBClassifier(n_estimators =100,\n                            colsample_bytree = 0.85,\n                            max_depth=80,\n                            reg_alpha=1,\n                            reg_lambda=10,\n                            subsample=0.95,\n                            learning_rate=0.05,\n                            gamma=1,\n                            min_child_weight=0.01,\n                            sampling_method='uniform',\n                            random_state=47)\nX_tr, X_te, y_tr, y_te = train_test_split(X_train, Y_train, test_size=0.05)\nmodel_xgb.fit(X_tr, y_tr)\nprint(\"train : \",model_xgb.score(X_tr, y_tr))\nprint(\"test  : \",model_xgb.score(X_te, y_te))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = pd.read_csv(\"../input/titanic/test.csv\")\nId = temp.PassengerId\n\n# Splitting dataset into test\ntest = ds[len(dstrain):]\nX_test = test.drop(\"Survived\", axis=1)\n\nfinal_predictions_lr = model_lr.predict(X_test)\nfinal_predictions_rf = model_rf.predict(X_test)\nfinal_predictions_xgb = model_xgb.predict(X_test)\n\n\noutputlr = pd.DataFrame({'PassengerId': Id, 'Survived': final_predictions_lr.astype(int)})\noutputrf = pd.DataFrame({'PassengerId': Id, 'Survived': final_predictions_rf.astype(int)})\noutputxgb = pd.DataFrame({'PassengerId': Id, 'Survived': final_predictions_xgb.astype(int)})\n#outputlr.to_csv('../output/submissionlr.csv', index=False)\n#outputrf.to_csv('../output/submissionrf.csv', index=False)\n#outputxgb.to_csv('../output/submissionxgb.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bonus - VotingClassifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_vt = VotingClassifier([('LR', model_lr),\n                            ('RF', model_rf),\n                            ('XGB', model_xgb)],\n                            voting='hard')\n# Différence entre voting = soft / hard :\n# voting = soft => on additionne les probabilités de chaque classe pour tous les classifiers, on prend la classe qui a la somme\n# de proba la plus importante (fonctionne bien si pas trop de disparités entre classifiers)\n# voting = hard => on prend la classe qui a été prédite par le plus de classfiers\n\nmodel_vt.fit(X_train, Y_train)\nprint(model.score(X_train, Y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"final_predictions_vt = model_vt.predict(X_test)\noutputvt = pd.DataFrame({'PassengerId': Id, 'Survived': final_predictions_vt.astype(int)})\n#outputvt.to_csv('../output/submissionvt.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6) Résultats\n\nScores Kaggle :\n- Logistic Regression : **0.78947**<br>\n- RandomForest        : **0.75598**<br>\n- XGBoost             : **0.78229**<br>\n- Voting              : **0.76794**<br>\n\n","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}