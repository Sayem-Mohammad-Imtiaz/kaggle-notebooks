{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Packages","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport nltk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nnltk.download('vader_lexicon')\nfrom datetime import datetime\nimport spacy as sp\nnlps = sp.load('en')\nfrom spacy.matcher import PhraseMatcher, Matcher\nfrom collections import Counter\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/reddit-rwallstreetbets/r_wallstreetbets_posts.csv')","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load tickers\nimport requests # allows for file downloading\nimport os\n\nfilename = 'ticker.txt'\nurl = 'https://www.sec.gov/include/ticker.txt'\nif os.path.exists(filename):\n    print(f'{filename} already downloaded')\nelse:\n    r = requests.get(url, allow_redirects=True)\n    with open(filename, 'wb') as file:\n        file.write(r.content)\n    print(f'{filename} has been downloaded')\n","metadata":{"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# put tickers into dataframe\ntickers = pd.read_csv('ticker.txt', delimiter = \"\\t\", header=None)\ntickers.columns = ['symbol', 'code']\n\n# Note: some tickers are the same as common words or letters such as NAN, K, AND\ntickers['symbol'] = tickers['symbol'].str.upper() \nprint(tickers.iloc[4101])\n\n# dropping 'NAN' North American Nickel ticker for now\ntickers = tickers.dropna()\nprint(tickers.isnull().values.any())\ntickers.head(5)","metadata":{"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.drop(columns = ['awarders', 'over_18', 'author_flair_text', 'removed_by','full_link','author'])","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data.columns)\nprint(data.shape)\n\n# analysis of date and scores\nmax_date, min_date = datetime.fromtimestamp(max(data.created_utc)), datetime.fromtimestamp(min(data.created_utc))\nprint('date ranges: ', min_date, max_date)\nprint('score range ', max(data.score), min(data.score))\nprint('num comments range ', max(data.num_comments), min(data.num_comments))\n\n# find score and comments distributions\nno_score_data = data[data.score==0]\nlow_score_data =data[data.score<= 100]\nmid_score_data = data[(data.score <= 1000) & (data.score > 100)]\nhigh_score_data = data[data.score > 1000]\nprint('scores: ', len(no_score_data), len(low_score_data), len(mid_score_data), len(high_score_data))\nprint('avg score: ', np.mean(data.score.values), 'std score: ', np.std(data.score.values))","metadata":{"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# keep posts with score > 100 or num comments > 100 or total awards > 10\n# determine best threshold for keeping posts\ndf = data[(data.score >= 100) | (data.num_comments >= 50) | (data.total_awards_received >= 10)] \nprint('comments avg and std: ', df.num_comments.mean(), df.num_comments.std())\nprint(df.shape)","metadata":{"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Vader Sentiment Scores","metadata":{}},{"cell_type":"code","source":"# Calculate polarity scores\nsia = SIA()\n# baseline compound is pos if >= 0.05, neg if <= -0.05 and neu else\n# change baseline to 0 for binary classification\ndef calculate_sentiment(text):\n    pol_score = sia.polarity_scores(text)\n    if pol_score['compound'] >= 0.05:\n        return 1\n    elif pol_score['compound'] <= -0.05:\n        return -1\n    else:\n        return 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add sentiment and compound col to df\ndf['sentiment'] = df.apply(lambda x: calculate_sentiment(x.title), axis=1)\ndf['compound'] = df.apply(lambda x: sia.polarity_scores(x.title)['compound'], axis=1)\ndf.head(3)","metadata":{"_kg_hide-output":true,"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# counting sentiments \nsentiments = [-1, 0, 1]\nsentiments_count = [sum(df.sentiment == -1), sum(df.sentiment == 0), sum(df.sentiment == 1)]\ndf.sentiment.value_counts()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TextBlob Sentiment Scores","metadata":{}},{"cell_type":"code","source":"# Calculating TextBlob Scores\n\nfrom textblob import TextBlob\n\ndef calculate_sentiment_tb(text,compound=True):\n    '''\n    text: string input\n    compound: if true, return compound score. Otherwise return -1, 0, or 1\n    \n    '''\n    blob = TextBlob(text)\n    \n    if compound: \n        return blob.polarity\n    \n    else: \n        if blob.polarity >= 0.05:\n            return 1\n        elif blob.polarity <= -0.05:\n            return -1\n        else:\n            return 0\n        \ndf['tb_sentiment'] = df.apply(lambda x: calculate_sentiment_tb(x.title), axis=1)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(3)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Finding Stock Tickers in Posts","metadata":{}},{"cell_type":"code","source":"# stocks to find,\nstocks = ['SPY', 'GME', 'AMC', 'TSLA', 'PLTR', 'APPL', 'AMD', 'BB', 'AMZN', 'NIO', 'NVDA', 'MU', 'RH', 'SNAP', 'NOK', 'SPCE']\ncompany_names = {'S&P': 'SPY', 'GAMESTOP': 'GME', 'AMC': 'AMC', 'TESLA': 'TSLA', 'PALANTIR': 'PLTR', 'APPLE': 'APPL', 'AMD': 'AMD', 'BLACKBERRY': 'BB',\n                 'AMAZON': 'AMZN', 'NIO': 'NIO', 'NVIDIA': 'NVDA', 'MICRON': 'MU', 'RESTORATION': 'RH', 'SNAPCHAT': 'SNAP', 'NOKIA': 'NOK', 'VIRGIN': \"SPCE\"}","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pattern matching with most frequent stocks \nnlp_freq = sp.blank('en')\nmatcher_freq = PhraseMatcher(nlp_freq.vocab, attr='TEXT')\nfreq_token_list = [nlp_freq(item) for item in stocks + list(company_names)]\nmatcher_freq.add('Freq Stocks', freq_token_list)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# find tickers and company names only from specified list\ndef find_tickers_and_names(title):\n    doc = nlp_freq(title.upper())\n    matches = matcher_freq(doc)\n    found_items = set([str(doc[match[1]: match[2]]).replace(' ', '') for match in matches])\n    tickers_list = set()\n    for item in found_items:\n        if item in company_names:\n            tickers_list.add(company_names[item])\n        else:\n            tickers_list.add(item)\n    for ticker in tickers_list:\n        ticker_freq[ticker] += 1\n    return '_'.join(tickers_list)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ticker_freq = Counter()\ndf['orgs'] = df.apply(lambda x: find_tickers_and_names(x.title), axis=1)\nprint(ticker_freq, sum(ticker_freq.values()))","metadata":{"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create new dataframe for only posts with specific organizations mentioned\ndf_stocks = df[df.orgs != '']\nprint(df_stocks.shape)\ndf_stocks.head()","metadata":{"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Finding Posts by Stock Ticker","metadata":{}},{"cell_type":"code","source":"def get_daily_sentiment(dataframe, ticker, sentiment_type='compound'):\n    '''\n    get the daily sentiment assosicated with a stock\n    \n    ticker: string input\n    sentiment_type: 'sentiment', 'compound',or 'tb_sentiment'\n    \n    '''\n    df_ticker = dataframe.copy()[dataframe.orgs.str.contains(ticker)]\n    df_ticker['Date'] = pd.to_datetime(df_ticker['created_utc'],unit='s').dt.date\n    df_ticker_scores = df_ticker.groupby(df_ticker['Date'])[sentiment_type].mean()\n    df_ticker_scores = df_ticker_scores.to_frame()\n    df_ticker_scores.reset_index(inplace=True)\n\n    \n    return df_ticker_scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tsla_scores = get_daily_sentiment(df,'TSLA',sentiment_type='tb_sentiment')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Getting Stock Market Data","metadata":{}},{"cell_type":"code","source":"pip install yfinance","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import yfinance as yf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_market_data(ticker, df_scores, min_date,drop_NaN=True): \n    '''\n    get the daily price of a stock with sentiment scores\n    min_date: earilest date to get price. 'YYYY-MM-DD'\n    \n    fills in 'NaN' when market data not available (weekends & holidays) \n    \n    '''\n    ticker = yf.Ticker(ticker)\n    min_date = pd.to_datetime(min_date).date()\n    max_date = max(df_scores.Date)\n    hist = ticker.history(start=min_date, end=max_date)\n    \n    df_scores_date = df_scores[df_scores.Date > min_date]\n    df_scores_date.set_index('Date', inplace=True, drop=True)\n    \n    df_wsb = pd.concat([hist, df_scores_date], axis=1)\n    \n    if drop_NaN: \n        df_wsb = df_wsb.dropna()\n    \n    return df_wsb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tsla_wsb = get_market_data('TSLA',df_tsla_scores,'2020-01-01',drop_NaN=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Test Split","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import TimeSeriesSplit\n\ndataset = df_tsla_wsb.values\n\n# Set the X without Open or Date\nX_MinMax = MinMaxScaler()\ny_MinMax = MinMaxScaler()\nX = X_MinMax.fit_transform(dataset[:, 1:])\ny = y_MinMax.fit_transform(dataset[:, 0].reshape(-1, 1)) # open price\n\n#X = (dataset[:, 1:6])\n#y = (dataset[:, 0].reshape(-1, 1))\n\ntscv = TimeSeriesSplit()\n\nfor train_index, test_index in tscv.split(X):\n    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    \nX_train_scale = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\nX_test_scale = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LSTM","metadata":{}},{"cell_type":"code","source":"# Create LSTM trained on sentiment analysis data\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\n\nmodel = Sequential()\n\n\n\nmodel.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(units = 50, return_sequences = True))\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(units = 50, return_sequences = True))\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(units = 50))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(units = 1))\n\nmodel.compile(optimizer = 'adam', loss = 'mean_squared_error')\n\n\nmodel.fit(X_train_scale, y_train, epochs = 225, batch_size = 32)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\npredicted_stock_tsla = model.predict(X_test_scale)\n\ntestScore = np.sqrt(mean_squared_error(y_test[:], predicted_stock_tsla[:,0]))\n\nprint('Root mean square error is {}'.format(testScore))\n\nplt.plot(y_MinMax.inverse_transform(predicted_stock_tsla),label='testing predicted')\nplt.plot(y_MinMax.inverse_transform(y_test),label='testing actual')\nplt.title('TSLA Open Value Prediction with TextBlob Sentiment')\nplt.legend()\nplt.show()\nplt.clf()\nplt.cla()\nplt.close()\n","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_stock_tsla_train = model.predict(X_train_scale)\n\ntrainScore = np.sqrt(mean_squared_error(y_train[:], predicted_stock_tsla_train[:,0]))\n\nprint('Root mean square error is {}'.format(trainScore))\n\nplt.plot(y_MinMax.inverse_transform(predicted_stock_tsla_train),label='training predicted')\nplt.plot(y_MinMax.inverse_transform(y_train),label='training actual')\nplt.title('TSLA Open Value Prediction with TextBlob Sentiment')\nplt.legend()\nplt.show()\nplt.clf()\nplt.cla()\nplt.close()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LSTM Without Sentiment Input","metadata":{}},{"cell_type":"code","source":"# Test the model without sentiment\n\n# Set the X without Open or Date\nX_MinMax = MinMaxScaler()\ny_MinMax = MinMaxScaler()\nX = X_MinMax.fit_transform(dataset[:, 1:-1])\ny = y_MinMax.fit_transform(dataset[:, 0].reshape(-1, 1))\n\ntscv = TimeSeriesSplit()\n\nfor train_index, test_index in tscv.split(X):\n    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    \nX_train_scale = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\nX_test_scale = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n\n\n# Create LSTM trained on sentiment analysis data\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\n\nmodel = Sequential()\n\n\n\nmodel.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(units = 50, return_sequences = True))\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(units = 50, return_sequences = True))\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(units = 50))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(units = 1))\n\nmodel.compile(optimizer = 'adam', loss = 'mean_squared_error')\n\n\nmodel.fit(X_train_scale, y_train, epochs = 225, batch_size = 32)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_stock_tsla = model.predict(X_test_scale)\n\ntestScore = np.sqrt(mean_squared_error(y_test[:], predicted_stock_tsla[:,0]))\n\nprint('Root mean square error is {}'.format(testScore))\n\nplt.plot(y_MinMax.inverse_transform(predicted_stock_tsla),label='predicted')\nplt.plot(y_MinMax.inverse_transform(y_test),label='actual')\nplt.legend()\nplt.title('Without Sentiment Data tsla test')\nplt.show()\nplt.clf()\nplt.cla()\nplt.close()\n","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_stock_tsla_train = model.predict(X_train_scale)\n\ntrainScore = np.sqrt(mean_squared_error(y_train[:], predicted_stock_tsla_train[:,0]))\n\nprint('Root mean square error is {}'.format(trainScore))\n\nplt.plot(y_MinMax.inverse_transform(predicted_stock_tsla_train),label='predicted')\nplt.plot(y_MinMax.inverse_transform(y_train),label='actual')\nplt.legend()\nplt.title('Without Sentiment Data tsla train')\nplt.show()\nplt.clf()\nplt.cla()\nplt.close()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}