{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\n\nIn this Kernel we introduce a general ML pipeline for text classification, with focus on what is specific for Chinese text.\n\n\n# Analysis preparation\n\n## Load packages\n\nMost of the packages are usual ones used for simple NLP and classification; in this case, we are also imported **jieba**, a package for Chinese language basic NLP.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport json\nimport matplotlib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\n%matplotlib inline \nfrom wordcloud import WordCloud, STOPWORDS\nfrom joblib import Parallel, delayed\nimport tqdm\nimport jieba\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, f1_score, recall_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load data\n\nThe data contains news articles in Chinese simplified."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_df = pd.read_csv(\"../input/chinese-official-daily-news-since-2016/chinese_news.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Glimpse the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Rows: {data_df.shape[0]}, Cols: {data_df.shape[1]}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data exploration\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Samples with content null: {data_df.loc[data_df['content'].isnull()].shape[0]}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Samples with headline null: {data_df.loc[data_df['headline'].isnull()].shape[0]}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's drop the rows will null content. We will not include in the analysis the samples with articles without content."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df = data_df.loc[~data_df['content'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"New data shape: {data_df.shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualization utility\n\n\nWe download the fonts to display the Chinese characters."},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget https://github.com/adobe-fonts/source-han-sans/raw/release/SubsetOTF/SourceHanSansCN.zip\n!unzip -j \"SourceHanSansCN.zip\" \"SourceHanSansCN/SourceHanSansCN-Regular.otf\" -d \".\"\n!rm SourceHanSansCN.zip\n!ls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we load the font from the downloaded resource."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.font_manager as fm\nfont_path = './SourceHanSansCN-Regular.otf'\nprop = fm.FontProperties(fname=font_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Target distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_count(feature, title, df, font_prop=prop, size=1):\n    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n    total = float(len(df))\n    g = sns.countplot(df[feature], order = df[feature].value_counts().index[:20], palette='Set3')\n    g.set_title(\"Number and percentage of {}\".format(title))\n    if(size > 2):\n        plt.xticks(rotation=90, size=8)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height/total),\n                ha=\"center\") \n    ax.set_xticklabels(ax.get_xticklabels(), fontproperties=font_prop);\n    plt.show()    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_count('tag', 'tag (all data)', font_prop=prop, df=data_df,size=1.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Date info extraction\n\n\nWe extract from the data the year, month and day of week."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df['datetime'] = data_df['date'].apply(lambda x: pd.to_datetime(x))\ndata_df['year'] = data_df['datetime'].dt.year\ndata_df['month'] = data_df['datetime'].dt.month\ndata_df['dayofweek'] = data_df['datetime'].dt.dayofweek","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cut phrases in ideograms groups\n\nChinese does not have flexionary forms and also does not use spaces between ideograms to mark separate words. In the same time, some concepts are using 2 ore more ideograms in a sequence. The reader will `cut` in mind during reading the sequences of ideograms in groups, corresponding to different concepts, based on context. \nWe will use **jieba** library to separate the ideograms in groups."},{"metadata":{"trusted":true},"cell_type":"code","source":"def jieba_cut(x, sep=' '):\n    return sep.join(jieba.cut(x, cut_all=False))\n\nprint('raw', data_df['headline'][0])\nprint('cut', jieba_cut(data_df['headline'][0], ', '))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We apply now the above defined function to the whole dataset. We are doing this for both the content and headline features."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndata_df['headline_cut'] = Parallel(n_jobs=4)(\n    delayed(jieba_cut)(x) for x in tqdm.tqdm_notebook(data_df['headline'].values)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndata_df['content_cut'] = Parallel(n_jobs=4)(\n    delayed(jieba_cut)(x) for x in tqdm.tqdm_notebook(data_df['content'].values)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prop = fm.FontProperties(fname=font_path, size=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After we cut the ideograms sequences in groups (each corresponding to one concept - or token) we represent the most frequenty used with wordclouds."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nstopwords = set(STOPWORDS)\n\ndef show_wordcloud(data, font_path=font_path, title = None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        font_path=font_path,\n        max_words=50,\n        max_font_size=40, \n        scale=5,\n        random_state=1\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(10,10))\n    plt.axis('off')\n    if title: \n        prop = fm.FontProperties(fname=font_path)\n        fig.suptitle(title, fontsize=40, fontproperties=prop)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_wordcloud(data_df['headline_cut'], font_path, title = 'Prevalent words in headline, all data')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.tag.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We show now the most frequent groups of ideograms grouped by target value (or tag)."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_tag_df = data_df.loc[data_df.tag=='详细全文']\nshow_wordcloud(data_tag_df['headline_cut'], font_path, title = 'Prevalent words in headline, tag=详细全文')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_tag_df = data_df.loc[data_df.tag=='国内']\nshow_wordcloud(data_tag_df['headline_cut'], font_path, title = 'Prevalent words in headline, tag=国内')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_tag_df = data_df.loc[data_df.tag=='国际']\nshow_wordcloud(data_tag_df['headline_cut'], font_path, title = 'Prevalent words in headline, tag=国际')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split data to train-test"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df, test_df = train_test_split(data_df, test_size = 0.2, random_state = 42) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"train: {train_df.shape}, test: {test_df.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_count('tag', 'tag (train)', font_prop=prop, df=train_df,size=1.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_count('tag', 'tag (test)', font_prop=prop, df=test_df,size=1.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_vect_feature(feature, df, max_features=5000):\n    start_time = time.time()\n    cv = CountVectorizer(max_features=max_features,\n                             ngram_range=(1, 1),\n                             stop_words='english')\n    X_feature = cv.fit_transform(df[feature])\n    print('Count Vectorizer `{}` completed in {} sec.'.format(feature, round(time.time() - start_time,2)))\n    return X_feature, cv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_headline, cv = count_vect_feature('headline_cut', train_df, 20000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_content, cv = count_vect_feature('content_cut', train_df, 30000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"target =  'tag'\nX = X_content\ny = train_df[target].values\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size = 0.2, random_state = 42) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X.shape, valid_X.shape, train_y.shape, valid_y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SVC Model\n\n\nWe use first a SVC model (with linear kernel).\n\n\n### Model fit"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nclf_svc = SVC(kernel='linear')\nclf_svc = clf_svc.fit(train_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_confusion_matrix(valid_y, predicted, size=1, font_prop=prop, trim_labels=False):\n    mat = confusion_matrix(valid_y, predicted)\n    plt.figure(figsize=(4*size, 4*size))\n    f, ax = plt.subplots(1,1, figsize=(4*size,4*size))\n    sns.set()\n    target_labels = np.unique(valid_y)\n    if(trim_labels):\n        target_labels = [x[0:70] for x in target_labels]\n    sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n                xticklabels=target_labels,\n                yticklabels=target_labels\n               )\n    ax.set_xticklabels(ax.get_xticklabels(), fontproperties=font_prop);\n    ax.set_yticklabels(ax.get_yticklabels(), fontproperties=font_prop);\n    plt.xlabel('true label')\n    plt.ylabel('predicted label')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_valid = clf_svc.predict(valid_X)\nprediction_acc = np.mean(predicted_valid == valid_y)\nprediction_f1_score = f1_score(valid_y, predicted_valid, average='weighted')\nprediction_recall = recall_score(valid_y, predicted_valid, average='weighted')\nprint(\"Valid:\\n========================================================\")\nprint(f\"Feature: {target} \\t| Prediction accuracy: {prediction_acc}\")\nprint(f\"Feature: {target} \\t| Prediction F1-score: {prediction_f1_score}\")\nprint(f\"Feature: {target} \\t| Prediction recall: {prediction_recall}\")\nshow_confusion_matrix(valid_y, predicted_valid, font_prop=prop,size=1.5)\nprint(classification_report(valid_y, predicted_valid))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MultinomialNB model"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nclf_nb = MultinomialNB(fit_prior='true')\nclf_nb = clf_nb.fit(train_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_valid = clf_nb.predict(valid_X)\nprediction_acc = np.mean(predicted_valid == valid_y)\nprediction_f1_score = f1_score(valid_y, predicted_valid, average='weighted')\nprediction_recall = recall_score(valid_y, predicted_valid, average='weighted')\nprint(\"Valid:\\n========================================================\")\nprint(f\"Feature: {target} \\t| Prediction accuracy: {prediction_acc}\")\nprint(f\"Feature: {target} \\t| Prediction F1-score: {prediction_f1_score}\")\nprint(f\"Feature: {target} \\t| Prediction recall: {prediction_recall}\")\nshow_confusion_matrix(valid_y, predicted_valid, font_prop=prop,size=1.5)\nprint(classification_report(valid_y, predicted_valid))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\n\nWe used two different models: MultinomialNB (based on Naive Bayes) and SCV (based on SVM).\n\nSVC model performed better, with weighted and macro average scores for performance, recall and f1-score (and corresponding scores per class) better than for MultinomialNB scores."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}