{"cells":[{"metadata":{"id":"GEvjmIMTNoGJ"},"cell_type":"markdown","source":"# Welcome to the Emojify Challenge!"},{"metadata":{"id":"o0lfhjio9teD","outputId":"62f3ac29-2cd4-4cd5-f4f8-3b92be8aa64b","trusted":true},"cell_type":"code","source":"pip install emoji","execution_count":null,"outputs":[]},{"metadata":{"id":"6v18PJ7QNoGQ","trusted":true},"cell_type":"code","source":"##################################################\n# Imports\n##################################################\n\nimport emoji\nimport numpy as np\nimport cv2\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport re\nimport spacy\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom keras.utils.vis_utils import plot_model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import *\nfrom sklearn.model_selection import StratifiedKFold\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras import backend\nfrom keras.initializers import Constant\n\n\n##################################################\n# Params\n##################################################\n\nDATA_BASE_FOLDER = '/kaggle/input/emojify-challenge'\n\n\n##################################################\n# Utils\n##################################################\n\ndef label_to_emoji(label):\n    \"\"\"\n    Converts a label (int or string) into the corresponding emoji code (string) ready to be printed\n    \"\"\"\n    return emoji.emojize(emoji_dictionary[str(label)], use_aliases=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"WLAp4iBI7v86","trusted":true},"cell_type":"code","source":"#### Function for plotting confusion matrix ######\n\ndef plot_cf_matrix(y_true, y_pred):\n  cf_matrix= confusion_matrix(y_true, y_pred);\n  heatmap = sns.heatmap(cf_matrix, annot=True, cmap='YlOrRd')\n  plt.xlabel(\"Predicted value\") \n  plt.ylabel(\"True value\") \n  return plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"7x045yKtYNXS","trusted":true},"cell_type":"code","source":"###################################################\n# Plots accuracy and loss in Train and Validation #\n###################################################\n\ndef plot_accuracy_loss(history_l, K, parameters = None):\n    plt.figure(figsize=(20, 7))\n    colors = [\"red\", \"blue\", \"orange\", \"green\", \"cyan\",\"brown\",\"grey\"]\n    #Plot accuracy\n    plt.subplot(1, 2, 1)\n    plt.title('Train and Validation Accuracy by Epochs')\n    plt.xlabel('Epochs', fontweight='bold', fontsize=15)\n    plt.ylabel('Accuracy', fontweight='bold', fontsize=15)\n    if parameters == None:\n      for i in range(0, K):\n          plt.plot(history_l[i].history['accuracy'], label = 'Train Accuracy Fold ' + str(i+1), color = colors[i])\n          plt.plot(history_l[i].history['val_accuracy'], label = 'Validation Accuracy Fold ' + str(i+1), color = colors[i], linestyle = \":\")\n          plt.legend(fontsize='small')\n    else:\n      for i in range(0, K):\n          plt.plot(history_l[i].history['accuracy'], label = 'Train Accuracy ' + str(parameters[i]), color = colors[i])\n          plt.plot(history_l[i].history['val_accuracy'], label = 'Validation Accuracy ' + str(parameters[i]), color = colors[i], linestyle = \":\")\n          plt.legend(fontsize='medium')\n    # Plot loss\n    plt.subplot(1, 2, 2)\n    plt.title('Train and Validation Loss by Epochs')\n    plt.xlabel('Epochs', fontweight='bold', fontsize=15)\n    plt.ylabel('Loss', fontweight='bold', fontsize=15)\n    if parameters == None:\n      for i in range(0, K):\n          plt.plot(history_l[i].history['loss'], label = 'Train Accuracy Loss Fold' + str(i+1), color = colors[i])\n          plt.plot(history_l[i].history['val_loss'], label = 'Validation Accuracy Loss Fold' + str(i+1), color = colors[i], linestyle = \":\")\n          plt.legend(fontsize='small')\n    else:\n      for i in range(0, K):\n          plt.plot(history_l[i].history['loss'], label = 'Train Accuracy Loss ' + str(parameters[i]), color = colors[i])\n          plt.plot(history_l[i].history['val_loss'], label = 'Validation Accuracy Loss ' + str(parameters[i]), color = colors[i], linestyle = \":\")\n          plt.legend(fontsize='medium')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"ljEIDhrJNoGR"},"cell_type":"markdown","source":"# Dataset"},{"metadata":{"id":"cuXYLDP0NoGR","outputId":"84dfac2b-be34-4967-9629-89769129f2d9","trusted":true},"cell_type":"code","source":"\n##################################################\n# Load dataset\n##################################################\n\ndf_train = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'train.csv'))\ny_train = df_train['class']\ndf_validation = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'validation.csv'))\ny_validation = df_validation['class']\nprint(df_validation.columns)\ndf_test = pd.read_csv(os.path.join('/kaggle/input/emojify', 'test.csv'))\ny_test = df_test['class']\n\nemoji_dictionary = {\n    '0': '\\u2764\\uFE0F',\n    '1': ':baseball:',\n    '2': ':smile:',\n    '3': ':disappointed:',\n    '4': ':fork_and_knife:'\n}\n\n# See some data examples\nprint('EXAMPLES:\\n####################')\nfor idx in range(10):\n  print(f'{df_train[\"phrase\"][idx]} -> {label_to_emoji(y_train[idx])}')\n\nprint(df_train.head()) \n","execution_count":null,"outputs":[]},{"metadata":{"id":"jfw3XkYeNoGS"},"cell_type":"markdown","source":"# Word embeddings\n\nWords can be represented as n-dimentional vectors where the distance between points has a correspondence respect to similarity between word semantics (similar words are closer, while dissimilar ones are distant). This representation is known as word embeddings and here is extrapolated and pre-computed from the [GloVe](https://nlp.stanford.edu/projects/glove/) model. \n\nHere is depicted an example of bi-dimensional word embeddings:\n![word embedding](https://shanelynnwebsite-mid9n9g1q9y8tt.netdna-ssl.com/wp-content/uploads/2018/01/word-vector-space-similar-words.jpg)\n\nIn our case a single word is represented by a vector of length 25.\n\n# Phrase representation\n\nAll the phrases are padded to the phrase of maximum length, in this case `max_len = 10`, and each phrase is represented by the concatenation of his word embeddings (each phrase thus is a 10 * 25 = 250 dimentional vector)."},{"metadata":{"id":"DYpoyHqJNoGS","outputId":"7d0ccfd7-0a4e-486e-e3ce-48806276e95b","trusted":true},"cell_type":"code","source":"# Load phrase representation gloVe\nx_train = np.load(\n    os.path.join(DATA_BASE_FOLDER, \n                 'train.npy')).reshape(len(df_train), -1)\nx_validation = np.load(\n    os.path.join(DATA_BASE_FOLDER, \n                 'validation.npy')).reshape(len(df_validation), -1)\n\n                \nprint(f'Word embedding size: {x_train.shape[-1]}')\n","execution_count":null,"outputs":[]},{"metadata":{"id":"xCDEI6huUxSm"},"cell_type":"markdown","source":"\n\n# Visualizzazione Dati\n\n\n"},{"metadata":{"id":"7wzLMd9xUpvw","outputId":"49f5059f-ffad-4826-800a-e48864ec246d","trusted":true},"cell_type":"code","source":"# pooled dati per GloVe\nnew_x_pooled = np.vstack((x_train,x_validation))\nnew_y_pooled = np.hstack((y_train, y_validation))\n\n# pooled string data\ndf_new_train = np.vstack((df_train,df_validation))         \ndf_string_train = [\"\".join(x[0]) for x in df_new_train]\ndf_string_test = [\"\".join(x[0]) for x in pd.DataFrame.to_numpy(df_test)]\n","execution_count":null,"outputs":[]},{"metadata":{"id":"iD6m72w8YFk_","outputId":"ec4edba7-1bcc-4831-ea6b-87421d6ace26","trusted":true},"cell_type":"code","source":"# words occurrences\nword_frequency = {}\n\nnew_str = ' '.join(df_string_train);\nfor word in new_str.lower().split():\n    if word not in word_frequency:\n      word_frequency[word] = 0    \n    word_frequency[word] += 1\n\nnewd = dict(sorted(word_frequency.items(), key=lambda item: item[1], reverse=True))\nprint(len(newd))\n\n\n# plot \nplt.plot(list(newd.keys()),list(newd.values()))\nplt.xticks(list(newd.keys())[::10], rotation = \"vertical\");\nplt.title(\"Word frequency\");","execution_count":null,"outputs":[]},{"metadata":{"id":"dudtL9lh_fBQ"},"cell_type":"markdown","source":"Class count"},{"metadata":{"id":"ARiDmh8G_9hj","outputId":"629c922b-2500-42d9-a041-5293164491ca","trusted":true},"cell_type":"code","source":"sns.countplot(new_y_pooled).set_title('Frequency of target variables in pooled dataset')\nplt.xlabel('Class');","execution_count":null,"outputs":[]},{"metadata":{"id":"3GHxMmas_FdH"},"cell_type":"markdown","source":"\n# Pre-processing data\n\n\n\n\n\n\n\n"},{"metadata":{"id":"LZky6ohVBnBo"},"cell_type":"markdown","source":"processing data for *Bag-of-Words*"},{"metadata":{"id":"NIMPWH5DcyJR"},"cell_type":"markdown","source":"Convert a collection of text documents to a matrix of token counts\n\n"},{"metadata":{"id":"6v9Jz_iylIk0","trusted":true},"cell_type":"code","source":"vect = CountVectorizer().fit(df_string_train)\nstandCV_comp = vect.transform(df_string_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"-gxKxupAaPOl"},"cell_type":"markdown","source":"Using Stopwords to reduce number of features\n"},{"metadata":{"id":"AyjrkJboYn55","outputId":"59164917-9f70-4044-f0b2-b93638deede9","trusted":true},"cell_type":"code","source":"# Using Stopwords\nprint(\"Number of stop words: {}\".format(len(ENGLISH_STOP_WORDS)))\nprint(\"Every 10th stopwords: \\n{}\".format(list(ENGLISH_STOP_WORDS)[::10]))","execution_count":null,"outputs":[]},{"metadata":{"id":"YLBCBKQqZRoX","outputId":"ac9fa495-dc15-4587-a7e9-2d676b810e28","trusted":true},"cell_type":"code","source":"#the comparison between using stop_words and not\nvect_sw = CountVectorizer(min_df=1, stop_words='english').fit(df_string_train)\nstopw_x_train = vect_sw.transform(df_string_train)\nprint('X_train shape using Stop words: {}'.format(stopw_x_train.shape[1]))\nprint('Standard CountVectorizer shape: {}'.format(standCV_comp.shape[1]))\nprint('Thank to Stopwords we discard {} features (words).'.format(standCV_comp.shape[1]-stopw_x_train.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"id":"3Fy0PC_BFM4d"},"cell_type":"markdown","source":"Transform data with Lemmatization\n\n"},{"metadata":{"id":"1ehJwpgvBjod","trusted":true},"cell_type":"code","source":"#Convert each document into a list of tokens using the tokenizer parameter\n#regexp used in CounterVector\nregexp = re.compile('(?u)\\\\b\\\\w\\\\w+\\\\b')\n\nen_nlp = spacy.load('en')\nold_tokenizer = en_nlp.tokenizer\n#replace the tokenizer with regexp\nen_nlp.tokenizer = lambda string: old_tokenizer.tokens_from_list(regexp.findall(string))\n#create a custom tokenizer using the spacy document pipeline\ndef our_tokenizer(document):\n  doc_spacy = en_nlp(document)\n  return [token.lemma_ for token in doc_spacy]\n\n#define a count vectorizer with the custom tokenizer to do Lemmatization\nlemma_vect = CountVectorizer(tokenizer = our_tokenizer, min_df = 1, stop_words = 'english')\nlemma_vect_tfidf = TfidfVectorizer(tokenizer = our_tokenizer, min_df = 1, stop_words= 'english')","execution_count":null,"outputs":[]},{"metadata":{"id":"kCTEg3yBBjr6","outputId":"9864e9a4-7a8f-4937-9bbf-f854c1bcb3db","trusted":true},"cell_type":"code","source":"x_train_lemma = lemma_vect.fit_transform(df_string_train);\nx_train_lemma_tfidf = lemma_vect_tfidf.fit_transform(df_string_train);\n#standard CountVectorizer for comparison\n\nprint('Standard CountVectorizer shape: {}'.format(standCV_comp.shape))\nprint('X_train_lemma shape: {}'.format(x_train_lemma.shape))\nprint('Thank to Lemmatization we discard {} features'.format(standCV_comp.shape[1]-x_train_lemma.shape[1]))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"V72M8_zCc2oX"},"cell_type":"markdown","source":"**TF-IDF** \n\nInstead of dropping features that are unimportant, another approach is to rescale features by how informative we expect them to be. To do this we can use term frequency-inverse document frequency (Tf-IDF). The intuition of this method is to give high weight to any term that appears often in a particular document, in this case phrase, but not in many documents in the corpus.\n"},{"metadata":{"id":"DAF5NQ_rLmXP"},"cell_type":"markdown","source":"For GloVe we don't use standard preprocessing steps like stemming, lowercasing or stopword removal. The problem is that we are working with pre-trained embeddings and so, using standard preprocessing, we could loose valuable information, which would help our neural networks to figure things out."},{"metadata":{"id":"QYmRIlt_NoGS"},"cell_type":"markdown","source":"# Model\n\nHere you have to implement a model (or more models, for finding the most accurate) for classification.\n\nYou can use the sklearn (or optionally other more advanced frameworks such as pytorch or tensorflow) package that contains a pool of models already implemented that perform classification. (SVMs, NNs, LR, kNN, ...)"},{"metadata":{"id":"1IGGQLTnPt-F"},"cell_type":"markdown","source":"# Logistic Regression\n"},{"metadata":{"id":"arYY7q2eQW7Y"},"cell_type":"markdown","source":"**Standard Bag-of-words**"},{"metadata":{"id":"_ugn3XCHQ9Gg","outputId":"2286d74c-71c6-4726-94e6-2e03c059a42b","trusted":true},"cell_type":"code","source":"#\npipe_bof_lr = make_pipeline(CountVectorizer(min_df = 1), LogisticRegression())\n#\nparam_grid = {'logisticregression__C': [0.01, 0.1, 1, 5, 10, 50,100],\n              'countvectorizer__ngram_range': [(1,1),(1,2)],\n              'countvectorizer__stop_words':['english', None]}\ngrid_bof_lr = GridSearchCV(pipe_bof_lr, param_grid , cv = 5)\ngrid_bof_lr.fit(df_string_train, new_y_pooled)\n\nprint(\"Best cross-validation score: {:.4f}\".format(grid_bof_lr.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_bof_lr.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"id":"bCyWrzLjZhob","outputId":"fc3b19f2-55eb-49ef-9b18-e81e04cd7ed3","trusted":true},"cell_type":"code","source":"param_grid = {'C': [0.01, 0.1, 1, 5, 10, 50,100]}\ngrid_bof_lr = GridSearchCV(LogisticRegression(), param_grid= param_grid, cv = 5)\ngrid_bof_lr.fit(x_train_lemma, new_y_pooled) # lemma with stopwords\n\nprint(\"Best cross-validation score: {:.4f}\".format(grid_bof_lr.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_bof_lr.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"id":"lCqE_g8kQWs-"},"cell_type":"markdown","source":"**Tf-idf**"},{"metadata":{"id":"Pq4SYhWzQ8W1","outputId":"ba8d6591-b724-41fe-dd21-542463f85613","trusted":true},"cell_type":"code","source":"pipe_tfidf_lr = make_pipeline(TfidfVectorizer(min_df = 1), LogisticRegression())\n#\nparam_grid = {'logisticregression__C': [0.1, 1, 5, 10, 25, 50,100],\n              'tfidfvectorizer__ngram_range': [(1,1),(1,2)],\n              'tfidfvectorizer__stop_words':['english', None]}\ngrid_tfidf_lr = GridSearchCV(pipe_tfidf_lr, param_grid , cv = 5)\ngrid_tfidf_lr.fit(df_string_train, new_y_pooled)\n\nprint(\"Best cross-validation score: {:.4f}\".format(grid_tfidf_lr.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_tfidf_lr.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"id":"L_o2yw_1lXzi","outputId":"839ee881-12ec-47d3-ed31-f408c42f9ef1","trusted":true},"cell_type":"code","source":"param_grid = {'C': [0.01, 0.1, 1, 5, 10, 50,100]}\ngrid_tfidf_lr = GridSearchCV(LogisticRegression(), param_grid= param_grid, cv = 5)\ngrid_tfidf_lr.fit(x_train_lemma_tfidf, new_y_pooled) # lemma with stopwords\n\nprint(\"Best cross-validation score: {:.4f}\".format(grid_tfidf_lr.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_tfidf_lr.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"id":"4PAJ5AOcQWf7"},"cell_type":"markdown","source":"**GloVe**"},{"metadata":{"id":"0l1RfgolQ9lo","outputId":"667bf702-9127-4ed3-f4a4-617e66176078","trusted":true},"cell_type":"code","source":"#\nparam_grid = {'logisticregression__C': [0.01, 0.05, 0.1, 0.5, 1]}\n#pipe_glove_lr = make_pipeline(MinMaxScaler(feature_range= (np.min(new_x_pooled),np.max(new_x_pooled))),LogisticRegression(max_iter=1000))\npipe_glove_lr = make_pipeline(LogisticRegression(max_iter=1000))\n#pipe_glove_lr = make_pipeline(StandardScaler(),LogisticRegression(max_iter=1000))\n\ngrid_glove_lr = GridSearchCV(pipe_glove_lr, param_grid , cv = 5)\ngrid_glove_lr.fit(new_x_pooled, new_y_pooled)\nprint(\"Best cross-validation score: {:.4f}\".format(grid_glove_lr.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_glove_lr.best_params_))\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"iHJ3QZhWtIlY"},"cell_type":"markdown","source":"**Best Logistic Regression model**"},{"metadata":{"id":"ShpKZIJ6vYh2","outputId":"bd17dcde-fb47-4fcf-f63a-1f8c4b3aedd5","trusted":true},"cell_type":"code","source":"vect_lr = TfidfVectorizer(min_df = 1, stop_words = 'english', ngram_range = (1, 1)).fit(df_string_train)\nx_train_vect_lr = vect_lr.transform(df_string_train)\nx_test_vect_lr =  vect_lr.transform(df_string_test)\nbest_lr = LogisticRegression(C = 10).fit(x_train_vect_lr, new_y_pooled)\ny_pred_lr = best_lr.predict(x_test_vect_lr)\n#accuracy(y_pred_lr,y_test)\nprint(\"Best accuracy score for Logistic Regression: {:.4f}\".format(best_lr.score(x_test_vect_lr,y_test)))\nprint(classification_report(y_test, y_pred_lr ))\nplot_cf_matrix(y_test, y_pred_lr)\n\n# Using lemmatization --> ending up with the same accuracy score\n# x_train_lemma_lr = lemma_vect_tfidf.fit_transform(df_string_train)\n# x_test_lemma_lr = lemma_vect_tfidf.transform(df_string_test)\n# best_lr = LogisticRegression(C = 10).fit(x_train_lemma_lr, new_y_pooled)\n# y_pred_lr = best_lr.predict(x_test_lemma_lr)\n# print(\"Best accuracy score for Logistic Regression: {:.4f}\".format(best_lr.score(x_test_lemma_lr,y_test)))\n# print(classification_report(y_test, y_pred_lr ))\n# plot_cf_matrix(y_test, y_pred_lr)","execution_count":null,"outputs":[]},{"metadata":{"id":"7NerJBRHPw7V"},"cell_type":"markdown","source":"# Random Forest \\ Decision tree"},{"metadata":{"id":"2LxrWsXqZ7RG"},"cell_type":"markdown","source":"**Standard Bag-of-words**"},{"metadata":{"id":"QeyasSXJZ7oG","outputId":"06bd6fb2-7d4b-4232-a461-87bf5c7ab9e5","trusted":true},"cell_type":"code","source":"\npipe_bof_rf = make_pipeline(CountVectorizer(min_df = 1), RandomForestClassifier())\n\nparam_grid = {'randomforestclassifier__n_estimators': [500],\n              'randomforestclassifier__max_features': [15,20,30],\n              'countvectorizer__ngram_range': [(1,1),(1,2)],\n              'countvectorizer__stop_words':['english', None]}\ngrid_bof_rf = GridSearchCV(pipe_bof_rf, param_grid , cv = 5)\ngrid_bof_rf.fit(df_string_train, new_y_pooled)\n\nprint(\"Best cross-validation score: {:.4f}\".format(grid_bof_rf.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_bof_rf.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"id":"hppwqc0JZ73m","outputId":"0b47b09e-e940-46ea-844d-359ab75d0be5","trusted":true},"cell_type":"code","source":"param_grid = {'n_estimators': [500],\n              'max_features': [20,25 ,30]}\ngrid_bof_rf = GridSearchCV(RandomForestClassifier(), param_grid= param_grid, cv = 5)\ngrid_bof_rf.fit(x_train_lemma, new_y_pooled) # lemma with stopwords\n\nprint(\"Best cross-validation score: {:.4f}\".format(grid_bof_rf.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_bof_rf.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"id":"ODSN8_vsZ8Sb"},"cell_type":"markdown","source":"**Tf-idf**"},{"metadata":{"id":"nqleAZglZ8xj","outputId":"8211d8ac-1f7d-49d7-f90b-420a97deced5","trusted":true},"cell_type":"code","source":"pipe_tfidf_rf = make_pipeline(TfidfVectorizer(min_df = 1), RandomForestClassifier())\n#\nparam_grid = {'randomforestclassifier__n_estimators': [500],\n              'randomforestclassifier__max_features': [15,20,30],\n              'tfidfvectorizer__ngram_range': [(1,1),(1,2)],\n              'tfidfvectorizer__stop_words':['english', None]}\ngrid_tfidf_rf = GridSearchCV(pipe_tfidf_rf, param_grid , cv = 5)\ngrid_tfidf_rf.fit(df_string_train, new_y_pooled)\nprint(\"Best cross-validation score: {:.7f}\".format(grid_tfidf_rf.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_tfidf_rf.best_params_))    ","execution_count":null,"outputs":[]},{"metadata":{"id":"o-jUQ257Z9EK","outputId":"1925f6ac-502f-4e90-dcc8-512d8831088c","trusted":true},"cell_type":"code","source":"param_grid = {'n_estimators': [500],\n              'max_features': [20,25 ,30]}\ngrid_tfidf_rf = GridSearchCV(RandomForestClassifier(), param_grid= param_grid, cv = 5)\ngrid_tfidf_rf.fit(x_train_lemma_tfidf, new_y_pooled) # lemma with stopwords\n\nprint(\"Best cross-validation score: {:.4f}\".format(grid_tfidf_rf.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_tfidf_rf.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"id":"VigavlJiZ9i9"},"cell_type":"markdown","source":"**GloVe**"},{"metadata":{"id":"4KgxtAwIZ90q","outputId":"853d518d-4e1a-470c-830f-70d988c75fdf","trusted":true},"cell_type":"code","source":"#  Decision Tree model don't need rescaling of the data\nparam_grid = {'randomforestclassifier__n_estimators': [500],\n              'randomforestclassifier__max_features': [20,25 ,30]}\npipe_glove_rf = make_pipeline(RandomForestClassifier())\ngrid_glove_rf = GridSearchCV(pipe_glove_rf, param_grid , cv = 5)\ngrid_glove_rf.fit(new_x_pooled, new_y_pooled)\nprint(\"Best cross-validation score: {:.4f}\".format(grid_glove_rf.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_glove_rf.best_params_))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"zA8XtjBD-kAD"},"cell_type":"markdown","source":"**Best Random Forest model**"},{"metadata":{"id":"yBpfZPHl-jvD","outputId":"931cd72d-95e1-4d35-9ee0-73d80b988064","trusted":true},"cell_type":"code","source":"vect_rf = CountVectorizer(min_df = 1, stop_words = 'english', ngram_range = (1, 1)).fit(df_string_train)\nx_train_vect_rf = vect_rf.transform(df_string_train)\nx_test_vect_rf =  vect_rf.transform(df_string_test)\nbest_rf = RandomForestClassifier(n_estimators=500,max_features=15).fit(x_train_vect_rf, new_y_pooled)\ny_pred_rf = best_rf.predict(x_test_vect_rf)\nprint(\"Best accuracy score for Random Forest: {:.4f}\".format(best_rf.score(x_test_vect_rf,y_test)))\nprint(classification_report(y_test, y_pred_rf ))\nplot_cf_matrix(y_test, y_pred_rf)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"_0no5WoWe4Tg"},"cell_type":"markdown","source":"# K-nearest neighbors"},{"metadata":{"id":"1b49JDo5fBTH"},"cell_type":"markdown","source":"**Standard Bag-of-words**"},{"metadata":{"id":"WWpb9oanfAQm","outputId":"005f524b-8dd2-4432-d89a-669f46911bfd","trusted":true},"cell_type":"code","source":"pipe_bof_knn = make_pipeline(CountVectorizer(min_df = 1), KNeighborsClassifier())\n\nparam_grid = {'kneighborsclassifier__n_neighbors':[2,3,4,5,7,9],\n              'kneighborsclassifier__metric': ['minkowski', 'euclidean'],\n              'countvectorizer__ngram_range': [(1,1),(1,2)],\n              'countvectorizer__stop_words':['english', None]}\ngrid_bof_knn = GridSearchCV(pipe_bof_knn, param_grid , cv = 5)\ngrid_bof_knn.fit(df_string_train, new_y_pooled)\n\nprint(\"Best cross-validation score: {:.4f}\".format(grid_bof_knn.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_bof_knn.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"id":"GmktEMD5fbuC","outputId":"1491ccef-929b-4a56-d414-1266a0545a9b","trusted":true},"cell_type":"code","source":"param_grid = {'n_neighbors':[2,3,4,5,7,9],\n             'metric': ['minkowski', 'euclidean']}\ngrid_bof_knn = GridSearchCV(KNeighborsClassifier(), param_grid= param_grid, cv = 5)\ngrid_bof_knn.fit(x_train_lemma, new_y_pooled) # lemma with stopwords\n\nprint(\"Best cross-validation score: {:.4f}\".format(grid_bof_knn.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_bof_knn.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"id":"sqD5eZvvfIeA"},"cell_type":"markdown","source":"**Tf-idf**"},{"metadata":{"id":"KGLozEw7fIqA","outputId":"62f6acd5-9c56-4682-877e-402c76018765","trusted":true},"cell_type":"code","source":"pipe_tfidf_knn = make_pipeline(TfidfVectorizer(min_df = 1), KNeighborsClassifier())\n#\nparam_grid = {'kneighborsclassifier__n_neighbors':[2,3,4,5,7,9],\n              'kneighborsclassifier__metric': ['minkowski', 'euclidean'],\n              'tfidfvectorizer__ngram_range': [(1,1),(1,2)],\n              'tfidfvectorizer__stop_words':['english', None]}\ngrid_tfidf_knn = GridSearchCV(pipe_tfidf_knn, param_grid , cv = 5)\ngrid_tfidf_knn.fit(df_string_train, new_y_pooled)\nprint(\"Best cross-validation score: {:.7f}\".format(grid_tfidf_knn.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_tfidf_knn.best_params_))   ","execution_count":null,"outputs":[]},{"metadata":{"id":"9WXdTNMuiNCT","outputId":"ff9d4d86-243f-43b4-d875-c6b35df0e35d","trusted":true},"cell_type":"code","source":"param_grid = {'n_neighbors':[2,3,4,5,7,9],\n             'metric': ['minkowski', 'euclidean']}\ngrid_tfidf_knn = GridSearchCV(KNeighborsClassifier(), param_grid= param_grid, cv = 5)\ngrid_tfidf_knn.fit(x_train_lemma_tfidf, new_y_pooled) # lemma with stopwords\n\nprint(\"Best cross-validation score: {:.4f}\".format(grid_tfidf_knn.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_tfidf_knn.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"id":"pLchrElGfKPg"},"cell_type":"markdown","source":"**GloVe**"},{"metadata":{"id":"3U1cqq62fLM7","outputId":"a6288dd8-7df1-4ec8-90e1-0ad693daaaad","trusted":true},"cell_type":"code","source":"param_grid = {'kneighborsclassifier__n_neighbors':[2,3,4,5,7,9],\n             'kneighborsclassifier__metric': ['minkowski', 'euclidean']}\npipe_glove_knn= make_pipeline(MinMaxScaler(),KNeighborsClassifier())\n#pipe_glove_knn = make_pipeline(KNeighborsClassifier())\n#pipe_glove_knn = make_pipeline(StandardScaler(),KNeighborsClassifier())\n\ngrid_glove_knn = GridSearchCV(pipe_glove_knn, param_grid , cv = 5)\ngrid_glove_knn.fit(new_x_pooled, new_y_pooled)\nprint(\"Best cross-validation score: {:.4f}\".format(grid_glove_knn.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_glove_knn.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"id":"m3G5oYrbfLcW"},"cell_type":"markdown","source":"**Best K-NN model**"},{"metadata":{"id":"JYEJe9fWfL0d","outputId":"4db036bb-dd4b-4ff0-d48f-4a1738b6e671","trusted":true},"cell_type":"code","source":"vect_knn = TfidfVectorizer(min_df = 1, stop_words = 'english', ngram_range = (1, 1)).fit(df_string_train)\n#vect_knn = lemma_vect_tfidf.fit(df_string_train)\nx_train_vect_knn = vect_knn.transform(df_string_train)\nx_test_vect_knn =  vect_knn.transform(df_string_test)\nbest_knn = KNeighborsClassifier(metric = 'minkowski', n_neighbors =  3).fit(x_train_vect_knn, new_y_pooled)\n#best_knn = KNeighborsClassifier(metric = 'minkowski', n_neighbors =  4).fit(x_train_vect_knn, new_y_pooled)\n\ny_pred_knn = best_knn.predict(x_test_vect_knn)\nprint(\"Best accuracy score for K-NN: {:.4f}\".format(best_knn.score(x_test_vect_knn,y_test)))\nprint(classification_report(y_test, y_pred_knn))\nplot_cf_matrix(y_test, y_pred_knn)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"N5bM1UGFPtra"},"cell_type":"markdown","source":"# Support Vector Machine"},{"metadata":{"id":"YaRSd5x1mqbV"},"cell_type":"markdown","source":"**Standard Bag-of-words**"},{"metadata":{"id":"fZmpvKzUmoSK","outputId":"7b0abb8a-c23b-4181-ad51-2791e0a3a8ae","trusted":true},"cell_type":"code","source":"pipe_bof_svm = make_pipeline(CountVectorizer(min_df = 1), SVC())\n\nparam_grid = {'svc__C':[0.1,1,10,100, 1000],\n             'svc__gamma':[0.001,0.01,0.1,1,10,100],\n             'svc__kernel': ['linear', 'rbf'],\n              'countvectorizer__ngram_range': [(1,1),(1,2)],\n              'countvectorizer__stop_words':['english', None]}\ngrid_bof_svm = GridSearchCV(pipe_bof_svm, param_grid , cv = 5)\ngrid_bof_svm.fit(df_string_train, new_y_pooled)\n\nprint(\"Best cross-validation score: {:.4f}\".format(grid_bof_svm.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_bof_svm.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"id":"yiBEGvyxuJ1_","outputId":"e2706420-3133-417b-9019-bc040ce2ec53","trusted":true},"cell_type":"code","source":"param_grid = {'C':[0.01,0.1,1,10,100],\n             'gamma':[0.001,0.01,0.1,1,10,100],\n              'kernel': ['linear', 'rbf']}\ngrid_bof_svm = GridSearchCV(SVC(), param_grid= param_grid, cv = 5)\ngrid_bof_svm.fit(x_train_lemma, new_y_pooled) # lemma with stopwords\n\nprint(\"Best cross-validation score: {:.4f}\".format(grid_bof_svm.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_bof_svm.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"id":"rTFfyBdFmrF0"},"cell_type":"markdown","source":"**Tf-idf**"},{"metadata":{"id":"a4z4Q7VQmre2","outputId":"752144ec-d882-4f24-955a-cc5993546e0b","trusted":true},"cell_type":"code","source":"pipe_tfidf_svm = make_pipeline(TfidfVectorizer(min_df = 1), SVC())\n#\nparam_grid = {'svc__C':[1,10,25,50],\n             'svc__gamma':[0.01,0.05,0.1,0.5,1],\n              'svc__kernel': ['linear', 'rbf'],\n              'tfidfvectorizer__ngram_range': [(1,1),(1,2)],\n              'tfidfvectorizer__stop_words':['english', None]}\ngrid_tfidf_svm = GridSearchCV(pipe_tfidf_svm, param_grid , cv = 5)\ngrid_tfidf_svm.fit(df_string_train, new_y_pooled)\nprint(\"Best cross-validation score: {:.4f}\".format(grid_tfidf_svm.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_tfidf_svm.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"id":"pUOEvFXGpT_l","outputId":"1b286162-6d7d-4730-c65d-37f02287e67a","trusted":true},"cell_type":"code","source":"param_grid = {'C':[0.01,0.1,1,10,100],\n             'gamma':[0.001,0.01,0.1,1,10,100],\n              'kernel': ['linear', 'rbf'],}\ngrid_tfidf_svm = GridSearchCV(SVC(), param_grid= param_grid, cv = 5)\ngrid_tfidf_svm.fit(x_train_lemma_tfidf, new_y_pooled) # lemma with stopwords\n\nprint(\"Best cross-validation score: {:.4f}\".format(grid_tfidf_svm.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_tfidf_svm.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"id":"5iyiHjQ-mrzo"},"cell_type":"markdown","source":"**GloVe**"},{"metadata":{"id":"N5gOpTcumsZd","outputId":"447e21ca-7f07-4432-a903-03f0eeaded72","trusted":true},"cell_type":"code","source":"param_grid = {'svc__C':[0.01,0.1,1,10,50,100],\n             'svc__gamma':[0.001,0.005,0.01,0.1,1,10],\n              'svc__kernel': ['linear', 'rbf']}\n#pipe_glove_svm = make_pipeline(MinMaxScaler(),SVC())\n#pipe_glove_svm = make_pipeline(SVC())\npipe_glove_svm = make_pipeline(StandardScaler(),SVC())\n\ngrid_glove_svm = GridSearchCV(pipe_glove_svm, param_grid , cv = 5)\ngrid_glove_svm.fit(new_x_pooled, new_y_pooled)\nprint(\"Best cross-validation score: {:.4f}\".format(grid_glove_svm.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid_glove_svm.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"id":"0_ijKlJavQ2i"},"cell_type":"markdown","source":"**Best SVM model**"},{"metadata":{"id":"VnYY0E5V9PbX","outputId":"d6bbcb4e-53c2-4657-a4f9-400300a994b4","trusted":true},"cell_type":"code","source":"vect_svm = TfidfVectorizer(min_df = 1, stop_words = 'english', ngram_range = (1, 2)).fit(df_string_train)\nx_train_vect_svm = vect_svm.transform(df_string_train)\nx_test_vect_svm =  vect_svm.transform(df_string_test)\nbest_svm = SVC(C = 10, gamma = 0.1, kernel = 'rbf').fit(x_train_vect_svm, new_y_pooled)\ny_pred_svm = best_svm.predict(x_test_vect_svm)\n# can't increase the result even if using lemmatization\nprint(\"Best accuracy score for Supported Vector Machinie: {:.4f}\".format(best_svm.score(x_test_vect_svm,y_test)))\nprint(classification_report(y_test, y_pred_svm ))\nplot_cf_matrix(y_test, y_pred_svm)","execution_count":null,"outputs":[]},{"metadata":{"id":"PZeDxYWPPtOf"},"cell_type":"markdown","source":"# Neural Networks"},{"metadata":{"id":"LbkvFtxObp_z","trusted":true},"cell_type":"code","source":"# one hot-encoding for train and validation\nfrom keras.utils.np_utils import to_categorical\n\n\ncategorical_y_train = to_categorical(y_train)\ncategorical_y_validation = to_categorical(y_validation)\n\n# merge one hot-encoding\ntargets = np.concatenate((categorical_y_train, categorical_y_validation), axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"id":"mTQaD_q2ZHYs"},"cell_type":"markdown","source":"# Naive Neural Networks"},{"metadata":{"id":"_g_MMmW2E3xR"},"cell_type":"markdown","source":"Test different value of activation function"},{"metadata":{"id":"KSwLEmyaE4nE","outputId":"c6deef23-89c6-4d81-a3e7-ab69d6dcdc56","trusted":true},"cell_type":"code","source":"parameters = ['relu', 'tanh', 'sigmoid']\nevolution_activation = []\nfor activation in parameters:\n  naive_model = Sequential()\n  naive_model.add(Dense(64, activation = activation, input_dim=250))\n  naive_model.add(Dropout(0.5))\n  naive_model.add(Dense(5, activation='softmax'))\n  naive_model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n  history = naive_model.fit(x_train, categorical_y_train, batch_size=32, epochs=75,validation_data=(x_validation, categorical_y_validation))\n  evolution_activation.append(history)\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"4Q6OR1xnPF1x","outputId":"b74354db-104d-409a-9d99-d7967c0b1b25","trusted":true},"cell_type":"code","source":"plot_accuracy_loss(evolution_activation, len(parameters), parameters)","execution_count":null,"outputs":[]},{"metadata":{"id":"DCmW3XLnE3Qo"},"cell_type":"markdown","source":"Test different value of dropout rate"},{"metadata":{"id":"pCSXp7OnE43K","trusted":true},"cell_type":"code","source":"parameters = [0.001, 0.25, 0.35, 0.5]\nevolution_dropout = []\nfor dropout in parameters:\n  naive_model = Sequential()\n  naive_model.add(Dense(64, activation = \"relu\", input_dim=250))\n  naive_model.add(Dropout(dropout))\n  naive_model.add(Dense(5, activation='softmax'))\n  naive_model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n  history = naive_model.fit(x_train, categorical_y_train, batch_size=32, epochs=75,validation_data=(x_validation, categorical_y_validation))\n  evolution_dropout.append(history)\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"N70y18XWPGdF","outputId":"33f00f05-152a-4079-d1c5-ef3825ecd92d","trusted":true},"cell_type":"code","source":"plot_accuracy_loss(evolution_dropout, len(parameters), parameters)","execution_count":null,"outputs":[]},{"metadata":{"id":"Zjid2j0hE5ck"},"cell_type":"markdown","source":"K-Fold cross-validation\n"},{"metadata":{"id":"Ql9xu3IxXqg5","trusted":true},"cell_type":"code","source":"\ndef get_model_name(k):\n    return 'model_'+str(k)+'.h5'\n\nK = 7   # numbers of folds\nskf = StratifiedKFold(n_splits = K, random_state = 7, shuffle = True) # creating the stratified k-fold cross validation\n","execution_count":null,"outputs":[]},{"metadata":{"id":"EZn9--jTNoGU","outputId":"80053706-cfd0-46d8-f9f6-2e9dcbfefae7","trusted":true},"cell_type":"code","source":"validation_accuracy = []\nvalidation_loss = []\nk_fold_evolution = []\n\nsave_dir = '/saved_models/'\nfold_var = 1\n\nfor train_index, val_index in skf.split(np.zeros(132),new_y_pooled):\n  training_data = new_x_pooled[train_index]\n  training_targets = targets[train_index]\n  validation_data = new_x_pooled[val_index]\n  validation_targets = targets[val_index]\n  # create and compile new model\n  naive_model = Sequential()\n  naive_model.add(Dense(64, activation = \"relu\", input_dim=250))\n  naive_model.add(Dropout(0.35))\n  naive_model.add(Dense(5, activation='softmax'))\n  naive_model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n\n  print('------------------------------------------------------------------------')\n  print('------------------------ history for fold {} ----------------------------'.format(fold_var))\n  print('------------------------------------------------------------------------')\n  history = naive_model.fit(training_data, training_targets, batch_size=32, epochs=75,validation_data=(validation_data, validation_targets));\n\n  # CREATE CALLBACKS\n  checkpoint = ModelCheckpoint(save_dir+get_model_name(fold_var), \n\t\t\t\t\t\t\tmonitor='val_accuracy', verbose=1, \n\t\t\t\t\t\t\tsave_best_only=True, mode='max')\n  callbacks_list = [checkpoint]\n\t# This saves the best model\n  # Generate generalization metrics\t\n  results = naive_model.evaluate(validation_data,validation_targets,verbose=0)\n  results = dict(zip(naive_model.metrics_names,results))\n  validation_accuracy.append(results['accuracy'])\n  validation_loss.append(results['loss'])\n  k_fold_evolution.append(history)\n\t\n  backend.clear_session()\n  fold_var += 1 # Increase fold number\n\n    \n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"mFqdksFnAdzz","outputId":"2c45e96e-fc8d-498c-9b39-f5fe542e01cc","trusted":true},"cell_type":"code","source":"# print useful result of stratified k-fold cross validation\nprint('Average scores for all folds:')\nprint(f'> Accuracy: {np.mean(validation_accuracy)*100}% (+- {np.std(validation_accuracy)*100}%)')\nprint(f'> Loss: {np.mean(validation_loss)}\\n\\n')\n# plotting resuls of stratified k-fold cross validation\nplot_accuracy_loss(k_fold_evolution,K)\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"2h47M6IDaDVT"},"cell_type":"markdown","source":"# Deep Neural Network\n"},{"metadata":{"id":"BThJWgOhaaN5"},"cell_type":"markdown","source":"Test different type of architecture"},{"metadata":{"id":"Zsg7Z6bmaa5I","outputId":"91a0cdf8-254d-4a43-d034-37f0d4521ed4","trusted":true},"cell_type":"code","source":"first_layer_dim = [128,64]\nsecond_layer_dim = [64,32,16]\nevolution_architecture = []\nfor dim_1 in first_layer_dim:\n  for dim_2 in second_layer_dim:\n    dnn_model = Sequential()\n    dnn_model.add(Dense(dim_1, activation = \"relu\", input_dim=250))\n    dnn_model.add(Dropout(0.5))\n    dnn_model.add(Dense(dim_2, activation = \"relu\"))\n    dnn_model.add(Dense(5, activation='softmax'))\n    dnn_model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n    history = dnn_model.fit(x_train, categorical_y_train, batch_size=32, epochs=75,validation_data=(x_validation, categorical_y_validation))\n    evolution_architecture.append(history)","execution_count":null,"outputs":[]},{"metadata":{"id":"lbEu4_5HabN5","outputId":"0956de6e-2fee-45fb-8190-83e7bba117ad","trusted":true},"cell_type":"code","source":"plot_accuracy_loss(evolution_architecture,6)","execution_count":null,"outputs":[]},{"metadata":{"id":"Ao-078Jvabkk"},"cell_type":"markdown","source":"Test different value of activation function"},{"metadata":{"id":"duytFZ2Bab3F","outputId":"05c8d5b6-ee8f-4177-f5c6-de2d86b19cc9","trusted":true},"cell_type":"code","source":"parameters = ['relu', 'tanh', 'sigmoid']\nevolution_activation = []\nfor activation in parameters:\n  dnn_model = Sequential()\n  dnn_model.add(Dense(128, activation = activation, input_dim=250))\n  dnn_model.add(Dropout(0.5))\n  dnn_model.add(Dense(32, activation = activation))\n  dnn_model.add(Dense(5, activation='softmax'))\n  dnn_model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n  history = dnn_model.fit(x_train, categorical_y_train, batch_size=32, epochs=75,validation_data=(x_validation, categorical_y_validation))\n  evolution_activation.append(history)\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"dpbq0Akkc_mW","outputId":"66978e4a-1391-45d6-ea1e-60b4d18f9e6d","trusted":true},"cell_type":"code","source":"plot_accuracy_loss(evolution_activation, len(parameters), parameters)","execution_count":null,"outputs":[]},{"metadata":{"id":"NeeBOD1daci5"},"cell_type":"markdown","source":"Test different value of dropout rate"},{"metadata":{"id":"g4RIm5Liac18","outputId":"8644cd85-877b-4fb7-8a86-162cb0b8f90d","trusted":true},"cell_type":"code","source":"parameters = [0.001, 0.25, 0.35, 0.5]\nevolution_dropout = []\nfor dropout in parameters:\n  dnn_model = Sequential()\n  dnn_model.add(Dense(128, activation = \"sigmoid\", input_dim=250))\n  dnn_model.add(Dropout(dropout))\n  dnn_model.add(Dense(32, activation = \"sigmoid\"))\n  dnn_model.add(Dense(5, activation='softmax'))\n  dnn_model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n  history = dnn_model.fit(x_train, categorical_y_train, batch_size=32, epochs=75,validation_data=(x_validation, categorical_y_validation))\n  evolution_dropout.append(history)\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"TMGH_NKnadFp","outputId":"4630ea1f-c087-45bf-e800-8dff2f6f7574","trusted":true},"cell_type":"code","source":"plot_accuracy_loss(evolution_dropout, len(parameters), parameters)","execution_count":null,"outputs":[]},{"metadata":{"id":"PGuocO-ipOQ9"},"cell_type":"markdown","source":"*K*-Fold cross-validation\n"},{"metadata":{"id":"_vqBYrviuMZN","trusted":true},"cell_type":"code","source":"def get_model_name(k):\n    return 'model_'+str(k)+'.h5'\n\nK = 7   # numbers of folds\nskf = StratifiedKFold(n_splits = K, random_state = 7, shuffle = True) # creating the stratified k-fold cross validation\n","execution_count":null,"outputs":[]},{"metadata":{"id":"sVb8iVQFpW9j","trusted":true},"cell_type":"code","source":"validation_accuracy = []\nvalidation_loss = []\nk_fold_evolution = []\n\nsave_dir = '/saved_models/'\nfold_var = 1\n\nfor train_index, val_index in skf.split(np.zeros(132),new_y_pooled):\n  training_data = new_x_pooled[train_index]\n  training_targets = targets[train_index]\n  validation_data = new_x_pooled[val_index]\n  validation_targets = targets[val_index]\n  # create and compile new model\n  dnn_model = Sequential()\n  dnn_model.add(Dense(128, activation = \"sigmoid\", input_dim=250))\n  dnn_model.add(Dropout(0.25))\n  dnn_model.add(Dense(32, activation = \"sigmoid\"))\n  dnn_model.add(Dense(5, activation='softmax'))\n  dnn_model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n  print('------------------------------------------------------------------------')\n  print('------------------------ history for fold {} ----------------------------'.format(fold_var))\n  print('------------------------------------------------------------------------')\n  history = dnn_model.fit(training_data, training_targets, batch_size=32, epochs=75,validation_data=(validation_data, validation_targets));\n\n  # CREATE CALLBACKS\n  checkpoint = ModelCheckpoint(save_dir+get_model_name(fold_var), \n\t\t\t\t\t\t\tmonitor='val_accuracy', verbose=1, \n\t\t\t\t\t\t\tsave_best_only=True, mode='max')\n  callbacks_list = [checkpoint]\n\t# This saves the best model\n  # Generate generalization metrics\t\n  results = dnn_model.evaluate(validation_data,validation_targets,verbose=0)\n  results = dict(zip(dnn_model.metrics_names,results))\n  validation_accuracy.append(results['accuracy'])\n  validation_loss.append(results['loss'])\n  k_fold_evolution.append(history)\n\t\n  backend.clear_session()\n  fold_var += 1 # Increase fold number\n\n    \n","execution_count":null,"outputs":[]},{"metadata":{"id":"q6_k5eDwDPhy","outputId":"f57299d4-2fa2-491d-b04b-cc8a601467ae","trusted":true},"cell_type":"code","source":"# print useful result of stratified k-fold cross validation\nprint('Average scores for all folds:')\nprint(f'> Accuracy: {np.mean(validation_accuracy)*100}% (+- {np.std(validation_accuracy)*100}%)')\nprint(f'> Loss: {np.mean(validation_loss)}\\n\\n')\n# plotting resuls of stratified k-fold cross validation\nplot_accuracy_loss(k_fold_evolution,K)","execution_count":null,"outputs":[]},{"metadata":{"id":"eBl9qJPuuVix"},"cell_type":"markdown","source":"# LSTM\n"},{"metadata":{"id":"UIOPRa3Tb9Fy","outputId":"a57088c6-077c-41ef-f4a9-536663825a29","trusted":true},"cell_type":"code","source":"# creating the matrix of total words with gloVe values\nindex_words = {}\nembeddings_index = {}\n\ni = 0\nfn = open('../input/glove-global-vectors-for-word-representation/glove.twitter.27B.25d.txt','r')\n\nword_arr = set()\nfor line in fn:\n    values = line.split()\n    word = values[0]\n    word_arr.add(word)\n    w_value = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = w_value\nfn.close()\nprint('Library composed of {} word vectors.'.format(len(embeddings_index)))\n\nfor w in sorted(word_arr):\n    index_words[w] = i\n    i += 1\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"txST-iaZr9-9","trusted":true},"cell_type":"code","source":"# creating the matrix of our sentences populated with gloVe values\ndef index_sentences(txt, idx_word, maxlen):\n  n_rows = txt.shape[0]\n  sentence_indices = np.zeros((n_rows, maxlen))\n\n  for row in range(n_rows):\n      sentence_words = txt[row].lower().split()\n      col_sentence = 0\n      for w in sentence_words:\n          sentence_indices[row, col_sentence] = idx_word[w]\n          col_sentence += 1\n  return sentence_indices\n            ","execution_count":null,"outputs":[]},{"metadata":{"id":"uMcqqGiTp7tB","trusted":true},"cell_type":"code","source":"feat_train = index_sentences(df_train['phrase'].values, index_words, 10)\nfeat_validation = index_sentences(df_validation['phrase'].values, index_words, 10)\n\nfeatures = np.concatenate((feat_train, feat_validation), axis = 0)\ntargets = np.concatenate((to_categorical(y_train), to_categorical(y_validation)), axis = 0)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"hbdfpXAiHjmX","outputId":"6cc08f4d-e74a-4c7e-bca6-76f4476b8a40","trusted":true},"cell_type":"code","source":"sequence_length = 10\nembedding_dim = 25 \nnumber_of_words = len(index_words.values())\n\n# first create a matrix of zeros, this is our embedding matrix\nembedding_matrix = np.zeros((len(embeddings_index), embedding_dim))\n\nprint(embedding_matrix.shape)\n\n# then fill the matrix in order to be processed by the model\nfor word, i in index_words.items():\n    try:\n      embedding_matrix[i, :] = embeddings_index[word]\n    except:\n      continue\n","execution_count":null,"outputs":[]},{"metadata":{"id":"1AySt3cTJd5U"},"cell_type":"markdown","source":"Test different value of SpatialDropout"},{"metadata":{"id":"BTPHzlo3Krhh","trusted":true},"cell_type":"code","source":"parameters = [0.01, 0.1, 0.25, 0.5]\nevolution_dropout = []\n\nfor dropout in parameters:\n  lstm_model = Sequential()\n  lstm_model.add(Embedding(number_of_words,\n                      25,\n                      embeddings_initializer=Constant(embedding_matrix),\n                      input_length=sequence_length,\n                      trainable=True))\n  lstm_model.add(SpatialDropout1D(dropout))\n  lstm_model.add(Bidirectional(LSTM(64, return_sequences=True)))\n  lstm_model.add(Bidirectional(LSTM(32)))\n  lstm_model.add(Dropout(0.25))\n  lstm_model.add(Dense(units=5, activation='softmax'))\n  lstm_model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n  history = lstm_model.fit(feat_train, categorical_y_train, batch_size=32, epochs=75, verbose=1, validation_data=(feat_validation ,categorical_y_validation))\n  evolution_dropout.append(history)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"ZJXBJinbbLPA","outputId":"f42fbef4-ed2f-457f-e3a8-d9afc0773718","trusted":true},"cell_type":"code","source":"plot_accuracy_loss(evolution_dropout, len(parameters), parameters)","execution_count":null,"outputs":[]},{"metadata":{"id":"L3TOchRsKEbV"},"cell_type":"markdown","source":"Test different value of Dropuot"},{"metadata":{"id":"1zocGyaVKE6o","trusted":true},"cell_type":"code","source":"parameters = [0.1, 0.25, 0.5]\nevolution_dropout = []\n\nfor dropout in parameters:\n  lstm_model = Sequential()\n  lstm_model.add(Embedding(number_of_words,\n                      25,\n                      embeddings_initializer=Constant(embedding_matrix),\n                      input_length=sequence_length,\n                      trainable=True))\n  lstm_model.add(SpatialDropout1D(0.25))\n  lstm_model.add(Bidirectional(LSTM(64, return_sequences=True)))\n  lstm_model.add(Bidirectional(LSTM(32)))\n  lstm_model.add(Dropout(dropout))\n  lstm_model.add(Dense(units=5, activation='softmax'))\n  lstm_model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n  history = lstm_model.fit(feat_train, categorical_y_train, batch_size=32, epochs=75, verbose=1, validation_data=(feat_validation ,categorical_y_validation))\n  evolution_dropout.append(history)","execution_count":null,"outputs":[]},{"metadata":{"id":"gWKXAX6uKFS3","trusted":true},"cell_type":"code","source":"plot_accuracy_loss(evolution_dropout, len(parameters), parameters)","execution_count":null,"outputs":[]},{"metadata":{"id":"ulA_xj5YmuS7"},"cell_type":"markdown","source":"*K*-Fold cross-validation\n"},{"metadata":{"id":"APRYCaWXnPoT","trusted":true},"cell_type":"code","source":"def get_model_name(k):\n    return 'model_'+str(k)+'.h5'\n\nK = 7   # numbers of folds\nskf = StratifiedKFold(n_splits = K, random_state = 7, shuffle = True) # creating the stratified k-fold cross validation\n","execution_count":null,"outputs":[]},{"metadata":{"id":"B2ZdR_gKou8T","outputId":"562b2245-83ef-407d-8665-ed50fd4bc791","trusted":true},"cell_type":"code","source":"validation_accuracy = []\nvalidation_loss = []\nk_fold_evolution = []\n\nsave_dir = '/saved_models/'\nfold_var = 1\n\nfor train_index, val_index in skf.split(np.zeros(132),new_y_pooled):\n  training_data = features[train_index]\n  training_targets = targets[train_index]\n  validation_data = features[val_index]\n  validation_targets = targets[val_index]\n  # create and compile new model  \n  lstm_model = Sequential()\n  lstm_model.add(Embedding(number_of_words,\n                      25,\n                      embeddings_initializer=Constant(embedding_matrix),\n                      input_length=sequence_length,\n                      trainable=True))\n  lstm_model.add(SpatialDropout1D(0.25))\n  lstm_model.add(Bidirectional(LSTM(64, return_sequences=True)))\n  lstm_model.add(Bidirectional(LSTM(32)))\n  lstm_model.add(Dropout(0.25))\n  lstm_model.add(Dense(units=5, activation='softmax'))\n  lstm_model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n  print('------------------------------------------------------------------------')\n  print('------------------------ history for fold {} ----------------------------'.format(fold_var))\n  print('------------------------------------------------------------------------')\n  history = lstm_model.fit(training_data, training_targets, batch_size=32, epochs=75,validation_data=(validation_data, validation_targets));\n\n  # CREATE CALLBACKS\n  checkpoint = ModelCheckpoint(save_dir+get_model_name(fold_var), \n\t\t\t\t\t\t\tmonitor='val_accuracy', verbose=1, \n\t\t\t\t\t\t\tsave_best_only=True, mode='max')\n  callbacks_list = [checkpoint]\n\t# This saves the best model\n  # Generate generalization metrics\t\n  results = lstm_model.evaluate(validation_data,validation_targets,verbose=0)\n  results = dict(zip(lstm_model.metrics_names,results))\n  validation_accuracy.append(results['accuracy'])\n  validation_loss.append(results['loss'])\n  k_fold_evolution.append(history)\n\t\n  backend.clear_session()\n  fold_var += 1 # Increase fold number\n","execution_count":null,"outputs":[]},{"metadata":{"id":"49Uz6DBA8ffJ","outputId":"530f2988-608a-48f5-8f69-e80b15d8bbec","trusted":true},"cell_type":"code","source":"# print useful result of stratified k-fold cross validation\nprint('Average scores for all folds:')\nprint(f'> Accuracy: {np.mean(validation_accuracy)*100}% (+- {np.std(validation_accuracy)*100}%)')\nprint(f'> Loss: {np.mean(validation_loss)}\\n\\n')\n# plotting resuls of stratified k-fold cross validation\nplot_accuracy_loss(k_fold_evolution,K)","execution_count":null,"outputs":[]},{"metadata":{"id":"3pkxJJ-UNoGV"},"cell_type":"markdown","source":"# Evaluation"},{"metadata":{"id":"MsbxcpoVNoGV","trusted":true},"cell_type":"code","source":"##################################################\n# Evaluate the model here\n##################################################\n\n# Use this function to evaluate your model\ndef accuracy(y_pred, y_true):\n    '''\n    input y_pred: ndarray of shape (N,)\n    input y_true: ndarray of shape (N,)\n    '''\n    return (1.0 * (y_pred == y_true)).mean()\n\n# Report the accuracy in the train and validation sets.","execution_count":null,"outputs":[]},{"metadata":{"id":"NmxJIwg9NoGV"},"cell_type":"markdown","source":"# Send the submission for the challenge"},{"metadata":{"id":"zEFbThqApT7k","trusted":true},"cell_type":"code","source":"\ny_test_pred = None\n\n# Create submission\nsubmission = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'sample_submission.csv'))\nx_test = np.load(os.path.join(DATA_BASE_FOLDER, 'test.npy')).reshape(len(submission), -1)\nsubmission[\"phrase\"] = submission[\"phrase\"].str.replace(\"\\t\", \"\")\n# print(submission[\"phrase\"]) \nstring_submission = [\"\".join(x) for x in submission[\"phrase\"]]\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"Hg_fsqr5sfB1"},"cell_type":"markdown","source":"SVM Submission"},{"metadata":{"id":"vz_NtLVQsdfY","outputId":"4158056d-d74c-4408-aa00-4bd5891e581e","trusted":true},"cell_type":"code","source":"vect_svm = TfidfVectorizer(min_df = 1, stop_words = 'english', ngram_range = (1, 2)).fit(df_string_train)\nx_train_vect_svm = vect_svm.transform(df_string_train)\nx_test_vect_svm =  vect_svm.transform(string_submission)\nbest_svm = SVC(C = 10, gamma = 0.1, kernel = 'rbf').fit(x_train_vect_svm, new_y_pooled)\ny_pred_svm = best_svm.predict(x_test_vect_svm)\nprint(y_pred_svm)","execution_count":null,"outputs":[]},{"metadata":{"id":"N4caZq0cuY9G"},"cell_type":"markdown","source":"Logistic Regression Submission"},{"metadata":{"id":"8jn0Op2HuTNi","outputId":"1d77d8e7-f3f6-4a6f-fb15-7796e744437d","trusted":true},"cell_type":"code","source":"vect_lr = TfidfVectorizer(min_df = 1, stop_words = 'english', ngram_range = (1, 1)).fit(df_string_train)\nx_train_vect_lr = vect_lr.transform(df_string_train)\nx_test_vect_lr =  vect_lr.transform(string_submission)\nbest_lr = LogisticRegression(C = 10).fit(x_train_vect_lr, new_y_pooled)\ny_pred_lr = best_lr.predict(x_test_vect_lr)\nprint(y_pred_lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##################################################\n# Save your test prediction in y_test_pred\n##################################################\n\ny_test_pred = y_pred_lr\n\n# Create submission\nsubmission = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'sample_submission.csv'))\n\nx_test = np.load(os.path.join(DATA_BASE_FOLDER, 'test.npy')).reshape(len(submission), -1)\n\nif y_test_pred is not None:\n     submission['class'] = y_test_pred\n\nsubmission = submission[['Unnamed: 0', 'class']]\nsubmission = submission.rename(columns={'Unnamed: 0': 'id'})\nsubmission.to_csv('my_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LSTM Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ny_test_pred = None\n\n# Create submission\nsubmission = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'sample_submission.csv'))\nx_test = np.load(os.path.join(DATA_BASE_FOLDER, 'test.npy')).reshape(len(submission), -1)\nsubmission[\"phrase\"] = submission[\"phrase\"].str.replace(\"\\t\", \"\")\n# print(submission[\"phrase\"]) \nstring_submission = [\"\".join(x) for x in submission[\"phrase\"]]\n","execution_count":null,"outputs":[]},{"metadata":{"id":"i_9qpMGIwxvC","trusted":true,"collapsed":true},"cell_type":"code","source":"\n# create and compile new model  \nlstm_model = Sequential()\nlstm_model.add(Embedding(number_of_words,\n                    25,\n                    embeddings_initializer=Constant(embedding_matrix),\n                    input_length=sequence_length,\n                    trainable=True))\nlstm_model.add(SpatialDropout1D(0.25))\nlstm_model.add(Bidirectional(LSTM(64, return_sequences=True)))\nlstm_model.add(Bidirectional(LSTM(32)))\nlstm_model.add(Dropout(0.25))\nlstm_model.add(Dense(units=5, activation='softmax'))\nlstm_model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n\nhistory = lstm_model.fit(features, targets, batch_size=32, epochs=75);\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"DqL4Y4YDyEF9","trusted":true},"cell_type":"code","source":"\nfeat_submission = index_sentences(submission['phrase'].values, index_words, 10)\ny_pred_lstm = lstm_model.predict(feat_submission)\ny_pred_lstm = np.argmax(y_pred_lstm, axis=1)\n\nprint(y_pred_lstm)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"7WsXXzwoNoGV","trusted":true},"cell_type":"code","source":"##################################################\n# Save your test prediction in y_test_pred\n##################################################\n\ny_test_pred = y_pred_lstm\n\n# Create submission\nsubmission = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'sample_submission.csv'))\n\nx_test = np.load(os.path.join(DATA_BASE_FOLDER, 'test.npy')).reshape(len(submission), -1)\n\nif y_test_pred is not None:\n     submission['class'] = y_test_pred\n\nsubmission = submission[['Unnamed: 0', 'class']]\nsubmission = submission.rename(columns={'Unnamed: 0': 'id'})\nsubmission.to_csv('my_submission.csv', index=False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}