{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Read Data"},{"metadata":{},"cell_type":"markdown","source":"#### Excel & CSV"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\n# excel\ndf = pd.read_excel('/kaggle/input/allianz-news-jan-jun-2020/allianz_news_jan_jun_2020.xlsx')\ndf.head()\n\n# csv\ndf = pd.read_csv('/kaggle/input/allianz-news-jan-jun-2020/allianz_news_jan_jun_2020.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### JSON"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport json\n\n# option 1\npath = '../input/stanford-covid-vaccine/'\ndf = pd.read_json(f'{path}/train.json', lines=True).drop(columns='index')\ntest = pd.read_json(f'{path}/test.json', lines=True).drop(columns='index')\nsubmission = pd.read_csv(f'{path}/sample_submission.csv')\n\n# option 2\nwith open(\"../input/allianz-twitter-tweets-2011-2020/tweet.js\", 'rb') as handle:\n    df = json.load(handle)\n    \n# option 3\ndf = pd.read_json('/kaggle/input/allianz-twitters-tweet-2011-2019/20191102-tweet.js')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Predict"},{"metadata":{},"cell_type":"markdown","source":"### Text Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = model.predict(df_submit_text_list)\ntarget","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Image Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature, target = next(iter(testloader))\nfeature, target = feature.to(device), target.to(device)\n\n\n# alt 1\nwith torch.no_grad():\n    model.eval()\n    output = model(feature)\n    preds = output.argmax(1)\npreds\n\n# alt 2\nwith torch.no_grad():\n    model.eval()\n    output = model(feature)\n    preds = (output > 0.5).to(torch.float32)\npreds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualization"},{"metadata":{},"cell_type":"markdown","source":"### Text Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize Target Label\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ny_train.shape, y_test.shape\n\n\n# y_train.shape\nsns.set(style=\"darkgrid\")\nsns.countplot(x=y_train)\nplt.title(\"y_train\");\n\n\n# y_test.shape\nsns.set(style=\"darkgrid\")\nsns.countplot(x=y_test)\nplt.title(\"y_test\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Word Cloud\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Image Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\n\n\nfig, axes = plt.subplots(6, 6, figsize=(24, 24))\nfor image, label, pred, ax in zip(feature, target, preds, axes.flatten()):\n    ax.imshow(image.permute(1, 2, 0).cpu())\n    font = {\"color\": 'r'} if label != pred else {\"color\": 'g'}        \n    label, pred = label2cat[label.item()], label2cat[pred.item()]\n    ax.set_title(f\"L: {label} | P: {pred}\", fontdict=font);\n    ax.axis('off');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Save Model"},{"metadata":{},"cell_type":"markdown","source":"### with JCOPML"},{"metadata":{"trusted":true},"cell_type":"code","source":"from jcopml.utils import save_model\n\nsave_model(model_logreg_tfidf, \"model_logreg_tfidf.pkl\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### with Pickle"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\npickle.dump(model, open(\"knn.pkl\", 'wb'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### with os"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\nos.makedirs(\"model/fasttext/\", exist_ok=True)\nmodel_fasttext.save(\"model/fasttext/capres_sentiment.fasttext\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Model"},{"metadata":{},"cell_type":"markdown","source":"### with JCOPML"},{"metadata":{"trusted":true},"cell_type":"code","source":"from jcopml.utils import load_model\n\nmodel = load_model('model/knn.pkl')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### with Pickle"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\nmodel = pickle.load(open(\"knn.pkl\", \"rb\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### with os"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submit"},{"metadata":{},"cell_type":"markdown","source":"### Text Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submit = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\ndf_submit.drop(columns=[\"keyword\", \"location\"], inplace=True)\ndf_submit_id_list = df_submit.id.values.tolist()\ndf_submit_text_list = df_submit.text.values.tolist()\ndf_submit.head()\n\n\n\nprint(\"df_submit.id: \", len(df_submit.id))\nprint(\"df_submit.text: \", len(df_submit.text))\nprint(\"df_submit_id_list: \", len(df_submit_id_list))\nprint(\"df_submit_text_list: \", len(df_submit_text_list))\n\n\n\ntarget = model.predict(df_submit_text_list)\ntarget\n\n\n\ndf_submit_final = pd.DataFrame({\n    \"id\": df_submit_id_list,\n    \"target\": target\n})\n\n\n\ndf_submit_final.set_index('id', inplace=True)\ndf_submit_final.head()\n\n\n\ndf_submit_final.to_csv(\"disaster_tweet_v10.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set_final = datasets.ImageFolder(\"/kaggle/input/chest-xray-pneumonia/chest_xray/test\", transform=test_transform)\ntestloader_final = DataLoader(test_set_final, batch_size=bs)\n\n\n\nwith torch.no_grad():\n    test_cost_final, test_score_final = loop_fn(\"test\", test_set_final, testloader_final, model, criterion, optimizer, device)\n    print(f\"Test Accuracy: {test_score_final}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Preprocessing (manual)"},{"metadata":{},"cell_type":"markdown","source":"### Lower Case Normalization, Removing with Regex, Tokenization, Punctuation & Stop Words Removal"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk \nimport re\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom string import punctuation\n\nsw = stopwords.words(\"indonesian\") + stopwords.words(\"english\")\n\n\n\n# 1) Normalization to Lower Case & Removing \"https: ...\"\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(\"\\n\", \" \", text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = text.split()\n    text = \" \".join(text)\n    return text\n\ndf1 = df.Isi_Tweet.apply(str).apply(lambda x:clean_text(x))\n\n\n# display text samples\nfor i in range(len(df1)):\n    print(df1[i])\n    if i == 10:\n        break\n\nprint(\"The length of dataframe is\", len(df1), \"rows\")\n        \n\n\n    \n    \n# 2) Sentence & Word Tokenization; Punctuation and Words Removal\ndf1_clean_text = []\nfor i in range(len(df1)):\n    x = df1[i]\n    # x_sent_token = sent_tokenize(x)\n    # x_sent_token\n    x_word_tokens = word_tokenize(x)\n    # x_word_tokens\n#     print(x)\n    \n#     print(df1[i])\n    \n    # punctuation removal\n    x_word_tokens_removed_punctuations = [w for w in x_word_tokens if w not in punctuation]\n#     print(x_word_tokens_removed_punctuations, \"punctuation\")\n    \n    # numeric removal\n    x_word_tokens_removed_punctuations = [w for w in x_word_tokens_removed_punctuations if w.isalpha()]\n#     print(x_word_tokens_removed_punctuations, \"numeric\")\n    \n    # stopwords removal\n    x_word_tokens_removed_punctuation_removed_sw = [w for w in x_word_tokens_removed_punctuations if w not in sw]\n#     print(x_word_tokens_removed_punctuation_removed_sw, \"stopwords\")\n\n    # rejoining the words into one string/sentence as inputted before being tokenized\n    x_word_tokens_removed_punctuation_removed_sw = \" \".join(x_word_tokens_removed_punctuation_removed_sw)\n#     print(x_word_tokens_removed_punctuation_removed_sw)\n    \n    df1_clean_text.append(x_word_tokens_removed_punctuation_removed_sw)\n    \n    \n# display text vs processed text\nfor i,j in zip(df1[0:10], df1_clean_text[0:10]):\n    print(i)\n    print(j)\n    print()\n    \n    \n    \n\n\n# list (df1_clean_text) to series (df1_clean_text_series)\n\n# list\nprint(type(df1_clean_text))\nprint(len(df1_clean_text))\n\n# converting list to pandas series\ndf1_clean_text_series = pd.Series(df1_clean_text)\n\nprint(type(df1_clean_text_series))\nprint(len(df1_clean_text_series))\n\n\n# create new df\ndf['Isi_Tweet'] = df1_clean_text_series\ndf.head(10)\n\n\n# count total of words of old df vs new df \n\ntotal_words_new_df = df.Isi_Tweet.apply(lambda x: len(x.split(\" \"))).sum()\n\nprint(\"old df: \", total_words_old_df, \"words\")\nprint(\"new df: \", total_words_new_df, \"words\")\nprint(\"text processing has reduced the number of words by\", round((total_words_old_df-total_words_new_df)/total_words_old_df*100), \"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encoding with Word2Vec (FastText)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1) Prepare Corpus\nfrom tqdm.auto import tqdm\nfrom gensim.models import FastText\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\nfrom jcopml.pipeline import num_pipe, cat_pipe\nfrom jcopml.plot import plot_missing_value\nfrom jcopml.feature_importance import mean_score_decrease\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom string import punctuation\n\n# sw is list of stopwords (ID + EN) and punctuation\nsw = stopwords.words(\"indonesian\") + stopwords.words(\"english\") + list(punctuation)\n\n\nsentences = [word_tokenize(text.lower()) for text in tqdm(df.Isi_Tweet)]\n\n\n# 2) Train FastText Model\nimport os\n\nmodel_fasttext = FastText(sentences, size=128, window=5, min_count=3, workers=4, iter=100, sg=0, hs=0)\n\n# save\nos.makedirs(\"model/fasttext/\", exist_ok=True)\nmodel_fasttext.save(\"model/fasttext/capres_sentiment.fasttext\")\n\n\n# 3) Encoding\nfrom tqdm.auto import tqdm\nfrom gensim.models import FastText\n\nw2v = FastText.load(\"model/fasttext/capres_sentiment.fasttext\").wv\n\n\ndef simple_encode_sentence(sentence, w2v, stopwords=None):\n    if stopwords is None:\n        vecs = [w2v[word] for word in word_tokenize(sentence)]\n    else:\n        vecs = [w2v[word] for word in word_tokenize(sentence) if word not in stopwords]\n    sentence_vec = np.mean(vecs, axis=0)\n    return sentence_vec\n\ndef better_encode_sentence(sentence, w2v, stopwords=None):\n    if stopwords is None:\n        vecs = [w2v[word] for word in word_tokenize(sentence)]\n    else:\n        vecs = [w2v[word] for word in word_tokenize(sentence) if word not in stopwords]\n        \n    vecs = [vec / np.linalg.norm(vec) for vec in vecs if np.linalg.norm(vec) > 0]\n    sentence_vec = np.mean(vecs, axis=0)\n    return sentence_vec\n\n\nvecs = [better_encode_sentence(sentence, w2v, stopwords=sw) for sentence in df.Isi_Tweet]\nvecs = np.array(vecs)\nvecs\n\n\n# 4) When used in Dataset Splitting\nX = vecs\ny = df.Sentimen\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Algorithm"},{"metadata":{},"cell_type":"markdown","source":"### Sentiment Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# LOGISTIC REGRESSION\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom jcopml.tuning import random_search_params as rsp\nfrom jcopml.tuning import random_search_params as gsp\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n\n\npipeline = Pipeline([\n    ('prep', TfidfVectorizer(tokenizer=word_tokenize, ngram_range=(1, 3))),\n#     ('prep', CountVectorizer(tokenizer=word_tokenize, ngram_range=(1, 3))), #choose bow or tfidf\n    ('algo', LogisticRegression(solver='lbfgs', n_jobs=-1, random_state=42))\n])\n\nmodel_logreg_tfidf = RandomizedSearchCV(pipeline, rsp.logreg_params, cv=5, n_iter=50, n_jobs=-1, verbose=1, random_state=42)\n# model_logreg_tfidf = GridSearchCV(pipeline, gsp.logreg_params, cv=5, n_jobs=-1, verbose=1) # it takes longer time\nmodel_logreg_tfidf.fit(X_train, y_train)\n\nprint(model_logreg_tfidf.best_params_)\nprint(model_logreg_tfidf.score(X_train, y_train), model_logreg_tfidf.best_score_, model_logreg_tfidf.score(X_test, y_test))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LINEAR SVM\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom jcopml.tuning import random_search_params as rsp\nfrom jcopml.tuning import random_search_params as gsp\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n\n\npipeline = Pipeline([\n    ('prep', CountVectorizer(tokenizer=word_tokenize, ngram_range=(1, 3))),\n    ('algo', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n])\n\n\nparameter = {\n    'algo__loss': ['hinge', 'log', 'modified_huber', 'perceptron'],\n    'algo__penalty': ['l2', 'l1', 'elasticnet'],\n    'algo__alpha': [0.0001, 0.0002, 0.0003], \n    'algo__max_iter': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n    'algo__tol': [0.0001, 0.0002, 0.0003]\n}\n# model_sgd_bow = GridSearchCV(pipeline, parameter, cv=5, n_jobs=-1, verbose=1) # it takes longer time\nmodel_sgd_bow = RandomizedSearchCV(pipeline, parameter, cv=50, n_jobs=-1, verbose=1)\nmodel_sgd_bow.fit(X_train, y_train)\n\n\nprint(model_sgd_bow.best_params_)\nprint(model_sgd_bow.score(X_train, y_train), model_sgd_bow.best_score_, model_sgd_bow.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Image Classification with Densenet"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Architecture & Config\nfrom torchvision.models import densenet121\nfrom jcopdl.layers import linear_block\nfrom tqdm.auto import tqdm\n\ndnet = densenet121(pretrained=True)\n\n# freeze model\nfor param in dnet.parameters():\n    param.requires_grad = False\n    \n\ndnet.classifier = nn.Sequential(\n    nn.Linear(1024, 42),\n    nn.LogSoftmax()\n)\ndnet\n\n\nclass CustomDensenet121(nn.Module):\n    def __init__(self, output_size):\n        super().__init__()\n        self.dnet = densenet121(pretrained=True)\n        self.freeze()\n        self.dnet.classifier = nn.Sequential(\n#             linear_block(1024, 1, activation=\"lsoftmax\")\n            nn.Linear(1024, output_size),\n            nn.LogSoftmax(dim=1)\n        )\n        \n    def forward(self, x):\n        return self.dnet(x)\n\n    def freeze(self):\n        for param in self.dnet.parameters():\n            param.requires_grad = False\n            \n    def unfreeze(self):        \n        for param in self.dnet.parameters():\n            param.requires_grad = True  \n\n            \nconfig = set_config({\n    \"output_size\": len(train_set.classes),\n    \"batch_size\": bs,\n    \"crop_size\": crop_size\n})\n\n\n\n    \n# Phase 1: Adaptation (lr standard + patience low)\nmodel = CustomDensenet121(config.output_size).to(device)\ncriterion = nn.NLLLoss()\noptimizer = optim.AdamW(model.parameters(), lr=0.001)\ncallback = Callback(model, config, early_stop_patience=2, outdir=\"model\")\n\n# Phase 1: Training\ndef loop_fn(mode, dataset, dataloader, model, criterion, optimizer, device):\n    if mode == \"train\":\n        model.train()\n    elif mode == \"test\":\n        model.eval()\n    cost = correct = 0\n    for feature, target in tqdm(dataloader, desc=mode.title()):\n        feature, target = feature.to(device), target.to(device)\n        output = model(feature)\n        loss = criterion(output, target)\n        \n        if mode == \"train\":\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        \n        cost += loss.item() * feature.shape[0]\n        correct += (output.argmax(1) == target).sum().item()\n    cost = cost / len(dataset)\n    acc = correct / len(dataset)\n    return cost, acc\n\nwhile True:\n    train_cost, train_score = loop_fn(\"train\", train_set, trainloader, model, criterion, optimizer, device)\n    with torch.no_grad():\n        test_cost, test_score = loop_fn(\"test\", val_set, valloader, model, criterion, optimizer, device)\n    \n    # Logging\n    callback.log(train_cost, test_cost, train_score, test_score)\n\n    # Checkpoint\n    callback.save_checkpoint()\n        \n    # Runtime Plotting\n    callback.cost_runtime_plotting()\n    callback.score_runtime_plotting()\n    \n    # Early Stopping\n    if callback.early_stopping(model, monitor=\"test_score\"):\n        callback.plot_cost()\n        callback.plot_score()\n        break\n\n\n\n\n# Phase 2: Fine Tuning (lr low + patience high)\nmodel.unfreeze()\noptimizer = optim.AdamW(model.parameters(), lr=1e-5)\n\ncallback.reset_early_stop()\ncallback.early_stop_patience = 5\n\n# Phase 2: Training\nwhile True:\n    train_cost, train_score = loop_fn(\"train\", train_set, trainloader, model, criterion, optimizer, device)\n    with torch.no_grad():\n        test_cost, test_score = loop_fn(\"test\", val_set, valloader, model, criterion, optimizer, device)\n    \n    # Logging\n    callback.log(train_cost, test_cost, train_score, test_score)\n\n    # Checkpoint\n    callback.save_checkpoint()\n        \n    # Runtime Plotting\n    callback.cost_runtime_plotting()\n    callback.score_runtime_plotting()\n    \n    # Early Stopping\n    if callback.early_stopping(model, monitor=\"test_score\"):\n        callback.plot_cost()\n        callback.plot_score()\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import Data from Kaggle to Google Colab\n\nreference of access kaggle dataset for google colab\n- https://www.kaggle.com/general/51898\n- https://medium.com/@saedhussain/google-colaboratory-and-kaggle-datasets-b57a83eb6ef8"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Colab library to upload files to notebook\nfrom google.colab import files\n\n# Upload kaggle API key file\n# download kaggle.json from Kaggle - My Account - Create New API Token\nuploaded = files.upload()\n\n\n# Install Kaggle library & set on kaggle.json on the root\n!pip install -q kaggle\n!mkdir -p ~/.kaggle\n!cp kaggle.json ~/.kaggle/\n!ls ~/.kaggle\n!chmod 600 /root/.kaggle/kaggle.json\n\n\n# Download data for productdetection2\n# copy the API Command Link by right click menu at the right side of new notebook and copy the link\n!kaggle datasets download -d maharajaarizona/productdetection2\n\n\n# unzip data\n!unzip /content/productdetection2.zip","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting Train Data\n\nif the image data is not yet split, use this function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def stratified_split_folder(input_dir, output_dir, valid_size=0.2):\n    labels = [folder for folder in os.listdir(input_dir) if not folder.startswith(\".\")]\n    for label in labels:\n        # Dapatkan semua nama file untuk label tertentu\n        files = glob(f\"{input_dir}/{label}/*.jpg\")\n        \n        # Shuffle split\n        shuffle(files)\n        n_test = int(valid_size * len(files))\n        train, valid = files[-n_test:], files[-n_test:]\n        \n        # Untuk semua yang merupakan validation data, pindahkan ke folder baru\n        os.makedirs(f\"{output_dir}/{label}\", exist_ok=True)\n        for file in valid:\n            fname = os.path.basename(file)\n            os.rename(file, f\"{output_dir}/{label}/{fname}\")\n            \n\n            \nstratified_split_folder(input_dir=\"resized/train\", output_dir=\"resized/valid\", valid_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read JSON"},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(\"../input/ndsc-beginner/categories.json\", 'rb') as handle:\n    category_details = json.load(handle)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}