{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd # Data processing, CSV file I/O \nimport numpy as np # Linear Algebra\nimport matplotlib.pyplot as plt # Plotting graphs\n\nimport tensorflow as tf\n\n# Since I have a GPU & I've GPU enabled, I am going to use the GPU version of keras \n# (NOTE: Ignore if you do not have GPU enabled)\nfrom keras import backend as K\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\nK.tensorflow_backend._get_available_gpus()\n\nprint(f'Tensorflow {tf.__version__}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import required libraries\n\n# EDA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Basic ML Libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\n# Deep Learning libraries\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPool2D, GlobalAveragePooling2D, Dropout, Flatten, Dense, BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler\n\n%matplotlib inline\nplt.style.use('ggplot')\nsns.set_style('whitegrid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.listdir('.')\nos.listdir('../input')\ntrain_data = pd.read_csv('../input/dataset/train.csv')\ntest_data = pd.read_csv('../input/dataset/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rows, cols = 4, 4\n\n# Creating a matplotlib figure that holds 16 subplots (for 16 digit images)\nplt.figure(figsize=(8,7))\nplt.suptitle('Training Data')\n\n# Plotting the first 16 datapoints from the training data\nfor i in range(4):\n    for j in range(4):\n        index = (i * cols) + j # Row major ordering\n        plt.subplot(rows, cols, index + 1)\n        plt.xticks([])\n        plt.yticks([])\n        \n        label = train_data['label'].values[index]\n        image = train_data.drop('label', axis=1).iloc[index].values.reshape(28, 28) # Reshaping the 1D array into a 2D array of pixel values\n        \n        plt.title(label)\n        \n        # Using a binary color map\n        plt.imshow(image, cmap='binary')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A function to scale the values and return the data as numpy arrays\ndef preprocess_data(df):\n    # Training Data\n    if 'label' in df.columns: \n        df_x = df.drop('label', axis=1) / 255.0\n        df_y = df['label']\n        \n        df_x = df_x.values.reshape(df.shape[0], 28, 28, 1)        \n        return (df_x, df_y)\n    \n    # Testing Data\n    else: \n        df_x = df.div(255.0)\n        \n        df_x = df_x.values.reshape(df.shape[0], 28, 28, 1)\n        return df_x\n\n# Applying the preprocess_data function to both the train & test data\ntrain_X, train_y = preprocess_data(train_data)\ntest_X = preprocess_data(test_data)\n\nprint(f'Training data shape: X-{train_X.shape} & y-{train_y.shape}')\nprint(f'Testing data shape: X-{test_X.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initializing some constants for the Neural Network Architecture\nINPUT_SHAPE = train_X.shape[1:] # (28, 28, 1)\n\n# The number of training examples per batch of training\nBATCH_SIZE = 128\n\n# The number of epochs or iterations of the training loop\nEPOCHS = 4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Building the model\nmodel = tf.keras.models.Sequential()\n\nmodel.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same', input_shape=INPUT_SHAPE))\nmodel.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same'))\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\nmodel.add(tf.keras.layers.Dropout(0.2))\nmodel.add(tf.keras.layers.BatchNormalization())\n\nmodel.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))\nmodel.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2))) \nmodel.add(tf.keras.layers.Dropout(0.2))\nmodel.add(tf.keras.layers.BatchNormalization())\n\nmodel.add(tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same'))\nmodel.add(tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same'))\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2))) \nmodel.add(tf.keras.layers.Dropout(0.2))\nmodel.add(tf.keras.layers.BatchNormalization())\n\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(512, activation='relu'))\nmodel.add(tf.keras.layers.Dense(10, activation='softmax'))\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 'adam' is the most used optimizer\n# The loss function used is SCC because it's a multi-class classification problem with integer value classes\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n\n# Fitting the model and using 10% of the data for validation\nhistory = model.fit(x=train_X, y=train_y, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating the predictions file to submit\nsubmission_df = pd.read_csv('../input/submission/sample_submission.csv')\npredictions = model.predict(test_X)\nsubmission_vals = []\n\nfor i in range(len(predictions)):\n    submission_vals.append(np.argmax(predictions[i]))\n\nsubmission_df['Label'] = submission_vals\n\n# Saving the submission file\nsubmission_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nimport matplotlib.pyplot as plt\nimport numpy\n\n# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Model accuracy is more than 98% and we can have more if we added more epochs."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}