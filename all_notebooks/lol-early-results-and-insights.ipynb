{"cells":[{"metadata":{},"cell_type":"markdown","source":"# How to win League of Legends?"},{"metadata":{},"cell_type":"markdown","source":"#### The following notebook shows the results of our joint work on the Machine Learning Course Final Project"},{"metadata":{},"cell_type":"markdown","source":"## Our goal"},{"metadata":{},"cell_type":"markdown","source":"We wanted to create a model that would be able to judge the final result of the match from the match statistics from the 10th minute of the game. As data, we used 90 features 45 per team."},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm_notebook\nimport sklearn.preprocessing\n\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\n\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import RidgeClassifier, LogisticRegression\nfrom sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\nfrom catboost import CatBoostClassifier\nfrom lightgbm  import LGBMClassifier\n\nfrom skopt import BayesSearchCV\n\nfrom mlxtend.classifier import StackingClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data\n"},{"metadata":{},"cell_type":"markdown","source":"## Data collecting"},{"metadata":{},"cell_type":"markdown","source":"We collected data using a hand-written Python class that creates datasets using the [API](https://developer.riotgames.com/) provided by Riot Games. More info about data collecting proccess in the rest of the repository."},{"metadata":{},"cell_type":"markdown","source":"### Getting data"},{"metadata":{},"cell_type":"markdown","source":"We collected 5 datasets with information about games from 5 leagues: bronze, silver, gold, platinum and diamond.\nIt turned out that there is huge problem with that datasets because data from different lines of the game is mixed.\nWe tried to restore true data in following \"best\" datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_bronze = pd.read_csv(\"/kaggle/input/ml-project-data/eun1_BRONZE_RANKED_SOLO_CLEAN.csv\")\ndf_silver = pd.read_csv(\"/kaggle/input/ml-project-data/eun1_SILVER_RANKED_SOLO_CLEAN.csv\")\ndf_gold = pd.read_csv(\"/kaggle/input/ml-project-data/eun1_GOLD_RANKED_SOLO_CLEAN.csv\")\ndf_platinum = pd.read_csv(\"/kaggle/input/ml-project-data/eun1_PLATINUM_RANKED_SOLO_CLEAN.csv\")\ndf_diamond = pd.read_csv(\"/kaggle/input/ml-project-data/eun1_DIAMOND_RANKED_SOLO_CLEAN.csv\")\n\n\ndf_bronze_best = pd.read_csv(\"/kaggle/input/ml-project-data/eun1_BRONZE_RANKED_SOLO_BEST.csv\")\ndf_silver_best = pd.read_csv(\"/kaggle/input/ml-project-data/eun1_SILVER_RANKED_SOLO_BEST.csv\")\ndf_gold_best = pd.read_csv(\"/kaggle/input/ml-project-data/eun1_GOLD_RANKED_SOLO_BEST.csv\")\ndf_platinum_best = pd.read_csv(\"/kaggle/input/ml-project-data/eun1_PLATINUM_RANKED_SOLO_BEST.csv\")\ndf_diamond_best = pd.read_csv(\"/kaggle/input/ml-project-data/eun1_DIAMOND_RANKED_SOLO_BEST.csv\")\n\n\n\n\ndfs={\n    'bronze': df_bronze,\n    'silver': df_silver,\n    'gold': df_gold,\n    'platinum': df_platinum,\n    'diamond': df_diamond,\n}\n\ndfs_best={\n    'bronze': df_bronze_best,\n    'silver': df_silver_best,\n    'gold': df_gold_best,\n    'platinum': df_platinum_best,\n    'diamond': df_diamond_best,\n}\n\nfor name, df in dfs_best.items():\n    print(name)\n    print(len(dfs_best[name]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Each dataset contains 6-10k rows."},{"metadata":{},"cell_type":"markdown","source":"The example rows of data od diamond dataframe:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_diamond.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To \"best\" dataset we also translated champion_id to champion_name and for each champion derived its attribute. These are only categorical columns in our dataset. Second column will be the target of our classifiers."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_diamond_best.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data pre-processing"},{"metadata":{},"cell_type":"markdown","source":"We decided to use one hot encode to encode attributes of champions."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\ndef get_rid_of_categorical_columns(df):\n    champion_attribute_feats = [col for col in df.columns if col.endswith('champion_attribute')]\n    onehotencoder = OneHotEncoder()\n    attributes_encoding = onehotencoder.fit_transform(df[champion_attribute_feats]).toarray()\n    columns_names_encoding = np.repeat(champion_attribute_feats, 6) \n    categories = ['Assassin', 'Fighter', 'Mage', 'Marksman', 'Support', 'Tank']*10\n    columns_names_encoding = [col + \"_\" + cat for col, cat in zip(columns_names_encoding, categories)]\n    to_drop_feats = [col for col in df.columns if col.endswith('champion_attribute') or col.endswith('champion_name')]\n    df = df.drop(columns = to_drop_feats)\n    return pd.concat([df, pd.DataFrame(attributes_encoding, columns=columns_names_encoding,index=df.index)], axis=1)\n\nfor name, df in dfs_best.items():\n    dfs_best[name] = get_rid_of_categorical_columns(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = dfs_best['diamond']\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To leave only significant columns we delete we ones with only one values and the ones which are highly correlated to another column."},{"metadata":{"trusted":true},"cell_type":"code","source":"def drop_one_value_columns(df):\n    to_drop = df.columns[df.nunique()==1]\n    print(\"Dropping one value columns:\")\n    print(to_drop)\n    return df.drop(columns=to_drop)\n\n\ndef drop_high_correlated_columns(df, corr_threshold = 0.95):\n    df_corr = df.corr().unstack().reset_index()\n    to_drop = df_corr.iloc[:,0][(abs(df_corr.iloc[:,-1])>corr_threshold) & (df_corr.iloc[:,0] < df_corr.iloc[:,1])]\n    print(\"Dropping high correlated columns:\")\n    print(to_drop)\n    return df.drop(columns=to_drop)\n\nbool_columns = df.columns[(df.nunique()<3)]\nnumerical_columns = df.columns[(df.nunique()>=3)]\n\nfor name, df in dfs_best.items():\n    dfs_best[name] = drop_one_value_columns(df)\n    dfs_best[name] = drop_high_correlated_columns(df)\n    \nfor name, df in dfs.items():\n    dfs[name] = drop_one_value_columns(df)\n    dfs[name] = drop_high_correlated_columns(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll  present our data on histograms. There are all columns with statistics about first team which are not binary. "},{"metadata":{"trusted":true},"cell_type":"code","source":"first_team_columns  = [col for col in numerical_columns if col.startswith('1')]\ndfs['diamond'][first_team_columns].hist(bins=50, figsize=(20, 20))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the distribution of data is nice -- we don't strictly need log transformation. There are no visible outliers.\nLet's point out that the distribution of features doesn't vary on different lines (middle, top support etc.). This is concerning and it turned out that is because these data are not true.\n\nAfter our attempt to restore the true distribution of values we obtained these different results."},{"metadata":{"trusted":true},"cell_type":"code","source":"first_team_columns  = [col for col in numerical_columns if col.startswith('1')]\ndfs_best['diamond'][first_team_columns].hist(bins=50, figsize=(20, 20))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the part of preprocessing we also tried to apply some transformations to improve our dataset.\nTurned out that it doesn't have visible impact on results and in the and we used simple min-max scaler."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler, StandardScaler, PowerTransformer\nfrom sklearn.decomposition import PCA\n\ndef min_max_scaled(df, numerical_columns):\n    scaler = MinMaxScaler()\n    scaler.fit(df[numerical_columns])\n    df[numerical_columns] = scaler.transform(df[numerical_columns])\n    return df\n\ndef standard_scaled(df, numerical_columns):\n    scaler = StandardScaler()\n    scaler.fit(df[numerical_columns])\n    df[numerical_columns] = scaler.transform(df[numerical_columns])\n    return df\n\ndef power_yeo_scaled(df, numerical_columns):\n    scaler = PowerTransformer(method='yeo-johnson')\n    scaler.fit(df[numerical_columns])\n    df[numerical_columns] = scaler.transform(df[numerical_columns])\n    return df\n\ndef power_box_cox_scaled(df, numerical_columns):\n    scaler = PowerTransformer(method='box-cox')\n    scaler.fit(df[numerical_columns])\n    df[numerical_columns] = scaler.transform(df[numerical_columns])\n    return df\n\ndef PCA_scaled(df):\n    scaler = PCA()\n    scaler.fit(df.iloc[:,1:])\n    df.iloc[:,1:] = scaler.transform(df.iloc[:,1:])\n    return df\n\ndef PCA_reduced(df, n_components):\n    scaler = PCA(n_components = n_components)\n    scaler.fit(df.iloc[:,1:])\n    print(scaler)\n    return pd.concat([df.iloc[:,0], pd.DataFrame(scaler.transform(df.iloc[:,1:]))], axis=1)\n\n\nfor name, df in dfs_best.items():\n    dfs_best[name] = min_max_scaled(df, numerical_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfs_best['diamond'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Early results"},{"metadata":{"trusted":true},"cell_type":"code","source":"clfs = {\n    'SVC': SVC(),\n    'Logistic Regression': LogisticRegression(),\n    'Ridge Regression': RidgeClassifier(),\n    'Random Forest': RandomForestClassifier(),\n    'XGBoost': XGBClassifier(use_label_encoder=False),\n    'AdaBoost': AdaBoostClassifier(),\n    'Bernouli Naive Bayes': BernoulliNB(),\n    'Gaussian Naive Bayes': GaussianNB(),\n    'KNN': KNeighborsClassifier(),\n    'Simple Neural Network': MLPClassifier(),\n    'LGBM': LGBMClassifier(),\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We wanted to try our data on few pure classifiers with defalut settings.\nHere are our early results:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\nfrom sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n\ndef evaluate_model(model, X, y):\n    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=1)\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, verbose=1, n_jobs=3, error_score='raise')\n    return scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"df_scores = pd.DataFrame(None, columns=['league', 'model', 'unshuffled', 'accuracy'])\n\nfor league, df in dfs.items():\n    X = df.iloc[:,2:]\n    y = df.iloc[:,1]\n    model_scores = defaultdict()\n    for name, model in clfs.items():\n        print('Evaluating {}'.format(name))\n        scores = evaluate_model(model, X, y)\n        model_scores[name] = scores\n    df_temp = pd.DataFrame.from_dict(model_scores)\n    df_temp = pd.melt(df_temp)\n    df_temp.columns = ['model','accuracy']\n    df_temp['league'] = league\n    df_temp['unshuffled'] = False\n    df_scores = pd.concat([df_scores, df_temp])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for league, df in dfs_best.items():\n    X = df.iloc[:,2:]\n    y = df.iloc[:,1]\n    model_scores = defaultdict()\n    for name, model in clfs.items():\n        print('Evaluating {}'.format(name))\n        scores = evaluate_model(model, X, y)\n        model_scores[name] = scores\n    df_temp = pd.DataFrame.from_dict(model_scores)\n    df_temp = pd.melt(df_temp)\n    df_temp.columns = ['model','accuracy']\n    df_temp['league'] = league\n    df_temp['unshuffled'] = True\n    df_scores = pd.concat([df_scores, df_temp])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is shown the performence of different clasifiers on raw and restored dataset (diamond)."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.dpi'] = 100\nsns.set_style(\"whitegrid\")\nfilter_diamond = df_scores['league'] == 'diamond'\nsns.catplot(x=\"model\", y=\"accuracy\", hue='unshuffled',\n            data=df_scores[filter_diamond], kind=\"swarm\", legend_out=True, s = 3)\nplt.xticks(rotation=70)\nplt.title('Performance of Different Models Using 5-Fold Cross-Validation, diamond league')\n#plt.legend(bbox_to_anchor=(1.01, 1),borderaxespad=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The comparizon of accuracy of all these classifiers on different leagues and unshuffled dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"filter_best_df = df_scores['unshuffled'] == True\nplt.rcParams['figure.dpi'] = 100\nsns.catplot(x=\"model\", y=\"accuracy\", row='league', data=df_scores[filter_best_df], kind=\"swarm\")\nplt.xticks(rotation=70)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that on this stage best performing methods are Rigde regression and logistic regression.\nThe best results are obtained on bronze league -- up to 72,5% accuracy. The hardest league to predict is diamond -- we can only get 70% accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"filter_best_df_diamond = (df_scores['unshuffled'] == True) & (df_scores['league'] == 'diamond')\ndf_temp = df_scores[filter_best_df_diamond].groupby(by='model')['accuracy'].apply(list)\nmodel_scores = dict(zip(df_temp.index, df_temp.values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\n\ndef plot_results(model_scores, name):\n    \n    model_names = list(model_scores.keys())\n    results = [model_scores[model] for model in model_names]\n    fig = go.Figure()\n    for model, result in zip(model_names, results):\n        fig.add_trace(go.Box(\n            y=result,\n            name=model,\n            boxpoints='all',\n            jitter=0.5,\n            whiskerwidth=0.2,\n            marker_size=2,\n            line_width=1)\n        )\n    \n    fig.update_layout(\n    title='Performance of Different Models Using 5-Fold Cross-Validation',\n    paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)',\n    xaxis_title='Model',\n    yaxis_title='Accuracy',\n    showlegend=False)\n    fig.show()\n    \nplot_results(model_scores, name='base_models_cv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also wanted to know if the data from different leagues differs a lot. We tried to train model on the data of one league and predict on other. We use Ridge classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_results = pd.DataFrame(None, columns=['train_league', 'test_league', 'accuracy'])\nmodel = RidgeClassifier()\n\n\nfor train_league in dfs_best.keys():\n    \n    df = dfs_best[train_league]\n    X = df.iloc[:,2:]\n    y = df.iloc[:,1]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    model_scores = defaultdict()\n    model = RidgeClassifier()\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    acc = accuracy_score(y_test, predictions)\n    model_scores[train_league] = acc\n\n    for league, df_league in dfs_best.items():\n        if(league != train_league):\n            X = df_league.iloc[:,2:]\n            y = df_league.iloc[:,1]\n            predictions = model.predict(X)\n            acc = accuracy_score(y, predictions)\n            model_scores[league] = acc\n\n    df_temp = pd.DataFrame.from_dict([model_scores])\n    df_temp = pd.melt(df_temp)\n    df_temp.columns = ['test_league','accuracy']\n    df_temp['train_league'] = train_league\n    df_temp\n    df_results = pd.concat([df_results, df_temp])\n    \ndf_results.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy doesn't differ a lot. Using platinum as training set gives the best results."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.dpi'] = 100\nsns.catplot(x=\"test_league\", y=\"accuracy\", col='train_league', data=df_results, kind=\"point\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The easiest to predict is bronze league."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x=\"train_league\", y=\"accuracy\", col='test_league', data=df_results, kind=\"point\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = dfs_best['bronze']\ndf = drop_one_value_columns(df)\ndf = get_rid_of_categorical_columns(df)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import feature_selection\n\ndef enc_attrs(series):\n    if type(series) != 'str':\n        return series\n    dic = series.values\n    return pd.Series(attr_enc[x] for x in series)\n\n# X = df_attrs.iloc[:,2:][filter(lambda x: x.endswith(\"attribute\"),df_attrs.columns)].apply(enc_attrs)\ncolumns = list(filter(lambda x: x.startswith(\"1\"), df.columns[2:]))\n#columns = df.columns[2:]\nX = df[columns]\n#X = df.iloc[:,2:]\nY = df.iloc[:,1]\nfs = feature_selection.SelectKBest(feature_selection.f_classif, k=30)\nfs.fit_transform(X,Y)\nprs = list(zip(columns,fs.scores_))\n# prs.sort(reverse = True,key = lambda x : x[1])\n# for p in prs:\n#     print(f\"feature: {p[0]}, score: {p[1]}\")\n    \ndf_feature_importance = pd.DataFrame(None)\ndf_feature_importance['feature'] = columns\ndf_feature_importance['score'] = fs.scores_\ndf_feature_importance.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style(\"whitegrid\")\nplt.rcParams['figure.dpi'] = 150\nplt.rcParams['figure.figsize'] = [10, 5]\n\ndf_feature_importance = df_feature_importance.sort_values(by='score', ascending = False)\n\nax = sns.barplot(x='score', y='feature', data=df_feature_importance[:20])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df_diamond_best.copy()\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"first_gold_columns = [col for col in df.columns if col.startswith('1') and 'gold' in col]\nsecond_gold_columns = [col for col in df.columns if col.startswith('2') and 'gold' in col]\ndf['1_mean_gold'] = df[first_gold_columns].apply(np.mean, axis=1)\ndf['1_gold_std'] = df[first_gold_columns].apply(np.std, axis=1)\ndf['2_mean_gold'] = df[second_gold_columns].apply(np.mean, axis=1)\ndf['2_gold_std'] = df[second_gold_columns].apply(np.std, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = get_rid_of_categorical_columns(df)\ndf = min_max_scaled(df, numerical_columns)\ndf = drop_high_correlated_columns(df)\ndf = drop_one_value_columns(df)\n\ncolumns = list(filter(lambda x: x.startswith(\"1\"), df.columns[2:]))\n#columns = df.columns[2:]\nX = df[columns]\n#X = df.iloc[:,2:]\nY = df.iloc[:,1]\nfs = feature_selection.SelectKBest(feature_selection.f_classif, k=30)\nfs.fit_transform(X,Y)\nprs = list(zip(columns,fs.scores_))\n# prs.sort(reverse = True,key = lambda x : x[1])\n# for p in prs:\n#     print(f\"feature: {p[0]}, score: {p[1]}\")\n    \ndf_feature_importance = pd.DataFrame(None)\ndf_feature_importance['feature'] = columns\ndf_feature_importance['score'] = fs.scores_\ndf_feature_importance = df_feature_importance.sort_values(by='score', ascending = False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feature_importance[:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## On kaggle dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_kaggle = pd.read_csv(\"/kaggle/input/league-of-legends-diamond-ranked-games-10-min/high_diamond_ranked_10min.csv\")\ndf_kaggle.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = list(filter(lambda x: x.startswith(\"red\"), df_kaggle.columns[2:]))\nX = df_kaggle[columns]\nY = df_kaggle.iloc[:,1]\nfs = feature_selection.SelectKBest(feature_selection.f_classif, k=15)\nfs.fit_transform(X,Y)\nprs = list(zip(columns,fs.scores_))\n# prs.sort(reverse = True,key = lambda x : x[1])\n# for p in prs:\n#     print(f\"feature: {p[0]}, score: {p[1]}\")\n    \ndf_feature_importance = pd.DataFrame(None)\ndf_feature_importance['feature'] = columns\ndf_feature_importance['score'] = fs.scores_\ndf_feature_importance.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style(\"whitegrid\")\nplt.rcParams['figure.dpi'] = 150\nplt.rcParams['figure.figsize'] = [10, 5]\n\ndf_feature_importance = df_feature_importance.sort_values(by='score', ascending = False)\n\nax = sns.barplot(x='score', y='feature', data=df_feature_importance[:15])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}