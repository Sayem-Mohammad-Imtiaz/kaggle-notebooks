{"cells":[{"metadata":{},"cell_type":"markdown","source":"<center>\n    \n![](https://kritikalsolutions.com/wp-content/uploads/2019/10/ml-making-iot-banner-.jpg)\n\n\n</center>"},{"metadata":{},"cell_type":"markdown","source":"<h1><center>HR ANALYTICS</center></h1>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction import DictVectorizer as DV\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom catboost import CatBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import linear_model\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn import tree\nfrom sklearn import svm\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import r2_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1><center>Mother of Algorithms</center></h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def AlgoFun (X_train, Y_train, X_test, Y_test, Algo, Result):\n    \n    if (Algo == 'LOG'):\n        log_reg=LogisticRegression(C=1000,max_iter=50000)\n        log_reg.fit(X_train, Y_train)\n        Y_pred = log_reg.predict(X_test)\n        \n    elif (Algo == 'LIN'):\n        model = linear_model.LinearRegression()\n        model.fit(X_train, Y_train)\n        Y_pred = model.predict(X_test)\n        \n    elif (Algo == 'KNN'):\n        KNN=KNeighborsClassifier(n_neighbors=20)\n        KNN.fit(X_train, Y_train)\n        Y_pred=KNN.predict(X_test)\n\n    elif (Algo == 'RFC'):\n        Clf =  RandomForestClassifier(n_estimators = 500, n_jobs = -1)\n        Clf.fit(X_train, Y_train)\n        Y_pred=Clf.predict(X_test) \n        compare1 = pd.DataFrame()\n        compare1[0] = Clf.feature_importances_\n        compare1[1] = X_test.columns\n        print('Feature importance: ')\n        print(compare1.sort_values(by=0,ascending= False))\n        \n    elif (Algo == 'NN'):\n        NN = MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1, max_iter=1000)\n        NN.fit(X_train, Y_train)\n        Y_pred = NN.predict(X_test)\n        \n    elif (Algo == 'DTR'):\n        DTR = tree.DecisionTreeClassifier()\n        DTR.fit(X_train, Y_train)\n        Y_pred=DTR.predict(X_test)\n        \n    elif (Algo == 'GSCV'):\n        estimator = RandomForestRegressor(random_state = 42,criterion='mse')\n        para_grids = {\n                    \"n_estimators\" : [10,50,100],\n                    \"max_features\" : [\"auto\", \"log2\", \"sqrt\"],\n                    'max_depth' : [4,5,6,7,8,9,15],\n                    \"bootstrap\"    : [True, False]\n                }\n        Grid = GridSearchCV(estimator, para_grids,cv= 5)\n        Grid.fit(X_train, Y_train)\n        best_param = Grid.best_estimator_\n        print(best_param)\n        Y_pred = best_param.predict(X_test)\n\n        \n    elif (Algo == 'RFR'):\n        # Using the best model from Grid Serach CV\n        model = RandomForestRegressor(max_depth=15, random_state=42) \n        model.fit(X_train, Y_train)\n        Y_pred = model.predict(X_test)\n      \n    elif (Algo == 'GBR'):   \n        GBR = GradientBoostingRegressor(n_estimators=100, max_depth=4)\n        GBR.fit(X_train, Y_train)\n        Y_pred = GBR.predict(X_test)\n        \n    elif (Algo == 'GNB'):   \n        GNB = GaussianNB()\n        GNB.fit(X_train, Y_train)\n        Y_pred = GNB.predict(X_test)\n      \n    elif (Algo == 'ADA'):\n        model = AdaBoostRegressor(random_state=0, n_estimators=100)\n        model.fit(X_train, Y_train)\n        Y_pred = model.predict(X_test)\n        \n    elif (Algo == 'XGB'):\n        model = XGBRegressor()\n        model.fit(X_train, Y_train)\n        Y_pred = model.predict(X_test)\n        \n    elif (Algo == 'LGB'):\n        model = LGBMClassifier(objective='multiclass', random_state=5)\n        model.fit(X_train, Y_train)\n        Y_pred = model.predict(X_test)        \n   \n    elif (Algo == 'CAT'):\n        Cat = CatBoostClassifier(silent = True)\n        details = Cat.fit(X_train, Y_train)\n        Y_pred = Cat.predict(X_test)\n        \n    elif (Algo == 'SVM'):\n        model = svm.SVC(kernel='linear') # Linear Kernel\n        model.fit(X_train, Y_train)\n        Y_pred = model.predict(X_test)       \n            \n    else:\n        print(\"Wrong Algo\")\n        \n    Y_test=pd.DataFrame(Y_test).iloc[:, [0]].to_numpy()\n    Y_pred=pd.DataFrame(Y_pred).iloc[:, [0]].to_numpy()\n    \n    ActVPred = pd.DataFrame({'Actual': Y_test[:,0], 'Predicted': Y_pred[:,0]})\n    print(ActVPred)\n\n    #Checking the accuracy\n    print('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, Y_pred))\n    print('Mean Squared Error:', metrics.mean_squared_error(Y_test, Y_pred))\n    print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, Y_pred)))\n    print('Model R2 scores: ', r2_score(Y_test, Y_pred))\n\n    if (Result == 'Binary'):\n        Count_row = []\n        Visual_rep = []\n\n        index = 0\n        for i, row in ActVPred.iterrows():\n            if (row['Predicted'] < 0.5):\n                Visual_rep.append(0)\n            else:\n                Visual_rep.append(1)\n\n            if (row['Actual'] < 1):\n                if (row['Predicted'] < 0.5):\n                    Count_row.append(1)\n                else:\n                    Count_row.append(0)\n            else:\n                if (row['Predicted'] >= 0.5):\n                    Count_row.append(1)\n                else:\n                    Count_row.append(0)\n            index = index + 1\n\n        print('--------------------------------------------------------------------------')\n        print(Algo)\n        print('Model accruracy scores: {:.3f}'.format(Count_row.count(1)/index))\n   \n        ax = plt.subplots(figsize=(10, 10))\n        ax = sns.heatmap(confusion_matrix(Visual_rep,Y_test),annot=True,cmap='coolwarm',fmt='d')\n        ax.set_title('Prediction vs Original Data (Confusion Matrix)',fontsize=18)\n        ax.set_xticklabels(['Actual 0','Actual 1'],fontsize=18)\n        ax.set_yticklabels(['Predicted 0','Predicted 1'],fontsize=18)\n\n        filename = 'ConfusionMatrix.jpg'\n        plt.savefig(Algo + \" \" + filename)\n        plt.show()\n    \n    elif (Result == 'Analog'):\n        fig, ax = plt.subplots()\n        minimum = min (Y_test.min(), Y_pred.min())\n        maximum = max (Y_test.max(), Y_pred.max())\n        ax.scatter(Y_test, Y_pred)\n        ax.plot([minimum, maximum], [minimum, maximum], 'k--', lw=4)\n        ax.set_xlabel('Measured')\n        ax.set_ylabel('Predicted')\n        filename = 'Result Plot.jpg'\n        plt.savefig(Algo + \" \" + filename)\n        plt.show()\n    \n    else:\n        print(\"Wrong parameter\")\n\n    return (ActVPred)\n\ndef Data_clean_func (Data, Drop_col, Date_col, Y_column, Data_Variable_fill=True, Encoder = 'OHE'):\n\n    Data = Data.drop(Drop_col, axis = 1)\n\n    for col in Date_col:\n        Data[col + '_year'] = pd.DatetimeIndex(Data[col]).year\n        Data[col + '_month'] = pd.DatetimeIndex(Data[col]).month\n        Data[col + '_day'] = pd.DatetimeIndex(Data[col]).day\n        Data.drop(col, axis=1, inplace=True)\n\n    Columns_integer_data = Data.describe().columns\n    Columns_variable_data = Data.columns[~Data.columns.isin(Data.describe().columns)]\n    Data1=Data[Columns_integer_data]\n    Data1=Data1.fillna(Data1.mean())\n\n    for col in Date_col:\n        Data1[col + '_year'] = Data1[col + '_year'].astype('int32')\n        Data1[col + '_month'] = Data1[col + '_month'].astype('int32')\n        Data1[col + '_day'] = Data1[col + '_day'].astype('int32')\n\n    if Data_Variable_fill == True:\n        for col in Columns_variable_data:\n            Data[col] = Data[col].fillna(Data[col].value_counts().index[0])\n\n    Result = pd.concat([Data1, Data[Columns_variable_data]], axis=1)\n    Result = Result.dropna()\n    Columns_integer_data = Data.describe().columns\n    Columns_variable_data = Data.columns[~Data.columns.isin(Data.describe().columns)]\n\n    if Encoder == 'Label':\n        labelencoder = LabelEncoder()\n        for col in Columns_variable_data:\n            Result[col] = labelencoder.fit_transform(Result[col])\n    elif Encoder == 'OHE':\n        Result = pd.get_dummies(Result, prefix_sep='_', drop_first=True)\n    elif Encoder == 'Dict_Vect':\n        vectorizer = DV(sparse = False)\n        Result1 = Result[Columns_variable_data].T.to_dict().values()\n        Result1 = pd.DataFrame(vectorizer.fit_transform(Result1))\n        Result = pd.concat([Result[Columns_integer_data], Result1], axis=1)\n    else:\n        print('Wrong Encoder option')\n\n\n    X_column = Result.columns[~Result.columns.isin(Y_column)]\n    X_train, X_test, Y_train, Y_test  = train_test_split(Result[X_column], Result[Y_column], test_size=0.3,random_state=42)\n    \n    return X_train, X_test, Y_train, Y_test\n\ndef Data_clean_func1 (Data, Drop_col, Date_col, Y_column, Data_Variable_fill=True, Encoder = 'OHE'):\n\n    Data = Data.drop(Drop_col, axis = 1)\n\n    for col in Date_col:\n        Data[col + '_year'] = pd.DatetimeIndex(Data[col]).year\n        Data[col + '_month'] = pd.DatetimeIndex(Data[col]).month\n        Data[col + '_day'] = pd.DatetimeIndex(Data[col]).day\n        Data.drop(col, axis=1, inplace=True)\n\n    Columns_integer_data = Data.describe().columns\n    Columns_variable_data = Data.columns[~Data.columns.isin(Data.describe().columns)]\n    Data1=Data[Columns_integer_data]\n    Data1=Data1.fillna(Data1.mean())\n\n    for col in Date_col:\n        Data1[col + '_year'] = Data1[col + '_year'].astype('int32')\n        Data1[col + '_month'] = Data1[col + '_month'].astype('int32')\n        Data1[col + '_day'] = Data1[col + '_day'].astype('int32')\n\n    if Data_Variable_fill == True:\n        for col in Columns_variable_data:\n            Data[col] = Data[col].fillna(Data[col].value_counts().index[0])\n\n    Result = pd.concat([Data1, Data[Columns_variable_data]], axis=1)\n    Result = Result.dropna()\n    Columns_integer_data = Data.describe().columns\n    Columns_variable_data = Data.columns[~Data.columns.isin(Data.describe().columns)]\n\n    if Encoder == 'Label':\n        labelencoder = LabelEncoder()\n        for col in Columns_variable_data:\n            Result[col] = labelencoder.fit_transform(Result[col])\n    elif Encoder == 'OHE':\n        Result = pd.get_dummies(Result, prefix_sep='_', drop_first=True)\n    elif Encoder == 'Dict_Vect':\n        vectorizer = DV(sparse = False)\n        Result1 = Result[Columns_variable_data].T.to_dict().values()\n        Result1 = pd.DataFrame(vectorizer.fit_transform(Result1))\n        Result = pd.concat([Result[Columns_integer_data], Result1], axis=1)\n    else:\n        print('Wrong Encoder option')\n\n\n    X_column = Result.columns[~Result.columns.isin(Y_column)]\n    \n    X_data = Result[X_column]\n    Y_data = Result[Y_column]\n    \n    return X_data, Y_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1><center>Training Data Load</center></h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"Data = pd.read_csv('../input/hr-analytics-job-change-of-data-scientists/aug_train.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1><center>Data Check</center></h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1><center>Clean Data</center></h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"Drop_col = ['enrollee_id']\nDate_col = []\nY_column = ['target']\n\nX_train, X_test, Y_train, Y_test = Data_clean_func(Data,Drop_col,Date_col,Y_column,True,'Dict_Vect')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1><center>Random Forest Regression </center></h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"result = AlgoFun(X_train, Y_train, X_test, Y_test,'RFR', 'Binary')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1><center>Random Forest Classifier</center></h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"result = AlgoFun(X_train, Y_train, X_test, Y_test,'RFC', 'Binary')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1><center>XG Boost</center></h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"result = AlgoFun(X_train, Y_train, X_test, Y_test,'XGB', 'Binary')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1><center>Linear Regression</center></h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"result = AlgoFun(X_train, Y_train, X_test, Y_test,'LIN', 'Binary')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1><center>CAT Boost</center></h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"result = AlgoFun(X_train, Y_train, X_test, Y_test,'CAT', 'Binary')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Drop_col = ['enrollee_id']\nDate_col = []\nY_column = ['target']\n\nData = pd.read_csv('../input/hr-analytics-job-change-of-data-scientists/aug_train.csv')\nX_train, Y_train = Data_clean_func1(Data,Drop_col,Date_col,Y_column,True,'Dict_Vect')\n\nY_column = []\nData = pd.read_csv('../input/hr-analytics-job-change-of-data-scientists/aug_test.csv')\nX_test = Data_clean_func1(Data,Drop_col,Date_col,Y_column,True,'Dict_Vect')\n  \nCat = CatBoostClassifier(silent = True)\ndetails = Cat.fit(X_train, Y_train)\nY_pred = Cat.predict(X_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}