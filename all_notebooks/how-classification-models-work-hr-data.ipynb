{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"text-align:center;\">WHO IS GOING TO SWITCH THE JOB</h1>"},{"metadata":{},"cell_type":"markdown","source":"## PIPELINES\nTo solve any data science related problem we have to follow some of the procedures to get the better results.so we will look everything step by step.\n1. FRAME THE PROBLEM\n2. DATA COLLECTION\n3. DATA CLEANING AND PROCESSING\n4. EXPLORATORY DATA ANALYSIS\n5. MODEL SELECTION"},{"metadata":{},"cell_type":"markdown","source":"## 1. FRAME THE PROBLEM\nA company which is active in Big Data and Data Science wants to hire data scientists among people who successfully pass some courses which conduct by the company. Many people signup for their training. Company wants to know which of these candidates are really wants to work for the company after training or looking for a new employment because it helps to reduce the cost and time as well as the quality of training or planning the courses and categorization of candidates. Information related to demographics, education, experience are in hands from candidates signup and enrollment.\n\n#### WHAT WE HAVE TO DO\n* Predict the probability of a candidate will work for the company\n* Interpret model(s) such a way that illustrate which features affect candidate decision\n\nallright so we have frame our problem now we will move to one step ahead."},{"metadata":{},"cell_type":"markdown","source":"## 2. DATA COLLECTION\nIn this step we will collect data and know some info about data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# SOME USEFUL IMPORTS\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"AS we can see that there are three files in input data.\n1. Sample submission\n2. Aug test Data\n3. Aug train data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# coillecting data from kaggle input and checking size \ndf_train=pd.read_csv('/kaggle/input/hr-analytics-job-change-of-data-scientists/aug_train.csv')\ndf_test=pd.read_csv('/kaggle/input/hr-analytics-job-change-of-data-scientists/aug_test.csv')\nprint('SHAPE OF TRAIN DATA',df_train.shape)\nprint('SHAPE OF TRAIN DATA',df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# IN train data we have 14 columns which includes the target label\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check if our data is blanced or not\nsns.countplot(df_train['target'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"AS we can cleraly see our data is not balanced..so we will balanced it in later steps."},{"metadata":{},"cell_type":"markdown","source":"## 3. DATA CLEANING AND PROCESSING\nIN this step we will process data and clean data both on test set as well as train set parallely. Lets check for missing values in dataset.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#counting null values\nprint(df_train.isnull().sum())\nprint('\\n')\nprint(df_test.isnull().sum())\n# lets create heatmap for it.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(15, 5), sharey=True)\nfig.suptitle('CHECKING MISSING VALUES')\n\n# train data\nsns.heatmap(df_train.isnull(),cbar=False,ax=axes[0])\naxes[0].set_title('TRAIN DATA')\n\n# test data\nsns.heatmap(df_test.isnull(),cbar=False,ax=axes[1])\naxes[1].set_title('TEST DATA')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First of all we have to handle missing values...handling missing values is one of the hardest task for data scientist...their are lot of techniqye by which we can handle missing values.\n1. Imputation by mean median or mode\n2. Imutation by introducing new variable in categorical variable like gender we can fill nan value by unknown or others.\n3. by deleting the whole row or columns having higher null values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# starting with gender\nfig, axes = plt.subplots(1, 2, figsize=(15, 5), sharey=True)\n\n\nsns.countplot(df_train['gender'],hue=df_train['major_discipline'],ax=axes[0])\naxes[0].set_title('with major discipline')\n\n\nsns.countplot(df_train['gender'],hue=df_train['education_level'])\naxes[1].set_title('with education level')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"AS we can see that major discipline have some connection with gender.\n* Mostly male belongs to STEM and less Male are in humanities and NO major\n* mostly woman goes for arts \nso we will fill gender by considering these things."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['gender'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# so now we will fill nan value with male if the person major discipline is STEM,BUSINESS_DEGREE or OTHERS\ndf_train.loc[df_train[\"major_discipline\"]==\"STEM\", \"gender\"] = \"Male\"\ndf_train.loc[df_train[\"major_discipline\"]==\"Business Degree\", \"gender\"] = \"Male\"\ndf_train.loc[df_train[\"major_discipline\"]==\"Other\", \"gender\"] = \"Male\"\n\n# and for others we will fill with female\ndf_train['gender'].fillna(\"Female\",inplace=True)\n\n# for test data also\n# so now we will fill nan value with male if the person major discipline is STEM,BUSINESS_DEGREE or OTHERS\ndf_test.loc[df_test[\"major_discipline\"]==\"STEM\", \"gender\"] = \"Male\"\ndf_test.loc[df_test[\"major_discipline\"]==\"Business Degree\", \"gender\"] = \"Male\"\ndf_test.loc[df_test[\"major_discipline\"]==\"Other\", \"gender\"] = \"Male\"\n\n# and for others we will fill with female\ndf_test['gender'].fillna(\"Female\",inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets check for nan value in gender\nprint(df_train['gender'].isnull().sum())\n\nprint(df_test['gender'].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now we will fill missing values in company size\nprint(df_train['company_size'].value_counts())\n# to make our code robust we fill nan value with unknown category\ndf_train['company_size'].fillna('unknown',inplace=True)\n# for test data also\ndf_test['company_size'].fillna('unknown',inplace=True)\n# lets check for nan value now\nprint('nbull value in company size')\nprint(df_train['company_size'].isnull().sum())\nprint(df_test['company_size'].isnull().sum())\n\n\n# now we will fill missing values in company type\nprint(df_train['company_type'].value_counts())\n# to make our code robust we fill nan value with unknown category\ndf_train['company_type'].fillna('unknown',inplace=True)\n# for test data also\ndf_test['company_type'].fillna('unknown',inplace=True)\n# lets check for nan value now\nprint('nbull value in company type')\nprint(df_train['company_type'].isnull().sum())\nprint(df_test['company_type'].isnull().sum())\n\n\n# now we will fill missing values in company size\nprint(df_train['major_discipline'].value_counts())\n# to make our code robust we fill nan value with unknown category\ndf_train['major_discipline'].fillna('unknown',inplace=True)\n# for test data also\ndf_test['major_discipline'].fillna('unknown',inplace=True)\n# lets check for nan value now\nprint('nbull value in major_discipline')\nprint(df_train['major_discipline'].isnull().sum())\nprint(df_test['major_discipline'].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Now we are done with filling missing values... for others features we simply delewte those rows which have any null value associated with it."},{"metadata":{"trusted":true},"cell_type":"code","source":"# droping rest of rows as they have less data\ndf_train.dropna(inplace=True)\ndf_test.dropna(inplace=True)\nprint(df_train.isnull().sum())\nprint('\\n')\nprint(df_test.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"so we have handeled missing values no we will move to one further step."},{"metadata":{},"cell_type":"markdown","source":"## 3. EXPLORATORY DATA ANALYSIS\nIn this step we will learn about and go into deep dive into data with manu visulization tools."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(3, 3, figsize=(25, 15), sharey=True)\n\n\nsns.countplot(df_train['gender'],hue=df_train['target'],ax=axes[0,0])\n\n\nsns.countplot(df_train['relevent_experience'],hue=df_train['target'],ax=axes[0,1])\n\n\nsns.countplot(df_train['enrolled_university'],hue=df_train['target'],ax=axes[0,2])\n\n\nsns.countplot(df_train['experience'],hue=df_train['target'],ax=axes[1,0])\n\n\nsns.countplot(df_train['company_size'],hue=df_train['target'],ax=axes[1,1])\n\n\nsns.countplot(df_train['company_type'],hue=df_train['target'],ax=axes[1,2])\n\n\nsns.countplot(df_train['last_new_job'],hue=df_train['target'],ax=axes[2,0])\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see in experience columns that there are values in ranges so we have to fill appropriately."},{"metadata":{"trusted":true},"cell_type":"code","source":"#replacing <1 with 0 and >20 with 21\ndf_train['experience'].replace({'<1':'0','>20':'21'},inplace=True)\ndf_train['experience'].value_counts()\ndf_train['experience'] = df_train.experience.astype(int)# converting its datatypes from object to integer\n\ndf_test['experience'].replace({'<1':'0','>20':'21'},inplace=True)\ndf_test['experience'].value_counts()\ndf_test['experience'] = df_test.experience.astype(int)# converting its datatypes from object to integer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"# similarly doing this thing for last new job columns\ndf_train['last_new_job'].replace({'never':'0','>4':'5'},inplace=True)\ndf_train['last_new_job'].value_counts()\ndf_train['last_new_job'] = df_train.last_new_job.astype(int)# converting its datatypes from object to integer\n\ndf_test['last_new_job'].replace({'never':'0','>4':'5'},inplace=True)\ndf_test['last_new_job'].value_counts()\ndf_test['last_new_job'] = df_test.last_new_job.astype(int)# converting its datatypes from object to integer\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay! everything is good till here.lets check our dataset again."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"NOW we will split our target variable from train data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train=df_train.drop(['target','enrollee_id'],axis=1)\nY_train=df_train['target']\nX_test=df_test.drop(['enrollee_id'],axis=1)\nprint(X_train.shape)\nprint(Y_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before moving ahead we will combine test data and train data together so that we can do one hot encoding without loosing any feature.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# combining two data frame and making on dataframe to do encoding\nldf=pd.concat([X_train,X_test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now we will do one hot encoding\nldf=pd.get_dummies(ldf)\nldf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now we will seperate training data and test data..as we already know that upto 18014 index we have train_data\nX_train=ldf.iloc[:18014]\nX_test=ldf.iloc[18014:]\nprint(X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_Val, Y_train, Y_Val = train_test_split(X_train, Y_train, test_size=0.3,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score,confusion_matrix,f1_score\nfrom sklearn.model_selection import RandomizedSearchCV\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creation of model\nmodel=RandomForestClassifier(n_estimators=100)\nmodel.fit(X_train,Y_train)\nY_pred=model.predict(X_Val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets check the accuracy of our model\nprint('accuracy of model is',accuracy_score(Y_Val, Y_pred))\nprint('confusion matrix',confusion_matrix(Y_Val, Y_pred))\nprint('F1-Score of model is',f1_score(Y_Val, Y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"AS you can see our accuracy is average but f1-score is very poor..we have to increase f1 score of model.\nNow we will tune the parameter to check if model performs better or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"param={'bootstrap': [True, False],\n 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n 'max_features': ['auto', 'sqrt'],\n 'min_samples_leaf': [1, 2, 4],\n 'min_samples_split': [2, 5, 10],\n 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#rf=RandomForestClassifier()\n#random_search= RandomizedSearchCV(estimator = rf, param_distributions = param, \n#                         cv = 3, n_jobs = -1, verbose = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#random_search.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#random_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=RandomForestClassifier(n_estimators=2000,min_samples_split=2,min_samples_leaf=2,max_features='sqrt',max_depth=70,bootstrap=False)\nmodel.fit(X_train,Y_train)\nY_pred=model.predict(X_Val)\n#lets check the accuracy of our model now\nprint('accuracy of model is',accuracy_score(Y_Val, Y_pred))\nprint('confusion matrix',confusion_matrix(Y_Val, Y_pred))\nprint('F1-Score of model is',f1_score(Y_Val, Y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our model accuracy have little increased... it can still be increased by various tecnique such as:\n1. Applying some more algorithms who knows may knn gives better accuracy\n2. or reducing our model complexity..like principle component analyis..feature reduction\n3. more hyperParameter tuning."},{"metadata":{},"cell_type":"markdown","source":"**SO WE ARE DONE WITH THIS> IF YOU LIKE THIS NOTEBOOK PLEASE GIVE AN UPVOTE. IF ANY SUGGESTIONS COMMENT DOWN.**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}