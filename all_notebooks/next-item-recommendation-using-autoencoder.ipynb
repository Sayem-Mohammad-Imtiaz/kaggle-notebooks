{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebook, I will present how a simple AutoEncoder recommends the next item for the given basket. I chose this dataset for this tutorial because the dataset is small enough to implement our recommendation system quickly.\n\n### Before start\n- First of all, I really appreciate [@Aditya Mittal](https://www.kaggle.com/mittalvasu95) providing this dataset.\n- I am sorry for my poor English in advance."},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 1234\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/the-bread-basket/bread basket.csv')\nprint(\"The shape of df: \", df.shape)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split data into train, validation and test\n- There are 9,465 transactions\n- For this kind of data, we shouldn't split data randomly because we want to predict \"future\" transactions when \"past\" transactions are given.\n- Let's use the last 1,000 transactions as test data and last 1,000 transaction of remaining transactions as validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The number of unique transactions: \", df['Transaction'].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['dataset'] = 'train'\ndf.loc[df['Transaction'].isin(df['Transaction'].unique()[-1000:]), 'dataset'] = 'test'\ndf.loc[df['Transaction'].isin(df['Transaction'].unique()[-2000:-1000]), 'dataset'] = 'valid'\n\nprint(\"The number of train transactions: \", df.loc[df['dataset'] == 'train', 'Transaction'].nunique())\nprint(\"The number of validation transactions: \", df.loc[df['dataset'] == 'valid', 'Transaction'].nunique())\nprint(\"The number of test transactions: \", df.loc[df['dataset'] == 'test', 'Transaction'].nunique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Apply label encoding to `Item`\n- There are many ways to implement label-encoding. Among them, I use `pandas.Categorical`"},{"metadata":{"trusted":true},"cell_type":"code","source":"label_encoder = pd.Categorical(df['Item'])\nlabel_encoder = {k: v for v, k in enumerate(label_encoder.categories)}\ndf['Item_encoded'] = df['Item'].apply(lambda x: label_encoder[x])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- (optional) It is helpful to print `label_encoder` for understanding\n\n~~~python\nprint(label_encoder)\n~~~"},{"metadata":{},"cell_type":"markdown","source":"## Create `torch.nn.Dataset`\n- Honestly, it is not necessary to make `torch.nn.Dataset` for small dataset. (But, I'm sure it is worth using it!)"},{"metadata":{"trusted":true},"cell_type":"code","source":"class BasketDataset(Dataset):\n    def __init__(self, df, dim_input, mode):\n        super(BasketDataset, self).__init__()\n        self.df = df\n        self.dim_input = dim_input\n        self.mode = mode\n        self.indices = df['Transaction'].unique()\n        \n    def __len__(self):\n        return len(self.indices)\n        \n    def __getitem__(self, index):\n        transaction_id = self.indices[index]\n        items = df.loc[df['Transaction'] == transaction_id, 'Item_encoded'].values\n        \n        X = torch.zeros(self.dim_input, dtype=torch.float32)\n        y = torch.zeros(self.dim_input, dtype=torch.float32)\n        X[items] = 1\n        y[items] = 1\n        \n        return X, y\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- (optional) Print `X` and `y` generated by BasketDataset\n\n~~~python\ndataset = BasketDataset(df=df[df['dataset'] == 'train'],\n                        dim=94,\n                        mode='train')\n\nX, y = dataset[0]\nprint('X: ', X)\nprint('y: ', y)\n~~~"},{"metadata":{},"cell_type":"markdown","source":"## AutoEncoder\n- I use a very simple AutoEncoder, that is, it has only one hidden layer.\n- Dropout is used to only Encoder.\n- Activation functions of Encoder and Decoder are sigmoid.\n- It sounds like very poor model, but it is very powerful !"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, dim_input, dim_latent, dropout):\n        super(Encoder, self).__init__()\n        self.latent_layer = nn.Linear(dim_input, dim_latent)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        x = self.latent_layer(x)\n        x = torch.sigmoid(x)\n        x = self.dropout(x)\n        return x\n    \nclass Decoder(nn.Module):\n    def __init__(self, dim_output, dim_latent):\n        super(Decoder, self).__init__()\n        self.output_layer = nn.Linear(dim_latent, dim_output)\n        \n    def forward(self, x):\n        x = self.output_layer(x)\n        x = torch.sigmoid(x)\n        return x\n    \nclass AutoEncoder(nn.Module):\n    def __init__(self, dim_input, dim_latent, dropout):\n        super(AutoEncoder, self).__init__()\n        self.encoder = Encoder(dim_input, dim_latent, dropout)\n        self.decoder = Decoder(dim_input, dim_latent)\n    \n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- (optional) To the best of my knowledge, the original AutoEncoder shares weights of encoder and decoder, which called `Tied AutoEncoder`. Build  `TiedAutoEncoder` and compare it's performance with AutoEncoder (Honestly, I don't know how to implement `TiedAutoEncoder`T_T)."},{"metadata":{},"cell_type":"markdown","source":"## Fitter\n- Next, we will make a class that trains, evaluates, and predicts for given model and data loaders.\n- Let's make a helper class storing and averaging the losses first."},{"metadata":{"trusted":true},"cell_type":"code","source":"class AverageMeter:\n    def __init__(self):\n        self.reset()\n    \n    def reset(self):\n        self.value = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n    \n    def update(self, value, n):\n        self.value = value\n        self.sum += value * n\n        self.count += n\n        self.avg = self.sum / self.count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Fitter:\n    def __init__(self, model, lr, n_epochs):\n        self.model = model\n        self.lr = lr\n        self.n_epochs = n_epochs\n        \n        \n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.model.to(self.device)\n        \n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n        self.criterion = torch.nn.BCELoss()\n        \n        self.best_summary_loss = 10 ** 5\n        \n    def fit(self, train_loader, valid_loader):\n        for epoch in range(self.n_epochs):\n            # Train\n            t = time.time()\n            summary_loss = self.train_one_epoch(train_loader)\n            print(\n                f'\\rEpoch:{epoch + 1}/{self.n_epochs} | ' +\n                f'Train loss: {summary_loss.avg:.7f} | ' +\n                f'Elapsed time: {time.time() - t:.3f} |'\n            )\n            \n            # Evaluation\n            t = time.time()\n            summary_loss = self.evaluate(valid_loader)\n            print(\n                f'\\rEpoch:{epoch + 1}/{self.n_epochs} | ' +\n                f'Validation loss: {summary_loss.avg:.7f} | ' +\n                f'Elapsed time: {time.time() - t:.3f}'\n            )\n        # End for (n_epochs)\n        \n    def train_one_epoch(self, train_loader):\n        self.model.train()\n        summary_loss = AverageMeter()\n        t = time.time()\n        \n        for step, (X, y) in enumerate(train_loader):\n            print(\n                f'Train step: {step + 1}/{len(train_loader)} | ' +\n                f'Summary loss: {summary_loss.avg:.7f} | ' +\n                f'Time: {time.time() - t:.3f} |', end='\\r'\n            )\n            X = X.to(self.device)\n            y = y.to(self.device)\n            batch_size = X.shape[0]\n            \n            self.optimizer.zero_grad()\n            output = self.model(X)\n            loss = self.criterion(output, y)\n            loss.backward()\n            summary_loss.update(loss.detach().item(), batch_size)\n            self.optimizer.step()\n        # End for (one epoch)\n        return summary_loss\n        \n    def evaluate(self, valid_loader):\n        self.model.eval()\n        summary_loss = AverageMeter()\n        t = time.time()\n        \n        with torch.no_grad():\n            for step, (X, y) in enumerate(valid_loader):\n                print(\n                    f'Valid step: {step + 1}/{len(valid_loader)} | ' +\n                    f'Summary loss: {summary_loss.avg:.7f} | ' + \n                    f'Time: {time.time() - t:.3f} |', end='\\r'\n                )\n\n                X = X.to(self.device)\n                y = y.to(self.device)\n                batch_size = X.shape[0]\n\n                self.optimizer.zero_grad()\n                output = self.model(X)\n                loss = self.criterion(output, y)\n                summary_loss.update(loss.detach().item(), batch_size)\n            # End for (One epoch)\n        # End with (validataion)\n        return summary_loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- (optional, but necessary) You will see that our losses are very small. Why? There are many zeros in our target `y`, so that if our model predicts all values as 0 the average of losses go to 0. We can alleviate this problem by providing different weights to 0 and 1. To this end, we should make our own loss function.\n\n~~~python\ndef weighted_binary_cross_entropy(output, target, weights=None):\n    '''\n    code from https://discuss.pytorch.org/t/solved-class-weight-for-bceloss/3114/2\n    '''    \n    if weights is not None:\n        assert len(weights) == 2\n        \n        loss = weights[1] * (target * torch.log(output)) + \\\n               weights[0] * ((1 - target) * torch.log(1 - output))\n        \n    else:\n        loss = target * torch.log(output) + (1 - target) * torch.log(1 - output)\n\n    return torch.neg(torch.mean(loss))\n~~~"},{"metadata":{},"cell_type":"markdown","source":"## Let's train our model\n- Define `BasketDataset` and pass it through `torch.nn.DataLoader`\n- Define our `AutoEncoder` model\n- Combine and train our model and data loader"},{"metadata":{"trusted":true},"cell_type":"code","source":"DIM_INPUT = 94 # The number of unique items\nDIM_LATENT = 64 # The number of nodes of the latent layer\nBATCH_SIZE = 16\nDROPOUT = 0.1\nLR = 0.001\nN_EPOCHS = 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = BasketDataset(df=df[df['dataset'] == 'train'],\n                              dim_input=DIM_INPUT,\n                              mode='train')\nvalid_dataset = BasketDataset(df=df[df['dataset'] == 'valid'],\n                              dim_input=DIM_INPUT,\n                              mode='train')\n\ntrain_loader = DataLoader(train_dataset,\n                          batch_size=BATCH_SIZE,\n                          shuffle=True,\n                          drop_last=True)\n\nvalid_loader = DataLoader(valid_dataset,\n                          batch_size=BATCH_SIZE,\n                          shuffle=False,\n                          drop_last=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = AutoEncoder(DIM_INPUT, DIM_LATENT, DROPOUT)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fitter = Fitter(model, LR, N_EPOCHS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fitter.fit(train_loader, valid_loader)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Recommend items for given basket\n- I will show you some good results.\n- Note that actually our model is not good because of the following:\n    - Since almost all customers buy only 1~2 items, then our model cannot learn latent space enough.\n    - It tends to recommend popular items such as `bread` or `coffee` (due to data imbalance)"},{"metadata":{"trusted":true},"cell_type":"code","source":"label_decoder = {v: k for k, v in label_encoder.items()}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = BasketDataset(df=df[df['dataset'] == 'test'],\n                              dim_input=DIM_INPUT,\n                              mode='test')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Let's take a look the 707th transaction in the test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_id = 707\n\nX, y = test_dataset[sample_id]\n\nprint([label_decoder[item.item()] for item in torch.where(X == 1)[0]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Let's assume the customer picks from `Bread` to `Salad` only. Then our model can recommend `Spanish Brunch`?\n- To this end, create `X_denoised` basket that has no `Spanish Brunch`"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_denoised = X.clone()\nX_denoised[torch.where(X == 1)[0][-1]] = 0\n\nbasket = [label_decoder[item.item()] for item in torch.where(X_denoised == 1)[0]]\nprint(basket)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Among the model's output, `Spanish Brunch` has the maximum logit value, except for the items tht are already in the basket.\n- That is, our model recommends `Spanish Brunch` to the customer."},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cpu')\nmodel.to(device)\nmodel.eval()\n\noutput = model(X_denoised).detach().numpy()\nTopK = np.argsort(-output)[:10]\n\nprint([label_decoder[item] for item in TopK if label_decoder[item] not in basket])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The 700th transaction in the test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_id = 700\n\nX, y = test_dataset[sample_id]\nprint([label_decoder[item.item()] for item in torch.where(X == 1)[0]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_denoised = X.clone()\nX_denoised[torch.where(X == 1)[0][-1]] = 0\n\nbasket = [label_decoder[item.item()] for item in torch.where(X_denoised == 1)[0]]\nprint(basket)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cpu')\nmodel.to(device)\nmodel.eval()\n\noutput = model(X_denoised).detach().numpy()\nTopK = np.argsort(-output)[:10]\n\nprint([label_decoder[item] for item in TopK if label_decoder[item] not in basket])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## To do\n- Visualize the latent space of items. Are they clustered properly? \n- Try the Denoising AutoEncoder that masks some items of an input, but still have to reconsturct the original input. For example, let's assumt an input $X$ has items `['Coffee', 'Drinking chocolate spoons ', 'Juice', 'Mineral water', 'Salad', 'Sandwich']`. The input and output of AutoEncoder are $X$ itself. However, the Denoising AutoEncoder has to reconstruct $X$ for the given denoised input $X_{\\text{denoised}}$ whose some items are masked, for example $X_{\\text{denoised}}$=`['Coffee', 'Juice', 'Salad', 'Sandwich']`. Denoising AutoEncoder provides more robust model."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}