{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"studentVle_df1 = pd.read_csv('../input/ouladdata/studentVle_0.csv') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"studentVle_df2 = pd.read_csv('../input/ouladdata/studentVle_1.csv') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"studentVle_df3 = pd.read_csv('../input/ouladdata/studentVle_2.csv') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"studentVle_df4 = pd.read_csv('../input/ouladdata/studentVle_3.csv')  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"studentVle_df5 = pd.read_csv('../input/ouladdata/studentVle_4.csv') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"studentVle_df6 = pd.read_csv('../input/ouladdata/studentVle_5.csv') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"studentVle_df7 = pd.read_csv('../input/ouladdata/studentVle_6.csv') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"studentVle_df8 = pd.read_csv('../input/ouladdata/studentVle_7.csv') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"studentVle_df = pd.concat([studentVle_df1,studentVle_df2, studentVle_df3, studentVle_df4, studentVle_df5, studentVle_df6, studentVle_df7, studentVle_df8])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"studentInfo_df = pd.read_csv('../input/ouladdata/studentInfo.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"studentAssessment_df = pd.read_csv('../input/ouladdata/studentAssessment.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assessments_df = pd.read_csv('../input/ouladdata/assessments.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"student_info = studentInfo_df\nvle_activity = studentVle_df\nstudent_assessment = studentAssessment_df\nassessments = assessments_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we only need code_module so that we can perform a join with studenAssessment and then studentInfo tables\nassessments.drop(['code_presentation','assessment_type','date','weight'], axis = 1, inplace = True)\n#assessments.code_module.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#merge individual assessments data with assessment data, so that we know which module each assessment belongs to\ncomb_assess = pd.merge(student_assessment,assessments,on='id_assessment')\ncomb_assess.drop(['is_banked','date_submitted'],axis = 1,inplace=True)\ncomb_assess.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#group by student and then subgroup by module, so that we retain ability to evaluate different module predictability\ngrouped = comb_assess.groupby(['id_student','code_module']).mean()\ngrouped.sort_values('id_student')\n#we can't keep id_assessment, because we have just grouped by module. We have to group by module, as our vle interaction data is \n#only grouped by module\ngrouped.drop(['id_assessment'],axis=1,inplace = True)\ngrouped.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#put it all together\nstudent_all_info = pd.merge(student_info,grouped,on='id_student')\n#just getting a feel for it - how many modules is each student enrolled in?\nfig1 = student_all_info.groupby(['id_student']).code_module.count().sort_values().hist()\nfig1.set_title('Number of modules by student')\nfig1.set_xlabel('Number of modules')\nfig1.set_ylabel('Number of students')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#total number of clicks per student by module\nvle_grouped = vle_activity.groupby(['id_student','code_module']).sum()\n#we have to drop the columns below as we have grouped by student and subgrouped by module, so they are meaningless\nvle_grouped.drop(['id_site','date'],axis=1,inplace=True)\nvle_grouped.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#left join as we want to keep student info where no clicks were made\ndf = pd.merge(student_all_info,vle_grouped,on = ['id_student','code_module'],how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove rows where there are null values for sum_click. There are only 201 so it won't have a huge impact\ndf.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.code_module = pd.Categorical(df.code_module)\ndf.code_presentation = pd.Categorical(df.code_presentation)\ndf.gender = pd.Categorical(df.gender)\ndf.region = pd.Categorical(df.region)\ndf.highest_education = pd.Categorical(df.highest_education)\ndf.imd_band = pd.Categorical(df.imd_band)\ndf.age_band = pd.Categorical(df.age_band)\ndf.disability = pd.Categorical(df.disability)\ndf.final_result = pd.Categorical(df.final_result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()\ndf.dropna(inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.final_result = df.final_result.replace({'Withdrawn':0,'Fail':1,'Pass':2,'Distinction':3})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier \nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n\ndef split_data(X, Y, pct=0.2):\n    ''' \n       Divide data into training / validation set \n    '''\n   # X = X.to_numpy()\n   # samples = X.shape[0]\n   # n_train = int(samples * pct) \n   # indices = np.arange(samples)\n  #  np.random.shuffle(indices)\n\n    # Training set\n   # Xt = X[indices[:n_train]]\n   # Yt = Y[indices[:n_train]]\n\n    # Validation set\n  #  Xv = X[indices[n_train:]]\n   # Yv = Y[indices[n_train:]]\n    \n    Xt, Xv, Yt, Yv = train_test_split(X, Y, test_size=0.15)\n    return (Xt, Yt), (Xv, Yv)\n\n\ndef fit_data(X, Y, kwargs={}, Model=DecisionTreeClassifier):\n    '''\n    Fit model and test performance on a validation set\n    \n    kwargs:\n     - max_depth\n     - max_leaf_nodes\n     - min_impurity_split\n     - min_samples_split\n     - min_samples_leaf\n     - criterion\n     - class_weight\n    '''\n    training, (Xv, Yv) = split_data(X, Y)\n\n    model = Model(**kwargs).fit(*training)\n    Y_hat = model.predict(Xv)\n\n    print('Accuracy:', accuracy_score(Yv, Y_hat))\n    print('F1:', f1_score(Yv, Y_hat, average='weighted'))\n    print('Confusion Matrix:\\n', confusion_matrix(Yv, Y_hat))\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Y = df['final_result']\n#X = df.drop('final_result', axis=1)\n#X = pd.get_dummies(X)\n\n#fit_data(X, Y)\n\ndf.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in df.columns.values]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = df['final_result'].replace({1:0, 2:1, 3:1}) # Fail, Withdraw, Pass, Distinction -> Incomplete, Complete\nX = df.drop('final_result', axis=1)\nX = pd.get_dummies(X)\n\nfit_data(X, Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.metrics import make_scorer\nfrom time import time\n\n# Create a timing decorator \ndef timer(f):\n    def wrapper(*args, **kwargs):\n        start  = time()\n        result = f(*args, **kwargs)\n        print('Execution time: %.2f seconds' % (time() - start))\n        return result\n    return wrapper\n\n\n# Split data using sklearn\nXt, Xv, Yt, Yv = train_test_split(X, Y, test_size=0.2)\n\n@timer\ndef run_search_dt(n_jobs=1):\n    search = GridSearchCV(estimator = DecisionTreeClassifier(),\n                          param_grid= {'max_depth'      : np.arange(2, 11),\n                                       'max_leaf_nodes' : np.linspace(8, 100, 10, dtype=np.int32),\n                                       'criterion'      : ['gini', 'entropy'],\n                                       'class_weight'   : ['balanced', None]},\n                          scoring = make_scorer(f1_score),\n                          n_jobs  = n_jobs,\n                          cv      = 3) # 3 fold cross validation\n    search.fit(Xt, Yt)\n    display(pd.DataFrame(search.cv_results_))\n    print('Best score: %.3f' % search.best_score_)\n    return search.best_params_\n    \n    \nparams = run_search_dt()\nprint(params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = run_search_dt(4)\nfit_data(X, Y, params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nfrom xgboost.sklearn  import XGBClassifier\n#X = pd.get_dummies(X)\nimport re\n\nregex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n\nX.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X.columns.values]\ntimed_test = timer(fit_data)\n\nfor M in [AdaBoostClassifier, XGBClassifier]:\n    print(M.__name__)\n    timed_test(X, Y, Model=M)\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = fit_data(X, Y, Model=XGBClassifier)\nimportance = sorted(list(zip(X.columns, model.feature_importances_)), key=lambda k:k[1], reverse=True)\nprint('Top three features:')\ndisplay(importance[:3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimp = np.array(importance)\nimp = pd.DataFrame(imp[:,1], index=imp[:,0]).T\n\nplt.figure(figsize=(10,10));\nsns.barplot(data=imp, orient='h', palette=\"rainbow\").set_xlabel('Relative Importance %');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost.plotting import plot_tree\nplt.figure(figsize=(14,13));\nplot_tree(model, ax=plt.gca());","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import learning_curve, ShuffleSplit\n\ndef plot_learning_curve(X, Y, Model=XGBClassifier):\n    cv = ShuffleSplit(n_splits=30, test_size=0.2)\n    train_sizes = np.linspace(0.1,1,8)\n    train_sizes, train_scores, test_scores = learning_curve(Model(), \n                                                            X, Y, cv=cv, \n                                                            train_sizes=train_sizes, n_jobs=4)\n    train_mean = np.mean(train_scores, axis=1)\n    train_std  = np.std(train_scores, axis=1)\n    test_mean  = np.mean(test_scores, axis=1)\n    test_std   = np.std(test_scores, axis=1)\n\n    c1 = plt.plot(train_sizes, train_mean, 'o-', label=\"Training score\")[0].get_color()\n    c2 = plt.plot(train_sizes, test_mean, 'o-', label=\"Cross-validation score\")[0].get_color()\n\n    plt.fill_between(train_sizes, train_mean-train_std, train_mean+train_std, alpha=0.1, color=c1)\n    plt.fill_between(train_sizes, test_mean-test_std, test_mean+test_std, alpha=0.1, color=c2)\n\n    plt.legend(loc=\"best\")\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    \nplot_learning_curve(X, Y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#vle = pd.read_csv(vle_activity)\n#display(vle)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"studentVle_df['date'].hist();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s = studentVle_df.groupby(['code_module', 'code_presentation', 'id_student', 'date'])['sum_click'].sum().reset_index()\ndisplay(s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s.groupby(['date'])['sum_click'].sum().reset_index().plot(x='date', y='sum_click', color='#bcbd22');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_clicks(data, day):\n    # Get all data points on this day or prior, and sum them over the three columns we care about\n    clicks = s[s['date'] <= day].groupby(['code_module', 'code_presentation', 'id_student'])['sum_click'].sum().reset_index()\n    \n    # Merge this data with the appropriate rows in the full data set\n    full   = pd.merge(df, clicks, on=['code_module', 'code_presentation', 'id_student'], how='outer')\n    \n    # Replace any missing values with 0\n    full['sum_click'].fillna(0, inplace=True)\n    return full","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm \n\n# If multiprocessing isn't available, can still do the same work normally\nxy_performance = []\nfor day in tqdm(sorted(s['date'].unique())):\n    wclicks = add_clicks(df, day)\n   # cleaned = clean_data(wclicks)\n    \n    Y = wclicks['final_result'].replace({1:0, 2:1, 3:1}) # Fail, Withdraw, Pass, Distinction -> Incomplete, Complete\n    X = wclicks.drop('final_result', axis=1)\n    Xt, Xv, Yt, Yv = train_test_split(X, Y, test_size=0.2)\n\n    model = XGBClassifier().fit(Xt, Yt)\n    Y_hat = model.predict(Xv)\n    xy_performance.append([day, f1_score(Yv, Y_hat)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}