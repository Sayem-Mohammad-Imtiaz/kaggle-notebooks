{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# variables\n\n# paths for files\napp_path_str = \"../input/credit-card-approval-prediction/application_record.csv\"\ncredit_path_str = \"../input/credit-card-approval-prediction/credit_record.csv\"\n# For random forest, a number of trees must be selected.\n# The higher number, the more thorough the calculation, but it takes longer to run.\nnumber_of_trees = 200\n# Target column for random forest prediction\ntarget_column_name = 'high_risk'\n# Usually, decision trees can be large.  Setting this variable to 3 or 4 makes the result tree easier to see and interpret.\ntree_depth = 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Load data\n# create dataframe from data\ndf_app = pd.read_csv(app_path_str)\ndf_app.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Load data\n# create dataframe from data\n\ndf_credit = pd.read_csv(credit_path_str)\ndf_credit.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Replace C and X with 0, expanding the 0 group to 0-29 days past due, so that we have all numeric categories for delinquency status.\ndf_credit['STATUS'] = df_credit['STATUS'].replace(['X'],0)\ndf_credit['STATUS'] = df_credit['STATUS'].replace(['C'],0)\n\n# check rows,cols\ndf_app.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert status to numeric and group-max by status for each unique id.  \n# This will be a proxy for whether an applicant will be approved, since there is no yes/no flag for approved in the data set.\ndf_credit['STATUS'] = df_credit['STATUS'].apply(pd.to_numeric) \n# Select highest status, i.e. the highest level of delinquency for each customer id\ndf_credit = df_credit.groupby('ID')['STATUS'].max().reset_index()\n# export data to csv file\ndf_credit.to_csv('df_credit.csv',index=False)\n\ndf_credit.groupby('ID')['STATUS'].count().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Join grouped status table to df_app by ID \ndf_consol = pd.merge(df_app, df_credit, left_on='ID', right_on='ID')\ndf_consol.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert status to binary.  If < 1, then \ndf_consol['high_risk'] = np.where(df_consol['STATUS']<1, 0, 1)\n# convert days old to years\ndf_consol['age_years'] = round(df_consol['DAYS_BIRTH']/-365,0).astype(int)\ndf_consol['years_employed'] = round(df_consol['DAYS_EMPLOYED']/-365,0).astype(int)\n\n\ndf_consol.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Encode categorical columns\ndf_formatted = pd.get_dummies(df_consol, columns=['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', \n                                   'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'OCCUPATION_TYPE'],               \n               prefix=[\"gender\", \"own_car\", 'own_property', 'income_type', 'education', 'family_status', 'housing_type',\n                      'occupation_type'])\n# check length-rows and width-columns of data\ndf_formatted.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop columns not needed\ndf_formatted.drop(['ID'], axis=1, inplace=True)\ndf_formatted.drop(['STATUS'], axis=1, inplace=True)\ndf_formatted.drop(['DAYS_BIRTH'], axis=1, inplace=True)\ndf_formatted.drop(['DAYS_EMPLOYED'], axis=1, inplace=True)\ndf_formatted.drop(['own_car_N'], axis=1, inplace=True)\ndf_formatted.drop(['own_property_N'], axis=1, inplace=True)\n\ndf_formatted.to_csv('df_formatted.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use numpy to convert to arrays.\n# NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, \n# along with a large collection of high-level mathematical functions to operate on these arrays.\nimport numpy as np\n\n# Assign target variable to separate array\ntarget = np.array(df_formatted[target_column_name])\n\n# Remove target column from features\nfeatures = df_formatted.drop(target_column_name, axis = 1)\n\n# Saving feature names for later use\nfeature_list = list(features.columns)\n\n# convert features dataframe to array\nfeatures = np.array(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Using Skicit-learn to split data into training and testing sets.\n#  Scikit-learn (formerly scikits.learn and also known as sklearn) is a free software machine learning library for the Python programming language.\n#  It features various classification, #  regression and clustering algorithms including support vector machines, random forests, \n#  gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.\nfrom sklearn.model_selection import train_test_split\n\n# Split the data into training and testing sets.  test_size is n% of the rows. The other % will train the model.\ntrain_features, test_features, train_target, test_target = train_test_split(features, target, test_size = 0.25, random_state = 42)\n\n# Check to see that training features and labels have the same rows, and testing features and labels have the same rows\nprint('Training Features Shape:', train_features.shape)\nprint('Training target Shape:', train_target.shape)\nprint('Testing Features Shape:', test_features.shape)\nprint('Testing target Shape:', test_target.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the model we are using\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Instantiate model.  n_estimators is the number of decision trees you want to use\nrf = RandomForestRegressor(n_estimators = number_of_trees, random_state = 42)\n\n# Train the model on training data\nrf.fit(train_features, train_target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import tools needed for visualization\nfrom sklearn.tree import export_graphviz\nfrom IPython.display import Image\n# pydot may need to be installed. \ntry:\n    import pydot\nexcept ImportError as e:\n    !pip install pydot\n    import pydot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Limit depth of tree to n levels\nrf_small = RandomForestRegressor(n_estimators=10, max_depth = tree_depth)\nrf_small.fit(train_features, train_target)\n# Extract the small tree\ntree_small = rf_small.estimators_[5]\n# Save the tree as a png image\nexport_graphviz(tree_small, out_file = 'small_tree.dot', feature_names = feature_list, rounded = True, precision = 1)\n(graph, ) = pydot.graph_from_dot_file('small_tree.dot')\ngraph.write_png('small_tree.png')\n# show png file\nImage(graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get numerical feature importances\nimportances = list(rf.feature_importances_)\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfcorr = df_formatted[['AMT_INCOME_TOTAL','age_years','years_employed', 'high_risk']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import packages\nimport seaborn as sn\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set width and height \nf = plt.figure() \nf.set_figwidth(15) \nf.set_figheight(12) \n\n# create matrix\nsn.heatmap(dfcorr.corr(), annot = True, vmin=-1, vmax=1, center= 0, cmap= 'Blues', linewidths=1, linecolor='black')\n# Make x and y descriptions larger so they are easier to read\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\n\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}