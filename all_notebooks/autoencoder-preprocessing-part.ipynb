{"cells":[{"metadata":{},"cell_type":"markdown","source":"**I am trying to use the concept:**\n* adding skip connections that allow feature representations to pass through the bottleneck in autoencoder\n* If you find this useful, do upvote","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Image denoising is to remove noise from a noisy image, so as to restore the true image\n* In this notebook FER2013 dataset is used which contains approx 35 thousand images of 7 different emotions\n* Image is grayscale of size 48*48","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Importing libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from keras.datasets import fashion_mnist, mnist\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Dropout\nfrom keras.models import Model\n\nimport os,cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 20, 10\n\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd# Any results you write to the current directory are saved as output.\nfrom IPython.display import display, Image\n\nfrom keras.preprocessing.image import load_img\nfrom keras import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.models import load_model\nfrom keras.optimizers import Adam\nfrom keras.utils.vis_utils import plot_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout\n\n# Any results you write to the current directory are saved as output.\nfrom IPython.display import display, Image","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extract data from CSV","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the data\nfilname = '../input/facial-expression/fer2013/fer2013.csv'\n\n#different labels of images(not useful known about for current problem)\nlabel_map = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n\n#different features/columns names\nnames=['emotion','pixels','usage']\n\n#Reading data in dataframe\ndf=pd.read_csv('../input/facial-expression/fer2013/fer2013.csv',names=names, na_filter=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"im = df['pixels']\nim.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Adding labels and images(pixel values) in respective array","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#reading data and labels from dataset and appending in list\n\ndef getData(filname):\n    # images are 48x48\n    # N = 35887\n    Y = []\n    X = []\n    first = True\n    for line in open(filname):\n        if first:\n            first = False\n        else:\n            row = line.split(',')\n            Y.append(int(row[0]))\n            X.append([int(p) for p in row[1].split()])\n\n    X, Y = np.array(X), np.array(Y)\n    return X, Y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = 'i am, hapy'\na.split(',')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#extracting data from dataset\nX, Y = getData(filname)\nnum_class = len(set(Y))\n#print(num_class)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Before reshaping","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reshaping images","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# keras with tensorflow backend\nN, D = X.shape\n\n#reshaping the dataset\nX = X.reshape(N, 48, 48, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data After Reshaping","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Extracting Data and splitting train and test ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**The important thing is that everytime you use 42, you will always get the same output the first time you make the split. This is useful if you want reproducible results, for example in the documentation, so that everybody can consistently see the same numbers when they run the examples.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting data in train, test\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Image before preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n=10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(48, 48))\nfor i in range(n):\n    ax = plt.subplot(1, n, i+1)\n    plt.imshow(x_train[i].reshape(48, 48))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Form of reshape-->(Number of images,height, width, number of channels)**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Image After Normalizing IMAGE","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Normalizing the images\nx_train = x_train.astype('float32') / 255.\nx_test = x_test.astype('float32') / 255.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(48, 48))\nfor i in range(n):\n    ax = plt.subplot(1, n, i+1)\n    plt.imshow(x_train[i].reshape(48, 48))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Image after adding Noise","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#reshaping the images\nx_train = np.reshape(x_train, (len(x_train), 48, 48, 1))  # adapt this if using `channels_first` image data format\nx_test = np.reshape(x_test, (len(x_test), 48, 48, 1))  # adapt this if using `channels_first` image data format\n\n\n#adding noise in data\nnoise_factor = 0.1\n\nx_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) \nx_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(48, 48))\nfor i in range(n):\n    ax = plt.subplot(1, n, i+1)\n    plt.imshow(x_train_noisy[i].reshape(48, 48))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Image after cliping","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#clipping put data near to 0--->0 aand data near to 1-->1\nx_train_noisy = np.clip(x_train_noisy, 0., 1.)\nx_test_noisy = np.clip(x_test_noisy, 0., 1.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(48, 48))\nfor i in range(n):\n    ax = plt.subplot(1, n, i+1)\n    plt.imshow(x_train_noisy[i].reshape(48, 48))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**0 to 1 ko range ma vaye thik6...tara tyo range bahek aru ma gayo vanee chai tei ramge ma rakne**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Later Part","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}