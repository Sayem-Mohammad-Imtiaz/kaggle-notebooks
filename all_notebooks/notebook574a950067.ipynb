{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<table align=\"left\" width=100%>\n    <tr>\n        <td>\n            <div align=\"middle\">\n                <font color=\"#21618C\" size=5px>\n                  <b>EMPLOYEE ATTRITION BY HR ANALYSIS\n                    </b>\n                </font>\n            </div>\n        </td>\n    </tr>\n</table>","metadata":{}},{"cell_type":"markdown","source":"## Problem Statement","metadata":{}},{"cell_type":"markdown","source":"-We analyse and predict, the dataset containing information about the employees working in an organisation and the factors affecting the chances of attrition based on the geographical, familial and economical conditions as well as their job history. \n\n-We are building a suitable model to predict the attrition considering relevant features and parameters, thus making it easier for HR department to make appropriate decisions.","metadata":{}},{"cell_type":"markdown","source":"## Data Description ","metadata":{}},{"cell_type":"markdown","source":"AGE - Numerical Value\n\nATTRITION - Employee leaving the company (0=Current Employee, 1=Voluntary Resignation)\n\nBUSINESS TRAVEL - (1=No Travel, 2=Travel Frequently, 3=Tavel Rarely)\n\nDAILY RATE - Salary Level\n\nDEPARTMENT - (1=HR, 2=R&D, 3=Sales)\n\nDISTANCE FROM HOME - The distance from work to home\n\nEDUCATION - (1=Below College, 2=College, 3=Bachelor, 4=Master, 5=Doctor)\n\nEMPLOYEE COUNT - Numerical Value\n\nEDUCATION FIELD - (1=HR, 2=LIFE SCIENCES, 3=MARKETING, 4=MEDICAL SCIENCES,\n 5=OTHERS, 6= TEHCNICAL)\n\nEMPLOYEE NUMBER\t- EMPLOYEE ID\n\nENVIROMENT SATISFACTION\t- Satisfaction with the environment\n\nGENDER - (1=FEMALE, 2=MALE)\n\nHOURLY RATE - Hourly Salary\n\nJOB INVOLVEMENT - (1=Low, 2=Medium, 3=High, 4=Very High)\n\nJOB LEVEL - Level of Job\n\nJOB ROLE - Position\n\nJOB SATISFACTION - (1=Low, 2=Medium, 3=High, 4=Very High)\n\nMARITAL STATUS - (1=Divorced, 2=Married, 3=Single)\n\nMONTHLY INCOME - Monthly Salary\n\nMONTHY RATE - MONTHY RATE\n\nNUMCOMPANIES WORKED - Number of companies worked\t\n\nOVER 18 - (Y=YES, N=NO)\n\nOVERTIME - (YES, NO)\n\nPERCENT SALARY HIKE - Percentage increase in salary\n\nPERFORMANCE RATING - (1=Low, 2=Good, 3=Excellent, 4=Outstanding)\n\nRELATIONSHIP SATISFACTION - (1=Low, 2=Medium, 3=High, 4=Very High)\n\nSTANDARD HOURS - Standard working hours\n\nSTOCK OPTIONS LEVEL - Stock options\n\nTOTAL WORKING YEARS - Number of years worked\n\nTRAINING TIMES LAST YEAR - Hours spent for training\n\nWORK LIFE BALANCE - Time spent between work and personal life\n\nYEARS AT COMPANY - Total number of years at the company\n\nYEARS IN CURRENT ROLE - Number of years in current role\n\nYEARS SINCE LAST PROMOTION - Years since last promotion\n\nYEARS WITH CURRENT MANAGER - Years spent with current manager","metadata":{}},{"cell_type":"markdown","source":"## Table of Content\n\n1. **[Import Libraries](#import_lib)**\n2. **[Set Options](#set_options)**\n3. **[Read Data](#RD)**\n4. **[Data Analysis and Preparation](#data_preparation)**\n5. **[Base Model](#LogisticReg)**\n","metadata":{}},{"cell_type":"markdown","source":"## 1. Import Libraries","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt \n\nfrom pandas.api.types import is_string_dtype\nfrom pandas.api.types import is_numeric_dtype\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.externals.six import StringIO  \nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import export_graphviz\nimport featuretools as ft\n\nimport statsmodels\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nimport pydotplus\nfrom IPython.display import Image  \nimport graphviz","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Set Options ","metadata":{}},{"cell_type":"code","source":"# pd.options.display.max_columns = None\n\n# pd.options.display.max_rows = None\n\n# np.set_printoptions(suppress=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Read Data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"IBM HR Data new.csv\")\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Data Analysis and Preparation","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<table align=\"left\">\n    <tr>\n        <td>\n            <div align=\"left\", style=\"font-size:120%\">\n                <font color=\"#21618C\">\n                    <b>We see that there are 23,436 observations and 37 features </br></b>\n                </font>\n            </div>\n        </td>\n    </tr>\n</table>\n\n\n","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe(include='object')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total = df.isnull().sum()\npercent = ((df.isnull().sum()/df.isnull().count())*100).sort_values(ascending=False)\n\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\nprint(missing_data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Attrition'] = df['Attrition'].replace(['Current employee','Voluntary Resignation'],[0,1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df['Education']==6]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(15655,inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Education'] = df['Education'].replace([1,2,3,4,5],['Below college','College','Bachelors','Master','Doctorate'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(['EmployeeCount','EmployeeNumber','Application ID','Over18'],inplace=True,axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['EnvironmentSatisfaction','JobSatisfaction','PerformanceRating', 'RelationshipSatisfaction','StockOptionLevel','WorkLifeBalance']]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Gender'] = df['Gender'].replace(['Male','Female'],[0,1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df['Gender']=='2']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(17027,inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Null Imputation","metadata":{}},{"cell_type":"code","source":"df['Age'] = df['Age'].fillna(df['Age'].mean())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Attrition'] = df['Attrition'].fillna(df['Attrition'].mode()[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['BusinessTravel'] = df['BusinessTravel'].fillna(df['BusinessTravel'].mode()[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['DailyRate'] = df['DailyRate'].fillna(df['DailyRate'].mean())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Department'] = df['Department'].fillna(df['Department'].mode()[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['DistanceFromHome'] = df['DistanceFromHome'].astype(float)\ndf['DistanceFromHome'] = df['DistanceFromHome'].fillna(df['DistanceFromHome'].mean())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Education'] = df['Education'].fillna(df['Education'].mode()[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['EducationField'] = df['EducationField'].fillna(df['EducationField'].mode()[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['EnvironmentSatisfaction'] = df['EnvironmentSatisfaction'].fillna(df['EnvironmentSatisfaction'].median())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Gender'] = df['Gender'].fillna(df['Gender'].mode()[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['HourlyRate'] = df['HourlyRate'].astype(float)\ndf['HourlyRate'] = df['HourlyRate'].fillna(df['HourlyRate'].mean())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['JobInvolvement'] = df['JobInvolvement'].fillna(df['JobInvolvement'].median())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['JobLevel'] = df['JobLevel'].fillna(df['JobLevel'].mean())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['JobRole'] = df['JobRole'].fillna(df['JobRole'].mode()[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['JobSatisfaction'] = df['JobSatisfaction'].astype(float)\ndf['JobSatisfaction'] = df['JobSatisfaction'].fillna(df['JobSatisfaction'].mean())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['MaritalStatus'] = df['MaritalStatus'].fillna(df['MaritalStatus'].mode()[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['MonthlyIncome'] = df['MonthlyIncome'].astype(float)\ndf['MonthlyIncome'] = df['MonthlyIncome'].fillna(df['MonthlyIncome'].median())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['MonthlyRate'] = df['MonthlyRate'].fillna(df['MonthlyRate'].mean())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['NumCompaniesWorked'] = df['NumCompaniesWorked'].fillna(df['NumCompaniesWorked'].median())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['OverTime'] = df['OverTime'].fillna(df['OverTime'].mode()[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['PercentSalaryHike'] = df['PercentSalaryHike'].astype(float)\ndf['PercentSalaryHike'] = df['PercentSalaryHike'].fillna(df['PercentSalaryHike'].mean())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['PerformanceRating'] = df['PerformanceRating'].fillna(df['PerformanceRating'].median())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['RelationshipSatisfaction'] = df['RelationshipSatisfaction'].fillna(df['RelationshipSatisfaction'].mean())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['StandardHours'] = df['StandardHours'].fillna(df['StandardHours'].mean())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['StockOptionLevel'] = df['StockOptionLevel'].fillna(df['StockOptionLevel'].median())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['TotalWorkingYears'] = df['TotalWorkingYears'].fillna(df['TotalWorkingYears'].median())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['TrainingTimesLastYear'] = df['TrainingTimesLastYear'].fillna(df['TrainingTimesLastYear'].median())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['WorkLifeBalance'] = df['WorkLifeBalance'].fillna(df['WorkLifeBalance'].mean())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['YearsAtCompany'] = df['YearsAtCompany'].fillna(df['YearsAtCompany'].median())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['YearsInCurrentRole'] = df['YearsInCurrentRole'].fillna(df['YearsInCurrentRole'].median())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['YearsSinceLastPromotion'] = df['YearsSinceLastPromotion'].fillna(df['YearsSinceLastPromotion'].median())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['YearsWithCurrManager'] = df['YearsWithCurrManager'].fillna(df['YearsWithCurrManager'].median())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Employee Source'] = df['Employee Source'].fillna(df['Employee Source'].mode()[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop('StandardHours',axis=1,inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking for Outliers","metadata":{}},{"cell_type":"code","source":"Q1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3 - Q1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_out = ((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"percent_out = ((((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).sum()/df.count())*100).sort_values(ascending=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outliers = pd.concat([total_out, percent_out], axis=1, keys=['Total', 'Percent'])\noutliers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col = ['BusinessTravel','Education','Department','EducationField','Gender','JobRole','MaritalStatus', 'OverTime','Employee Source']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfnew = pd.get_dummies(df[col] , drop_first = True) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(col,inplace=True,axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.concat([df,dfnew],axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.T","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(df['MonthlyIncome'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['MonthlyIncome'] = df['MonthlyIncome'].apply(lambda x : np.log(x+1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(df['MonthlyIncome'])","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['YearsAtcompany_1_10']=df['YearsAtCompany'][df['YearsAtCompany']<11]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['YearsAtcompany_11+']=df['YearsAtCompany'][df['YearsAtCompany']>=11]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['YearsAtcompany_1_10'] = df['YearsAtcompany_1_10'].fillna(0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['YearsAtcompany_11+'] = df['YearsAtcompany_11+'].fillna(0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['YearsAtcompany_1_10']=df['YearsAtcompany_1_10'].apply(lambda x: 1 if x>1 else x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['YearsAtcompany_11+']=df['YearsAtcompany_11+'].apply(lambda x: 1 if x>1 else x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['NumCompaniesWorked'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['NumCompaniesWorked_0_5']=df['NumCompaniesWorked'][df['NumCompaniesWorked']<=5]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['NumCompaniesWorked_6+']=df['NumCompaniesWorked'][df['NumCompaniesWorked']>5]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['NumCompaniesWorked_0_5'] = df['NumCompaniesWorked_0_5'].fillna(0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['NumCompaniesWorked_6+'] = df['NumCompaniesWorked_6+'].fillna(0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['NumCompaniesWorked_0_5']=df['NumCompaniesWorked_0_5'].apply(lambda x: 1 if x>1 else x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['NumCompaniesWorked_6+']=df['NumCompaniesWorked_6+'].apply(lambda x: 1 if x>1 else x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['PerformanceRating'].value_counts()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['PerformanceRating_3']=df['PerformanceRating'][df['PerformanceRating']==3]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['PerformanceRating_4']=df['PerformanceRating'][df['PerformanceRating']==4]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['PerformanceRating_3'] = df['PerformanceRating_3'].fillna(0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['PerformanceRating_4'] = df['PerformanceRating_4'].fillna(0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['PerformanceRating_3']=df['PerformanceRating_3'].apply(lambda x: 1 if x>=1 else x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['PerformanceRating_4']=df['PerformanceRating_4'].apply(lambda x: 1 if x>=1 else x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['StockOptionLevel'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['StockOptionLevel0_1']=df['StockOptionLevel'][df['StockOptionLevel']<=1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['StockOptionLevel2_3']=df['StockOptionLevel'][df['StockOptionLevel']>1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['StockOptionLevel0_1'] = df['StockOptionLevel0_1'].fillna(0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['StockOptionLevel2_3'] = df['StockOptionLevel2_3'].fillna(0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['StockOptionLevel0_1']=df['StockOptionLevel0_1'].apply(lambda x: 1 if x>=1 else x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['StockOptionLevel2_3']=df['StockOptionLevel2_3'].apply(lambda x: 1 if x>=1 else x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['TotalWorkingYears'].value_counts()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['TotalWorkingYears_1-10']=df['TotalWorkingYears'][df['TotalWorkingYears']<=10]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['TotalWorkingYears_11+']=df['TotalWorkingYears'][df['TotalWorkingYears']>10]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['TotalWorkingYears_1-10'] = df['TotalWorkingYears_1-10'].fillna(0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['TotalWorkingYears_11+'] = df['TotalWorkingYears_11+'].fillna(0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['TotalWorkingYears_1-10']=df['TotalWorkingYears_1-10'].apply(lambda x: 1 if x>=1 else x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['TotalWorkingYears_11+']=df['TotalWorkingYears_11+'].apply(lambda x: 1 if x>=1 else x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['TrainingTimesLastYear'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['TrainingTimesLastYear_0_4']=df['TrainingTimesLastYear'][df['TrainingTimesLastYear']<=4]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['TrainingTimesLastYear_5+']=df['TrainingTimesLastYear'][df['TrainingTimesLastYear']>4]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['TrainingTimesLastYear_0_4'] = df['TrainingTimesLastYear_0_4'].fillna(0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['TrainingTimesLastYear_5+'] = df['TrainingTimesLastYear_5+'].fillna(0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['TrainingTimesLastYear_0_4']=df['TrainingTimesLastYear_0_4'].apply(lambda x: 1 if x>=1 else x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['TrainingTimesLastYear_5+']=df['TrainingTimesLastYear_5+'].apply(lambda x: 1 if x>=1 else x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['YearsInCurrentRole'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['YearsInCurrentRole0_10']=df['YearsInCurrentRole'][df['YearsInCurrentRole']<=10]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['YearsInCurrentRole11+']=df['YearsInCurrentRole'][df['YearsInCurrentRole']>10]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['YearsInCurrentRole0_10'] = df['YearsInCurrentRole0_10'].fillna(0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['YearsInCurrentRole11+'] = df['YearsInCurrentRole11+'].fillna(0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['YearsInCurrentRole0_10']=df['YearsInCurrentRole0_10'].apply(lambda x: 1 if x>=1 else x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['YearsInCurrentRole11+']=df['YearsInCurrentRole11+'].apply(lambda x: 1 if x>=1 else x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['YearsSinceLastPromotion'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['YearsSinceLastPromotion0_10']=df['YearsSinceLastPromotion'][df['YearsSinceLastPromotion']<=10]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['YearsSinceLastPromotion11+']=df['YearsSinceLastPromotion'][df['YearsSinceLastPromotion']>10]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['YearsSinceLastPromotion0_10'] = df['YearsSinceLastPromotion0_10'].fillna(0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['YearsSinceLastPromotion11+'] = df['YearsSinceLastPromotion11+'].fillna(0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['YearsSinceLastPromotion0_10']=df['YearsSinceLastPromotion0_10'].apply(lambda x: 1 if x>=1 else x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['YearsSinceLastPromotion11+']=df['YearsSinceLastPromotion11+'].apply(lambda x: 1 if x>=1 else x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['YearsWithCurrManager'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['YearsWithCurrManager0_8']=df['YearsWithCurrManager'][df['YearsWithCurrManager']<=8]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['YearsWithCurrManager9+']=df['YearsWithCurrManager'][df['YearsWithCurrManager']>8]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['YearsWithCurrManager0_8'] = df['YearsWithCurrManager0_8'].fillna(0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['YearsWithCurrManager9+'] = df['YearsWithCurrManager9+'].fillna(0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['YearsWithCurrManager0_8']=df['YearsWithCurrManager0_8'].apply(lambda x: 1 if x>=1 else x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['YearsWithCurrManager9+']=df['YearsWithCurrManager9+'].apply(lambda x: 1 if x>=1 else x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(['YearsWithCurrManager','YearsAtCompany', 'YearsInCurrentRole','YearsSinceLastPromotion','PerformanceRating','NumCompaniesWorked','StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear'],axis=1,inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## BASE MODEL - Logistic Regression ","metadata":{}},{"cell_type":"code","source":"X=df.drop('Attrition',axis=1)\ny=df['Attrition']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logreg_scaled_features = LogisticRegression()\nlogreg_scaled_features.fit(X_train,y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = logreg_scaled_features.predict(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\n\n# label the confusion matrix  \nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\n\n# set size of the plot\nplt.figure(figsize = (8,5))\n\n# plot a heatmap\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = classification_report(y_test,y_pred)\n\n# print the result\nprint(result)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TN = cm[0,0]\n\n# True Positives are denoted by 'TP'\n# Actual '1' values which are classified correctly\nTP = cm[1,1]\n\n# False Negatives are denoted by 'FN'\n# Actual '1' values which are classified wrongly as '0'\nFN = cm[1,0]\n\n# False Positives are denoted by 'FP'\n# Actual 'O' values which are classified wrongly as '1'\nFP = cm[0,1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(TN,',',TP,',',FN,',',FP)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcParams['figure.figsize']=(8,5)\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.plot(fpr,tpr)\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.plot([0, 1], [0, 1],'r--')\n\nplt.title('ROC curve for logistic Regression')\nplt.xlabel('False positive rate (1-Specificity)')\nplt.ylabel('True positive rate (Sensitivity)')\n\nplt.text(x = 0.05, y = 0.8, s =('AUC Score:',round(metrics.roc_auc_score(y_test, y_pred),4)))\nplt.grid(True)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = ['Model', 'AUC Score', 'Precision Score', 'Recall Score','Accuracy Score','f1-score']\n\n# creating an empty dataframe of the colums\nresult_tabulation = pd.DataFrame(columns = cols)\nLogistic_regression = pd.Series({'Model': \"Logistic regression \",\n                     'AUC Score' : metrics.roc_auc_score(y_test, y_pred),\n                 'Precision Score': metrics.precision_score(y_test, y_pred),\n                 'Recall Score': metrics.recall_score(y_test, y_pred),\n                 'Accuracy Score': metrics.accuracy_score(y_test, y_pred),\n                  'f1-score': metrics.f1_score(y_test, y_pred)})\n\n\n# appending our result table\nresult_tabulation = result_tabulation.append(Logistic_regression, ignore_index = True)\n\n# view the result table\nresult_tabulation","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Applying Smote","metadata":{}},{"cell_type":"code","source":"from collections import Counter\nfrom imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state=42)\nX_res, y_res = sm.fit_resample(X, y)\nprint('Resampled dataset shape %s' % Counter(y_res))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size = 0.30, random_state = 0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logreg_scaled_features = LogisticRegression()\nlogreg_scaled_features.fit(X_train,y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = logreg_scaled_features.predict(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\n\n# label the confusion matrix  \nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\n\n# set size of the plot\nplt.figure(figsize = (8,5))\n\n# plot a heatmap\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = classification_report(y_test,y_pred)\n\n# print the result\nprint(result)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TN = cm[0,0]\n\n# True Positives are denoted by 'TP'\n# Actual '1' values which are classified correctly\nTP = cm[1,1]\n\n# False Negatives are denoted by 'FN'\n# Actual '1' values which are classified wrongly as '0'\nFN = cm[1,0]\n\n# False Positives are denoted by 'FP'\n# Actual 'O' values which are classified wrongly as '1'\nFP = cm[0,1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(TN,',',TP,',',FN,',',FP)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcParams['figure.figsize']=(8,5)\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.plot(fpr,tpr)\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.plot([0, 1], [0, 1],'r--')\n\nplt.title('ROC curve')\nplt.xlabel('False positive rate (1-Specificity)')\nplt.ylabel('True positive rate (Sensitivity)')\n\nplt.text(x = 0.05, y = 0.8, s =('AUC Score:',round(metrics.roc_auc_score(y_test, y_pred),4)))\nplt.grid(True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Logistic_regression_After_smote = pd.Series({'Model': \"Logistic regression After Smote \",\n                     'AUC Score' : metrics.roc_auc_score(y_test, y_pred),\n                 'Precision Score': metrics.precision_score(y_test, y_pred),\n                 'Recall Score': metrics.recall_score(y_test, y_pred),\n                 'Accuracy Score': metrics.accuracy_score(y_test, y_pred),\n                  'f1-score': metrics.f1_score(y_test, y_pred)})\n\n\n# appending our result table\nresult_tabulation = result_tabulation.append(Logistic_regression_After_smote, ignore_index = True)\n\n# view the result table\nresult_tabulation","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcParams['figure.figsize']=(28,10)\n\nresult_tabulation.plot(secondary_y=['Accuracy Score','Precision Score'], mark_right=True)\n\nplt.xticks([0,1,2,3,4,5,6,7,8,9], list(result_tabulation.Model))\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_tabulation.to_excel('result.xlsx')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfnew=pd.concat([X_res,y_res],axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfnew","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfnew.to_excel('New IBM.xlsx')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=dfnew.drop('Attrition',axis=1)\ny=dfnew['Attrition']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.feature_selection import RFE\nfrom sklearn.feature_selection import RFE\nmodel = LogisticRegression()\n#Initializing RFE model\nrfe = RFE(model, 1)\n#Transforming data using RFE\nX_rfe = rfe.fit_transform(X,y)  \n#Fitting the data to model\nmodel.fit(X_rfe,y)\nprint(rfe.support_)\nprint(rfe.ranking_)\n#no of features\nnof_list=np.arange(1,67)            \nhigh_score=0\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nof=0           \nscore_list =[]\nfor n in range(len(nof_list)):\n    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)\n    model = LogisticRegression()\n    rfe = RFE(model,nof_list[n])\n    X_train_rfe = rfe.fit_transform(X_train,y_train)\n    X_test_rfe = rfe.transform(X_test)\n    model.fit(X_train_rfe,y_train)\n    score = model.score(X_test_rfe,y_test)\n    score_list.append(score)\n    if(score>high_score):\n        high_score = score\n        nof = nof_list[n]\nprint(\"Optimum number of features: %d\" %nof)\nprint(\"Score with %d features: %f\" % (nof, high_score))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = list(X.columns)\nmodel = LogisticRegression()\n#Initializing RFE model\nrfe = RFE(model, 45)             \n#Transforming data using RFE\nX_rfe = rfe.fit_transform(X,y)  \n#Fitting the data to model\nmodel.fit(X_rfe,y)              \ntemp = pd.Series(rfe.support_,index = cols)\nselected_features_rfe = temp[temp==True].index\nprint(selected_features_rfe)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfn=dfnew[['EnvironmentSatisfaction', 'JobInvolvement', 'JobSatisfaction',\n       'MonthlyIncome', 'Gender', 'BusinessTravel_Travel_Frequently',\n       'BusinessTravel_Travel_Rarely', 'Education_Doctorate',\n       'Department_Research & Development', 'EducationField_Life Sciences',\n       'EducationField_Marketing', 'EducationField_Medical',\n       'EducationField_Other', 'EducationField_Technical Degree',\n       'JobRole_Human Resources', 'JobRole_Laboratory Technician',\n       'JobRole_Manager', 'JobRole_Manufacturing Director',\n       'JobRole_Research Director', 'JobRole_Research Scientist',\n       'JobRole_Sales Executive', 'OverTime_Yes',\n       'Employee Source_Company Website', 'Employee Source_GlassDoor',\n       'Employee Source_Indeed', 'Employee Source_Jora',\n       'Employee Source_LinkedIn', 'Employee Source_Recruit.net',\n       'Employee Source_Referral', 'Employee Source_Seek',\n       'YearsAtcompany_11+', 'NumCompaniesWorked_0_5', 'NumCompaniesWorked_6+',\n       'PerformanceRating_3', 'PerformanceRating_4', 'StockOptionLevel0_1',\n       'StockOptionLevel2_3', 'TotalWorkingYears_11+',\n       'TrainingTimesLastYear_0_4', 'TrainingTimesLastYear_5+',\n       'YearsInCurrentRole0_10', 'YearsInCurrentRole11+',\n       'YearsSinceLastPromotion0_10', 'YearsWithCurrManager0_8',\n       'YearsWithCurrManager9+']]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=dfn\ny=dfnew['Attrition']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logreg_scaled_features = LogisticRegression()\nlogreg_scaled_features.fit(X_train,y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = logreg_scaled_features.predict(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\n\n# label the confusion matrix  \nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\n\n# set size of the plot\nplt.figure(figsize = (8,5))\n\n# plot a heatmap\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = classification_report(y_test,y_pred)\n\n# print the result\nprint(result)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TN = cm[0,0]\n\n# True Positives are denoted by 'TP'\n# Actual '1' values which are classified correctly\nTP = cm[1,1]\n\n# False Negatives are denoted by 'FN'\n# Actual '1' values which are classified wrongly as '0'\nFN = cm[1,0]\n\n# False Positives are denoted by 'FP'\n# Actual 'O' values which are classified wrongly as '1'\nFP = cm[0,1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(TN,',',TP,',',FN,',',FP)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcParams['figure.figsize']=(8,5)\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nplt.plot(fpr,tpr)\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.plot([0, 1], [0, 1],'r--')\n\nplt.title('ROC curve')\nplt.xlabel('False positive rate (1-Specificity)')\nplt.ylabel('True positive rate (Sensitivity)')\n\nplt.text(x = 0.05, y = 0.8, s =('AUC Score:',round(metrics.roc_auc_score(y_test, y_pred),4)))\nplt.grid(True)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Logistic_regression_after_FS = pd.Series({'Model': \"Logistic regression after feature selection\",\n                     'AUC Score' : metrics.roc_auc_score(y_test, y_pred),\n                 'Precision Score': metrics.precision_score(y_test, y_pred),\n                 'Recall Score': metrics.recall_score(y_test, y_pred),\n                 'Accuracy Score': metrics.accuracy_score(y_test, y_pred),\n                  'f1-score': metrics.f1_score(y_test, y_pred)})\n\n\n# appending our result table\nresult_tabulation = result_tabulation.append(Logistic_regression_after_FS, ignore_index = True)\n\n# view the result table\nresult_tabulation","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Decision Tree","metadata":{}},{"cell_type":"code","source":"decision_tree_classification = DecisionTreeClassifier(criterion='entropy')\n\n# train model\ndecision_tree = decision_tree_classification.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decision_tree_pred = decision_tree.predict(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, decision_tree_pred)\n\n# label the confusion matrix  \nconf_matrix = pd.DataFrame(data = cm, columns = ['Predicted:0','Predicted:1'], index = ['Actual:0','Actual:1'])\n\n# set sizeof the plot\nplt.figure(figsize = (8,5))\n\n# plot a heatmap\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap=\"YlGnBu\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TN = cm[0,0]\n\n# True Positives are denoted by 'TP'\n# Actual '1' values which are classified correctly\nTP = cm[1,1]\n\n# False Negatives are denoted by 'FN'\n# Actual '1' values which are classified wrongly as '0'\nFN = cm[1,0]\n\n# False Positives are denoted by 'FP'\n# Actual 'O' values which are classified wrongly as '1'\nFP = cm[0,1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy is:\",metrics.accuracy_score(y_test,decision_tree_pred))\n\nprint('train score:',decision_tree.score(X_train,y_train))\n\nprint('test score:',decision_tree.score(X_test,y_test))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = classification_report(y_test, decision_tree_pred)\n\n# print the result\nprint(result)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set the figure size\nplt.rcParams['figure.figsize']=(8,5)\n\nfpr, tpr, thresholds = roc_curve(y_test, decision_tree_pred)\n\n# plot the ROC curve\nplt.plot(fpr,tpr)\n\n# set limits for x and y axes\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\n# plot the straight line showing worst prediction for the model\nplt.plot([0, 1], [0, 1],'r--')\n\n# add the AUC score\nplt.text(x = 0.05, y = 0.8, s =('AUC Score:', round(metrics.roc_auc_score(y_test, decision_tree_pred),4)))\n\n\n# name the plot, and both axes\nplt.title('ROC curve')\nplt.xlabel('False positive rate (1-Specificity)')\nplt.ylabel('True positive rate (Sensitivity)')\n\n# plot the grid\nplt.grid(True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Decision_tree_metrics = pd.Series({'Model': \"Decision Tree \",\n                     'AUC Score' : metrics.roc_auc_score(y_test, decision_tree_pred),\n                 'Precision Score': metrics.precision_score(y_test, decision_tree_pred),\n                 'Recall Score': metrics.recall_score(y_test, decision_tree_pred),\n                 'Accuracy Score': metrics.accuracy_score(y_test, decision_tree_pred),\n                 \n                  'f1-score':metrics.f1_score(y_test, decision_tree_pred)})\n\n\n\n# appending our result table\nresult_tabulation = result_tabulation.append(Decision_tree_metrics , ignore_index = True)\n\n# view the result table\nresult_tabulation","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pruned Decision Tree","metadata":{}},{"cell_type":"code","source":"pruned = DecisionTreeClassifier(criterion=\"entropy\", max_depth=25)\n\n# train the classifier\ndecision_tree_prune = pruned.fit(X_train,y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decision_tree_prune_pred = decision_tree_prune.predict(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, decision_tree_prune_pred)\n\n# label the confusion matrix  \nconf_matrix = pd.DataFrame(data = cm, columns = ['Predicted:0','Predicted:1'], index = ['Actual:0','Actual:1'])\n\n# set sizeof the plot\nplt.figure(figsize = (8,5))\n\n# plot a heatmap\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap=\"YlGnBu\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TN = cm[0,0]\n\n# True Positives are denoted by 'TP'\n# Actual '1' values which are classified correctly\nTP = cm[1,1]\n\n# False Negatives are denoted by 'FN'\n# Actual '1' values which are classified wrongly as '0'\nFN = cm[1,0]\n\n# False Positives are denoted by 'FP'\n# Actual 'O' values which are classified wrongly as '1'\nFP = cm[0,1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = classification_report(y_test,decision_tree_prune_pred)\n\n# print the result\nprint(result)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy is:\",metrics.accuracy_score(y_test,decision_tree_prune_pred))\n\nprint('train score:',decision_tree_prune.score(X_train,y_train))\n\nprint('test score:',decision_tree_prune.score(X_test,y_test))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcParams['figure.figsize']=(8,5)\n\nfpr, tpr, thresholds = roc_curve(y_test, decision_tree_prune_pred)\n\n# plot the ROC curve\nplt.plot(fpr,tpr)\n\n# set limits for x and y axes\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\n# plot the straight line showing worst prediction for the model\nplt.plot([0, 1], [0, 1],'r--')\n\n# add the AUC score\nplt.text(x = 0.05, y = 0.8, s =('AUC Score:', round(metrics.roc_auc_score(y_test, decision_tree_prune_pred),4)))\n\n\n# name the plot, and both axes\nplt.title('ROC curve')\nplt.xlabel('False positive rate (1-Specificity)')\nplt.ylabel('True positive rate (Sensitivity)')\n\n# plot the grid\nplt.grid(True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Pruned_Decision_tree_metrics = pd.Series({'Model': \"Pruned Decision Tree \",\n                     'AUC Score' : metrics.roc_auc_score(y_test, decision_tree_prune_pred),\n                 'Precision Score': metrics.precision_score(y_test, decision_tree_prune_pred),\n                 'Recall Score': metrics.recall_score(y_test, decision_tree_prune_pred),\n                 'Accuracy Score': metrics.accuracy_score(y_test, decision_tree_prune_pred),\n               \n                  'f1-score':metrics.f1_score(y_test, decision_tree_prune_pred)})\n\n\n\n# appending our result table\nresult_tabulation = result_tabulation.append(Pruned_Decision_tree_metrics , ignore_index = True)\n\n# view the result table\nresult_tabulation","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = {\"criterion\": [\"gini\", \"entropy\"],\n              \"min_samples_split\": [10, 20],\n              \"max_depth\": [3, 5, 10, 20,25],\n              \"min_samples_leaf\": [30, 100, 300],\n              \"max_leaf_nodes\": [None,2,3,5],\n              }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decision_tree_Gridsearch = DecisionTreeClassifier()\ndecision_tree_Gridsearch = GridSearchCV(decision_tree_Gridsearch, param_grid, cv=10)\ndecision_tree_Gridsearch.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Decision GridSearch","metadata":{}},{"cell_type":"code","source":"decision_tree_Gridsearch.best_params_","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decision_tree_best_parameters = DecisionTreeClassifier(max_depth= decision_tree_Gridsearch.best_params_.get('max_depth'), \n                                                       min_samples_leaf= decision_tree_Gridsearch.best_params_.get('min_samples_leaf'), \n                                                       min_samples_split= decision_tree_Gridsearch.best_params_.get('min_samples_split'),\n                                                       criterion=decision_tree_Gridsearch.best_params_.get('criterion')).fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decision_tree_best_parameters_pred = decision_tree_best_parameters.predict(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, decision_tree_best_parameters_pred)\n\n# label the confusion matrix  \nconf_matrix = pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\n\n# set sizeof the plot\nplt.figure(figsize = (8,5))\n\n# plot a heatmap\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = classification_report(y_test,decision_tree_best_parameters_pred)\n\n# print the result\nprint(result)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcParams['figure.figsize']=(8,5)\n\nfpr, tpr, thresholds = roc_curve(y_test, decision_tree_best_parameters_pred)\n\n# plot the ROC curve\nplt.plot(fpr,tpr)\n\n# set limits for x and y axes\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\n# plot the straight line showing worst prediction for the model\nplt.plot([0, 1], [0, 1],'r--')\n\n# add the AUC score\nplt.text(x = 0.05, y = 0.8, s =('AUC Score:', round(metrics.roc_auc_score(y_test, decision_tree_best_parameters_pred),4)))\n\n\n# name the plot, and both axes\nplt.title('ROC curve')\nplt.xlabel('False positive rate (1-Specificity)')\nplt.ylabel('True positive rate (Sensitivity)')\n\n# plot the grid\nplt.grid(True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Decision_tree_GridSearch_metrics = pd.Series({'Model': \"Decision Tree (GridSearchCV) \",\n                     'AUC Score' : metrics.roc_auc_score(y_test, decision_tree_best_parameters_pred),\n                 'Precision Score': metrics.precision_score(y_test, decision_tree_best_parameters_pred),\n                 'Recall Score': metrics.recall_score(y_test, decision_tree_best_parameters_pred),\n                 'Accuracy Score': metrics.accuracy_score(y_test, decision_tree_best_parameters_pred),\n     \n                 'f1-score':metrics.f1_score(y_test, decision_tree_best_parameters_pred)})\n\n\n\n# appending our result table\nresult_tabulation = result_tabulation.append(Decision_tree_GridSearch_metrics , ignore_index = True)\n\n# view the result table\nresult_tabulation","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf=RandomForestClassifier(n_estimators=5,max_depth=30)\n#Train the model using the training sets y_pred=clf.predict(X_test)\nran=clf.fit(X_train,y_train)\n#predict the model\ny_pred=clf.predict(\n    X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport numpy as np; np.random.seed(0)\nimport matplotlib.pylab as plt\nimport matplotlib.transforms\n\ndata = np.random.randint(100, size=(5,5))\nakws = {\"ha\": 'left',\"va\": 'top'}\nax = sns.heatmap(data,  annot=True, annot_kws=akws)\n\nfor t in ax.texts:\n    trans = t.get_transform()\n    offs = matplotlib.transforms.ScaledTranslation(0.75, 0.5,\n                    matplotlib.transforms.IdentityTransform())\n    t.set_transform( offs + trans )\n\nplt.show()\noffs = matplotlib.transforms.ScaledTranslation(0.50, 0.50,\n                    matplotlib.transforms.IdentityTransform())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\n\n# label the confusion matrix  \nconf_matrix = pd.DataFrame(data = cm, columns = ['Predicted:0','Predicted:1'], index = ['Actual:0','Actual:1'])\n\n# set sizeof the plot\nplt.figure(figsize = (8,5))\n\n# plot a heatmap\nakws = {\"ha\": 'center',\"va\": 'center'}\nax = sns.heatmap(conf_matrix, annot=True, fmt='d', cmap=\"YlGnBu\" )\nfor t in ax.texts:\n    trans = t.get_transform()\n    offs = matplotlib.transforms.ScaledTranslation(-0.45,0.45,\n                    matplotlib.transforms.IdentityTransform())\n    t.set_transform( offs + trans )\n\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = classification_report(y_test, y_pred)\n\n# print the result\nprint(result)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy is:\",metrics.accuracy_score(y_test,y_pred))\n\nprint('train score:',ran.score(X_train,y_train))\n\nprint('test score:',ran.score(X_test,y_test))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcParams['figure.figsize']=(8,5)\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\n\n# plot the ROC curve\nplt.plot(fpr,tpr)\n\n# set limits for x and y axes\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\n# plot the straight line showing worst prediction for the model\nplt.plot([0, 1], [0, 1],'r--')\n\n# add the AUC score\nplt.text(x = 0.05, y = 0.8, s =('AUC Score:', round(metrics.roc_auc_score(y_test, y_pred),4)))\n\n\n# name the plot, and both axes\nplt.title('ROC curve')\nplt.xlabel('False positive rate (1-Specificity)')\nplt.ylabel('True positive rate (Sensitivity)')\n\n# plot the grid\nplt.grid(True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ran_metrics = pd.Series({'Model': \"RandomForest \",\n                     'AUC Score' : metrics.roc_auc_score(y_test, y_pred),\n                 'Precision Score': metrics.precision_score(y_test, y_pred),\n                 'Recall Score': metrics.recall_score(y_test, y_pred),\n                 'Accuracy Score': metrics.accuracy_score(y_test, y_pred),\n\n                  'f1-score':metrics.f1_score(y_test, y_pred)})\n\n\n\n# appending our result table\nresult_tabulation = result_tabulation.append(ran_metrics , ignore_index = True)\n\n# view the result table\nresult_tabulation","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest Using Cross Valid","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.model_selection import cross_val_score\nprint(cross_val_score(RandomForestClassifier(max_depth=25), X, y, cv=10))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Naive Bayes","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\n\n# label the confusion matrix  \nconf_matrix = pd.DataFrame(data = cm, columns = ['Predicted:0','Predicted:1'], index = ['Actual:0','Actual:1'])\n\n# set sizeof the plot\nplt.figure(figsize = (8,5))\n\n# plot a heatmap\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap=\"YlGnBu\")\nplt.show()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = classification_report(y_test, y_pred)\n\n# print the result\nprint(result)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy is:\",metrics.accuracy_score(y_test,y_pred))\n\nprint('train score:',classifier.score(X_train,y_train))\n\nprint('test score:',classifier.score(X_test,y_test))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcParams['figure.figsize']=(8,5)\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\n\n# plot the ROC curve\nplt.plot(fpr,tpr)\n\n# set limits for x and y axes\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\n# plot the straight line showing worst prediction for the model\nplt.plot([0, 1], [0, 1],'r--')\n\n# add the AUC score\nplt.text(x = 0.05, y = 0.8, s =('AUC Score:', round(metrics.roc_auc_score(y_test, y_pred),4)))\n\n\n# name the plot, and both axes\nplt.title('ROC curve')\nplt.xlabel('False positive rate (1-Specificity)')\nplt.ylabel('True positive rate (Sensitivity)')\n\n# plot the grid\"\nplt.grid(True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Naive_bayes = pd.Series({'Model': \"Naive Bayes \",\n                     'AUC Score' : metrics.roc_auc_score(y_test, y_pred),\n                 'Precision Score': metrics.precision_score(y_test, y_pred),\n                 'Recall Score': metrics.recall_score(y_test, y_pred),\n                 'Accuracy Score': metrics.accuracy_score(y_test, y_pred),\n\n                  'f1-score':metrics.f1_score(y_test, y_pred)})\n\n\n\n# appending our result table\nresult_tabulation = result_tabulation.append(Naive_bayes , ignore_index = True)\n\n# view the result table\nresult_tabulation","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcParams['figure.figsize']=(28,10)\n\nresult_tabulation.plot()\n\nplt.xticks([0,1,2,3,4,5,6,7,8,9], list(result_tabulation.Model))\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_tabulation.to_excel('result.xlsx')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ensemble","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n\n\ncls=AdaBoostClassifier(DecisionTreeClassifier(),n_estimators=200)\ncls.fit(X_train,y_train)\nyp=cls.predict(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, yp)\n\n# label the confusion matrix  \nconf_matrix = pd.DataFrame(data = cm, columns = ['Predicted:0','Predicted:1'], index = ['Actual:0','Actual:1'])\n\n# set sizeof the plot\nplt.figure(figsize = (8,5))\n\n# plot a heatmap\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap=\"YlGnBu\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = classification_report(y_test, y_pred)\n\n# print the result\nprint(result)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import model_selection","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nseed = 8\nkfold = model_selection.KFold(n_splits = 3, \n                       random_state = seed) \n  \n# initialize the base classifier \nbase_cls = DecisionTreeClassifier() \n  \n# no. of base classifier \nnum_trees = 500\n  \n# bagging classifier \nmodel = BaggingClassifier(base_estimator = base_cls, \n                          n_estimators = num_trees, \n                          random_state = seed) \n  \nresults = model_selection.cross_val_score(model, X, y, cv = kfold) \nprint(\"accuracy :\") \nprint(results.mean())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = 8\nkfold = model_selection.KFold(n_splits = 3, \n                       random_state = seed) \n  \n# initialize the base classifier \nbase_cls = DecisionTreeClassifier() \n  \n# no. of base classifier \nnum_trees = 500\n  \n# bagging classifier \nmodel = AdaBoostClassifier(base_estimator = base_cls, \n                          n_estimators = num_trees, \n                          random_state = seed) \n  \nresults = model_selection.cross_val_score(model, X, y, cv = kfold) \nprint(\"accuracy :\") \nprint(results.mean()) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}