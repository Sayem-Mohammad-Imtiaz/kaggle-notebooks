{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip3 install jupyterthemes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom colorama import Fore, Style\nimport os\nimport sys\nfrom jupyterthemes import jtplot\njtplot.style(theme=\"monokai\", context=\"notebook\", ticks=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/mushroom-classification/mushrooms.csv')\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Fore.YELLOW, \"Loading data information ...\", Style.RESET_ALL)\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check class distribution\nsns.countplot(x=\"class\", data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"for cols in df.columns:\n    unique_values = df[cols].unique()\n    print(Fore.YELLOW, f\"Number of unique values in '{cols}':\", Style.RESET_ALL, len(unique_values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, Y = df.drop(\"class\", axis=1), df[\"class\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n\nlabel_encoder = LabelEncoder()\nfor i in X.columns:\n    X[i] = label_encoder.fit_transform(X[i])\n    \nlabel_encoder = LabelEncoder()\nY = label_encoder.fit_transform(Y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Poisonous = 1 \n## Edible = 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.get_dummies(X, columns=X.columns, drop_first=True)\nX.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nstsc = StandardScaler()\nXtrain = stsc.fit_transform(Xtrain)\nXtest = stsc.transform(Xtest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\nXtrain = pca.fit_transform(Xtrain)\nXtest = pca.transform(Xtest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"D = X.shape[1]\n\nYtrain = Ytrain.reshape(-1, 1)\nYtest = Ytest.reshape(-1, 1)\n\nprint(Fore.YELLOW, \"Shapes for Training Data....\", Style.RESET_ALL)\nprint(f\"Shape of Xtrain : {Xtrain.shape}\")\nprint(f\"Shape of Ytrain : {Ytrain.shape}\")\n\n\nprint(Fore.BLUE, \"Shapes for Testing Data....\", Style.RESET_ALL)\nprint(f\"Shape of Xtest : {Xtest.shape}\")\nprint(f\"Shape of Ytest : {Ytest.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Fore.YELLOW, \"Creating PyTorch Datasets for computation\")\n\ntrain_dataset = torch.utils.data.TensorDataset(torch.from_numpy(Xtrain.astype(np.float32)), torch.from_numpy(Ytrain.astype(np.float32)))\ntest_dataset = torch.utils.data.TensorDataset(torch.from_numpy(Xtest.astype(np.float32)), torch.from_numpy(Ytest.astype(np.float32)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 128\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Logistic(nn.Module):\n    \n    def __init__(self, n_units, n_classes):\n        \n        super(Logistic, self).__init__()\n        \n        self.seq = nn.Sequential(nn.Linear(n_units, n_classes), nn.Sigmoid())\n        \n    def forward(self, X):\n        X = self.seq(X)\n        \n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logregmodel = Logistic(Xtrain.shape[1], 1)\nlogregmodel.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the loss and optimizer\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(logregmodel.parameters())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the training loop \ndef batch_gd(model, criterion, optimizer, train_loader, test_loader, epochs=20):\n    \n    \"\"\"\n    ----------------------------------------------------\n    Description : Function to do batch gradient descent \n                  on the input dataset\n                  \n    Arguments :\n    \n    model -- a pytorch model \n    criterion -- a pytorch module which contains the loss\n    optimizer -- a pytorcch module which contains the optimizers used for batch gradient descent\n    train_loader -- a pytorch dataloader representing the training set\n    test_loader -- a pytorch dataloader representing the testing set\n    epochs -- an integer representing the number of training loops to go through\n    \n    Return :\n    \n    train_losses -- a numpy array containing the loss values encountered during training\n    test_losses -- a numpy array containing the loss values encountered during validation\n    \n    Usage :\n    \n    train_loss, test_val = batch_gd(model, criterion, optimizer, train_loader, test_loader, epochs=10000)\n    \n    -------------------------------------------------------    \n    \n    \"\"\"\n    \n    train_losses = np.zeros(epochs)\n    test_losses = np.zeros(epochs)\n    \n    for epoch in range(epochs):\n        \n        train_loss = []\n        \n        for inputs, targets in train_loader:\n            \n            # Move the inputs and targets to the device\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            # Zero-initiialize the optimizer gradients\n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            \n            # Backward and optimize\n            loss.backward()\n            optimizer.step()\n            \n            \n            train_loss.append(loss.item())\n            \n        \n        test_loss = []\n        \n        for inputs, targets in test_loader:\n            \n            # Move the inputs and targets to the device\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            \n            test_loss.append(loss.item())\n            \n        \n        train_loss = np.mean(train_loss)\n        test_loss = np.mean(test_loss)\n        \n        train_losses[epoch] = train_loss\n        test_losses[epoch] = test_loss\n        \n        print(f\"Epoch : {epoch+1}/{epochs} | Train Loss : {train_loss} | Test Loss : {test_loss}\")\n            \n        \n    return train_losses, test_losses\n            \n            ","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"train_losses, test_losses = batch_gd(logregmodel, criterion, optimizer, train_loader, test_loader, epochs=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title(\"Epochs vs Losses\")\nplt.plot(train_losses, label=\"Train losses\")\nplt.plot(test_losses, label=\"Test losses\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Losses\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the model accuracy\n\ndef get_accuracy(evalmodel, train_loader, test_loader):\n    \n    \"\"\"\n    -----------------------------------------------\n    Description : To calculate the accuracy rate of the model\n    \n    Arguments :\n    \n    model : a pytorch model \n    train_loader : a pytorch data loader representing the training set\n    test_loader : a pytorch data loader representing the testing set\n    \n    Return:\n    \n    train_acc : a float value representing the training accuracy of the model\n    test_acc : a float value representing the testing accuracy of the model\n    \n    \n    Usage :\n    \n    trainAcc, testAcc = get_accuracy(model, train_loader, test_loader)\n    --------------------------------------------------\n    \n    \"\"\"\n    \n    \n    n_correct = 0\n    n_total = 0\n    \n    for inputs, targets in train_loader:\n        \n        # move targets to the device\n        inputs, targets = inputs.to(device), targets.to(device)\n        \n        # Forward pass\n        outputs = evalmodel(inputs).detach().numpy()\n        \n        n_correct += np.mean(targets.detach().numpy() == np.round(outputs))\n        \n        n_total += 1\n        \n    \n    train_acc = n_correct / n_total\n    \n    \n    for inputs, targets in test_loader:\n        \n        # move targets to the device\n        inputs, targets = inputs.to(device), targets.to(device)\n        \n        # Forward pass\n        outputs = evalmodel(inputs).detach().numpy()\n        \n        n_correct += np.mean(targets.detach().numpy() == np.round(outputs))\n        n_total += 1\n        \n    \n    test_acc = n_correct / n_total\n    \n    \n    return train_acc, test_acc\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_acc , test_acc = get_accuracy(logregmodel, train_loader, test_loader)\n\nprint(f\"Training Accuracy : {train_acc} || Testing Accuracy : {test_acc}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Logistic Model seems to be a pretty good model. But let's see what an ANN can do in comparison to the logistic model."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Artificial Neural Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ANN(nn.Module):\n    \n    def __init__(self, n_features, n_classes):\n        \n        super(ANN, self).__init__()\n        \n        self.dense = nn.Sequential(\n                nn.Linear(n_features, 20),\n                nn.ReLU(),\n                nn.Linear(20, 10),\n                nn.ReLU(),\n                nn.Linear(10, n_classes),\n                nn.Sigmoid()\n        )\n        \n    def forward(self, X):\n        \n        X = self.dense(X)\n        \n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"annmodel = ANN(Xtrain.shape[1], 1)\nannmodel.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = nn.BCELoss()\noptimizer = torch.optim.Adam(annmodel.parameters())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_losses, test_losses = batch_gd(annmodel, criterion, optimizer, train_loader, test_loader, epochs=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title(\"Epochs vs Losses\")\nplt.plot(train_losses, label=\"Training loss\")\nplt.plot(test_losses, label=\"Test loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_acc, test_acc = get_accuracy(annmodel, train_loader, test_loader)\n\nprint(f\"Training Acc : {train_acc} | Test Acc : {test_acc}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}