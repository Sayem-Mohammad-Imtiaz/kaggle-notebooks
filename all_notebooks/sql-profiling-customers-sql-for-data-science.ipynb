{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-03T09:29:08.539849Z","iopub.execute_input":"2021-06-03T09:29:08.540724Z","iopub.status.idle":"2021-06-03T09:29:08.557549Z","shell.execute_reply.started":"2021-06-03T09:29:08.54061Z","shell.execute_reply":"2021-06-03T09:29:08.555929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### [THis is the original Kernel](https://www.kaggle.com/naim99/sql-profiling-customers-sql-for-data-science)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sqlite3\nimport nltk\nimport string","metadata":{"execution":{"iopub.status.busy":"2021-06-03T09:29:08.560047Z","iopub.execute_input":"2021-06-03T09:29:08.560418Z","iopub.status.idle":"2021-06-03T09:29:10.446224Z","shell.execute_reply.started":"2021-06-03T09:29:08.560376Z","shell.execute_reply":"2021-06-03T09:29:10.445261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create a SQL connection to our SQLite database\ncon = sqlite3.connect('../input/amazon-customers-data/database.sqlite')","metadata":{"execution":{"iopub.status.busy":"2021-06-03T09:29:10.448178Z","iopub.execute_input":"2021-06-03T09:29:10.448624Z","iopub.status.idle":"2021-06-03T09:29:10.464133Z","shell.execute_reply.started":"2021-06-03T09:29:10.44859Z","shell.execute_reply":"2021-06-03T09:29:10.463036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/amazon-customers-data/Reviews.csv\") ","metadata":{"execution":{"iopub.status.busy":"2021-06-03T09:31:24.977147Z","iopub.execute_input":"2021-06-03T09:31:24.977533Z","iopub.status.idle":"2021-06-03T09:31:32.544046Z","shell.execute_reply.started":"2021-06-03T09:31:24.977502Z","shell.execute_reply":"2021-06-03T09:31:32.542698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2021-06-03T09:31:33.324497Z","iopub.execute_input":"2021-06-03T09:31:33.324884Z","iopub.status.idle":"2021-06-03T09:31:33.332268Z","shell.execute_reply.started":"2021-06-03T09:31:33.324853Z","shell.execute_reply":"2021-06-03T09:31:33.331147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_sql_query(\"SELECT * FROM Reviews\", con)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# THESE ARE USEFUL QUERIES THAT ARE CAN BE CUSTOMIZED FOR THIS DATASET : ","metadata":{}},{"cell_type":"code","source":"#Find the cities with the most customers and rank in descending order. \n\"\"\"\nSELECT City,\n       COUNT(*)\nFROM Customers\nGROUP BY City\nORDER BY COUNT(*) DESC\n\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Retrieve all the records from the Employees table.\n\n\"\"\"\n\nSELECT * FROM Employees\n\n\"\"\"\n#Retrieve the FirstName, LastName, Birthdate, Address, City, and State from the Employees table.\n\n\"\"\"\nSELECT FirstName, LastName, Birthdate, Address, City, State \nFROM Employees\nWHERE BirthDate = '1965-03-03 00:00:00'\n\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Retrieve all the columns from the Tracks table, but only return 20 rows.\n\n\"\"\"\n\nSELECT * from Tracks\nLIMIT 20\n\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Find all the tracks that have a length of 5,000,000 milliseconds or more.\n\n\"\"\"\nSELECT COUNT(TrackId)\nFROM TRACKS\nWHERE Milliseconds >= 5000000 \n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Find all the invoices whose total is between $5 and $15 dollars.\n\n\"\"\"\n\nSELECT InvoiceID,Total\nFROM Invoices\nWHERE Total > 5 AND Total < 15\n\n\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Find all the customers from the following States: RJ, DF, AB, BC, CA, WA, NY.\n\n\"\"\"\nSELECT FirstName, LastName, Company, State\nFROM Customers\nWHERE State IN ('RJ','DF','AB','BC','CA','WA','NY')\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Find all the invoices for customer 56 and 58 where the total was between $1.00 and $5.00.\n\n\"\"\"\n\nSELECT CustomerId, InvoiceId, Total, InvoiceDate\nFROM Invoices\nWHERE CustomerID IN (56,58) AND \nTotal BETWEEN 1 AND 5\n\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Find all the tracks whose name starts with 'All'.\n\n\"\"\"\n\nSELECT TrackId, Name\nFROM Tracks\nWHERE Name LIKE 'All%'\n\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Find all the customer emails that start with \"J\" and are from gmail.com.\n\n\"\"\"\n\nSELECT CustomerId, Email\nFROM Customers\nWHERE Email LIKE \"J%@gmail.com\"\n\n\n\"\"\" ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Find all the invoices from Brasilia, Edmonton, and Vancouver and sort in descending order by invoice ID.\n\n\"\"\"\nSELECT InvoiceId, BillingCity, Total\nFROM Invoices\nWHERE BillingCity IN ('Brasilia','Edmonton','Vancouver')\nORDER BY InvoiceId DESC\n\n\"\"\"\n#Show the number of orders placed by each customer and sort the result by the number of orders in descending order.\n\n\"\"\"\n\nSELECT CustomerId, COUNT(*) AS Orders\nFROM Invoices\nGROUP BY CustomerId\nORDER BY Orders DESC\n\n\"\"\"\n\n#Find the albums with 12 or more tracks.\n\n\"\"\"\n\nSELECT AlbumId, Count(*) AS Ntracks\nFROM Tracks\nGROUP BY AlbumId\nHAVING COUNT (*) >= 12\n\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SELECT  count(gender), max(revenu), min(revenu), avg(revenu) avg_revenue, count(behavior) FROM user_tables \nGROUP BY region\nWHERE gender = \"M\" and revenu > avg_revenue and region = 'la marsa' \n\n----------------------------------------------------------------------------------------------------------------------------------------\n2- \nHereâ€™s an example of an SQL query that pulls client performance data by  c_ID , region ,Group , Subsciption and  metrics (i.e. purchases , behavior ) \n\nSELECT c_ID, region ,Group , Subsciption, SUM(purchases) toatal purchases, SUM(behavior) total behavior , AVG(purchases) , AVG(behavior)   \n\nFROM `my-dataset`\n\nGROUP BY c_ID, region , Group\n\nORDER BY toatal purchases DESC\n\nLIMIT 1000\n----------------------------------------------------------------------------------\n3- \n\nSELECT Date,\n       SUM(purchases) AS total_purchases\nFROM my_dataset\nWHERE Date BETWEEN '2021-05-01' AND '2021-06-01'\nGROUP BY Date\nHAVING  SUM(total_purchases) > 100\nORDER BY Date DESC \n---------------------------------------------------------------------------------------------------------------------------------------\n4- \n\nSELECT AVG(purchases)\nFROM my_dataset \n\nSELECT c_ID, SUM(purchases) , AVG(purchases) \n WHERE (region, Group, age_range) = ('TUNIS', 'high users', '18-24')  \n FROM products\n GROUP BY c_ID  \n-------------------------------------------------------------------------------------------------------------------\n5- \nSQL statement which selects the highest purchases  for each of the regions of the customers.\n\nSELECT region ,MAX(purchases) \nFROM my_datatset\nGROUP BY region\n-----------------------------------------------------------------------------------------------------------------------\n6- \n\n#purchases greater than the average \nSELECT COUNT(*) FROM products WHERE purchases > AVG(purchases) \n-------------------------------------------------------------\n7- \n#select the c_ID , region , group with maximum purchases\nSELECT c_ID, region,Group MAX(purchases) FROM products\nSELECT c_ID, region,Group  MAX(behavior) FROM products\n--------------------------------------------------------------\n8- \n# extract the region where there are low purchases amounts\n\nSELECT MIN(purchases)\nFROM `my_dataset`\nWHERE `Group` = 'High value customers' AND `region` = 'TUNIS' \n\n\nSELECT c_ID, Group, purchases\n  FROM titles\n  WHERE NOT Group = 'Y' OR Group = 'Z'\n        AND NOT purchases < 20;  \n\n SELECT c_ID,\n         type = 'X' AS \"X_type\",\n         type = 'Y' AS \"Y_type\",\n         purchases,\n         purchases < 20 AS \"<20?\"\n    FROM my_dataset;  \n     \n--------------------------------------------------------------\n9-\nSELECT `Group`, MIN(purchases)\nFROM `my_dataset`\nGROUP BY `Group` \n--------------------------------------------------------------\n10- \nSELECT c_ID , SUM(purchases) AS total_purchases\nFROM my_dataset\nWHERE region = 'TUNIS' \n--------------------------------------------------------------\n11- \nSELECT * FROM my_dataset\nWHERE Group = 'x' AND purchases < 20 \n\nSELECT * FROM my_dataset\nWHERE region = 'Monastir' OR purchases = AVG(purchases)  \n\nSELECT * FROM my_dataset\nWHERE ( Group = 'x' AND purchases < 20 )\nOR ( region = 'Monastir' AND purchases < 200 ) \n---------------------------------------------------------------\n12- \nSELECT count(gender), max(revenu), min(revenu), avg(revenu), count(behavior)\nFROM my_dataset\nWHERE region IN ('TUNIS', 'ARIANA','BEN AAROUS' ,'MANOUBA') \n--------------------------------------------------------------------------------\n14- \nSELECT *\nFROM my_dataset\nWHERE Age BETWEEN 18 AND 24 \n--------------------------------------------------------------------------------\n15- \n#la liste des clients qui ont des purchases plus de 40 :\n\nSELECT c_ID, SUM(purchases)\nFROM my_dataset\nGROUP BY c_ID\nHAVING SUM(purchases) > 40  \n\nSELECT region, SUM(purchases)\nFROM my_dataset\nGROUP BY region\nHAVING SUM(purchases) > 40  \n\nSELECT Group, SUM(purchases)\nFROM my_dataset\nGROUP BY Group \nHAVING SUM(purchases) > 40  \n\n-------------------------------------------------------------------------------------------------\n16- \nwe can compute a moving average that averages the previous three periods for example :\n\nselect period, \n       purchases, \n       avg(purchases) over (order by period rows between 3 preceding and current row)\nfrom my_dataset;\n \n---------------------------------------------------------------------------------------------------\n17- \nTop client Per Region    : it gives like a bonus or gold client per region ! ((Graph to show in Map the top client )) \nselect *\nfrom\n(\n     select region, \n            c_ID, \n            rank() over(partition by region order by sum(purchases) desc) as ranking\n     from my_dataset \n     group by region, \n              c_ID \n\n) temp\nwhere ranking = 1\n------------------------------------------------------------------------------------------------------\n18- \n#select top 10  purchases\t\nSELECT TOP (10) c_ID, \n                purchases, \n                region, \n                Group\nFROM My_dataset\nORDER BY purchases;\n-------------------------------------------------------------------------------------------------------\n19- \n#the top K purchasess along with their ranking.( K = 10 , TOP 10 ) \n \n SELECT c_ID, purchases, \n        RANK() OVER(ORDER BY purchases DESC),\n        DENSE_RANK() OVER(ORDER BY purchases DESC),\n        ROW_NUMBER() OVER(ORDER BY purchases DESC)  \n FROM my_dataset\n FETCH FIRST K ROWS ONLY\n-------------------------------------------------------------------------------------------------------\n20- \nfind out which region//GROUP//SUBSCRIPTION has the highest average spening//behavior. The following query groups the data by region, determines the average purchases for each region, and ranks the resulting averages. \n\nSELECT region, INT(AVG(purchases)) AS AVERAGE, \n         RANK() OVER(ORDER BY AVG(purchases) DESC) AS AVG_purchases \n   FROM My_dataset \n   GROUP BY  region \n\nSELECT Group, INT(AVG(purchases)) AS AVERAGE, \n         RANK() OVER(ORDER BY AVG(purchases) DESC) AS AVG_purchases \n   FROM My_dataset \n   GROUP BY  Group\n\nSELECT Subscription, INT(AVG(purchases)) AS AVERAGE, \n         RANK() OVER(ORDER BY AVG(purchases) DESC) AS AVG_purchases \n   FROM My_dataset \n   GROUP BY  Subscription \n----------------------------------------------------------------------------------------------------------\n21- \nThe list of employees along with how their behavior//purchases ranks within their regions. \n\n\nSELECT c_ID, region, behavior, \n        DENSE_RANK() OVER(PARTITION BY region ORDER BY behavior DESC) \n          AS behavior_RANK_IN_region \n   FROM my_dataset\n   WHERE region IN ('TUNIS', 'ARIANA','BEN AAROUS' ,'MANOUBA')  \n\nSELECT c_ID, region, purchases, \n        DENSE_RANK() OVER(PARTITION BY region ORDER BY purchases DESC) \n          AS purchases_RANK_IN_region \n   FROM my_dataset\n   WHERE region IN ('TUNIS', 'ARIANA','BEN AAROUS' ,'MANOUBA')  ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CREDITDS : ","metadata":{}},{"cell_type":"markdown","source":"[GitHub SQL-for-Data-Science](https://github.com/trentparkinson/SQL-for-Data-Science)","metadata":{}}]}