{"cells":[{"metadata":{"_uuid":"9f3a357c-3318-4aac-885a-882f2c35f69a","_cell_guid":"1c7c112f-dc2c-4e90-bfab-69f2f2b7b3fd","trusted":true},"cell_type":"markdown","source":"# Autori","execution_count":null},{"metadata":{"_uuid":"4b8cde19-48ce-490a-859a-727d4427459b","_cell_guid":"e5f6c3d5-3fcf-49f6-a359-cc90eefb0024","trusted":true},"cell_type":"markdown","source":"* Rossi Giorgio, matricola: 928570\n* Repetto Valeria, matricola: 939740","execution_count":null},{"metadata":{"_uuid":"c5995ffa-296a-4ac7-b962-4058c5161e32","_cell_guid":"81351a8f-70eb-4a0c-b4dd-c88c8d3c0923","trusted":true},"cell_type":"markdown","source":"# Sommario","execution_count":null},{"metadata":{"_uuid":"e4b15681-3efc-4da4-a636-2a5218a6b13a","_cell_guid":"2d4750a1-ee5d-4057-a602-df0488436eab","trusted":true},"cell_type":"markdown","source":"Sulla base del tutorial https://www.tensorflow.org/tutorials/generative/dcgan#what_are_gans, nel seguente progetto è stato deciso di utilizzare un dataset differente per generare volti umani, utilizzando immagini disponibili al link(https://www.kaggle.com/jessicali9530/celeba-dataset).","execution_count":null},{"metadata":{"_uuid":"13fdfc35-df74-4e03-90f4-f69a667d2128","_cell_guid":"d8770f6e-57eb-45b6-8ff1-e4440b15fd07","trusted":true},"cell_type":"markdown","source":"# DC-GAN","execution_count":null},{"metadata":{"_uuid":"f0a73c8f-669e-4a94-9ade-46afd9f49029","_cell_guid":"58f0be3e-5326-4df6-996c-5598778a11f5","trusted":true},"cell_type":"markdown","source":"Le GAN (Generative Adversial Networks) sono dei network generativi costituiti da due modelli, il generatore e il discriminatore, rappresentati da due reti neurali differenti, che vengono allenate simultaneamente in maniera avversa: il generatore impara a produrre dei dati(generalmente immagini, video o audio) che somiglino più possibile a quelli reali mentre il discriminatre impara a distinguere i dati reali da quelli falsi. \nIn un sistema così costituito il generatore imparera a creare,a partire da un rumore iniziale, dati con l'obbiettivo di \"ingannare\" il discriminatore, e quest'ultimo \"guida\" la sua contro parte a creare immaginisempre più realistiche.\n\nLe DC GAN(Deep Convolutional GAN) utilizzano delle tecniche del deep learning nel training delle GANs. Queste sono principalmente i Convolutional layers al fine di aumentare e diminuire la dimensione spaziale delle features del problema, e la BatchNormalization che viene utilizzata per normalizzare i features vectors a media e varianza unitaria in ogni layer. Queste servono a dare stabilità al problema di learning.","execution_count":null},{"metadata":{"_uuid":"bdc8154b-415b-403c-af12-4573b049cd2b","_cell_guid":"2483e8a1-80ce-4f44-b977-92d01a25a0e8","trusted":true},"cell_type":"markdown","source":"> # Estrazione delle immagini dal dataset","execution_count":null},{"metadata":{"_uuid":"476cb760-c5e2-4ef6-aa48-5623bc37a167","_cell_guid":"37ef66b4-0b86-43b7-a942-8efadd4ddd4f","trusted":true},"cell_type":"markdown","source":"Le immagini sono state tagliate in maniera tale da ottenere solo l'ovale del viso, con l'obiettivo di ridurre le informazioni inutili e quindi rumore. In seguito, per rapidità computazionale, le immagini sono state portate ad una dimensione di (28x28x1), eliminando dunque i colori e tenendo solo la scala dei grigi ed i valori dei pixel sono stati normalizzati tra -1 e 1.","execution_count":null},{"metadata":{"_uuid":"65c24ee4-694e-47d4-a216-852a1e04aa2b","_cell_guid":"7d38e17f-9a05-4aaa-b665-bde8b66d3a83","trusted":true},"cell_type":"code","source":"import glob\nimport imageio\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport PIL\nfrom PIL import Image\nfrom tensorflow.keras import layers\nimport time\nimport tensorflow as tf\nfrom IPython import display\n\npath_celeb = []\ntrain_path_celeb = \"/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/\"\nprint(len(os.listdir(train_path_celeb)))\n\nfor path in os.listdir(train_path_celeb):\n    if '.jpg' in path:\n        path_celeb.append(os.path.join(train_path_celeb, path))\n       \nnew_path=path_celeb[0:40000]\ncrop = (30, 55, 150, 175) #croping size for the image so that only the face at centre is obtained\n#images = [np.array((Image.open(path).crop(crop)).resize((28,28))) for path in new_path]\n#images = [np.array((Image.open(path)).resize((28,28))) for path in new_path]\n\nimages = [np.array((Image.open(path).crop(crop)).resize((64,64))) for path in new_path]\nprint('min e max',np.array(images).min(),np.array(images).max())\nplt.imshow(images[3])\n\n\"\"\"\ndef rgb2gray(rgb):\n\n    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n\n    return gray\nfor i in range(len(images)):\n    images[i]=rgb2gray(images[i])\n    \n\n    \nprint(np.array(images).shape)\n \"\"\"\nfor i in range(len(images)):\n    images[i] = ((images[i] - images[i].min())/(255 - images[i].min()))\n    images[i] = images[i]*2-1\n    \nimages = np.array(images)\ntrain_images = images.reshape(images.shape[0], 64, 64, 3).astype('float32')\nprint('min e max',train_images.min(),train_images.max())\n\nprint(images.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"975c0c7e-866e-44e0-885a-e62f9593cc01","_cell_guid":"40215eaa-7248-40fd-9931-f3ebacc1e0bb","trusted":true},"cell_type":"markdown","source":"Di tutte le immagini disponibli è stato preso un campione di 50000 foto.","execution_count":null},{"metadata":{"_uuid":"b6b04188-84ac-4fc2-9335-d0c25b98cbe6","_cell_guid":"8e8b6be2-b8a7-46d7-94a5-f04550062fe2","trusted":true},"cell_type":"code","source":"BUFFER_SIZE = 50000\nBATCH_SIZE = 256\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12af3711-0c2b-4a7a-afc0-2b53cf4fd3df","_cell_guid":"baf14939-92aa-4396-8fcf-7c3eb7b658e3","trusted":true},"cell_type":"markdown","source":"> # Creazione modelli","execution_count":null},{"metadata":{"_uuid":"8a001031-25de-4447-a8b1-59899a78ebac","_cell_guid":"c65ac615-d24c-40d8-9db1-1041c4f4e8b4","trusted":true},"cell_type":"markdown","source":"> > # Generatore\n\nIl generatore è in grado di creare un immagine a partire da un rumore random. Il modello è costruito a partire da uno strato Dense, che prende il rumore in input, e successivamente esegue un processo di upsampling successivi fino a raggiungere un immagine delle dimensioni desiderate, nel nostro caso(28X28X1). Per ogni strato, ad eccezione di quello finale che usa tanh, la funzione di attivazione usata è stata la LeakyReLU, largamente utilizzata nelle reti neurali perchè ovvia il problema di saturazione del gradiente.","execution_count":null},{"metadata":{"_uuid":"c0f538eb-656f-47be-9510-4776d501af35","_cell_guid":"359af454-03af-464d-93da-474186f13eb4","trusted":true},"cell_type":"code","source":"\ndef make_generator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(4*4*1024, use_bias=False, input_shape=(100,)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Reshape((4, 4, 1024)))\n    assert model.output_shape == (None, 4, 4, 1024) # Note: None is the batch size\n\n    model.add(layers.Conv2DTranspose(512,(5,5), strides=(1, 1), padding='same', use_bias=False))\n    #assert model.output_shape == (None, 8, 8, 512)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(256, (5,5), strides=(2, 2), padding='same', use_bias=False))\n    #assert model.output_shape == (None, 16, 16, 256)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    \n    \n    model.add(layers.Conv2DTranspose(128, (5,5), strides=(2, 2), padding='same', use_bias=False))\n   # assert model.output_shape == (None, 32, 32, 128)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    \n    model.add(layers.Conv2DTranspose(64, (5,5), strides=(2, 2), padding='same', use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(3,(5,5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf8d4a4c-c443-4360-b059-324a866568e2","_cell_guid":"c656e5f6-e159-4c2b-88e9-bb3f89d74eda","trusted":true},"cell_type":"markdown","source":"Di seguito un esempio di immagine generata a partire da un rumore random","execution_count":null},{"metadata":{"_uuid":"ea8c46f5-4633-452f-9fc1-15be11ddeab8","_cell_guid":"b87bf4e1-c64b-4116-9c4a-3c6e4893d373","trusted":true},"cell_type":"code","source":"generator = make_generator_model()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"samples = 10\nx_fake = generator.predict(np.random.normal(loc=0, scale=1, size=(samples,100)))\nprint(x_fake.shape)\nfor k in range(samples):\n    plt.subplot(2, 5, k+1)\n    plt.imshow(x_fake[k].reshape(64,64,3))\n    plt.xticks([])\n    plt.yticks([])\n\n        \nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"_uuid":"efe25400-87dc-4a4c-95a7-f7d9a8445bb3","_cell_guid":"e14fd3b2-775b-47db-863e-d25226b55bbf","trusted":true},"cell_type":"markdown","source":"> > # Discriminatore\n\nIl discriminatore segue il medello di una rete per classificazione di immagini con convolutional neural networks.\nIl modello verrà allenato in modo da classificare con un valore positivo le immagini reali, e conseguentemente con un numero negativo le immagini false.","execution_count":null},{"metadata":{"_uuid":"ba7fe4f8-8f7b-4920-a35f-81973eb91ed3","_cell_guid":"11efd092-902c-4ce8-821d-c9a4b3363049","trusted":true},"cell_type":"code","source":"def make_discriminator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Conv2D(32,(5,5), strides=(2, 2), padding='same',\n                                     input_shape=[64, 64, 3]))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Conv2D(64,(5,5), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n    \n    model.add(layers.Conv2D(128,(5,5), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n    \n    model.add(layers.Conv2D(256,(5,5), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n\n    return model\n\ndiscriminator = make_discriminator_model()\n#decision = discriminator(generated_image)\n#print (decision)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1980458-849b-419d-89c7-bf90847dc246","_cell_guid":"2d68030d-ad71-4805-b576-39bfdcc5f41b","trusted":true},"cell_type":"markdown","source":"> # Loss e ottimizzazione\n\nIl fulcro del funzionamento delle G.A.N. sta, come suggerisce il nome, nella competizione tra generatore e discriminatore, pertanto ciascuno dei due avra la propria loss function così come la propria funzione di ottimizzazione.","execution_count":null},{"metadata":{"_uuid":"2cf9a85e-93a9-406e-9133-45b13edd3b2e","_cell_guid":"f05da3c6-928e-4fd9-a5e7-30ca0d1cf57a","trusted":true},"cell_type":"code","source":"# This method returns a helper function to compute cross entropy loss\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c2356ff-b371-471c-9179-c4a5750d0798","_cell_guid":"9075fde6-3e7b-410f-a524-34e3e4a71919","trusted":true},"cell_type":"markdown","source":"> > # Loss Discriminatore\n\nLa loss function del discriminatore deve dare una stima di quanto il modello usato riesca a distinguere immagini di volti umani  vere da quelle false. Per ottenere ciò la loss confronta la classificazione di immagini vere eseguita dal discriminatore con un array di 1, mentre la decisione presa dallo stesso modello per le immagini create dal generatore verrà confrontata con un array di 0.","execution_count":null},{"metadata":{"_uuid":"7b9c6baa-14d7-46a2-bf82-80b4a3c9902e","_cell_guid":"42f8c21b-acb5-4c56-af78-5c5b942157ef","trusted":true},"cell_type":"code","source":"def discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57aa5b5e-d246-4af9-b591-e2cd9eecf6f7","_cell_guid":"f8486bee-d758-4762-8d54-efe4e5b07b5e","trusted":true},"cell_type":"markdown","source":"> > # Loss Generatore\n\nContrariamente alla sua controparte la loss function del generatore deve riuscire a quantificare quanto bene il modello riesce a \"imbrogliare\" il discriminatore, pertanto la classificazione fatta dal discriminatore viene confrontata con un array di 1.\nIn questo caso il goal del modello è massimizzare la loss.","execution_count":null},{"metadata":{"_uuid":"19f3fce3-4e07-4cc1-8831-86a5ac91022e","_cell_guid":"0cd09348-7517-4a24-9842-7d175e2f2d64","trusted":true},"cell_type":"code","source":"def generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6714510c-3da9-4518-82e7-ba2e5f990914","_cell_guid":"e0cc26d6-3233-4fdb-ada0-53ff5cf742c0","trusted":true},"cell_type":"markdown","source":"> > # Ottimizazione\n\nPer i due modelli sono state create due ottimizzazioni diverse, per le ragioni già esposte sopra.","execution_count":null},{"metadata":{"_uuid":"1c393743-d65b-4f70-8313-b901231fdf24","_cell_guid":"f6e7868c-4073-4c6e-86d5-63b362de89da","trusted":true},"cell_type":"code","source":"generator_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4,beta_1=0.5)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4,beta_1=0.5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10bf3ef8-6b42-4778-b4e2-0113b6eb553f","_cell_guid":"4946c772-ad1a-46d3-918c-1fccb1d38cb2","trusted":true},"cell_type":"markdown","source":"Di seguito è mostrato un chekpoint.","execution_count":null},{"metadata":{"_uuid":"fd7c6f40-5724-4e90-bd0e-cbec2fa45e06","_cell_guid":"0efd03fa-60a1-473d-87db-aa9e26b4c1d1","trusted":true},"cell_type":"code","source":"checkpoint_dir = './training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n                                 discriminator_optimizer=discriminator_optimizer,\n                                 generator=generator,\n                                 discriminator=discriminator)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"39e550e5-fcc8-41bb-8248-c7a48346609f","_cell_guid":"627227c8-afe0-4dd4-8ddf-abf39573dea4","trusted":true},"cell_type":"markdown","source":"# Training loop","execution_count":null},{"metadata":{"_uuid":"5421f893-8936-4008-be52-905f6b1f121a","_cell_guid":"3e4346e0-a562-4dce-ac41-a6786d2f5b3c","trusted":true},"cell_type":"markdown","source":"Il ciclo di training parte da un rumore random che viene dato in pasto al generatore, che lo usa per dare alla luce un'immagine; il discriminatore quindi procede alla classificazione di un'immagine vera, estratta dal data set, e dell'immagine generata in precedenza. \nPer ciascun modello viene calcolata la loss, e conseguentemente il gradiente attravero cui poi i due modelli vengono rispettivamente aggiornati.","execution_count":null},{"metadata":{"_uuid":"9a1ad75a-1d32-4a56-8de5-d63633aa4d8f","_cell_guid":"cd1efc1b-ef39-4a7a-b2d8-d49968f89ece","trusted":true},"cell_type":"code","source":"EPOCHS = 300\nnoise_dim = 100\nnum_examples_to_generate = 16\n\n# We will reuse this seed overtime (so it's easier)\nseed = tf.random.normal([num_examples_to_generate, noise_dim])\n# Notice the use of `tf.function`\n# This annotation causes the function to be \"compiled\".\n@tf.function\ndef train_step(images):\n    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n      generated_images = generator(noise, training=True)\n\n      real_output = discriminator(images, training=True)\n      fake_output = discriminator(generated_images, training=True)\n\n      gen_loss = generator_loss(fake_output)\n      disc_loss = discriminator_loss(real_output, fake_output)\n\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n    return gen_loss, disc_loss","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7311d7cc-97ed-4f70-b9d5-a66850d9a13f","_cell_guid":"cab3384a-e150-400b-8f00-5cbc249c40c0","trusted":true},"cell_type":"markdown","source":"Per ogni step di training è stata salvata la loss di discriminatore e generatore al fine di studiarne l'andamento al variare dell'epoch","execution_count":null},{"metadata":{"_uuid":"8bea321d-a849-409d-bc7d-e811ea438130","_cell_guid":"399d6f92-26f9-4fd5-ad32-b73bce0b7ff3","trusted":true},"cell_type":"code","source":"def train(dataset, epochs):\n  D_loss=[] #list to collect loss for the discriminator model\n  G_loss=[] #list to collect loss for generator model  \n  for epoch in range(epochs):\n    start = time.time()\n    cont=0\n    gen_mediandum=[]\n    disc_mediandum=[]\n    for image_batch in dataset:\n      cont+=1  \n      gen_loss, disc_loss=train_step(image_batch)\n      \n      gen_mediandum.append(gen_loss)\n      disc_mediandum.append(disc_loss)\n      if (cont==int(BUFFER_SIZE/BATCH_SIZE)+1):\n          \n          G_loss.append(np.array(gen_mediandum).mean()) #list to collect loss for generator model \n          D_loss.append(np.array(disc_mediandum).mean()) #list to collect loss for the discriminator model\n          gen_mediandum=[]\n          disc_mediandum=[]\n    # Produce images for the GIF as we go\n    display.clear_output(wait=True)\n    generate_and_save_images(generator,\n                             epoch + 1,\n                             seed)\n\n    # Save the model every 15 epochs\n    if (epoch + 1) % 15 == 0:\n      checkpoint.save(file_prefix = checkpoint_prefix)\n\n    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n  # Generate after the final epoch\n  display.clear_output(wait=True)\n  generate_and_save_images(generator,\n                           epochs,\n                           seed)\n  return G_loss,D_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tensor_to_image(tensor):\n  #tensor = tensor\n  tensor = np.array(tensor, dtype=np.uint8)\n  if np.ndim(tensor)>3:\n    assert tensor.shape[0] == 1\n    tensor = tensor[0]\n  return PIL.Image.fromarray(tensor)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6997f7ba-403c-4f30-b518-7e10cfae6d1f","_cell_guid":"7e2fe16f-c55d-4f0c-92f1-3ddcd2f54198","trusted":true},"cell_type":"code","source":"\ndef generate_and_save_images(model, epoch, test_input):\n  if ((epoch<85) or (epoch>298)):  \n      # Notice `training` is set to False.\n      # This is so all layers run in inference mode (batchnorm).\n      predictions = model(test_input, training=False)\n\n      fig = plt.figure(figsize=(4,4))\n\n      for i in range(predictions.shape[0]):\n          plt.subplot(4, 4, i+1)\n          plt.imshow(predictions[i,:,:,0] )\n         \n          plt.axis('off')\n      plt.suptitle('Image at epoch: %i' %epoch)    \n      plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n      plt.show()\n    \nG_loss,D_loss=train(train_dataset, EPOCHS)\nprint('vabene2')\n\ncheckpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\ndef display_image(epoch_no):\n  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))\ndisplay_image(EPOCHS)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"895a7c89-00f6-44b1-a3c8-9f0c30682684","_cell_guid":"bb76f0a9-40b3-40d5-9166-2ad4ee92ca3c","trusted":true},"cell_type":"markdown","source":"# Studio delle funzioni di perdita","execution_count":null},{"metadata":{"_uuid":"cdf348f4-b73c-48ab-8862-0831ce9218ee","_cell_guid":"865060fc-e084-472a-a16b-bb077883a8db","trusted":true},"cell_type":"markdown","source":"Per trovare il numero ottimale di epoch da utilizzare, sono stati inseriti in un grafico le funzioni di loss del generatore e del discriminatore e la loro differenza(diff_loss).","execution_count":null},{"metadata":{"_uuid":"a09649fd-243a-49ea-a216-03049776ed7a","_cell_guid":"51a63852-adf7-440f-bd8c-49fe9cfd1102","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.plot(G_loss,color='red',label='Generator_loss')\nplt.plot(D_loss,color='blue',label='Discriminator_loss')\nplt.legend()\nplt.xlabel('epochs')\nplt.ylabel('loss') \nplt.title('Model loss per epoch')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6eb98e65-cc50-435a-895b-211bb0483775","_cell_guid":"1b7b0b96-f1df-4954-acfb-f984d9628c8d","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\ndiff_loss=(np.array(G_loss)-np.array(D_loss))\nplt.plot(diff_loss,color='green',label='Diff_loss')\nprint(\"number of epoch for max diff\",max(diff_loss))\nplt.legend()\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.title('Model loss per epoch')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed86b7b0-3a02-4053-b759-0f4c4d771511","_cell_guid":"a86930d1-e84c-485c-a41b-fe2636514a5f","trusted":true},"cell_type":"markdown","source":"L'algoritmo di ottimizzazione ricerca i valori dei parametri che massimizzino la loss del generatore ed al tempo stesso minimizzino quella del discriminatore. Studiandone la differenza è possibile rendersi conto di come il numero ottimale di epoch si aggiri intorno a 80-85 e di come, man mano che il numero aumenta, avvenga il fenomeno dell'overfitting: questo è stato riscontrato negli esempi generati, dato che le immagini corrispondenti ad alte epoch(esempio 300) sono meno accurate e verosimili ad occhio umano rispetto quelle generate ed epoch ottimale(80)","execution_count":null},{"metadata":{"_uuid":"1bd2bfea-6f29-48c9-bd0d-e0a382b3eeae","_cell_guid":"a21f1692-435a-49f0-8b15-07324e933836","trusted":true},"cell_type":"markdown","source":"# Referenze","execution_count":null},{"metadata":{"_uuid":"f94eb7ad-6d2c-4f55-bab1-10acff04ae67","_cell_guid":"b8069947-7d6b-473e-98f4-3f926e60b1f2","trusted":true},"cell_type":"markdown","source":"* https://www.tensorflow.org/tutorials/generative/dcgan#what_are_gans\n* https://www.kaggle.com/sayakdasgupta/fake-faces-with-dcgans#Reference\n* https://www.freecodecamp.org/news/an-intuitive-introduction-to-generative-adversarial-networks-gans-7a2264a81394/","execution_count":null},{"metadata":{"_uuid":"0fd088c5-d01e-4e8a-b2d5-e964bda7307d","_cell_guid":"e5583473-a203-4da7-b061-34b11896d23c","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}