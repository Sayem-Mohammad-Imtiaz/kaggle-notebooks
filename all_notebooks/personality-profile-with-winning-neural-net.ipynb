{"cells":[{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","_uuid":"6e70600a3fccd615f8f29671db0b147751a30b32"},"cell_type":"markdown","source":"# Predicting Personalities based on tweets"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","_uuid":"449208aa45613cfa16b21ca7147db32c5f93cd48"},"cell_type":"markdown","source":"\n### (MBTI) Myers-Briggs Personality Type Dataset\nIncludes a large number of people's MBTI type and content written by them\n\n### About this Dataset\n#### Context\nThe Myers Briggs Type Indicator (or MBTI for short) is a personality type system that divides everyone into 16 distinct personality types across 4 axis:\n\nIntroversion (I) – Extroversion (E) -- Mind\nIntuition (N) – Sensing (S) -- Energy\nThinking (T) – Feeling (F) -- Nature\nJudging (J) – Perceiving (P) -- Tactics\n\n#### Content\nThis dataset contains over 8600 rows of data, on each row is a person’s:\n* Type (This persons 4 letter MBTI code/type)\n* A section of each of the last 50 things they have posted (Each entry separated by \"|||\" (3 pipe characters))\n\n#### Acknowledgements\nThis data was collected through the PersonalityCafe forum, as it provides a large selection of people and their MBTI personality type, as well as what they have written.\n\n#### Inspiration\nSome basic uses could include:\n\nUse machine learning to evaluate the MBTIs validity and ability to predict language styles and behaviour online.\nProduction of a machine learning algorithm that can attempt to determine a person’s personality type based on some text they have written.\n\n---"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","_uuid":"129a6e7fda1b78db8f3f0d6403b0ff37ffbec810"},"cell_type":"markdown","source":"# Import libraries"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","trusted":true,"_uuid":"cd30e904f3e6224d75761311f30f4ce9b1250f97","collapsed":true},"cell_type":"code","source":"# Basic imports\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport string\nimport re\nimport pickle\nfrom sklearn.externals import joblib\n\n# NLP imports\nimport nltk\n\n# SK-Learn\n# NLP and preprocessing\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.feature_extraction.text import (TfidfVectorizer,\n                                             TfidfTransformer,\n                                             CountVectorizer)\n\n# Classifiers and ML\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.preprocessing import (FunctionTransformer,\n                                   StandardScaler,\n                                   label_binarize)\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.model_selection import (train_test_split,\n                                     GridSearchCV,\n                                     cross_val_score)\nfrom sklearn import metrics\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import (ExtraTreesClassifier,\n                              RandomForestClassifier,\n                              AdaBoostClassifier,\n                              GradientBoostingClassifier,\n                              BaggingClassifier)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import (LinearDiscriminantAnalysis,\n                                           QuadraticDiscriminantAnalysis)\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom mlxtend.preprocessing import DenseTransformer\n\n# Show plots within notebook\n%matplotlib inline\n\n# Set plotting style\nsns.set_style('white')\n","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"934a2d605b61ff4cd8d34cfded6588eb6b5a8ca0"},"cell_type":"markdown","source":"# Import, inspect and clean training data"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","_uuid":"93618b8f740fbc3101b9e6f44f98736e0090436e"},"cell_type":"markdown","source":"For the purpose of this sprint our dataset has already been split into a test and training set. \nThe organization of the data for this sprint will be as follow:\n\n* The training data will be split into a training, validation and test sets. The models will be trained on the training sets, and through cross validation of the validation sets will be optimised. The test sets will only be used at the end to assess the performance of the model, and there will thus be no leakage of these sets into the models\n* The unlabeled test set will then lastly be used to predict new labels, and assess the performance of the best constructed model."},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","trusted":true,"_uuid":"16ad401e7804f383bc7681093bc6db30167921e0","collapsed":true},"cell_type":"code","source":"train = pd.read_csv('../input/mbti_1.csv')\n","execution_count":5,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","scrolled":false,"trusted":true,"_uuid":"818e320eb19a6b1508ccbfc417b3a5041142b3c6","collapsed":true},"cell_type":"code","source":"all_personalities = train.copy()\nall_personalities.info()\n","execution_count":6,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"159c0c424f2f0a66711f2fb879c70947b47ab11a","collapsed":true},"cell_type":"code","source":"all_viz = all_personalities.copy()\n\nall_viz['mind'] = [word[0] for word in all_viz['type']]\nall_viz['energy'] = [word[1] for word in all_viz['type']]\nall_viz['nature'] = [word[2] for word in all_viz['type']]\nall_viz['tactics'] = [word[3] for word in all_viz['type']]\n\ncount_person = all_viz.groupby('type').count().sort_values('posts',\n                                                           ascending=False)\n","execution_count":6,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dbc41b5e440d7d26c748e6f133b10a876de31ed7","collapsed":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(12, 8))\ncount_person['posts'].plot(kind='bar', edgecolor='black',\n                           linewidth=1.2, width=0.8)\nax.set_xlabel('Personalities')\nax.set_ylabel('Number of people')\nax.set_title('Number of posts per personality type');\n","execution_count":7,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"26ba1b06c60dd262432b097ad13173d9ac1ae8cb"},"cell_type":"code","source":"def count_person(df, type, one, two):\n    \"\"\"Count the number of individuals with each of the type personality\n    type at each of the four axes\n\n    Parameters:\n    -----------\n    df -- Input datafram containing posts and personality types\n    type -- The personality axis to be assessed\n    one -- The first personality in this axis\n    two -- The second personality in this axis\n\n    \"\"\"\n    one_count = 0\n    two_count = 0\n    for i in df[type]:\n        if i == one:\n            one_count += 1\n        else:\n            two_count += 1\n\n    return one_count, two_count\n","execution_count":8,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fad1920b8af64818713a0f14c70dd6bea4636922","collapsed":true},"cell_type":"code","source":"i_count, e_count = count_person(all_viz, 'mind', 'I', 'E')\nn_count, s_count = count_person(all_viz, 'energy', 'N', 'S')\nf_count, t_count = count_person(all_viz, 'nature', 'F', 'T')\nj_count, p_count = count_person(all_viz, 'tactics', 'J', 'P')\n\npersonality_axes = ['mind', 'energy', 'nature', 'tactics',\n                    'mind_l', 'energy_l', 'nature_l', 'tactics_l']\n\ncount_axes = pd.DataFrame([[i_count, e_count], [n_count, s_count],\n                           [f_count, t_count], [j_count, p_count],\n                           ['I', 'E'], ['N', 'S'],\n                           ['F', 'T'], ['J', 'P']],\n                          index=personality_axes)\ncount_axes = count_axes.T\ncount_axes\n","execution_count":9,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c7e23202544fd83f2914d8ad924b23aeb7c7453","collapsed":true},"cell_type":"code","source":"plt.rcParams.update({'font.size': 14})\n\nf, ax = plt.subplots(figsize=(8, 8), nrows=2, ncols=2)\nplt.tight_layout(pad=3)\ncols = ['skyblue', 'sandybrown']\n\nax[0, 0].bar(count_axes['mind_l'], count_axes['mind'],\n            edgecolor='black', linewidth=1.2, width=0.8, color=cols)\nax[0, 0].set_xlabel('Mind')\nax[0, 0].set_ylabel('Number of people')\nax[0, 0].set_title('Number of posts for all four personality axes')\n\nax[0, 1].bar(count_axes['energy_l'], count_axes['energy'],\n            edgecolor='black', linewidth=1.2, width=0.8, color=cols)\nax[0, 1].set_xlabel('Energy')\nax[0, 1].set_ylabel('Number of people')\n\nax[1, 0].bar(count_axes['nature_l'], count_axes['nature'],\n            edgecolor='black', linewidth=1.2, width=0.8, color=cols)\nax[1, 0].set_xlabel('Nature')\nax[1, 0].set_ylabel('Number of people')\n\nax[1, 1].bar(count_axes['tactics_l'], count_axes['tactics'],\n            edgecolor='black', linewidth=1.2, width=0.8, color=cols)\nax[1, 1].set_xlabel('Tactics')\nax[1, 1].set_ylabel('Number of people');","execution_count":10,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2c50ec7121041bd16ddbd48693a63a22bd41d9b3"},"cell_type":"code","source":"twt_tkn = TweetTokenizer()\nall_viz['tkn_s'] = all_viz.apply(lambda row: twt_tkn.tokenize(row['posts']),\n                                 axis=1)\nall_viz['lenght_words'] = all_viz['tkn_s'].apply(len)\n","execution_count":11,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"503b7ced9169559748a6c4b9cada4a4a2bd3e959","collapsed":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10, 6))\nall_viz['lenght_words'].hist(bins=20, edgecolor='black',\n                             linewidth=1.2, grid=False,\n                             color='skyblue')\nax.set_xlabel('Number of words')\nax.set_ylabel('Number of persons')\nax.set_title('The number of words posted by each person');\n","execution_count":12,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0539c573852e1819e2cbaabdd2c3c0cf7278bb41","collapsed":true},"cell_type":"code","source":"all_viz[['lenght_words', 'type']].hist(bins=20, by='type',\n                                       edgecolor='black',\n                                       linewidth=1.2,\n                                       grid=False,\n                                       color='skyblue',\n                                       figsize=(10, 10));\n","execution_count":13,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","_uuid":"d5c58d73c009226e481903422cf045d80fd6eca5"},"cell_type":"markdown","source":"# Natural Language Processing "},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","trusted":true,"_uuid":"899058f4001accb4b468d73b4ed96b43a25db83f","collapsed":true},"cell_type":"code","source":"all_personalities.columns = ['personality', 'post']\n\nall_personalities.head()\n","execution_count":14,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","_uuid":"49747ad9eb476400e1c00114f4304c9e63e49709"},"cell_type":"markdown","source":"Add labels to df which has all the samples and create seperate, subsampled, df's for each of the personality axes"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","trusted":true,"collapsed":true,"_uuid":"461ba3c2e84b21704720b530bda1833a8920def6"},"cell_type":"code","source":"def add_indi_labels(X):\n    \"\"\"Add individual labels for all four personality axes.\n\n    Parameters:\n    -----------\n    X -- Dataframe containing a column 'type' which contains the Meyers\n    Briggs's Type Indicator parsonality in the format 'INFJ'\n\n    Returns:\n    -----------\n    full_df -- A dataframe which now includes four additional columns for each\n    personality axis. These columns are encoded in 0's and 1's, corresponding\n    to the labels of the new columns, with 1 indicating that the sample has\n    said label and 0 indicating the complement personalit at that asix.\n    \"\"\"\n\n    full_df = X\n    full_df['I'] = full_df['type'].apply(lambda x: x[0] == 'I').astype('int')\n    full_df['N'] = full_df['type'].apply(lambda x: x[1] == 'N').astype('int')\n    full_df['F'] = full_df['type'].apply(lambda x: x[2] == 'F').astype('int')\n    full_df['J'] = full_df['type'].apply(lambda x: x[3] == 'J').astype('int')\n\n    return full_df\n\n\ndef subsample(X, pers_axis):\n    \"\"\"Create individual dataframes for each personlaity axis, and subsample\n    to the personality which is the lowest in that axis.\n\n    Parameters:\n    -----------\n    X -- Dataframe containing a column 'personality' which contains the Meyers\n    Briggs's Type Indicator parsonality in the format 'INFJ'\n    pers_axis -- String with the personality axis of which the the new\n    dataframe should be made, options are 'mind', 'energy', 'nature', 'tactics'\n\n    Returns:\n    -----------\n    subsampled_df -- A Dataframe specific to a personality axis, which contains\n    only the posts per individual and a column corresponding to the personality\n    axis specified in pers_axis. The labels in the personality axis have been\n    binerized, where 1's indicate identity with label and 0's indicate the\n    other personality.\n    \"\"\"\n\n    f_df = X.copy()\n    f_df['mind'] = [word[0] for word in f_df.loc[:, 'personality']]\n    f_df['energy'] = [word[1] for word in f_df.loc[:, 'personality']]\n    f_df['nature'] = [word[2] for word in f_df.loc[:, 'personality']]\n    f_df['tactics'] = [word[3] for word in f_df.loc[:, 'personality']]\n\n    min_c = f_df.groupby(pers_axis).count().min()\n    max_t = f_df.groupby(pers_axis).count().idxmax()\n    min_t = f_df.groupby(pers_axis).count().idxmin()\n\n    max_df = f_df[f_df.loc[:, pers_axis] == max_t[0]].sample(int(min_c[0]))\n    min_df = f_df[f_df.loc[:, pers_axis] == min_t[0]]\n    new_df = pd.concat([max_df, min_df]).sample(frac=1)\n\n    X = new_df.loc[:, 'post']\n    y = new_df.loc[:, pers_axis]\n\n    labels = label_binarize(y, classes=[str(min_t[0]), str(max_t[0])])\n    code_lbs = np.ravel(labels)\n    X_df = pd.DataFrame(X).reset_index(drop=True)\n    subsampled_df = X_df.join(pd.DataFrame(code_lbs, columns=[max_t[0]]))\n    return subsampled_df\n","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"009b2050229e125c866864fbe5d3c344fcb4f597"},"cell_type":"markdown","source":"Define functions that allows objects to be saved from python and be reloaded"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"cfb144d4464e9abcf2782f018160cbdc50b5dcec"},"cell_type":"code","source":"def save_obj(obj, name):\n    \"\"\"Using pickle, save objects from python.\n\n    Save obj as a pk1 file in the ./obj/ directory within the current\n    directory. Create the obj directory before using this function.\n\n    Parameters:\n    -----------\n    obj -- Object to be saved.\n    name -- Name which object is to be saved as.\n    \"\"\"\n\n    with open('obj/' + name + '.pkl', 'wb') as f:\n        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n\n\ndef load_obj(name):\n    \"\"\"Using pickle, load objects to python.\n\n    Load obj from a pk1 file in the ./obj/ directory within the current\n    directory.This function specifically loads object created using the\n    save_obj function.\n\n    Parameters:\n    -----------\n    name -- Name of object to be loaded, without its extention.\n\n    Returns:\n    -----------\n    Python object previously 'pickled'.\n    \"\"\"\n\n    with open('obj/' + name + '.pkl', 'rb') as f:\n        return pickle.load(f)\n","execution_count":9,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a55699df535a45cf22e762f75382f4dc0beed49f","collapsed":true},"cell_type":"code","source":"df_mind = subsample(all_personalities, 'mind')\ndf_nature = subsample(all_personalities, 'nature')\ndf_energy = subsample(all_personalities, 'energy')\ndf_tactics = subsample(all_personalities, 'tactics')","execution_count":12,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"27ce8c1182575f6f58ff8031904f6246468e1869"},"cell_type":"code","source":"train_labeled = []\ntrain_labeled = add_indi_labels(train)","execution_count":13,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","trusted":true,"_uuid":"3a63659b615a8499afce225cc9aef84d22c4509d","collapsed":true},"cell_type":"code","source":"df_mind.head()","execution_count":11,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","_uuid":"26638fba8db13d73528f5a92e4a44e8fbde6caa3"},"cell_type":"markdown","source":"---"},{"metadata":{"_uuid":"41f68cf872545760557db549fdf28351ebed735f"},"cell_type":"markdown","source":"### Define functions that will preprocess the data, make lists of stopwords and tokenize sentences or posts"},{"metadata":{"_uuid":"ec07fba006b2daa66b66a918c0f2f0b844940f6a"},"cell_type":"markdown","source":"Below are functions to preprocess the twitter posts, make a list of stopwords to be removed, and a custom tokenizer which specifically tokenize tweets, lemmatize words and remove urls to replace them with the domains (e.g., www.youtube.com)\n\nThese function are not directly implemented, but are rather called within the vectorize step performed later."},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","trusted":true,"collapsed":true,"_uuid":"e78aa045d621aa513ccb0fe073eb93ce497b64bc"},"cell_type":"code","source":"def preprocess(stringss):\n    \"\"\"Preprocess text for Natural Language Processing.\n\n    This function preprocesses text data for Natural Language processing\n    by removing punctuation, except emotive punctuation ('!', '?').\n\n    Parameters:\n    -----------\n    stringss -- Input string which is to be processed.\n\n    Returns:\n    -----------\n    String of which the punctuation has been removed.\n    \"\"\"\n\n    import string\n    keep_punc = ['?', '!']\n    punctuation = [str(i) for i in string.punctuation]\n    punctuation = [punc for punc in punctuation if punc not in keep_punc]\n    s = ''.join([punc for punc in stringss if punc not in punctuation])\n    return s\n\n\ndef make_stopwords(other_stopwords, remove_from):\n    \"\"\"Make a list of stopwords to be removed during Natural Language Processing.\n\n    Function which creates a list of words commonly used in the English\n    language, which can be supplemented with more specified (other_stopwords)\n    words, or from which specified words (remove_from) are removed.\n\n    Parameters:\n    -----------\n    other_stopwords -- Additional stopwords to be included in the list of\n    stopwords.\n    remove_from -- Words to be excluded from the list of stopwords\n\n    Returns:\n    -----------\n    stopwords_punc_personality -- A list of stopwords.\n    \"\"\"\n\n    from nltk.corpus import stopwords\n    stopw_all = stopwords.words('english')\n    stopwords_punc = [word for word in stopw_all if word not in remove_from]\n    stopwords_punc_personality = other_stopwords + stopwords_punc\n    return stopwords_punc_personality\n","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"93e6e9d1d6b09d798499413682f5ea893be869de"},"cell_type":"markdown","source":"Create three lists of stopwords to be assessed for their influence on the ability to classify personalities:\n\n* List 1: Contains words commonly used in the english language, all 16 personality types.\n\n* List 2: Contains words commonly used in the english language, except personal and possessive pronouns.\n\n* List 3: Contains all 16 personality types and words commonly used in the english language, except personal and possessive pronouns."},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","trusted":true,"collapsed":true,"_uuid":"da92b453f5b0fe14e76cc8a1eaadd87332d6d22a"},"cell_type":"code","source":"personalities = ['infj', 'intj', 'isfj', 'istj',\n                 'infp', 'intp', 'isfp', 'istp',\n                 'enfj', 'entj', 'esfj', 'estj',\n                 'enfp', 'entp', 'esfp', 'estp', '...']\nempty_list = []\nstopwords_all = make_stopwords(personalities, empty_list)\n\nstops = ['...']\nwords_to_keep = stopwords_all[0:52]\nstopwords_no_pro = make_stopwords(stops, words_to_keep)\n\nstopwords_pers_no_pro = make_stopwords(personalities, words_to_keep)\n","execution_count":21,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","trusted":true,"collapsed":true,"_uuid":"75950b3bb1f2ed425d219934e0571ac1443abb8c"},"cell_type":"code","source":"def tokenize(string, token=TweetTokenizer(), lemma=WordNetLemmatizer()):\n    \"\"\"Tokenize sentences into words.\n\n    Create word tokens from strings. This function uses the TweetTokwnizer by\n    default to tokenize words from sentences, as well as lemmatizing the words\n    and changing full url's to their domains (e.g., change\n    https://www.youtube.com/watch?v=TjJUqWXK68Y will return www.youtube.com).\n\n    Parameters:\n    -----------\n    string -- Input string which is to be processed.\n    tokenzier -- Tokenizer to be used in creating tokens,\n                 default=TweetTokenizer()\n    lemmatizer -- Lemmatizer to be used in lemmatization\n                  default=WordNetLemmatizer()\n\n    Returns:\n    -----------\n    Returns a string of words which have been tokenized, lemmatized and urls\n    which has been removed.\n    \"\"\"\n\n    token = TweetTokenizer()\n    lemma = WordNetLemmatizer()\n\n    tokens = token.tokenize(string)\n    toks = []\n    for i in tokens:\n        lemmas = lemma.lemmatize(i)\n        toks.append(lemmas)\n\n    pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n    websites = []\n    for i in toks:\n        websites.append(re.findall(pattern_url, i))\n    websites2 = [x for x in websites if x]\n    websites3 = []\n    for i in websites2:\n        if str(i).split('/')[2] not in websites3:\n            websites3.append(str(i).split('/')[2])\n\n    for idx, i in enumerate(toks):\n        for a in websites3:\n            if str(a) in i:\n                toks[idx] = a\n\n    df = ' '.join(toks)\n\n    return df\n","execution_count":22,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","_uuid":"412cdc1f6c149a26b2ad5eea5f7b49d58256cf61"},"cell_type":"markdown","source":"# Build classification models"},{"metadata":{"_uuid":"886be9459760cbcc6fbf7d13753004f1678118f7"},"cell_type":"markdown","source":"Perform test train splits on df's from all four axes, and create a dictionary of these train and test sets, each corresponding to a personality axis, which will be used when performing classification."},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","trusted":true,"collapsed":true,"_uuid":"7569e71ff4fcab0aa76a1f0f568025ef751856ab"},"cell_type":"code","source":"X_tr_m, X_te_m, y_tr_m, y_te_m = train_test_split(df_mind['post'],\n                                                  df_mind['I'])\nX_tr_e, X_te_e, y_tr_e, y_te_e = train_test_split(df_energy['post'],\n                                                  df_energy['N'])\nX_tr_n, X_te_n, y_tr_n, y_te_n = train_test_split(df_nature['post'],\n                                                  df_nature['F'])\nX_tr_t, X_te_t, y_tr_t, y_te_t = train_test_split(df_tactics['post'],\n                                                  df_tactics['P'])\n","execution_count":21,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"65a479cf7e77fdb26f947605e486285176e25c90"},"cell_type":"code","source":"train_tests = {'mind': [X_tr_m, X_te_m, y_tr_m, y_te_m],\n                'energy': [X_tr_e, X_te_e, y_tr_e, y_te_e],\n                'nature': [X_tr_n, X_te_n, y_tr_n, y_te_n],\n                'tactics': [X_tr_t, X_te_t, y_tr_t, y_te_t]}\n","execution_count":22,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","_uuid":"d3a0c4bf646281b82d0d21653f351542d0a40c35"},"cell_type":"markdown","source":"### Set up pipelines"},{"metadata":{"_uuid":"778679bf06eaca4f92c76bae85137c9cad25160a"},"cell_type":"markdown","source":"Below an instance is initialized of the TfidfVectorizer with the custom preprocessor, stopwords and tokenizer. \n\nThis vectorizer is then merged with the additional features transformer. This will result in a feature vector including both the additional features and word vector.\n\nLastly, this is combined in a very simple pipeline with a Logistic Regression Classifier to provide a baseline classifier. "},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","trusted":true,"collapsed":true,"_uuid":"7dc71ca4bef5309b43fd8aa5bac0aa3d60a0744b"},"cell_type":"code","source":"vectorizer = TfidfVectorizer(preprocessor=preprocess,\n                             stop_words=stopwords_no_pro,\n                             tokenizer=tokenize)\n\npipeline = Pipeline([\n    ('features', vectorizer),\n    ('classify', LogisticRegression())\n])\n","execution_count":23,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","_uuid":"08195858f9ac3929d14d8d9f6f06c31ad9c85af6"},"cell_type":"markdown","source":"### Run pipelines"},{"metadata":{"_uuid":"6a95cca7f9739046d20953c0493341bdf43ab20c"},"cell_type":"markdown","source":"Below the basic pipeline is run for each of the four personality axes (the reason we have a trains_tests dictionary), and several performance metrics are predicted off the fit model, which is stored in a dictionary, which is subsequently stored in disk. "},{"metadata":{"trusted":true,"_uuid":"bfebeb0d94e2002f49c7147f854bdfcae31b4a5d","collapsed":true},"cell_type":"code","source":"scores = {}\nfor k, i in train_tests.items():\n    pipeline.fit(i[0], i[2])\n    scores[str(k) + '_score'] = pipeline.score(i[0], i[2])\n    predict = pipeline.predict(i[1])\n    predict_train = pipeline.predict(i[0])\n    scores[str(k) + '_confusion'] = metrics.confusion_matrix(i[3], predict)\n    scores[str(k) + '_accuracy_train'] = metrics.accuracy_score(i[2],\n                                                                predict_train)\n    scores[str(k) + '_accuracy_test'] = metrics.accuracy_score(i[3],\n                                                               predict)\n    scores[str(k) + '_prec_test'] = metrics.precision_score(i[3],\n                                                            predict,\n                                                            average='weighted')\n    scores[str(k) + '_recall_test'] = metrics.recall_score(i[3],\n                                                           predict,\n                                                           average='weighted')\n    scores[str(k) + '_f1_train'] = metrics.f1_score(i[2],\n                                                    predict_train,\n                                                    average='weighted')\n    scores[str(k) + '_f1_test'] = metrics.f1_score(i[3],\n                                                   predict,\n                                                   average='weighted')\n","execution_count":59,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"402ffa0453266f2477d126a1bd56bafce537bf7a","collapsed":true},"cell_type":"code","source":"accuracy = pd.DataFrame([scores['mind_accuracy_test'],\n                         scores['energy_accuracy_test'],\n                         scores['nature_accuracy_test'],\n                         scores['tactics_accuracy_test']],\n                        columns=['accuracy'])\naxes = pd.DataFrame(['Mind', 'Energy', 'Nature', 'Tactics'],\n                    columns=['type'])\naccuracy_b = accuracy.join(axes)\n\nf1 = pd.DataFrame([scores['mind_f1_test'],\n                   scores['energy_f1_test'],\n                   scores['nature_f1_test'],\n                   scores['tactics_f1_test']],\n                  columns=['accuracy'])\naxes = pd.DataFrame(['Mind', 'Energy', 'Nature', 'Tactics'],\n                    columns=['type'])\nf1_b = f1.join(axes)\n","execution_count":60,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7180e9bd0c1e60069c90f28b3d7f03b31fa45629","collapsed":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(12, 5), nrows=1, ncols=2)\n\nax[0].bar(accuracy_b['type'], accuracy_b['accuracy'],\n          color=['steelblue', 'khaki', 'mediumseagreen', 'darksalmon'])\nax[0].set_title('Accuracy of base classification model')\nax[1].bar(f1_b['type'], f1_b['accuracy'],\n          color=['dodgerblue', 'gold', 'forestgreen', 'tomato'])\nax[1].set_title('F1 score of base classification model');\n","execution_count":61,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","_uuid":"7732554822cced4003d18bb3eae25fd611161c75"},"cell_type":"markdown","source":"# Model selection, validation and evaluation through gridsearches"},{"metadata":{"_uuid":"1775b761ff7ae11399b9d14db7bdb03d1fcde83d"},"cell_type":"markdown","source":"Because we are going to build numerous models, we will define a function which applies the model to be build to all four personality axes. We will used this function to perform grid searches on each of the personality axes with a set number of parameters."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6ed9963a81df80d069538824d6fc46b89242fad6"},"cell_type":"code","source":"def grid_search_p_model(grid, trains_tests):\n    \"\"\"Perform a gridsearch on multiple binary axes.\n\n    This function performs multiple gridsearches on each axis specified in\n    the train_tests sets. For each of the axes a gridsearch will be performed\n    on the grid instance specified. Following the gridsearch, the models\n    are assessed for the parameters, best score, accuracy, recall, precision\n    and the f1 score. This all is output into a dictionary, with a key\n    specifying the metric and for which model.\n\n    Parameters:\n    -----------\n    grid -- GridSearchCV instance created with specific parameters to be\n    searched\n    trains_tests -- dictionary of which the keys are an axes for binary\n    classification, and the values are test train splits along that axis.\n\n    Returns:\n    -----------\n    Dictionary object where keys indicate the axis along with various metrics.\n    The metrics assessed are:\n        -> Model with best parameters\n        -> Accuracy score of the best model\n        -> The cross validation results\n        -> The train accuracy score\n        -> The confusion matrix\n        -> Test accuracy score\n        -> Precision\n        -> Recall\n        -> F1 Test score\n        -> F1 Train score\n\n    \"\"\"\n    s = {}\n    for k, i in trains_tests.items():\n        grid.fit(i[0], i[2])\n        s[str(k) + '_best_params'] = grid.best_params_\n        s[str(k) + '_best_score'] = grid.best_score_\n        s[str(k) + '_grid_results'] = grid.cv_results_\n        s[str(k) + '_score'] = grid.score(i[0], i[2])\n        predict = grid.predict(i[1])\n        predict_train = grid.predict(i[0])\n        s[str(k) + '_confusion'] = metrics.confusion_matrix(i[3],\n                                                            predict)\n        s[str(k) + '_accuracy_train'] = metrics.accuracy_score(i[2],\n                                                               predict_train)\n        s[str(k) + '_accuracy_test'] = metrics.accuracy_score(i[3],\n                                                              predict)\n        s[str(k) + '_prc_test'] = metrics.precision_score(i[3],\n                                                          predict,\n                                                          average='weighted')\n        s[str(k) + '_recall_test'] = metrics.recall_score(i[3],\n                                                          predict,\n                                                          average='weighted')\n        s[str(k) + '_f1_train'] = metrics.f1_score(i[2],\n                                                   predict_train,\n                                                   average='weighted')\n        s[str(k) + '_f1_test'] = metrics.f1_score(i[3],\n                                                  predict,\n                                                  average='weighted')\n    return s\n","execution_count":23,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","_uuid":"42efb8a5b2847214c52c6b9d72cb1a4019020255"},"cell_type":"markdown","source":"### Does my own NLP and additional features actually increase the performance of my model?\n\n#### We first check if removing the stopwords has an influence, tokenization using a custom tokenizer and preprocessing the data\n\nBelow we perform grid searched, which takes in lists of parameters for which each permutation of those parameters will be used to assess the performance of the classifier. The output of a gridseach is a dictionary which provide test and train metrics for all the models tested. Additionally, the resulting grid object is fit with the best model and can further be used to make predictions as in all sklearn classifiers. \n\nThe aim of this gridsearch is thus to assess whether custom preprocessing, tokenization and removing stopwords have an influence on classification."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fb9e351c96f3ddecc94a2b30ada878cee7396bdc"},"cell_type":"code","source":"param_grid = {'features__stop_words': [stopwords_all,\n                                       stopwords_no_pro,\n                                       stopwords_pers_no_pro,\n                                       None],\n              'features__tokenizer': [tokenize, None],\n              'features__preprocessor': [preprocess, None]}\n\nscoring = ['accuracy', 'precision', 'recall', 'roc_auc', 'f1']\ngrid = GridSearchCV(pipeline, param_grid, refit='accuracy', verbose=4,\n                    n_jobs=-1, scoring=scoring, return_train_score=True)\n","execution_count":63,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"585ee019f51c9775fedb85ec9b633e37fae2524c","collapsed":true},"cell_type":"code","source":"scr_tkn = grid_search_p_model(grid, train_tests)","execution_count":64,"outputs":[]},{"metadata":{"_uuid":"a31f636d17ab6dca0a4af095a9acd8b0b881e4a3"},"cell_type":"markdown","source":"This is an example of the features selected by the model, the preprocess function, stopwords_all as stopwords and no tokeniation function."},{"metadata":{"trusted":true,"_uuid":"df5feaf93523bd924149ce486871cea7af2a5045","collapsed":true},"cell_type":"code","source":"scr_tkn['mind_best_params']","execution_count":65,"outputs":[]},{"metadata":{"_uuid":"0c25d70728568037b8b1d6ef63b37248dac04ab7"},"cell_type":"markdown","source":"Here, overall the model that performed best did not make use of the custor vectorizer, but included the custor preprocessor, and the removal of stopwords (The 1st list of stopwords)."},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","_uuid":"9453e123643bd812e8ed70d47f89d029afb64c51"},"cell_type":"markdown","source":"### How are we going to change the vectorizer to increase the performance of the model\n\nThe aim of this gridsearch is to find hyperparameters for which the vectorizer performs best."},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","trusted":true,"collapsed":true,"_uuid":"e8a567f61b22069a5cfef58c18e709b5602a02e0"},"cell_type":"code","source":"pipeline = Pipeline([\n    ('features', TfidfVectorizer(preprocessor=preprocess,\n                                 stop_words=stopwords_all)),\n    ('classify', LogisticRegression())\n])\n\nparam_grid = {'features__min_df': [0.0, 0.1, 0.2, 0.3],\n              'features__max_features': [1000, 5000,\n                                         50000, 100000],\n              'features__ngram_range': [(1, 1), (1, 2), (1, 3)]}\n\nscoring = ['accuracy', 'precision', 'recall', 'roc_auc', 'f1']\ngrid = GridSearchCV(pipeline, param_grid, refit='accuracy',\n                    verbose=4, n_jobs=-1,\n                    scoring=scoring, return_train_score=True)\n","execution_count":66,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","trusted":true,"_uuid":"1211f6641791f83cc3b11491d3d8c340fe6f2459","collapsed":true},"cell_type":"code","source":"scr_vec = grid_search_p_model(grid, train_tests)\n","execution_count":67,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8921a8e3cbad67a841008596068a028047c278ad","collapsed":true},"cell_type":"code","source":"print('The mind axis:', scr_vec['mind_best_params'])\nprint('The energy axis:', scr_vec['energy_best_params'])\nprint('The nature axis:', scr_vec['nature_best_params'])\nprint('The tactics axis:', scr_vec['tactics_best_params'])\n","execution_count":68,"outputs":[]},{"metadata":{"_uuid":"d29922a5ead5877fc9b77589b0ea3edb251800ed"},"cell_type":"markdown","source":"Here, the model that performed best were variable across all axes, with differences in the number of features and n_grams.\nWhen we build a final model, we will assess this again and use the most optimal parameters for each axis. The addition of n_grams to the model greatly increases the memory load, and computational complexity, and will be excluded from the current model."},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","_uuid":"4eff86cef0d43b1f0e2dd027a6692bdaa5c8c86d"},"cell_type":"markdown","source":"### Which classifier performs best with base assumptions\n\nBefore going into detail with each classifier, we assess which classifier perform best with base assumptions. \nTo increase computational performance we reduce the dimentionality of the input dataset using singular value decomposition into 1000 dimnetions, and vectorize using the above identified hyperparameters."},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","trusted":true,"collapsed":true,"_uuid":"eef57bd937f27852ab1a47ae3e8eb48f455d2aac"},"cell_type":"code","source":"vectorizer = TfidfVectorizer(preprocessor=preprocess, stop_words=stopwords_all)\n\npipeline_classif = Pipeline([\n    ('features', vectorizer),\n    ('densify', DenseTransformer()),\n    ('svd', TruncatedSVD(n_components=1000)),\n    ('classify', LogisticRegression())\n])\n\nparam_grid = {'classify':[\n    LogisticRegression(),\n    KNeighborsClassifier(3),\n    SVC(kernel=\"linear\", C=0.025),\n    SVC(gamma=2, C=1),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis(),\n    DecisionTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n    AdaBoostClassifier(),\n    MLPClassifier(alpha=1, max_iter=2000)\n]}\nscoring = ['accuracy', 'precision', 'recall', 'roc_auc', 'f1']\ngrid = GridSearchCV(pipeline_classif, param_grid, refit='accuracy', verbose=4,\n                    n_jobs=4, scoring=scoring, return_train_score=True)\n","execution_count":70,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","trusted":true,"_uuid":"9fee9faae8b35a4ca0f2ba5d77bda6093e49444e","scrolled":true,"collapsed":true},"cell_type":"code","source":"scr_bsc = grid_search_p_model(grid, train_tests)\n","execution_count":71,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"78eb9036af3e4f774babcaf54d17b3dea430697a"},"cell_type":"code","source":"accu = scr_bsc['mind_grid_results']['mean_test_accuracy']\nnames = []\nfor i in scr_bsc['mind_grid_results']['param_classify']:\n    names.append(str(i)[:7])\n\naccu_b = pd.DataFrame([accu, names], index=['score', 'clf']).T\n\ntime = scr_bsc['mind_grid_results']['mean_fit_time']\n\ntime_b = pd.DataFrame([time, names], index=['score', 'clf']).T\n","execution_count":72,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"660fe25080a0e5bd51b9f7de71fb064af0c79a86","collapsed":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(12, 5), nrows=1, ncols=1)\n\nax.bar(accu_b['clf'], accu_b['score'],\n       color=['gold', 'limegreen', 'tan', 'darkorange',\n              'seagreen', 'darkcyan', 'royalblue', 'mediumpurple',\n              'palevioletred', 'orchid', 'crimson'])\nax.set_title('Accuracy of base classification model')\nax.tick_params(rotation=90)\n","execution_count":73,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c83ea3bc24979e0d61e074512c89ffc5b3be3016","collapsed":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(12, 5), nrows=1, ncols=1)\n\nax.bar(time_b['clf'], time_b['score'],\n       color=['gold', 'limegreen', 'tan', 'darkorange',\n              'seagreen', 'darkcyan', 'royalblue', 'mediumpurple',\n              'palevioletred', 'orchid', 'crimson'])\nax.set_title('Time taken to fit base model')\nax.tick_params(rotation=90)\n","execution_count":74,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","_uuid":"1f9d942cf7a7edfa32013f70d5adefb55cbe0941"},"cell_type":"markdown","source":"From this it looks like the Neural Net performs the best of all classifiers. Followed by SVM, KNN and Losgistic Regression.\nLet us run gridsearches for each of these classifiers, and see which one performs best. After which we can compare all the models again and perform an ensamble approach on the best one. "},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","_uuid":"d889379e06e11eded7932a8fce95faf85f42e37a"},"cell_type":"markdown","source":"---"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","_uuid":"047de4430a8e5519599c8bb65b77c3f5c52d38a0"},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{"_uuid":"7cf1064db773ba17bbf2ffc10f3d5bcaeb037805"},"cell_type":"markdown","source":"Below we do a gridsearch to optimize the hyperparameters of a logistic regression classifier. We solve for C, penalty, the solver and the maximum number of features for the vecorizer."},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","trusted":true,"collapsed":true,"_uuid":"fd7384738cca236e122bfa0fc1ec9ae103636e67"},"cell_type":"code","source":"vectorizer = TfidfVectorizer(preprocessor=preprocess,\n                             stop_words=stopwords_no_pro)\n\npipeline_logis = Pipeline([\n    ('features', vectorizer),\n    ('densify', DenseTransformer()),\n    ('svd', TruncatedSVD(n_components=1000)),\n    ('classify', LogisticRegression())\n])\n\npenalty = ['l1', 'l2']\nC = np.logspace(0, 3, 4)\nsolver = ['liblinear', 'saga']\n\nparam_grid = {'classify__C': C,\n              'classify__penalty': penalty,\n              'classify__solver': solver,\n              'features__max_features': [1001, 10000, 50000]}\n\nscoring = ['accuracy', 'precision', 'recall', 'roc_auc', 'f1']\ngrid = GridSearchCV(pipeline_logis, param_grid, refit='accuracy', verbose=4,\n                    n_jobs=4, scoring=scoring, return_train_score=True)","execution_count":33,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"3e7f9b9c5d828fbc5df5d13a51acc5bb7875d94a","collapsed":true},"cell_type":"code","source":"scr_log = grid_search_p_model(grid, train_tests)","execution_count":34,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f5418236232053d279efc1d569e0fb94f0137fe","collapsed":true},"cell_type":"code","source":"print('Mind test F1 score:', scr_log['mind_f1_test'])\nprint('Energy test F1 score:', scr_log['energy_f1_test'])\nprint('Nature test F1 score:', scr_log['nature_f1_test'])\nprint('Tactics test F1 score:', scr_log['tactics_f1_test'])\n","execution_count":35,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","_uuid":"74b8a3219962618b034517c3b3b47e3f72123d5f"},"cell_type":"markdown","source":"---"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","_uuid":"5449330320580e8c529d2a9c1985c2b9aa5e6e97"},"cell_type":"markdown","source":"## K-Nearest Neighbours"},{"metadata":{"_uuid":"8f75246e37198df8b9455fdc6ff0fd02fd0bea4f"},"cell_type":"markdown","source":"Below we do a gridsearch to optimize the hyperparameters of a K-nearest Neighbours classifier. We solve for the number of neighbours to include."},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","trusted":true,"collapsed":true,"_uuid":"6cf291fbc9b8e583843a3ef617cdedf213d9739d"},"cell_type":"code","source":"vectorizer = TfidfVectorizer(preprocessor=preprocess, stop_words=stopwords_all)\n\npipeline_knn = Pipeline([\n    ('features', vectorizer),\n    ('densify', DenseTransformer()),\n    ('svd', TruncatedSVD(n_components=1000)),\n    ('classify', KNeighborsClassifier())\n])\n\nneighbours = [2, 5, 10, 50, 100]\n\nparam_grid = {'classify__n_neighbors': neighbours}\n\nscoring = ['accuracy', 'precision', 'recall', 'roc_auc', 'f1']\ngrid = GridSearchCV(pipeline_knn, param_grid, refit='accuracy', verbose=4,\n                    n_jobs=4, scoring=scoring, return_train_score=True)\n","execution_count":38,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"6159b3ddf81e872267c2c766b29f698e390fd25e","collapsed":true},"cell_type":"code","source":"scores_knn = grid_search_p_model(grid, train_tests)","execution_count":39,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86595f5fe7e5aa8580a469db1fe44514fdba64a4","collapsed":true},"cell_type":"code","source":"print('Mind test F1 score:', scores_knn['mind_f1_test'])\nprint('Energy test F1 score:', scores_knn['energy_f1_test'])\nprint('Nature test F1 score:', scores_knn['nature_f1_test'])\nprint('Tactics test F1 score:', scores_knn['tactics_f1_test'])","execution_count":40,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2fed7b16c55c79c1c54d2df38c7e9eef8ce8420d"},"cell_type":"code","source":"accu = scores_knn['mind_grid_results']['mean_test_accuracy']\nneigh = list(scores_knn['mind_grid_results']['param_classify__n_neighbors'])\n\naccu_b = pd.DataFrame([accu, neigh], index=['score', 'neigh']).T\n","execution_count":41,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47a6870ed6e34e32f455d60e01a8f4434343544d","collapsed":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(8, 6))\n\nplt.plot(accu_b['neigh'], accu_b['score'])\nax.set_title('Accuracy at various number of neighbors')\nax.set_xlabel('Neighbors')\nax.set_ylabel('Test Accuracy');\n","execution_count":65,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","_uuid":"12b26ae27a3631f411963ed805eb7015ca978259"},"cell_type":"markdown","source":"---"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","_uuid":"6cae451c9fed471cb1326edd36287cf220dae9af"},"cell_type":"markdown","source":"## Support Vector Machine"},{"metadata":{"_uuid":"a97dd14a63514440be6cb3f90fc90461ec00c965"},"cell_type":"markdown","source":"Below we do a gridsearch to optimize the hyperparameters of a Support Vector Machine classifier. We solve for C, gamma and the kernel to use."},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","trusted":true,"collapsed":true,"_uuid":"a805e7fb23af94d838cd172865bc96dd65cd5b13"},"cell_type":"code","source":"vectorizer = TfidfVectorizer(preprocessor=preprocess, stop_words=stopwords_all)\n\npipeline_svm = Pipeline([\n    ('features', vectorizer),\n    ('densify', DenseTransformer()),\n    ('svd', TruncatedSVD(n_components=1000)),\n    ('classify', SVC())\n])\n\nkernel = ['linear',  'rbf']\nC = np.logspace(-1, 2, 4)\ngamma = np.logspace(-4, 0, 4)\n\nparam_grid = {'classify__kernel': kernel,\n              'classify__C': C,\n              'classify__gamma': gamma}\n\nscoring = ['accuracy', 'precision', 'recall', 'roc_auc', 'f1']\ngrid = GridSearchCV(pipeline_svm, param_grid, refit='accuracy', verbose=4,\n                   n_jobs=4, scoring=scoring, return_train_score=True)\n","execution_count":43,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","trusted":true,"_uuid":"4947a65fad231f05b4ffbb39c4f36dae2ef25f88","scrolled":true,"collapsed":true},"cell_type":"code","source":"scores_svm = grid_search_p_model(grid, train_tests)\n","execution_count":44,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5d4ca79bc0aad9e0508750d804f73ff0348aa61","collapsed":true},"cell_type":"code","source":"\nprint('Mind test F1 score:', scores_svm['mind_f1_test'])\nprint('Energy test F1 score:', scores_svm['energy_f1_test'])\nprint('Nature test F1 score:', scores_svm['nature_f1_test'])\nprint('Tactics test F1 score:', scores_svm['tactics_f1_test'])\n","execution_count":66,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","_uuid":"d578a2336c388b863fcd0b72d0078433063bc7dc"},"cell_type":"markdown","source":"---"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","_uuid":"0a85e69ce4b56699f4f69996f6fc073a7f95a07d"},"cell_type":"markdown","source":"## Linear Discriminant Analysis"},{"metadata":{"_uuid":"8304011825f9aab3a5a2ad66c08458182f2513e9"},"cell_type":"markdown","source":"Below we do a gridsearch to optimize the hyperparameters of a Linear Discriminant Analysis. We solve for the number of components to be included in the value decomposition."},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","trusted":true,"collapsed":true,"_uuid":"9345575aff8f86a98e3a029774b0131bae1a6e00"},"cell_type":"code","source":"vectorizer = TfidfVectorizer(preprocessor=preprocess, stop_words=stopwords_all)\n\npipeline_lda = Pipeline([\n    ('features', vectorizer),\n    ('densify', DenseTransformer()),\n    ('svd', TruncatedSVD(n_components=1000)),\n    ('classify', LinearDiscriminantAnalysis())\n])\n\nsolver = ['svd', 'lsqr']\nn_components = [2, 50, 100, 500, None]\n\nparam_grid = {'classify__solver': solver,\n              'classify__n_components': n_components}\n\nscoring = ['accuracy', 'precision', 'recall', 'roc_auc', 'f1']\ngrid = GridSearchCV(pipeline_lda, param_grid, refit='accuracy', verbose=4,\n                    n_jobs=4, scoring=scoring, return_train_score=True)\n","execution_count":46,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","trusted":true,"_uuid":"96759da5d4043f451390afda5b78662d3a4bea11","collapsed":true},"cell_type":"code","source":"scores_lda = grid_search_p_model(grid, train_tests)\n","execution_count":47,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21399f63251069c6cba08228598b70aa71cf503a","collapsed":true},"cell_type":"code","source":"\nprint('Mind test F1 score:', scores_lda['mind_f1_test'])\nprint('Energy test F1 score:', scores_lda['energy_f1_test'])\nprint('Nature test F1 score:', scores_lda['nature_f1_test'])\nprint('Tactics test F1 score:', scores_lda['tactics_f1_test'])\n","execution_count":48,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","_uuid":"4e7e47cbab49971b033bfd329c6e0bc60f3a28fd"},"cell_type":"markdown","source":"---"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","_uuid":"eafd11a3c26f8551ceb85aa0e5ea682148b83edc"},"cell_type":"markdown","source":"## Neural Network"},{"metadata":{"_uuid":"8269e1c91ff16f4ba1808f559a5ab26a1d92c5a7"},"cell_type":"markdown","source":"Below we do a gridsearch to optimize the hyperparameters of a Neural Network classifier. We solve for the number and size of the neuronal layers, the alpha, and the activation function to use."},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","trusted":true,"collapsed":true,"_uuid":"d3b2002d9896102dcd4e61d42373099e980c8783"},"cell_type":"code","source":"vectorizer = TfidfVectorizer(preprocessor=preprocess, stop_words=stopwords_all)\n\npipeline_nn = Pipeline([\n    ('features', vectorizer),\n    ('densify', DenseTransformer()),\n    ('svd', TruncatedSVD(n_components=1000)),\n    ('classify', MLPClassifier(max_iter=2000))\n])\n\nlayers = [(1000,), (500,), (100, )]\nalpha = np.logspace(-4, 0, 5)\nactivation = ['logistic', 'relu']\n\nparam_grid = {'classify__hidden_layer_sizes': layers,\n              'classify__alpha': alpha,\n              'classify__activation': activation}\n\nscoring = ['accuracy', 'precision', 'recall', 'roc_auc', 'f1']\ngrid = GridSearchCV(pipeline_nn, param_grid, refit='accuracy', verbose=4,\n                    n_jobs=4, scoring=scoring, return_train_score=True)\n","execution_count":25,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","trusted":true,"_uuid":"78f809c7c53ba7ef00e1a81d2de1f8da6233182c","collapsed":true},"cell_type":"code","source":"scores_nn = grid_search_p_model(grid, train_tests)\n","execution_count":26,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9810e55d871ee96c8b495469fa604d46fee57df1","collapsed":true},"cell_type":"code","source":"print('Mind test F1 score:', scores_nn['mind_f1_test'])\nprint('Energy test F1 score:', scores_nn['energy_f1_test'])\nprint('Nature test F1 score:', scores_nn['nature_f1_test'])\nprint('Tactics test F1 score:', scores_nn['tactics_f1_test'])\n","execution_count":27,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","_uuid":"30ad8a38a5e48581737037ebad06bbbb42bceedb"},"cell_type":"markdown","source":"---"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","_uuid":"b7183ddf78fd3ce74d49bab6150c560842ab7f9f"},"cell_type":"markdown","source":"## Decision tree based"},{"metadata":{"_uuid":"dcd49592e1e017a3aa0ec73f73d3474c3f091885"},"cell_type":"markdown","source":"Below we do a gridsearch to optimize the hyperparameters of a decision tree based ensemble approach, random forests. We solve for the number of estimators, the maximum number of features, the selection criterion, whether to bootstrap and the depth of the forest."},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","trusted":true,"collapsed":true,"_uuid":"430db03a41b8d16994e317ca97ac97f9399be4b7"},"cell_type":"code","source":"vectorizer = TfidfVectorizer(preprocessor=preprocess,\n                             stop_words=stopwords_no_pro)\n\npipeline_rf = Pipeline([\n    ('features', vectorizer),\n    ('densify', DenseTransformer()),\n    ('svd', TruncatedSVD(n_components=1000)),\n    ('classify', RandomForestClassifier())\n])\n\nn_estimators = [100, 250, 500, 1000]\nmax_features = ['auto', 'log2', 'sqrt']\ncriterion = [\"gini\", \"entropy\"]\nbootstrap = [True, False]\nmax_depth = [3, 5, None]\n\nparam_grid = {'classify__n_estimators': n_estimators,\n              'classify__max_features': max_features,\n              'classify__criterion': criterion,\n              'classify__bootstrap': bootstrap,\n              'classify__max_depth': max_depth}\n\nscoring = ['accuracy', 'precision', 'recall', 'roc_auc', 'f1']\ngrid = GridSearchCV(pipeline_rf, param_grid, refit='accuracy', verbose=4,\n                    n_jobs=4, scoring=scoring, return_train_score=True)","execution_count":30,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","trusted":true,"_uuid":"04e552115d0adafeb1fd43eed9e0aa6ee56f4deb","scrolled":true,"collapsed":true},"cell_type":"code","source":"scores_rnf = grid_search_p_model(grid, train_tests)\n","execution_count":31,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"527129c88a4b42bca08f7727e18e5ae5d551c25b","collapsed":true},"cell_type":"code","source":"print('Mind test F1 score:', scores_rnf['mind_f1_test'])\nprint('Energy test F1 score:', scores_rnf['energy_f1_test'])\nprint('Nature test F1 score:', scores_rnf['nature_f1_test'])\nprint('Tactics test F1 score:', scores_rnf['tactics_f1_test'])\n","execution_count":67,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","_uuid":"8d6666e15396a0933c4bb7f2483765021cb60976"},"cell_type":"markdown","source":"---"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","_uuid":"ca2fc22b2052dc59d9f5de9513c44f07e7a66941"},"cell_type":"markdown","source":"## Now compare the best models from each classifier and see how the classifiers stack up"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"99490b93077d7e41e1b2c21c969ab909a60c7216"},"cell_type":"code","source":"accu_m = [scr_log['mind_f1_test'], scores_knn['mind_f1_test'],\n          scores_lda['mind_f1_test'], scores_svm['mind_f1_test'],\n          scores_nn['mind_f1_test'], scores_rnf['mind_f1_test']]\n\naccu_e = [scr_log['energy_f1_test'], scores_knn['energy_f1_test'],\n          scores_lda['energy_f1_test'], scores_svm['energy_f1_test'],\n          scores_nn['energy_f1_test'], scores_rnf['energy_f1_test']]\n\naccu_n = [scr_log['nature_f1_test'], scores_knn['nature_f1_test'],\n          scores_lda['nature_f1_test'], scores_svm['nature_f1_test'],\n          scores_nn['nature_f1_test'], scores_rnf['nature_f1_test']]\n\naccu_t = [scr_log['tactics_f1_test'], scores_knn['tactics_f1_test'],\n          scores_lda['tactics_f1_test'], scores_svm['tactics_f1_test'],\n          scores_nn['tactics_f1_test'], scores_rnf['tactics_f1_test']]\n\nnames = ['Logistic', 'KNN', 'LDA', 'SVM', 'NeuralNetw', 'RandomF']\n\naccu_mind = pd.DataFrame([accu_m, names], index=['score', 'clf']).T\naccu_energy = pd.DataFrame([accu_e, names], index=['score', 'clf']).T\naccu_nature = pd.DataFrame([accu_n, names], index=['score', 'clf']).T\naccu_tactics = pd.DataFrame([accu_t, names], index=['score', 'clf']).T\n","execution_count":68,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c5dffcc3e114eca68777310f2ff5c19d65c2970","collapsed":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(12, 12), nrows=2, ncols=2)\nplt.tight_layout(pad=3, h_pad=8)\ncols = ['gold', 'limegreen', 'darkorange',\n        'darkcyan', 'royalblue', 'mediumpurple']\nax[0, 0].bar(accu_mind['clf'], accu_mind['score'], color=cols)\nax[0, 0].set_title('F1 test score of best mind classification models')\nax[0, 0].tick_params(rotation=90)\n\nax[0, 1].bar(accu_energy['clf'], accu_energy['score'], color=cols)\nax[0, 1].set_title('F1 test score of best energy classification models')\nax[0, 1].tick_params(rotation=90)\n\nax[1, 0].bar(accu_nature['clf'], accu_nature['score'], color=cols)\nax[1, 0].set_title('F1 test score of best nature classification models')\nax[1, 0].tick_params(rotation=90)\n\nax[1, 1].bar(accu_tactics['clf'], accu_tactics['score'], color=cols)\nax[1, 1].set_title('F1 test score of best tactics classification models')\nax[1, 1].tick_params(rotation=90);\n","execution_count":69,"outputs":[]},{"metadata":{"_uuid":"f45467a0199187bd953b4b6a1a3f96b23dccf894"},"cell_type":"markdown","source":"From this it is clear that Nearal Networks, Logistic regression and Support Vector Machines consistently performs better than the other classifiers. Especially on the datasets which has les samples due to class balancing."},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","_uuid":"7feec52606fc6139da684e203a3b6c77c2cde872"},"cell_type":"markdown","source":"---"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","_uuid":"fd8cb4c1f601618c26cb725af6853e139418f28b"},"cell_type":"markdown","source":"## Lastly, lets reduce the dimensions"},{"metadata":{"_uuid":"9cddcf4009b60f8dedea21bd9ddabc1d5ecb79c6"},"cell_type":"markdown","source":"For all the models performed so far, we included a dimentionality reduction method to reduce the computational load and collinearity. This also greatly improved the performance of the model, and here we optimize which number of components are ideal for reducing the dimentionality of the data."},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","trusted":true,"collapsed":true,"_uuid":"394f077330c9efa77e137dec3bf5c5728844ff73"},"cell_type":"code","source":"vectorizer = TfidfVectorizer(preprocessor=preprocess,\n                             stop_words=stopwords_no_pro)\n\npipeline_svd = Pipeline([\n    ('features', vectorizer),\n    ('densify', DenseTransformer()),\n    ('svd', TruncatedSVD()),\n    ('classify', LogisticRegression(penalty='l1', C=1000))\n])\n\nnum_components = [50, 100, 500, 1000, 5000]\n\nparam_grid = {\n    'svd__n_components': num_components\n}\nscoring = ['accuracy', 'precision', 'recall', 'roc_auc', 'f1']\ngrid = GridSearchCV(pipeline_svd, param_grid, refit='accuracy', verbose=4,\n                    n_jobs=-1, scoring=scoring, return_train_score=True)\n","execution_count":53,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","trusted":true,"_uuid":"5c682867882e95dad1e582c906694418e7123d3e","scrolled":true,"collapsed":true},"cell_type":"code","source":"scores_svd = grid_search_p_model(grid, train_tests)\n","execution_count":54,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e0c238439b6e80360aac6d03729ccd9154eac6f2","collapsed":true},"cell_type":"code","source":"print('Mind test F1 score:', scores_svd['mind_f1_test'])\nprint('Energy test F1 score:', scores_svd['energy_f1_test'])\nprint('Nature test F1 score:', scores_svd['nature_f1_test'])\nprint('Tactics test F1 score:', scores_svd['tactics_f1_test'])\n","execution_count":55,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1e2fc7969db434fdbefd059408e663a7fee22676"},"cell_type":"code","source":"accu = scores_svd['mind_grid_results']['mean_test_accuracy']\nneigh = list(scores_svd['mind_grid_results']['param_svd__n_components'])\n\naccu_b = pd.DataFrame([accu, neigh], index=['score', 'neigh']).T\n","execution_count":56,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"236ac72b10acb0a5b14b2dc7162fa9feec9dd4e4","collapsed":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(8, 6))\n\nplt.plot(accu_b['neigh'], accu_b['score'])\nax.set_title('Accuracy at various number of components')\nax.set_xlabel('Components')\nax.set_ylabel('Test Accuracy');\n","execution_count":70,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","_uuid":"ea43b1d720ab862e2b5fb186f3ca6c8d4807a0e3"},"cell_type":"markdown","source":"---"},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","_uuid":"23f180ba2fd35ebede4ef6eee974a2d1474b7388"},"cell_type":"markdown","source":"## Emsemble Methods"},{"metadata":{"_uuid":"acaa5beac273ca479d411753ca20f96c0e700cb2"},"cell_type":"markdown","source":"Lastly, we perform emsemble approached to see if they can improve the performace of our model, and optimize some hyperparameters for these models."},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","trusted":true,"collapsed":true,"_uuid":"3fce59ff6e694f6fd069dc98be78d768f3e04db2"},"cell_type":"code","source":"vectorizer = TfidfVectorizer(preprocessor=preprocess,\n                             stop_words=stopwords_no_pro)\nclassifier = LogisticRegression(penalty='l1', C=1000)\n\npipeline_svd = Pipeline([\n    ('features', vectorizer),\n    ('densify', DenseTransformer()),\n    ('svd', TruncatedSVD()),\n    ('classify', AdaBoostClassifier())\n])\n\nbooster = [GradientBoostingClassifier(),\n           AdaBoostClassifier(classifier),\n           BaggingClassifier(classifier)]\nn_estimators = [50, 100, 500, 1000]\nparam_grid = {\n    'classify': booster,\n    'classify__n_estimators': n_estimators\n}\nscoring = ['accuracy', 'precision', 'recall', 'roc_auc', 'f1']\ngrid = GridSearchCV(pipeline_svd, param_grid, refit='accuracy', verbose=4,\n                    n_jobs=4, scoring=scoring, return_train_score=True)\n","execution_count":63,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XRu0fLIQXFSc","trusted":true,"_uuid":"22f14ddfb9869de78c2925e12169fa0de0114e1e","scrolled":true,"collapsed":true},"cell_type":"code","source":"scores_ensm = grid_search_p_model(grid, train_tests)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be7c9cc8f6bdd4065a19f36e911b1b85aacdbdf5","collapsed":true},"cell_type":"code","source":"print('Mind test F1 score:', scores_ensm['mind_f1_test'])\nprint('Energy test F1 score:', scores_ensm['energy_f1_test'])\nprint('Nature test F1 score:', scores_ensm['nature_f1_test'])\nprint('Tactics test F1 score:', scores_ensm['tactics_f1_test'])\n","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"c69e39e17b529469b31fe99822ee91554ab495c9"},"cell_type":"markdown","source":"From this it looks like the ensemble approaches does not provide a significant amount of improvement on the other classifiers without extensive optimization of the hyper parameters.\n\n---"},{"metadata":{"_uuid":"8ed2937e182b9b30881395dda5603240c7d26ecf"},"cell_type":"markdown","source":"## Performance of a multi-label classifier"},{"metadata":{"_uuid":"0a620cb32a36a6fe037e533622b9475c00749266"},"cell_type":"markdown","source":"One of the best performing classifiers were the Neural network classifier. In a last attempt to get the best performing classifier, we will train a multi-label neural network classifier. This neural network will predict all four axes in one classifier, reducing the need to subsample, and balance the dataset, as well as allowing a single classifier to classify the dataset, reducing computational time. "},{"metadata":{"_uuid":"4070b61345dfb213d9336f603bc05c50f949d3bf"},"cell_type":"markdown","source":"This dataframe contains the individual labels seperated per personality axis, which will be used in the multilabel classification"},{"metadata":{"trusted":true,"_uuid":"594d1393c65fac56c9cfc129c6ed877300c1c95d","collapsed":true},"cell_type":"code","source":"train_labeled.head()","execution_count":14,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"76e0eda95387f241dbfca9b1bdf8c698d2883d2b"},"cell_type":"code","source":"X_nn = train_labeled['posts']\ny_nn = train_labeled[['I', 'N', 'F', 'J']]\n","execution_count":15,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3f3af3db153d1b7f791aaa3f4b90d3f4a5a6932e"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_nn, y_nn)","execution_count":16,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba851e3661ff840d17a26cdfa78617576e19491b","collapsed":true},"cell_type":"code","source":"pipeline = Pipeline([ \n    ('vectorize', TfidfVectorizer(stop_words=stopwords_no_pro)), \n    ('svd', TruncatedSVD(n_components=500)),\n    ('scaler', StandardScaler()),\n    ('classify', MLPClassifier(max_iter=2000))\n])\npipeline.fit(X_train, y_train)\n","execution_count":24,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"54e61de4193e702853688db6607e01af9363c964","collapsed":true},"cell_type":"code","source":"pipeline.score(X_test, y_test)\n","execution_count":25,"outputs":[]},{"metadata":{"_uuid":"253144525646b687ce8a3bafdcb296629274a414"},"cell_type":"markdown","source":"This is the score for the overall model, i.e., per personality (e.g., INFJ).\n\nThis score may look quite low, but for comparison the classifying the full personalities using a Logistic Regression classifier only provides an accuracy of 0.27.\n\n---"},{"metadata":{"_uuid":"adf56f4aadb74c98a950df2b08e25d1563275514"},"cell_type":"markdown","source":"This shows the performance of the model on each of the personality axes, i.e., (I vs. E), etc."},{"metadata":{"trusted":true,"_uuid":"59d4ff8fde19179bae3000db6b747d7d35b849fe","collapsed":true},"cell_type":"code","source":"predict = pipeline.predict(X_test)\nprint('The classification scores: ','\\n',\n      metrics.classification_report(y_test, predict))\n","execution_count":30,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c50b1fe92a2904cdb14cac02f6b927fa82aa6ccd","collapsed":true},"cell_type":"code","source":"print('This is the mind accuracy score of my model',\n      metrics.accuracy_score(y_test['I'], pd.DataFrame(predict)[0]))\nprint('This is the energy accuracy score of my model',\n      metrics.accuracy_score(y_test['N'], pd.DataFrame(predict)[1]))\nprint('This is the nature accuracy score of my model',\n      metrics.accuracy_score(y_test['F'], pd.DataFrame(predict)[2]))\nprint('This is the tactics accuracy score of my model',\n      metrics.accuracy_score(y_test['J'], pd.DataFrame(predict)[3]))\n","execution_count":27,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b3283da2f91287b3164a535a302284b79bd55e0c"},"cell_type":"code","source":"param_grid = {\n              'classify__hidden_layer_sizes': [(5000,), (1000,),\n                                               (500,), (250, )],\n              'classify__alpha': [1e-1, 1e-3, 1e-5, 1e-7],\n              'classify__activation': ['logistic', 'relu'],\n              'svd__n_components': [250, 500, 1000]}\n\ngrid = GridSearchCV(pipeline, param_grid, refit=True, verbose=4, n_jobs=4)\n","execution_count":31,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71dda5778e8b1538844a0f27fc4075c763d2f75e","scrolled":true,"collapsed":true},"cell_type":"code","source":"grid.fit(X_train,y_train)\n","execution_count":32,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6c2ee11ae65a0a992f250fb03b1ce0f2d0cc76af","collapsed":true},"cell_type":"code","source":"grid.best_params_\n","execution_count":154,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0a70b6a8c0087360a69869519aa088e0c6704525","collapsed":true},"cell_type":"code","source":"grid.best_score_\n","execution_count":155,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8135ee546d3b2480e5a36e755e543103769b1716","collapsed":true},"cell_type":"code","source":"grid.score(X_test, y_test)\n","execution_count":156,"outputs":[]},{"metadata":{"_uuid":"cb2cf8aa6c5a16160bb405e44482f8ee63cb158f"},"cell_type":"markdown","source":"While the overall performance of this model (i.e., in predicting the full personalities) is quite low, predicting the individual axes of each personality is quite accurate. And this is thus the model that we will use in predicting the personalities."},{"metadata":{"_uuid":"c543b5247e7c630458653703825bcfd0949b18fc"},"cell_type":"markdown","source":"# Final model construction "},{"metadata":{"trusted":false,"_uuid":"c7c5659fa2955c29ab429821ed5b320e7d2dd16a","collapsed":true},"cell_type":"code","source":"pipeline = Pipeline([\n    ('vectorize', TfidfVectorizer(stop_words=stopwords_no_pro)),\n    ('svd', TruncatedSVD(n_components=250)),\n    ('scaler', StandardScaler()),\n    ('classify', MLPClassifier(max_iter=2000, activation='logistic',\n                               alpha=1e-05, hidden_layer_sizes=(1000,)))\n])\npipeline.fit(X_train, y_train)\n","execution_count":282,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e2d797050c9b93e760f1159468c7368422696438","collapsed":true},"cell_type":"code","source":"pipeline.score(X_test, y_test)\n","execution_count":283,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c7f2656d0ce79c0b62b152a0fe59fe06a2af49e3"},"cell_type":"code","source":"predict = pd.DataFrame(pipeline.predict(X_test))\n","execution_count":319,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"21f815da396db3dd21048acb771d016e1bd76c99","collapsed":true},"cell_type":"code","source":"scr = pd.DataFrame([[metrics.accuracy_score(y_test['I'], predict[0]),\n                     metrics.accuracy_score(y_test['N'], predict[1]),\n                     metrics.accuracy_score(y_test['F'], predict[2]),\n                     metrics.accuracy_score(y_test['J'], predict[3])],\n                    [metrics.matthews_corrcoef(y_test['I'], predict[0]),\n                     metrics.matthews_corrcoef(y_test['N'], predict[1]),\n                     metrics.matthews_corrcoef(y_test['F'], predict[2]),\n                     metrics.matthews_corrcoef(y_test['J'], predict[3])],\n                    [metrics.f1_score(y_test['I'], predict[0]),\n                     metrics.f1_score(y_test['N'], predict[1]),\n                     metrics.f1_score(y_test['F'], predict[2]),\n                     metrics.f1_score(y_test['J'], predict[3])],\n                    [metrics.recall_score(y_test['I'], predict[0]),\n                     metrics.recall_score(y_test['N'], predict[1]),\n                     metrics.recall_score(y_test['F'], predict[2]),\n                     metrics.recall_score(y_test['J'], predict[3])],\n                    [metrics.precision_score(y_test['I'], predict[0]),\n                     metrics.precision_score(y_test['N'], predict[1]),\n                     metrics.precision_score(y_test['F'], predict[2]),\n                     metrics.precision_score(y_test['J'], predict[3])]],\n                   index=['Accuracy', 'Matthews', 'F1',\n                          'Recall', 'Precision'],\n                   columns=['Mind', 'Energy', 'Nature', 'Tactics']).T\n\nscr","execution_count":329,"outputs":[]},{"metadata":{"_uuid":"929314203ad199d77e32bca74a437c13e1517b74"},"cell_type":"markdown","source":"But how does this compare to the multiple single label classifiers constructed above, and which classifier will we choose as the winner?"},{"metadata":{"trusted":false,"_uuid":"3ca96f238628ea4fc94d190d5237fd63bca0189f","collapsed":true},"cell_type":"code","source":"accu_m = [scr_log['mind_f1_test'], scores_knn['mind_f1_test'],\n          scores_lda['mind_f1_test'], scores_svm['mind_f1_test'],\n          scores_nn['mind_f1_test'], scores_rnf['mind_f1_test'],\n          metrics.f1_score(y_test['I'], predict[0])]\n\naccu_e = [scr_log['energy_f1_test'], scores_knn['energy_f1_test'],\n          scores_lda['energy_f1_test'], scores_svm['energy_f1_test'],\n          scores_nn['energy_f1_test'], scores_rnf['energy_f1_test'],\n          metrics.f1_score(y_test['N'], predict[1])]\n\naccu_n = [scr_log['nature_f1_test'], scores_knn['nature_f1_test'],\n          scores_lda['nature_f1_test'], scores_svm['nature_f1_test'],\n          scores_nn['nature_f1_test'], scores_rnf['nature_f1_test'],\n          metrics.f1_score(y_test['F'], predict[2])]\n\naccu_t = [scr_log['tactics_f1_test'], scores_knn['tactics_f1_test'],\n          scores_lda['tactics_f1_test'], scores_svm['tactics_f1_test'],\n          scores_nn['tactics_f1_test'], scores_rnf['tactics_f1_test'],\n          metrics.f1_score(y_test['J'], predict[3])]\n\nnames = ['Logistic', 'KNN', 'LDA', 'SVM',\n         'NeuralNetw', 'RandomF', 'MultilabelNN']\n\naccu_mind = pd.DataFrame([accu_m, names], index=['score', 'clf']).T\naccu_energy = pd.DataFrame([accu_e, names], index=['score', 'clf']).T\naccu_nature = pd.DataFrame([accu_n, names], index=['score', 'clf']).T\naccu_tactics = pd.DataFrame([accu_t, names], index=['score', 'clf']).T\n\n\nf, ax = plt.subplots(figsize=(12, 12), nrows=2, ncols=2)\nplt.tight_layout(pad=3, h_pad=8)\ncols = ['gold', 'limegreen', 'darkorange',\n        'darkcyan', 'royalblue', 'mediumpurple', 'crimson']\nax[0, 0].bar(accu_mind['clf'], accu_mind['score'], color=cols)\nax[0, 0].set_title('F1 test score of best mind classification models')\nax[0, 0].tick_params(rotation=90)\n\nax[0, 1].bar(accu_energy['clf'], accu_energy['score'], color=cols)\nax[0, 1].set_title('F1 test score of best energy classification models')\nax[0, 1].tick_params(rotation=90)\n\nax[1, 0].bar(accu_nature['clf'], accu_nature['score'], color=cols)\nax[1, 0].set_title('F1 test score of best nature classification models')\nax[1, 0].tick_params(rotation=90)\n\nax[1, 1].bar(accu_tactics['clf'], accu_tactics['score'], color=cols)\nax[1, 1].set_title('F1 test score of best tactics classification models')\nax[1, 1].tick_params(rotation=90);\n","execution_count":484,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"0540568cb0b2f1d451c54e195d6dd31eeb7b90b9"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"colab":{"default_view":{},"name":"Pipeline_personality_sprintipynb","provenance":[],"version":"0.3.2","views":{}},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}