{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Executive Summary\n\nIn this project, the aim was to make use of clustering techniques such as DBSCAN and KPrototype to identify important variables, in a flight delay dataset, which affect flight performance and answer some hypotheses which can lead to business insights. The report covers all steps involved from data retrieval to exploration and modeling and recommendation. In conclusion, clusters were obtained using both the techniques, however, the more versatile K-Prototype algorithm provided more meaningful ones which answered some of the questions. Having said this, further analyses, research and modeling attempts are required in order to fully address the hypotheses contained in the goal."},{"metadata":{},"cell_type":"markdown","source":"## Research questions"},{"metadata":{},"cell_type":"markdown","source":"We formulated the following questions to assist our research aim.\n* Does day of the month play a role in flight delays?\n* Are there specific months which have more flight delays than others?\n* Are longer duration flights more susceptible to delays than short haul flights?\n* Are cancellations related to delays?"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Importing required packages\nimport numpy as np\nimport pandas as pd \nimport random\nfrom kmodes.kprototypes import KPrototypes\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Reading the Data\nData = pd.read_csv('../input/DelayedFlights.csv')\n#Dropping the index column\nData= Data.drop(Data.columns[0], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Pre-processing\n"},{"metadata":{},"cell_type":"markdown","source":"## Sanity checks\nSanity checks were carried out to ensure that the given values are rational. The ArrTime,\nCRSEArrivalTime, DepTime and CRSEDepTime columns were checked for negative time values\nsince these would not make sense."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Sanity Check for Negative values in ArrTime, CRSEArrivalTime, DepTime and CRSEDepTime\nprint(all(i < 0 for i in Data['ArrTime']))\nprint(all(i < 0 for i in Data['CRSArrTime']))\nprint(all(i < 0 for i in Data['DepTime']))\nprint(all(i < 0 for i in Data['CRSDepTime']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting Categorical Variables to object type\nData[Data.columns[0:4]]=Data[Data.columns[0:4]].astype(object)\nData[Data.columns[8:10]]=Data[Data.columns[8:10]].astype(object)\nData[Data.columns[21:24]]=Data[Data.columns[21:24]].astype(object)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Renaming the labels within the variables\nData['Cancelled']=Data['Cancelled'].replace([1,0],[\"Cancelled\",\"Not Cancelled\"])\nData['Diverted']=Data['Diverted'].replace([1,0],[\"Diverted\",\"Not Diverted\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing values imputation\nAfter the sanity checks, we went for missing values imputation. Here, we checked the numerical\ncolumns for missing values using the is.na.sum function and imputed them using the linear\ninterpolate function. The following columns had missing values which were handled: ArrTime,\nActualElapsedTime, CRSElapsedTime, AirTime, ArrDelay, TaxiIn, TaxiOut, CarrierDelay,\nWeatherDelay, NASDelay, SecurityDelay and LateAircraftDelay."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking for null values\nData.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Null value Imputation using Interpolation Method\nData['ActualElapsedTime']=Data['ActualElapsedTime'].interpolate(method='linear',limit_direction ='both',axis=0) #Interploation\nData['CRSElapsedTime']=Data['CRSElapsedTime'].interpolate(method='linear',limit_direction ='both',axis=0) #Interploation\nData['AirTime']=Data['AirTime'].interpolate(method='linear',limit_direction ='both',axis=0) #Interploation\nData['ArrTime']=Data['ArrTime'].interpolate(method='linear',limit_direction ='both',axis=0) #Interploation\nData['ArrDelay']=Data['ArrDelay'].interpolate(method='linear',limit_direction ='both',axis=0) #Interploation\nData['CarrierDelay']=Data['CarrierDelay'].interpolate(method='linear',limit_direction ='both',axis=0) #Interploation\nData['WeatherDelay']=Data['WeatherDelay'].interpolate(method='linear',limit_direction ='both',axis=0) #Interploation\nData['NASDelay']=Data['NASDelay'].interpolate(method='linear',limit_direction ='both',axis=0) #Interploation\nData['LateAircraftDelay']=Data['LateAircraftDelay'].interpolate(method='linear',limit_direction ='both',axis=0) #Interploation\nData['SecurityDelay']=Data['SecurityDelay'].interpolate(method='linear',limit_direction ='both',axis=0) #Interploation\nData['TaxiIn']=Data['TaxiIn'].interpolate(method='linear',limit_direction ='both',axis=0) #Interploation\nData['TaxiOut']=Data['TaxiOut'].interpolate(method='linear',limit_direction ='both',axis=0) #Interploation\nData.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Typo correction\nWe checked whether the categorical variables have typos using the values.count function."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking value counts for categorical variables\n\n# Filter categorical variables\nnum_cols = Data._get_numeric_data().columns\ncols = Data.columns\ncat_cols = list(set(cols) - set(num_cols))\ncat_cols\n\n#Value counts\nfor col in cat_cols:\n    if col in ['DayOfWeek','UniqueCarrier','Month', 'Cancelled','DayofMonth','CancellationCode','Diverted']:\n        print(Data[col].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration\nThe dataset columns were explored using univariate and bivariate visualisations. This was\nfollowed by a comprehensive correlation plot to understand their relationships and select\nsignificant variables. For the purpose of the report, only significant variables, relationships and\ntheir explorations have been discussed below."},{"metadata":{},"cell_type":"markdown","source":"## Univariate Distribution of Features"},{"metadata":{},"cell_type":"markdown","source":"### Univariate Distribution of Numerical Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a function to plot Box plot and Histogram\ndef hist_box_plot(df,feature, fig_num):\n    sns.set(color_codes = 'Blue', style=\"whitegrid\")\n    sns.set_style(\"whitegrid\", {'axes.grid' : False})\n    sns.set_context(rc = {'patch.linewidth': 0.0})\n    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(8,3))\n    filtered = df.loc[~np.isnan(df[feature]), feature]\n    sns.boxplot(filtered, ax = ax1, color = 'steelblue') # boxplot\n    sns.distplot(filtered, kde=True, hist=True, kde_kws={'linewidth': 1}, color = 'steelblue', ax = ax2) # histogram\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_num = 1        \nfor col in Data.select_dtypes(include=[np.number]).columns:\n    if col in ['DepTime','ActualElapsedTime','CRSElapsedTime','AirTime','ArrDelay','DepDelay','Distance','TaxiIn','TaxiOut']:\n        hist_box_plot(Data,col, fig_num)\n        fig_num = fig_num + 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Univariate Distribution of Categorical Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a function to plot Count plot\ndef count_plot(df,feature):\n    sns.set(color_codes = 'Blue', style=\"whitegrid\")\n    sns.set_style(\"whitegrid\", {'axes.grid' : False})\n    sns.set_context(rc = {'patch.linewidth': 0.0})\n    fig = plt.subplots(figsize=(10,3))\n    sns.countplot(x=feature, data=df, color = 'steelblue') # countplot\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter categorical variables\nnum_cols = Data._get_numeric_data().columns\ncols = Data.columns\ncat_cols = list(set(cols) - set(num_cols))\ncat_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in cat_cols:\n    if col in ['DayOfWeek','UniqueCarrier','Month', 'Cancelled','DayofMonth','CancellationCode','Diverted']:\n        count_plot(Data,col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bivariate Distribution of Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def biplot(df, x_name, y_name):\n    fig, ax = plt.subplots()\n    ax.grid(False)\n    x = df[x_name]\n    y = df[y_name]\n    plt.scatter(x,y,c='blue', edgecolors='none',alpha=0.5)\n    plt.xlabel(x_name)\n    plt.ylabel(y_name)\n    plt.title('{x_name} vs. {y_name}'.format(x_name=x_name, y_name=y_name))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ArrDelay vs Month\nThis was checked to understand if there is any truth to the hypothesis that\ncertain months are worse for Arrival delays than others. The plot shows that very little variation\nbetween the month and Arrival delays and doesnâ€™t support the hypothesis. There are a few\noutliers for some months of the year namely, February, April, May and June."},{"metadata":{"trusted":true},"cell_type":"code","source":"biplot(df=Data,x_name='Month',y_name='ArrDelay')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ArrDelay vs DepDelay \nThis relationship was checked to understand if Arrival Delay of a flight\nhad an impact on Departure Delay as well and thus an effect on the performance of the\nairline/flight. From the plot, we observe that there is a linear dependency between the two\nvariables indicating that a flightâ€™s delay on Arrival has a role to play in its Departure getting\ndelayed, which is logical."},{"metadata":{"trusted":true},"cell_type":"code","source":"biplot(df=Data,x_name='DepDelay',y_name='ArrDelay')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ArrDelay vs Distance  \nThis relationship was explored to understand if there is any relation\nbetween Arrivals getting delayed as a result of the longer Distance of the flight. From the plot, we\nobserve that there is actually an inverse correlation, shorter flights are the ones seeming to be\ndelayed more on Arrival."},{"metadata":{"trusted":true},"cell_type":"code","source":"biplot(df=Data,x_name='Distance',y_name='ArrDelay')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cancellation vs Months \nThis relationship was explored to understand if there are any particular\nmonths where cancellations are particularly more common. The plot shows that this is indeed\nthe case (probably because end of the year has Christmas and New Year holidays which is a peak\ntravel season)."},{"metadata":{"trusted":true},"cell_type":"code","source":"biplot(df=Data,x_name='Month',y_name='CancellationCode')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation Between Categorical and Numerical"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Installation of the Package follows the following steps:\n# git clone https://github.com/shakedzy/dython.git\n!pip install dython","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert all the columns in float to integer for correlation plot as float is not handled\nfor y in Data.columns:\n    if(Data[y].dtype == np.float64):\n        Data[y] = Data[y].astype(int)\n\nData.dtypes\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ignoring Year, since the data set is for 2008 and UniqueCarrier,FlightNum,and TailNum, since they won't be effective in correlation\nData_Correlation=Data.iloc[:, [1,2,3,4,5,6,7,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28]]\nData_Correlation.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from dython.model_utils import roc_graph\nfrom dython.nominal import associations\n\ndef associations_example():\n    associations(Data_Correlation,nominal_columns=['Origin','Dest','Cancelled','CancellationCode','Diverted','DayofMonth',\n                                           'DayOfWeek','Month'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"]=20,20\nassociations_example()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plot has been made using Theilâ€™s U, which is also referred to as the Uncertainty Coefficient,\nwhich calculates conditional entropy between two variables â€“ it tries to ascertain how many\npossible states does the second variable have given a value of the first variable. This plotâ€™s code\nis available using dython package (which works with Python 3.0 and later) (Zychlinski, 2018).\nFrom the above plot, the following are significant observations:\n\n* Variables such as Distance was highly correlated with ActualElapsedTime, CRSElapsedTime, and AirTime (0.95, 0.98 and 0.98). Hence, to avoid multi collinearity, we chose only ActualElapsedTime variable for further study.\n* Variables such as ArrDelay and DepDelay were highly highly correlated (0.95) which makes sense because a flight which is delayed on Arrival will most likely leave late and hence be Delayed on Departure. So, one of these two, DepDelay was chosen.\n* Cancelled and CancellationCode variables were having a correlation of 1, because cancellation codes are used only in the case when there are Cancellations (and thus Cancelled column has 1). Here, we chose Cancelled variable.\n* Origin and Dest were moderately correlated (0.47, 0.45 and 0.43, 0.48) with UniqueCarrier and FlightNum variables respectively. This explains the real-world scenario where certain Carriers fly only between certain cities.\n* TaxiIn and TaxiOut are moderately positively correlated (0.44 and 0.40) with Destination and Origin respectively, here we take TaxiOut as the variable to study (related to departure)."},{"metadata":{},"cell_type":"markdown","source":"## Analyses\nFrom the exploratory analysis and preliminary data modelling, we decided to use three numerical\nvariables along with one categorical variable (total four with a different one for each analysis) for\nour modelling:\n* Cancelled, DayOfMonth, DayofWeek, Monthâ€“ categorical variables\n* ActualElapsedTime, TaxiOut and DepDelay â€“ numerical variables"},{"metadata":{},"cell_type":"markdown","source":"## Data Transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Standardizing all the numerical variables\nfrom sklearn import preprocessing\nNum_features=Data.select_dtypes(include=[np.number]).columns\nData[Num_features]=preprocessing.MinMaxScaler().fit_transform(Data[Num_features])\nData.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ## Hopkins Statistics\nTo understand if the dataset can be clustered, we used the Hopkins statistic, which tests the\nspatial randomness of the data and indicates the cluster tendency or how well the data can be\nclustered. It calculates the probability that a given data is generated by a uniform distribution\n(Alboukadel Kassambara, n.d.).\nThe inference is as follows for a data of dimensions â€˜dâ€™:\n* If the value is around 0.5 or lesser, the data is uniformly distributed and hence it is unlikely to have statistically significant clusters.\n* If the value is between {0.7, ..., 0.99}, it has a high tendency to cluster and therefore likely to have statistically significant clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Hopkins Statistic is a way of measuring the cluster tendency of a data set.\nfrom sklearn.neighbors import NearestNeighbors\nfrom random import sample\nfrom numpy.random import uniform\nimport numpy as np\nfrom math import isnan\n \ndef hopkins(X):\n    d = X.shape[1]\n    #d = len(vars) # columns\n    n = len(X) # rows\n    m = int(0.1 * n) # heuristic from article [1]\n    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)\n \n    rand_X = sample(range(0, n, 1), m)\n \n    ujd = []\n    wjd = []\n    for j in range(0, m):\n        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)\n        ujd.append(u_dist[0][1])\n        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)\n        wjd.append(w_dist[0][1])\n \n    H = sum(ujd) / (sum(ujd) + sum(wjd))\n    if isnan(H):\n        print(ujd, wjd)\n        H = 0\n \n    return H","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Use a random sample of Data for faster computation\nData = Data.sample(20000,random_state=41)\nData.head()\n#Resetting the indexs\nData=Data.reset_index(drop=True)\n#Rename the levels within in the CancellationCode column\nData['CancellationCode']=Data['CancellationCode'].replace(['N','A','B','C'],[0,1,2,3])\nData['CancellationCode']=Data['CancellationCode'].astype(object)\nData['Cancelled']=Data['Cancelled'].replace([\"Cancelled\",\"Not Cancelled\"],[1,0])\nData['Cancelled']=Data['Cancelled'].astype(object)\nData.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking whether data can be clustered\nNum_features =Data.select_dtypes(include=[np.number]).columns\nhopkins(Data[Num_features])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Result: This test is run (code: (MATEVZKUNAVER, 2017)) on all the numerical variables of the\nentire dataset and the test statistic we got is 0.96 which indicates that data has a high tendency\nto cluster."},{"metadata":{},"cell_type":"markdown","source":"### Prinicpal Component Analysis\n\nPrincipal component analysis (PCA) is a technique used to emphasize variation and bring out\nstrong patterns in a dataset. The PCs are dimensions along which the data points are spread out\n(Amy, 2017). We use PCA to understand if the clusters are meaningful along certain features.\n\nFor this attempt, the first step was to standardise the data. Then, we selected three numerical\nvariables namely, ActualElapsedTime, TaxiOut and DepDelay, and proceeded to obtain the\ncumulative variance by trying out different number of principal components. The optimum value\nthe PCA achieved was three, which captured a cumulative variance of ~99% (code: (Vlo, 2017)).\nThese PCs were utilised in the K-Prototype model. The optimal number of clusters (k) of two was\nobtained using the elbow plot for the K-Prototype model (explained later). Of all the categorical\nvariables, â€˜Cancelledâ€™ variable was used to visualise the clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Selection of variables for PCA\nData_pca= Data[['Cancelled','ActualElapsedTime','TaxiOut', 'DepDelay']]\nprint (Data_pca.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Principal Component\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=3, whiten=True)\nNum_features=Data_pca.select_dtypes(include=[np.number]).columns\nx=Data_pca[Num_features]\nprincipalComponents = pca.fit_transform(x)\n\n# Cumulative Explained Variance\ncum_explained_var = []\nfor i in range(0, len(pca.explained_variance_ratio_)):\n    if i == 0:\n        cum_explained_var.append(pca.explained_variance_ratio_[i])\n    else:\n        cum_explained_var.append(pca.explained_variance_ratio_[i] + \n                                 cum_explained_var[i-1])\n\nprint(cum_explained_var)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Principal Components converted to a Data frame\nprincipalDf  = pd.DataFrame(data = principalComponents\n             , columns = ['principal component 1', 'principal component 2', 'principal component 3'])\nprincipalDf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Concatenating the PCAs with the categorical variable\nfinalDf_Cat = pd.concat([principalDf, Data_pca['Cancelled']], axis = 1)\nfinalDf_Cat.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Choosing optimal K value\ncost = []\nX = finalDf_Cat\nfor num_clusters in list(range(2,7)):\n    kproto = KPrototypes(n_clusters=num_clusters, init='Huang', random_state=42,n_jobs=-2,max_iter=15,n_init=50) \n    kproto.fit_predict(X, categorical=[3])\n    cost.append(kproto.cost_)\n\nplt.plot(cost)\nplt.xlabel('K')\nplt.ylabel('cost')\nplt.show","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting the dataset into matrix\nX = finalDf_Cat.as_matrix()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running K-Prototype clustering\nkproto = KPrototypes(n_clusters=2, init='Huang', verbose=0, random_state=42,max_iter=20, n_init=50,n_jobs=-2,gamma=.25) \nclusters = kproto.fit_predict(X, categorical=[3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualize K-Prototype clustering on the PCA projected Data\ndf=pd.DataFrame(finalDf_Cat)\ndf['Cluster_id']=clusters\nprint(df['Cluster_id'].value_counts())\nsns.pairplot(df,hue='Cluster_id',palette='Dark2',diag_kind='kde')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above plot, we can observe that there is approximately distinct cluster formation.\nTherefore, this substantiates the approach of clustering for this dataset. We now proceed with\nrunning the clustering with two algorithms, namely, DBSCAN (for numeric variables) and KPrototype\n(for mix of categorical and numerical variables)."},{"metadata":{},"cell_type":"markdown","source":"### DBSCAN Clustering Algorithm\n\nThis clustering model works with numerical variables, by aggregation depending on density of\npoints near the chosen centroids. This model is robust for numerical variables, doesnâ€™t need the\nnumber of clusters to be specified prior and can deal with arbitrary shaped clusters (Ren, 2019).\nThe eps parameter for the model is chosen as below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Selection of numerical variables for DBSCAN\nData_DBSCAN = Data[['ActualElapsedTime','TaxiOut', 'DepDelay']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Choosing eps parameter using the K-distance graph:\nWe used the below k distance graph from sklearn to find the optimal value of eps, which was\nfound to be 0.035 (also from model iterations) and chose min_samples to be 4 according to the\ncriteria that min_samples >= D+1 where D is the dimensions of the data (Ren, 2019) (here, we\nhad 3 numerical variables)."},{"metadata":{"trusted":true},"cell_type":"code","source":"#selection of eps value\nfrom sklearn.neighbors import NearestNeighbors\nnbrs=NearestNeighbors().fit(Data_DBSCAN)\ndistances, indices = nbrs.kneighbors(Data_DBSCAN,20)\nkDis = distances[:,10]\nkDis.sort()\nkDis = kDis[range(len(kDis)-1,0,-1)]\nplt.plot(range(0,len(kDis)),kDis)\nplt.xlabel('Distance')\nplt.ylabel('eps')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#DBSCAN Algorithm\nfrom sklearn.cluster import DBSCAN\ndbs_1= DBSCAN(eps=0.035, min_samples=4)\nresults = dbs_1.fit(Data_DBSCAN).labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualize DBSCAN clustering \ndf_DBSCAN=Data_DBSCAN\ndf_DBSCAN['Cluster_id_DBSCAN']=results\nprint (df_DBSCAN['Cluster_id_DBSCAN'].value_counts())\nsns.pairplot(df_DBSCAN,hue='Cluster_id_DBSCAN',palette='Dark2',diag_kind='kde')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, using the eps and min_samples parameters, the DBScan model was run (code: (Ren, 2019))\non a standardised sample of 20,000 observations randomly selected using the â€˜sampleâ€™ function\nof â€˜randomâ€™ package.\n\nFinally, from the above clusters plot here, we find that there are three clusters formed (-1,0,1) but\nthey are not clear in distinction, but majority of the points are in one cluster (as per the cluster_id\ncolumn in the plot above). The variable â€˜ActualElapsedTimeâ€™ is the one with the most distinction\nbetween the clusters.\n\nSince the above clustering algorithm can only handle numerical variables, it proves to be\ninsufficient since a categorical variable needs to be included in order to effectively explore the\nresearch questions. Hence, we proceed with k-prototype clustering model which can handle both\nnumerical and categorical variables."},{"metadata":{},"cell_type":"markdown","source":"### Kprototype Clustering Algorithm\n\nThis is a clustering model which includes a combination of k-means and k-modes models to\nachieve clustering of data points around certain prototypes (similar concept as centroids). It\ncalculates Euclidean distances for numerical variables and Similarities for categorical variables\nand uses an assumed gamma as weightage (which decides the preference towards categorical or\nnumerical variables), to assign points to a cluster. This is iteratively done by re-assigning points\nto the right clusters each time and stops when the desired number of moves or iterations has\nbeen achieved (Huang, 1997).\n\nGamma is guided by the average standard deviation of the numeric attributes. In cases where\ngamma is not specified (from domain knowledge), it is automatically calculated from the data.\nIn our model, the parameter value of gamma was found to be 0.15 (which gave somewhat distinct\nclusters) after iteration using fractions of sigma, which is the standard deviation of the chosen numerical variables. Smaller values (closer to 0) of gamma favour the numerical variables while\nthose larger favour (away from 0) the categorical variables (Huang, 1997)."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Getting the list of Numerical and Categorical Variables\nnum_cols = Data._get_numeric_data().columns\nprint (num_cols)\ncols = Data.columns\ncat_cols = list(set(cols) - set(num_cols))\ncat_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Selection of variables for Kprototype Clustering Algorithm\nData_k= Data[['Cancelled','ActualElapsedTime','TaxiOut', 'DepDelay']]\nprint (Data_k.dtypes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Choosing K (number of clusters) from the elbow plot:\nHere, we use the elbow plot between cost and number of clusters (k), where cost is a combined\nsimilarity measure for both numeric and categorical variables calculated between the objects and\nthe cluster prototypes (Huang, 1997), for the K-prototype clustering model (similar to the\nprocess for k-means but utilising categorical variables as well) to arrive at the optimal number of\nclusters. From the above, we select two as the possible number of clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Choosing optimal K value\ncost = []\nX = Data_k\nfor num_clusters in list(range(2,7)):\n    kproto = KPrototypes(n_clusters=num_clusters, init='Huang', random_state=42,n_jobs=-2,max_iter=15,n_init=50) \n    kproto.fit_predict(X, categorical=[0])\n    cost.append(kproto.cost_)\n\nplt.plot(cost)\nplt.xlabel('K')\nplt.ylabel('cost')\nplt.show","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting the dataset into matrix\nX = Data_k.as_matrix()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running K-Prototype clustering\nkproto = KPrototypes(n_clusters=2, init='Huang', verbose=0, random_state=42,max_iter=20, n_init=50,n_jobs=-2,gamma=0.15) \nclusters = kproto.fit_predict(X, categorical=[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualize K-Prototype clustering \ndf_Cancelled=pd.DataFrame(Data_k)\ndf_Cancelled['Cluster_id_K_Prototype']=clusters\nprint (df_Cancelled['Cluster_id_K_Prototype'].value_counts())\nsns.pairplot(df_Cancelled,hue='Cluster_id_K_Prototype',palette='Dark2',diag_kind='kde')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Selection of variables for Kprototype Clustering Algorithm\nData_k= Data[['DayofMonth','ActualElapsedTime','TaxiOut', 'DepDelay']]\nprint (Data_k.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting the dataset into matrix\nX = Data_k.as_matrix()\n# Running K-Prototype clustering\nkproto = KPrototypes(n_clusters=2, init='Huang', verbose=0, random_state=42,max_iter=20, n_init=50,n_jobs=-2,gamma=0.15) \nclusters = kproto.fit_predict(X, categorical=[0])\n#Visualize K-Prototype clustering\ndf=pd.DataFrame(Data_k)\ndf['Cluster_id']=clusters\nprint(df['Cluster_id'].value_counts())\nsns.pairplot(df,hue='Cluster_id',palette='Dark2',diag_kind='kde')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Selection of variables for Kprototype Clustering Algorithm\nData_k= Data[['DayOfWeek','ActualElapsedTime','TaxiOut', 'DepDelay']]\nprint(Data_k.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting the dataset into matrix\nX = Data_k.as_matrix()\n# Running K-Prototype clustering\nkproto = KPrototypes(n_clusters=2, init='Huang', verbose=0, random_state=42,max_iter=20, n_init=50,n_jobs=-2,gamma=0.15) \nclusters = kproto.fit_predict(X, categorical=[0])\n#Visualize K-Prototype clustering \ndf=pd.DataFrame(Data_k)\ndf['Cluster_id']=clusters\nprint(df['Cluster_id'].value_counts())\nsns.pairplot(df,hue='Cluster_id',palette='Dark2',diag_kind='kde')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Selection of variables for Kprototype Clustering Algorithm\nData_k= Data[['Month','ActualElapsedTime','TaxiOut', 'DepDelay']]\nprint(Data_k.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting the dataset into matrix\nX = Data_k.as_matrix()\n# Running K-Prototype clustering\nkproto = KPrototypes(n_clusters=2, init='Huang', verbose=0, random_state=42,max_iter=20, n_init=50,n_jobs=-2,gamma=0.15) \nclusters = kproto.fit_predict(X, categorical=[0])\n#Visualize K-Prototype clustering \ndf=pd.DataFrame(Data_k)\ndf['Cluster_id']=clusters\nprint(df['Cluster_id'].value_counts())\nsns.pairplot(df,hue='Cluster_id',palette='Dark2',diag_kind='kde')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above plots, it is observed that there are two clusters (0,1) and the variables Day of the\nmonth or Day of the week or Month of the year do not play a significant role in distinction between\nclusters. For Cancelled variable, we do see distinct clusters for both cancelled and not cancelled\nlevels, but the number of points is very small in cancelled level to make a conclusion."},{"metadata":{},"cell_type":"markdown","source":"### Cluster Agreement \n\nWe have seen that K-prototype is better in terms of its ability to handle categorical variables\nalong with numerical ones. Also, the clusters are more meaningful with the K-prototype model."},{"metadata":{"trusted":true},"cell_type":"code","source":"#join DBScan and K-prototype data frames\nClusters=pd.concat([df_Cancelled, df_DBSCAN, Data[['DayOfWeek','DayofMonth','Month']]], axis = 1)\nClusters=Clusters.iloc[:, [0,1,2,3,4,8,9,10,11]]\nClusters.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For further clarity, we compare the cluster agreements between DBSCAN and K-prototype for 10\npairs of random values. The pairs considered are two adjacent rows, for example, rows index of\n18625 and 19746 form the first pair, 11209 and 12974 form the second pair and so on."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Verify if randow rows have the same cluster Id between DBScan and K-Prototype\nrandom_rows=Clusters.sample(20,random_state=36)\nrandom_rows.iloc[:, [0,1,2,3,4,5]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above shows random 20 rows of the data with the cluster ids of both the models. We\nobserve that for majority of the pairs of data points (19 pairs), there is agreement among both\nDBSCAN and K-Prototype models of assigning the points together in the same cluster. This is\nalso influenced by the fact that clusters formed by DBSCAN were imbalanced. In one of the cases\n(19612 and 16607), there is agreement with both models classifying the points in the pair to\ndifferent clusters. Only in one pair (18625 and 19746) was there a disagreement in terms of Kprototype\nclassifying them in different clusters but DBSCAN putting them in the same cluster."},{"metadata":{"trusted":true},"cell_type":"code","source":"#crosstab table by Day of Week and DBScan Cluster IDs\npd.crosstab(Clusters.DayOfWeek, Clusters.Cluster_id_DBSCAN, margins=True,normalize='columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#crosstab table by Day of Week and K-Prototype Cluster IDs\npd.crosstab(Clusters.DayOfWeek, Clusters.Cluster_id_K_Prototype, margins=True,normalize='columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#crosstab table by Day of Month and K-Prototype Cluster IDs\npd.crosstab(Clusters.DayofMonth, Clusters.Cluster_id_K_Prototype, margins=True,normalize='columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#crosstab table by Day of Month and DBScan Cluster IDs\npd.crosstab(Clusters.DayofMonth, Clusters.Cluster_id_DBSCAN, margins=True,normalize='columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#crosstab table by Month and K-Prototype Cluster IDs\npd.crosstab(Clusters.Month, Clusters.Cluster_id_K_Prototype, margins=True,normalize='columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#crosstab table by Month and DBScan Cluster IDs\npd.crosstab(Clusters.Month, Clusters.Cluster_id_DBSCAN, margins=True,normalize='columns')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion & Recommendation\nFrom our above analysis, we can conclude the following:\n1. Day of the month, Day of the week, or Month do not help in distinct cluster formation and hence we can conclude that these variables do not seem to affect the performance of flights.\n2. For Cancelled variable, we see that there are distinct clusters when the flight is cancelled but given that we have a limited random sample, we cannot conclusively say that the cancellation of flights depends on delays.\n3. We did not find any parameter of the models which gave meaningful distinct clusters with respect to Departure delays â€“ in all cases there was overlap between clusters.\n4. For the ActualElapsedTime variable (time between departure and arrival), there are distinct clusters being formed based on shorter vs longer flights, but it is not clear if there is a relationship with delays.\n\nIn light of these conclusions, we can recommend that K-prototype model be used with respect to\nmodelling especially where categorical variables are involved along with the numerical variables.\nHowever, further analysis and modelling is required to improve the findings and conclusively\nanswer our chosen research questions in order to derive business insights."},{"metadata":{},"cell_type":"markdown","source":"## References\n* Alboukadel Kassambara. (n.d.). Assessing Clustering Tendency. Retrieved May 2019,from Data Novia: https://www.datanovia.com/en/lessons/assessing-clusteringtendency/\n* Amy. (2017, September 8). [Python]Principal Component Analysis and K-means clustering with IMDB movie datasets. Retrieved May 2019, from Amysfernweh Wordpress: https://amysfernweh.wordpress.com/2017/09/08/pythonprincipalcomponent-analysis-and-k-means-clustering-with-imdb-movie-datasets/\n* Gonzalez, G. (2016, November 12). Airlines Delay. Retrieved April 2019, from Kaggle:https://www.kaggle.com/giovamata/airlinedelaycauses\n* Huang, Z. (1997). Clustering Large Data Sets With Mixed Numeric and Categorical Values.\n* MATEVZKUNAVER. (2017, June 20). Hopkins test for cluster tendency. Retrieved May 2019, from Wordpress.com:https://matevzkunaver.wordpress.com/2017/06/20/hopkins-test-for-clustertendency/\n* Ren, Y. (2019, April). Practical Data Science lecture slides - RMIT. Melbourne,Victoria, Australia.\n* Singh, A. (2018, August 7). K-Prototype Clustering. Retrieved 2019 April, from Github:https://github.com/aryancodify/Clustering/blob/master/KPrototype%2Bclustering.ipynb\n* Vlo. (2017, April 11). Cumulative Explained Variance for PCA in Python. Retrieved May 2019, from Stack Overflow:https://stackoverflow.com/questions/43355044/cumulative-explained-variance-forpca-\nin-python\n* Zychlinski, S. (2018, February 24). The Search for Categorical Correlation. Retrieved from Towards Data Science: https://towardsdatascience.com/the-search-forcategorical-correlation-a1cf7f1888c9"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}