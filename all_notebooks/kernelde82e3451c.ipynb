{"cells":[{"metadata":{"_uuid":"44cf32d4-7544-4d51-bdbb-f01aff7ec379","_cell_guid":"a53f0a72-869c-4fa9-bb6a-11ef9379e4d5","trusted":true},"cell_type":"markdown","source":"   #     Real State Pricing in Germany","execution_count":null},{"metadata":{"_uuid":"5722144d-26f2-47f0-bcad-46658eab6b23","_cell_guid":"f259bd7d-fc9b-48fe-a041-1a2d70f8196b","trusted":true},"cell_type":"markdown","source":"# Problem definition\n\nHow are German new development real estate apartments priced? (In relation to “macro” factors)","execution_count":null},{"metadata":{"_uuid":"194df329-6501-47be-8e4e-44ea2c035080","_cell_guid":"a1962414-f720-4686-ad5a-147db9e40f05","trusted":true},"cell_type":"markdown","source":"# Hyphotesis\n\nPrices in large urban areas with mayor access to facilities like, hospitals, restaurants, etc have a higher price than the rest of the areas.","execution_count":null},{"metadata":{"_uuid":"9d4ef7bf-85fc-4451-b198-b406ebd7728e","_cell_guid":"8f32fb02-6c8c-49fc-a7bc-e0ebe6d5b67f","trusted":true},"cell_type":"markdown","source":"# Implementation overview\n\n#### Data collection\n* location (e.g. ZIP code, expensive neighbourhood)\n* demographics (e.g. local population structure, income levels)\n* infrastructure (e.g. proximity to public transportation, schools,hospitals, playgrounds, restaurants)? \n\n#### Data Analysis\n* Delimitation of the data\n* Outlayer analysis\n* Normality of the Residuals\n* Correlationship analysis\n* Heteroskedasticity\n* Linearity\n\n#### Model Specification\n#### Result Visualization \n#### Conclusion","execution_count":null},{"metadata":{"_uuid":"cce04672-afe2-4932-9e3b-66d424127d29","_cell_guid":"69f58d2e-2d61-4dfd-8184-acd78626c864","trusted":true},"cell_type":"markdown","source":"## Prepare Notebook","execution_count":null},{"metadata":{"_uuid":"b82354d6-f4ed-45a5-be13-6ce50eb897f5","_cell_guid":"61207096-b846-48fa-9038-310542bad359","trusted":true},"cell_type":"code","source":"import geopandas as gpd\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\nimport seaborn as sns\nplt.style.use('seaborn')\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3fa05cfc-2085-4eca-b480-183452491bd4","_cell_guid":"be4e2ee2-03ab-4dea-acbe-31d196148862","trusted":true},"cell_type":"markdown","source":"## Germany Data \n\nThe main data source for this notebook is\n\n- `master_data.csv`: csv file with the macroeconomic factors on distric level, mainly obtained from http://www.statistikportal.de/en\n- `osm.csv`:  csv file with the facilities on a postal code level, obtained from OpenStreet Map\n- `price.csv`:  csv file with the pricing, obtained with web Scrapping from https://www.immobilienscout24.de/\n- `plz-gebiete.shp`: shapefile with germany postal codes polygons.\n- `zuordnung_plz_ort.csv`: postal code to city and bundesland mapping.\n- `plz_einwohner.csv`: population is assigned to each postal code area.\n\n[www.suche-postleitzahl.org/downloads](https://www.suche-postleitzahl.org/downloads). Here we download three data sets:","execution_count":null},{"metadata":{"_uuid":"9d36938a-015b-464e-acf2-9e1a977e6239","_cell_guid":"216af1e9-9f1f-4b81-ac36-a5eb3a925f6a","trusted":true},"cell_type":"markdown","source":"### Germany Maps\n\nTo prove our hypothesis we have created a new feature wich contains the Ecluedian disntance of the center of every postal code the top 10 most populated cities in Germany. Also included the variable wether a postal code belongs to a east or west, As many studies suggested there is a highest price in the west of Germany","execution_count":null},{"metadata":{"_uuid":"5ef0506d-2159-40c5-9c0c-6c386ecacd5e","_cell_guid":"c6ff27ee-78fc-4509-97fa-9d8f26939e44","trusted":true},"cell_type":"code","source":"plz_shape_df = gpd.read_file('../input/realstate/plz-gebiete.shp', dtype={'plz': str})\nplz_shape_df['lat'] = plz_shape_df['geometry'].centroid.y\nplz_shape_df['lon'] = plz_shape_df['geometry'].centroid.x\nplz_shape_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad80982a-a75a-442a-9444-0297be994ece","_cell_guid":"a5289c89-2f4d-43ff-96c1-5bba83e5799d","trusted":true},"cell_type":"markdown","source":"The `geometry` column contains the polygons which define the postal code's shape. We can use [geopandas mapping tools](http://geopandas.org/mapping.html) to generate the map with the `plot` method.","execution_count":null},{"metadata":{"_uuid":"26e4e4bf-a8ef-4dad-85af-c4e72208ebe2","_cell_guid":"22e6424c-36a4-489c-aa87-532802d289e3","trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = [16, 11]\n\n# Get lat and lng of Germany's main cities. \ntop_cities = {\n    'Berlin': (13.404954, 52.520008), \n    'Cologne': (6.953101, 50.935173),\n    'Düsseldorf': (6.782048, 51.227144),\n    'Frankfurt am Main': (8.682127, 50.110924),\n    'Hamburg': (9.993682, 53.551086),\n    'Leipzig': (12.387772, 51.343479),\n    'Munich': (11.576124, 48.137154),\n    'Dortmund': (7.468554, 51.513400),\n    'Stuttgart': (9.181332, 48.777128),\n    'Nuremberg': (11.077438, 49.449820),\n    'Hannover': (9.73322, 52.37052)\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6552c2b8-cfb2-4059-a874-08a0e0c15dc7","_cell_guid":"37095fde-775e-46ac-8244-d4378f494e98","trusted":true},"cell_type":"code","source":"# Create feature distance to closest big city\n# Create feature east or west of germany\ndef get_closest_city(lat, lon):\n    distances = []\n    for city in top_cities:\n        #print(top_cities[city][0])\n        dist = np.sqrt( (lat - top_cities[city][1]) ** 2 + (lon - top_cities[city][0]) ** 2)\n        distances.append(dist)\n    \n    return np.min(distances)\n    \nplz_shape_df['distance'] =  plz_shape_df.apply(lambda x: get_closest_city(x['lat'], x['lon']), axis=1)\nplz_shape_df['east'] =  plz_shape_df.apply(lambda x: 1 if x['lon'] > 10.35 else 0,  axis=1)\nplz_shape_df['west'] =  plz_shape_df.apply(lambda x: 1 if x['lon'] <= 10.35 else 0,  axis=1)\n\nplz_shape_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for name in ['east', 'west']:\n    if plz_shape_df[name].isna().values.any():\n        print(\"there is na\", name)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"362228e8-9991-4ff2-9e90-1350915b1318","_cell_guid":"b0fe30ac-9bd2-4251-8157-e2e1101d5770","trusted":true},"cell_type":"markdown","source":"### Data Aggregation by Postal Code","execution_count":null},{"metadata":{"_uuid":"11be4bf1-2dfc-4d79-95bc-ecc7d4b30af4","_cell_guid":"5437bc57-306c-4482-9c48-6d0182890d93","trusted":true},"cell_type":"markdown","source":"Since a distric area is too big, we have decided to segment our data in a smaller division wich is Postal Code. Other divisions like 100 m grid or 1000 m grid can be posible as well, but since postal codes area more human readable. The data collection for a smaller division than that would take very long time.","execution_count":null},{"metadata":{"_uuid":"13ebb5ab-cc7a-46c9-9f4f-120072ada220","_cell_guid":"0e1160a6-0841-46f1-9bc7-d8c3903999cf","trusted":true},"cell_type":"code","source":"# Display on how the segmentation of the country was made\nplz_shape_df = plz_shape_df \\\n    .assign(first_dig_plz = lambda x: x['plz'].str.slice(start=0, stop=1))\nplz_shape_df['first_dig_plz'] = plz_shape_df['first_dig_plz'].astype(float)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cfd385e7-c339-4bbc-9675-45bc761ddcc7","_cell_guid":"9910fc3e-e6fa-4247-9510-7e2f4df73de9","trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\n\nplz_shape_df.plot(\n    ax=ax, \n    column='first_dig_plz', \n    categorical=True, \n    legend=True, \n    legend_kwds={'title':'First Digit', 'loc':'lower right'},\n    cmap='tab20',\n    alpha=0.9\n)\n\nfor c in top_cities.keys():\n\n    ax.text(\n        x=top_cities[c][0], \n        y=top_cities[c][1] + 0.08, \n        s=c, \n        fontsize=12,\n        ha='center', \n    )\n\n    ax.plot(\n        top_cities[c][0], \n        top_cities[c][1], \n        marker='o',\n        c='black', \n        alpha=0.5\n    )\n\nax.set(\n    title='Germany First-Digit-Postal Codes Areas', \n    aspect=1.3,\n    facecolor='white'\n);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"efbcbe39-9db8-47e0-bf30-14a80bccad24","_cell_guid":"73433b9f-9a10-4f52-9c22-c00ace2e4242","trusted":true},"cell_type":"markdown","source":"Map each postal code to the corresponding region:","execution_count":null},{"metadata":{"_uuid":"5d808e3f-758d-4775-a7b0-2a6982754bcb","_cell_guid":"2de5ca49-ef1b-438f-80e3-7b92d6f04cba","trusted":true},"cell_type":"code","source":"# Merge data.\nplz_region_df = pd.read_csv(\n    '../input/realstate/zuordnung_plz_ort.csv', \n    sep=',', \n    dtype={'plz': str}\n)\n\nplz_region_df.drop('osm_id', axis=1, inplace=True)\n\nplz_region_df.head()\n\ngermany_df = pd.merge(\n    left=plz_shape_df, \n    right=plz_region_df, \n    on='plz',\n    how='inner'\n)\ngermany_df.drop(['note'], axis=1, inplace=True)\ngermany_df[germany_df['ort'].str.match('Kö')]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for name in ['east', 'west']:\n    if germany_df[name].isna().values.any():\n        print(\"there is na\", name)\ngermany_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cities_df = gpd.read_file('../input/realstate/de.csv')\ncities_df.index = cities_df.city\ncities_df = cities_df.rename(\n    index={'Munich': 'München', 'Cologne': 'Köln', 'Frankfurt': 'Frankfurt am Main'},\n    columns={'city': 'ort', 'lat': 'lat_city', 'lng': 'lon_city'})\ncities_df['ort'] = cities_df.index\ncities_df['lat_city'] = cities_df['lat_city'].astype(float)\ncities_df['lon_city'] = cities_df['lon_city'].astype(float)\ncities_df = cities_df[['ort', 'lat_city', 'lon_city']]\ncities_df.head()\ncities_df[cities_df['ort'].str.match('Kö')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(germany_df.shape)\ngermany_df = pd.merge(\n    left=germany_df, \n    right=cities_df, \n    on='ort',\n    how='left'\n)\nprint(germany_df.shape)\ngermany_df.head()\n\ngermany_df['east_city'] =  germany_df.apply(lambda x: 1 if x['lon_city'] < x['lon'] else 0,  axis=1)\ngermany_df['north_city'] =  germany_df.apply(lambda x: 1 if x['lat_city'] <= x['lat'] else 0,  axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"germany_df[germany_df['ort'].str.match('Münche')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data loading and merging of the Sets\n\nLoad habitants per zip code ","execution_count":null},{"metadata":{"_uuid":"04cddede-6bf4-4f0e-835d-12dc4627a2ea","_cell_guid":"42852786-e885-4bfe-88d0-3d1d2f952d10","trusted":true},"cell_type":"code","source":"plz_einwohner_df = pd.read_csv(\n    '../input/realstate/plz_einwohner.csv', \n    sep=',', \n    dtype={'plz': str, 'einwohner': int}\n)\n\nplz_einwohner_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load macro economic factors","execution_count":null},{"metadata":{"_uuid":"67921db3-d9a8-4211-9ab4-5dc5a8edd40c","_cell_guid":"6a3e6eb5-6618-499e-9228-50b223869662","trusted":true},"cell_type":"code","source":"master_data = pd.read_csv(\n    '../input/realstate/master_data.csv', \n    sep=',', \n    dtype={'plz': str, \n           'einwohner_plz': int\n          }\n)\n\nmaster_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Open Street Maps\n\n* cafe\n* doctors\n* fast_food\n* restaurant\n* hospital\n\nhttps://wiki.openstreetmap.org/wiki/Key:amenity","execution_count":null},{"metadata":{"_uuid":"eec9bc34-8a85-4589-98b3-62f356fa3a73","_cell_guid":"8c473058-c3bf-41b5-8813-05762edabccf","trusted":true},"cell_type":"code","source":"\nosm_plz_df = pd.read_csv(\n    '../input/realstate/osm.csv', \n    sep=',', \n    dtype={'zip_code': str, \n           #'einwohner_plz': int\n          }\n)\nosm_plz_df = osm_plz_df.rename(columns={'zip_code': 'plz'})\namenities_df = osm_plz_df.groupby(['plz', 'amenity']).size().reset_index(name='count')\nzip_code = osm_plz_df.groupby(['plz']).size().reset_index(name='osm')\namenities_df.sort_values('plz', ascending=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* bus\n* train\n* university","execution_count":null},{"metadata":{"_uuid":"34e48d78-8e7b-4557-8686-038f9fcc2718","_cell_guid":"05dc207a-5060-470a-aaad-a38a95f9cf8d","trusted":true},"cell_type":"code","source":"# Merge data.\n# habitants per zip code\ngermany_df2 = pd.merge(\n    left=germany_df, \n    right=plz_einwohner_df, \n    on='plz',\n    how='left'\n)\n# macro factors by distric\ngermany_df2 = pd.merge(\n    left=germany_df2, \n    right=master_data, \n    on='plz',\n    how='left'\n)\n\namenities = osm_plz_df['amenity'].unique()\n\n\nfor amenity in amenities:\n# restuarants, cafe, hospital, doctor, fast_food\n    df = amenities_df[amenities_df.amenity == amenity]\n    df[amenity] = df['count'].clip(upper=300)\n    #df.fillna(0)\n    germany_df2 = pd.merge(\n        left=germany_df2, \n        right=df[['plz', amenity]], \n        on='plz',\n        how='left'\n    )\n    #germany_df2[amenity] = germany_df2[amenity].fillna(0)\n    \n# bus, train, university\ntags = ['university', 'train_station', \"'bus': 'yes'\"]\nfor tag in tags:\n    df = osm_plz_df[osm_plz_df['tag'].str.contains(tag)].groupby(['plz']).size().reset_index(name=tag)\n    germany_df2 = pd.merge(\n        left=germany_df2, \n        right=df[['plz', tag]], \n        on='plz',\n        how='left'\n    )\n    #germany_df2[tag] = germany_df2[tag].fillna(0)\n    \ntags = ['university', 'train_station', 'bus']\ngermany_df2 = germany_df2.rename(columns={\"'bus': 'yes'\": 'bus'})\n\n# total\ngermany_df2 = pd.merge(\n    left=germany_df2, \n    right=zip_code[['plz', 'osm']], \n    on='plz',\n    how='left'\n)\n# total\ngermany_df2 = pd.merge(\n    left=germany_df2, \n    right=zip_code[['plz', 'osm']], \n    on='plz',\n    how='left'\n)\ngermany_df2 = germany_df2.drop_duplicates(subset='plz')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for name in ['east', 'east_city']:\n    if germany_df2[name].isna().values.any():\n        print(\"there is na\", name)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8eaf68bc-fda3-4d80-8a22-ac1689754fd0","_cell_guid":"3217d6cf-7f64-42fd-ba41-78d4da8635d4","trusted":true},"cell_type":"code","source":"# data should be splitted into train and test before scaling\n\ndef fill_missing(df):\n    columns = list(df.columns)\n    datatypes = ['geometry', 'Gangelt']\n    df = df.replace(r'[-|#|x]', np.nan, regex=True)\n\n    for name, types in zip(df.columns, df.dtypes):\n        #print(name)\n        if name != 'plz' and str(types) not in datatypes:\n            try:\n                df[name] = df[name].astype(float)\n                df[name] = df[name].fillna(df[name].mean())\n                #df[name] = (df[name] - df[name].min())/(df[name].max() - df[name].min())\n            except:\n                pass\n        #print(df[name].dtypes)\n        #print(\"there is nan\", df[name].isnull().values.any())\n\n\n    return df\n\ngermany_df3 = fill_missing(germany_df2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7754fbf6-4690-49ec-b164-89af697aaebc","_cell_guid":"4c133606-b738-406d-85f2-397ef2c668db","trusted":true},"cell_type":"markdown","source":"## Country Maps\n\nGenerate map:","execution_count":null},{"metadata":{"_uuid":"df4639a0-6b13-4acb-a660-b9f1cce9baa8","_cell_guid":"41928d14-80d7-4c0d-975c-5c40b47ba909","trusted":true},"cell_type":"code","source":"\ndef plot_country(germany_df, col_name):\n    \n    fig, ax = plt.subplots()\n    germany_df.plot(\n        ax=ax, \n        column=col_name, \n        categorical=False, \n        legend=True, \n        cmap='GnBu',\n        #alpha=100\n    )\n\n    for c in top_cities.keys():\n\n        ax.text(\n            x=top_cities[c][0], \n            y=top_cities[c][1] + 0.08, \n            s=c, \n            fontsize=12,\n            ha='center', \n        )\n\n        ax.plot(\n            top_cities[c][0], \n            top_cities[c][1], \n            marker='o',\n            c='black', \n            alpha=0.5\n        )\n    ax.set(\n        title=('Germany: %s per Postal Code' % col_name), \n        aspect=1.3, \n        facecolor='lightblue'\n    );\nplot_country(germany_df2, 'east')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9eba29b3-708b-4c9d-aae8-f88f17c9ff2e","_cell_guid":"e41d994c-1185-4128-b073-ef21a8986a8b","trusted":true},"cell_type":"markdown","source":"## City Maps\n\nWe can now filter for cities using the `ort` feature.","execution_count":null},{"metadata":{"_uuid":"91cdf852-fa58-4fdc-a27e-87ff9e7cdb5b","_cell_guid":"9273e905-0f3a-4fda-96c4-0aaf3a258553","trusted":true},"cell_type":"markdown","source":"- Berlin","execution_count":null},{"metadata":{"_uuid":"c54eb01e-82dd-47f4-9180-a54de6569d25","_cell_guid":"1b73ee91-aa31-4de3-8cc4-a3e3d9bf1fc1","trusted":true},"cell_type":"code","source":"def plot_city(df, city_name, col_name, col_name2=None):\n    query = 'ort ==  \"%s\"' % city_name\n    print(query)\n    berlin_df = df.query(query)\n    \n    if col_name2 is not None:\n        fig, (ax1, ax2) = plt.subplots(1,2)\n    else:\n        fig, ax1 = plt.subplots()\n    berlin_df.plot(\n        ax=ax1, \n        column=col_name, \n        categorical=False, \n        legend=True, \n        cmap='GnBu',\n        #scheme='quantiles',\n        #k=5\n    )\n    title = '%s: Number of %s per Postal Code' % (city_name, col_name)\n    ax1.set(\n        title=title, \n        aspect=1.3,\n        facecolor='lightblue'\n    );\n    \n    if col_name2 is not None:\n        berlin_df.plot(\n            ax=ax2, \n            column=col_name2, \n            categorical=False, \n            legend=True, \n            cmap='GnBu',\n        )\n\n        ax2.set(\n            title = '%s: Number of %s per Postal Code' % (city_name, col_name2),\n            aspect=1.3,\n            facecolor='lightblue'\n        );\n    \nplot_city(germany_df2, \"Frankfurt am Main\", \"east_city\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff2493c4-1372-41fc-8849-11f8b23ae466","_cell_guid":"e3d58e2c-bec5-4557-ae62-9ad747f5b1db","trusted":true},"cell_type":"markdown","source":"A property constructed after 2010 is considered to be an aprtment it should have atmost 6 rooms and should be priced more than 10000 Euros. Moreover we are just considering Single Family House, Multi Family House, Semi-Detached House and Mid- Terrace House nothing apart from these. ","execution_count":null},{"metadata":{"_uuid":"d437a088-0d2c-4ee6-af29-2c1b85ff5754","_cell_guid":"482e2a74-8d88-45e7-bb09-b95eb4d5e75e","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import scale\nimport statsmodels.stats.api as sms\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\ndata = pd.read_csv(\"../input/germany-housing-rent-and-price-data-set-apr-20/apr20_price.csv\")\n\ndata[\"obj_yearConstructed\"] = data[\"obj_yearConstructed\"].astype(float)\n\nx = data.copy()\nx = x[x[\"obj_yearConstructed\"] >= 2000] \n\nx = x[x[\"obj_noRooms\"] <= 6]\nx = x[x[\"obj_purchasePrice\"] >= 10000]\nx = x[x[\"obj_purchasePrice\"] < x[\"obj_purchasePrice\"].quantile(0.99) ]\n\n\nbtype =[\"single_family_house\",\"multi_family_house\",\"semidetached_house\",\"mid_terrace_house\"]\n\nx = x[x[\"obj_buildingType\"].isin(btype)]\nx['geo_plz'] = x['geo_plz'].astype(int)\nx['geo_plz'] = x['geo_plz'].astype(str)\nx['geo_plz'] = x['geo_plz'].apply(lambda x: x.zfill(5))\nx['plz'] = x['geo_plz']\n\n\navg_price_df = x.groupby(['obj_regio1', 'plz']).mean().reset_index(); \n\n\nx_copy = x.copy()\nx_copy = pd.merge(\n    left=x_copy, \n    right=germany_df2, \n    on='plz',\n    how='left'\n)\n  \n\n    \nz = pd.merge(\n    left=germany_df3, \n    right=avg_price_df, \n    on='plz',\n    how='left'\n)\nx_copy = x_copy[x_copy['obj_purchasePrice'].notna()]\nx_copy['log_price'] = np.log(x_copy['obj_purchasePrice'])\nx_copy = x_copy.replace(r'[-|#|x]', np.nan, regex=True)\nx_copy.dropna(inplace=True, subset=['east', 'east_city', 'north_city'])\nprint(x_copy.shape)\nx_copy.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_copy['east'].isna().sum())\nprint(z['east'].isna().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"master_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(\"zip code specif variables\")\n\namenity = amenities[1:-1]\nzip_code_columns = [*amenity, *tags, 'east', 'north_city', 'east_city', 'distance', 'einwohner', 'qkm/plz', 'gdp_habitant']\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Morans' I","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"!pip install libpysal\n!pip install esda\nimport libpysal as lps\n\n#print(z.columns)\n#z = fill_missing(z)\nwq =  lps.weights.Queen.from_dataframe(z[['geometry', *zip_code_columns]])\nwq.transform = 'r'\n#wq.sparse, y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove island, no neighbors \nz1 = z.copy()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nz1['log_price'] = np.log(z1['obj_purchasePrice'])\ny = z1['log_price']\n\ny_lag = lps.weights.lag_spatial(wq, y)\nz1['y_lag'] = y_lag\nz1['einwohner_lag'] = lps.weights.lag_spatial(wq, z1['einwohner'])\nimport mapclassify as mc\n\nplot_city(z1, \"Berlin\", 'einwohner_lag', 'einwohner')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Global Spatial Autocorrelation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import esda\nz3 = z1[z1['y_lag'] > 2]\nz3 = fill_missing(z3) # data filled with average\nwq3 =  lps.weights.Queen.from_dataframe(z3[['geometry', *zip_code_columns]])\nmi = esda.moran.Moran(z3['log_price'], wq3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mi.I ## moran i ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sbn\nsbn.kdeplot(mi.sim, shade=True)\nplt.vlines(mi.I, 0, 1, color='r')\nplt.vlines(mi.EI, 0,1)\nplt.xlabel(\"log transform of price\")\nplt.title(\"Moran's I\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mi.p_sim","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Local Autocorrelation: Hot Spots, Cold Spots, and Spatial Outliers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lag_nan_max = np.nanmax(y_lag) ; lag_nan_min = np.nanmin(y_lag) ;lag_nan_mean = np.nanmean(y_lag)\n\nplt.plot(z1['log_price'], z1['y_lag'], '.', color = 'firebrick') \n\nplt.vlines(z1['log_price'].mean(),lag_nan_min,lag_nan_max,linestyle = '--')\nplt.hlines(z1['y_lag'].mean(), z1['log_price'].min(), z1['log_price'].max(), linestyle=\"--\")\n\n#plt.title('Moran Scatterplot')\n#plt.ylabel('Spatial Lag Price')\n#plt.xlabel('Price')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nz2 = z1[z1['y_lag'] > 2]\n\n#plt.plot(z2['log_price'], z2['y_lag'], '.', color = 'firebrick') \nax = sns.regplot(x=z2['log_price'], y=z2['y_lag'], color=\"firebrick\")\n\nplt.vlines(z2['log_price'].mean(), \n           z2['y_lag'].min(),\n           z2['y_lag'].max(),\n           linestyle = '--')\n\nplt.hlines(z2['y_lag'].mean(),\n           z2['log_price'].min(),\n           z2['log_price'].max(),\n           linestyle=\"--\")\n\n#plt.title('Moran Scatterplot')\n#plt.ylabel('Spatial Lag Price')\n#plt.xlabel('Price')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, instead of a single I statistic, we have an array of local Ii statistics, stored in the .Is attribute, and p-values from the simulation are in p_sim.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"li = esda.moran.Moran_Local(z3['log_price'], wq3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sig = li.p_sim < 0.05\nhotspot = sig * li.q==1\ncoldspot = sig * li.q==3\ndoughnut = sig * li.q==2\ndiamond = sig * li.q==4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spots = ['n.sig.', 'hot spot']\nlabels = [spots[i] for i in hotspot*1]\nz3['labels'] = labels\nlen(labels)\n#from matplotlib import colors\n#hmap = colors.ListedColormap(['red', 'lightgrey'])\n\nplot_city(z3, \"Berlin\", \"labels\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Interaction, Transformation\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"residentital_b_type = ['residencial_b_1_p', 'residencial_b_2_p', 'residencial_b_3_p']\nnew_apartment_permint = [ 'new_apartment_permit_1', 'new_apartment_permit_2_3', 'new_apartment_permit_4_5', 'new_apartment_permit_6']\n\nfloor_area = [ 'floor_area_settlement', 'floor_area_traffic', 'floor_area_vegetation']\nfloor_use = ['floor_use_industry_commerce', 'floor_use_leisure']\n\npeople = ['age_3', 'age_3_6', 'age_6_15', 'age_15-18', 'age_18_25', 'age_25_30', 'age_30_40', 'age_40_50', 'age_50_60', 'age_60_75', 'age_75_x']\nnew_apartment_permit = ['new_apartment_permit', 'new_apartment_permit_1', 'new_apartment_permit_2_3', 'new_apartment_permit_4_5', 'new_apartment_permit_6']\n\nother = ['plz', 'note', 'city', 'landkreis id', 'ags', 'ags_text', 'state id',\n           'state name', 'region id', 'region/city', 'district id', 'DG',\n           'district name', 'qkm/plz', 'einwohner/ plz', 'gdp_habitant', \n           'residencial_building', 'floor_area', 'new_apartment_permit',\n          'house_hold_size', 'floor_use', 'house_hold_size_1', 'floor_use_residential', 'tax_payers', 'income_total', 'house_hold_size_6_p',\n          'house_hold_type_1', 'house_hold_type_2', 'house_hold_type_3', 'house_hold_type_4',  'floor_use_residential', 'wage_n_income', 'gpd_employee']\n\n\n\ndistric_columns = [c for c in master_data.columns if c not in other]\nprint(\"distric specific variables\")\nprint(sorted(distric_columns))\n\nfeatures = [*zip_code_columns, *distric_columns]\ndf = x_copy.copy()\ndf = df.replace(r'[-|#|x]', np.nan, regex=True)\ndf[features] = df[features].astype(float)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(people)\nprint(floor_area)\nprint(floor_use)\nprint(residentital_b_type)\nprint(new_apartment_permint)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"zip_code_columns = [*amenity, *tags]\ndistric_columns = [c for c in master_data.columns if c not in other]\n\n\nfeatures2 = ['distance', 'einwohner']\nfeatures3 = []\nname = 'einwohner'\ndf_habitant = df.copy()\n\nfactor = df_habitant['einwohner'] / df_habitant['habitants']\n\nfactor_2 = germany_df3['einwohner'].astype(float) / germany_df3['habitants'].astype(float)\ndf_habitant['density'] = df_habitant['einwohner'] / df_habitant['qkm']\n\nfor name in people:\n    df_habitant[name] = df_habitant[name].astype(float) \n    \n    df_habitant[name + '_hab'] = df_habitant[name] * factor\n    germany_df3[name + '_hab'] = germany_df3[name] * factor_2\n    \nfor name in [*amenity, *tags]:\n    \n    df_habitant[name] = df_habitant[name].astype(float) \n    df_habitant[name + '_hab'] = df_habitant[name] * factor\n    germany_df3[name + '_hab'] = germany_df3[name] * factor_2\n    \n    features2.append(name + '_hab')\n  \n\ndf_habitant['pop_density'] = df_habitant['einwohner'] / df_habitant['qkm/plz']\ngermany_df3['pop_density'] = germany_df3['einwohner'] / germany_df3['qkm/plz']\n\n\n\nfor name in distric_columns:\n    if name not in [*floor_area, *floor_use, 'habitants', *people]:\n        df_habitant[name] = df_habitant[name].astype(float) \n        df_habitant[name + '_hab'] = df_habitant[name] * factor\n        germany_df3[name + '_hab'] = germany_df3[name] * factor_2\n        if name not in [ *residentital_b_type]:\n            features2.append(name + '_hab')\n\nname = 'qkm/plz'\nfactor = df_habitant['qkm/plz'] / df_habitant['qkm']\nfactor_2 = germany_df3['qkm/plz'] / germany_df3['qkm']\nfor name in [*floor_area, *floor_use]:\n    df_habitant[name] = df_habitant[name].astype(float) \n    df_habitant[name + '_qkm_plz'] = df_habitant[name] * factor\n    germany_df3[name + '_qkm_plz'] = germany_df3[name] * factor_2\n    \n    features3.append(name + '_qkm_plz')\n\n\n        \n# interactions on an observations level\ndf_habitant['young'] = df_habitant['age_3_hab'] + df_habitant['age_3_6_hab'] + df_habitant['age_6_15_hab'] + df_habitant['age_15-18_hab'] + df_habitant['age_18_25_hab']\ndf_habitant['middle_age'] = df_habitant['age_25_30_hab'] + df_habitant['age_30_40_hab'] + df_habitant['age_40_50_hab']\ndf_habitant['old_age'] = df_habitant['age_50_60_hab'] + df_habitant['age_60_75_hab'] + df_habitant['age_75_x_hab']\ndf_habitant['young_ratio']= df_habitant['young'] / ( df_habitant['young']  + df_habitant['middle_age'] + df_habitant['old_age'])\ndf_habitant['middle_ratio']= df_habitant['middle_age'] / ( df_habitant['young']  + df_habitant['middle_age'] + df_habitant['old_age'])\ndf_habitant['old_age_ratio'] = df_habitant['old_age'] / ( df_habitant['young']  + df_habitant['middle_age'] + df_habitant['old_age'])\ndf_habitant['floor_area_per_veg'] =  df_habitant['floor_area_vegetation'] / ( df_habitant['floor_area_settlement'] + df_habitant['floor_area_traffic'] + df_habitant['floor_area_vegetation'] )                                                                           \ndf_habitant['salary_per_employed'] = df_habitant['income_total'].astype(float)  / df_habitant['employed'].astype(float)  \ndf_habitant['residencial_b_1_p_ratio'] = df_habitant['residencial_b_1_p_hab'] / ( df_habitant['residencial_b_1_p_hab'] +  df_habitant['residencial_b_2_p_hab'] +  df_habitant['residencial_b_3_p_hab']) \ndf_habitant['residencial_b_2_p_ratio'] = df_habitant['residencial_b_2_p_hab'] / ( df_habitant['residencial_b_1_p_hab'] +  df_habitant['residencial_b_2_p_hab'] +  df_habitant['residencial_b_3_p_hab']) \ndf_habitant['residencial_b_3_p_ratio'] = df_habitant['residencial_b_3_p_hab'] / ( df_habitant['residencial_b_1_p_hab'] +  df_habitant['residencial_b_2_p_hab'] +  df_habitant['residencial_b_3_p_hab']) \n\n# iteractions zip code level\ngermany_df3['young'] = germany_df3['age_3_hab'] + germany_df3['age_3_6_hab'] + germany_df3['age_6_15_hab'] + germany_df3['age_15-18_hab'] + germany_df3['age_18_25_hab']\ngermany_df3['middle_age'] = germany_df3['age_25_30_hab'] + germany_df3['age_30_40_hab'] + germany_df3['age_40_50_hab']\ngermany_df3['old_age'] = germany_df3['age_50_60_hab'] + germany_df3['age_60_75_hab'] + germany_df3['age_75_x_hab']\ngermany_df3['young_ratio']= germany_df3['young'] / ( germany_df3['young']  + germany_df3['middle_age'] + germany_df3['old_age'])\ngermany_df3['middle_ratio']= germany_df3['middle_age'] / ( germany_df3['young']  + germany_df3['middle_age'] + germany_df3['old_age'])\ngermany_df3['old_age_ratio'] = germany_df3['old_age'] / ( germany_df3['young']  + germany_df3['middle_age'] + germany_df3['old_age'])\ngermany_df3['floor_area_per_veg'] =  germany_df3['floor_area_vegetation'] / ( germany_df3['floor_area_settlement'] + germany_df3['floor_area_traffic'] + germany_df3['floor_area_vegetation'] )                                                                           \ngermany_df3['salary_per_employed'] = germany_df3['income_total'].astype(float)  / germany_df3['employed'].astype(float)  \ngermany_df3['residencial_b_1_p_ratio'] = germany_df3['residencial_b_1_p_hab'] / ( germany_df3['residencial_b_1_p_hab'] +  germany_df3['residencial_b_2_p_hab'] +  germany_df3['residencial_b_3_p_hab']) \ngermany_df3['residencial_b_2_p_ratio'] = germany_df3['residencial_b_2_p_hab'] / ( germany_df3['residencial_b_1_p_hab'] +  germany_df3['residencial_b_2_p_hab'] +  germany_df3['residencial_b_3_p_hab']) \ngermany_df3['residencial_b_3_p_ratio'] = germany_df3['residencial_b_3_p_hab'] / ( germany_df3['residencial_b_1_p_hab'] +  germany_df3['residencial_b_2_p_hab'] +  germany_df3['residencial_b_3_p_hab']) \n\nfeatures2.append('young_ratio')\nfeatures2.append('middle_ratio')\nfeatures2.append('old_age_ratio')\nfeatures2.append('floor_area_per_veg')\nfeatures2.append('salary_per_employed')\n\nfeatures2.append('residencial_b_1_p_ratio')\nfeatures2.append('residencial_b_2_p_ratio')\nfeatures2.append('residencial_b_3_p_ratio')\n\nfeatures2.append('pop_density')\n\nfor col in features2:\n    \n    if col not in ['east', 'north_city', 'east_city', 'habitants', 'qkm', *people]:\n        df_habitant.loc[df_habitant[col] < 0, col] = 0.0001\n        df_habitant.loc[df_habitant[col] == 0, col] = 0.0001\n        df_habitant[col + '_log'] = np.log(df_habitant[col])\n        germany_df3[col + '_log'] = np.log(germany_df3[col])\n        \n        features3.append(col + '_log')\n        \n        #df_habitant[col + '_p1'] = np.power(df_habitant[col],1)\n        #features3.append(col + '_p1')\n        #df_habitant[col + '_p2'] = np.power(df_habitant[col],2)\n        #features3.append(col + '_p2')\n        #df_habitant[col + '_p3'] = np.power(df_habitant[col],3)\n        #features3.append(col + '_p3')  \n\nfeatures2 = sorted(list(set([*features3, 'east', 'north_city', 'east_city', 'einwohner', 'qkm/plz', ])))\nprint(features2)\nprint(len(features2))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(germany_df3.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train test Split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_habitant.shape)\ndf_train, df_test, y_train, y_test = train_test_split(df_habitant, df_habitant['log_price'], test_size=0.33, random_state=42)\nprint(df_train.shape)\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#df_train = fill_missing(df_train) # average\n#df_filled[features] = np.round(imp.transform(df[features].values))\n\nX = df_train.copy()\ny = df_train['log_price']\n\nfrom sklearn.impute import SimpleImputer # fill missing values\nimp_mean = SimpleImputer(strategy='mean')\n\nimp_mean.fit(X[features2])\n\nimputed_train_df = X.copy()\nX[features2] = imp_mean.transform(X[features2])\n\nfor name in features2:\n    if X[name].isna().values.any():\n        print(\"there is na\", name)\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nscaler = StandardScaler()\n\nfeatures2 = sorted(list(set(features2)))\nprint(features2)\n                 \nfor name in features2:\n    if X[name].isna().values.any():\n        print(\"there is na\", name)\nX[features2] = scaler.fit_transform(X[features2])  # scaling of the features\n\nX = sm.add_constant(X[features2])\n\n\nprint(\"there is nan\", y.isna().values.any())\n\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = sm.OLS(y, X).fit() \nresults = reg\nreg.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = pd.Series(reg.params)\nprint(params.sort_values().head(10))\nprint(params.sort_values(ascending=False).head(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Normality of the residuals\n\n* Studentized Residuals\nThe basic idea is to delete the observations one at a time, each time refitting the regression model on the remaining n–1 observations. Then, we compare the observed response values to their fitted values based on the models with the ith observation deleted. This produces deleted residuals. Standardizing the deleted residuals produces studentized residuals\nhttps://online.stat.psu.edu/stat462/node/247/","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"influence = reg.get_influence()\nresid_student = influence.resid_studentized\n(cooks, p) = influence.cooks_distance\n(dffits, p) = influence.dffits\nleverage = influence.hat_matrix_diag\n\nprint ('\\n')\nprint ('Leverage v.s. Studentized Residuals')\nsns.regplot(leverage, reg.resid_pearson,  fit_reg=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(resid_student.size)\nres = pd.concat([pd.Series(cooks, name = \"cooks\"), pd.Series(dffits, name = \"dffits\"), pd.Series(leverage, name = \"leverage\"), pd.Series(resid_student, name = \"resid_student\")], axis = 1)\nres_orig = res.copy()\nres_copy = res.copy()\nX_copy = X.copy()\ny_copy = y.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = pd.Series(reg.params)\nprint(params.sort_values().head(12))\nprint(params.sort_values(ascending=False).head(12))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r_sort = res.sort_values(by = 'resid_student')\nprint ('-'*30 + ' top 5 most negative residuals ' + '-'*30)\nprint (r_sort.head())\nprint ('\\n')\n\nprint ('-'*30 + ' top 5 most positive residuals ' + '-'*30)\nprint (r_sort.tail())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Geographical analysis of the residuals","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(res.shape, df_train.shape)\ndf_res = pd.concat([res, df_train], axis=1)\ndf_res.resid_student = np.abs(df_res.resid_student)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_res = df_res.groupby(['plz']).mean().reset_index(); \ngermany_dfx = pd.merge(\n    left=z, \n    right=avg_res, \n    on='plz',\n    how='left'\n)\n\ngermany_dfx = germany_dfx.sort_values(by='resid_student', ascending=False)\nplot_country(germany_dfx, 'resid_student')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Top irregular observations, mostly in rural areas or huge postal codes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(germany_dfx.columns)\ngermany_dfx = germany_dfx.drop_duplicates(subset = [\"plz\"])\ngermany_dfx[['ort', 'plz', 'resid_student', 'bundesland']].head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Outlayer removal based on the resid student value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"limit = 2 #resid student\n\n#X = X[features2]\n#y = X['log_price']\n\nfor name in features2:\n    if X[name].isna().values.any():\n        print(\"there is na\", name)\n\ny1 = y[np.asarray(res['resid_student']) < limit]\nX1 = X[np.asarray(res['resid_student']) < limit]\n\nres1 = res[res['resid_student'] < limit]\n\ny1 = y1[np.asarray(res1['resid_student']) > -limit]\nX1 = X1[np.asarray(res1['resid_student']) > -limit]\n\nres1 = res1[res1['resid_student'] > -limit]\n\n\n\nlimit_lev = 0.05\n\ny1 = y1[np.asarray(res1['leverage']) < limit_lev ]\nX1 = X1[np.asarray(res1['leverage']) < limit_lev ]\n\n\ny1 = y1[X1['new_apartment_permit_6_hab_log'] > -5]\nX1 = X1[X1['new_apartment_permit_6_hab_log'] > -5]\n\n\nX1 = X1[y1.values > 12]\ny1 = y1[y1.values > 12]\n\nres1 = res1[res1['leverage'] < limit_lev ]\n\nsns.regplot(res1['leverage'], res1['resid_student'],  fit_reg=False)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Selection\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Add variable wich reduces the MSE on each step, select max 15 variables, using 5 fold cross validation\nThis section of code is ML course in RWTH","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_validate, KFold\nfrom itertools import combinations\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LinearRegression\n# Data preparation, as we have done before.\nlabel_column = 'log_price'\n\n\n\n# We use the linear model for our example.\nkfold = KFold(n_splits = 5, shuffle = True, random_state = 0)\nmodel = make_pipeline(\n    StandardScaler(),\n    LinearRegression())\n\n# Gets the MSE for a model with zero features, just predicting the mean.\ndef get_base_mse(y):\n    predictions = [np.mean(y)] * len(y)\n    return mean_squared_error(predictions, y)\n\n# Gets the MSE of our model, for a given dataset, estimated with 5-fold cross-validation.\ndef k_fold_mse(X, y):\n    scores = cross_validate(model, X, y, scoring = 'neg_mean_squared_error', cv = kfold)\n    result = np.mean(scores['test_score']) * -1\n    return result\n\ndef forward_stepwise_selection(X, y, d):\n    # Start with the MSE of the zero-features model.\n    feature_columns = d.columns\n    print(feature_columns)\n    current_mse = get_base_mse(y)\n    \n    # This array contains the indices of the columns (features)\n    # currently giving the best ecountered model. At the beginning,\n    # this is an empty numpy array.\n    current_features = np.array([], dtype = int)\n    \n    # This array contains the indices of all the columns (features)\n    # in our dataset. In other words, it is a numpy array containing\n    # [0, 1, ..., p-1] where p is the number of features.\n    all_features = np.arange(len(feature_columns))\n    \n    # In the extreme case, when adding a feature always improves the\n    # model, we terminate when we added all features.\n    # In that case, current_features == all_features.\n    while not len(current_features) > 15: #np.array_equal(current_features, all_features):\n        # This variable will contain the index of the *new* feature\n        # we want to add to the model. If no improving feature is found,\n        # then this variable will keep value None.\n        selected_feature = None\n        \n        # For features not yet in the model...\n        for feature in (set(all_features) - set(current_features)):\n            # Build a new set of features, adding the new one to the\n            # ones already in the model.\n            new_features = np.append(current_features, feature)\n            \n            # Estimate the mse of the new model.\n            mse = k_fold_mse(X[:,new_features], y)\n            \n            # If it's better than the current best, update the best\n            # current MSE and mark this feature as the selected new\n            # feature.\n            if mse < current_mse:\n                current_mse = mse\n                selected_feature = feature\n                \n                \n                \n        # If we found an improving feature...\n        if selected_feature is not None:\n            #... add it to the current features.\n            current_features = np.append(current_features, selected_feature)\n\n        else:\n            # Otherwise, terminate.\n            break\n            \n            print(\"current_features\")\n            for idx in sorted(current_features):\n                print(f\"\\t{d.columns[idx]}\")\n    \n    return current_features, current_mse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_idx, mse = forward_stepwise_selection(X1[features2].values, y1.values, X1[features2])\nprint(f\"MSE of the selected model: {mse:.3f}\")\nprint(features_idx)\nprint(X1.columns[features_idx])\nselected_features = X1.columns[features_idx]\n\nX1[selected_features] = scaler.fit_transform(X1[selected_features])\n\nreg = sm.OLS(y1, sm.add_constant(X1[selected_features])).fit()\nprint(\"there is nan\", y1.isna().values.any())\nprint(\"there is null\", y1.isnull().values.any())\n\nresults = reg\nreg.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After removing the outlayes, we make sure the residuals have a normal distribution","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Jarque–Bera\nIn statistics, the Jarque–Bera test is a goodness-of-fit test of whether sample data have the skewness and kurtosis matching a normal distribution. The test is named after Carlos Jarque and Anil K. Bera. The test statistic is always nonnegative. If it is far from zero, it signals the data do not have a normal distribution.\nhttps://en.wikipedia.org/wiki/Jarque%E2%80%93Bera_test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.compat import lzip\nname = ['Jarque-Bera', 'Chi^2 two-tail prob.', 'Skew', 'Kurtosis']\ntest = sms.jarque_bera(reg.resid)\nlzip(name, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sns.kdeplot(np.array(reg.resid), bw=10)\nsns.distplot(np.array(reg.resid), hist=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.stats as scipystats\nimport pylab\nscipystats.probplot(reg.resid, dist=\"norm\", plot=pylab)\npylab.show()\n# https://songhuiming.github.io/pages/2016/12/31/linear-regression-in-python-chapter-2/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfeatures_idx, mse = forward_stepwise_selection(X1.values, y1.values, X1)\nprint(f\"MSE of the selected model: {mse:.3f}\")\n\nprint(features_idx)\nprint(X1.columns[features_idx])\nselected_features = X.columns[features_idx]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = sm.OLS(y1, sm.add_constant(X1[selected_features])).fit()\nresults = reg\nreg.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = pd.Series(reg.params)\nprint(params.sort_values().head(15))\nprint(params.sort_values(ascending=False).head(15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X1[selected_features].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Multicollinearity","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Multicollinearity is a measure of the relation between so-called independent variables within a regression. This phenomenon occurs when two or more predictor variables in a regression analysis are strongly associated or correlated with one another. When this occurs, one predictor variable can be used to predict the variable that it experiences multicollinearity with.\nhttps://medium.com/@mackenziemitchell6/multicollinearity-6efc5902702","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Condition number test\n\nOne way to assess multicollinearity is to compute the condition number. Values over 20 are worrisome (see Greene 4.9). The first step is to normalize the independent variables to have unit length:**\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"np.linalg.cond(reg.model.exog) # Condition number","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The largest condition index is called the condition number. A condition number between 10 and 30 indicates the presence of multicollinearity and when a value is larger than 30, the multicollinearity is regarded as strong.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#sns.pairplot( pd.concat([y1, X1[selected_features]], axis=1), kind=\"reg\", corner=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corrMatrix = pd.concat([y1, X1[selected_features]], axis=1).corr()\nplt.figure(figsize=(15,10))\n#print(corrMatrix.sort_values('log_price', ascending=True)['log_price'].head(30))\n#print(corrMatrix.sort_values('log_price', ascending=False)['log_price'].head(30))\nsns.heatmap(corrMatrix, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The Variance Inflation Factor (VIF)\n\nThe Variance Inflation Factor (VIF) is a measure of colinearity among predictor variables within a multiple regression. It is calculated by taking the the ratio of the variance of all a given model's betas divide by the variane of a single beta if it were fit alone.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nselected_features = list(selected_features)\n#selected_features.remove('tax_payers_hab_log')\nX_train = np.asarray(X1[selected_features])\nvif = pd.DataFrame()\nvif[\"features\"] = X1[selected_features].columns\nvif[\"VIF Factor\"] = [variance_inflation_factor(X_train, i) for i in range(X_train.shape[1])]\n\n#first use variables realted to pop\nvif.sort_values([\"VIF Factor\"], ascending=False).head(30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Homoscedasticity\n### Breusch Pagan Test for homoscedasticity\n\nIt is one of the most common tests for heteroskedasticity. It begins by allowing the heteroskedasticity process to be a function of one or more of your independent variables, and it’s usually applied by assuming that heteroskedasticity may be a linear function of all the independent variables in the model Now, for BP test, the null assumes homoskedasticity. So if p_val < 0.05 level of significance; you reject the null and infer the presence of heteroskedasticity and if p_val > level of significance ; you fail to reject the null and conclude there may not be heteroskedasticity.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"name = ['Lagrange multiplier statistic', 'p-value',\n        'f-value', 'f p-value']\ntest = sms.het_breuschpagan(reg.resid, reg.model.exog)\nlzip(name, test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot of the predicted squared residuals\n\nNo aparent trend, wich is good","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"resid = reg.resid\nplt.scatter(reg.predict(), resid ** 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot of residuals in ascending order of price \n\nfrom statsmodels.sandbox.regression.predstd import wls_prediction_std\nnsample = 50\nsig = 0.5\nx = np.linspace(0, 20, X1.shape[0])\nbeta = [0.5, 0.5, -0.02, 5.]\n\ny_true = y1.copy()\n\nind = np.argsort(y_true, axis=0)\n\nprstd, iv_l, iv_u = wls_prediction_std(reg)\n\n\nfig, ax = plt.subplots(figsize=(8,6))\n\n#ax.plot(x, y, 'o', label=\"data\")\nax.plot(x, reg.resid.iloc[ind], 'r.', label=\"Residuals\")\nax.set_ylim([-3, 3])\n#ax.plot(x, y_true.iloc[ind], 'b-', label=\"True\")\nax = sns.regplot(x=x, y=reg.resid.iloc[ind], scatter=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## If there was not log-transform of the features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_features_no_log = [w.replace('_log', '') for w in selected_features]\nselected_features_no_log\n\n#df_train.dropna(inplace=True, subset='log_price')\nX_no_log = df_train[selected_features_no_log].copy()\ny_log = df_train['log_price']\nX_no_log = X_no_log[y_log.notna()]\ny_log = y_log[y_log.notna()]\nprint(X_no_log[selected_features_no_log].shape)\nimp_mean = SimpleImputer(strategy='mean')\nimp_mean.fit(X_no_log[selected_features_no_log])\nX_no_log[selected_features_no_log] = imp_mean.transform(X_no_log[selected_features_no_log])\nX_no_log[selected_features_no_log] = scaler.fit_transform(X_no_log[selected_features_no_log])\nX_no_log.head()\nreg_no_log = sm.OLS(y_log, sm.add_constant(X_no_log[selected_features_no_log])).fit()\n\n\n\nnsample = 50\nsig = 0.5\nx = np.linspace(0, 20, X_no_log.shape[0])\nbeta = [0.5, 0.5, -0.02, 5.]\n\ny_true = y_log.copy()\nind = np.argsort(y_true, axis=0)\nprint(ind.shape)\nprint(y_true.shape)\nprint(X_no_log.shape)\nprint(x.shape)\nreg_no_log.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sns.pairplot( pd.concat([y1, X1[selected_features]], axis=1), kind=\"reg\", corner=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfig, ax = plt.subplots(figsize=(8,6))\nax.plot(x, reg_no_log.resid.iloc[ind], 'r.', label=\"Residuals\")\nax.set_ylim([-3, 3])\nax = sns.regplot(x=x, y=reg_no_log.resid.iloc[ind], scatter=False)\n# plot residuals in a model where log transformation is not applied","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Predicted value vs residual, in a model where no log transformation of the values is appliend, on smaller price values there is a high variation of the resitual and on higher values a small variation, wich is not good","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.scatter(reg_no_log.predict(), reg_no_log.resid ** 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tests on Nonlinearity\n\nLinearity - the relationships between the predictors and the outcome variable should be linear Homogeneity of variance (homoscedasticity) - the error variance should be constant\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"names = selected_features\nfrom pylab import *\ni = 1\nj = 1\nfig = plt.figure(figsize=(30, 20))\n\nX_plot = X1.copy()\ny_plot = y1.copy()\nprint(X_plot.shape)\nprint(y_plot.shape)\nfor name in names[:]:\n\n    reg1 = sm.OLS(y_plot, sm.add_constant(X_plot[name])).fit()\n    ax1 = fig.add_subplot( 4 ,8,i)\n    ax1.scatter(X_plot[name], y_plot)\n    ax1.plot(X_plot[name], reg1.params[0] + reg1.params[1] * X_plot[name], '-', color='r')\n    #ax1.ylabel('Log of Price')\n    #ax1.xlabel(name)\n    ax1.set_xlabel(name)\n    ax1.set_ylabel('Log of Price')\n    ax1.set_xlim([-7, 7])\n    ax1.set_ylim([10, 15])\n    i += 1\n    \nplt.show()\n#results = reg\n#reg.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation the train data set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.sandbox.regression.predstd import wls_prediction_std\nnsample = 50\nsig = 0.5\nx = np.linspace(0, 20, X1.shape[0])\nbeta = [0.5, 0.5, -0.02, 5.]\n\ny_true = y1.copy()\n\nind = np.argsort(y_true, axis=0)\n\nprstd, iv_l, iv_u = wls_prediction_std(reg)\n\n\nfig, ax = plt.subplots(figsize=(8,6))\n\n#ax.plot(x, y, 'o', label=\"data\")\nax.plot(x, reg.predict(sm.add_constant(X1[selected_features].iloc[ind])), 'r--.', label=\"OLS\")\nax.plot(x, y_true.iloc[ind], 'b-', label=\"True\")\n\n#ax.plot(x, iv_u, 'r--')\n#ax.plot(x, iv_l, 'r--')\nax.legend(loc='best');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test Data Set \n\nHow does the model behave on unseen data?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.sandbox.regression.predstd import wls_prediction_std\nnsample = 50\nsig = 0.5\n\n\nX2 = df_test[selected_features]\n\nimp_mean = SimpleImputer(strategy='mean')\nimp_mean.fit(X2[selected_features])\nX2[selected_features] = imp_mean.transform(X2[selected_features])\n\nX2[selected_features] = scaler.fit_transform(X2[selected_features])\n\ny_true = df_test['log_price'].copy().astype(float)\n\nX2 = X2[y_true.notna()]\ny_true = y_true[y_true.notna()]\n\nX2 = X2[y_true.values > 12]\ny_true = y_true[y_true.values > 12]\ny_pred = reg.predict(sm.add_constant(X2[selected_features]))\nx = np.linspace(0, 20, X2.shape[0])\nbeta = [0.5, 0.5, -0.02, 5.]\n\nind = np.argsort(y_true, axis=0)\nprstd, iv_l, iv_u = wls_prediction_std(reg)\nfig, ax = plt.subplots(figsize=(8,6))\n#ax.plot(x, y, 'o', label=\"data\")\nax.plot(x, y_pred.iloc[ind], 'r--.', label=\"OLS\")\nax.plot(x, y_true.iloc[ind], 'b-', label=\"True\")\n\n#ax.plot(x, iv_u, 'r--')\n#ax.plot(x, iv_l, 'r--')\nax.legend(loc='best');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tools import eval_measures\nprint(\"MSE\", eval_measures.mse(y_true, y_pred))\nprint(\"RMSE\", eval_measures.rmse(y_true, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Retrain the model using the whole data set\n\n(not including outlayers previously detected on train set but everything in test set)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train[selected_features].shape)\nprint(res.shape)\nmask1 = res.copy()\nmask1['resid_student'] = np.abs(mask1['resid_student'])\nprint(mask1.shape)\ndf_train2 = df_train[ np.asarray(mask1['resid_student'] < limit)]\nmask1 = mask1[mask1['resid_student'] < limit]\ndf_train2 = df_train2[ np.asarray(mask1['leverage'] < limit_lev )]\nprint(\"remove resid student outlatyer\", df_train2.shape)\nprint(\"df_text_shape\", df_test.shape)\ndf_test.head()\ndf_both = pd.concat([df_train, df_test])\nprint(\"df_both\", df_both.shape)\ndf_both = df_both[df_both['log_price'].notna()]\nprint(\"df_both\", df_both.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp_mean = SimpleImputer(strategy='mean')\nX_final = df_both[selected_features].copy()\n\nimp_mean.fit(X_final[selected_features])\n\nX_final[selected_features] = imp_mean.transform(X_final[selected_features])\nX_final[selected_features] = imp_mean.transform(X_final[selected_features])\n\nscaler = StandardScaler()\n\nX_final[selected_features] = scaler.fit_transform(X_final[selected_features])\nprint(X_final.shape)\nX_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg_final = sm.OLS(df_both['log_price'], sm.add_constant(X_final[selected_features[:]])).fit()\nreg_final.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot( pd.concat([df_both['log_price'], X_final[selected_features[11:16]]], axis=1), kind=\"reg\", corner=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nresults = pd.DataFrame(reg_final.summary().tables[1])\nresults = results.rename(columns=results.iloc[0])\nresults = results.drop(results.index[0])\nresults.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(germany_df3.shape)\ngermany_df3 = germany_df3.replace([np.inf, -np.inf], np.nan)\n\nfor col in germany_df3[selected_features].columns:\n    print(col, ' ', germany_df3[col].isna().values.sum())\n    print(col, ' ', germany_df3[col].notnull().values.sum())\n    print(col, 'fini ', np.isfinite(germany_df3[col]).all())\n    \n\ngermany_dfinal = germany_df3[selected_features].copy()\nprint(germany_dfinal.shape)\n\ngermany_dfinal = germany_dfinal.dropna(subset=[*selected_features], how=\"all\")\ngermany_dfinal = germany_dfinal[np.isfinite(germany_dfinal)]\nprint(germany_dfinal.shape)\nscaler = StandardScaler()\n\ngermany_dfinal[selected_features] = scaler.fit_transform(germany_dfinal[selected_features])\n\n    \nprediction = reg_final.predict(sm.add_constant((germany_dfinal[selected_features])))\n\nprint(prediction.shape)\n\ngermany_df3['pred'] = prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"germany_dfinal[selected_features].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_country(germany_df3, 'pred')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_city(germany_df3, \"Aachen\", 'pred')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}