{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Df=pd.read_csv(\"../input/league-of-legends-diamond-ranked-games-10-min/high_diamond_ranked_10min.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Info \n\n- Shape : (9879,40)\n- No missing values\n- Types : All columns are numerical features (36:Float64, 4:int64)\n- Target : **blueWins**\n\n## Comments \n\n- Blue team and red team have many symetric result\n- EliteMonster = Drake + Herald","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1-Visualisation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Df.shape)\nDf.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Df.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.heatmap(Df.isna(),cbar=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This dataset hasn't any missing values ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Df = Df.copy()\nDf.drop(columns=[\"gameId\"],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Df_Discret = Df.select_dtypes('int64')\nDf_Continious = Df.select_dtypes('float64')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Float64 Features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in Df_Continious.columns:\n    plt.figure()\n    sns.distplot(Df[col])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We notice that Blue Average level and Red Average level are not really continious because in this game level are integer from 0 to 18 and each player in the game have roughly the same level","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(Df_Continious)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lots of features look to be Linearly correlated, the reason is that majority of those feature tend to increase during the game so they will simultaneously increase","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Int64 features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nfor col in Df_Discret.columns:\n    plt.figure()\n    plt.title(\"{}\".format(col))\n    Df[col].plot.hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Comments :**\n\n- Ratio Win/lose more or less equal to 1\n- In general, during the 10 first minutes, a team(red or blue) put less than 50 Wards. \n- Gold and Experience Diff are centred variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in Df.columns:\n    print(f'{col :-<50} {\"Average :\" ,Df[col].mean()} {\"Std :\",Df[col].std()}')\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we will compare the different feature of the first 10 minutes of game between red team and blue team when the blue team win","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Df_win = Df[Df[\"blueWins\"]==1]\nprint(Df_win.shape)\nAvg = Df_win.groupby([\"blueWins\"]).mean()\nAvg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### All those pie graph suppose that the blue team won the game","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Values = [Avg[\"blueFirstBlood\"].values,Avg[\"redFirstBlood\"].values]\nindexes = ['blue','red']\nSérie = pd.DataFrame(Values,index = indexes,columns = ['FirstBlood'])\nprint(Série)\nSérie.plot.pie(y='FirstBlood',fontsize=18,figsize=(9,9),title='First Blood',autopct='%1.1f%%',cmap=\"coolwarm\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Comment:** When the blue team get the first blood, they will win the game in 60.6% of the cases","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### Herald Vs Dragon (in case blue team win)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Values_drake = [Avg[\"blueDragons\"].values,1-Avg[\"redDragons\"].values-Avg[\"blueDragons\"].values,Avg[\"redDragons\"].values]\nindexes_drake = ['blue','No drake','red']\nSerie_drake = pd.DataFrame(Values_drake,index = indexes_drake,columns = ['Nb'])\nprint(Serie_drake)\n\nValues_her = [Avg[\"blueHeralds\"].values,1-Avg[\"redHeralds\"].values-Avg[\"blueHeralds\"].values,Avg[\"redHeralds\"].values]\nindexes_her = ['blue','No Herald','red']\nSerie_her = pd.DataFrame(Values_her,index = indexes_her,columns = ['Nb'])\nprint(Serie_her)\n\nplt.figure()\nSerie_drake.plot.pie(y='Nb',fontsize=18,figsize=(9,9),title='Drake',autopct='%1.1f%%',cmap=\"coolwarm\")\nSerie_her.plot.pie(y='Nb',fontsize=18,figsize=(9,9),title='Herald',autopct='%1.1f%%',cmap=\"coolwarm\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The drake is an important objectif at the beginning of the game and grant an not negligeable advantage ! The Herald is not very contested in the ten first minutes. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Avg_EliteMonster = Df.groupby([\"blueEliteMonsters\"]).mean()\nAvg_EliteMonster\nsns.barplot(x= Avg_EliteMonster.index , y=Avg_EliteMonster[\"blueWins\"],palette=\"rocket\")\nprint(Avg_EliteMonster[\"blueWins\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"EliteMonsters are very important target in the game , as you can see, if in the first 10 minutes the blue team killed Herald and Drake, they will win in 73.5% of the time","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Values = [Avg[\"blueWardsPlaced\"].values,Avg[\"redWardsPlaced\"].values]\nindexes = ['blue','red']\nSérie = pd.DataFrame(Values,index = indexes,columns = ['Wards'])\nprint(Série)\nSérie.plot.pie(y='Wards',fontsize=18,figsize=(9,9),title='Wards Placed',autopct='%1.1f%%',cmap=\"coolwarm\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will observe the distribution of the different features depending on if the blue team win the game or lose the game","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Df_lose = Df[Df[\"blueWins\"]==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in Df_Continious.columns:\n    plt.figure()\n    sns.distplot(Df_win[col], label='Blue team win',color='blue')\n    sns.distplot(Df_lose[col], label='Red team win',color='red')\n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Some features in int64 has also interesting distributions\ncolumns = [\"blueTotalGold\",\"blueTotalExperience\",\"blueTotalMinionsKilled\",\"blueGoldDiff\",\"blueExperienceDiff\"]\nfor col in columns:\n    plt.figure()\n    sns.distplot(Df_win[col], label='Blue team win',color='blue')\n    sns.distplot(Df_lose[col], label='Red team win',color='red')\n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will observe the influence of the features with a finite number of values (I filtered the discret feature because some of them take a wide range of values)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Binary_col = []\nfor col in Df.select_dtypes(\"int64\").drop(columns =[\"blueWins\"]):\n    if len(Df[col].unique()) <5:\n        Binary_col.append(col)\nBinary_col","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in Binary_col:\n    plt.figure()\n    sns.heatmap(pd.crosstab(Df[\"blueWins\"],Df[col]),annot=True,fmt='d')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,12))\ncorr = Df.corr()\nmask = np.triu(np.ones(corr.shape)).astype(np.bool)\nsns.heatmap(corr,annot=False,mask = mask,cmap = \"coolwarm\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsns.clustermap(Df.corr(),annot=False,cmap=\"coolwarm\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Df.corr()['blueWins'].sort_values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The difference of Gold between the two team could be a very interesting feature to predict if the blue team will win or not. \nThis can be explained in the game by the fact that, the richer a team is, the stronger she is. Same principle with Experience\n\nThe rest of the features are some target in the game which give to the team gold and experience, so they are linked more or less to the gold diff and the experience diff. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nX1 = Df[\"blueGoldDiff\"][Df[\"blueWins\"]==1]\nY1 = Df[\"blueWins\"][Df[\"blueWins\"]==1]\nX2 = Df[\"blueGoldDiff\"][Df[\"blueWins\"]==0]\nY2 = Df[\"blueWins\"][Df[\"blueWins\"]==0]\nplt.scatter(X1,Y1,c=\"blue\",label=\"Blue Team Win\")\nplt.scatter(X2,Y2,c=\"red\",label=\"Red Team Win\")\nplt.grid()\nplt.title(\"Gold difference\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can imagine to use LogisticRegression but it's probably not enough in order to make good predictions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2 Preprocessing\n- Separate the datasets into a train and a test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = Df.drop(columns=[\"blueWins\"])\ny = Df[\"blueWins\"]\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)\n\nprint(\"X_train shape :\",X_train.shape)\nprint(\"X_test shape :\",X_test.shape)\nprint(\"y_train shape :\",y_train.shape)\nprint(\"y_test shape :\",y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_train.value_counts())\nprint(y_test.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.svm import SVC #Good on small datasets\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.feature_selection import SelectKBest,f_classif\nfrom sklearn.preprocessing import PolynomialFeatures","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model0 = DecisionTreeClassifier(random_state=0)\nmodel1 = RandomForestClassifier(random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation Process","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import learning_curve\n\n\ndef evaluation(model):\n    \n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    \n    print(confusion_matrix(y_test,y_pred))\n    print(classification_report(y_test,y_pred))\n    \n    N, train_score, val_score = learning_curve(model, X_train,y_train,cv=4,scoring='f1',\n                                              train_sizes=np.linspace(0.1,1,10))\n    \n    plt.figure(figsize=(12,8))\n    plt.plot(N,train_score.mean(axis=1),label='train_score')\n    plt.plot(N,val_score.mean(axis=1),label='val_score')\n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For each model that we will assess, we print the confusion matrix, f1 score, recall and precision\n\nOur metrics target will be the accuracy of the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation(model0)\nevaluation(model1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It look likes our two simple models are overfitting the training set. Time to use Feature Selection because many of our features are redundant","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we will train different models and see which one is the most interesting to improve","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessor = make_pipeline(PolynomialFeatures(2,include_bias=False),SelectKBest(f_classif,k=10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RandomForest = make_pipeline(preprocessor,RandomForestClassifier(random_state=0))\nAdaBoost = make_pipeline(preprocessor, AdaBoostClassifier(random_state=0))\nSVM = make_pipeline(preprocessor,StandardScaler(),SVC(random_state=0))\nKNN = make_pipeline(preprocessor,StandardScaler(),KNeighborsClassifier())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_of_models = {\"RandomForest\":RandomForest,\"AdaBoost\":AdaBoost,\"SVM\":SVM,\"KNN\":KNN}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nfor key,model in dict_of_models.items():\n    print(key)\n    evaluation(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SVM and AdaBoost are doing well, let's try to imporve SVM ! (maybe Data Augmentation can be interesting for the SVM because we see that his performance keep increasing with Training_set size","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SVM","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\"svc__gamma\":[1e-3,1e-4],\n              \"svc__C\":[1,10,100,1000],\"svc__degree\":[2,3,4,5]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid = RandomizedSearchCV(SVM,params,scoring=\"accuracy\",cv=4, n_iter=10)\n\ngrid.fit(X_train, y_train)\n#Very Long , use RandomizedSearchCV\n\nprint(grid.best_params_)\n\ny_pred = grid.predict(X_test)\n\nprint(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We try \"n_iter\" combinations of hyperparameter and get a combination with the best performance for our model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Precision recall curve\n\nCheck if we can change the decision boundarie to balance recall and precision performance. Spoiler: I'll not use it","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"precision,recall,threshold = precision_recall_curve(y_test,grid.best_estimator_.decision_function(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(threshold,precision[:-1],label ='precision')\nplt.plot(threshold,recall[:-1],label ='recall')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_final(model,X,threshold=0):\n    return model.decision_function(X) > threshold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model_final(grid.best_estimator_,X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_final = grid.best_estimator_\nprint(\"f1_score = \" ,f1_score(y_test,y_pred))\nprint(model_final.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally we have a prediction of 71%","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Visualisation/Dimensionality reduction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Dimensionality reduction can be very long","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2, random_state=42)\nX_reduced = tsne.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize=(13,10))\nplt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, cmap=\"coolwarm\")\nplt.axis('off')\nplt.colorbar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dimension reduction technique to visualize our data in 2D","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}