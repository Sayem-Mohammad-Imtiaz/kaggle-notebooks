{"cells":[{"metadata":{"_uuid":"fa00280e5d0d896b34fc5d07f7a16ef98f53d055"},"cell_type":"markdown","source":"**Happiness score prediction**\n\nThis notebook is about exploring and comparing Decision Tree and Random Forest models in a task: predict the happiness score for some countries.\n\nFirstly, we should explore one of datasets to understand the use of columns and its' values types."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\ndata = pd.read_csv('../input/2015.csv')\nprint(data.columns)\ndata[:10]","execution_count":309,"outputs":[]},{"metadata":{"_uuid":"70a4ab8f366c27becc1663e1759295910384bbb0"},"cell_type":"markdown","source":"This dataset has no any missing values, so we can select predictors just having some knowledges about key factors. The folowing predictors can do a big influence on the target variable - Happiness Score."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":true},"cell_type":"code","source":"y = data['Happiness Score'] # target variable\nhappiness_score_predictors = ['Country','Economy (GDP per Capita)', 'Family', 'Health (Life Expectancy)', 'Freedom', 'Trust (Government Corruption)',\n       'Generosity', 'Dystopia Residual']\nX = data[happiness_score_predictors] # predictors","execution_count":310,"outputs":[]},{"metadata":{"_uuid":"e3959554b71ba73e04b743176fba20a56808c15b"},"cell_type":"markdown","source":"In the following steps we evently split the entire data in two groups - train and validation subsets. A validation subset called that because here we just compare the predictive models.\n\nThe first model is Decision Tree. We need to create an object of function *DecisionTreeRegressor()*, fit it with train data, make prediction with validation data and calculate the difference between expected and actual values."},{"metadata":{"_cell_guid":"f94f1463-42da-44a1-a8d5-3bc510c6057c","_uuid":"55afcb7fac4a7bc432f83887260450690aeb3b13","trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\nmodel_decision_tree = DecisionTreeRegressor()\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\nmodel_decision_tree.fit(train_X.drop(['Country'], axis=1), train_y)\nprediction_tree = model_decision_tree.predict(val_X.drop(['Country'], axis=1))\n#val_y - prediction # to see the actual difference between expected and calculated values\nerror_tree = mean_absolute_error(val_y, prediction_tree)\nprint(error_tree)","execution_count":311,"outputs":[]},{"metadata":{"_uuid":"b54ac9939ef2439dd6955a789810a869d47279ac"},"cell_type":"markdown","source":"The second model is Random Forest. We proceed it with the same steps as for previous model."},{"metadata":{"_cell_guid":"66a841ca-a886-42e8-8e50-6fd6f63f054f","_uuid":"c10042254388a78e8738e2f77faa8606a61be55d","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nmodel_random_forest = RandomForestRegressor()\nmodel_random_forest.fit(train_X.drop(['Country'], axis=1), train_y)\nprediction_forest = model_random_forest.predict(val_X.drop(['Country'], axis=1))\nerror_forest = mean_absolute_error(val_y, prediction_forest)\nprint(error_forest)","execution_count":312,"outputs":[]},{"metadata":{"_cell_guid":"43fee41c-4782-4a8c-a4f0-683909926a49","_uuid":"00101bde5fe534e5f53740191455a8f1a42609ca"},"cell_type":"raw","source":"Now let's visualize this comparison using standard tools of Matplotlib. By the way, 'Country' variable deliberately was left in X subset to collerate Happiness Score with country name."},{"metadata":{"trusted":true,"_uuid":"467695198bcea157308dea707fe9b85699dd476e"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndt = data[:40]['Country']\nsorted_val_y = val_y.sort_values(ascending=False)\n\nplt.plot(np.sort(prediction_tree), marker='o', label='Decision Tree model')\nplt.plot(np.sort(prediction_forest), marker='o', label='Random Forest model')\nplt.plot(np.sort(val_y.values), marker='o', label='Actual')\nplt.legend()\nplt.xticks(range(len(val_y.values)), val_X['Country'], rotation = 60)\n\nfig_size = plt.rcParams[\"figure.figsize\"]\nfig_size[0] = 30\nfig_size[1] = 5\nplt.rcParams[\"figure.figsize\"] = fig_size\nplt.show()","execution_count":319,"outputs":[]},{"metadata":{"_uuid":"f9131d662ed2b88f25aa555650c7f2d3a4ee1375"},"cell_type":"markdown","source":"In this kernel there were used two of the most simple predictive models. Even thought this dataset is not a very good choice for predictive analysis, created models can be used with some new subsets of countries data. For example, when GDP or Generosity lavel changes the target value can also changes. Further we will use two other datasets of 2016 and 2017 to create something else interesting. But as for now, we can change the output of the plot by varying the *random_state* parameter of *train_test_split* function."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}