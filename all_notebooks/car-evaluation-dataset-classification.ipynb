{"cells":[{"metadata":{},"cell_type":"markdown","source":"### **In this notebook I will try to do an exploratory data analysis and classification for the Car Evaluation dataset, found in the UCI machine learning repository.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline \nimport seaborn as sns\n\n\ndata = pd.read_csv('../input/car-evaluation-data-set/car_evaluation.csv', header = None)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_names = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']\n\n\ndata.columns = col_names\n\ncol_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in data.columns:\n    print(data[col].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop(['class'], axis=1)\ny = data['class']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_test.value_counts()) #we secured the 20% in the classes represenation","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use an ordinal encoder since there is ordinality in our data"},{"metadata":{"trusted":true},"cell_type":"code","source":"mapping = [{'col':'buying', 'mapping':{'low':0, 'med':1, 'high':2, 'vhigh':3}},\n          {'col':'maint', 'mapping':{'low':0, 'med':1, 'high':2, 'vhigh':3}},\n          {'col':'doors', 'mapping':{'2':0, '3':1, '4':2, '5more':3}},\n          {'col':'persons', 'mapping':{'2':0, '4':1, 'more':2}},\n          {'col':'lug_boot', 'mapping':{'small':0, 'med':1, 'big':2}},\n          {'col':'safety', 'mapping':{'low':0, 'med':1, 'high':2}}]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import category_encoders as ce\n\nencoder = ce.OrdinalEncoder(cols=['byuing', 'maint', 'doors', 'persons', 'lug_boot', 'safety'], mapping = mapping)\n\nX_train = encoder.fit_transform(X_train)\nX_test = encoder.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will produce synthetic data to balance the classes"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\nsm = SMOTE(random_state = 42)\n           \nX_train, y_train = sm.fit_sample(X_train, y_train) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_train.value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree: GINI criterion"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import log_loss\nfrom sklearn.tree import DecisionTreeClassifier\n\ntraining_loss = []\ntest_loss = []\n\ndef tree_scores(i):\n    clf_gini = DecisionTreeClassifier(criterion='gini', max_depth=i, random_state = 42)\n    \n    \n    clf_gini.fit(X_train, y_train)\n    y_pred_gini = clf_gini.predict_proba(X_test)\n    y_pred_train_gini = clf_gini.predict_proba(X_train)\n    \n     \n\n    training_loss.append(log_loss(y_train, y_pred_train_gini))\n\n    test_loss.append(log_loss(y_test, y_pred_gini))\n     \n        \nfor i in range(1,11):\n    tree_scores(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = [1,2,3,4,5,6,7,8,9,10]\n\n# plotting the line 1 points \nplt.plot(y, training_loss, label = \"training loss \")\n\n# plotting the line 2 points \nplt.plot(y, test_loss, label = \"test loss \")\nplt.xlabel('Max Depth')\n# Set the y axis label of the current axis.\nplt.ylabel('Log-Loss')\n# Set a title of the current axes.\nplt.title('Log-Loss plot ')\n# show a legend on the plot\nplt.legend()\n# Display a figure.\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nclf_gini = DecisionTreeClassifier(criterion='gini', max_depth=7, random_state = 42)\n\nprint('Cross-Validation Score:',np.mean(cross_val_score(clf_gini, X_train, y_train, cv=10)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclf_gini = DecisionTreeClassifier(criterion='gini', max_depth=7, random_state = 42)\n\nclf_gini.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_gini = clf_gini.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training set score: {:.4f}'.format(clf_gini.score(X_train, y_train)))\n\nprint('Test set score: {:.4f}'.format(clf_gini.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport graphviz\nfrom sklearn import tree\n\n\n\n\ndot_data = tree.export_graphviz(clf_gini, out_file=None, \n                              feature_names=X_train.columns,  \n                              class_names=y_train,  \n                              filled=True, rounded=True,  \n                              special_characters=True)\n\ngraph = graphviz.Source(dot_data) \n\ngraph ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\n\ncf_matrix = confusion_matrix(y_test, y_pred_gini)\n\ncf_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nax= plt.subplot()\nsns.heatmap(cf_matrix, annot=True, ax = ax)  #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix')\nax.xaxis.set_ticklabels(y_test.unique())\nax.yaxis.set_ticklabels(y_test.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred_gini))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree: ENTROPY criterion"},{"metadata":{"trusted":true},"cell_type":"code","source":"training_loss = []\ntest_loss = []\n\ndef tree_scores(i):\n    clf_en = DecisionTreeClassifier(criterion='entropy', max_depth=i, random_state = 42)\n    \n    \n    clf_en.fit(X_train, y_train)\n    y_pred_en = clf_en.predict_proba(X_test)\n    y_pred_train_en = clf_en.predict_proba(X_train)\n    \n     \n\n    training_loss.append(log_loss(y_train, y_pred_train_en))\n\n\n    test_loss.append(log_loss(y_test, y_pred_en))\n     \n        \nfor i in range(1,11):\n    tree_scores(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = [1,2,3,4,5,6,7,8,9,10]\n\n# plotting the line 1 points \nplt.plot(y, training_loss, label = \"training loss \")\n\n# plotting the line 2 points \nplt.plot(y, test_loss, label = \"test loss \")\nplt.xlabel('Max Depth')\n# Set the y axis label of the current axis.\nplt.ylabel('Log-Loss')\n# Set a title of the current axes.\nplt.title('Log-Loss plot ')\n# show a legend on the plot\nplt.legend()\n# Display a figure.\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_en = DecisionTreeClassifier(criterion='entropy', max_depth=6, random_state = 42)\n\nprint('Cross-Validation Score:',np.mean(cross_val_score(clf_en, X_train, y_train, cv=20)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_en = DecisionTreeClassifier(criterion='entropy', max_depth=6, random_state = 42)\n\nclf_en.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_en = clf_en.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the scores on training and test set\n\nprint('Training set score: {:.4f}'.format(clf_en.score(X_train, y_train)))\n\nprint('Test set score: {:.4f}'.format(clf_en.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dot_data = tree.export_graphviz(clf_en, out_file=None, \n                              feature_names=X_train.columns,  \n                              class_names=y_train,  \n                              filled=True, rounded=True,  \n                              special_characters=True)\n\ngraph = graphviz.Source(dot_data) \n\ngraph ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred_en)\n\nprint('Confusion matrix\\n\\n', cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax)  #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix')\nax.xaxis.set_ticklabels(y_test.unique())\nax.yaxis.set_ticklabels(y_test.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred_en))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# GRADIENT BOOSTING"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\ntraining_loss = []\ntest_loss = []\n\ndef tree_scores(i):\n    clf_gb = GradientBoostingClassifier( max_depth=i, random_state = 42)\n    \n    \n    clf_gb.fit(X_train, y_train.values.ravel())\n    y_pred_gb = clf_gb.predict_proba(X_test)\n    y_pred_train_gb = clf_gb.predict_proba(X_train)\n    \n     \n\n    training_loss.append(log_loss(y_train, y_pred_train_gb))\n\n\n    test_loss.append(log_loss(y_test, y_pred_gb))\n     \n        \nfor i in range(1,11):\n    tree_scores(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = [1,2,3,4,5,6,7,8,9,10]\n\n# plotting the line 1 points \nplt.plot(y, training_loss, label = \"training loss \")\n\n# plotting the line 2 points \nplt.plot(y, test_loss, label = \"test loss \")\nplt.xlabel('Max Depth')\n# Set the y axis label of the current axis.\nplt.ylabel('Log-Loss')\n# Set a title of the current axes.\nplt.title('Log-Loss plot ')\n# show a legend on the plot\nplt.legend()\n# Display a figure.\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_gb = GradientBoostingClassifier( max_depth=5, random_state = 42)\n\nprint('Cross-Validation Score:',np.mean(cross_val_score(clf_gb, X_train, y_train.values.ravel(), cv=10)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_gb = GradientBoostingClassifier( max_depth=5, random_state = 42)\n\nclf_gb.fit(X_train, y_train.values.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_gb = clf_gb.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the scores on training and test set\n\nprint('Training set score: {:.4f}'.format(clf_gb.score(X_train, y_train)))\n\nprint('Test set score: {:.4f}'.format(clf_gb.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dot_data = tree.export_graphviz(clf_gb.estimators_[0, 0], out_file=None, \n                              feature_names=X_train.columns,  \n                              class_names=['1','2','3','4'],  \n                              filled=True, rounded=True,  \n                              special_characters=True)\n\ngraph = graphviz.Source(dot_data) \n\ngraph ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred_gb)\n\nprint('Confusion matrix\\n\\n', cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax)  #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix')\nax.xaxis.set_ticklabels(pd.unique(y_test.values.ravel()))\nax.yaxis.set_ticklabels(pd.unique(y_test.values.ravel()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred_gb))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBOOST"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\nclf = xgb.XGBClassifier(max_depth=2, n_jobs = 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_loss = []\ntest_loss = []\n\ndef tree_scores(i):\n    clf_xgb = xgb.XGBClassifier( max_depth=i, random_state = 42, n_jobs = 4)\n    \n    \n    clf_xgb.fit(X_train, y_train.values.ravel())\n    y_pred_xgb = clf_xgb.predict_proba(X_test)\n    y_pred_train_xgb = clf_xgb.predict_proba(X_train)\n    \n     \n\n    training_loss.append(log_loss(y_train, y_pred_train_xgb))\n\n    test_loss.append(log_loss(y_test, y_pred_xgb))\n     \n        \nfor i in range(1,11):\n    tree_scores(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = [1,2,3,4,5,6,7,8,9,10]\n\n# plotting the line 1 points \nplt.plot(y, training_loss, label = \"training loss \")\n\n# plotting the line 2 points \nplt.plot(y, test_loss, label = \"test loss \")\nplt.xlabel('Max Depth')\n# Set the y axis label of the current axis.\nplt.ylabel('Log-Loss')\n# Set a title of the current axes.\nplt.title('Log-Loss plot ')\n# show a legend on the plot\nplt.legend()\n# Display a figure.\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_xgb = xgb.XGBClassifier( max_depth=4, random_state = 42, n_jobs = 4)\n\nprint('Cross-Validation Score:',np.mean(cross_val_score(clf_xgb, X_train, y_train.values.ravel(), cv=10)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_xgb = xgb.XGBClassifier( max_depth=4, random_state = 42, n_jobs = 4)\n\nclf_xgb.fit(X_train, y_train.values.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_xgb = clf_xgb.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the scores on training and test set\n\nprint('Training set score: {:.4f}'.format(clf_xgb.score(X_train, y_train)))\n\nprint('Test set score: {:.4f}'.format(clf_xgb.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(30, 30))\nxgb.plot_tree(clf_xgb, num_trees=4, ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred_xgb)\n\nprint('Confusion matrix\\n\\n', cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax)  #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix')\nax.xaxis.set_ticklabels(pd.unique(y_test.values.ravel()))\nax.yaxis.set_ticklabels(pd.unique(y_test.values.ravel()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred_xgb))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}