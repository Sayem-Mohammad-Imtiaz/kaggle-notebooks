{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Can Graph Neural Network solve sudoku?\n\nThe n-Sudoku graph is a graph with $n^4$ vertices, corresponding to the cells of an $n^2$ by $n^2$ grid. Two distinct vertices are adjacent if and only if they belong to the same row, column, or n-by-n box. In our case $n = 3$.\n\nSolving a sudoku can be seen as node classification problem over graph: each blank cell (node) has to be mapped into one of the following class C={1,...,9}\n\n## Dataset and preprocessing\n\nTo train and validate the model, it is used only a subset of size 100000 of the *9 milion sudoku dataset*.\nEach puzzle is first flattened into a vector and then each cell is one-hot-encoded.\n\nTo show the power of GNN it is only used 20% of the subset dataset to train the model.\n\n## Architecture \n\nA graph neural network is defined by stacking various \"aggregation layer\" that transform the representation of a given node into a new representation based on the node itsel and its neighbours defined by the graph.\nAs aggregation layer is used the classic self-attention layer but every node is allowed to attend only to its neighbours using a mask.\n\n## Training \n\nThe model is trained to classify all the node in the sudoku graph and not only the blank cell, using cross-entropy loss, for a maximum of 20 epochs.\n\nThe model achieve over 90% accuracy over the validation set after only 2 epochs.\n\n##Â Conclusion\n\nAfter 20 epochs the model achieve over 97% accuracy and there is still room for improvement."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import networkx as nx\nimport torch\nfrom torch import nn\nimport numpy as np\nimport pandas as pd\nfrom random import shuffle, sample\nimport pytorch_lightning as pl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 9 milion csv\nnum_examples = 100000\ntrain_split = 0.5\n\ndef encode(list_x):\n    x = torch.LongTensor(list_x)\n    x_encoded = torch.zeros((81, 10), dtype=torch.float)\n    x_encoded[torch.arange(81), x.reshape(-1)] = 1\n\n    return x_encoded\n\nprint(\"loading\")\ndf = next(\n            pd.read_csv('../input/sudoku/sudoku.csv', chunksize=(num_examples))\n        )\n\npuzzle, solution = df[[\"puzzle\", \"solution\"]].values.T\nprint(f\"total row: {df.shape[0]}\")\n\nprint(\"creating graphs ...\")\ngraphs = [(encode([int(d) for d in p]), \n           torch.LongTensor([int(d)-1 for d in s])) for p,s in zip(puzzle, solution)]\n\nprint(\"shuffling\")\nshuffle(graphs)\n\nprint(\"splitting\")\ntrain_9m = graphs[:int(num_examples*train_split)]\nval_9m = graphs[int(num_examples*train_split):]\nprint(len(train_9m), len(val_9m))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Dataset(torch.utils.data.Dataset):\n  'Characterizes a dataset for PyTorch'\n  def __init__(self, dataset):\n        # dataset is a list of tuples (x, y)\n        self.dataset = dataset\n\n  def __len__(self):\n        'Denotes the total number of samples'\n        return len(self.dataset)\n\n  def __getitem__(self, index):\n        # Load data and get label\n        data = self.dataset[index]\n        X = data[0]\n        y = data[1]\n\n        return X, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_set = Dataset(train_9m)\nvalid_set = Dataset(val_9m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class GNNTransformer(nn.Module):\n\n    def __init__(self):\n        super(GNNTransformer, self).__init__()\n\n        G = nx.sudoku_graph()\n        # If a BoolTensor is provided, positions with True is not allowed to attend while False values will be unchanged\n        self.register_buffer(\"A\", torch.BoolTensor(1 - nx.adjacency_matrix(G).todense()))\n\n        hidden_dim = 128\n        num_heads = 4\n        num_layers = 8\n\n        # node embedding to higher dimension\n        self.embedding = nn.Linear(10, hidden_dim)\n\n        # aggregation layers\n        self.transformers = nn.ModuleList([nn.MultiheadAttention(hidden_dim, num_heads) for _ in range(num_layers)])\n        self.pre_norm_layers = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n        self.post_norm_layers = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n\n        # mlp\n        self.mlp = nn.Sequential(*[\n                                   nn.Linear(hidden_dim, hidden_dim*2),\n                                   nn.ReLU(),\n                                   nn.Linear(hidden_dim * 2, 9)\n        ])\n\n    def forward(self, batch):\n\n        # batch is a matrix of size b x 81 x 10\n        x = self.embedding(batch)\n        residual = x\n\n        for transformer, pre_norm_layer, post_norm_layer in zip(self.transformers, self.pre_norm_layers, self.post_norm_layers):\n            x = pre_norm_layer(x)\n\n            # input requirements: sequence length (81) x batch x hidden_dim\n            x = x.transpose(0,1)\n            x, _ = transformer(x, x, x, attn_mask=self.A)\n            \n            # input requirements: batch x sequence length (81) x hidden_dim\n            x = x.transpose(0,1)\n            x = residual + nn.functional.relu(post_norm_layer(x))\n\n            residual = x\n\n        logits = self.mlp(x)\n\n        return logits\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class PlModel(pl.LightningModule):\n\n    def __init__(self):\n        super().__init__()\n\n        self.model = GNNTransformer()\n\n        # metric to log\n        self.metric = pl.metrics.Accuracy()\n\n        # define loss\n        self.loss = torch.nn.CrossEntropyLoss()\n\n    def forward(self, batch):\n        return self.model(batch)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr = 1e-4, weight_decay=0.)\n        return optimizer\n\n    def training_step(self, batch, batch_idx):\n\n        # x shape: bacth size x num nodes (81) x num features (10)\n        x = batch[0]\n\n        # y shape: batch size x num_nodes (81) \n        y = batch[1].reshape(-1)\n        \n        # logits: batch size x num_nodes (81) x num classes (9)\n        logits = self(x).reshape(-1, 9)\n\n        J = self.loss(logits, y)\n\n        y_pred = torch.nn.Softmax(dim=1)(logits)\n\n        # logs metrics for each training_step,\n        # and the average across the epoch, to the progress bar and logger\n        self.log('train_loss', J, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n        self.log('train_acc', self.metric(y_pred, y), on_step=False, on_epoch=True, prog_bar=True, logger=True)\n\n        return J\n\n    def validation_step(self, batch, batch_idx):\n\n        # x shape: bacth size x num nodes (81) x num features (10)\n        x = batch[0]\n        \n        mask = (x.argmax(dim=2) == 0).reshape(-1)\n\n        # y shape: batch size x num_nodes (81) \n        y = batch[1].reshape(-1)\n        \n        # logits: batch size x num_nodes (81) x num classes (9)\n        logits = self(x).reshape(-1, 9)\n\n        J = self.loss(logits, y)\n\n        y_pred = torch.nn.Softmax(dim=1)(logits)\n\n        # logs metrics for each training_step,\n        # and the average across the epoch, to the progress bar and logger\n        self.log('val_loss', J, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n        self.log('val_acc', self.metric(y_pred[mask], y[mask]), on_step=False, on_epoch=True, prog_bar=True, logger=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(training_set, batch_size=16, shuffle=True)\nvalid_loader = torch.utils.data.DataLoader(valid_set, batch_size=16, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = PlModel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer = pl.Trainer(\n    max_epochs=20, \n    progress_bar_refresh_rate=20, \n    gradient_clip_val=0.1, \n    gpus=1,\n    )\ntrainer.fit(model, train_loader, valid_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}