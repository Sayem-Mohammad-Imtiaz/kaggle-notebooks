{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Data Reading\n\nIn this tutorial, we will read the datasets to be used in this project."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport os","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fake News Net\n\nSample reading and processing of the Fake News Net dataset."},{"metadata":{},"cell_type":"markdown","source":"Reading/processing fake headlines."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"d_fake = pd.read_csv('../input/fake-news-data/fnn_politics_fake.csv')\nheadlines_fake = d_fake.drop(['id', 'news_url', 'tweet_ids'], axis=1).rename(columns={'title': 'headline'})\nheadlines_fake['fake'] = 1\nheadlines_fake.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reading/processing real headlines."},{"metadata":{"trusted":true},"cell_type":"code","source":"d_real = pd.read_csv('../input/fake-news-data/fnn_politics_real.csv')\nheadlines_real = d_real.drop(['id', 'news_url', 'tweet_ids'], axis=1).rename(columns={'title': 'headline'})\nheadlines_real['fake'] = 0\nheadlines_real.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Concatenating fake and real headlines + shuffling."},{"metadata":{"trusted":true},"cell_type":"code","source":"data1 = pd.concat([headlines_fake, headlines_real])\ndata1 = data1.sample(frac=1).reset_index(drop=True)\ndata1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fake News Dataset\n\n"},{"metadata":{},"cell_type":"markdown","source":"The *Fake News Dataset* is stored in directories and not in a single file. Therefore, we need to iterate over the directory files and store them separately. The fake and real news are stored in different directories, so we will write a data reading function to streamline the process."},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_data(d):\n    \"\"\"Each file has a headline as the first line, followed by some white space and then the article content.\n    We need to exract the headline and the content of each file and store them in lists.\"\"\"\n    fnames = os.listdir(d)\n    headlines, contents = [], []\n    for fname in fnames:\n        f = open(d + '/' + fname)\n        text = f.readlines()\n        f.close()\n\n        if len(text) == 2:\n            # One of the lines is missing\n            if len(text[1]) <= 1:\n                # There is no article content or headline\n                continue\n        elif len(text) >= 3:\n            # More than one empty line encountered\n            text[1] = text[-1]\n        else:\n            # Only one or zero lines is file\n            continue\n        \n        headline, content = text[0][:-1].strip().rstrip(), text[1][:-1]\n        headlines.append(headline)\n        contents.append(content)\n    \n    return headlines, contents","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First we'll read the fake news."},{"metadata":{"trusted":true},"cell_type":"code","source":"fake_dir = '../input/fake-news-data/fnd_news_fake'\nfake_headlines, fake_content = read_data(fake_dir)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we'll store them in a `DataFrame`."},{"metadata":{"trusted":true},"cell_type":"code","source":"fake_headlines = pd.DataFrame(fake_headlines, columns=['headline'])\nfake_headlines['fake'] = 1\nfake_headlines.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we repeat the process for real articles."},{"metadata":{"trusted":true},"cell_type":"code","source":"real_dir = '../input/fake-news-data/fnd_news_real'\nreal_headlines, real_content = read_data(real_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"real_headlines = pd.DataFrame(real_headlines, columns=['headline'])\nreal_headlines['fake'] = 0\nreal_headlines.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we will concatenate real and fake headlines."},{"metadata":{"trusted":true},"cell_type":"code","source":"data2 = pd.concat([fake_headlines, real_headlines])\ndata2 = data2.sample(frac=1).reset_index(drop=True)\ndata2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## New York Times Comments"},{"metadata":{"trusted":true},"cell_type":"code","source":"fnames = os.listdir('../input/nyt-comments')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## A Million News Headlines"},{"metadata":{"trusted":true},"cell_type":"code","source":"million = pd.read_csv('../input/million-headlines/abcnews-date-text.csv', delimiter=',')\nmillion = million.drop(['publish_date'], axis=1).rename(columns={'headline_text': 'headline'})\nmillion.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## All the News"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_news = pd.read_csv('../input/all-the-news/articles3.csv')\nall_news = all_news.rename(columns={'title': 'headline'})\nall_news = all_news['headline']\nall_news.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The Examiner\n\nA dataset containing clickbait articles."},{"metadata":{"trusted":true},"cell_type":"code","source":"examiner = pd.read_csv('../input/examine-the-examiner/examiner-date-text.csv')\nexaminer = examiner.drop(['publish_date'], axis=1).rename(columns={'headline_text': 'headline'})\nexaminer.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reuters"},{"metadata":{"trusted":true},"cell_type":"code","source":"reuters = pd.read_csv('../input/fake-news-data/reuters-newswire-2017.v5.csv')\nreuters = reuters.drop(['publish_time'], axis=1).rename(columns={'headline_text': 'headline'})\nreuters.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analysis\n\nWe are now going to bring the first two datasets together and perform some basic analysis tasks."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([data1, data2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, we are going to plot the lengths of headlines in characters."},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nimport matplotlib.pyplot as plt\n\nlengths = Counter(data['headline'].str.len())\nkeys, values = list(lengths.keys()), list(lengths.values())\nplt.bar(keys, values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And next we are going to plot the lengths in words."},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\n\nlengths = defaultdict(int)\nfor h in data['headline']:\n    lengths[len(h.split())] += 1\n\nkeys, values = list(lengths.keys()), list(lengths.values())\nplt.bar(keys, values)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}