{"cells":[{"metadata":{},"cell_type":"markdown","source":"Here, I have predicted the sentiment score of the Title and the Headline of the news articles. \n\nThe target columns are:\n- `SentimentTitle`, which is the sentiment score of the Title\n- `SentimentHeadline`, which is the sentiment score of the Headline\n\nI have used Custom Transform pipelines with Multi-Output Regressor in `scikit-learn`"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\nimport nltk\n# nltk.download('stopwords')\n# print('Downloaded Stopwords')\nfrom nltk.corpus import stopwords\nimport re\nfrom xgboost import XGBRegressor\nimport xgboost as xgb\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.naive_bayes import MultinomialNB\nimport seaborn as sns\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom spacy.lang.en import English\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom sklearn.base import TransformerMixin\nfrom sklearn.feature_extraction.text import TfidfVectorizer \nfrom sklearn.feature_extraction.text import TfidfTransformer\nstop_words = STOP_WORDS\nimport string\npunctuations = string.punctuation\nfrom sklearn.feature_extraction.text import HashingVectorizer","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/news-popularity-in-multiple-social-media-platforms/train_file.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[0,'Headline']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[0,'Title']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_val = pd.DataFrame(train.isnull().sum())\nmissing_val = missing_val.reset_index()\nmissing_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['Source'].isna()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Topic'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### EDA & Data Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nstopwords = nltk.corpus.stopwords.words('english')\nstopwords.extend(['Palestinian','Palestine','Microsoft','Economy','Obama','Barack'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nplt.figure(figsize=(12,6))\ntext = ' '.join(train.Title[train['Topic']=='economy'])\nwc = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nplt.figure(figsize=(12,6))\ntext = ' '.join(train.Title[train['Topic']=='obama'])\nwc = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nplt.figure(figsize=(12,6))\ntext = ' '.join(train.Title[train['Topic']=='microsoft'])\nwc = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nplt.figure(figsize=(12,6))\ntext = ' '.join(train.Title[train['Topic']=='palestine'])\nwc = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style='darkgrid',palette='Set1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = sns.jointplot(x='SentimentTitle',y='SentimentHeadline',data=train,kind = 'reg')\n_.annotate(stats.pearsonr)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Bar graph exploring total sentiment for the different topics\n\ntrain.groupby('Topic').agg('sum')[['SentimentHeadline', 'SentimentTitle']].plot(kind='bar', figsize=(25, 7),\n                                                          stacked=True, color=['b', 'r', 'g']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,15))\n_ = sns.heatmap(train[['Facebook','GooglePlus','LinkedIn','SentimentTitle','SentimentHeadline']].corr(), square=True, cmap='Blues',linewidths=0.5,linecolor='w',annot=True)\nplt.title('Correlation matrix ')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading spacy English model"},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = English()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Custom Tokenizer Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def spacy_tokenizer(sentence):\n    # Creating our token object, which is used to create documents with linguistic annotations.\n    mytokens = nlp(sentence)\n\n    # here the token is converted into lowercase if it is a Pronoun and if it is not a Pronoun then it is lemmatized and lowercased    \n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n\n    # Removing stop words using stopword from spacy library and punctuations from string library\n    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n\n    # return preprocessed list of tokens\n    return mytokens","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Custom Transformer and text cleaner "},{"metadata":{"trusted":true},"cell_type":"code","source":"class predictors(TransformerMixin):\n    def transform(self, X, **transform_params):\n        \n        return [clean_text(text) for text in X]\n\n    def fit(self, X, y, **fit_params):\n        return self\n\n    def get_params(self, deep=True):\n        return {}\n\n\ndef clean_text(text):\n   \n    return text.strip().lower()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Converting words to word vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"bow_vector = CountVectorizer(max_features = 100,tokenizer = spacy_tokenizer,ngram_range=(1,2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Spearating Title and headline, so that they can be trained separately"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_title = train.loc[:,'Title'].values\ny_train_title = train.loc[:,['SentimentTitle']].values\n\nX_train_headline = train.loc[:,'Headline'].values\ny_train_headline = train.loc[:,['SentimentHeadline']].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_title.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_headline.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Splitting both Title and Headline into training and testing sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train_title, x_valid_title, Y_train_title, y_valid_title = train_test_split(X_train_title, y_train_title, shuffle = True, test_size = 0.15)\nx_train_headline, x_valid_headline, Y_train_headline, y_valid_headline = train_test_split(X_train_headline, y_train_headline, shuffle = True, test_size = 0.15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost and Random Forrest Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgboost = MultiOutputRegressor(XGBRegressor())\nrand_for = MultiOutputRegressor(RandomForestRegressor(n_estimators=100,\n                                                          max_depth=None,\n                                                          random_state=0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Defining separate pipelines for title and headline. You can choose which regressor you want to use. In this notebook I have used the Random Forrest Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe_title = Pipeline([(\"cleaner\", predictors()),\n                 ('vectorizer', bow_vector),\n                 ('tfidf',TfidfTransformer()),\n                 ('regressor', rand_for)])\n\npipe_headline = Pipeline([(\"cleaner\", predictors()),\n                 ('vectorizer', bow_vector),\n                 ('tfidf',TfidfTransformer()),\n                 ('regressor', rand_for)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train the Regressors for title and headline respectively"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe_title.fit(x_train_title,Y_train_title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe_headline.fit(x_train_headline,Y_train_headline)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we shall predict on the validation sets and then see what score we obtain"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred_title=pipe_title.predict(x_valid_title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred_headline=pipe_headline.predict(x_valid_headline)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculating the Mean Absolute errors for both Title and Headline sentiments"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nmae_title=mean_absolute_error(y_valid_title,test_pred_title)\nmae_headline=mean_absolute_error(y_valid_headline,test_pred_headline)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we caclulate our final score. Score is calulated as \n\nmax(0, 1 - ((0.4*(mean abs error of title)+(0.6*(mean abs error of headline)))"},{"metadata":{"trusted":true},"cell_type":"code","source":"score=1-((0.4*mae_title)+(0.6*mae_headline))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Score = {} \\nScore(out of 100%) = {}%\".format(score,score*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We achieved a score of 89.9. That is pretty good. This score is an indication of how close our predicted values were to the target values. It cannot exacly be termed as `accurcacy`, because this is not a classification problem. Our sentiment score is a real number between -1 and 1"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}