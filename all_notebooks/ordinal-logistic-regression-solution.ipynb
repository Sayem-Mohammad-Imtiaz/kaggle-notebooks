{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Wine Quality Red - Ordinal Logistic Solution"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n%autosave 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn import datasets\n%matplotlib inline\nfrom sklearn.cluster import KMeans\nfrom matplotlib import style\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nstyle.use('ggplot')\nfrom sklearn import preprocessing\nimport math\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\ndf_copy=df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#present data\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### count plot for quality"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df['quality'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['quality'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### From the above barplot we see that the data is not equally distributed, and that much more observations are at the median quality levels 5, 6 that on the best or poor levels. \n"},{"metadata":{},"cell_type":"markdown","source":"### barplot data presentation"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(3, 4, figsize=(20, 10))\nfor value, subplot in zip(range(0,11), ax.flatten()):\n    sns.barplot(x=df['quality'], y=df[df.columns[value]],ax=subplot)\n#    sns.regplot(x=df[df.columns[value]], y=df['quality'],ax=subplot,truncate=True,scatter=False)\nfig.tight_layout() \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### From the above barplots we can see that visually, most of the quality can be explaind by the following parameters: volatile acidity, citric acid, chlorides, sulphates, alcohol.  An interesting parameter is the sulfur dioxide,  which at low values may indicate low or high quality, but at high values indiocate average quality."},{"metadata":{},"cell_type":"markdown","source":"### In this analysis, we will initially use all parameters, and see whether the visual impression relates to the  according p-values obtained."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df,hue='quality')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df[['volatile acidity', 'quality']].copy()\n\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = list(df)\ncol=col[0:11]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### showing only diagonal pairplot, with distribution per quality."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in col:\n    df1 = df[[i, 'quality']].copy()\n    sns.pairplot(df1, hue='quality',size=5)\nplt.show()   \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### from the above distributions we see that the parameters which may have more influence on quality level are: fixed acidity, volatile acidity, citric acid, density, sulphates, alcohol.  We'll see next by P-values if the visual interpreatation is verified."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlation"},{"metadata":{},"cell_type":"markdown","source":"### Cross correlation is investigated to see whether correlation parameters are needed in the regression in the form of XiXj"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_corr=df.corr()\ndf_corr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_list = df.columns.values.tolist()\ncol_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2=df.copy()\ndf2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.drop(['quality'], axis='columns', inplace=True)\ndf2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_list2 = df2.columns.values.tolist()\ncol_list2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### If correlations above 0.8 exist, they will be added as parameters do the dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"    count=0\n    for i in range(1,11):\n        for j in range(i+1,11):\n            if (abs(df_corr.iloc[i][j])>0.8 and i!=j):\n                mlt=\"*\"\n                string=col_list[i]+mlt+col_list[j]\n                df2[string]=df2[col_list[i]]*df2[col_list[j]]\n                count=count+1\n        \ndf2.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(count)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The parameters are not highly correlated, so we will not use correlated parameters in the analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation heatmap\nmask=np.array(df_corr)\nfig=plt.gcf()\nfig.set_size_inches(30,12)\nsns.heatmap(df_corr,annot=True,cbar=True,square=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### No high correlations found, the dataset will remain as given."},{"metadata":{},"cell_type":"markdown","source":"# Ordinal Logistic regression"},{"metadata":{},"cell_type":"markdown","source":"### Ordinal Logistic Regression is the common analysis method used for ordinal dependent parameters. This model is used in the medicine ans social arts worlds, where subjective dpenedent variables exist, such as level of pain, satisfaction or cancer development level. Using a non-ordinal classification method misses the ordinality of the dependent parameter, which is essential for good prediction and problen description. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report\nimport statsmodels.api as sm \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Python has no bulit-in model for ordinal logistic regression, therefore it will be implemented here. We will use the proportional odds model for the Ordinal Logistic Regression. Denoting the number of categories by K, We make (K-1) logistic regression comparisons, each time cutting the entire dataset by two and comparing all the lower categories to higher categories. Each of these constitutes a sub-model and finally they will all be blended into one holisic model. In our example, since there are 6 quality categories, we make 5 logistic regression models."},{"metadata":{"trusted":true},"cell_type":"code","source":"df34=df.copy()\ncount34=0\nfor i in range (0,1599):\n    if (df['quality'].iloc[i]>=4):\n        df34['quality'].iloc[i]=1\n        \n    else:\n        df34['quality'].iloc[i]=0 \n        count34=count34+1\n#df34  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The count_ij parameters below indicate the number of observations in the lower class. Recalling there are 1,599 observations totally, this variable indicates the level of unbalance in the specific sub-model partition. The unbalance existing in the dataset is enhanced by the Ordinal Logistic Regression analysis methiod"},{"metadata":{"trusted":true},"cell_type":"code","source":"count34","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df45=df.copy()\ncount45=0\nfor i in range (0,1599):\n    if (df['quality'].iloc[i]>=5):\n        df45['quality'].iloc[i]=1\n        \n    else:\n        df45['quality'].iloc[i]=0 \n        count45=count45+1\n#df45","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count45","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df56=df.copy()\ncount56=0\nfor i in range (0,1599):\n    if (df['quality'].iloc[i]>=6):\n        df56['quality'].iloc[i]=1\n        \n    else:\n        df56['quality'].iloc[i]=0 \n        count56=count56+1\n#df56","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count56","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df67=df.copy()\ncount67=0\nfor i in range (0,1599):\n    if (df['quality'].iloc[i]>=7):\n        df67['quality'].iloc[i]=1\n        \n    else:\n        df67['quality'].iloc[i]=0 \n        count67=count67+1\n#df67","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count67","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df78=df.copy()\ncount78=0\nfor i in range (0,1599):\n    if (df['quality'].iloc[i]>=8):\n        df78['quality'].iloc[i]=1\n        \n    else:\n        df78['quality'].iloc[i]=0 \n        count78=count78+1\n#df78","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count78","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let us now train and predict each of the four logistic Regression "},{"metadata":{"trusted":true},"cell_type":"code","source":"X34=df34.iloc[:,0:11] \ny34=df34['quality']\nX_train34, X_test34, y_train34, y_test34 = train_test_split(X34, y34, test_size=0.2,random_state=0)\nlgr34=LogisticRegression()\nlgr34.fit(X_train34, y_train34)\npred34=lgr34.predict(X_test34)\npred34","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_test34,pred34)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We see that due to low number of observations in quality level 3, all predictions are 1. The AUC is 0.5, meaning the regression has no significance."},{"metadata":{"trusted":true},"cell_type":"code","source":"df34","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X45=df45.iloc[:,0:11] \ny45=df45['quality']\nX_train45, X_test45, y_train45, y_test45 = train_test_split(X45, y45, test_size=0.2,random_state=0)\nlgr45=LogisticRegression()\nlgr45.fit(X_train45, y_train45)\npred45=lgr45.predict(X_test45)\npred45","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_test45,pred45)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df45","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We received a bad AUC and homogeneous predicted vector here as well, due to lack of observations in categories 3+4."},{"metadata":{"trusted":true},"cell_type":"code","source":"X56=df56.iloc[:,0:11] \ny56=df56['quality']\nX_train56, X_test56, y_train56, y_test56 = train_test_split(X56, y56, test_size=0.2,random_state=0)\nlgr56=LogisticRegression()\nlgr56.fit(X_train56, y_train56)\npred56=lgr56.predict(X_test56)\npred56","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_test56,pred56)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The AUC received here is not optimal, but reasinable for a single sub-model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg56 = sm.Logit(y_train56, X_train56).fit() \nprint(log_reg56.summary()) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Some of the P-values obtained are higher than 0.05, and will be omitted later from the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"X67=df67.iloc[:,0:11] \ny67=df67['quality']\nX_train67, X_test67, y_train67, y_test67 = train_test_split(X67, y67, test_size=0.2,random_state=0)\nlgr67=LogisticRegression()\nlgr67.fit(X_train67, y_train67)\npred67=lgr67.predict(X_test67)\npred67","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_test67,pred67)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The AUC received here is not optimal, but reasinable for a single sub-model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg67 = sm.Logit(y_train67, X_train67).fit() \nprint(log_reg67.summary()) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Some of the P-values obtained are higher than 0.05, and will be omitted later from the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"X78=df78.iloc[:,0:11] \ny78=df78['quality']\nX_train78, X_test78, y_train78, y_test78 = train_test_split(X78, y78, test_size=0.2,random_state=0)\nlgr78=LogisticRegression()\nlgr78.fit(X_train78, y_train78)\npred78=lgr78.predict(X_test78)\npred78","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_test78,pred78)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We received a bad AUC and homogeneous predicted vector here as well, and a homogeneous predicted Y vector due to lack of observations in categor"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Due to the results of the first run, we hve two types of conclusions:\n### 1.  We see that due to low number of observations in the 3, 3+4 and 8 quality categories, the model could not    \n###      fit well, and did not take into account the extremal qualities\n### 2. Due to low P-values, we will omit from all sub-models the following parameters: fixed acidity, free sulfur rate, PH.\n###   From sub-model 5-6 we will also omit residual sugar, and from submodel 6-7  we omit citric acid. The \n###   following are the results of the new runs."},{"metadata":{},"cell_type":"markdown","source":"### Omitting categories fixed acidity, free sulfur rate, PH from entire model"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_low= df.drop(['fixed acidity','free sulfur dioxide','pH'],1)\ndf_low","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df34=df_low.copy()\ncount34=0\nfor i in range (0,1599):\n    if (df_low['quality'].iloc[i]>=4):\n        df34['quality'].iloc[i]=1\n        \n    else:\n        df34['quality'].iloc[i]=0 \n        count34=count34+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df34","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df45=df_low.copy()\ncount45=0\nfor i in range (0,1599):\n    if (df_low['quality'].iloc[i]>=5):\n        df45['quality'].iloc[i]=1\n        \n    else:\n        df45['quality'].iloc[i]=0 \n        count45=count45+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df45","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df56=df_low.copy()\ndf56=df56.drop('residual sugar',1)\ncount56=0\nfor i in range (0,1599):\n    if (df_low['quality'].iloc[i]>=6):\n        df56['quality'].iloc[i]=1\n        \n    else:\n        df56['quality'].iloc[i]=0 \n        count56=count56+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df56","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df67=df_low.copy()\ndf67=df67.drop('citric acid',1)\ncount67=0\nfor i in range (0,1599):\n    if (df_low['quality'].iloc[i]>=7):\n        df67['quality'].iloc[i]=1\n        \n    else:\n        df67['quality'].iloc[i]=0 \n        count67=count67+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df67","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df78=df_low.copy()\ncount78=0\nfor i in range (0,1599):\n    if (df['quality'].iloc[i]>=8):\n        df78['quality'].iloc[i]=1\n        \n    else:\n        df78['quality'].iloc[i]=0 \n        count78=count78+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df78","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let us run all regressions again, and check for AUC and P-values"},{"metadata":{"trusted":true},"cell_type":"code","source":"X34=df34.iloc[:,0:8] \ny34=df34['quality']\nX_train34, X_test34, y_train34, y_test34 = train_test_split(X34, y34, test_size=0.2,random_state=0)\nlgr34=LogisticRegression()\nlgr34.fit(X_train34, y_train34)\npred34=lgr34.predict(X_test34)\npred34","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_test34,pred34)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The still bad AUC means it is truly a low observations problem, and not insignificant variables causing noisy result."},{"metadata":{"trusted":true},"cell_type":"code","source":"X45=df45.iloc[:,0:8] \ny45=df45['quality']\nX_train45, X_test45, y_train45, y_test45 = train_test_split(X45, y45, test_size=0.2,random_state=0)\nlgr45=LogisticRegression()\nlgr45.fit(X_train45, y_train45)\npred45=lgr45.predict(X_test45)\npred45","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_test45,pred45)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df45 ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The still bad AUC means it is truly a low observations problem, and not insignificant variables causing noisy result."},{"metadata":{"trusted":true},"cell_type":"code","source":"X56=df56.iloc[:,0:7] \ny56=df56['quality']\nX_train56, X_test56, y_train56, y_test56 = train_test_split(X56, y56, test_size=0.2,random_state=0)\nlgr56=LogisticRegression()\nlgr56.fit(X_train56, y_train56)\npred56=lgr56.predict(X_test56)\npred56","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_test56,pred56)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Small improvement relative to previous run"},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg56 = sm.Logit(y_train56, X_train56).fit() \nprint(log_reg56.summary()) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Citric acid highly insignificant, and will be omitted now."},{"metadata":{"trusted":true},"cell_type":"code","source":"df56=df56.drop('citric acid',1)\ncount56=0\nfor i in range (0,1599):\n    if (df_low['quality'].iloc[i]>=6):\n        df56['quality'].iloc[i]=1\n        \n    else:\n        df56['quality'].iloc[i]=0 \n        count56=count56+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df56","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X56=df56.iloc[:,0:6] \ny56=df56['quality']\nX_train56, X_test56, y_train56, y_test56 = train_test_split(X56, y56, test_size=0.2,random_state=0)\nlgr56=LogisticRegression()\nres56=lgr56.fit(X_train56, y_train56)\npred56=lgr56.predict(X_test56)\npred56","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_test56,pred56)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Still non-optimal AUC"},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg56 = sm.Logit(y_train56, X_train56).fit() \nprint(log_reg56.summary()) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### All P-values are significant now."},{"metadata":{"trusted":true},"cell_type":"code","source":"X67=df67.iloc[:,0:7] \ny67=df67['quality']\nX_train67, X_test67, y_train67, y_test67 = train_test_split(X67, y67, test_size=0.2,random_state=0)\nlgr67=LogisticRegression()\nres67=lgr67.fit(X_train67, y_train67)\npred67=lgr67.predict(X_test67)\npred67","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_test67,pred67)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Still non-optimal AUC"},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg67 = sm.Logit(y_train67, X_train67).fit() \nprint(log_reg67.summary()) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### All P-values smaller than 0.05 and siginificant "},{"metadata":{"trusted":true},"cell_type":"code","source":"X78=df78.iloc[:,0:8] \ny78=df78['quality']\nX_train78, X_test78, y_train78, y_test78 = train_test_split(X78, y78, test_size=0.2,random_state=0)\nlgr78=LogisticRegression()\nres78=lgr78.fit(X_train78, y_train78)\npred78=lgr78.predict(X_test78)\npred78","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_test78,pred78)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The still bad AUC means it is truly a low observations problem, and not insignificant variables causing noisy result."},{"metadata":{},"cell_type":"markdown","source":"# SMOTE algorithm"},{"metadata":{},"cell_type":"markdown","source":"### Therefore we will use the SMOTE algorithm, which will oversample the low-observation quality levels and undersample the rest,to get better results."},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# performing SMOTE for 3-4\nX34_resampled, y34_resampled = SMOTE().fit_sample(X34, y34)\nX34_resampled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# performing SMOTE for 4-5\nX45_resampled, y45_resampled = SMOTE().fit_sample(X45, y45)\nX45_resampled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# performing SMOTE for 6-7\nX67_resampled, y67_resampled = SMOTE().fit_sample(X67, y67)\nX67_resampled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# performing SMOTE for 7-8\nX78_resampled, y78_resampled = SMOTE().fit_sample(X78, y78)\nX78_resampled","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comparing the independent parameter vecotr before and after SMOTE algorithm implementation:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y34.mean(), y34_resampled.mean())\nprint(y45.mean(), y45_resampled.mean())\nprint(y67.mean(), y67_resampled.mean())\nprint(y78.mean(), y78_resampled.mean())\nprint(y56.mean() )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We see the previously unbalanced y vector is now balanced."},{"metadata":{},"cell_type":"markdown","source":"### Repeating sub-modeling for post-SMOTE dataset."},{"metadata":{},"cell_type":"markdown","source":"### SMOTE for 3-4 submodel"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train34_re, X_test34_re, y_train34_re, y_test34_re = train_test_split(X34_resampled, y34_resampled, test_size=0.2,random_state=0)\nlgr34_re=LogisticRegression()\nlgr34_re.fit(X_train34_re, y_train34_re)\npred34_re=lgr34_re.predict(X_test34_re)\npred34_re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_test34_re,pred34_re)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Higher AUC after SMOTE"},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg34_re = sm.Logit(y_train34_re, X_train34_re).fit() \nprint(log_reg34_re.summary()) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### P-value for sulphates and residual sugar too high - above 0.05. We shall omit them from sub-model, and re-run\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df34=df34.drop(['sulphates','residual sugar'],1)\ncount34=0\nfor i in range (0,1599):\n    if (df_low['quality'].iloc[i]>=4):\n        df34['quality'].iloc[i]=1\n        \n    else:\n        df34['quality'].iloc[i]=0 \n        count34=count34+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df34","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Implement SMOTE again"},{"metadata":{"trusted":true},"cell_type":"code","source":"X34=df34.iloc[:,0:6] \ny34=df34['quality']\nX_train34, X_test34, y_train34, y_test34 = train_test_split(X34, y34, test_size=0.2,random_state=0)\nlgr34=LogisticRegression()\nlgr34.fit(X_train34, y_train34)\npred34=lgr34.predict(X_test34)\npred34","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X34_resampled, y34_resampled = SMOTE().fit_sample(X34, y34)\nX34_resampled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y34.mean(), y34_resampled.mean())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The dataset is now balanced"},{"metadata":{},"cell_type":"markdown","source":"### Run submodel again"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train34_re, X_test34_re, y_train34_re, y_test34_re = train_test_split(X34_resampled, y34_resampled, test_size=0.2,random_state=0)\nlgr34_re=LogisticRegression()\nres34=lgr34_re.fit(X_train34_re, y_train34_re)\npred34_re=lgr34_re.predict(X_test34_re)\npred34_re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_test34_re,pred34_re)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg34_re = sm.Logit(y_train34_re, X_train34_re).fit() \nprint(log_reg34_re.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### All P-values significant now"},{"metadata":{},"cell_type":"markdown","source":"### SMOTE for 4-5 submodel"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train45_re, X_test45_re, y_train45_re, y_test45_re = train_test_split(X45_resampled, y45_resampled, test_size=0.2,random_state=0)\nlgr45_re=LogisticRegression()\nlgr45_re.fit(X_train45_re, y_train45_re)\npred45_re=lgr45_re.predict(X_test45_re)\npred45_re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_test45_re,pred45_re)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg45_re = sm.Logit(y_train45_re, X_train45_re).fit() \nprint(log_reg45_re.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### density has high P-value, and will now be omitted from sub-model"},{"metadata":{"trusted":true},"cell_type":"code","source":"df45=df45.drop('density',1)\ncount45=0\nfor i in range (0,1599):\n    if (df_low['quality'].iloc[i]>=5):\n        df45['quality'].iloc[i]=1\n        \n    else:\n        df45['quality'].iloc[i]=0 \n        count45=count45+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df45","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X45=df45.iloc[:,0:7] \ny45=df45['quality']\nX_train45, X_test45, y_train45, y_test45 = train_test_split(X45, y45, test_size=0.2,random_state=0)\nlgr45=LogisticRegression()\nlgr45.fit(X_train45, y_train45)\npred45=lgr45.predict(X_test45)\npred45","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X45_resampled, y45_resampled = SMOTE().fit_sample(X45, y45)\nX45_resampled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train45_re, X_test45_re, y_train45_re, y_test45_re = train_test_split(X45_resampled, y45_resampled, test_size=0.2,random_state=0)\nlgr45_re=LogisticRegression()\nres45=lgr45_re.fit(X_train45_re, y_train45_re)\npred45_re=lgr45_re.predict(X_test45_re)\npred45_re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_test45_re,pred45_re)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg45_re = sm.Logit(y_train45_re, X_train45_re).fit() \nprint(log_reg45_re.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### All P-values below 0.05."},{"metadata":{},"cell_type":"markdown","source":"### SMOTE 6-7"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train67_re, X_test67_re, y_train67_re, y_test67_re = train_test_split(X67_resampled, y67_resampled, test_size=0.2,random_state=0)\nlgr67_re=LogisticRegression()\nres67=lgr67_re.fit(X_train67_re, y_train67_re)\npred67_re=lgr67_re.predict(X_test67_re)\npred67_re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_test67_re,pred67_re)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg67_re = sm.Logit(y_train67_re, X_train67_re).fit() \nprint(log_reg67_re.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### All P-values are below 0.05"},{"metadata":{"trusted":true},"cell_type":"code","source":"df78","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SMOTE 7-8"},{"metadata":{"trusted":true},"cell_type":"code","source":"X78-df78.iloc[:,0:8]\nX_train78_re, X_test78_re, y_train78_re, y_test78_re = train_test_split(X78_resampled, y78_resampled, test_size=0.2,random_state=0)\nlgr78_re=LogisticRegression()\nres78=lgr78_re.fit(X_train78_re, y_train78_re)\npred78_re=lgr78_re.predict(X_test78_re)\npred78_re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_test78_re,pred78_re)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### High AUC here"},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg78_re = sm.Logit(y_train78_re, X_train78_re).fit() \nprint(log_reg78_re.summary()) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### All P-values below 0.05"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ROC Curves for sub-models"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nfpr, tpr, thresholds = metrics.roc_curve(y_test34_re, pred34_re)\nplt.plot(fpr, tpr)\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\nprint('AUC=',roc_auc_score(y_test34_re,pred34_re))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = metrics.roc_curve(y_test45_re, pred45_re)\nplt.plot(fpr, tpr)\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\nprint('AUC=',roc_auc_score(y_test45_re,pred45_re))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = metrics.roc_curve(y_test56, pred56)\nplt.plot(fpr, tpr)\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\nprint('AUC=',roc_auc_score(y_test56,pred56))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = metrics.roc_curve(y_test67_re, pred67_re)\nplt.plot(fpr, tpr)\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\nprint('AUC=',roc_auc_score(y_test67_re,pred67_re))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = metrics.roc_curve(y_test78_re, pred78_re)\nplt.plot(fpr, tpr)\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\nprint('AUC=',roc_auc_score(y_test78_re,pred78_re))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Entire Model Goodness of fit"},{"metadata":{},"cell_type":"markdown","source":"### The goodness of fit will be determined using two methods: 1. Hosmer–Lemeshow criterion, see reference [2] 2. Running observations through the model, and finding their predicted quality level. the result is analyzed using chi squared statistic."},{"metadata":{},"cell_type":"markdown","source":"### finding the probability of each quality level for each observation:"},{"metadata":{},"cell_type":"markdown","source":"### Pi are the predicted probabilities for each quality level.\n### Pgti are the probability found in each cut for the qualiti level to be i or above."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['P3']=0\ndf['P4']=0\ndf['P5']=0\ndf['P6']=0\ndf['P7']=0\ndf['P8']=0\ndf['Score']=0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Pgt4']=0\ndf['Pgt5']=0\ndf['Pgt6']=0\ndf['Pgt7']=0\ndf['Pgt8']=0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Writing all sub-model coefficients"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3-4 model coefficients\nprint(res34.coef_)\nprint(res34.intercept_)\ndf34_col=df34.columns.tolist()\nprint(df34_col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4-5 model coefficients\nprint(res45.coef_)\nprint(res45.intercept_)\ndf45_col=df45.columns.tolist()\nprint(df45_col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 5-6 model coefficients\nprint(res56.coef_)\nprint(res56.intercept_)\ndf56_col=df56.columns.tolist()\nprint(df56_col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 6-7 model coefficients\nprint(res67.coef_)\nprint(res67.intercept_)\ndf67_col=df67.columns.tolist()\nprint(df67_col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 7-8 model coefficients\nprint(res78.coef_)\nprint(res78.intercept_)\ndf78_col=df78.columns.tolist()\nprint(df78_col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calculating Probabilities for each observation, and overall Score, for the Hosmer–Lemeshow criterion"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for all observations\nfor i in range (0,1599):\n    #for all models\n    \n    linear34=res34.intercept_[0]\n    for lin in range (0,len(df34_col)-1):\n            # calculate linear alpha+beta*X\n            linear34=linear34+res34.coef_[0,lin]*df34[df34_col[lin]].iloc[i]\n    #Calculating probability of being equal or larger that quality level 4\n    df['Pgt4'].iloc[i]=math.exp(linear34)/(1+math.exp(linear34))\n    # Calculating probability of quality level 3\n    df['P3'].iloc[i]=1-df['Pgt4'].iloc[i]\n    \n    linear45=res45.intercept_[0]\n    for lin in range (0,len(df45_col)-1):\n            # calculate linear alpha+beta*X\n            linear45=linear45+res45.coef_[0,lin]*df45[df45_col[lin]].iloc[i]\n    df['Pgt5'].iloc[i]=math.exp(linear45)/(1+math.exp(linear45))\n    df['P4'].iloc[i]=df['Pgt4'].iloc[i]-df['Pgt5'].iloc[i]\n    \n    linear56=res56.intercept_[0]\n    for lin in range (0,len(df56_col)-1):\n            # calculate linear alpha+beta*X\n            linear56=linear56+res56.coef_[0,lin]*df56[df56_col[lin]].iloc[i]\n    df['Pgt6'].iloc[i]=math.exp(linear56)/(1+math.exp(linear56))\n    df['P5'].iloc[i]=df['Pgt5'].iloc[i]-df['Pgt6'].iloc[i] \n    \n    linear67=res67.intercept_[0]\n    for lin in range (0,len(df67_col)-1):\n            # calculate linear alpha+beta*X\n            linear67=linear67+res67.coef_[0,lin]*df67[df67_col[lin]].iloc[i]\n    df['Pgt7'].iloc[i]=math.exp(linear67)/(1+math.exp(linear67))\n    df['P6'].iloc[i]=df['Pgt6'].iloc[i]-df['Pgt7'].iloc[i] \n    \n    linear78=res78.intercept_[0]\n    for lin in range (0,len(df78_col)-1):\n            # calculate linear alpha+beta*X\n            linear78=linear78+res78.coef_[0,lin]*df78[df78_col[lin]].iloc[i]\n    df['Pgt8'].iloc[i]=math.exp(linear78)/(1+math.exp(linear78))\n    df['P7'].iloc[i]=df['Pgt7'].iloc[i]-df['Pgt8'].iloc[i]   \n    \n    df['P8'].iloc[i]=df['Pgt8'].iloc[i]\n    # Calculating score acording to Hosmer–Lemeshow method\n    df['Score'].iloc[i]=3*df['P3'].iloc[i]+4*df['P4'].iloc[i]+5*df['P5'].iloc[i]+6*df['P6'].iloc[i]+7*df['P7'].iloc[i]+8*df['P8'].iloc[i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### checking whether some of Pi is equal or close to 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"k=0\nfor j in range (0,1599):\n    cnt=0\n    for i in range (0,5):\n        cnt=cnt+df.iloc[j,12+i]\n    if (cnt<0.9):\n#        print (j,df['quality'].iloc[j],cnt)\n        k=k+1\nprint(k)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### As we can see, most observations yield excelent results, while others yield quite inadequate Pi. Thus the criterion cannot be implemented. we previously saw that the AUC yielded un-optimal results for some sub-models, while all P-values for all submodels are very low. This implies that although the chosen parameters for each sub-model are significant' they fail to explain the entire model, and more parameters are needed. we should also recall, that hree of the 11 parameters chosen wre found insignificant (fixed acidity, free sulfur chloride, pH) "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## different way - all observations will be passed through the entire model, and observed quality will be found. Then all will be sorted by size or sub model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# for all observations\ndf['observed']=0\nfor i in range (0,1599):\n    #for all models\n    df['observed'].iloc[i]=3\n    linear34=res34.intercept_[0]\n    for lin in range (0,len(df34_col)-1):\n            # calculate linear alpha+beta*X\n            linear34=linear34+res34.coef_[0,lin]*df34[df34_col[lin]].iloc[i]\n    df['Pgt4'].iloc[i]=math.exp(linear34)/(1+math.exp(linear34))\n    # artificially introducing threshold level\n    if (df['Pgt4'].iloc[i]>=0.04):\n        df['observed'].iloc[i]+=1\n  #  df['P3'].iloc[i]=1-df['Pgt4'].iloc[i]\n    \n    linear45=res45.intercept_[0]\n    for lin in range (0,len(df45_col)-1):\n            # calculate linear alpha+beta*X\n            linear45=linear45+res45.coef_[0,lin]*df45[df45_col[lin]].iloc[i]\n    df['Pgt5'].iloc[i]=math.exp(linear45)/(1+math.exp(linear45))\n    if (df['Pgt5'].iloc[i]>=0.17):\n        df['observed'].iloc[i]+=1\n#    df['P4'].iloc[i]=df['Pgt4'].iloc[i]-df['Pgt5'].iloc[i]\n    \n    linear56=res56.intercept_[0]\n    for lin in range (0,len(df56_col)-1):\n            # calculate linear alpha+beta*X\n            linear56=linear56+res56.coef_[0,lin]*df56[df56_col[lin]].iloc[i]\n    df['Pgt6'].iloc[i]=math.exp(linear56)/(1+math.exp(linear56))\n    if (df['Pgt6'].iloc[i]>=0.49):\n        df['observed'].iloc[i]+=1\n#   df['P5'].iloc[i]=df['Pgt5'].iloc[i]-df['Pgt6'].iloc[i] \n    \n    linear67=res67.intercept_[0]\n    for lin in range (0,len(df67_col)-1):\n            # calculate linear alpha+beta*X\n            linear67=linear67+res67.coef_[0,lin]*df67[df67_col[lin]].iloc[i]\n    df['Pgt7'].iloc[i]=math.exp(linear67)/(1+math.exp(linear67))\n    if (df['Pgt7'].iloc[i]>=0.999):\n        df['observed'].iloc[i]+=1\n#    df['P6'].iloc[i]=df['Pgt6'].iloc[i]-df['Pgt7'].iloc[i] \n    \n    linear78=res78.intercept_[0]\n    for lin in range (0,len(df78_col)-1):\n            # calculate linear alpha+beta*X\n            linear78=linear78+res78.coef_[0,lin]*df78[df78_col[lin]].iloc[i]\n    df['Pgt8'].iloc[i]=math.exp(linear78)/(1+math.exp(linear78))\n    if (df['Pgt8'].iloc[i]>=0.5):\n        df['observed'].iloc[i]+=1\n#    df['P7'].iloc[i]=df['Pgt7'].iloc[i]-df['Pgt8'].iloc[i]   \n    \n#    df['P8'].iloc[i]=df['Pgt8'].iloc[i]\n    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calculating chi2"},{"metadata":{"trusted":true},"cell_type":"code","source":"chi2=0\nfor i in range (0,1599):\n    chi2+=(df['quality'].iloc[i]-df['observed'].iloc[i])**2/df['quality'].iloc[i]\nchi2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Obtained very high chi2"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calculating chi2 components"},{"metadata":{"trusted":true},"cell_type":"code","source":"chi2_3= ((df.quality == 3).sum()-(df.observed==3).sum())**2/(df.quality == 3).sum()\nchi2_4=((df.quality == 4).sum()-(df.observed==4).sum())**2/(df.quality == 4).sum()\nchi2_5=((df.quality == 5).sum()-(df.observed==5).sum())**2/(df.quality == 5).sum()\nchi2_6=((df.quality == 6).sum()-(df.observed==6).sum())**2/(df.quality == 6).sum()\nchi2_7=((df.quality == 7).sum()-(df.observed==7).sum())**2/(df.quality == 7).sum()\nchi2_8=((df.quality == 8).sum()-(df.observed==8).sum())**2/(df.quality == 4).sum()\nprint('chi2_3=',chi2_3,'chi2_4=',chi2_4,'chi2_3=',chi2_3,'chi2_4=',chi2_4,'chi2_5=',chi2_5, 'chi2_6=',chi2_6, 'chi2_7=',chi2_7, 'chi2_8=',chi2_8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The most significant contributors to Chi2 are the 5-6, 6-7 and 7-8 submodels"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Note the lines similar to (df['Pgt7'].iloc[i]>=0.999). This is an attempt to produce an artificial logistic regression threshold. Even at a 0.999 thresholds, the difference between observed and predicted count is still too large. "},{"metadata":{"trusted":true},"cell_type":"code","source":"(df.quality == 3).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(df.observed==3).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(df.quality == 4).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(df.observed==4).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(df.quality == 5).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(df.observed==5).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(df.quality == 6).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(df.observed==6).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(df.quality == 7).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(df.observed==7).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(df.quality == 8).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(df.observed==8).sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Running from high to low quality levels, to see if this is a cause of model mismatch"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['observed']=0\nfor i in range (0,1599):\n    #for all models\n    df['observed'].iloc[i]=3\n    \n    linear34=res34.intercept_[0]\n    for lin in range (0,len(df34_col)-1):\n            # calculate linear alpha+beta*X\n            linear34=linear34+res34.coef_[0,lin]*df34[df34_col[lin]].iloc[i]\n    df['Pgt4'].iloc[i]=math.exp(linear34)/(1+math.exp(linear34))\n    if (df['Pgt4'].iloc[i]>=0.04):\n        df['observed'].iloc[i]+=1\n  #  df['P3'].iloc[i]=1-df['Pgt4'].iloc[i]\n    \n    linear45=res45.intercept_[0]\n    for lin in range (0,len(df45_col)-1):\n            # calculate linear alpha+beta*X\n            linear45=linear45+res45.coef_[0,lin]*df45[df45_col[lin]].iloc[i]\n    df['Pgt5'].iloc[i]=math.exp(linear45)/(1+math.exp(linear45))\n    if (df['Pgt5'].iloc[i]>=0.17):\n        df['observed'].iloc[i]+=1\n#    df['P4'].iloc[i]=df['Pgt4'].iloc[i]-df['Pgt5'].iloc[i]\n    \n    linear56=res56.intercept_[0]\n    for lin in range (0,len(df56_col)-1):\n            # calculate linear alpha+beta*X\n            linear56=linear56+res56.coef_[0,lin]*df56[df56_col[lin]].iloc[i]\n    df['Pgt6'].iloc[i]=math.exp(linear56)/(1+math.exp(linear56))\n    if (df['Pgt6'].iloc[i]>=0.49):\n        df['observed'].iloc[i]+=1\n#   df['P5'].iloc[i]=df['Pgt5'].iloc[i]-df['Pgt6'].iloc[i] \n    \n    linear67=res67.intercept_[0]\n    for lin in range (0,len(df67_col)-1):\n            # calculate linear alpha+beta*X\n            linear67=linear67+res67.coef_[0,lin]*df67[df67_col[lin]].iloc[i]\n    df['Pgt7'].iloc[i]=math.exp(linear67)/(1+math.exp(linear67))\n    if (df['Pgt7'].iloc[i]>=0.999):\n        df['observed'].iloc[i]+=1\n#    df['P6'].iloc[i]=df['Pgt6'].iloc[i]-df['Pgt7'].iloc[i] \n    \n    linear78=res78.intercept_[0]\n    for lin in range (0,len(df78_col)-1):\n            # calculate linear alpha+beta*X\n            linear78=linear78+res78.coef_[0,lin]*df78[df78_col[lin]].iloc[i]\n    df['Pgt8'].iloc[i]=math.exp(linear78)/(1+math.exp(linear78))\n    if (df['Pgt8'].iloc[i]>=0.5):\n        df['observed'].iloc[i]+=1\n#    df['P7'].iloc[i]=df['Pgt7'].iloc[i]-df['Pgt8'].iloc[i]   \n    \n#    df['P8'].iloc[i]=df['Pgt8'].iloc[i]\n    \n#    df['Score'].iloc[i]=3*df['P3'].iloc[i]+4*df['P4'].iloc[i]+5*df['P5'].iloc[i]+6*df['P6'].iloc[i]+7*df['P7'].iloc[i]+8*df['P8'].iloc[i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range (3,9):\n    print((df.quality == i).sum())\n    print((df.observed==i).sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### There still is a model mismatch, due to the reasons written above"},{"metadata":{},"cell_type":"markdown","source":"### References\n1.\t\"A Simple Approach to Ordinal Classification\", Eibe Frank and Mark Hall, European Conference on Machine Learning, ECML 2001: Machine Learning: ECML 2001 pp 145-156\n2.\t\"Tests for goodness of fit in ordinal logistic regression models\", Morten W. Fagerland & David W. Hosmer (2016), Journal of Statistical Computation and Simulation, DOI: 10.1080/00949655.2016.1156682\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}