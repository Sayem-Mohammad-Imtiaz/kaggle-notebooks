{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 用卷积神经网络 CNN 识别饮料数据集"},{"metadata":{},"cell_type":"markdown","source":"### 饮料数据集加载"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# 图片输入输出\nfrom skimage import io\n\n# 系统命令\nimport os\n\nfrom skimage.transform import resize # resize函数可以调整图像的大小\n\n# file_path字符串存放数据文件夹的地址\ntrain_file_path = \"../input/drinks-yygroup/train/\"\ntest_file_path = \"../input/drinks-yygroup/test/\"\n\ntrain_label_dict = {}\ntxt_train = open(\"../input/drinks-yygroup/train.txt\")\nwhile True:\n    line = txt_train.readline()\n    if line == \"\":\n        break\n    train_name, label = line[:-1].split(\"\\t\")\n    #train_label_dict[train_name] = label\n    train_label_dict[train_name] = int(label) - 1\ntxt_train.close()\nprint(\"number of train: \", len(train_label_dict))\n\n\ntest_label_dict = {}\ntxt_test = open(\"../input/drinks-yygroup/test.txt\")\nwhile True:\n    line = txt_test.readline()\n    if line == \"\":\n        break\n    test_name, label = line[:-1].split(\"\\t\")\n    #test_label_dict[test_name] = label\n    test_label_dict[test_name] = int(label) - 1\ntxt_test.close()\nprint(\"number of test: \", len(test_label_dict))\n\ndef plotit(fig1, fig2, label):\n    plt.figure(1)\n    plt.title(\"label = \" + str(label) + \"，shape = \" + str(fig1.shape))\n    plt.imshow(fig1)\n    plt.figure(2)\n    plt.title(\"label = \" + str(label) + \"，shape = \" + str(fig2.shape))\n    plt.imshow(fig2)\n    plt.show()\n\nx_train = []\ny_train = []\nfor file in os.listdir(train_file_path):\n    train = io.imread(os.path.join(train_file_path, file))\n    resized_train = resize(train[:, :, :3], (32, 32))\n    x_train.append(resized_train)\n    train_name = file.split(\".\")[0]\n    y_train.append([train_label_dict[train_name]])\nx_train = np.array(x_train)\ny_train = np.array(y_train)\nprint(\"shape of x_train: \", x_train.shape)\nprint(\"shape of y_train: \", y_train.shape)\n\nx_test = []\ny_test = []\nfor file in os.listdir(test_file_path):\n    test = io.imread(os.path.join(test_file_path, file))\n    resized_test = resize(test[:, :, :3], (32, 32))\n    x_test.append(resized_test)\n    test_name = file.split(\".\")[0]\n    y_test.append([test_label_dict[test_name]])\n#     plotit(test, resized_test, test_label_dict[test_name])\nx_test = np.array(x_test)\ny_test = np.array(y_test)\nprint(\"shape of x_test: \", x_test.shape)\nprint(\"shape of y_test: \", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 导入库 和 数据预处理"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential                               # 序列模型，线性逐层叠加\nfrom keras.layers import Dense, Activation, Flatten, Dropout      # 导入全连接层、激活函数层、二维转一维、Dropout等神经网络常用层\nfrom keras.optimizers import SGD                                  # 导入随机梯度下降优化器\nimport matplotlib.pyplot as plt # 导入matplotlib库\nfrom keras.utils import to_categorical\n\n# 数据预处理\nnum_classes = 6          # 数据一共有6类\ny_train = to_categorical(y_train, num_classes) # 将训练数据的标签独热编码\ny_test = to_categorical(y_test, num_classes)   # 将测试数据的标签独热编码\n\nprint(\"shape of x_train: \", x_train.shape)\nprint(\"shape of y_train: \", y_train.shape)\nprint(\"shape of x_test: \", x_test.shape)\nprint(\"shape of y_test: \", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 模型搭建"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Conv2D, MaxPooling2D  # 从keras导入卷积层和最大池化层\n\nmodel = Sequential()\nmodel.add(Conv2D(8, (3, 3), padding='same',    # 添加卷积层；32：卷积核的个数;（3，3）:卷积核大小；padding='same'：图片卷积后大小不变\n                 input_shape=x_train.shape[1:]))# 第一个卷基层需要告诉它输入图片大小，以方便网络推导后面所需参数\nmodel.add(Activation('relu'))                   # 使用relu作为激活函数\nmodel.add(Conv2D(8, (5, 5)))                   # 添加卷积层\nmodel.add(Activation('relu'))                   # 使用relu作为激活函数\nmodel.add(MaxPooling2D(pool_size=(2, 2)))       # 最大池化层，在2*2的区域中选取最大的数\n\nmodel.add(Flatten())\nmodel.add(Dense(20))\nmodel.add(Activation('relu'))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('softmax'))\n\n# 模型编译\nmodel.compile(loss='categorical_crossentropy',  # 损失函数使用多类交叉熵损失函数\n              optimizer=\"adam\",                 # 优化器采用adam\n              metrics=['accuracy'])             # 用精度作为性能评价指标\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 模型训练"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32               # 每次输入8张图片,前向传播求出损失函数平均值，然后反向传播一次更新梯度\nepochs = 20                  # 保证所有训练数据被输入网络五次\nhistory = model.fit(x_train, y_train,                   # 训练数据\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=1,                          # 越大，训练过程中显示的信息越详细             \n                    validation_data=(x_test, y_test))   # 验证集\nscore = model.evaluate(x_test, y_test, verbose=0)       # 模型评估，返回模型的loss和metric\nprint('Test loss:', score[0])                           # 测试集上模型损失\nprint('Test accuracy:', score[1])                       # 测试集上模型精度","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 图片预测"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ny_pred = model.predict(x_test) # 预测测试集中图片的类别\nfor i in range(x_test.shape[0]):\n    pred = [round(y, 2) for y in y_pred[i]]\n    plt.title(\" prob=\" + str(pred) + \", predict=\" + str(np.argmax(y_pred[i])) + \", label=\" + str(np.argmax(y_test[i])))\n    plt.imshow(x_test[i])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 用卷积神经网络 CNN 识别CIFAR-10数据集"},{"metadata":{},"cell_type":"markdown","source":"### Cifar10数据集导入"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nwith open(\"../input/youthaiimageclassification/cifar10.pkl\", \"rb\") as f:\n    (x_train, y_train), (x_test, y_test) = pickle.load(f)\n\n    \nfrom keras.models import Sequential                               # 序列模型，线性逐层叠加\nfrom keras.layers import Dense, Activation, Flatten, Dropout      # 导入全连接层、激活函数层、二维转一维、Dropout等神经网络常用层\nfrom keras.optimizers import SGD                                  # 导入随机梯度下降优化器\nimport matplotlib.pyplot as plt # 导入matplotlib库\nfrom keras.utils import to_categorical\n\n# 数据预处理\nprint(\"shape of x_train: \", x_train.shape)\nprint(\"shape of y_train: \", y_train.shape)\nprint(\"shape of x_test: \", x_test.shape)\nprint(\"shape of y_test: \", y_test.shape)\nx_train = x_train / 255  # 数据归一化\nx_test = x_test / 255\nnum_classes = 10         # 数据一共有10类\ny_train = to_categorical(y_train, num_classes) # 将训练数据的标签独热编码\ny_test = to_categorical(y_test, num_classes)   # 将测试数据的标签独热编码","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 模型搭建"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Conv2D, MaxPooling2D  # 从keras导入卷积层和最大池化层\n\nmodel = Sequential()\nmodel.add(Conv2D(16, (5, 5), padding='same',    # 添加卷积层；16：卷积核的个数;（5，5）:卷积核大小；padding=’same‘：图片卷积后大小不变\n                 input_shape=x_train.shape[1:]))# 第一个卷基层需要告诉它输入图片大小，以方便网络推导后面所需参数\nmodel.add(Activation('relu'))                   # 使用relu作为激活函数\nmodel.add(Conv2D(32, (5, 5)))                   # 添加卷积层\nmodel.add(Activation('sigmoid'))                # 使用sigmoid作为激活函数\nmodel.add(MaxPooling2D(pool_size=(2, 2)))       # 最大池化层，在2*2的区域中选取最大的数\nmodel.add(Dropout(0.25))                        # 添加dropout层，dropout层在每一个batchsize训练中随机使网络中一些节点失效(0.25的概率)\n\nmodel.add(Conv2D(64, (5, 5), padding='same'))   # 添加卷积层；64：卷积核的个数;（5，5）:卷积核大小；padding=’same‘：图片卷积后大小不变\nmodel.add(Activation('relu'))                   # 使用relu作为激活函数\nmodel.add(MaxPooling2D(pool_size=(2, 2)))       # 最大池化层，在2*2的区域中选取最大的数\nmodel.add(Dropout(0.25))                        # 添加dropout层，dropout层在每一个batchsize训练中随机使网络中一些节点失效(0.25的概率)\n\nmodel.add(Flatten())\nmodel.add(Dense(100))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('softmax'))\n\n# 模型编译\nmodel.compile(loss='categorical_crossentropy',  # 损失函数使用多类交叉熵损失函数\n              optimizer=\"adam\",                 # 优化器采用adam\n              metrics=['accuracy'])             # 用精度作为性能评价指标\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 模型训练"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nbatch_size = 32               # 每次输入32张图片,前向传播求出损失函数平均值，然后反向传播一次更新梯度\nepochs = 5                    # 保证所有训练数据被输入网络五次\nhistory = model.fit(x_train, y_train,                   # 训练数据\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=1,                          # 越大，训练过程中显示的信息越详细             \n                    validation_data=(x_test, y_test))   # 验证集\nscore = model.evaluate(x_test, y_test, verbose=0)       # 模型评估，返回模型的loss和metric\nprint('Test loss:', score[0])                           # 测试集上模型损失\nprint('Test accuracy:', score[1])                       # 测试集上模型精度\n'''","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}