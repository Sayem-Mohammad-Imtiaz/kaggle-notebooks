{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport json\n\nimport spacy\nfrom spacy import displacy\nfrom collections import Counter\nimport en_core_web_lg\n\nnlp = en_core_web_lg.load()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport string\n\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom imblearn.under_sampling import InstanceHardnessThreshold\nfrom sklearn.svm import LinearSVC\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.naive_bayes import MultinomialNB, ComplementNB\nfrom sklearn.svm import LinearSVC\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils.multiclass import unique_labels\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.set()\nfrom IPython.core.pylabtools import figsize\nfigsize(20, 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Just disable some annoying warning\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset\nName:              **20 Newsgroups**\n\nSize:              **18828**\n\nNumber of classes: **20**\nURL: [http://qwone.com/~jason/20Newsgroups/](http://qwone.com/~jason/20Newsgroups/)\n\nThe 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of my knowledge, it was originally collected by Ken Lang, probably for his Newsweeder: Learning to filter netnews paper, though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering."},{"metadata":{},"cell_type":"markdown","source":"# Read and parse dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/newsgroup20bbcnews/news_group20.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset investigation"},{"metadata":{},"cell_type":"markdown","source":"## Samples Count"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Labels distribution\n\nFrom the graph below we see that our dataset is unbalanced. For classes `talk.religion.misc`, `talk.politics.misc`, `alt.atheism` we have a lot fewer samples, so that our classifier could be skewed towards bigger balanced labels.\n\nThese are the options to solve this issue:\n1. Undersampling - deleting samples from bigger classes in order to balance dataset. One of the most important points here is to keep the initial sample distribution inside the class group.\n2. Oversampling - generating new samples inside the minor classes in order to balance dataset. One of the most crucial issues here is that newly generated samples could represent fake-good distribution in the class group, which will not match to its real distribution.\n3. Balancing between under- and over- sampling\n4. Do nothing (don't solve the issue, but it is always an option :) )\n\nAny of these options are possible when features from text are extracted, so it would be discussed later."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.category.value_counts().plot.barh()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset preprocessing and feature extraction"},{"metadata":{},"cell_type":"markdown","source":"## Text preprocessing \n\nIn order to extract features from raw text, we have to somehow preprocess it. Typical simplified text preprocessing workflow:\n\n<img src=\"https://i.ibb.co/Prrpgxg/nlp-preproc.png\" />\n\n1. **Tokenization** - inteligent splitting text into some kind of tokens (sentences, words, etc.)\n2. **Cleaning** - cleaning text from all undesirable symbols or tokens or lines or whatever, it could be for example stop words or punctuation. \n3. **Steaming** - rocess of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language. [refference](https://www.datacamp.com/community/tutorials/stemming-lemmatization-python)\n4. **Lemmatization** - unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called Lemma. A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words. [refference](https://www.datacamp.com/community/tutorials/stemming-lemmatization-python)\n5. **Feature extraction** - vectorization of the ouput stems/lemmas. For example counting for each document or using TF-IDF.\n\nIn our case I have chosen such workflow:\n1. **Tokenization** - using [spaCy](https://spacy.io/) python library\n2. **Cleaning** - after investigating the content of raw emails, I have decided the next steps:\n    1. Delete symbols ```!\"#%&\\'*+,-<=>?[\\\\]^_`{|}~```\n    2. Delete lines, which begins with `From:` or end with `writes:`, autogenerated content by email host.\n    3. Delete `email strings`, `From:`, `Re:`, `Subject:`\n    4. Delete numbers\n3. **Lemmatization** - I have chosen lemmatization(`spaCy`) in favor of steaming(`nltk`), because it has shown ability to generate more robust results. I've used [spaCy](https://spacy.io/) python library. \n4. **Feature extraction** - TF-IDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport string\n\nfrom sklearn.base import TransformerMixin\n\nclass TextPreprocessor(TransformerMixin):\n    def __init__(self, text_attribute):\n        self.text_attribute = text_attribute\n        \n    def transform(self, X, *_):\n        X_copy = X.copy()\n        X_copy[self.text_attribute] = X_copy[self.text_attribute].apply(self._preprocess_text)\n        return X_copy\n    \n    def _preprocess_text(self, text):\n        return self._lemmatize(self._leave_letters_only(self._clean(text)))\n    \n    def _clean(self, text):\n        bad_symbols = '!\"#%&\\'*+,-<=>?[\\\\]^_`{|}~'\n        text_without_symbols = text.translate(str.maketrans('', '', bad_symbols))\n\n        text_without_bad_words = ''\n        for line in text_without_symbols.split('\\n'):\n            if not line.lower().startswith('from:') and not line.lower().endswith('writes:'):\n                text_without_bad_words += line + '\\n'\n\n        clean_text = text_without_bad_words\n        email_regex = r'([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)'\n        regexes_to_remove = [email_regex, r'Subject:', r'Re:']\n        for r in regexes_to_remove:\n            clean_text = re.sub(r, '', clean_text)\n\n        return clean_text\n    \n    def _leave_letters_only(self, text):\n        text_without_punctuation = text.translate(str.maketrans('', '', string.punctuation))\n        return ' '.join(re.findall(\"[a-zA-Z]+\", text_without_punctuation))\n    \n    def _lemmatize(self, text):\n        doc = nlp(text)\n        words = [x.lemma_ for x in [y for y in doc if not y.is_stop and y.pos_ != 'PUNCT' \n                                    and y.pos_ != 'PART' and y.pos_ != 'X']]\n        return ' '.join(words)\n    \n    def fit(self, *_):\n        return self","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_preprocessor = TextPreprocessor(text_attribute='text')\ndf_preprocessed = text_preprocessor.transform(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature extraction\n\nTo train TF-IDF vectorizer we have to split our dataset into `train`/`test` parts, I have chosen typical `70`/`30` ratio."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(df_preprocessed, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To make **TF-IDF** vectorizer efficient, we have to specify rather large vocabulary(`max_features`). It causes very very large dataset dimensionality. Usually this causes RAM issues or computational time issues on the model training step. Therefore, `sklearn.feature_extraction.text.TfidfVectorizer` returns sparse matrix as the output, which is much more memmory efficient and computational time efficient for some classifier models, which are able to deal with sparse matrices. But some of further preprocessing steps, are not able to deal with this, so the next steps will be very memory-consuming. Because of that I have chosen only `10 000` words vocabulary."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_vectorizer = TfidfVectorizer(analyzer = \"word\", max_features=10000)\n\nX_tfidf_train = tfidf_vectorizer.fit_transform(train['text'])\nX_tfidf_test = tfidf_vectorizer.transform(test['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train['category']\ny_test = test['category']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Further preprocessing"},{"metadata":{},"cell_type":"markdown","source":"## Dataset balancing\n\nAs was afore-mentioned this dataset has to be somehow balanced, to get more robust results. In this case I prefer **undersampling** techniques, because it is more secure to delete than to generate, because this dataset is big enough to leave enough training samples.\n\nI have chosen [imbalanced-learn](https://imbalanced-learn.readthedocs.io) python package, which provides many models for balancing purposes.\n\nAs the **undersampling** model `imblearn.under_sampling.InstanceHardnessThreshold` with `sklearn.svm.LinearSVC` estimator was chosen. For more information about this undersampling method follow this [link](https://imbalanced-learn.readthedocs.io/en/stable/under_sampling.html#instance-hardness-threshold)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.calibration import CalibratedClassifierCV\nfrom imblearn.under_sampling import InstanceHardnessThreshold\nfrom sklearn.svm import LinearSVC\n\niht = InstanceHardnessThreshold(random_state=0, n_jobs=11,\n                                 estimator=CalibratedClassifierCV(\n                                     LinearSVC(C=100, penalty='l1', max_iter=500, dual=False)\n                                 ))\nX_resampled, y_resampled = iht.fit_resample(X_tfidf_train, y)\nprint(sorted(Counter(y_resampled).items()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Dataset shape: \", X_resampled.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = X_resampled, y_resampled\nX_test, y_test = X_tfidf_test, y_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Features scaling (standartization)\n\nMost models requires features normalization to `(0, 1)` range in order to converge faster, for the rest models this step is optional. I think a good choice is to do that."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nX_norm = scaler.fit_transform(X.toarray())\nX_test_norm = scaler.transform(X_test.toarray())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Features selection\n\nWe will select features based on the model `sklearn.svm.LinearSVC`, `sklearn.feature_selection.SelectFromModel` is able to select representative features based on coeficients or feature importances of the trained model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\n\nlsvc = LinearSVC(C=100, penalty='l1', max_iter=500, dual=False)\nlsvc.fit(X_norm, y)\nfs = SelectFromModel(lsvc, prefit=True)\nX_selected = fs.transform(X_norm)\nX_test_selected = fs.transform(X_test_norm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Markdown, display\n\ndef show_top10_features(classifier, feature_names, categories):\n    for i, category in enumerate(categories):\n        top10 = np.argsort(classifier.coef_[i])[-10:]\n        display(Markdown(\"**%s**: %s\" % (category, \", \".join(feature_names[top10]))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = np.array(tfidf_vectorizer.get_feature_names())\nshow_top10_features(lsvc, feature_names, lsvc.classes_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This output shows classes and most important features for them. Most of them makes good sence with the corresponding category. Sometimes there are some irrelevantes like `rec.motorcycles: dog`."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"New dataset shape: \", X_selected.shape)\nprint(\"Features reducted: \", X_norm.shape[1] - X_selected.shape[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that we have reducted features space more than twice."},{"metadata":{},"cell_type":"markdown","source":"# Model training and evaluation\n\nI've tried more than 10 different model instances, with a great batch of different hyperparameters. The experimets were held using `sklearn.model_selection.GridSearchCV` with cross validation size 5. These experiments run more than 2 days non stop on my PC, and here I want to show condensed results of 5 models, their evaluation and result selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.naive_bayes import MultinomialNB, ComplementNB\nfrom sklearn.svm import LinearSVC\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this snippet was taken from https://gist.github.com/shaypal5/94c53d765083101efc0240d776a23823\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils.multiclass import unique_labels\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef print_confusion_matrix(confusion_matrix, \n                           class_names, \n                           figsize = (15,15), \n                           fontsize=12,\n                           ylabel='True label',\n                           xlabel='Predicted label'):\n    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n    \n    Arguments\n    ---------\n    confusion_matrix: numpy.ndarray\n        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n        Similarly constructed ndarrays can also be used.\n    class_names: list\n        An ordered list of class names, in the order they index the given confusion matrix.\n    figsize: tuple\n        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n        the second determining the vertical size. Defaults to (10,7).\n    fontsize: int\n        Font size for axes labels. Defaults to 14.\n        \n    Returns\n    -------\n    matplotlib.figure.Figure\n        The resulting confusion matrix figure\n    \"\"\"\n    df_cm = pd.DataFrame(\n        confusion_matrix, index=class_names, columns=class_names, \n    )\n    fig = plt.figure(figsize=figsize)\n    try:\n        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n    except ValueError:\n        raise ValueError(\"Confusion matrix values must be integers.\")\n    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n    plt.ylabel(ylabel)\n    plt.xlabel(xlabel)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate_model(model, X, y, X_test, y_test, target_names=None):\n    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n    scores_test = cross_val_score(model, X_test, y_test, cv=5, scoring='accuracy')\n    \n    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n    print(\"Accuracy test: %0.2f (+/- %0.2f)\" % (scores_test.mean(), scores_test.std()))\n    \n    print(\"Test classification report: \")\n    if target_names is None:\n        target_names = model.classes_\n    print(classification_report(y_test, model.predict(X_test), target_names=target_names))\n    print(\"Test confusion matrix: \")\n    print_confusion_matrix(confusion_matrix(y_test, model.predict(X_test)), class_names=target_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Multinominal Naive Bayes\n\nFor `MultinomialNB` we see that the most misclassified groups are `alt.atheism`, `talk.politics.misc` and `talk.religion.misc`. A lot of `talk.religion.misc` were classified into very close group `soc.religion.christian`. And probably in this dataset contains mostly republican mails, if so many `talk.politics.misc` were classified in `talk.politics.guns`."},{"metadata":{"trusted":true},"cell_type":"code","source":"mb = MultinomialNB()\nmb.fit(X_selected, y_resampled)\nevaluate_model(mb, X_selected, y, X_test_selected, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Complement Naive Bayes\n\nFor `ComplementNB` we see that the overall accuracy is better than in previous case, but `alt.atheism`, `talk.politics.misc` and `talk.religion.misc` are misclassified even worse."},{"metadata":{"trusted":true},"cell_type":"code","source":"cb = ComplementNB()\ncb.fit(X_selected, y_resampled)\nevaluate_model(cb, X_selected, y, X_test_selected, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic regression\nFor `LogisticRegression` `alt.atheism` is classified much more better, `talk.politics.misc` and `talk.religion.misc` are still misclassified, but looks better than the previous cases. This classifier but has some problems with hardware of different companies."},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(C=10000, penalty='l1', multi_class='ovr')\nlr.fit(X_selected, y)\nevaluate_model(lr, X_selected, y, X_test_selected, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear SVC\n`LinearSVC` looks a lot like `LogisticRegression`, but has more misclassifications in different groups and lover overall test accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"lsvc = LinearSVC(C=1000, penalty='l1', max_iter=500, dual=False)\nlsvc.fit(X_selected, y)\nevaluate_model(lsvc, X_selected, y, X_test_selected, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SGD\n`SGDClassifier` one of the most robust concerning `alt.atheism`, `talk.politics.misc` and `talk.religion.misc` groups, with good overall test accuracy and other metrics."},{"metadata":{"trusted":true},"cell_type":"code","source":"sgd = SGDClassifier(alpha=.0001, max_iter=50, loss='log',\n                                       penalty=\"elasticnet\", n_jobs=-1)\nsgd.fit(X_selected, y)\nevaluate_model(sgd, X_selected, y, X_test_selected, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we sum up these results we can see, that `LogisticRegression` and `SGDClassifier` gives us the best results, but sometimes in come cases `MultinominalNB` shows a little bit beter performance. Why not to combine them into soft voting ensemble, that will balance these results. \n\nIn contrast to majority voting (hard voting), soft voting returns the class label as argmax of the sum of predicted probabilities. [Full description of soft voting](https://scikit-learn.org/stable/modules/ensemble.html#weighted-average-probabilities-soft-voting)"},{"metadata":{"trusted":true},"cell_type":"code","source":"vclf_sgd = VotingClassifier(estimators=[\n         ('lr', LogisticRegression(C=10000, penalty='l1', multi_class='ovr')),\n        ('mb', MultinomialNB()),\n        ('sgd', SGDClassifier(alpha=.0001, max_iter=50, loss='log',\n                                       penalty=\"elasticnet\"))\n], voting='soft', n_jobs=-1)\nvclf_sgd.fit(X_selected, y)\nevaluate_model(vclf_sgd, X_selected, y, X_test_selected, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Comparison\n\nNow we have to compare all these classifiers and select the best match for this task. I'll do it in a way of difference of the overall accuracy, and accuracy on the problematic groups."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"clfs = (('ComplementNB', cb), \n        ('MultinomialNB', mb),\n        ('LinearSVC', lsvc),\n        ('LogisticRegression', lr),\n        ('SGDClassifier', sgd),\n        ('VotingClassifier', vclf_sgd))\n\nfor _x, _y in ((0,0), (4,4), (18,18), (19,19)):\n    mtx = np.zeros((len(clfs), (len(clfs))), dtype=int)\n\n    for i, (label1, clf1) in enumerate(clfs):\n        for j, (label2, clf2) in enumerate(clfs):\n            mtx[i][j] = (confusion_matrix(y_test, clf1.predict(X_test_selected))\n                            -confusion_matrix(y_test, clf2.predict(X_test_selected)))[_x][_y]\n    display(Markdown(f\"Differect of correctly classified: **{clf2.classes_[_x]}**\"))\n    print_confusion_matrix(mtx, class_names=[l for l, _ in clfs], xlabel=\"\", ylabel=\"\", figsize=(5,5))\n    plt.show()\n\nmtx = np.zeros((len(clfs), (len(clfs))), dtype=int)\nfor i, (label1, clf1) in enumerate(clfs):\n    for j, (label2, clf2) in enumerate(clfs):\n        mtx[i][j] = (confusion_matrix(y_test, clf1.predict(X_test_selected))\n                        -confusion_matrix(y_test, clf2.predict(X_test_selected))).diagonal().sum()\ndisplay(Markdown(f\"**Overall** correctly classified\"))\nprint_confusion_matrix(mtx, class_names=[l for l, _ in clfs], xlabel=\"\", ylabel=\"\", figsize=(5,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can create a reusable pipline, which we can use in production or even on other datset"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport string\n\nfrom sklearn.base import TransformerMixin, BaseEstimator\n\nclass TextPreprocessor(BaseEstimator, TransformerMixin):\n    def __init__(self, text_attribute):\n        self.text_attribute = text_attribute\n    \n    def fit(self, X, y=None):\n        return self\n        \n    def transform(self, X, *_):\n        X_copy = X.copy()\n        return X_copy[self.text_attribute].apply(self._preprocess_text)\n    \n    def _preprocess_text(self, text):\n        return self._lemmatize(self._leave_letters_only(self._clean(text)))\n    \n    def _clean(self, text):\n        bad_symbols = '!\"#%&\\'*+,-<=>?[\\\\]^_`{|}~'\n        text_without_symbols = text.translate(str.maketrans('', '', bad_symbols))\n\n        text_without_bad_words = ''\n        for line in text_without_symbols.split('\\n'):\n            if not line.lower().startswith('from:') and not line.lower().endswith('writes:'):\n                text_without_bad_words += line + '\\n'\n\n        clean_text = text_without_bad_words\n        email_regex = r'([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)'\n        regexes_to_remove = [email_regex, r'Subject:', r'Re:']\n        for r in regexes_to_remove:\n            clean_text = re.sub(r, '', clean_text)\n\n        return clean_text\n    \n    def _leave_letters_only(self, text):\n        text_without_punctuation = text.translate(str.maketrans('', '', string.punctuation))\n        return ' '.join(re.findall(\"[a-zA-Z]+\", text_without_punctuation))\n    \n    def _lemmatize(self, text):\n        doc = nlp(text)\n        words = [x.lemma_ for x in [y for y in doc if not y.is_stop and y.pos_ != 'PUNCT' \n                                    and y.pos_ != 'PART' and y.pos_ != 'X']]\n        return ' '.join(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DenseTransformer(TransformerMixin):\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def transform(self, X, y=None, **fit_params):\n        return X.todense()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.pipeline import Pipeline\n\ntext_classification_pipeline = Pipeline([\n    ('text_preprocessor', TextPreprocessor(text_attribute='text')),\n    ('vectorizer', TfidfVectorizer(analyzer = \"word\", max_features=10000)),\n    ('balancer', InstanceHardnessThreshold(n_jobs=-1,\n                                 estimator=CalibratedClassifierCV(\n                                     LinearSVC(C=100, penalty='l1', max_iter=500, dual=False)\n                                 ))),\n    ('todense_converter', DenseTransformer()),\n    ('scaler', MinMaxScaler()),\n    ('feature_selector', SelectFromModel(LinearSVC(C=100, penalty='l1', max_iter=500, dual=False), prefit=False)),\n    ('classifier', VotingClassifier(estimators=[\n                         ('lr', LogisticRegression(C=10000, penalty='l1', multi_class='ovr')),\n                         ('mb', MultinomialNB()),\n                         ('sgd', SGDClassifier(alpha=.0001, max_iter=50, loss='log', penalty=\"elasticnet\"))\n                    ], voting='soft', n_jobs=-1))\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's test our pipeline only over parent categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"parent_cats = (\n    ('comp', ('comp.graphics',\n            'comp.os.ms-windows.misc',\n            'comp.sys.ibm.pc.hardware',\n            'comp.sys.mac.hardware',\n            'comp.windows.x')),\n    ('foresale', ('misc.forsale',)),\n    ('rec', ('rec.autos',\n            'rec.motorcycles',\n            'rec.sport.baseball',\n            'rec.sport.hockey')),\n    ('talk', ('talk.politics.misc',\n            'talk.politics.guns',\n            'talk.politics.mideast')),\n    ('sci', ('sci.crypt',\n            'sci.electronics',\n            'sci.med',\n            'sci.space')),\n    ('religion', ('talk.religion.misc',\n                'alt.atheism',\n                'soc.religion.christian'))\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_parent = df\n\nfor new_name, lst in parent_cats:\n    df_parent.loc[df_parent['category'].isin(lst), 'category'] = new_name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = train_test_split(df_parent, test_size=0.3)\n\nX_p = train.drop(columns=['category'])\ny_p = train['category']\n\nX_p_test = test.drop(columns=['category'])\ny_p_test = test['category']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npipeline = text_classification_pipeline\npipeline.fit(X_p, y_p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ny_pred = pipeline.predict(X_p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ny_test_pred = pipeline.predict(X_p_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_p, y_pred, target_names=pipeline.classes_))\nprint_confusion_matrix(confusion_matrix(y_p, y_pred), class_names=pipeline.classes_, figsize=(5,5), fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_p_test, y_test_pred, target_names=pipeline.classes_))\nprint_confusion_matrix(confusion_matrix(y_p_test, y_test_pred), class_names=pipeline.classes_, figsize=(5,5), fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that foresale is terribly misclassified, even though in our pipline we made class balancing. Of course every specific task, requires paramters tuning. I could be continuation of this project."},{"metadata":{},"cell_type":"markdown","source":"Let's try our pipeline on a completely different dataset I have found on web [link](https://www.kaggle.com/yufengdev/bbc-fulltext-and-category/downloads/bbc-text.csv/2)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_bbc = pd.read_csv('../input/newsgroup20bbcnews/bbc-text.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_bbc.category.value_counts().plot.barh()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = train_test_split(df_bbc, test_size=0.3)\n\nX_bbc = train.drop(columns=['category'])\ny_bbc = train['category']\n\nX_bbc_test = test.drop(columns=['category'])\ny_bbc_test = test['category']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npipeline = text_classification_pipeline\npipeline.fit(X_bbc, y_bbc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ny_pred = pipeline.predict(X_bbc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ny_test_pred = pipeline.predict(X_bbc_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_bbc, y_pred, target_names=pipeline.classes_))\nprint_confusion_matrix(confusion_matrix(y_bbc, y_pred), class_names=pipeline.classes_, figsize=(5,5), fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_bbc_test, y_test_pred, target_names=pipeline.classes_))\nprint_confusion_matrix(confusion_matrix(y_bbc_test, y_test_pred), class_names=pipeline.classes_, figsize=(5,5), fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We beat the authors solution! So this pipeline is reusable, but of course every concrete task needs tuning."},{"metadata":{},"cell_type":"markdown","source":"# Hardware benchmark\nMy hardware:\n\nProcessor: Intel® Core™ i7-8750H CPU @ 2.20GHz × 12\n\nRAM: 16 GB\n\n### News Group dataset:\n\nRAM consuming(max): 10 GB\n\nFit time: 12m 30s (17.572 samples/s)\n\nPredict time: 4m 30s (20.9 samples/s)\n\nThe most time consuming part is text processing, which can be somehow parallelized and optimized, but I'll leave for the future work. For now the model is production ready, but the speed is not, the algorithm needs to be optimized.\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}