{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Estonia | Person: survived or not ?  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://www.dw.com/image/49657763_304.jpg)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Hey guys, We all know about the Estonia Disaster. The Estonia disaster occurred on Wednesday, 28 September 1994, between about 00:55 and 01:50 (UTC+2). Today we are gonna to discuss about this disaster not the history of that but we are going to discuss on how many persons were alive in this disaster and how many are not. As come this dataset, there are six columns in that :\n* PassengerID\n* Country\n* FirstName\n* LastName\n* Sex\n* Age\n* Category\n* Survived\n\nBy the name of the columns we can easily understand what is the content and type of column. Supoose there is a Country column in that different countries are there from which countries people belonged. Hope you guys have an idea of that what we gonna to do it in that notebook. Right ?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"As we need to import some packages. And these packages are very common in use. ","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/passenger-list-for-the-estonia-ferry-disaster/estonia-passenger-list.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like our dataset first and then we will decide whether there is a need of preprocessing or not.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Guys, Info part of data tells clearly about the dataset because it tells about the index, name of column, how many nulls values it have, and the most important part of the columns are its data type. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are 989 values or rows and 8 columns. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By this dataset, there are 16 countries affected by this disaster and the name of these countries are as following :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.Country.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.Sex.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the category column of dataset there are two classes i.e P and C. It means\n* P - Passenger\n* C - Crew","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.Category.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So there are five categorical variables our eight. And the name of these :\n* Country\n* Firstname\n* Lastname\n* Age\n* Sex\n* Category\n\n\nCome to making model of predicting a person is survived or not it is clear Firstname and Lastname has no effect on the predictive column.\n\nSo we drop these columns by using drop function. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data =  data.drop(['Firstname', 'Lastname'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now our dataset looks like :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So guys we have to do some preprocessing in it because there are still some categorical variables in this. \n\nSo let's start.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"label = LabelEncoder()\ndata[\"Country\"] = label.fit_transform(data[\"Country\"])\ndata[\"Sex\"] = label.fit_transform(data[\"Sex\"])\ndata[\"Category\"] = label.fit_transform(data[\"Category\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So now we have gone through the preprocessing part now our dataset is looks like :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So here is the quick look at the dataset and this is just a hist graph but by heatmap we clearly understand which feature has more importances.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.hist(figsize = (10, 10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Heatmap\")\nsns.heatmap(data.corr(), center = 0,  annot = True, linewidth = .5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have to divide the dataset into its features part and the labelling part. I divide it into X which has all features and y which has labels of the features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.iloc[:, 0:5].values\ny = data.iloc[:, 5].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By this plot, we can see that most of the people are dead in this disaster and very few people are alive.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data[\"Survived\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So these are the shape of X and y which explain how many rows we are going to insert in the dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(\"X array has Shape\", X.shape)\nprint(\"Y array has shape\", y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For doing modelling part, we have to do import some essential libraries. In this we are going to use the Random Forest Classifier and find its accuracy and F1 score.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom datetime import datetime\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import f1_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Splittion of Dataset into TRAIN and TEST**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Guys, we are not going to use the library and find its accuarcy because it is very common method we all are used it. In this first I tuned some parameters and then by the tuned classifier we are going to find the accuarcy of that model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Here is the **List of Parameters**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'n_estimators' : [60, 70, 100, 110, 120],\n    'criterion'    : ['gini', 'entropy'],\n    'max_depth'    : [1, 2, 3, 4],\n    'min_samples_split' : [0.1, 0.7, 0.9, 1, 2],\n    'min_samples_leaf' : [0.1, 0.6, 0.9, 1, 2],\n    'min_weight_fraction_leaf' : [0.0, 0.1, 0.2, 0.3, 0.4],\n    'max_features' : ['auto', 'sqrt', 'log2'],\n    'max_leaf_nodes' : [1, 2, 3, 4, 5],\n    'min_impurity_decrease' : [0.1, 0.0, 0.2, 0.3],\n    'bootstrap' : [True, False],\n    'oob_score' : [True, False],\n    'verbose' : [0, 1, 2, 3],\n    'warm_start' : [True, False],\n    'class_weight' : ['balanced', 'balanced_subsample'],\n    'max_samples' : [1, 2, 3, 4, 5]\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By the function **timer** we can see the starting time and end time of tuning parameters.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = RandomForestClassifier()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Use the RandomSearchCV for tuning the classifier**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"random_search=RandomizedSearchCV(classifier,\n                                 param_distributions=params,\n                                 n_iter=10,\n                                 scoring='roc_auc',\n                                 n_jobs=10,\n                                 cv=7,verbose=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we go. Tuning is in progress.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = timer(None) \nrandom_search.fit(X_train,y_train)\ntimer(start_time) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Best Tuned Parameters** :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"random_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just call the classifier and find the accuracy of that.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = RandomForestClassifier(class_weight='balanced_subsample', max_depth=2,\n                       max_leaf_nodes=4, max_samples=2,\n                       min_impurity_decrease=0.3, min_samples_leaf=2,\n                       min_weight_fraction_leaf=0.3, n_estimators=70,\n                       verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score=cross_val_score(classifier , X_train, y_train ,cv=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We find the accuracy by **Cross Val Score Method** :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OH ! we finally find it. It is not bad but good.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have to find the F1 score so we have to find the predictions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score(y_test, y_pred, average='weighted')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**NOT GOOD BUT STILL OKAY**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We have done with RandomForestClassifier and we are looking forward to DecisionTreeClassifier.\nHere is the parameters of Decision Tree Classifier we are going to use random search cv.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"params_decision_tree = {\n    'criterion' : ['gini', 'entropy'],\n    'splitter'  : ['best', 'random'],\n    'max_features' : [0, 1, 2, 3],\n    'min_samples_split' : [2, 3, 4 ,5],\n    'min_samples_leaf' : [1, 2, 3, 4],\n    'random_state' : [0]\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And the process is same as Random Forest Classifier.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier_decision = DecisionTreeClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_search_dt=RandomizedSearchCV(classifier_decision,\n                                 param_distributions=params_decision_tree,\n                                 n_iter=10,\n                                 scoring='roc_auc',\n                                 n_jobs=10,\n                                 cv=7,verbose=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = timer(None) \nrandom_search_dt.fit(X_train,y_train)\ntimer(start_time) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_search_dt.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_search_dt.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier_decision = DecisionTreeClassifier(criterion='entropy', max_features=2, min_samples_leaf=4,\n                       min_samples_split=4, random_state=0, splitter='random')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have done with the decision tree classifier and we find the accuracy scores and then f1 scores.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"score=cross_val_score(classifier_decision , X_train, y_train ,cv=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Oops, 1% loss of accuracy in Decision Tree Classifier. So by looking at accuracy only Random Forest is best than Decision Tree Classifier. But we have to look at F1 scores of Decision Tree.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier_decision.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_dt = classifier_decision.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_dt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score(y_test, y_pred_dt, average='weighted')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OH ! F1 score is high. As we all know if a model have highest F1 scores then it will be supposed to be a good model. So as come to the conclusion part, Decision Tree Classifier is suitable for this dataset than the Ranodm Forest Classifier.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Hope you like this notebook very much and upvote if you like this please. Till then **Enjoy Machine Learning**","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}