{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import print_function\nimport sys\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.layers import Dense, Input, GlobalMaxPooling1D\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding\nfrom keras.models import Model\nfrom keras.initializers import Constant","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"BASE_DIR = ''\nGLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\nTEXT_DATA_DIR = os.path.join(BASE_DIR, '20_newsgroup')\nMAX_SEQUENCE_LENGTH = 1000\nMAX_NUM_WORDS = 20000\nEMBEDDING_DIM = 100\nVALIDATION_SPLIT = 0.2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Indexing word vectors.')\nembeddings_index = {}\nwith open(os.path.join(GLOVE_DIR, '/kaggle/input/glove6b/glove.6B.100d.txt')) as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, 'f', sep=' ')\n        embeddings_index[word] = coefs\nprint('Found %s word vectors.' % len(embeddings_index))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"texts = []\nlabels_index = {}\nlabels = []\nfor name in sorted(os.listdir('/kaggle/input/20-newsgroup-original/20_newsgroup/')):\n    path = os.path.join('/kaggle/input/20-newsgroup-original/20_newsgroup/', name)\n    if os.path.isdir(path):\n        label_id = len(labels_index)\n        labels_index[name] = label_id\n        for fname in sorted(os.listdir(path)):\n            if fname.isdigit():\n                fpath = os.path.join(path, fname)\n                args = {} if sys.version_info < (3,) else {'encoding': 'latin-1'}\n                with open(fpath, **args) as f:\n                    t = f.read()\n                    i = t.find('\\n\\n')\n                    if 0 < i:\n                        t = t[i:]\n                    texts.append(t)\n                labels.append(label_id)\nprint('Found %s texts.' % len(texts))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\ndata = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\nlabels = to_categorical(np.asarray(labels))\nprint('Shape of data tensor:', data.shape)\nprint('Shape of label tensor:', labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]\nnum_validation_samples = int(VALIDATION_SPLIT * data.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = data[:-num_validation_samples]\ny_train = labels[:-num_validation_samples]\nx_val = data[-num_validation_samples:]\ny_val = labels[-num_validation_samples:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    if i >= MAX_NUM_WORDS:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\nembedding_layer = Embedding(num_words,\n                            EMBEDDING_DIM,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\nx = Conv1D(128, 5, activation='relu')(embedded_sequences)\nx = MaxPooling1D(5)(x)\nx = Conv1D(128, 5, activation='relu')(x)\nx = MaxPooling1D(5)(x)\nx = Conv1D(128, 5, activation='relu')(x)\nx = GlobalMaxPooling1D()(x)\nx = Dense(128, activation='relu')(x)\npreds = Dense(len(labels_index), activation='softmax')(x)\n\nmodel = Model(sequence_input, preds)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x_train, y_train,\n          batch_size=128,\n          epochs=20,\n          validation_data=(x_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}