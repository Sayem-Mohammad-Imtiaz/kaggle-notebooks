{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üéØ Learning Objectives\n\n> - Document Classification\n> - Text Pre-processing\n> - Feature extraction\n> - Vocabulary creation\n\n# üìã Vocabulary & Feature Extraction\n> Given a document, you can represent it as a vector of dimension V, where V corresponds to your vocabulary size. As V gets larger, the vector becomes more sparse. Furthermore, we end up having many more features and end up training lot of parameters. This could result in larger training time, and large prediction time.\n\n# üî® Preprocessing\nWhen preprocessing, you have to perform the following:\n> 1. Eliminate handles and URLs\n> 2. Tokenize the string into words\n> 3. Remove stop words like \"and, is, a, on, etc.\"\n> 4. Stemming - or convert every word to its stem. Like dancer, dancing, danced, becomes 'danc'.\n> 5. Convert all your words to lower case.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer, CountVectorizer\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\ncategories = [\n    'alt.atheism',\n    'talk.religion.misc',\n    'comp.graphics',\n    'sci.space'\n]\n\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import Normalizer\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\nimport tensorflow as tf\n\nfrom tensorflow.keras import layers\nimport tensorflow_datasets as tfds","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train = fetch_20newsgroups(subset='train', \n                                categories=categories, \n                                shuffle=True, random_state=42)\nn_components = 5\nlabels = data_train.target\ntrue_k = np.unique(labels).shape[0]\n\n# Convert to TF-IDF format\nvectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english', use_idf=True)\nX_train = vectorizer.fit_transform(data_train.data)\n\n# Reduce dimensions\nsvd = TruncatedSVD(n_components)\nnormalizer = Normalizer(copy=False)\n# lsa = make_pipeline(svd, normalizer)\n\n# X_train = lsa.fit_transform(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data_train.data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(data_train.target).value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Order of labels in `target_names` can be different from `categories`\ndata_test = fetch_20newsgroups(subset='test', \n                               categories=categories, \n                               shuffle=True, random_state=42)\n\ntarget_names = data_train.target_names\n\n# Split a train set and test set\ny_train, y_test = data_train.target, data_test.target\n\nprint(\"Extracting features from the test data using the same vectorizer\")\nX_test = vectorizer.transform(data_test.data)\n# X_test = lsa.fit_transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ü§ñ Machine Learning","metadata":{}},{"cell_type":"markdown","source":"# ‚úîÔ∏è Logistic Regression","metadata":{}},{"cell_type":"code","source":"lr_clf = LogisticRegression()\nlr_clf.fit(X_train.toarray(), y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_pred = lr_clf.predict(X_train.toarray())\ntrain_score = accuracy_score(y_train, lr_pred) * 100\nprint(f\"Train accuracy score: {train_score:.2f}%\")\n\nlr_pred = lr_clf.predict(X_test.toarray())\ntest_score = accuracy_score(y_test, lr_pred) * 100\nprint(f\"Test accuracy score: {test_score:.2f}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lr_pred = lr_clf.predict(X_test.toarray())\ncm = confusion_matrix(y_test, lr_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                              display_labels=data_train.target_names)\n\n\n# NOTE: Fill all variables here with default values of the plot_confusion_matrix\nfig, ax = plt.subplots(figsize=(10, 10))\ndisp = disp.plot(xticks_rotation='vertical', ax=ax, cmap='summer')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(classification_report(y_test, lr_pred, output_dict=True)).T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ‚úîÔ∏è Naive Bayes","metadata":{}},{"cell_type":"code","source":"nb_clf = GaussianNB()\nnb_clf.fit(X_train.toarray(), y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb_pred = nb_clf.predict(X_train.toarray())\ntrain_score = accuracy_score(y_train, nb_pred) * 100\nprint(f\"Train accuracy score: {train_score:.2f}%\")\n\nnb_pred = nb_clf.predict(X_test.toarray())\ntest_score = accuracy_score(y_test, nb_pred) * 100\nprint(f\"Test accuracy score: {test_score:.2f}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# nb_pred = nb_clf.predict(X_test.toarray())\ncm = confusion_matrix(y_test, nb_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                              display_labels=data_train.target_names)\n\n\n# NOTE: Fill all variables here with default values of the plot_confusion_matrix\nfig, ax = plt.subplots(figsize=(10, 10))\ndisp = disp.plot(xticks_rotation='vertical', ax=ax, cmap='summer')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(classification_report(y_test, nb_pred, output_dict=True)).T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ‚úîÔ∏è Support Vector Machine","metadata":{}},{"cell_type":"code","source":"svm_clf = SVC()\nsvm_clf.fit(X_train.toarray(), y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svm_pred = svm_clf.predict(X_train.toarray())\ntrain_score = accuracy_score(y_train, svm_pred) * 100\nprint(f\"Train accuracy score: {train_score:.2f}%\")\n\nsvm_pred = svm_clf.predict(X_test.toarray())\ntest_score = accuracy_score(y_test, svm_pred) * 100\nprint(f\"Test accuracy score: {test_score:.2f}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svm_pred = svm_clf.predict(X_test.toarray())\ncm = confusion_matrix(y_test, svm_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                              display_labels=data_train.target_names)\n\n\n# NOTE: Fill all variables here with default values of the plot_confusion_matrix\nfig, ax = plt.subplots(figsize=(10, 10))\ndisp = disp.plot(xticks_rotation='vertical', ax=ax, cmap='summer')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(classification_report(y_test, svm_pred, output_dict=True)).T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ‚úîÔ∏è Convolutional Neural Networks - CNNs","metadata":{}},{"cell_type":"code","source":"data_train = fetch_20newsgroups(subset='train', \n                                categories=categories, \n                                shuffle=True, random_state=42)\n\ndata_test = fetch_20newsgroups(subset='test', \n                               categories=categories, \n                               shuffle=True, random_state=42)\nX_train = data_train.data\ny_train = data_train.target\n\nX_test = data_test.data\ny_test = data_test.target\n\nassert(len(X_train) == len(y_train))\nassert(len(X_test) == len(y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenization","metadata":{}},{"cell_type":"code","source":"tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n    X_train, target_vocab_size=2**18\n)\n\ntrain_inputs = [tokenizer.encode(text) for text in X_train]\ntest_inputs = [tokenizer.encode(text) for text in X_test]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Padding","metadata":{}},{"cell_type":"code","source":"MAX_LEN = max([len(sentence) for sentence in data_inputs])\ntrain_inputs = tf.keras.preprocessing.sequence.pad_sequences(train_inputs,\n                                                             value=0,\n                                                             padding=\"post\",\n                                                             maxlen=MAX_LEN)\n\ntest_inputs = tf.keras.preprocessing.sequence.pad_sequences(test_inputs,\n                                                            value=0,\n                                                            padding=\"post\",\n                                                            maxlen=MAX_LEN)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DCNN(tf.keras.Model):\n    \n    def __init__(self, vocab_size, emb_dim=128, nb_filters=50, FFN_units=512, nb_classes=2,\n                 dropout_rate=0.1, training=False, name=\"dcnn\"):\n        super(DCNN, self).__init__(name=name)\n        \n        self.embedding = layers.Embedding(vocab_size, emb_dim)\n        self.bigram = layers.Conv1D(filters=nb_filters, kernel_size=2, padding=\"valid\", activation=\"relu\")\n        self.trigram = layers.Conv1D(filters=nb_filters, kernel_size=3, padding=\"valid\", activation=\"relu\")\n        self.fourgram = layers.Conv1D(filters=nb_filters, kernel_size=4, padding=\"valid\", activation=\"relu\")\n        self.pool = layers.GlobalMaxPool1D() # no training variable so we can\n                                             # use the same layer for each\n                                             # pooling step\n        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n        self.dropout = layers.Dropout(rate=dropout_rate)\n        if nb_classes == 2:\n            self.last_dense = layers.Dense(units=1, activation=\"sigmoid\")\n        else:\n            self.last_dense = layers.Dense(units=nb_classes, activation=\"softmax\")\n    \n    def call(self, inputs, training):\n        x = self.embedding(inputs)\n        x_1 = self.bigram(x)\n        x_1 = self.pool(x_1)\n        x_2 = self.trigram(x)\n        x_2 = self.pool(x_2)\n        x_3 = self.fourgram(x)\n        x_3 = self.pool(x_3)\n        \n        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters)\n        merged = self.dense_1(merged)\n        merged = self.dropout(merged, training)\n        output = self.last_dense(merged)\n        \n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VOCAB_SIZE = tokenizer.vocab_size\n\nEMB_DIM = 200\nNB_FILTERS = 100\nFFN_UNITS = 256\nNB_CLASSES = len(set(y_train))\n\nDROPOUT_RATE = 0.2\n\nBATCH_SIZE = 32\nNB_EPOCHS = 5\n\nDcnn = DCNN(vocab_size=VOCAB_SIZE, emb_dim=EMB_DIM, nb_filters=NB_FILTERS,\n            FFN_units=FFN_UNITS, nb_classes=NB_CLASSES,\n            dropout_rate=DROPOUT_RATE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if NB_CLASSES == 2:\n    Dcnn.compile(loss=\"binary_crossentropy\",\n                 optimizer=\"adam\",\n                 metrics=[\"accuracy\"])\nelse:\n    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\n                 optimizer=\"adam\",\n                 metrics=[\"sparse_categorical_accuracy\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_path = \"./drive/My Drive/projects/CNN_for_NLP/ckpt/\"\n\nckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n\nif ckpt_manager.latest_checkpoint:\n    ckpt.restore(ckpt_manager.latest_checkpoint)\n    print(\"Latest checkpoint restored!!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Dcnn.fit(train_inputs,\n         y_train,\n         batch_size=BATCH_SIZE,\n         epochs=NB_EPOCHS)\nckpt_manager.save()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = Dcnn.evaluate(test_inputs, y_test, batch_size=BATCH_SIZE)\nprint(results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}