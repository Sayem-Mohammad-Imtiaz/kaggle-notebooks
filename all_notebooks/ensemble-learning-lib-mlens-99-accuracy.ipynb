{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Thank You for opening this notebook!!!\n\n## This notebook is for introduction to the MLens library\n In this notebook i have performed feature seleaction, Correlational plot, trained custom models like e.g. svm,Randomforest,etc.\n \n1. Support Vector Classifier(SVC)\n2. Random Forest Classifier\n3. LogisticRegression\n4. DecisionTreeClassifier\n5. GaussianNB\n6. KNeighborsClassifier\n7. AdaBoostClassifier\n8. BaggingClassifier  \n9. ExtraTreesClassifier\n10. Gradient boosting classifer\n    ","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plottting lib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport xgboost\nfrom xgboost import XGBClassifier\n### pre-processing lib\nfrom sklearn.pipeline import Pipeline,make_pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder,StandardScaler\nfrom sklearn. model_selection import train_test_split,GridSearchCV,KFold,cross_val_predict,RandomizedSearchCV\n### classification lib required\nfrom sklearn.ensemble import AdaBoostClassifier,BaggingClassifier,GradientBoostingClassifier,RandomForestClassifier,VotingClassifier,StackingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.decomposition import KernelPCA,PCA\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier,RidgeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.svm import SVC\n## different metrices\nfrom sklearn.metrics import accuracy_score,r2_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## importing data ","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data=pd.read_csv('/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### see variance, S.D. and mean so that we can think about normalization or not ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### co-relation plot for seeing how much data corelated to predicting label","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corelation=data.corr()\nplt.figure(figsize=(14,12))\nsns.heatmap(corelation,annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Simple feature Selection we are going to perform ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corelation['quality'].sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corelation.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_features=['volatile acidity', 'citric acid','sulphates', 'alcohol','quality']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_data=data[selected_features]\npd.plotting.scatter_matrix(feat_data,alpha=0.1,figsize=(10,10))\nplt.title('Scatter Matrix plot of selected features.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\npd.plotting.radviz(feat_data,'quality')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### lets split out data ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### i have commented below lines beacuase i am using feature selection but it eventually giving less accuracy and f1-score hence i consider taking full dataset with all columns and rows","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#labels=feat_data.pop('quality')\n#X_train,X_test,y_train,y_test=train_test_split(feat_data,labels,test_size=0.2,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels=feat_data.pop('quality')\nX_train,X_test,y_train,y_test=train_test_split(data,labels,test_size=0.2,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train,y_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## lets see how much accuracy we get through some normal classifiers","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"lets start with first our fav. classifier which is *svm*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### support vector machine classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"svm=SVC(gamma='scale', probability=True)\nsvm.fit(X_train,y_train)\n\nsvm_pred=svm.predict(X_test)\nprint(\"*\"* 30)\nscore=accuracy_score(y_test,svm_pred)\nprint(\"SVM accuracy is :{}\".format(score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"random_f=RandomForestClassifier(n_estimators=250)\nrandom_f.fit(X_train,y_train)\nrandom_f_pred=random_f.predict(X_test)\nprint(\"*\"* 30)\nscore=accuracy_score(y_test,random_f_pred)\nprint(\"random forest accuracy is :{}\".format(score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"log=LogisticRegression(solver='liblinear')\nlog.fit(X_train,y_train)\npred=log.predict(X_test)\nprint(\"*\"* 30)\nscore=accuracy_score(y_test,pred)\nprint(\"LogisticRegression accuracy is :{}\".format(score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Decision=DecisionTreeClassifier()\nDecision.fit(X_train,y_train)\npred=Decision.predict(X_test)\nprint(\"*\"* 30)\nscore=accuracy_score(y_test,pred)\nprint(\"DecisionTreeClassifier accuracy is :{}\".format(score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Nearest Neighbour classifier ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"guassian=GaussianNB()\nguassian.fit(X_train,y_train)\npred=guassian.predict(X_test)\nprint(\"*\"* 30)\nscore=accuracy_score(y_test,pred)\nprint(\"GaussianNB accuracy is :{}\".format(score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### k-nearest neighbors Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"KNN=KNeighborsClassifier()\nKNN.fit(X_train,y_train)\npred=KNN.predict(X_test)\nprint(\"*\"* 30)\nscore=accuracy_score(y_test,pred)\nprint(\"KNeighborsClassifier accuracy is :{}\".format(score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### AdaBoost Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Ada=AdaBoostClassifier()\nAda.fit(X_train,y_train)\npred=Ada.predict(X_test)\nprint(\"*\"* 30)\nscore=accuracy_score(y_test,pred)\nprint(\"AdaBoostClassifier accuracy is :{}\".format(score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### BaggingClassifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Bagging=BaggingClassifier(n_estimators=300)\nBagging.fit(X_train,y_train)\npred=Bagging.predict(X_test)\nprint(\"*\"* 30)\nscore=accuracy_score(y_test,pred)\nprint(\"BaggingClassifier accuracy is :{}\".format(score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ExtraTrees Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Ex_Tree=ExtraTreesClassifier(n_estimators=300)\nEx_Tree.fit(X_train,y_train)\npred=Ex_Tree.predict(X_test)\nprint(\"*\"* 30)\nscore=accuracy_score(y_test,pred)\nprint(\"ExtraTreesClassifier accuracy is :{}\".format(score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extreme gradient boosting classifier.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"XGB=XGBClassifier()\nXGB.fit(X_train,y_train)\npred=XGB.predict(X_test)\nprint(\"*\"* 30)\nscore=accuracy_score(y_test,pred)\nprint(\"XGBClassifier accuracy is :{}\".format(score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"upto this point we created simple models lets combine them so that they can act as an base model and from that we can train our meta model so that we can get significant amount of result.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Super-learner model ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. Define function in which all the model you want to use\n2. put them into and single list as shown below return it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_models():\n\tmodels = list()\n\tmodels.append(LogisticRegression(solver='liblinear'))\n\tmodels.append(DecisionTreeClassifier())\n\tmodels.append(SVC(gamma='scale', probability=True))\n\tmodels.append(GaussianNB())\n\tmodels.append(KNeighborsClassifier())\n\tmodels.append(AdaBoostClassifier())\n\tmodels.append(BaggingClassifier(n_estimators=10))\n\tmodels.append(RandomForestClassifier(n_estimators=10))\n\tmodels.append(ExtraTreesClassifier(n_estimators=10))\n\tmodels.append(XGBClassifier())\n\treturn models","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### if you have face any error regarding mlens.\n### make sure you have install mlens lib using \n> !pip install mlens\n> ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install mlens","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"import mlens and define super learners.\n\nhere base learner means this model learn something and based on this models output we are going to train our original model. basically its like public poll on the social topics or any topic this is an base idea behind this concept.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import mlens\nfrom mlens.ensemble import SuperLearner\ndef get_super_learner(X):\n\tensemble = SuperLearner(scorer=accuracy_score, folds=10, shuffle=True, sample_size=len(X))\n\t# add base models\n\tmodels = get_models()\n\tensemble.add(models)\n\t# add the meta model\n\tensemble.add_meta(LogisticRegression(solver='lbfgs'))\n\treturn ensemble","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble = get_super_learner(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit the super learner\nensemble.fit(X_train.values,y_train.values)\n# summarize base learners\nprint(ensemble.data)\n\n# make predictions on hold out set\n## here i face error with an pandas dataframe input hence i convert it into numpy array \n##may be mlens lib still not support direct pipeline of pandas dataframe\n## may be they will fix this issue further :)\n\nyhat = ensemble.predict(X_test.values)\n\nprint(\"*\"* 30)\nscore=accuracy_score(y_test,yhat)\nprint(\"Super Learner accuracy is :{}\".format(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\nprint(classification_report(y_test, yhat))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Here i got an result of almost 99% hence we can consider this an significant result**\n* **here is no overfitting happening and we can clearly see that with using super learner techinqe we can obtain significant results.**\n\n### **Note: this notebook goal is not achieve maximum result.. it is for learning and understanding the new concept and its utilization**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## **please...if you have read this notebook upto this point  and if you like make sure to upvote this notebook so that it will give some boost to work hard****","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}