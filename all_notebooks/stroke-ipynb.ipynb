{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Authored by: *Swaviman Kumar*<br>\n*Indraprastha Institute of Information Technology, Delhi* <br>","metadata":{}},{"cell_type":"markdown","source":"1. You need to download ‘Stroke Prediction Dataset’ data using the library Scikit learn; ref is given\nbelow. [5]<br>  <br>\n2. Divide the data randomly in training and testing with a 7:3 ratio 100 times, perform the following\ntasks with training data and test the performance on testing data. Testing data should remain\nunseen for all steps.\na. Apply one of the best-known imputation methods to handle the missing/infinite values\nand state the significance of the used method if required. [5]<br>\nb. Visualize the data in 3-D scatter plot and write the inferences, How the data look like. [5]<br>\nc. Make a boxplot for each feature and highlight the outlier, if any, then remove the outlier,\nagain visualize the data in 3-D scatter plot to show the outlier effect and write the\ninferences. [5]<br>\nd. Normalized the data if required, and write a note for what, why and how you performed\nnormalization.[5]<br>\ne. Balance the data if required; you may increase the sample using upsampling if needed.[5]<br>\nf. Perform at least three clustering methods with varying cluster sizes. Perform any three\nbest-known methods to find out correct cluster numbers for each method; how you\nfinalized this cluster number.[10]<br>\ng. Perform at least three supervised methods for classification, and report at least three\nperformance metrics out of (accuracy, precision, Cohen's kappa, F1-score, MCC,\nsensitivity and specificity) with proper reason. [10]<br>\n\nRef:\n1. https://www.kaggle.com/fedesoriano/stroke-prediction-dataset","metadata":{}},{"cell_type":"code","source":"pip install pyforest","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:02:57.404611Z","iopub.execute_input":"2021-06-15T18:02:57.405079Z","iopub.status.idle":"2021-06-15T18:03:02.601932Z","shell.execute_reply.started":"2021-06-15T18:02:57.405028Z","shell.execute_reply":"2021-06-15T18:03:02.601106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# importing necessary libraries\n\nimport pyforest\nimport sklearn\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.cluster import KMeans\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn import preprocessing\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:02.603187Z","iopub.execute_input":"2021-06-15T18:03:02.603436Z","iopub.status.idle":"2021-06-15T18:03:04.125606Z","shell.execute_reply.started":"2021-06-15T18:03:02.603409Z","shell.execute_reply":"2021-06-15T18:03:04.124396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1. You need to download ‘Stroke Prediction Dataset’ data using the library Scikit learn; ref is given below. [5]","metadata":{}},{"cell_type":"markdown","source":"The same csv file has been downloaded from the given link (https://www.kaggle.com/fedesoriano/stroke-prediction-dataset) <br>\nand it has been hosted on drive to make the code reproducible with no dependency.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"https://drive.google.com/uc?export=download&id=1AaUsEwFoAbHy1m1AFECwYsZmcdcIDPg3\")\ndf.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:04.127159Z","iopub.execute_input":"2021-06-15T18:03:04.127448Z","iopub.status.idle":"2021-06-15T18:03:05.401197Z","shell.execute_reply.started":"2021-06-15T18:03:04.127416Z","shell.execute_reply":"2021-06-15T18:03:05.400404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Basic analysis of the dataframe:","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:05.402953Z","iopub.execute_input":"2021-06-15T18:03:05.403331Z","iopub.status.idle":"2021-06-15T18:03:05.422063Z","shell.execute_reply.started":"2021-06-15T18:03:05.403276Z","shell.execute_reply":"2021-06-15T18:03:05.421063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. We could see most of the columns are of int or float type. A few are in Object type which will have to be ordinal encoded so as to be able to use all features in algorithms.<br>\n2. We can also see there are missing values in column bmi. ","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:05.423657Z","iopub.execute_input":"2021-06-15T18:03:05.42401Z","iopub.status.idle":"2021-06-15T18:03:05.435243Z","shell.execute_reply.started":"2021-06-15T18:03:05.423971Z","shell.execute_reply":"2021-06-15T18:03:05.434438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see only bmi column has 201 missing values.","metadata":{}},{"cell_type":"code","source":"df[\"bmi\"].describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:05.436384Z","iopub.execute_input":"2021-06-15T18:03:05.436626Z","iopub.status.idle":"2021-06-15T18:03:05.449544Z","shell.execute_reply.started":"2021-06-15T18:03:05.436595Z","shell.execute_reply":"2021-06-15T18:03:05.44872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The bmi column has 201 missing values. From the above describe method we see it has a maximum entry value of 97.6 which is an impossible bmi value for any human to have. Clearly it appears to be an outlier. So in order to replace the missing values we will use median bmi instead of mean in future.","metadata":{}},{"cell_type":"markdown","source":"### 2 Divide the data randomly in training and testing with a 7:3 ratio 100 times, perform the following tasks with training data and test the performance on testing data. Testing data should remain unseen for all steps.  <br>\n\n\n#### a. Apply one of the best-known imputation methods to handle the missing/infinite values and state the significance of the used method if required. [5]","metadata":{}},{"cell_type":"markdown","source":"Please note that before we perform a train test split, we have to perform EDA on the entire dataset. In order to achieve that we have performed train test split after the outlier removal and the EDA. Doing away with this would mean presence of anomaly & unexpected data in train or test data.","metadata":{}},{"cell_type":"markdown","source":"#### Treating missing values from the data","metadata":{}},{"cell_type":"code","source":"print(\"missing value count is \"+str(20100/5110)+\" % which is not significant\\nwith the median at \" +str(df['bmi'].median()))","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:05.450827Z","iopub.execute_input":"2021-06-15T18:03:05.451376Z","iopub.status.idle":"2021-06-15T18:03:05.457644Z","shell.execute_reply.started":"2021-06-15T18:03:05.451335Z","shell.execute_reply":"2021-06-15T18:03:05.456748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Significance: \nReplacing missing bmi values with the median, as we have already observed there are outliers present in bmi column and replacing with a mean (which gets influenced by outliers) should be avoided. ","metadata":{}},{"cell_type":"code","source":"df['bmi'] = df['bmi'].fillna(df['bmi'].median())\ndf[\"bmi\"].describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:05.458822Z","iopub.execute_input":"2021-06-15T18:03:05.459059Z","iopub.status.idle":"2021-06-15T18:03:05.473483Z","shell.execute_reply.started":"2021-06-15T18:03:05.459035Z","shell.execute_reply":"2021-06-15T18:03:05.472468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:05.475699Z","iopub.execute_input":"2021-06-15T18:03:05.475949Z","iopub.status.idle":"2021-06-15T18:03:05.485049Z","shell.execute_reply.started":"2021-06-15T18:03:05.475925Z","shell.execute_reply":"2021-06-15T18:03:05.484233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Missing values have been dealt with.","metadata":{}},{"cell_type":"markdown","source":"Plotting the distribution of bmi column to check skewness.","metadata":{}},{"cell_type":"code","source":"sns.FacetGrid(df, size=5) \\\n   .map(sns.distplot, \"bmi\") \\\n   .add_legend();\nsns.FacetGrid(df, hue=\"stroke\", size=5) \\\n   .map(sns.distplot, \"bmi\") \\\n   .add_legend();\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:05.486703Z","iopub.execute_input":"2021-06-15T18:03:05.486986Z","iopub.status.idle":"2021-06-15T18:03:06.457061Z","shell.execute_reply.started":"2021-06-15T18:03:05.486931Z","shell.execute_reply":"2021-06-15T18:03:06.456234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"the bmi column appears very slightly right skewed.","metadata":{}},{"cell_type":"code","source":"# Encoding categorical values using Ordinal Encoder from sklearn\n\nord_enc = OrdinalEncoder()\ndf[\"Gender_code\"] = ord_enc.fit_transform(df[[\"gender\"]])\ndf[\"Ever_married_code\"] = ord_enc.fit_transform(df[[\"ever_married\"]])\ndf[\"Work_Type_code\"] = ord_enc.fit_transform(df[[\"work_type\"]])\ndf[\"Residence_type_code\"] = ord_enc.fit_transform(df[[\"Residence_type\"]])\ndf[\"Smoking_Status_code\"] = ord_enc.fit_transform(df[[\"smoking_status\"]])\n\n\ndf.head(11)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:06.458461Z","iopub.execute_input":"2021-06-15T18:03:06.458825Z","iopub.status.idle":"2021-06-15T18:03:06.501679Z","shell.execute_reply.started":"2021-06-15T18:03:06.458787Z","shell.execute_reply":"2021-06-15T18:03:06.500784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we have dataframe with double the number of columns. Categorical cols have been encoded. We haven't used pd.get_dummies as introducing dummy var creates a very high dimensional sparse dataframe. Decision trees tend to overfit on data with a large number of features. Decision tree would suffer from curse of dimensionality if we introduced dummy vars. Hence we used Ordinal Encoder from sklearn instead.","metadata":{}},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:06.502787Z","iopub.execute_input":"2021-06-15T18:03:06.503039Z","iopub.status.idle":"2021-06-15T18:03:06.507769Z","shell.execute_reply.started":"2021-06-15T18:03:06.503014Z","shell.execute_reply":"2021-06-15T18:03:06.50702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_new = df[['id',\n       'Gender_code', 'age', 'hypertension', 'heart_disease',\n       'Ever_married_code', 'Work_Type_code', 'Residence_type_code', 'avg_glucose_level',\n       'bmi', 'Smoking_Status_code', 'stroke']]\n\ndf_new.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:06.508975Z","iopub.execute_input":"2021-06-15T18:03:06.509215Z","iopub.status.idle":"2021-06-15T18:03:06.530153Z","shell.execute_reply.started":"2021-06-15T18:03:06.509189Z","shell.execute_reply":"2021-06-15T18:03:06.529427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# perfectly numeric dataframe\ndf_new.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:06.531043Z","iopub.execute_input":"2021-06-15T18:03:06.531445Z","iopub.status.idle":"2021-06-15T18:03:06.546563Z","shell.execute_reply.started":"2021-06-15T18:03:06.531416Z","shell.execute_reply":"2021-06-15T18:03:06.545761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. b. Visualize the data in 3-D scatter plot and write the inferences, How the data look like. [5]","metadata":{}},{"cell_type":"markdown","source":"3d Scatter plot:","metadata":{}},{"cell_type":"markdown","source":"#### Age vs BMI vs Avg Glucose for stroke","metadata":{}},{"cell_type":"code","source":"sns.set(style = \"darkgrid\")\n\nfig = plt.figure(figsize=(12, 9))\nax = fig.add_subplot(111, projection = '3d')\n\nx = df_new['age']\ny = df_new['bmi']\nz = df_new['avg_glucose_level']\n\nax.set_xlabel(\"Age\")\nax.set_ylabel(\"BMI\")\nax.set_zlabel(\"Avg Glucose Level\")\n\nfor s in df_new.stroke.unique():\n  ax.scatter(x[df_new.stroke==s],y[df_new.stroke==s],z[df_new.stroke==s],label=s)\n\n#ax.scatter(x, y, z)\nax.legend()\nplt.xticks(np.arange(0, 100, 10))\nplt.yticks(np.arange(0, 200, 10))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:06.547776Z","iopub.execute_input":"2021-06-15T18:03:06.548021Z","iopub.status.idle":"2021-06-15T18:03:07.053733Z","shell.execute_reply.started":"2021-06-15T18:03:06.547995Z","shell.execute_reply":"2021-06-15T18:03:07.052818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Inference:\nIt is evident from the above 3d scatter plot that with rise in BMI index, the average blood glucose level of the individual also goes up & hence those individuals are more vulnerable to strokes.","metadata":{}},{"cell_type":"markdown","source":"### 2. c. Make a boxplot for each feature and highlight the outlier, if any, then remove the outlier, again visualize the data in 3-D scatter plot to show the outlier effect and write the inferences. [5]","metadata":{}},{"cell_type":"markdown","source":"Now let's find out outliers. Looking at the dataframe we can say only two of the columns, \"bmi\" & \"avg_glucose_level\" columns are vulnerable to outliers. Rest of the columns are either categorical values which do not contain any outliers or are numeric columns with discrete entries also without outliers. ","metadata":{}},{"cell_type":"markdown","source":"Boxplots for all the columns before outlier removal:","metadata":{}},{"cell_type":"code","source":"global new_df # declaring it as global so as to be able to \n              #use this var within local scopes of functions in future\nnew_df = df_new.copy() # Making a copy of the original dataframe for ease\n\ndf_0 = new_df[new_df['stroke'] == 0]\ndf_1 = new_df[new_df['stroke'] == 1]\nfig = plt.figure(figsize=(20,20))\n\n#\nfor i,b in enumerate(list(new_df.columns[0:30])):\n    \n    i +=1\n    ax = fig.add_subplot(4,3,i)\n    ax.boxplot([df_0[b], df_1[b]])\n\n    ax.set_title(b)\n\nsns.set_style(\"whitegrid\")\nplt.tight_layout()\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:07.055095Z","iopub.execute_input":"2021-06-15T18:03:07.055482Z","iopub.status.idle":"2021-06-15T18:03:08.737818Z","shell.execute_reply.started":"2021-06-15T18:03:07.055438Z","shell.execute_reply":"2021-06-15T18:03:08.73716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the box plots we are sure that only two of the mentioned columns contain outliers","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(5,5))\n\nplt.boxplot([df_new[\"avg_glucose_level\"], df_new[\"bmi\"]])\n\nsns.set_style(\"whitegrid\")\nplt.tight_layout()\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:08.738707Z","iopub.execute_input":"2021-06-15T18:03:08.739058Z","iopub.status.idle":"2021-06-15T18:03:08.92763Z","shell.execute_reply.started":"2021-06-15T18:03:08.73903Z","shell.execute_reply":"2021-06-15T18:03:08.926981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_new[\"bmi\"].value_counts().sort_index(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:08.928539Z","iopub.execute_input":"2021-06-15T18:03:08.928888Z","iopub.status.idle":"2021-06-15T18:03:08.939161Z","shell.execute_reply.started":"2021-06-15T18:03:08.92886Z","shell.execute_reply":"2021-06-15T18:03:08.938254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Treating Outliers with IQR method:","metadata":{}},{"cell_type":"code","source":"def remove_outlier(df, col):\n  q1 = df[col].quantile(0.25)\n  q3 = df[col].quantile(0.75)\n\n  iqr = q3 - q1\n  lower_bound  = q1 - (1.5  * iqr)\n  upper_bound = q3 + (1.5 * iqr)\n\n  out_df = df.loc[(df[col] > lower_bound) & (df[col] < upper_bound)]\n  return out_df","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:08.940215Z","iopub.execute_input":"2021-06-15T18:03:08.940534Z","iopub.status.idle":"2021-06-15T18:03:08.946076Z","shell.execute_reply.started":"2021-06-15T18:03:08.940507Z","shell.execute_reply":"2021-06-15T18:03:08.945141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1234 = remove_outlier(df_new, \"bmi\")\ndf1234.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:08.947158Z","iopub.execute_input":"2021-06-15T18:03:08.947426Z","iopub.status.idle":"2021-06-15T18:03:08.96809Z","shell.execute_reply.started":"2021-06-15T18:03:08.947401Z","shell.execute_reply":"2021-06-15T18:03:08.967013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We removed the rows where the respective bmi feature contained outliers. Now we end up with 4984 rows in total.","metadata":{}},{"cell_type":"code","source":"df12345 = remove_outlier(df1234, \"avg_glucose_level\")\ndf12345.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:08.969351Z","iopub.execute_input":"2021-06-15T18:03:08.96962Z","iopub.status.idle":"2021-06-15T18:03:08.988467Z","shell.execute_reply.started":"2021-06-15T18:03:08.969594Z","shell.execute_reply":"2021-06-15T18:03:08.987537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We removed the rows where the respective avg_glucose_level feature contained outliers. Now we end up with 4390 rows in total.","metadata":{}},{"cell_type":"code","source":"df12345[\"bmi\"].value_counts().sort_index(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:08.989734Z","iopub.execute_input":"2021-06-15T18:03:08.989986Z","iopub.status.idle":"2021-06-15T18:03:08.997683Z","shell.execute_reply.started":"2021-06-15T18:03:08.989958Z","shell.execute_reply":"2021-06-15T18:03:08.996794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"now it ranges within 11.3 & 46.2 unlike previously between 10.3 & 97.6.","metadata":{}},{"cell_type":"markdown","source":"Make a box plot post outlier removal:","metadata":{}},{"cell_type":"code","source":"df_copy = df12345.copy() # making a copy of the dataframe for easier handling","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:08.998986Z","iopub.execute_input":"2021-06-15T18:03:08.99927Z","iopub.status.idle":"2021-06-15T18:03:09.009284Z","shell.execute_reply.started":"2021-06-15T18:03:08.999243Z","shell.execute_reply":"2021-06-15T18:03:09.008084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndf_0 = df_copy[df_copy['stroke'] == 0]\ndf_1 = df_copy[df_copy['stroke'] == 1]\nfig = plt.figure(figsize=(20,20))\n\n#\nfor i,b in enumerate(list(df_copy.columns[0:30])):\n    \n    i +=1\n    ax = fig.add_subplot(4,3,i)\n    ax.boxplot([df_0[b], df_1[b]])\n\n    ax.set_title(b)\n\nsns.set_style(\"whitegrid\")\nplt.tight_layout()\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:09.010434Z","iopub.execute_input":"2021-06-15T18:03:09.010731Z","iopub.status.idle":"2021-06-15T18:03:10.60618Z","shell.execute_reply.started":"2021-06-15T18:03:09.010697Z","shell.execute_reply":"2021-06-15T18:03:10.605295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clearly the bmi & avg glucose level columns (and the rest of the columns as well) are now free from outliers.","metadata":{}},{"cell_type":"markdown","source":"### make 3d scatter plot after outlier removal:","metadata":{}},{"cell_type":"code","source":"sns.set(style = \"darkgrid\")\n\nfig = plt.figure(figsize=(12, 9))\nax = fig.add_subplot(111, projection = '3d')\n\nx = df12345['age']\ny = df12345['bmi']\nz = df12345['avg_glucose_level']\n\nax.set_xlabel(\"Age\")\nax.set_ylabel(\"BMI\")\nax.set_zlabel(\"Avg Glucose Level\")\n\nfor s in df12345.stroke.unique():\n  ax.scatter(x[df12345.stroke==s],y[df12345.stroke==s],z[df12345.stroke==s],label=s)\n\n#ax.scatter(x, y, z)\nax.legend()\nplt.xticks(np.arange(0, 100, 10))\nplt.yticks(np.arange(0, 200, 10))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:10.607451Z","iopub.execute_input":"2021-06-15T18:03:10.607719Z","iopub.status.idle":"2021-06-15T18:03:11.08182Z","shell.execute_reply.started":"2021-06-15T18:03:10.607685Z","shell.execute_reply":"2021-06-15T18:03:11.080673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Inference:\nWe performed the outlier removal as asked using the Inter Quartile Range method. \nWe also plotted 3D scatter plots before & after outlier removal as shown above.\nBoth the plots appear significantly different. However it is still not very obvious to separate the dots purely based on the three features. We will apply clustering & classification algorithms to understand the pattern.","metadata":{}},{"cell_type":"markdown","source":"### 2. d. Normalized the data if required, and write a note for what, why and how you performed normalization.[5]","metadata":{}},{"cell_type":"markdown","source":"#### Reason for normalization & Significance:\nHere comes the need for normalization. All the columns are not at a common comparable range. This makes the analysis difficult. Moreover the machine learning akgorithms we are about to implement ahead work better with normalized data. Hence we are applying MinMaxScaler method to normalize the dataframe.","metadata":{}},{"cell_type":"code","source":"df12345.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:11.08341Z","iopub.execute_input":"2021-06-15T18:03:11.083804Z","iopub.status.idle":"2021-06-15T18:03:11.102093Z","shell.execute_reply.started":"2021-06-15T18:03:11.08376Z","shell.execute_reply":"2021-06-15T18:03:11.101181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using MinMaxScaler to perform normalization\nscaler = MinMaxScaler() \nscaled_values = scaler.fit_transform(df_copy) \ndf_copy.loc[:,:] = scaled_values\n\nsns.set(rc={'figure.figsize':(16,8)}, font_scale=0.9, style='whitegrid')\ndf_copy.boxplot(widths = 0.9)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:11.106165Z","iopub.execute_input":"2021-06-15T18:03:11.106467Z","iopub.status.idle":"2021-06-15T18:03:11.643721Z","shell.execute_reply.started":"2021-06-15T18:03:11.106438Z","shell.execute_reply":"2021-06-15T18:03:11.642738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. e. Balance the data if required; you may increase the sample using upsampling if needed.[5]","metadata":{}},{"cell_type":"markdown","source":"Check if your dataset is balanced.","metadata":{}},{"cell_type":"code","source":"df12345[\"stroke\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:11.645487Z","iopub.execute_input":"2021-06-15T18:03:11.645744Z","iopub.status.idle":"2021-06-15T18:03:11.653417Z","shell.execute_reply.started":"2021-06-15T18:03:11.645718Z","shell.execute_reply":"2021-06-15T18:03:11.652461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"looks like we have got a very imbalanced dataset with only about 4% who got stroke and the rest 96% who didn't. <br><br>\nWe need to upsample the data so that we can make it balanced. The reason is, if we don't, with a very bad model which predicts not a stroke 100% of the time irrespective of the input data, we will still get a 96% accuracy which is quite high & clearly misleading.","metadata":{}},{"cell_type":"code","source":"from sklearn.utils import resample","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:11.654529Z","iopub.execute_input":"2021-06-15T18:03:11.65496Z","iopub.status.idle":"2021-06-15T18:03:11.660857Z","shell.execute_reply.started":"2021-06-15T18:03:11.654926Z","shell.execute_reply":"2021-06-15T18:03:11.659964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_minority = df12345[df12345['stroke']==1]\ndf_majority = df12345[df12345['stroke']==0]","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:11.661892Z","iopub.execute_input":"2021-06-15T18:03:11.662225Z","iopub.status.idle":"2021-06-15T18:03:11.67362Z","shell.execute_reply.started":"2021-06-15T18:03:11.662186Z","shell.execute_reply":"2021-06-15T18:03:11.672501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min_class = resample(df_minority, \n                             replace=True,     \n                             n_samples=4225,    \n                             random_state=10) \ndf_upsampled = pd.concat([min_class,df_majority])","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:11.674686Z","iopub.execute_input":"2021-06-15T18:03:11.675048Z","iopub.status.idle":"2021-06-15T18:03:11.688512Z","shell.execute_reply.started":"2021-06-15T18:03:11.675008Z","shell.execute_reply":"2021-06-15T18:03:11.687522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_upsampled[\"stroke\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:11.689835Z","iopub.execute_input":"2021-06-15T18:03:11.690211Z","iopub.status.idle":"2021-06-15T18:03:11.698017Z","shell.execute_reply.started":"2021-06-15T18:03:11.690161Z","shell.execute_reply":"2021-06-15T18:03:11.697054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_upsampled.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:11.699134Z","iopub.execute_input":"2021-06-15T18:03:11.699397Z","iopub.status.idle":"2021-06-15T18:03:11.716642Z","shell.execute_reply.started":"2021-06-15T18:03:11.69937Z","shell.execute_reply":"2021-06-15T18:03:11.71571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Imbalance issue has been dealt wth. ","metadata":{}},{"cell_type":"markdown","source":"### Train Test Split : \n\nNow that we have a clean dataframe devoid of any outliers or missing values, we can go ahead an perform train test split as we mentioned earlier.","metadata":{}},{"cell_type":"code","source":"list34 = list(df_upsampled.columns)\nlist34.remove('stroke')\nx = df_upsampled[list34]\ny = df_upsampled[[\"stroke\"]]\n\nX_train, X_test, Y_train, Y_test= train_test_split(x, y, test_size=0.3,random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:11.717659Z","iopub.execute_input":"2021-06-15T18:03:11.717903Z","iopub.status.idle":"2021-06-15T18:03:11.727337Z","shell.execute_reply.started":"2021-06-15T18:03:11.717879Z","shell.execute_reply":"2021-06-15T18:03:11.726539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. f. Perform at least three clustering methods with varying cluster sizes. Perform any three best-known methods to find out correct cluster numbers for each method; how you finalized this cluster number.[10]","metadata":{}},{"cell_type":"markdown","source":"#### Clustering Methos 1:\n\n#### K Means Clustering :\nWe will perform KMeans clustering at first. We will use the elbow method to find the elbow value, i.e. the optimum value of K for which we get the best clustering.<br>We are using varying cluster sizes as asked in the question.","metadata":{}},{"cell_type":"code","source":"n_clusters = [2,3,4,5,6,7,8,9,10] # number of clusters\nclusters_inertia = [] # inertia of clusters\ns_scores = [] # silhouette scores\n\nfor n in n_clusters:\n    KM_est = KMeans(n_clusters=n, init='k-means++').fit(X_train)\n    clusters_inertia.append(KM_est.inertia_)    # data for the elbow method\n    silhouette_avg = silhouette_score(X_train, KM_est.labels_)\n    s_scores.append(silhouette_avg) # data for the silhouette score method","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:11.728721Z","iopub.execute_input":"2021-06-15T18:03:11.729046Z","iopub.status.idle":"2021-06-15T18:03:33.449359Z","shell.execute_reply.started":"2021-06-15T18:03:11.729017Z","shell.execute_reply":"2021-06-15T18:03:33.448351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,5))\nax = sns.lineplot(n_clusters, clusters_inertia, marker='o', ax=ax)\nax.set_title(\"Elbow method\")\nax.set_xlabel(\"number of clusters\")\nax.set_ylabel(\"clusters inertia\")\nax.axvline(3, ls=\"--\", c=\"red\")\nax.axvline(4, ls=\"--\", c=\"red\")\nplt.grid()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:33.45074Z","iopub.execute_input":"2021-06-15T18:03:33.451144Z","iopub.status.idle":"2021-06-15T18:03:33.713657Z","shell.execute_reply.started":"2021-06-15T18:03:33.451101Z","shell.execute_reply":"2021-06-15T18:03:33.712039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this plot we observe that \"elbow\" has to be 3. Though a choice of 3 or 4 clusters seems to be fair. Let's see the silhouette score to be sure of the elbow point.","metadata":{}},{"cell_type":"code","source":"# Plot for Silhouette score to find the optimum K\n\nfig, ax = plt.subplots(figsize=(12,5))\nax = sns.lineplot(n_clusters, s_scores, marker='o', ax=ax)\nax.set_title(\"Silhouette score method\")\nax.set_xlabel(\"number of clusters\")\nax.set_ylabel(\"Silhouette score\")\nax.axvline(3, ls=\"--\", c=\"red\")\nplt.grid()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:33.714848Z","iopub.execute_input":"2021-06-15T18:03:33.71509Z","iopub.status.idle":"2021-06-15T18:03:33.972716Z","shell.execute_reply.started":"2021-06-15T18:03:33.715067Z","shell.execute_reply":"2021-06-15T18:03:33.971748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can say the best option would be 3. Hence we will go with K=3","metadata":{}},{"cell_type":"markdown","source":"#### K Means Clustering with Cluster size = 3 :","metadata":{}},{"cell_type":"code","source":"# To initialize and fit K-Means model\nKM_3_clusters = KMeans(n_clusters=3 , init='k-means++').fit(X_train)\nKM_3_clusters.labels_","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:33.974151Z","iopub.execute_input":"2021-06-15T18:03:33.974555Z","iopub.status.idle":"2021-06-15T18:03:35.213595Z","shell.execute_reply.started":"2021-06-15T18:03:33.974512Z","shell.execute_reply":"2021-06-15T18:03:35.212632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"KM_3_clusters.cluster_centers_","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:35.216549Z","iopub.execute_input":"2021-06-15T18:03:35.216833Z","iopub.status.idle":"2021-06-15T18:03:35.22473Z","shell.execute_reply.started":"2021-06-15T18:03:35.216807Z","shell.execute_reply":"2021-06-15T18:03:35.223672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"KM_3_clusters.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:35.225873Z","iopub.execute_input":"2021-06-15T18:03:35.226129Z","iopub.status.idle":"2021-06-15T18:03:35.247506Z","shell.execute_reply.started":"2021-06-15T18:03:35.226102Z","shell.execute_reply":"2021-06-15T18:03:35.2467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Clustering Methos 2:\n\n#### MiniBatchKMeans Clustering:\nWe are applying MiniBatchKMeans Clustering algorithm here.","metadata":{}},{"cell_type":"code","source":"\nn_clusters = [2,3,4,5,6,7,8,9,10] # number of clusters\nclusters_inertia = [] # inertia of clusters\ns_scores = [] # silhouette scores\n\n\nfrom sklearn.cluster import MiniBatchKMeans\nfor n in n_clusters:\n    KM_est = MiniBatchKMeans(n_clusters=n, init='k-means++').fit(X_train)\n    clusters_inertia.append(KM_est.inertia_)    # data for the elbow method\n    silhouette_avg = silhouette_score(X_train, KM_est.labels_)\n    s_scores.append(silhouette_avg) # data for the silhouette score method","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:35.248612Z","iopub.execute_input":"2021-06-15T18:03:35.248887Z","iopub.status.idle":"2021-06-15T18:03:42.100847Z","shell.execute_reply.started":"2021-06-15T18:03:35.24886Z","shell.execute_reply":"2021-06-15T18:03:42.099744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,5))\nax = sns.lineplot(n_clusters, clusters_inertia, marker='o', ax=ax)\nax.set_title(\"Elbow method\")\nax.set_xlabel(\"number of clusters\")\nax.set_ylabel(\"clusters inertia\")\nax.axvline(3, ls=\"--\", c=\"red\")\nax.axvline(4, ls=\"--\", c=\"red\")\nplt.grid()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:42.102203Z","iopub.execute_input":"2021-06-15T18:03:42.102476Z","iopub.status.idle":"2021-06-15T18:03:42.374433Z","shell.execute_reply.started":"2021-06-15T18:03:42.102448Z","shell.execute_reply":"2021-06-15T18:03:42.373499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot for Silhouette score to find the optimum K\n\nfig, ax = plt.subplots(figsize=(12,5))\nax = sns.lineplot(n_clusters, s_scores, marker='o', ax=ax)\nax.set_title(\"Silhouette score method\")\nax.set_xlabel(\"number of clusters\")\nax.set_ylabel(\"Silhouette score\")\nax.axvline(3, ls=\"--\", c=\"red\")\nplt.grid()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:42.375596Z","iopub.execute_input":"2021-06-15T18:03:42.375896Z","iopub.status.idle":"2021-06-15T18:03:42.63974Z","shell.execute_reply.started":"2021-06-15T18:03:42.375867Z","shell.execute_reply":"2021-06-15T18:03:42.638663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To initialize and fit Mini batch K-Means model\nMKM_3_clusters = MiniBatchKMeans(n_clusters=3 , init='k-means++').fit(X_train)\n\nMKM_3_clusters.labels_","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:42.641253Z","iopub.execute_input":"2021-06-15T18:03:42.641675Z","iopub.status.idle":"2021-06-15T18:03:42.710235Z","shell.execute_reply.started":"2021-06-15T18:03:42.641633Z","shell.execute_reply":"2021-06-15T18:03:42.709235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MKM_3_clusters.cluster_centers_","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:42.711679Z","iopub.execute_input":"2021-06-15T18:03:42.712032Z","iopub.status.idle":"2021-06-15T18:03:42.717285Z","shell.execute_reply.started":"2021-06-15T18:03:42.711993Z","shell.execute_reply":"2021-06-15T18:03:42.716712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MKM_3_clusters.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:42.718234Z","iopub.execute_input":"2021-06-15T18:03:42.718698Z","iopub.status.idle":"2021-06-15T18:03:42.739955Z","shell.execute_reply.started":"2021-06-15T18:03:42.718662Z","shell.execute_reply":"2021-06-15T18:03:42.739114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Clustering Methos 3:\n\n#### Agglomerative Clustering:\nWe are applying Agglomerative Clustering algorithm here.","metadata":{}},{"cell_type":"code","source":"# Importing AgglomerativeClustering from Sklearn\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Running Agglomerative Clustering\nno_of_clusters = []\nn_clusters = range(2, 10) # Range is arbitrarily chosen\nag_sil_score = [] # silouette scores\n\nfor p in n_clusters:\n    AG = AgglomerativeClustering(n_clusters=p).fit(X_train)\n    no_of_clusters.append((len(np.unique(AG.labels_))))\n    ag_sil_score.append(silhouette_score(X_train, AG.labels_))\n    \nresults = pd.DataFrame([n_clusters, no_of_clusters, ag_sil_score], index=['n_clusters','clusters', 'sil_score']).T\nresults.sort_values(by='sil_score', ascending=False).head() # display only 5 best scores","metadata":{"execution":{"iopub.status.busy":"2021-06-15T18:03:42.741206Z","iopub.execute_input":"2021-06-15T18:03:42.741601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For plotting silhoette score\n\nfig, ax = plt.subplots(figsize=(12,5))\nax = sns.lineplot(n_clusters, ag_sil_score, marker='o', ax=ax)\nax.set_title(\"Silhouette score method\")\nax.set_xlabel(\"number of clusters\")\nax.set_ylabel(\"Silhouette score\")\nax.axvline(3, ls=\"--\", c=\"red\")\nplt.grid()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the silhouette score model we observe that 3 is optimum cluster number.","metadata":{}},{"cell_type":"markdown","source":"#### Agglomerative Clustering with Cluster size = 3 :","metadata":{}},{"cell_type":"code","source":"# To initialize and fit agglomerative model\nAG = AgglomerativeClustering(n_clusters=3).fit(X_train)\nAG.labels_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AG.n_leaves_ # Shows Number of leaves in the hierarchical tree.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. g. Perform at least three supervised methods for classification, and report at least three performance metrics out of (accuracy, precision, Cohen's kappa, F1-score, MCC, sensitivity and specificity) with proper reason. [10]","metadata":{}},{"cell_type":"markdown","source":"#### Classification method 1:\n\n#### Naive bayes classification:","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, Y_train)\n\ny_pred  =  classifier.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix,accuracy_score\ncm = confusion_matrix(Y_test, y_pred)\nac = accuracy_score(Y_test,y_pred)\n\nprint(\"Accuracy:\",ac)\nprint(\"Precision:\",metrics.precision_score(Y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(Y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Classification method 2:\n\n#### Logistic Regression:","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(solver='liblinear', random_state=0)\nmodel.fit(X_train, Y_train)\nY_Pred2 = model.predict(X_test)\n\nfrom sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(Y_test, Y_Pred2)\ncnf_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_Pred2))\nprint(\"Precision:\",metrics.precision_score(Y_test, Y_Pred2))\nprint(\"Recall:\",metrics.recall_score(Y_test, Y_Pred2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Classification method 3:\n\n#### K-Nearest neighbour:","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nclassifier.fit(X_train, Y_train)\n\ny_pred2 = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix,accuracy_score\ncm1 = confusion_matrix(Y_test, y_pred2)\nac1 = accuracy_score(Y_test,y_pred2)\n\nprint(\"Accuracy:\",ac1)\nprint(\"Precision:\",metrics.precision_score(Y_test, y_pred2))\nprint(\"Recall:\",metrics.recall_score(Y_test, y_pred2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ml_names = ['Gaussian Naive Bayes', 'Logistic Regression', 'Decision Tree']\n\nacc_all = [acc_gnb, acc_logit, acc_dtree]\nprec_all = [prec_gnb, prec_logit, prec_dtree]\nf1_all = [f1_gnb, f1_logit, f1_dtree]\n\ndef autolabel(bars):\n    \"\"\"Attach a text label above each bar in displaying its height.\"\"\"\n    for bar in bars:\n        height = bar.get_height()\n        ax.annotate('{:.2f}'.format(height),\n                    xy=(bar.get_x() + bar.get_width() / 2, height),\n                    xytext=(0, 5),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    fontsize=12,\n                    rotation=90,\n                    ha='center', va='bottom')\n\nwidth = 0.25  # the width of the bars\nr1 = np.arange(len(ml_names))  # the label locations\nr2 = [x + width for x in r1]\nr3 = [x + width for x in r2]\n\n# plot sensitivity, specificity, and auc\nfig, ax = plt.subplots(figsize=(8,6))\nbar1 = ax.bar(r1, acc_all, width, label='Accuracy')\nbar2 = ax.bar(r2, prec_all, width, label='Precision')\nbar3 = ax.bar(r3, f1_all, width, label='F1')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylim([0,1.13])\nax.set_ylabel('Scores',fontsize=14)\n\n#ax.set_title('Performance benchmark across ML models')\nax.set_xticks(r2)\nax.set_xticklabels(ml_names)\nax.tick_params(axis='both', which='major', labelsize=12)\nax.set_xlabel(\"Machine Learning Model\",fontsize=14)\nax.legend(loc='lower left',ncol=3,bbox_to_anchor=(0.25,1),fontsize=12)\nautolabel(bar1)\nautolabel(bar2)\nautolabel(bar3)\nfig.tight_layout()\nfig.savefig(\"ml_benchmark_f1.pdf\", bbox_inches='tight')\nplt.show()    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}