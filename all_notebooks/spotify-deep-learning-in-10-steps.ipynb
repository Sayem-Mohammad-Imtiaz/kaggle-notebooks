{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Context\n* This is a dataset consisting of features for tracks fetched using Spotify's Web API. \n* The tracks are labeled '1' or '0' ('Hit' or 'Flop') depending on some criteria of the author.\n* This dataset can be used to make a classification model that predicts whether a track would be a 'Hit' or not.\n<br>\n<br>\n<font color = 'blue'>\n1. Content\n    * [Load Libraries](#1)\n    * [Load Dataset](#2)    \n    * [Balance The Dataset](#3)\n    * [Shuffle The Data](#4)\n    * [Standardize The Inputs](#5)\n    * [Split The Dataset into Train,Validation and Test](#6)\n    * [Create The Deep Learning Algorithm](#7)\n    * [Visualize Neural Network Loss History](#8)\n    * [Test The Model](#9)"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"1\"></a><br>\n## Load Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\n\n# Set random seed\nnp.random.seed(0)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"2\"></a><br>\n## Load Data\n* I will use just 70s that has much more datas."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/the-spotify-hit-predictor-dataset/dataset-of-70s.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"3\"></a><br>\n## Balance The Dataset\n* This is a balanced dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":" data.target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"4\"></a><br>\n## Shuffle The Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.sample(frac=1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"5\"></a><br>\n## Standardize The Inputs"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Drop Categorical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop([\"track\",\"artist\",\"uri\"],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unscaled_inputs = data.iloc[:,0:-1]\ntarget = data.iloc[:,[-1]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_inputs = preprocessing.scale(unscaled_inputs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"6\"></a><br>\n## Split The Dataset into Train,Validation and Test\n* 80% , 10% , 10%"},{"metadata":{"trusted":true},"cell_type":"code","source":"samples_count = scaled_inputs.shape[0]\n#\ntrain_samples_count = int(0.8*samples_count)\nvalidation_samples_count = int(0.1*samples_count)\ntest_samples_count = samples_count - train_samples_count - validation_samples_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train:\ntrain_inputs = scaled_inputs[:train_samples_count]\ntrain_targets = target[:train_samples_count]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# validation:\nvalidation_inputs = scaled_inputs[train_samples_count:train_samples_count+validation_samples_count]\nvalidation_targets = target[train_samples_count:train_samples_count+validation_samples_count]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test:\ntest_inputs = scaled_inputs[train_samples_count+validation_samples_count:]\ntest_targets = target[train_samples_count+validation_samples_count:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the number of targets that are 1s, the total number of samples, and the proportion for training, validation, and test.\nprint(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\nprint(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\nprint(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Save the three datasets in *.npz"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the three datasets in *.npz.\n# We will see that it is extremely valuable to name them in such a coherent way!\n\nnp.savez('Spotify_data_train', inputs=train_inputs, targets=train_targets)\nnp.savez('Spotify_data_validation', inputs=validation_inputs, targets=validation_targets)\nnp.savez('Spotify_data_test', inputs=test_inputs, targets=test_targets)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"7\"></a><br>\n## Create The Deep Learning Algorithm"},{"metadata":{},"cell_type":"markdown","source":"### Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"npz = np.load('Spotify_data_train.npz')\n# we extract the inputs using the keyword under which we saved them\n# to ensure that they are all floats, let's also take care of that\ntrain_inputs = npz['inputs'].astype(np.float)\n# targets must be int because of sparse_categorical_crossentropy (we want to be able to smoothly one-hot encode them)\ntrain_targets = npz['targets'].astype(np.int)\n\n# we load the validation data in the temporary variable\nnpz = np.load('Spotify_data_validation.npz')\n# we can load the inputs and the targets in the same line\nvalidation_inputs, validation_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n\n# we load the test data in the temporary variable\nnpz = np.load('Spotify_data_test.npz')\n# we create 2 variables that will contain the test inputs and the test targets\ntest_inputs, test_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the input and output sizes\ninput_size = 15 # count of features\noutput_size = 2 # count of targets\n# Use same hidden layer size for both hidden layers. Not a necessity.\nhidden_layer_size = 50 # counts of neurons\n    \n# define how the model will look like\nmodel = tf.keras.Sequential([\n    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 3nd hidden layer\n    # the final layer is no different, we just make sure to activate it with softmax\n    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Choose the optimizer and the loss function"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we define the optimizer we'd like to use, \n# the loss function, \n# and the metrics we are interested in obtaining at each iteration\n#custom_optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\nmodel.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# That's where we train the model we have built.\n# set the batch size\nbatch_size = 300\n# set a maximum number of training epochs\nmax_epochs = 6\n\n# fit the model\n# note that this time the train, validation and test data are not iterable\nhistory = model.fit(  train_inputs, # train inputs\n                      train_targets, # train targets\n                      batch_size=batch_size, # batch size\n                      epochs=max_epochs, # epochs that we will train for (assuming early stopping doesn't kick in)\n                      # callbacks are functions called by a task when a task is completed\n                      # task here is to check if val_loss is increasing\n                      #callbacks=[early_stopping], # early stopping\n                      validation_data=(validation_inputs, validation_targets), # validation data\n                      verbose = 2 # making sure we get enough information about the training process\n          )  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"8\"></a><br>\n## Visualize Neural Network Loss History"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get training and test loss histories\ntraining_loss = history.history['loss']\nvalidation_loss = history.history['val_loss']\n\n# Create count of the number of epochs\nepoch_count = range(1, len(training_loss) + 1)\n\n# Visualize loss history\nplt.plot(epoch_count, training_loss, 'r--')\nplt.plot(epoch_count, validation_loss, 'b-')\nplt.legend(['Training Loss', 'Validation Loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"9\"></a><br>\n## Test The Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)\nprint('\\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}