{"cells":[{"metadata":{},"cell_type":"markdown","source":"\n# GCN on AMLSim Micro Data x0019th Iteration","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Imports","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# All Imports\nimport os\nimport sys\nimport networkx as nx\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom tqdm import tqdm_notebook\nfrom math import ceil\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading Data from csv files\ntransactions = pd.read_csv(\"/kaggle/input/micro-amlsim-dataset/transactions.csv\")\naccounts = pd.read_csv(\"/kaggle/input/micro-amlsim-dataset/accounts.csv\")\ntransactions = transactions.head(4000)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Data preparation - categorical and scaling\n\ndef processing_dataframes():\n    print(\"Dropping NaN columns\")\n    accounts.dropna(axis=1, how='all', inplace=True)\n    print(\"Renaming columns wrt to Stellar Config\")\n    transactions.columns  = ['tran_id', 'source', 'target', 'tx_type', 'weight',\n       'tran_timestamp', 'is_sar', 'alert_id']\n    # Label encoder performing encoding for all objects\n    print(\"Label encoding categorical features\")\n    le = LabelEncoder()\n    for col in transactions.columns:\n        if transactions[col].dtype == \"O\":\n            transactions[col] = le.fit_transform(transactions[col].astype(str))   \n    le = LabelEncoder()\n    for col in accounts.columns:\n        if accounts[col].dtype == \"O\":\n            accounts[col] = le.fit_transform(accounts[col].astype(str))   \n            \n            \n    scaler = MinMaxScaler()\n    transactions['weight'] = scaler.fit_transform(transactions['weight'].values.reshape(-1,1))\n    print('\\n')\n    print(\"--> Account df done!\")\n    display(accounts.head())\n    print(\"--> Transaction df done!\")\n    display(transactions.head())    \nprocessing_dataframes()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transactions.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Graph Convolutional Networks (Class)","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Contains GCN Code\n\n# Helper functions\nfrom sklearn.metrics import roc_auc_score\ndef one_hot_encode(y):\n    mods = len(np.unique(y))\n    y_enc = np.zeros((y.shape[0], mods))\n    \n    for i in range(y.shape[0]):\n        y_enc[i, y[i]] = 1\n    return y_enc\n\nclass GraphConvolutionNetwork():\n    \n    def __init__(self, node_dim=2, graph_dim=2, nb_classes=2, nmax=15, alpha=0.025):\n        \"\"\"\n        Parameters of the model architecture\n        \n        \"\"\"\n        self.node_dim = node_dim\n        self.graph_dim = graph_dim\n        self.nb_classes = nb_classes\n        self.nmax = nmax\n        self.alpha = alpha\n        \n        self.build_model()\n        \n    def build_model(self):\n        self.adjs = tf.placeholder(tf.float32, shape=[None, self.nmax, self.nmax])\n        self.embeddings = tf.placeholder(tf.float32, shape=[None, self.nmax, self.node_dim])\n        self.targets = tf.placeholder(tf.float32, shape=[None, self.nb_classes])\n        \n        A1 = tf.Variable(tf.random_normal([self.graph_dim, self.node_dim], seed=None))\n        B1 = tf.Variable(tf.random_normal([self.graph_dim, self.node_dim]))\n        W  = tf.Variable(tf.random_normal([self.graph_dim, self.nb_classes]))\n        \n        M1 = tf.einsum('adc,adb->abc', self.embeddings, self.adjs)\n        H1 = tf.nn.relu(tf.tensordot(M1, A1, (2, 1)) + tf.tensordot(self.embeddings, B1, (2, 1)))\n        G1 = tf.reduce_mean(H1, 1)\n        \n        Y_OUT = tf.matmul(G1,W)\n        cost = tf.losses.softmax_cross_entropy(self.targets, Y_OUT)\n        \n        self.predictions = tf.argmax(Y_OUT, 1)\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.alpha)\n        self.train = optimizer.minimize(cost)\n        \n        self.sess = tf.Session()\n        self.sess.run(tf.global_variables_initializer())\n        \n    def fit(self, adj, embds, y, epochs=20, batch_size=10, shuffle=True):\n        self.scores = []\n        y_enc = one_hot_encode(y)\n        minibatches = ceil(len(adj) / batch_size)\n        \n        j = 0\n        for i in range(epochs):\n            INDS = np.array(range(len(adj)))\n            \n            if shuffle:\n                idx = np.random.permutation(y.shape[0]) \n                INDS = INDS[idx]\n                \n            mini = np.array_split(INDS, minibatches)\n            \n            for inds in mini:\n                j+=1\n                sys.stderr.write('\\rEpoch: %d/%d' % (j, epochs*minibatches))\n                sys.stderr.flush()\n                self.sess.run(self.train, feed_dict={self.adjs:adj[inds], self.embeddings:embds[inds], \n                                                self.targets:y_enc[inds]})                \n            self.scores.append(self.score(adj, embds, y))\n            #\n    def predict(self, adj, embds):\n        return self.sess.run(self.predictions, feed_dict={self.adjs:adj, self.embeddings:embds})\n    \n    def score(self, adj, embds,y):\n        y_ = self.predict(adj, embds)\n        return 100*(y==y_).mean()\n\n    def auc_score(self, adj, embds,y):\n        from sklearn.metrics import roc_auc_score\n        y_ = self.predict(adj, embds)\n        return roc_auc_score(y,y_)\n    \nclass MultiLaplacianGCN():\n    \n    def __init__(self, node_dim=2, graph_dim=[3,3], nb_classes=2, nmax=15, alpha=0.025):\n        \"\"\"\n        Parameters of the model architecture\n        \n        \"\"\"\n        self.graph_dims = [node_dim] + graph_dim\n        self.n_layers = len(graph_dim)\n        self.nb_classes = nb_classes\n        self.nmax = nmax\n        self.alpha = alpha\n        \n        self.build_model()\n        \n    def build_model(self):\n        self.adjs = tf.placeholder(tf.float32, shape=[None, self.nmax, self.nmax])\n        self.targets = tf.placeholder(tf.float32, shape=[None, self.nb_classes])\n        \n        self.A = {i+1: tf.Variable(tf.random_normal([self.graph_dims[i+1], self.graph_dims[i]])) \\\n             for i in range(self.n_layers)}\n        self.B = {i+1: tf.Variable(tf.random_normal([self.graph_dims[i+1], self.graph_dims[i]])) \\\n             for i in range(self.n_layers)}\n        self.W  = tf.Variable(tf.random_normal([self.graph_dims[-1], self.nb_classes]))\n        \n        \n        self.M, self.H, self.G = {}, {}, {}\n        \n        self.H[0] = tf.placeholder(tf.float32, shape=[None, self.nmax, self.graph_dims[0]])\n        \n        for i in range(1, self.n_layers+1):\n        \n            self.M[i] = tf.einsum('adc,adb->abc', self.H[i-1], self.adjs)\n            self.H[i] = tf.nn.relu(tf.tensordot(self.M[i], self.A[i], (2, 1)) \n                                   + tf.tensordot(self.H[i-1], self.B[i], (2, 1)))\n            self.G[i] = tf.reduce_mean(self.H[i], 1)\n        \n        Y_OUT = tf.matmul(self.G[self.n_layers], self.W)\n        cost = tf.losses.softmax_cross_entropy(self.targets, Y_OUT)\n        \n        self.predictions = tf.argmax(Y_OUT, 1)\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.alpha)\n        self.train = optimizer.minimize(cost)\n        \n        self.sess = tf.Session()\n        self.sess.run(tf.global_variables_initializer())\n        \n    def fit(self, adj, embds, y, epochs=20, batch_size=10, shuffle=True):\n        self.scores = []\n        minibatches = ceil(len(adj) / batch_size)\n        \n        y_enc = one_hot_encode(y)\n        \n        j = 0\n        for i in range(epochs):\n            INDS = np.array(range(len(adj)))\n            \n            if shuffle:\n                idx = np.random.permutation(y.shape[0]) \n                INDS = INDS[idx]\n                \n            mini = np.array_split(INDS, minibatches)\n            \n            for inds in mini:\n                j+=1\n                sys.stderr.write('\\rEpoch: %d/%d' % (j, epochs*minibatches))\n                sys.stderr.flush()\n                self.sess.run(self.train, feed_dict={self.adjs:adj[inds], self.H[0]:embds[inds], \n                                                self.targets:y_enc[inds]})\n                \n            self.scores.append(self.score(adj, embds, y))\n            \n        \n        \n    def predict(self, adj, embds):\n        return self.sess.run(self.predictions, feed_dict={self.adjs:adj, self.H[0]:embds})\n    \n    def score(self, adj, embds,y):\n        y_ = self.predict(adj, embds)\n        return 100*(y==y_).mean()\n    \n    def auc_score(self, adj, embds,y):\n        from sklearn.metrics import roc_auc_score\n        y_ = self.predict(adj, embds)\n        return roc_auc_score(y,y_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating Graphs from AMLSim Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating sub-graphs\ngraphs_aml = []\nfor idx, row in tqdm_notebook(transactions.iterrows(),total=transactions.shape[0]):\n    source = row['source']\n    tran_id = row['tran_id']\n    df = transactions[(transactions['source']==source)]\n    nodes_list = list(set(df['source'].tolist() + df['target'].tolist()))\n    tmp_node = accounts[accounts['acct_id'].isin(nodes_list)].set_index('acct_id')\n    graph = nx.from_pandas_edgelist(df, 'source', 'target', ['weight', 'tx_type'])\n    graphs_aml.append(graph)\ngraphs = graphs_aml","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sample AML Transaction Graph","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nx.draw(graphs[993])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature engineering source node with adjacent neighbour sub-graphs\n\ndef get_graph_features(G, all_embds, nmax=15):\n    n = len(G.nodes())\n    \n    node2id = {node:i for i, node in enumerate(G.nodes())}\n    id2node = {i:node for node,i in node2id.items()}\n\n    adj = np.zeros((nmax,nmax))\n    embds = np.zeros((nmax, all_embds.shape[1]))\n\n    for i in G.nodes():\n        embds[node2id[i]] = all_embds[i]\n        for j in G.neighbors(i):\n            adj[node2id[j],node2id[i]] = 1\n    \n    return adj, embds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GCN Current Experiment Parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBED_DIM = 5\nNB_SAMPLES = transactions.shape[0]\nVOCAB_SIZE = 325\nMAX_LENGTH = 10\nNB_CLASSES = 2\n# PROBAS = [0.3, 0.4, 0.55]\n# CENTERS  =[0.1, 0.15, 0.2]\nSHARE = .50\nGRAPH_DIM = 10\n\n\n# Relation and Target data for feeding into Neural Network\ngraphs = graphs_aml\ny = transactions['is_sar'].astype('uint')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generating random embeddings matrix based on Graph shapes // Should be equal to accounts nos.\nembds = np.random.normal(size = (accounts.shape[0],EMBED_DIM))\nembds.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating features\nAdjs, Ids = [], []\nfor graph in graphs:\n    adj, embds_g = get_graph_features(graph, embds, nmax=VOCAB_SIZE)\n    Adjs.append(adj)\n    Ids.append(embds_g)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Value counts for imbalance check\npd.DataFrame(y)['is_sar'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training Data Preparation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training data prep\n\nADJ = np.array(Adjs)\nID = np.array(Ids)\n\nCUT = int(NB_SAMPLES * SHARE)\nADJ_train, y_train, ADJ_test, y_test = ADJ[:CUT], y[:CUT], ADJ[CUT:], y[CUT:]\nID_train, ID_test = ID[:CUT], ID[CUT:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GCN Training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow.compat.v1 as tf\n\ntf.disable_v2_behavior()\n# Accomodating change for tensorflow 1.x behaviour \n\n# Running GCN\nmodel = GraphConvolutionNetwork(node_dim=EMBED_DIM, graph_dim=GRAPH_DIM, nb_classes=NB_CLASSES, \n             nmax=VOCAB_SIZE, alpha=0.025)\nmodel.fit(ADJ_train, ID_train, y_train, epochs=10, batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train-test results\ntrain_score = model.score(ADJ_train, ID_train, y_train)\ntest_score = model.score(ADJ_test, ID_test, y_test)\nprint(f\"Training score : {train_score}\")\nprint(f\"Test score : {test_score}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#AUC results\n\ntrain_score = model.auc_score(ADJ_train, ID_train, y_train)\ntest_score = model.auc_score(ADJ_test, ID_test, y_test)\nprint(f\"Training ROC AUC : {train_score}\")\nprint(f\"Test ROC AUC : {test_score}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"end of notebook","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Inference GCN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"payload = {\n    \"source\" : 92,\n    \"target\" : 83,\n    \"weight\" : 0.2,\n    \"tx_type\": 3,\n}\ninference_df = transactions[['source','target','weight','tx_type']].copy()\ninference_df.append(payload,ignore_index=True)\ndf = inference_df[(inference_df['source']==payload['source'])]\nnodes_list = list(set(df['source'].tolist() + df['target'].tolist()))\ntmp_node = accounts[accounts['acct_id'].isin(nodes_list)].set_index('acct_id')\ngraph = nx.from_pandas_edgelist(df, 'source', 'target', ['weight', 'tx_type'])\nnx.draw(graph)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"adj, embds_g = get_graph_features(graph, embds, nmax=VOCAB_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"adj = adj.reshape(1,325,325)\nembds_g = embds_g.reshape(1,325,5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.predict(adj, embds_g)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}