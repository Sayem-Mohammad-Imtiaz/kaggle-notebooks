{"cells":[{"metadata":{"id":"kdaCMz9b-z9p","trusted":true},"cell_type":"code","source":"#this cell is valid when you're working on google colab and you want to upload the data to colab environment to use in your notebook\n#uploading the data file from your Desktop\n#from google.colab import files\n#files.upload()","execution_count":null,"outputs":[]},{"metadata":{"id":"M8HugXeCo5o7"},"cell_type":"markdown","source":"## Loading all the relevant libraries"},{"metadata":{"id":"G-FoeZRno3F5","outputId":"96ad5301-33a0-4621-ad15-cf75699256ec","trusted":false},"cell_type":"code","source":"import pandas as pd\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n%matplotlib inline\npd.set_option('display.max_columns',40)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.model_selection import GridSearchCV\nimport plotly.offline as py\npy.init_notebook_mode(connected=False)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n","execution_count":null,"outputs":[]},{"metadata":{"id":"N4Ihx5mezY97","trusted":false},"cell_type":"code","source":"def configure_plotly_browser_state():\n  import IPython\n  display(IPython.core.display.HTML('''\n        <script src=\"/static/components/requirejs/require.js\"></script>\n        <script>\n          requirejs.config({\n            paths: {\n              base: '/static/base',\n              plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext',\n            },\n          });\n        </script>\n        '''))","execution_count":null,"outputs":[]},{"metadata":{"id":"X1Nt0cR3pEo0"},"cell_type":"markdown","source":"## Data Overview"},{"metadata":{"id":"6LVIaMQ9mlbA","outputId":"52e1b98a-7cd8-4a05-875f-07338ef97803","trusted":false},"cell_type":"code","source":"#Loading the dataset in Pandas dataframe\ndf_cancer = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')\ndf_cancer.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"yZPfCPVWmrp2","outputId":"6f6f1eb1-aa96-4ed0-9e6f-2ccb9a5d138e","trusted":false},"cell_type":"code","source":"print(df_cancer.columns)\nprint()\nprint(\"Cancer dataset dimensions : {}\".format(df_cancer.shape))\nprint()\nprint(\"Rows:\",df_cancer.shape[0])\nprint()\nprint(\"Columns:\",df_cancer.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"id":"l9IxphPzqGnL"},"cell_type":"markdown","source":"There is one column in the end which is random so we will drop this column"},{"metadata":{"id":"lnlXtVQcm50j","trusted":false},"cell_type":"code","source":"df_cancer = df_cancer.drop('Unnamed: 32',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"vT19szeAnLPW","outputId":"9de58962-0b33-4318-e4b1-79e50003dfdf","trusted":false},"cell_type":"code","source":"print(df_cancer.columns)\ndf_cancer.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"kPSa5MX5qpWI"},"cell_type":"markdown","source":"It is always a good practice to see some **stats(mean,median,percentiles)** of all the variables involve, and pandas has a describe() functions especially for this purpose.\nWe are doing Transpose of the describe() output since we have almost 30 columns to see.\nThis can also be used to see outliers without using any plot."},{"metadata":{"id":"IT5MplwNqgbe","outputId":"8eb20ed5-e904-4509-834f-7ba4c27061b8","trusted":false},"cell_type":"code","source":"df_cancer.describe().T","execution_count":null,"outputs":[]},{"metadata":{"id":"um9tNbRarisD"},"cell_type":"markdown","source":"### checking for any missing value in data, if there are any missing value we will be doing missing value imputation"},{"metadata":{"id":"rM934JcsrGI1","outputId":"29217548-cf38-4edb-8184-0156bab0cf2f","trusted":false},"cell_type":"code","source":"print(df_cancer.isnull().any().any())","execution_count":null,"outputs":[]},{"metadata":{"id":"kVOHWPUDsKvv"},"cell_type":"markdown","source":"And there are no missing records in the given data.\nWell done.\nLet's move ahead."},{"metadata":{"id":"20IOrVWasEyS"},"cell_type":"markdown","source":"## Exploratory Data Analysis(EDA)"},{"metadata":{"id":"hQMyvqyRsaRM"},"cell_type":"markdown","source":"Let's see what we have in our dependent variable(**diagnosis**). Here we are using graph objects of plotly library."},{"metadata":{"id":"UDzXZI5Zru3Q","outputId":"014832c8-7921-4ab0-f321-c77dcf1a8ae9","trusted":false},"cell_type":"code","source":"configure_plotly_browser_state()\ntrace = go.Pie(labels = ['benign','malignant'], values = df_cancer['diagnosis'].value_counts(), \n               textfont=dict(size=10), opacity = 0.7,\n               marker=dict(colors=['green', 'red'], \n               line=dict(color='#000000', width=1.0)))\n           \n\nlayout= go.Layout(\n        title={\n        'text': \"Distribution of dependent(diagnosis) variable\",\n        'y':0.8,\n        'x':0.45,\n        'xanchor': 'center',\n        'yanchor': 'top'})\n\nfig = go.Figure(data = [trace], layout=layout)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"WPVu0Hee4HR6"},"cell_type":"markdown","source":"number of benign classes are much more than malignant"},{"metadata":{"id":"YU8S5LNk4Rt_"},"cell_type":"markdown","source":"Mapping the categories of dependent variable to 1 and 0. We will be predicting whether the record is malignant cancer or not so it makes sense to tag malignant as 1"},{"metadata":{"id":"roxsGZwM4PTc","outputId":"0690bb60-5af8-4d9d-c868-3c1390a6301b","trusted":false},"cell_type":"code","source":"df_cancer['diagnosis']= df_cancer['diagnosis'].map({'M':1,'B':0})\ndf_cancer.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"dzW3vaMiQevl","outputId":"f96f8ece-8dd7-4b6f-d111-9353fd314c92","trusted":false},"cell_type":"code","source":"df_cancer['diagnosis'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"i72gudca4moh","outputId":"3da5af9a-348c-4586-fb54-6c050d362cc3","trusted":false},"cell_type":"code","source":"mal = df_cancer[(df_cancer['diagnosis'] != 0)]\nprint(mal.shape)\nben = df_cancer[(df_cancer['diagnosis'] == 0)]\nprint(ben.shape)\ndef show_plots(column, bin_size) :  \n    t1 = mal[column]\n    t2 = ben[column]\n    \n    hist_data = [t1, t2]\n    \n    group_labels = ['Malignant', 'Benign']\n    colors = ['red', 'green']\n\n    fig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = bin_size, curve_type='kde')\n    \n    fig['layout'].update(title = column)\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"MCm5wtfJMyhy","outputId":"1ae73ba2-d307-42ab-8ef1-9bd3fbbbb332","trusted":false},"cell_type":"code","source":"configure_plotly_browser_state()\nshow_plots('radius_mean', .3)\nshow_plots('texture_mean', .3)\nshow_plots('perimeter_mean',3)\nshow_plots('area_mean',20)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"wHHgmhncNNJj","outputId":"9647e7c7-7ccf-43d3-fe9a-724b8729de79","trusted":false},"cell_type":"code","source":"configure_plotly_browser_state()\nshow_plots('radius_se', 0.1)\nshow_plots('texture_se', .1)\nshow_plots('perimeter_se', .5)\nshow_plots('area_se', 5)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"1oWwf7NaRHYe","outputId":"1d64b5a5-5049-47e4-d4fa-41ad58c88d2b","trusted":false},"cell_type":"code","source":"configure_plotly_browser_state()\nshow_plots('radius_worst', .5)\nshow_plots('texture_worst', .5)\nshow_plots('perimeter_worst', 5)\nshow_plots('area_worst', 15)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"L4Lj0f3rR7jW"},"cell_type":"markdown","source":"heatmap is a good visualtization plot to see the corrleation among vaiables and there is not point of feeding highly correlated variables into any ML model because we are not providing any extra information through that variables and we are adding a complexity to any ML model by adding 1 variable. We want out model to as generic and simpleas possible."},{"metadata":{"id":"kcPlooNIRrtj","outputId":"062948cb-fc1b-436b-beaa-21cb73320343","trusted":false},"cell_type":"code","source":"plt.figure(figsize=(25,12))\nsns.heatmap(df_cancer.corr(),annot=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"4rI25l_HSZnc","outputId":"187eb24d-781b-4a30-febc-867ec55d8bf6","trusted":false},"cell_type":"code","source":"sns.scatterplot(x='area_mean',y='smoothness_mean',hue='diagnosis',data=df_cancer)","execution_count":null,"outputs":[]},{"metadata":{"id":"bnBTO_IFTFZp"},"cell_type":"markdown","source":"## Model Building"},{"metadata":{"id":"3x8on6c_W__y"},"cell_type":"markdown","source":"### Model1. Baseline Model\nWe will build a model by using all the variables present in our model using random forest classifier. This will be our baseline model which we will try to beat by using feature selection and also by changing the classifier to xgboost."},{"metadata":{"id":"03oe9XNPSt47","outputId":"a1b0800b-1952-42cd-b9bd-0d4061460240","trusted":false},"cell_type":"code","source":"features = ['radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']\nlen(features)","execution_count":null,"outputs":[]},{"metadata":{"id":"OTPPz4o9UVoi"},"cell_type":"markdown","source":"#### Splitting the data into train and val to build the model on train and validate it on val data."},{"metadata":{"id":"E9TLxY-hT2sT","outputId":"91dbdbce-e40f-45a9-a90a-85e3e97b63f1","trusted":false},"cell_type":"code","source":"X =df_cancer[features].values\ny =df_cancer['diagnosis']\nprint(X.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"FJHBo8H-T8A0","outputId":"f5317250-cae2-463f-f1c3-7d666b21817f","trusted":false},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X,y,test_size=0.3,random_state=22,stratify=y)\nprint(\"Shape of train dataset:\")\nprint(X_train.shape)\nprint(y_train.shape)\nprint(\"\\n\")\nprint(\"Shape of val dataset:\")\nprint(X_val.shape)\nprint(y_val.shape)\nprint(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"id":"LItQZn85VN8G"},"cell_type":"markdown","source":"#### Model Building"},{"metadata":{"id":"SYfWoX07UTvk","outputId":"f3eb2ec6-a791-4eea-d9f3-7cb2aec20f33","trusted":false},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nmodel1 = RandomForestClassifier(max_depth=1, random_state=0, verbose=0,n_estimators=50)\nmodel1.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"T8qfR6AbWqPi","trusted":false},"cell_type":"code","source":"y_pred1 = model1.predict(X_val)","execution_count":null,"outputs":[]},{"metadata":{"id":"2oqapjliW3kT"},"cell_type":"markdown","source":"#### Model Evaluation"},{"metadata":{"id":"FM6YQTDaXHkp","outputId":"08ddbeb9-a171-4d9b-d2c5-d6b15d1eb011","trusted":false},"cell_type":"code","source":"cnf1 = confusion_matrix(y_val,y_pred1)\nsns.heatmap(cnf1,annot=True,cmap='summer',fmt='g')","execution_count":null,"outputs":[]},{"metadata":{"id":"PnCsH9cdXLbS","outputId":"cdd1bfcf-8a27-4013-d023-db9128f93a37","trusted":false},"cell_type":"code","source":"acc1 = accuracy_score(y_val,y_pred1)\nprint(\"Accuracy: for baseline model is: %0.3f\"%acc1)\n\nprint(\"RF train accuracy: %0.3f\" % model1.score(X_train, y_train))\nprint(\"RF test accuracy: %0.3f\" % model1.score(X_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"id":"7ATeH1LDXVKf","outputId":"89d14248-05f3-4bfe-f054-ac4ce24e480f","trusted":false},"cell_type":"code","source":"print(classification_report(y_val,y_pred1))","execution_count":null,"outputs":[]},{"metadata":{"id":"A4MGCHA_zRfR","outputId":"11b0ed12-03bc-4493-b904-8b8e947190af","trusted":false},"cell_type":"code","source":"coef1= model1.feature_importances_\nprint(coef1.shape)\nprint(len(features))\ncoefs1 = pd.DataFrame({\"Features\":features,\"Coefficients\":coef1})\nfeature_imp1 = coefs1.sort_values(by='Coefficients',ascending=False)\nplt.figure(figsize=(15,10))\nsns.barplot(y='Features',x='Coefficients',data=feature_imp1)","execution_count":null,"outputs":[]},{"metadata":{"id":"Ujp9ouXBa5Mr"},"cell_type":"markdown","source":"### Model2. Random Forest using Parameter Tuning\nWe will use grid search to tune the parametrs of Random Forest"},{"metadata":{"id":"a4VR65LGXctc","trusted":false},"cell_type":"code","source":"param_grid={'n_estimators':[50,100,150,200,250],\n            'max_depth':[1,2,3,4],\n            'min_samples_split':[2,3,5],\n            'max_features':['auto','sqrt','log2']}","execution_count":null,"outputs":[]},{"metadata":{"id":"BGoAJN4_bnhV","outputId":"1802e707-d297-4308-eae1-ab4ecab82a93","trusted":false},"cell_type":"code","source":"model2= GridSearchCV(RandomForestClassifier(),param_grid,refit=True,verbose=0,n_jobs=-1)\nmodel2.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"1NikU-FtiCzi","outputId":"69adbd13-e1ae-4d5b-a6c9-4bad4852e7d4","trusted":false},"cell_type":"code","source":"print(model2.best_params_)\ny_pred2 = model2.predict(X_val)","execution_count":null,"outputs":[]},{"metadata":{"id":"rvM4otiiiGAR","outputId":"e97953f6-be27-4d80-d56f-f8659e7937b7","trusted":false},"cell_type":"code","source":"cnf2 = confusion_matrix(y_val,y_pred2)\nsns.heatmap(cnf2,annot=True,fmt='g',cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{"id":"rgJKoehZiJiZ","outputId":"8311555f-7565-463a-c4f6-1aacf41d7d51","trusted":false},"cell_type":"code","source":"acc2 = accuracy_score(y_val,y_pred2)\nprint(\"Accuracy with GridSearch: %0.3f\"%acc2)\n\nprint(\"RF train accuracy: %0.3f\" % model2.score(X_train, y_train))\nprint(\"RF test accuracy: %0.3f\" % model2.score(X_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"id":"ddGYdzRTiPD7","outputId":"e5f99210-8a46-4b18-fa25-645de1131e91","trusted":false},"cell_type":"code","source":"print(classification_report(y_val,y_pred2))","execution_count":null,"outputs":[]},{"metadata":{"id":"rlNI5RiUia_s","outputId":"02883f85-4273-424a-f258-3098a00415c0","trusted":false},"cell_type":"code","source":"coef2= model2.best_estimator_.feature_importances_\nprint(coef2.shape)\nprint(len(features))\ncoefs2 = pd.DataFrame({\"Features\":features,\"Coefficients\":coef2})\nfeature_imp2 = coefs2.sort_values(by='Coefficients',ascending=False)\nplt.figure(figsize=(15,10))\nsns.barplot(y='Features',x='Coefficients',data=feature_imp2)","execution_count":null,"outputs":[]},{"metadata":{"id":"7Zd9BFk8miNw"},"cell_type":"markdown","source":"***`We were able to increase our performance of 94% on val data to 98.8% on val data just by introducing paramter tuning using Grid Search CV.`***"},{"metadata":{"id":"YrGhVwDPnSWw"},"cell_type":"markdown","source":"### Model3.  Using Feature Selection\nAdding the feature selection before feeding all the variables to any model."},{"metadata":{"id":"aBLc1eHBmLg9","outputId":"272e4701-cf4c-4e25-b88f-61edc1d28db9","trusted":false},"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.svm import LinearSVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import ExtraTreesClassifier\nmodel3 = Pipeline([\n  ('feature_selection', SelectFromModel(ExtraTreesClassifier(n_estimators=50))),\n  ('classification', RandomForestClassifier())\n])\nmodel3.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"nq43Ctk2pEWc","outputId":"01297eb3-01a7-4a61-8aaf-61f4ef6981d1","trusted":false},"cell_type":"code","source":"y_pred3 = model3.predict(X_val)\ncnf3 = confusion_matrix(y_val,y_pred3)\nsns.heatmap(cnf3,annot=True,cmap='summer',fmt='g')\nacc3 = accuracy_score(y_val,y_pred3)\nprint(\"Accuracy on Model3 is: %0.3f\"%acc3)\nprint(\"RF train accuracy: %0.3f\" % model3.score(X_train, y_train))\nprint(\"RF test accuracy: %0.3f\" % model3.score(X_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"id":"pVHaF3k808jM","outputId":"903f2907-f28e-42c5-8078-f7a6906c39db","trusted":false},"cell_type":"code","source":"print(classification_report(y_val,y_pred3))","execution_count":null,"outputs":[]},{"metadata":{"id":"cjPMAJNwuSjw","outputId":"01449fc0-64c1-4ee0-991b-438d8fbe1f8e","trusted":false},"cell_type":"code","source":"#Feature Importance\nf1 = model3.steps[0][1].get_support()\nnew_f = [features[i] for i,val in enumerate(f1) if val==True]\nprint(new_f)\ncoef3 = model3.steps[1][1].feature_importances_\nprint(coef3.shape)\nprint(len(new_f))\ncoefs3 = pd.DataFrame({\"Features\":new_f,\"Coefficients\":coef3})\nfeature_imp3 = coefs3.sort_values(by='Coefficients',ascending=False)\nplt.figure(figsize=(15,10))\nsns.barplot(y='Features',x='Coefficients',data=feature_imp3)","execution_count":null,"outputs":[]},{"metadata":{"id":"2p88bNRI1ZS0"},"cell_type":"markdown","source":"***`We were able to increase our performance of 94% on val data to 98.2% on val data just by using Feature Selection of Extratree classifier. Here we are using 11 features to train our model that too without any paramter tuning and we are able to match the Model2 performance where we used 31 variables and parameter tuning.`***"},{"metadata":{"id":"4d0aZ3al2BiQ"},"cell_type":"markdown","source":"### Model4. Xgboost"},{"metadata":{"id":"L6TndUNDrY6H","outputId":"015fd467-1af2-40db-a2d9-8d0a972dd5da","trusted":false},"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nmodel4 = XGBClassifier()\nmodel4.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"CRadqWwf3Pyw","outputId":"8e11252c-2cb3-4ff3-9125-8924a3e18b37","trusted":false},"cell_type":"code","source":"y_pred4 = model4.predict(X_val)\ncnf4 = confusion_matrix(y_val,y_pred4)\nsns.heatmap(cnf4,annot=True,cmap='summer',fmt='g')\nacc4 = accuracy_score(y_val,y_pred4)\nprint(\"Accuracy on Model3 is: %0.3f\"%acc4)\nprint(\"RF train accuracy: %0.3f\" % model4.score(X_train, y_train))\nprint(\"RF test accuracy: %0.3f\" % model4.score(X_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"id":"RBgJqomV3bcX","outputId":"e57a2e10-c9d2-4b11-98ad-2f5cf939d3b0","trusted":false},"cell_type":"code","source":"print(classification_report(y_val,y_pred4))","execution_count":null,"outputs":[]},{"metadata":{"id":"-0w3q-Xf33P-","outputId":"6f635a80-6425-4a0a-b769-58617f3968fb","trusted":false},"cell_type":"code","source":"coef4= model4.feature_importances_\nprint(coef4.shape)\nprint(len(features))\ncoefs4 = pd.DataFrame({\"Features\":features,\"Coefficients\":coef4})\nfeature_imp4 = coefs4.sort_values(by='Coefficients',ascending=False)\nplt.figure(figsize=(15,10))\nsns.barplot(y='Features',x='Coefficients',data=feature_imp4)","execution_count":null,"outputs":[]},{"metadata":{"id":"j_VJc-AB4e2O"},"cell_type":"markdown","source":"***`We were able to increase our performance of 94% on val data(baseline model) to 98.8% on val data just by using Xgboost without any paramter tuning and we are able to match the Model2 performance where we used 31 variables and parameter tuning.\nWith only around 10 lines of code you can achieve the 98.8% accuracy on val data using Xgboost`***"},{"metadata":{},"cell_type":"markdown","source":"### Model5. Light GBM"},{"metadata":{"id":"CBCKYp_h4UQS","trusted":false},"cell_type":"code","source":"from lightgbm import LGBMClassifier\nmodel5 = LGBMClassifier()\nmodel5.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred5 = model5.predict(X_val)\ncnf5 = confusion_matrix(y_val,y_pred5)\nsns.heatmap(cnf5,annot=True,cmap='summer',fmt='g')\nacc5 = accuracy_score(y_val,y_pred5)\nprint(\"Accuracy on Model5 is: %0.3f\"%acc5)\nprint(\"Ligtgbm train accuracy: %0.3f\" % model5.score(X_train, y_train))\nprint(\"LightGBM test accuracy: %0.3f\" % model5.score(X_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_val,y_pred5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coef5= model5.feature_importances_\nprint(coef5.shape)\nprint(len(features))\ncoefs5 = pd.DataFrame({\"Features\":features,\"Coefficients\":coef5})\nfeature_imp5 = coefs5.sort_values(by='Coefficients',ascending=False)\nplt.figure(figsize=(15,10))\nsns.barplot(y='Features',x='Coefficients',data=feature_imp5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***`We were able to increase our performance of 94% on val data(baseline model) to 98.8% on val data just by using LightGBM without any paramter tuning and we are able to match the Model2 performance where we used 31 variables and parameter tuning.\nWith only around 10 lines of code you can achieve the 98.8% accuracy on val data using LightGBM\nModel Performance of XGboost and LightGBM are similar.`***"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}