{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1> Please do Upvote this kernel!!</h1>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ndf=pd.read_csv(\"/kaggle/input/covid19-research-preprint-data/COVID-19-Preprint-Data_ver2.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> Welcome to this notebook, Here lets get a glimpse of the data and list the columns</h3>\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(df.columns)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4> Ah of course the first thing I always would do is to split the Datetime into day,month,year and day in year(like 1st febuary is the 32nd day of the year , this helps in plotting :D)</h4>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\ndf[\"day\"]=df[\"Date of Upload\"].apply(lambda x: int(datetime.strptime(x,'%Y-%m-%d').day))\ndf[\"month\"]=df[\"Date of Upload\"].apply(lambda x:int( datetime.strptime(x,'%Y-%m-%d').month))\ndf[\"year\"]=df[\"Date of Upload\"].apply(lambda x: int( datetime.strptime(x,'%Y-%m-%d').year))\ndf[\"day_in_year\"]=df[\"Date of Upload\"].apply(lambda x:int( datetime.strptime(x,'%Y-%m-%d').timetuple().tm_yday))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4> I am going to drop the columns below as I do not aim to use it in my EDA</h4>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop([\"Preprint Link\",\"DOI\",\"Date of Upload\"],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4> Our first plot in the notebook is a simple one and I have done it using Altair, here I plot the number of abstracts(or generally papers ) submitted per day \n<ul><li> -X= day in the year</li>\n<li> -Y= the count of the number of abstracts(research papers) published in that particular day</li></ul>\n<h4> the below plot is a bubble plot which increases its size as the number of abstracts per day, you can hover for more info</h4>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import altair_render_script\nimport altair as alt\nalt.Chart(df.groupby([\"day_in_year\"]).count().reset_index()).mark_point().encode(\n    x='day_in_year',\n    y='Abstract',\n    tooltip=[\"Abstract\",\"day_in_year\"],\n    size=\"Abstract\"\n).interactive()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df[df[\"day_in_year\"]==137]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4> we can see above that after a certain day (about 137th day of the year) which is 16th May 2020</h4>\n<p><h4>Below we are going to plot the number of abstracts(research papers) published in the given months of 2020, we first filter out only months of 2020</h4></p>","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"\nalt.Chart(df[df[\"year\"]==2020].groupby([\"month\"]).count().reset_index()).mark_area(\n    line={'color':'darkblue'},\n    color=alt.Gradient(\n        gradient='linear',\n        stops=[alt.GradientStop(color='white', offset=0),\n               alt.GradientStop(color='blue', offset=1)],\n        x1=1,\n        x2=1,\n        y1=1,\n        y2=0\n\n    )\n).encode(\n    alt.X('month'),\n    alt.Y('Abstract',title=\"Abstract Count published\"),\n    tooltip=[\"month\",\"Abstract\"]\n).interactive()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4>Over here we will import nltk stopwords and remove the stopwords from the abstract and I have made a function for it, next I have done is as follows:\n<ul><li> I creat a function(get_top_n_words) for doing all in one </li>\n<li>it first removes stopwords using Countvectorerizer</li>\n<li>our function which takes n as input is the top n word frequencies we want</li>\n<li>using vecotor transform and bag of words we get the frequencies and the words of the whole <b>abstract column</b> which we will input as the agrument <b>corpus</b></li>\n<li> we then sort it according to the most number of frequencies and return the top n words and their frequencies in a <b>list of tuples</b></li>\n</ul>\n</h4>","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize \nstop_words=set(stopwords.words('english'))\ndef removeSW(x):\n    x=x.lower()\n    word_tokens = word_tokenize(x) \n    \n    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n    return \" \".join(filtered_sentence)\n\n\ndf[\"Abstract\"]=df[\"Abstract\"].apply(removeSW)\nfrom sklearn.feature_extraction.text import CountVectorizer\ndef get_top_n_words(corpus, n=None):\n  \n    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4>Lets get our unigrams of the abstract column and the title column using this function </h4>","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"unigrams=get_top_n_words(df[\"Abstract\"],20)\nunigrams_title=get_top_n_words(df[\"Title of preprint\"],20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"unigrams","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"unigrams_title","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4> Yes! I know both the list of tuples looks very much same since many of the title and abstract words are column, now lets get down in making a seperate dataframe and join this 2 list of tuples as rows and their values as column ,also a column for <b>type</b> indicates that wether it is abstract or title</h4>","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"d={\"word\":[],\"count\":[],\"type\":[]}\nfor i in unigrams:\n    d[\"word\"].append(i[0])\n    d[\"count\"].append(i[1])\n    d[\"type\"].append(\"Abstract\")\nfor i in unigrams_title:\n    d[\"word\"].append(i[0])\n    d[\"count\"].append(i[1])\n    d[\"type\"].append(\"Title\")\ncount_df=pd.DataFrame(d)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4> We plot!!</h4>","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"\nsource = count_df\n\nalt.Chart(source).mark_bar().encode(\n    tooltip=[\"word\",\"count\"],\n    column='type',\n    x='word',\n    y='count',\n    color='type'\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4> Now we modify the function before and create a new function called<b> get_top_gram</b> It takes one more argument called grams, which will be the number of grams we want, bigram trigram etc and after that I will plot the bigram as similarly as the unigrams, and then I plot the pentagrams!!<h4>","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def get_top_gram(corpus,grams, n=None):\n  \n    vec = CountVectorizer(ngram_range=grams,stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"bigrams=get_top_gram(df[\"Abstract\"],(2,2),20)\nbigrams_title=get_top_gram(df[\"Title of preprint\"],(2,2),20)\nd={\"word\":[],\"count\":[],\"type\":[]}\nfor i in bigrams:\n    d[\"word\"].append(i[0])\n    d[\"count\"].append(i[1])\n    d[\"type\"].append(\"Abstract\")\nfor i in bigrams_title:\n    d[\"word\"].append(i[0])\n    d[\"count\"].append(i[1])\n    d[\"type\"].append(\"Title\")\ncount_df=pd.DataFrame(d)\n\nsource = count_df\n\nalt.Chart(source).mark_bar().encode(\n    tooltip=[\"word\",\"count\"],\n    column='type',\n    x='word',\n    y='count',\n    color='type'\n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fivegrams=get_top_gram(df[\"Abstract\"],(5,5),20)\nfivegrams_title=get_top_gram(df[\"Title of preprint\"],(5,5),20)\nd={\"word\":[],\"count\":[],\"type\":[]}\nfor i in fivegrams:\n    d[\"word\"].append(i[0])\n    d[\"count\"].append(i[1])\n    d[\"type\"].append(\"Abstract\")\nfor i in fivegrams_title:\n    d[\"word\"].append(i[0])\n    d[\"count\"].append(i[1])\n    d[\"type\"].append(\"Title\")\ncount_df=pd.DataFrame(d)\n\nsource = count_df\n\nalt.Chart(source).mark_bar().encode(\n    tooltip=[\"word\",\"count\"],\n    column='type',\n    x='word',\n    y='count',\n    color='type'\n)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4> Now lets get onto the authors and the Universities!!</h4>\n<p> Here I use json to extract the dictionary present in the column <b>Author(s) Institutions</b></p>\n<p> after that I show the length of the set and list created, basically the set contains the number of unique Institutions and the list contains everytime the Institution is mentioned in the dataframe, we will use it ahead</p>\n","execution_count":null},{"metadata":{"tags":["outputPrepend"],"trusted":false},"cell_type":"code","source":"import json\nmyset=set()\nmylist=[]\nfor i in df[\"Author(s) Institutions\"].index:\n    l=set(json.loads(df.loc[i,\"Author(s) Institutions\"]).keys())\n    for j in l:\n        myset.add(j)\n        mylist.append(j)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(myset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(mylist)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4>Another function phew!!!, well this is quite simple we use the list and count how many times each intistute has appeared and then a dictionary example ({\"university\":10})</h4>\n<p> basically what we want to do is that the frequency represents how many times the institution has published a paper since its occurence in the dataframe in each row is a count for its publications!!!</p>","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def CountFrequency(my_list): \n      \n  \n   count = {} \n   for i in my_list: \n       count[i] = count.get(i, 0) + 1\n       \n   \n    \n   return count \nfrequency=CountFrequency(mylist)\n  \nd={\"UNI\":[],\"Publishes\":[]}\nfor i in frequency.keys():\n    d[\"UNI\"].append(i)\n    d[\"Publishes\"].append(frequency[i])\n\nuni_df=pd.DataFrame(d)\n\nuni_df=uni_df.sort_values(by=[\"Publishes\"],ascending=False)\nfor i in uni_df.index:\n    if len(uni_df.loc[i].UNI)<4:\n        uni_df.drop(i,axis=0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"source = uni_df.iloc[:50,:]\n\nalt.Chart(source).mark_bar().encode(\n    x='Publishes',\n    y=\"UNI\",\n    tooltip=[\"Publishes\"]\n).properties(height=700)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4> Voila! the graph above shows how many publications done by the institutions(here I plotted only the top 50 )<p>HOVER!!! and we find out <b> Oxford University</b> has published 61 times!!! \n</p>\n<p>Now, lets check for the authors , below I used the previous dictionary of frequencies we outputed and change the values to list of values example\n<p>{\"uni\":1}===>{\"uni\":[1,0]}</p>\nhere we add the 0 to all as we are going to store the authors count for this value!!!\n</p>\n</h4>","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"for i in frequency.keys():\n   frequency[i]=[frequency[i],0]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4>See the name of the institue is given and also the value with it are the authors number! which we did not consider above , well now we are!!</h4>","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df[\"Author(s) Institutions\"].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4> We now again use json to get the values of author institution column but we will extract the names of the Uni as a key for our dictionary and will upadte the auhors value to it</h4>","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"\nmylist=[]\nfor i in df[\"Author(s) Institutions\"].index:\n    l=set(json.loads(df.loc[i,\"Author(s) Institutions\"]).keys())\n    for j in l:\n        frequency[j][1]=frequency[j][1]+json.loads(df.loc[i,\"Author(s) Institutions\"])[j]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":" \nd={\"UNI\":[],\"AuthorCount\":[]}\nfor i in frequency.keys():\n    d[\"UNI\"].append(i)\n    d[\"AuthorCount\"].append(frequency[i][1])\n\nuni_df=pd.DataFrame(d)\n\nuni_df=uni_df.sort_values(by=[\"AuthorCount\"],ascending=False)\nfor i in uni_df.index:\n    if len(uni_df.loc[i].UNI)<4:\n        uni_df.drop(i,axis=0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4>We did the same procdeure and created a new dataframe for university and their authors count!</h4>","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"uni_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4> Finally we plot it!!, and hovering over the data we can see<b> Icahn School of Medicine</b> has the highest amount of authors who have published for covid-19 around 470</h4>","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"source = uni_df.iloc[:50,:]\n\nalt.Chart(source).mark_bar().encode(\n    x='AuthorCount',\n    y=\"UNI\",\n    tooltip=[\"AuthorCount\",\"UNI\"]\n).properties(height=700)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>Thank You!</h1>","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}