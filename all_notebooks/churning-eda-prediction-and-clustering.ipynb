{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\nThis is a comprehensive notebook that might be useful to data scientists in Telecom industry. It studies the case of Customers Churning which is very common in Telecommunication companies. In this project, I did EDA, perdictive modelling and customers clustering. Churn analysis is the evaluation of a companyâ€™s customer loss rate in order to reduce it. Also referred to as customer attrition rate. It's importatnt because keeping an existing customer saves more money to the company than attracting a new one. Churn rate has strong impact on the life time value of the customer because it affects the length of service and the future revenue of the company.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Load libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn import model_selection\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom mlxtend.preprocessing import minmax_scaling\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.decomposition import PCA\nimport pylab as pl\nfrom kmodes.kmodes import KModes\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read the data\n\nThe author has divided the original data into two datasets, train and test, where he sampled 80% of the data into train and validation data and 20% into test data.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/telecom-churn-datasets/churn-bigml-80.csv')\ntest = pd.read_csv('../input/telecom-churn-datasets/churn-bigml-20.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explore train and test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['State'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['State'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The percentage of customers churning from the company is: %{}'.format((train['Churn'].sum()) *100/train.shape[0]) ) # as the Churn column data type is boolean, every True value will be summed as '1'...I'll convert them later into binary 0's and 1's when I do the data cleaning part","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,6))\nsns.set_style('whitegrid')\nsns.barplot(x='State',y='Churn', data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x='Churn', y='Customer service calls',data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x='Churn', y='Account length',data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(train['Account length'], bins=400)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"churn_intl = train.groupby(['Churn','International plan']).size()\nchurn_intl.plot()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"churn_voicem = train.groupby(['Churn','Voice mail plan']).size()\nchurn_voicem.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Total charge'] = train['Total day charge'] + train['Total eve charge'] + train['Total night charge'] + train['Total intl charge']\ntest['Total charge'] = test['Total day charge'] + test['Total eve charge'] + test['Total night charge'] + test['Total intl charge']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='Churn',y='Total charge', data = train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the previous analysis we knew the following insights:\n* 14% of customers have churned.\n* Texas has the highest number of customer churns.\n* Churned customers have called customer service more than remaining customers. Maybe that means that customer service in this company needs more training in retaining customers.\n* Churned customers had higher charges to pay than remaining cutomers. Maybe that means that the company needs to work in more effective plans to facilitate late payments.\n* Account length (Account duration) is normally distributed.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data cleaning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now,we'll clean the data and prepare it for prediction.\n\n\nAs you noticed earlier, when we used .info() with both train and test datasets, we haven't found any null values ( luckily!), but if we had found them, we would either drop columns with the missing values or impute the missing values to the mean, median or mode of the values in the same column.\n\nWe still have columns with categorical values though (dtype = object), so we should deal with them because predictive models deal only with numerical values.\n\nFor 'Churn', 'International plan' and 'Voice mail plan' columns, I will use multiple techniques to deal with categorical values for illustration purpose, but you can use only one of them if you want, since each of the 3 column has only 2 unique values.\n\nBut first, let's work on a copy of the original dataset. ( It's always a good idea to work on copies, not on the original data)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train2 = train.copy()\ntest2 = test.copy()\ntrain2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train2['Churn'] = train2['Churn'].map({True:1,False:0}) # no need to do it for test dataset because Churn column will be dropped later.\n\ntrain2['International plan'].replace(['No','Yes'],[0,1],inplace=True)\ntest2['International plan'].replace(['No','Yes'],[0,1],inplace=True)\n\n# Now, I'll use the label encoder preprocessing technique:\n\nencoder = LabelEncoder()\ncoded_voicem_train = encoder.fit_transform(train2['Voice mail plan'])\ntrain2['Voice mail plan'] = coded_voicem_train\ncoded_voicem_test = encoder.transform(test2['Voice mail plan'])\ntest2['Voice mail plan'] = coded_voicem_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, some data might need scaling. I usually delay that until I choose the features that has higher correlation with the target ( aka feature selection or dimensionality reduction), then scale whatever data needs scaling in the features that I chose. That will lead us to the next step which is:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Feature selection means choosing the best features that highly affects the target and not redundant with each other.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"A good method we can use to study features correlation is .corr()","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train2.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also plot the correlation in heatmap to make it easier for us:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,15))\nsns.heatmap(train2.corr() , annot =True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can notice that:\nSome features are correlated to each other (have a high coefficient with each other).\n\n'Total day minutes' and 'Total day charge' for example are directly related because their coeff is 1, so we'll delete one of them. I choose to delete all the columns with the minutes count because they are redundant.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train3 = train2.drop(['Total day minutes','Total eve minutes','Total night minutes', 'Total intl minutes'], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we'll select the best features that have the highest correlation with the target 'Churn'.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['International plan','Total charge','Customer service calls']\nX_init = train3[features]\ny = train3['Churn']\nXtest_init = test2[features]\nytest = test2['Churn']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_init.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here comes the scaling part...\nThe data range in 'Total charge' is higher that other features, so we'll scale it.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# mix-max scale the data between 0 and 1\nX = minmax_scaling(X_init, columns = features)\nXtest = minmax_scaling(Xtest_init, columns = features)\nXtest","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Selection","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Here comes the juicy part !\nNow that our data is ready, lets build our model, but first we'll split X into training data(80%) and validation data(20%)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtrain,Xval,ytrain,yval = train_test_split(X,y,train_size=0.8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xval.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ytrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yval.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll make a list of tuples. Each tuple contains the model name and the model creation instance. Then, we'll use each model with cross validation technique k-folds to avoid over-fitting. The choice of the best model will depend on its score.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('RF', RandomForestClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\nresults = []\nnames = []\nfor name,model in models:\n    kfold = model_selection.KFold(n_splits=10)\n    cv_result = model_selection.cross_val_score(model,Xtrain,ytrain, cv = kfold, scoring = \"accuracy\")\n    names.append(name)\n    results.append(cv_result)\nfor i in range(len(names)):\n    print(names[i],results[i].mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"KNeighboursClassifier model has the highest score, hence it will be chosen. We'll choose the best n_neighbours parameter using Grid Search which is a class used to fine-tune your model to get the best results.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"chosen_model = KNeighborsClassifier()\nparam = {'n_neighbors': [1,2,3,4,5,6,7]}\ngrid = GridSearchCV(estimator= chosen_model, param_grid=param, cv=5)\ngrid.fit(Xtrain,ytrain)\nprint(grid.best_params_)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model = KNeighborsClassifier(n_neighbors=5)\nbest_model.fit(Xtrain,ytrain)\npred_val = best_model.predict(Xval)\npred = best_model.predict(Xtest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's evaluate our model:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy Score is:\")\nprint(accuracy_score(ytest, pred))\nprint(accuracy_score(yval, pred_val))\nprint()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Classification Report:\")\nprint(classification_report(ytest, pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf = confusion_matrix(ytest,pred)\nlabel = [\"0\",\"1\"]\nsns.heatmap(conf, annot=True, xticklabels=label, yticklabels=label)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Customers Clustering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Telecom companies use recommendation engines to suggest the best packages for the clients based on their history. Customer clustering helps alot in segmentation of customers into groups of similarities. I'm not going to build a recommendation engine here, but rather I'll do the clustering.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I'll start with k-means clustering which requires the data to be numerical, so I'll deal with the processed dataset.\nAlso, we'll make an elbow curve to determine the optimal number of clusters.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# train3 is fine for this.\nclust_data = train3.drop(['Churn','State'], axis=1)\ninertia = []\nfor i in range(1,11):\n    clust_model = KMeans(n_clusters= i , init='k-means++', n_init=10)\n    clust_model.fit(clust_data)\n    inertia.append(clust_model.inertia_)\n\nplt.figure(1 , figsize = (15 ,6))\nplt.plot(np.arange(1 , 11) , inertia , 'o')\nplt.plot(np.arange(1 , 11) , inertia , '-' , alpha = 0.5)\nplt.xlabel('Number of Clusters') , plt.ylabel('Inertia')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The optimal number of clusters is 4.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clust_model = KMeans(n_clusters= 4 , init='k-means++', n_init=10)\nclusters = clust_model.fit_predict(clust_data)\nprint(silhouette_score(clust_data, clusters))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's add the clusters to the original train data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['clusters'] = pd.Series(clusters,index=train.index)\ntrain","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's further inspect those clusters:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clust_churn = train.groupby('clusters').Churn.sum()\nclust_churn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['clusters'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['charge'] = train['Total charge']\ncharge_clust = train.groupby('clusters').charge.mean()\ncharge_clust","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}