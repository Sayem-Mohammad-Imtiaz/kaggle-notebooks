{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.set_option('display.max_columns',None)\npd.set_option('display.max_rows',100)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nplt.style.use(\"ggplot\")\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom scipy import stats","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(['Unnamed: 32','id'],axis = 1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_y = data['diagnosis']\ndata_x = data.drop('diagnosis',axis=1)\n\ndata_standardization = (data_x - data_x.mean())/data_x.std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_violen = pd.concat([data['diagnosis'],data_standardization.iloc[:,0:10]],axis=1)\ndata_violen = pd.melt(data_violen,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(20,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data_violen,split=True, inner=\"quart\")\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### from the above plot we can see the distribution of the values for the given feature\n\n### Findings....\n\n##### 1. From the above plot we can clearly see that radius mean,texture_mean,perimeter mean,area_mean,smoothness mean,compactness_mean,concavity mean,concave point mean are almost having different mean value for the distribution and that's a good sign for classification whereas symmetry mean and fractual dimension mean is having mean almost the same. Bad news for classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_violen = pd.concat([data['diagnosis'],data_standardization.iloc[:,10:20]],axis=1)\ndata_violen = pd.melt(data_violen,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(20,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data_violen,split=True, inner=\"quart\")\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_violen = pd.concat([data['diagnosis'],data_standardization.iloc[:,20:31]],axis=1)\ndata_violen = pd.melt(data_violen,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(20,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data_violen,split=True, inner=\"quart\")\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.boxplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data_violen)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Findings....\n\n##### 1. As we can see that features like texture_worst vs perimeter_worst and perimeter_worst vs area_worst and smoothness_worst vs compactness_worst and so on... are almost similar i.e their are much co-related to each other and since they are co-related, we can drop any one of them during feature selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"## we can visulize any one for the reference\n\nsns.jointplot(data_x.loc[:,'concavity_worst'], data_x.loc[:,'concave points_worst'], kind=\"regg\", color=\"#ce1414\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(data_x.loc[:,'perimeter_worst'], data_x.loc[:,'area_worst'], kind=\"regg\", color=\"#ce1414\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(data_x.loc[:,'symmetry_worst'], data_x.loc[:,'fractal_dimension_worst'], kind=\"regg\", color=\"#ce1414\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### for many comparsion we will use pairplot..."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"white\")\ndf = data_x.loc[:,['radius_worst','perimeter_worst','area_worst']]\ng = sns.PairGrid(df, diag_sharey=False)\ng.map_lower(sns.kdeplot, cmap=\"Blues_d\")\ng.map_upper(plt.scatter)\ng.map_diag(sns.kdeplot, lw=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## using swarm plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\ntic = time.time()\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data_violen)\n\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_violen = pd.concat([data['diagnosis'],data_standardization.iloc[:,0:10]],axis=1)\ndata_violen = pd.melt(data_violen,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(20,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data_violen)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_violen = pd.concat([data['diagnosis'],data_standardization.iloc[:,10:20]],axis=1)\ndata_violen = pd.melt(data_violen,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(20,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data_violen)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_violen = pd.concat([data['diagnosis'],data_standardization.iloc[:,20:31]],axis=1)\ndata_violen = pd.melt(data_violen,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(20,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data_violen)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Findings....\n\n##### 1. Just observe and see which feature can you classify easily and which one not. For ex: if you see radius worst we can classify it very easily but if we see smoothness_worst and symmetry_se, it is very difficult"},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(data_x.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = plt.hist(data[data[\"diagnosis\"] == \"M\"].radius_mean,bins=30,fc = (1,0,0,0.5),label = \"Malignant\")\nb = plt.hist(data[data[\"diagnosis\"] == \"B\"].radius_mean,bins=30,fc = (0,1,0,0.5),label = \"Bening\")\nplt.legend()\nplt.xlabel(\"Radius Mean Values\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram of Radius Mean for Bening and Malignant Tumors\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Findings....\n\n##### 1. we can see that the most frequent values of bening tumor is 13.5 and for malignant is 20\n##### 2. bening if you see follow a bit of normal distribution curve in comparsion with malignant\n##### 3. Variance for malignant is more than bening but mean value of bening is more tham malignant"},{"metadata":{},"cell_type":"markdown","source":"# Checking for outliers...."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_bening = data[data[\"diagnosis\"] == \"B\"]\ndata_malignant = data[data[\"diagnosis\"] == \"M\"]\ndesc = data_bening.radius_mean.describe()\nQ1 = desc[4]\nQ3 = desc[6]\nIQR = Q3-Q1\nlower_bound = Q1 - 1.5*IQR\nupper_bound = Q3 + 1.5*IQR\nprint(\"Anything outside this range is an outlier: (\", lower_bound ,\",\", upper_bound,\")\")\ndata_bening[data_bening.radius_mean < lower_bound].radius_mean\nprint(\"Outliers: \",data_bening[(data_bening.radius_mean < lower_bound) | (data_bening.radius_mean > upper_bound)].radius_mean.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"melted_data = pd.melt(data,id_vars = \"diagnosis\",value_vars = ['radius_mean', 'texture_mean'])\nplt.figure(figsize = (15,10))\nsns.boxplot(x = \"variable\", y = \"value\", hue=\"diagnosis\",data= melted_data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.hist(data_bening.radius_mean,bins=50,fc=(0,1,0,0.5),label='Bening',normed = True,cumulative = True)\n#sorted_data = np.sort(data_bening.radius_mean)\n#y = np.arange(len(sorted_data))/float(len(sorted_data)-1)\n#plt.plot(sorted_data,y,color='red')\n#plt.title('CDF of bening tumor radius mean')\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Effect size\n\n#####    -> One of the summary statistics.\n#####    -> It describes size of an effect. It is simple way of quantifying the difference between two groups.\n#####    -> In an other saying, effect size emphasises the size of the difference\n#####    -> Use cohen effect size\n#####    -> Cohen suggest that if d(effect size)= 0.2, it is small effect size, d = 0.5 medium effect size, d = 0.8 large effect size.\n#####    -> lets compare size of the effect between bening radius mean and malignant radius mean\n#####    -> Effect size is 2.2 that is too big and says that two groups are different from each other as we expect. Because our groups are            bening radius mean and malignant radius mean that are different from each other\n\n\n##### for more reference please visit https://machinelearningmastery.com/effect-size-measures-in-python/ "},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_diff = data_malignant.radius_mean.mean() - data_bening.radius_mean.mean()\nvar_bening = data_bening.radius_mean.var()\nvar_malignant = data_malignant.radius_mean.var()\nvar_pooled = (len(data_bening)*var_bening +len(data_malignant)*var_malignant ) / float(len(data_bening)+ len(data_malignant))\neffect_size = mean_diff/np.sqrt(var_pooled)\nprint(\"Effect size: \",effect_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### seeing the co-relation among the variables using scatter plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,10))\nsns.jointplot(data.radius_mean,data.area_mean,kind=\"regg\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## we can also look at the relationship using seaborn's pais-plot\n\nsns.set(style = \"white\")\ndf = data.loc[:,[\"radius_mean\",\"area_mean\",\"fractal_dimension_se\"]]\ng = sns.PairGrid(df,diag_sharey = False,)\ng.map_lower(sns.kdeplot,cmap=\"Blues_d\")\ng.map_upper(plt.scatter)\ng.map_diag(sns.kdeplot,lw =3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hypothesis Testing\n\n##### I want to learn that are radius mean and area mean related with each other? My null hypothesis is that \"relationship between radius mean and area mean is zero in tumor population'.Now we need to refute this null hypothesis in order to demonstrate that radius mean and area mean are related. (actually we know it from our previous experiences)"},{"metadata":{"trusted":true},"cell_type":"code","source":"statistic, p_value = stats.ttest_rel(data.radius_mean,data.area_mean)\nprint('p-value: ',p_value)\n\nif p_value < 0.05:\n    print('Reject the Null Hypothesis')\nelse:\n    print('Except the Null hypothesis')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}