{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Retail Giant Sales Forecasting Assignment"},{"metadata":{},"cell_type":"markdown","source":"## Problem Statement:\nOnlilne supergiant store, 'Global Mart' has worldwide operations - taking orders and delivering across the globe. Its major product categories are Consumer, Corporate and Home Office.\nAs a Sales Manager, we are expected to forecast the sales of products for the next 6 months - this will help managing inventory and business processes accordingly - for the combination of Market and Segment which is profitable with least variation in Profits."},{"metadata":{},"cell_type":"markdown","source":"## Dataset Available:\nWe have been provided with 4 year's sales data with following columns:\n1. Order Date: Represents the date (January 2011 to December 2014) on which the order was placed\n2. Segment: The segment to which the product belongs. It has 3 categories:\n    a. Consumer\n    b. Corporate\n    c. Home Office\n3. Market: The market to which the customer belongs. It has 7 categories:\n    a. Africa\n    b. APAC (for Asia Pacific)\n    c. Canada\n    d. EMEA (for Middle East)\n    e. EU (for European Union)\n    f. LATAM (for Latin America)\n    g. US (United States)\n4. Sales: Total sales value of the order/ transaction\n5. Profit: Total profit made on the transaction"},{"metadata":{},"cell_type":"markdown","source":"### Importing Required Packages\nWe would be primarily using pandas, numpy, seaborn, matplotlib for various calculations.\nWe might import other packages as the need arises"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \npd.set_option('display.max_colwidth', -1)\n\nimport numpy as np \n\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import xticks\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importing the Dataset\nWe will create a DataFrame, 'retail' to import the dataset, which is in csv format as of now."},{"metadata":{"trusted":true},"cell_type":"code","source":"retail = pd.read_csv('../input/globalsuperstoredata/GlobalSuperstoreData.csv', parse_dates=['Order Date'], dayfirst=True)\nretail.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's do some basic checks and try to understand the various parameters/ qualities of the dataset we have imported"},{"metadata":{"trusted":true},"cell_type":"code","source":"retail.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"retail.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"retail.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking the range of dates for the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\nprint(\"The dataset is with transactions between \", datetime.date(min(retail['Order Date'])),\n      \" and \", datetime.date(max(retail['Order Date'])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's perform some basic Data Cleaning"},{"metadata":{},"cell_type":"markdown","source":"### Checking columns with Null Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"def null_percentage():\n    null_cols = [col for col in retail.columns if retail[col].isnull().sum()>0]\n    null_retail = pd.DataFrame(round(100*retail[null_cols].isnull().mean(),2).sort_values(ascending = False))\n    null_retail.columns = [\"Null Percentage\"]\n    null_retail.sort_values(by = \"Null Percentage\", inplace = True, ascending = False)\n    return null_retail","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_percentage()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apparently, there are no columns which have null values, hence we do not need to impute missing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"retail.describe(include = 'object')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data seems to be well spread with 3 Segments and 7 Markets. Segment 'Consumer' and Market 'APAC' seem to be the leading categories"},{"metadata":{},"cell_type":"markdown","source":"Let's plot the Segments and Categories and check for their spread"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16,4))\nax = sns.countplot(x = 'Segment', data = retail, order = retail.Segment.value_counts().index, palette=\"Set2\")\nxticks(rotation = 90)\nfor p in ax.patches:\n    ax.annotate('{}'.format(p.get_height()), (p.get_x()+0.35, p.get_height()+50))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 3 Segments in which Global Mart classifies its products into\n\nThe highest number of transactions are for Consumer products, followed by Corporate and lowest for Home Office"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16,4))\nax = sns.countplot(x = 'Market', data = retail, order = retail.Market.value_counts().index, palette=\"Set2\")\nxticks(rotation = 90)\nfor p in ax.patches:\n    ax.annotate('{}'.format(p.get_height()), (p.get_x()+0.3, p.get_height()+50))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 7 Markets to which Global Mart caters to\n\nThe highest number of transactions are from APAC, followed by LATAM, EU and US\n\nCanada has lowest number of transactions"},{"metadata":{},"cell_type":"markdown","source":"#### Let's check the distribution of Sales and Profit"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (25,4))\nnum_cols = [\"Sales\",\"Profit\"]\n\nfor i in enumerate(num_cols):\n    plt.subplot(1,3,i[0]+1)\n    sns.distplot(retail[i[1]], hist = False, kde = True, color=\"blue\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sales is right skewed, majority transactions centered around 1,000, with a few worth more than 5,000. \n\nProfit seems to be normally distributed with lots of variations on either side of the mean (0)."},{"metadata":{"trusted":true},"cell_type":"code","source":"retail['Sales'].plot(figsize=(16, 4))\nplt.legend(loc='best')\nplt.title('Sales')\nplt.show(block=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = retail['Sales'].hist(figsize = (16, 4))\nplt.show(block=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's also check for outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.subplots(figsize = (16, 4))\nax = sns.boxplot(x = retail['Sales'], whis = 1.5)\nplt.show(block=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.subplots(figsize = (16, 4))\nax = sns.boxplot(x = retail['Profit'], whis = 1.5)\nplt.show(block=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are lots of outliers for Sales data on the higher end whereas Profit has outliers on both the directions (Profit can take a negative value whereas Sales can not). Let's also look at the quantile distribution of the Sales and Profit."},{"metadata":{"trusted":true},"cell_type":"code","source":"retail.describe(include = 'number', percentiles = [0.0,0.25,0.5,0.75,0.9,0.95,0.99,1.0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that there is a huge gap between 99th and 100th percentile. Considering this is real data and such huge sales are indeed possible, we chose not to remove/ limit the outliers."},{"metadata":{},"cell_type":"markdown","source":"Generally, the Price of an item can not determine its Profit, i.e. a product with lower Price may command a higher Profit depending on the costs involved. Let's validate this using correlation."},{"metadata":{"trusted":true},"cell_type":"code","source":"retail.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (8, 8))\nsns.heatmap(retail.corr(), annot = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this stage, the dataset seems to be in a good shape. Let's now convert the Order Date into Monthly format so that we can create aggregated Sales and Profit data."},{"metadata":{"trusted":true},"cell_type":"code","source":"retail['Order Date'] = pd.to_datetime(retail['Order Date']).dt.to_period('M')\nretail = retail.sort_values(by = ['Order Date'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's create a new DataFrame which includes a combination of Market and Segment. This will lead us to 21 newly created categories under Market-Segment, comprsing of all the combinations of Market with Segment."},{"metadata":{"trusted":true},"cell_type":"code","source":"retail = retail.copy()\nretail['Market-Segment'] = retail['Market'] + \"-\" + retail['Segment']\nretail.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will create a pivot with each of these 21 'Market-Segment' categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"retail_new = retail.pivot_table(index = 'Order Date', values = 'Profit', columns = 'Market-Segment', aggfunc = 'sum')\nretail_new","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will divide the dataset into Train and Test dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_len = 42","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"retail_new_Train = retail_new[0:train_len]\nretail_new_Train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's look at all the 'Markets' & 'Segments' again, along with the newly created 'Market-Segments'"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,4))\nax = sns.countplot(retail['Market'], order = retail.Market.value_counts().index, palette=\"Set2\")\nplt.title('Regional Markets (7)')\nfor p in ax.patches:\n    ax.annotate('{}'.format(p.get_height()), (p.get_x()+0.3, p.get_height()+50))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 7 Markets to which Global Mart caters to\n\nThe highest number of transactions are from APAC, followed by LATAM, EU and US\n\nCanada has lowest number of transactions"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,4))\nax = sns.countplot(retail['Segment'], order = retail.Segment.value_counts().index, palette=\"Set2\")\nplt.title('Customer Segments (3)')\nfor p in ax.patches:\n    ax.annotate('{}'.format(p.get_height()), (p.get_x()+0.35, p.get_height()+50))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 3 Segments in which Global Mart classifies its products into\n\nThe highest number of transactions are for Consumer products, followed by Corporate and lowest for Home Office"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,4))\nax = sns.countplot(retail['Market-Segment'], order = retail['Market-Segment'].value_counts().index, palette=\"Set2\")\nplt.title('Market-Segments (21)')\nplt.xticks(rotation = 90)\nfor p in ax.patches:\n    ax.annotate('{}'.format(p.get_height()), (p.get_x()+0.1, p.get_height()+50))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that there are a total of 21 Market-Segments with all the combinations of Market and Segment. The highest number of transactions are from APAC-Consumer, followed by LATAM-Consumer, US-Consumer and EU-Consumer. Canada seems to be have least number of transactions across Segments."},{"metadata":{},"cell_type":"markdown","source":"### Finding the most consitently profitable Market-Segment using CoV"},{"metadata":{},"cell_type":"markdown","source":"Before proceeding further with the model, let's calculate Coefficient of Variation (CoV) on Profit for each of these 21 Market-Segments (on Train data). The CoV is a relative measure of dispersion and assesss the degree of dispersion of a data relative to its mean. It is calculated as ratio of Standard Deviation of the distribution with its Mean."},{"metadata":{},"cell_type":"markdown","source":"CoV = (Standard Deviation) / Mean\n\nusually represented in percentage"},{"metadata":{"trusted":true},"cell_type":"code","source":"mean=round(np.mean(retail_new_Train),1)\nstd= round(np.std(retail_new_Train),1)\n\nCoV_df= pd.DataFrame(mean)\nCoV_df['Standard Deviation']= std\nCoV_df['CoV'] = round(std/mean*100,1)\n\nCoV_df= CoV_df.reset_index()\nCoV_df.columns= ['Market-Segment', 'Mean', 'Std', 'CoV (%)']\nCoV_df.sort_values(by='CoV (%)', ascending= True, inplace = True)\nCoV_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Considering we are focussing on only one Market-Segment which is profitable with least variation in Profits, we chose APAC-Consumer. The reason we prefer lowest CoV is because we want to estimate sales for the most consistently profitable Market-Segment, so that our sales forecasts are reliable."},{"metadata":{},"cell_type":"markdown","source":"### Let's extract APAC_Consumer as a separate DataFrame and perform further steps on this newly create DataFrame"},{"metadata":{"trusted":true},"cell_type":"code","source":"APAC_Consumer = retail[retail['Market-Segment'] == 'APAC-Consumer']\nAPAC_Consumer = APAC_Consumer.drop(columns = ['Segment', 'Market', 'Market-Segment'])\nAPAC_Consumer.set_index('Order Date')\n\nAPAC_Consumer.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's aggregate Sales and Profit by month, we will get data for 48 months (4 years)."},{"metadata":{"trusted":true},"cell_type":"code","source":"APAC_Consumer = APAC_Consumer.groupby(APAC_Consumer['Order Date']).sum()\nAPAC_Consumer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"APAC_Consumer.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's split this DataFrame into Train and Test datasets. Train dataset with 42 months' data and Test with the rest 6 months' data."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_len = 42","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"APAC_Consumer_Train = APAC_Consumer[0:train_len]\nAPAC_Consumer_Test = APAC_Consumer[train_len:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"APAC_Consumer_Train = APAC_Consumer_Train.reset_index()\nAPAC_Consumer_Train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before proceeding further, we need to convert data type of Order Data to timestamp so that the column can be used effectively."},{"metadata":{"trusted":true},"cell_type":"code","source":"APAC_Consumer_Train['Order Date'] = APAC_Consumer_Train['Order Date'].apply(lambda x: x.to_timestamp())\nAPAC_Consumer_Train['Order Date'].dtype","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"APAC_Consumer_Train = APAC_Consumer_Train.set_index(['Order Date'])\nAPAC_Consumer_Train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Time Series Decomposition"},{"metadata":{},"cell_type":"markdown","source":"We will now decompose the Time Series using Additive as well as Multiplicative methods. This will help us breakdown the series and observe Trend as well as Seasonality, if any."},{"metadata":{"trusted":true},"cell_type":"code","source":"x = list(APAC_Consumer_Train.Sales)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pylab import rcParams\nimport statsmodels.api as sm\nrcParams['figure.figsize'] = 16, 8\ndecomposition = sm.tsa.seasonal_decompose(x, model='additive', period = 12) # additive seasonal index\nfig = decomposition.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is an upward trend as well as seasonality (12 months) in the dataset\n\nThe residuals seem to have some pattern – we perform Multiplicative decomposition"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pylab import rcParams\nimport statsmodels.api as sm\nrcParams['figure.figsize'] = 16, 8\ndecomposition = sm.tsa.seasonal_decompose(x, model='multiplicative', period = 12) # additive seasonal index\nfig = decomposition.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is an upward trend as well as seasonality (12 months) in the dataset\n\nThe Residuals now seem to be randomly distributed"},{"metadata":{},"cell_type":"markdown","source":"### Choosing the Right Time Series Method"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from PIL import Image\n# import requests\n# from io import BytesIO\n\n# response = requests.get('https://miro.medium.com/max/700/1*G0rTkadf_010ewO7mhOiuA.png')\n# img = Image.open(BytesIO(response.content))\n# img","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Observations based on the dataset and above flowchart:\n1. Number of data points is more than 10, hence we will utilize Exponential Smoothing or ARIMA and we should NOT utilize Simple Moving Average or Naive Methods.\n2. We observe that there is an upward trend in the dataset\n3. We also observe Seasonality (at 12 months) in the dataset\n4. Exponential Smoothing: We should use Simple Exponential Smoothing, Holt's Exponential Smoothing, Holt Winter's Smoothing techniques among others.\n5. We shoule also look at ARIMA: There is an Upward Trend in the dataset, we can use ARIMA techniques. As we can observe there is seasonality as well, we can use SARIMA. However, we don't have any information on Exogenous variables, hence we would NOT use ARIMAX or SARIMAX."},{"metadata":{},"cell_type":"markdown","source":"We need to convert datatype of Order Date in Test series as well (as we did for the Train dataset)"},{"metadata":{"trusted":true},"cell_type":"code","source":"APAC_Consumer_Test = APAC_Consumer_Test.reset_index()\nAPAC_Consumer_Test['Order Date'] = APAC_Consumer_Test['Order Date'].apply(lambda x: x.to_timestamp())\nAPAC_Consumer_Test = APAC_Consumer_Test.set_index(['Order Date'])\nAPAC_Consumer_Test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building and Evaluating Time Series Forecasts"},{"metadata":{},"cell_type":"markdown","source":"## Smoothing Techniques"},{"metadata":{},"cell_type":"markdown","source":"With more than 10 observations depicting a clear upward Trend and Seasonality, we can use following Smoothing techniques:\n1. Simple Exponential Smoothing\n2. Holt’s Exponential Smoothing\n3. Holt Winter’s Additive Smoothing\n4. Holt Winter’s Multiplicative Smoothing"},{"metadata":{},"cell_type":"markdown","source":"### 1. Simple Exponential Smoothing"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.holtwinters import SimpleExpSmoothing\nAPAC_Consumer_SES = APAC_Consumer_Test.copy()\nmodel = SimpleExpSmoothing(APAC_Consumer_Train['Sales'])\nmodel_fit = model.fit(optimized = True)\nmodel_fit.params\nAPAC_Consumer_SES['Sales_Forecast'] = model_fit.forecast(6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plotting the Train, Test and Forecasts"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,4))\nplt.plot(APAC_Consumer_Train['Sales'], label='Train')\nplt.plot(APAC_Consumer_Test['Sales'], label='Test')\nplt.plot(APAC_Consumer_SES['Sales_Forecast'], label='Simple exponential smoothing forecast')\nplt.legend(loc='best')\nplt.title('Simple Exponential Smoothing Method')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Calculating Errors in Forecasted series, using RMSE and MAPE"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nrmse = np.sqrt(mean_squared_error(APAC_Consumer_Test['Sales'], APAC_Consumer_SES['Sales_Forecast'])).round(2)\nmape = np.round(np.mean(np.abs(APAC_Consumer_Test['Sales']-APAC_Consumer_SES['Sales_Forecast'])/APAC_Consumer_Test['Sales'])*100,2)\nresults = pd.DataFrame({'Method':['Simple Exponential Method'], 'RMSE': [rmse],'MAPE': [mape]})\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that Simple Exponential Smoothing gives us a flat estimate for the next six months and does not take care of movements within the months.\n\nThe RMSE is 22,992 and MAPE is 27.7"},{"metadata":{},"cell_type":"markdown","source":"## 2. Holt's Exponential Smoothing"},{"metadata":{},"cell_type":"markdown","source":"### 2.1 Holt's Exponential Smoothing with Trend"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.holtwinters import ExponentialSmoothing\nAPAC_Consumer_HES = APAC_Consumer_Test.copy()\nmodel = ExponentialSmoothing(np.asarray(APAC_Consumer_Train['Sales']) ,seasonal_periods=12 ,trend='additive', seasonal=None)\nmodel_fit = model.fit(optimized=True)\nprint(model_fit.params)\nAPAC_Consumer_HES['Sales_Forecast'] = model_fit.forecast(6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plotting the Train, Test and Forecasts"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,4))\nplt.plot(APAC_Consumer_Train['Sales'], label='Train')\nplt.plot(APAC_Consumer_Test['Sales'], label='Test')\nplt.plot(APAC_Consumer_HES['Sales_Forecast'], label='Holt\\'s Exponential Forecasting')\nplt.legend(loc='best')\nplt.title('Holt\\'s Exponential Smoothing Method')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Calculating Errors in Forecasted series, using RMSE and MAPE"},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse = np.sqrt(mean_squared_error(APAC_Consumer_Test['Sales'], APAC_Consumer_HES['Sales_Forecast'])).round(2)\nmape = np.round(np.mean(np.abs(APAC_Consumer_Test['Sales']-APAC_Consumer_HES['Sales_Forecast'])/APAC_Consumer_Test['Sales'])*100,2)\ntemp_results = pd.DataFrame({'Method':['Holt\\'s Exponential Trend Method'], 'RMSE': [rmse],'MAPE': [mape]})\nresults = pd.concat([results, temp_results])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that Holt’s Exponential Smoothing with Trend gives us an upward trending estimate for the next six months, however, does not take care of movements within the months.\n\nThe RMSE is 17,194 and MAPE is 25.0\n\nHolt’s Exponential Smoothing performs better than Simple Exponential Smoothing."},{"metadata":{},"cell_type":"markdown","source":"### 2.2 Holt Winters' additive method with trend and seasonality"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.holtwinters import ExponentialSmoothing\nAPAC_Consumer_HWA = APAC_Consumer_Test.copy()\nmodel = ExponentialSmoothing(np.asarray(APAC_Consumer_Train['Sales']) ,seasonal_periods=12 ,trend='add', seasonal='add')\nmodel_fit = model.fit(optimized=True)\nprint(model_fit.params)\nAPAC_Consumer_HWA['Sales_Forecast'] = model_fit.forecast(6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plotting the Train, Test and Forecasts"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,4))\nplt.plot(APAC_Consumer_Train['Sales'], label='Train')\nplt.plot(APAC_Consumer_Test['Sales'], label='Test')\nplt.plot(APAC_Consumer_HWA['Sales_Forecast'], label='Holt Winters\\'s additive forecast')\nplt.legend(loc='best')\nplt.title('Holt Winters\\' Additive Method')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Calculating Errors in Forecasted series, using RMSE and MAPE"},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse = np.sqrt(mean_squared_error(APAC_Consumer_Test['Sales'], APAC_Consumer_HWA['Sales_Forecast'])).round(2)\nmape = np.round(np.mean(np.abs(APAC_Consumer_Test['Sales']-APAC_Consumer_HWA['Sales_Forecast'])/APAC_Consumer_Test['Sales'])*100,2)\ntemp_results = pd.DataFrame({'Method':['Holt Winters\\'s Additive Method'], 'RMSE': [rmse],'MAPE': [mape]})\nresults = pd.concat([results, temp_results])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that Holt Winter’s Additive Smoothing with Trend gives us an upward trending estimate for the next six months and takes care of movements within the months.\n\nThe RMSE is 12,971 and MAPE is 17.6\n\nIt is better than both Simple Exponential as well as Holt’s Exponential Smoothing techniques."},{"metadata":{},"cell_type":"markdown","source":"### 2.3 Holt Winters' multiplicative method with trend and seasonality"},{"metadata":{"trusted":true},"cell_type":"code","source":"APAC_Consumer_HWM = APAC_Consumer_Test.copy()\nmodel = ExponentialSmoothing(np.asarray(APAC_Consumer_Train['Sales']) ,seasonal_periods=12 ,trend='add', seasonal='mul')\nmodel_fit = model.fit(optimized=True)\nprint(model_fit.params)\nAPAC_Consumer_HWM['Sales_Forecast'] = model_fit.forecast(6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plotting the Train, Test and Forecasts"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,4))\nplt.plot(APAC_Consumer_Train['Sales'], label='Train')\nplt.plot(APAC_Consumer_Test['Sales'], label='Test')\nplt.plot(APAC_Consumer_HWM['Sales_Forecast'], label='Holt Winters\\'s Multiplicative forecast')\nplt.legend(loc='best')\nplt.title('Holt Winters\\' Multiplicative Method')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Calculating Errors in Forecasted series, using RMSE and MAPE"},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse = np.sqrt(mean_squared_error(APAC_Consumer_Test['Sales'], APAC_Consumer_HWM['Sales_Forecast'])).round(2)\nmape = np.round(np.mean(np.abs(APAC_Consumer_Test['Sales']-APAC_Consumer_HWM['Sales_Forecast'])/APAC_Consumer_Test['Sales'])*100,2)\ntemp_results = pd.DataFrame({'Method':['Holt Winters\\'s Multiplicative Method'], 'RMSE': [rmse],'MAPE': [mape]})\nresults = pd.concat([results, temp_results])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that Holt Winter’s Multiplicative Smoothing with Trend gives us an upward trending estimate for the next six months and take care of movements within the months.\n\nThe RMSE is 11,753 and MAPE is 19.6\n\nIt is better than Simple Exponential and Holt’s Exponential but not as good as Holt Winter’s Additive Smoothing."},{"metadata":{},"cell_type":"markdown","source":"#### Overall, among the 4 Smoothing techniques, Holt Winter’s Additive Method has the least MAPE and hence, the best Smoothing technique for the given Time-Series data."},{"metadata":{},"cell_type":"markdown","source":"### Stationarity Vs. Non-Stationarity in the Time Series"},{"metadata":{},"cell_type":"markdown","source":"Before proceeding towards ARIMA methods, we will check for Stationarity of the Time-Series\n\nIf the Time-Series is not Stationary, we need to transform it in order to make it Stationary\n\nTo validate wether the Time Series possesses Stationarity or not, we will perform couple of statistical tests, namely ADF and KPSS"},{"metadata":{},"cell_type":"markdown","source":"### Augmented Dickey-Fuller (ADF) Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\nadf_test = adfuller(APAC_Consumer['Sales'])\nprint('ADF Statistics: %f' % adf_test[0])\nprint('Critical Value @ 0.05: %.2f' % adf_test[4]['5%'])\nprint('p-value: %f' % adf_test[1])\nprint(adf_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Based on the ADF Test, we observe that that the p-value (~0.20) is more than the critical value of  0.05 hence we fail to reject the null hypothesis i.e., the series is NOT stationary"},{"metadata":{},"cell_type":"markdown","source":"### Kwiatkowski-Phillips-Schmidt-Shin (KPSS) Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import kpss\nkpss_test = kpss(APAC_Consumer['Sales'])\nprint('KPSS Statistics: %f: ' % kpss_test[0])\nprint('Critical Value @ 0.05: %.2f' % kpss_test[3]['5%'])\nprint('p-value: %f' % kpss_test[1])\nprint(kpss_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The KPSS Test results into a p-value of 0.02, which is less than the critical value of 0.05, hence we reject the null hypotheses i.e., the series is NOT stationary"},{"metadata":{},"cell_type":"markdown","source":"We need the series to have Stationarity, hence we need to transform the series. We will use Box Cox Transformation and Differencing for this and will check the Stationarity once again using ADF and KPSS methods."},{"metadata":{},"cell_type":"markdown","source":"## Box Cox Transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"APAC_Consumer.index = APAC_Consumer.index.to_timestamp()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import boxcox\ndata_boxcox = pd.Series(boxcox(APAC_Consumer['Sales'], lmbda = 0), index = APAC_Consumer.index)\nplt.figure(figsize = (16,4))\nplt.plot(data_boxcox, label = 'After Box Cox Transformation')\nplt.legend(loc = 'best')\nplt.title('After Box Cox Transformation')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Differencing"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_boxcox_diff = pd.Series(data_boxcox - data_boxcox.shift())\ndata_boxcox_diff.dropna(inplace = True)\nplt.figure(figsize = (16,4))\nplt.plot(data_boxcox_diff, label = 'After Box Cox Transformation and Differencing')\nplt.legend(loc = 'best')\nplt.title('After Box Cox Transformation and Differencing')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Augmented Dickey-Fuller (ADF) Test post Box Cox Transformation and Differencing"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\nadf_test = adfuller(data_boxcox_diff)\nprint('ADF Statistics: %f' % adf_test[0])\nprint('Critical Value @ 0.05: %.2f' % adf_test[4]['5%'])\nprint('p-value: %f' % adf_test[1])\nprint(adf_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Post Box Cox Transformation and Differencing, the ADF test results in p-value of ~0.00, which is less than the critical value of 0.05 hence we reject the null hypothesis i.e., the series is now stationary"},{"metadata":{},"cell_type":"markdown","source":"### Kwiatkowski-Phillips-Schmidt-Shin (KPSS) Test post Box Cox Transformation and Differencing"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import kpss\nkpss_test = kpss(data_boxcox_diff)\nprint('KPSS Statistics: %f: ' % kpss_test[0])\nprint('Critical Value @ 0.05: %.2f' % kpss_test[3]['5%'])\nprint('p-value: %f' % kpss_test[1])\nprint(kpss_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Post Box Cox Transformation and Differencing, the KPSS test results in p-value of 0.10, which is more than the critical value of 0.05 hence we fail to reject the null hypotheses i.e., the series is now stationary"},{"metadata":{},"cell_type":"markdown","source":"Let's have a quick look at the ACF and PACF plots to visualize Autocorrelations"},{"metadata":{},"cell_type":"markdown","source":"#### Autocorrelation Function (ACF) Plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.graphics.tsaplots import plot_acf\nplt.figure(figsize = (16,4))\nplot_acf(data_boxcox_diff, ax = plt.gca(), lags = 30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Partical Autocorrelation (PACF) Plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.graphics.tsaplots import plot_pacf\nplt.figure(figsize = (16,4))\nplot_pacf(data_boxcox_diff, ax = plt.gca(), lags = 30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Auto Regressive Methods"},{"metadata":{},"cell_type":"markdown","source":"### 3. Auto Regressive Models"},{"metadata":{},"cell_type":"markdown","source":"With more than 10 observations depicting a clear upward Trend and Seasonality, we will be using following ARIMA techniques:\n1. Auto Regressive (AR) Smoothing\n2. Moving Average (MA) Smoothing\n3. Auto Regressive Moving Average (ARMA) Smoothing\n4. Auto Regressive Integrated Moving Average (ARIMA) Smoothing\n5. Seasonal Auto Regressive Integrated Moving Average (SARIMA) Smoothing"},{"metadata":{},"cell_type":"markdown","source":"We will split the boxcox series into train and test series"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_boxcox = data_boxcox[:train_len]\ntest_data_boxcox = data_boxcox[train_len:]\ntrain_data_boxcox_diff = data_boxcox_diff[:train_len-1]\ntest_data_boxcox_diff = data_boxcox_diff[train_len-1:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.1 Auto Regression (AR) Method"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.arima_model import ARIMA\nmodel = ARIMA(train_data_boxcox_diff, order = (1,0,0))\nmodel_fit = model.fit()\nprint(model_fit.params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Recover Original Time Series Forecast"},{"metadata":{"trusted":true},"cell_type":"code","source":"APAC_Consumer_AR = data_boxcox_diff.copy()\nAPAC_Consumer_AR['AR_Sales_Forecast_Boxcox_Diff'] = model_fit.predict(data_boxcox_diff.index.min(), data_boxcox_diff.index.max())\nAPAC_Consumer_AR['AR_Sales_Forecast_Boxcox'] = APAC_Consumer_AR['AR_Sales_Forecast_Boxcox_Diff'].cumsum()\nAPAC_Consumer_AR['AR_Sales_Forecast_Boxcox'] = APAC_Consumer_AR['AR_Sales_Forecast_Boxcox'].add(data_boxcox[0])\nAPAC_Consumer_AR['AR_Sales_Forecast'] = np.exp(APAC_Consumer_AR['AR_Sales_Forecast_Boxcox'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plotting the Train, Test and Forecasts"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,4))\nplt.plot(APAC_Consumer_Train['Sales'], label='Train')\nplt.plot(APAC_Consumer_Test['Sales'], label='Test')\nplt.plot(APAC_Consumer_AR['AR_Sales_Forecast'][APAC_Consumer_Test.index.min():], label='Auto Regression (AR) forecast')\nplt.legend(loc='best')\nplt.title('AR (Auto Regression) forecast')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Calculating Errors in Forecasted series, using RMSE and MAPE"},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse = np.sqrt(mean_squared_error(APAC_Consumer_Test['Sales'], APAC_Consumer_AR['AR_Sales_Forecast'][APAC_Consumer_Test.index.min():])).round(2)\nmape = np.round(np.mean(np.abs(APAC_Consumer_Test['Sales']-APAC_Consumer_AR['AR_Sales_Forecast'][APAC_Consumer_Test.index.min():])/APAC_Consumer_Test['Sales'])*100,2)\ntemp_results = pd.DataFrame({'Method':['AR (Auto Regression) Method'], 'RMSE': [rmse],'MAPE': [mape]})\nresults = pd.concat([results, temp_results])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that AR Forecast gives us an upward trending estimate for the next six months and take care of movements within the months.\n\nThe RMSE is 15,505 and MAPE is 27.3"},{"metadata":{},"cell_type":"markdown","source":"#### 3.2 Moving Average (MA) Method"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ARIMA(train_data_boxcox_diff, order = (0, 0, 1))\nmodel_fit = model.fit()\nprint(model_fit.params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Recover Original Time Series Forecast"},{"metadata":{"trusted":true},"cell_type":"code","source":"APAC_Consumer_MA = data_boxcox_diff.copy()\nAPAC_Consumer_MA['MA_Sales_Forecast_Boxcox_Diff'] = model_fit.predict(data_boxcox_diff.index.min(), data_boxcox_diff.index.max())\nAPAC_Consumer_MA['MA_Sales_Forecast_Boxcox'] = APAC_Consumer_MA['MA_Sales_Forecast_Boxcox_Diff'].cumsum()\nAPAC_Consumer_MA['MA_Sales_Forecast_Boxcox'] = APAC_Consumer_MA['MA_Sales_Forecast_Boxcox'].add(data_boxcox[0])\nAPAC_Consumer_MA['MA_Sales_Forecast'] = np.exp(APAC_Consumer_MA['MA_Sales_Forecast_Boxcox'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plotting the Train, Test and Forecasts"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,4))\nplt.plot(APAC_Consumer_Train['Sales'], label='Train')\nplt.plot(APAC_Consumer_Test['Sales'], label='Test')\nplt.plot(APAC_Consumer_MA['MA_Sales_Forecast'][APAC_Consumer_Test.index.min():], label='Moving Average (MA) forecast')\nplt.legend(loc='best')\nplt.title('MA (Moving Average) forecast')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Calculating Errors in Forecasted series, using RMSE and MAPE"},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse = np.sqrt(mean_squared_error(APAC_Consumer_Test['Sales'], APAC_Consumer_MA['MA_Sales_Forecast'][APAC_Consumer_Test.index.min():])).round(2)\nmape = np.round(np.mean(np.abs(APAC_Consumer_Test['Sales']-APAC_Consumer_MA['MA_Sales_Forecast'][APAC_Consumer_Test.index.min():])/APAC_Consumer_Test['Sales'])*100,2)\ntemp_results = pd.DataFrame({'Method':['MA (Moving Average) Method'], 'RMSE': [rmse],'MAPE': [mape]})\nresults = pd.concat([results, temp_results])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that MA Forecast gives us an upward trending estimate for the next six months and take care of movements within the months.\n\nThe RMSE is 52,903 and MAPE is 81.6\n\nThe MA forecast has a very high MAPE and is not as good as AR method."},{"metadata":{},"cell_type":"markdown","source":"#### 3.3 Auto Regressive Moving Average (ARMA) Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ARIMA(train_data_boxcox_diff, order = (1, 0, 1))\nmodel_fit = model.fit()\nprint(model_fit.params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Recover Original Time Series Forecast"},{"metadata":{"trusted":true},"cell_type":"code","source":"APAC_Consumer_ARMA = data_boxcox_diff.copy()\nAPAC_Consumer_ARMA['ARMA_Sales_Forecast_Boxcox_Diff'] = model_fit.predict(data_boxcox_diff.index.min(), data_boxcox_diff.index.max())\nAPAC_Consumer_ARMA['ARMA_Sales_Forecast_Boxcox'] = APAC_Consumer_ARMA['ARMA_Sales_Forecast_Boxcox_Diff'].cumsum()\nAPAC_Consumer_ARMA['ARMA_Sales_Forecast_Boxcox'] = APAC_Consumer_ARMA['ARMA_Sales_Forecast_Boxcox'].add(data_boxcox[0])\nAPAC_Consumer_ARMA['ARMA_Sales_Forecast'] = np.exp(APAC_Consumer_ARMA['ARMA_Sales_Forecast_Boxcox'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plotting the Train, Test and Forecasts"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,4))\nplt.plot(APAC_Consumer_Train['Sales'], label='Train')\nplt.plot(APAC_Consumer_Test['Sales'], label='Test')\nplt.plot(APAC_Consumer_ARMA['ARMA_Sales_Forecast'][APAC_Consumer_Test.index.min():], label='Auto Regressive Moving Average (MA) forecast')\nplt.legend(loc='best')\nplt.title('ARMA (Auto Regressive Moving Average) forecast')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Calculating Errors in Forecasted series, using RMSE and MAPE"},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse = np.sqrt(mean_squared_error(APAC_Consumer_Test['Sales'], APAC_Consumer_ARMA['ARMA_Sales_Forecast'][APAC_Consumer_Test.index.min():])).round(2)\nmape = np.round(np.mean(np.abs(APAC_Consumer_Test['Sales']-APAC_Consumer_ARMA['ARMA_Sales_Forecast'][APAC_Consumer_Test.index.min():])/APAC_Consumer_Test['Sales'])*100,2)\ntemp_results = pd.DataFrame({'Method':['ARMA (Auto Regressive Moving Average) Method'], 'RMSE': [rmse],'MAPE': [mape]})\nresults = pd.concat([results, temp_results])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that ARMA Forecast gives us an upward trending estimate for the next six months and take care of movements within the months.\n\nThe RMSE is 50,757 and MAPE is 77.7\n\nARMA is better than MA method but not as good as AR method."},{"metadata":{},"cell_type":"markdown","source":"#### 3.4 Auto Regressive Integrated Moving Average (ARIMA) Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We would pass on data_boxcox and not data_boxcox_diff because ARIMA takes care of differencing\nmodel = ARIMA(train_data_boxcox, order = (1, 1, 1))\nmodel_fit = model.fit()\nprint(model_fit.params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Recover Original Time Series Forecast"},{"metadata":{"trusted":true},"cell_type":"code","source":"APAC_Consumer_ARIMA = data_boxcox_diff.copy()\nAPAC_Consumer_ARIMA['ARIMA_Sales_Forecast_Boxcox_Diff'] = model_fit.predict(data_boxcox_diff.index.min(), data_boxcox_diff.index.max())\nAPAC_Consumer_ARIMA['ARIMA_Sales_Forecast_Boxcox'] = APAC_Consumer_ARIMA['ARIMA_Sales_Forecast_Boxcox_Diff'].cumsum()\nAPAC_Consumer_ARIMA['ARIMA_Sales_Forecast_Boxcox'] = APAC_Consumer_ARIMA['ARIMA_Sales_Forecast_Boxcox'].add(data_boxcox[0])\nAPAC_Consumer_ARIMA['ARIMA_Sales_Forecast'] = np.exp(APAC_Consumer_ARIMA['ARIMA_Sales_Forecast_Boxcox'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plotting the Train, Test and Forecasts"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,4))\nplt.plot(APAC_Consumer_Train['Sales'], label='Train')\nplt.plot(APAC_Consumer_Test['Sales'], label='Test')\nplt.plot(APAC_Consumer_ARIMA['ARIMA_Sales_Forecast'][APAC_Consumer_Test.index.min():], label='Auto Regressive Integrated Moving Average (MA) forecast')\nplt.legend(loc='best')\nplt.title('ARIMA (Auto Regressive Integrated Moving Average) forecast')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Calculating Errors in Forecasted series, using RMSE and MAPE"},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse = np.sqrt(mean_squared_error(APAC_Consumer_Test['Sales'], APAC_Consumer_ARIMA['ARIMA_Sales_Forecast'][APAC_Consumer_Test.index.min():])).round(2)\nmape = np.round(np.mean(np.abs(APAC_Consumer_Test['Sales']-APAC_Consumer_ARIMA['ARIMA_Sales_Forecast'][APAC_Consumer_Test.index.min():])/APAC_Consumer_Test['Sales'])*100,2)\ntemp_results = pd.DataFrame({'Method':['ARIMA (Auto Regressive Integrated Moving Average) Method'], 'RMSE': [rmse],'MAPE': [mape]})\nresults = pd.concat([results, temp_results])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that ARIMA Forecast gives us an upward trending estimate for the next six months and take care of movements within the months.\n\nThe RMSE is 50,757 and MAPE is 77.7\n\nARIMA has the same output as ARMA and hence, is as good as ARMA method.\n\nARIMA is better than MA method but not as good as AR method."},{"metadata":{},"cell_type":"markdown","source":"#### 4.5 Seasonal Auto Regressive Integrated Moving Average (SARIMA) Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.statespace.sarimax import SARIMAX\nmodel = SARIMAX(train_data_boxcox, order = (1, 1, 1), seasonal_order = (1,1,1,12))\nmodel_fit = model.fit()\nprint(model_fit.params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Recover Original Time Series Forecast"},{"metadata":{"trusted":true},"cell_type":"code","source":"APAC_Consumer_SARIMA = data_boxcox_diff.copy()\n# Integration is already taken care of in SARIMAX (unlike ARIMA)\nAPAC_Consumer_SARIMA['SARIMA_Sales_Forecast_Boxcox'] = model_fit.predict(data_boxcox_diff.index.min(), data_boxcox_diff.index.max())\nAPAC_Consumer_SARIMA['SARIMA_Sales_Forecast'] = np.exp(APAC_Consumer_SARIMA['SARIMA_Sales_Forecast_Boxcox'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plotting the Train, Test and Forecasts"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,4))\nplt.plot(APAC_Consumer_Train['Sales'], label='Train')\nplt.plot(APAC_Consumer_Test['Sales'], label='Test')\nplt.plot(APAC_Consumer_SARIMA['SARIMA_Sales_Forecast'][APAC_Consumer_Test.index.min():], label='SARIMA (Seasonal Auto Regressive Integrated Moving Average) forecast')\nplt.legend(loc='best')\nplt.title('Seasonal Auto Regressive Integrated Moving Average (MA) forecast')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Calculating Errors in Forecasted series, using RMSE and MAPE"},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse = np.sqrt(mean_squared_error(APAC_Consumer_Test['Sales'], APAC_Consumer_SARIMA['SARIMA_Sales_Forecast'][APAC_Consumer_Test.index.min():])).round(2)\nmape = np.round(np.mean(np.abs(APAC_Consumer_Test['Sales']-APAC_Consumer_SARIMA['SARIMA_Sales_Forecast'][APAC_Consumer_Test.index.min():])/APAC_Consumer_Test['Sales'])*100,2)\ntemp_results = pd.DataFrame({'Method':['SARIMA (Seasonal Auto Regressive Integrated Moving Average) Method'], 'RMSE': [rmse],'MAPE': [mape]})\nresults = pd.concat([results, temp_results])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that SARIMA Forecast gives us an upward trending estimate for the next six months and take care of movements within the months as well as Seasonality.\n\nThe RMSE is 11,179 and MAPE is 18.4\n\nSARIMA has lowest MAPE value among AR, MA, ARMA, ARIMA and SARIMA."},{"metadata":{},"cell_type":"markdown","source":"#### Overall, among the 5 ARIMA techniques, SARIMA Forecasting has the least MAPE and hence, the best Forecasting technique for the give Time-Series data."},{"metadata":{},"cell_type":"markdown","source":"## Overall, the best forecasting methods are as below (based on their MAPE values):\n### For Smoothing method, the best option is Holt Winter's Additive Method\n### For Auto-Regressive methods, the best option is SARIMA"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}