{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1>Tesla stock data from 2010 to 2020</h1>\n<h3>Fast LSTM one-day prediction model build</h3>\n<p>This notebook was created in order to show how to biuld an easy predictive model for the stock prices using the LSTM neural network.\n<p><u>Constraints:</u></p>\n<ul>\n    <li>7 prior days will be used for prediction</li>\n    <li>1 day ahead will be predicted</li>\n    <li>No additional model tuning will be performed</li>\n    <li>Target variable: Open price value</li>\n    <li>This is just brief LSTM model building, don't take it close</li>\n    <li>Upvote if you liked this notebook</li>\n    \n</ul>\n"},{"metadata":{},"cell_type":"markdown","source":"First, let's import all necessary libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport datetime\nimport math\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras import backend as K\nfrom keras.callbacks import ModelCheckpoint\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\n!pip install mplfinance\nimport mplfinance as mpf\nplt.style.use('seaborn-deep')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Afterwards, let's load and then perform additional mangling on the data:\n1. Parse Date column\n2. Set a Date column as index\n3. Rename feature columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"def parse(x):\n    return datetime.datetime.strptime(x, '%Y-%m-%d')\n\ndf = pd.read_csv('../input/tesla-stock-data-from-2010-to-2020/TSLA.csv',  parse_dates = True, index_col=0, date_parser=parse)\ndf.columns = ['Open', 'High', 'Low', 'Close', 'Adj_close', 'Volume']\ndf.index.name = 'Date'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('First observation date: %s \\nLast observation date: %s' % (df.index[0].date(), df.index[-1].date()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.var(df.Close - df.Adj_close))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, 'Close' and 'Adj_close' are identical, therefore we can boldly drop one of them."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['Adj_close'], inplace = True, axis = 1)\nc_names = [name for name in df.columns]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below is just a candlestick plot drawn by <b>mplfinance</b> library (wanted to test it). The development of this package is still in progress so for now there is a lack of some functions like plot resizing and ticks managing."},{"metadata":{"trusted":true},"cell_type":"code","source":"mpf.plot(df.iloc[-50:,:], type='candle', volume=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"values = df.values\n\nnum_f = len(df.columns)\n\ngroups = [x for x in range(num_f)]\n\nplt.figure(figsize = (12,16))\n\ni = 1\nfor group in groups:\n    plt.subplot(len(groups), 1, i)\n    plt.plot(values[:, group])\n    plt.title(df.columns[group], y=0.85, loc='center')\n    i += 1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we see that last days Tesla Stocks experienced exploding growth. This behaviour hasn't occurred previously and it is interesting how the LSTM model will predict the day next to the historical dataset final date."},{"metadata":{"trusted":true},"cell_type":"code","source":"def series_to_supervised(data, c_names, n_in=1, n_out=1, dropnan=True):\n    '''\n    This function reformats the dataset the way it can be fed to the LSTM.\n    '''\n    n_vars = 1 if type(data) is list else data.shape[1]\n   \n    df = pd.DataFrame(data)\n    cols, names = list(), list()\n    \n    # input sequence (t-n, ... t-1)\n    for i in range(n_in, 0, -1):\n\n        cols.append(df.shift(i))\n        names += ['%s(t-%d)' % (n, i) for n in c_names]\n    \n    # forecast sequence (t, t+1, ... t+n)\n    for i in range(0, n_out):\n        cols.append(df.shift(-i))\n        if i == 0:\n            names += [('%s(t)' % n) for n in c_names]\n        else:\n            names += [('%s(t+%d)' % (n, i)) for n in c_names]\n   \n    agg = pd.concat(cols, axis=1)\n    agg.columns = names\n    \n    if dropnan:\n        agg.dropna(inplace=True)\n    return agg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def root_mean_squared_error(y_true, y_pred):\n        return K.sqrt(K.mean(K.square(y_pred - y_true)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ensure all data is float\nvalues = values.astype('float32')\n\n# normalize features\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(values)\n\nn_days_back = 21\nn_days_future = 1\nn_features = num_f\n\n# frame as supervised learning\nreframed = series_to_supervised(scaled, c_names, n_days_back, n_days_future)\n\nprint(reframed.head(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That way our dataset is transformed the way it has 5 inputs for 21 days back to predict one day ahead."},{"metadata":{},"cell_type":"markdown","source":"Now, let's split the dataset into training and testing parts and then into X and y parts."},{"metadata":{"trusted":true},"cell_type":"code","source":"n_obs = n_days_back * n_features\n\ntarget_idx = [reframed.columns.to_list().index(col) for col in reframed.columns[n_obs:] if 'Open' in col]\n\n\n# split into train and test sets\nvalues = reframed.values\n\nn_train_days = 2000\n\ntrain = values[:n_train_days, :]\ntest = values[n_train_days:, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X, train_y = train[:, :n_obs], train[:, target_idx]\ntest_X, test_y = test[:, :n_obs], test[:, target_idx]\n\n# reshape input to fit the LSTM network requirements: [n_samples, window, n_features]\ntrain_X = train_X.reshape((train_X.shape[0], n_days_back, n_features))\ntest_X = test_X.reshape((test_X.shape[0], n_days_back, n_features))\n\nprint(train_X.shape, train_y.shape, test_X.shape, test_y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This part is responsible for modeling and model training:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# design network\nmodel = Sequential()\nmodel.add(LSTM(150, input_shape=(train_X.shape[1], train_X.shape[2])))\nmodel.add(Dense(train_y.shape[1]))\n\ncheckpoint = ModelCheckpoint('w.hdf5', monitor='val_loss', save_best_only=True)\n\ncallback_list = [checkpoint]\n\nmodel.compile(optimizer = 'adam', loss = root_mean_squared_error)\n\nt = model.fit(train_X, train_y, epochs=50,\n              batch_size=32,\n              validation_data=(test_X, test_y),\n              verbose=1,\n              callbacks = callback_list,\n              shuffle=False)\n\nmodel.load_weights('w.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot training history\nplt.figure(figsize = (9,7))\nplt.title('Training Loss / Validation Loss')\nplt.plot(t.history['loss'], 'tab:red', label='Training loss')\nplt.plot(t.history['val_loss'], 'tab:blue', label='Validation loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's make a prediction and make all together."},{"metadata":{"trusted":true},"cell_type":"code","source":"# make a prediction\nyhat = model.predict(test_X)\n\n# invert scaling\nyhat_inv = yhat / scaler.scale_[0]\ny_inv = test_y / scaler.scale_[0]\n\n# reshape back\nyhat_inv_rshp = yhat_inv.reshape((-1,1))\ny_inv_rshp = y_inv.reshape((-1,1))\n\n# calculate RMSE\nrmse = math.sqrt(mean_squared_error(y_inv_rshp, yhat_inv_rshp))\nprint('Test set RMSE: %.2f' % rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (18,7))\nplt.title('Real data vs Predicted on the Test set')\nplt.plot(y_inv_rshp, 'tab:green', label='Real data')\nplt.plot(yhat_inv_rshp, 'tab:blue', label='Predicted')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Initial set size: {df.shape[0]}')\nprint(f'Train set X size: {train_X.shape[0]}, train set y size: {train_y.shape[0]}')\nprint(f'Test set X size: {test_X.shape[0]}, test set y size: {test_y.shape[0]}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I'm gonna predict the Open price value that follows the final date in the dataset and compare it with the real value."},{"metadata":{"trusted":true},"cell_type":"code","source":"reframed.tail(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"last_day = reframed.iloc[-1:, -n_obs:]\nlast_day = last_day.values\nlast_day = last_day.reshape((-1, n_days_back, n_features))\n\nt_plus_one = model.predict(last_day)\nt_plus_one /= scaler.scale_[0]\nprint('Last observation\\'s date in the dateset is %s\\nLast observation\\'s value is %.2f' % (df.index[-1].date(), df.iloc[-1, 0]))\nprint('\\nPredicted observation\\'s date is %s\\nPredicted observation\\'s value is %.2f' % (df.index[-1].date() + datetime.timedelta(days=1), t_plus_one))\nprint('\\nReal value for %s is %.2f' % (df.index[-1].date() + datetime.timedelta(days=1), 882.96))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Conslusion</b>"},{"metadata":{},"cell_type":"markdown","source":"Even though the LSTM model showed quite feasible performance on the test set, it's prediction for the out-of-dataset value has a very big error. This discrepancy I could explain that there is an extremum value in the very last day of the dataset and the model, in general, has never faced such exploding growth before.<br><br>\nNonetheless, it predicted the growth, not decrease and I'm satisfied with that too."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}