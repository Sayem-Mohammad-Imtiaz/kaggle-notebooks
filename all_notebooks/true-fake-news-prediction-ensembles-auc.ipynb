{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Installing and importing relevant libraries","metadata":{}},{"cell_type":"code","source":"!pip3 install xgboost","metadata":{"execution":{"iopub.status.busy":"2021-05-20T13:52:55.336177Z","iopub.execute_input":"2021-05-20T13:52:55.336547Z","iopub.status.idle":"2021-05-20T13:53:04.818979Z","shell.execute_reply.started":"2021-05-20T13:52:55.336515Z","shell.execute_reply":"2021-05-20T13:53:04.817489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom wordcloud import WordCloud, STOPWORDS\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nimport string\nfrom scipy.sparse import csr_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve","metadata":{"execution":{"iopub.status.busy":"2021-05-20T13:53:04.821834Z","iopub.execute_input":"2021-05-20T13:53:04.822338Z","iopub.status.idle":"2021-05-20T13:53:08.838061Z","shell.execute_reply.started":"2021-05-20T13:53:04.822288Z","shell.execute_reply":"2021-05-20T13:53:08.836814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-20T13:53:08.842397Z","iopub.execute_input":"2021-05-20T13:53:08.84273Z","iopub.status.idle":"2021-05-20T13:53:08.861953Z","shell.execute_reply.started":"2021-05-20T13:53:08.842699Z","shell.execute_reply":"2021-05-20T13:53:08.860772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_df = pd.read_csv(\"../input/fake-and-real-news-dataset/True.csv\")\nfake_df = pd.read_csv(\"../input/fake-and-real-news-dataset/Fake.csv\")\nfake_df.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-05-20T13:53:08.867035Z","iopub.execute_input":"2021-05-20T13:53:08.867379Z","iopub.status.idle":"2021-05-20T13:53:11.247376Z","shell.execute_reply.started":"2021-05-20T13:53:08.867349Z","shell.execute_reply":"2021-05-20T13:53:11.246436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"fake_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T13:53:11.251259Z","iopub.execute_input":"2021-05-20T13:53:11.251598Z","iopub.status.idle":"2021-05-20T13:53:11.279958Z","shell.execute_reply.started":"2021-05-20T13:53:11.251569Z","shell.execute_reply":"2021-05-20T13:53:11.2786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T13:53:11.281908Z","iopub.execute_input":"2021-05-20T13:53:11.28239Z","iopub.status.idle":"2021-05-20T13:53:11.306516Z","shell.execute_reply.started":"2021-05-20T13:53:11.282352Z","shell.execute_reply":"2021-05-20T13:53:11.305377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#concatenate true and fake df\n# True == 1 , fake == 0\ntrue_df[\"label\"] = 1\nfake_df[\"label\"] = 0\nframes = [true_df, fake_df]\ncombined_df = pd.concat(frames)\ncombined_df.head()\n# combined_df.tail()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T13:53:11.308113Z","iopub.execute_input":"2021-05-20T13:53:11.308478Z","iopub.status.idle":"2021-05-20T13:53:11.337685Z","shell.execute_reply.started":"2021-05-20T13:53:11.308446Z","shell.execute_reply":"2021-05-20T13:53:11.336505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great, we can see that there are no null values within columns across both dataset, and a total of 23841 records for fake news, and 21417 records for true. The dataset is relatively balanced.\n\nLet's next take a look at the whether there are duplicate values ","metadata":{}},{"cell_type":"code","source":"print(\"********** fake news dataset **********\")\nprint(f\"Number of unique titles: \"+str(fake_df[\"title\"].nunique()))\nprint(f\"Number of unique text: \" +str(fake_df[\"text\"].nunique()))\nprint(f\"Number of unique subjects:\" +str(fake_df[\"subject\"].nunique()))\nprint(\"\\n\")\nprint(\"********** true news dataset **********\")\nprint(f\"Number of unique titles: \"+str(true_df[\"title\"].nunique()))\nprint(f\"Number of unique text: \" +str(true_df[\"text\"].nunique()))\nprint(f\"Number of unique subjects:\" +str(true_df[\"subject\"].nunique()))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T13:53:11.339249Z","iopub.execute_input":"2021-05-20T13:53:11.339622Z","iopub.status.idle":"2021-05-20T13:53:11.700536Z","shell.execute_reply.started":"2021-05-20T13:53:11.339583Z","shell.execute_reply":"2021-05-20T13:53:11.699342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"True news subjects: \")\nprint(true_df[\"subject\"].unique())\nprint(\"\\n\")\nprint(\"Fake news subjects: \")\nprint(fake_df[\"subject\"].unique())","metadata":{"execution":{"iopub.status.busy":"2021-05-20T13:53:11.701809Z","iopub.execute_input":"2021-05-20T13:53:11.702106Z","iopub.status.idle":"2021-05-20T13:53:11.713582Z","shell.execute_reply.started":"2021-05-20T13:53:11.702077Z","shell.execute_reply":"2021-05-20T13:53:11.712446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_df = combined_df.loc[:,[\"text\",\"label\"]]\ncombined_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T13:53:11.715111Z","iopub.execute_input":"2021-05-20T13:53:11.715482Z","iopub.status.idle":"2021-05-20T13:53:11.743857Z","shell.execute_reply.started":"2021-05-20T13:53:11.715449Z","shell.execute_reply":"2021-05-20T13:53:11.742562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Wordcloud for text for true news\")\nwordcloud2 = WordCloud(\n                          background_color='white',\n                          stopwords=STOPWORDS,\n                          max_words=200,\n                          max_font_size=40, \n                          random_state=42\n                         ).generate(str(true_df['text']))\n\nplt.imshow(wordcloud2)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T13:53:11.746805Z","iopub.execute_input":"2021-05-20T13:53:11.747305Z","iopub.status.idle":"2021-05-20T13:53:11.979531Z","shell.execute_reply.started":"2021-05-20T13:53:11.747252Z","shell.execute_reply":"2021-05-20T13:53:11.978268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Wordcloud for text for fake news\")\nwordcloud2 = WordCloud(\n                          background_color='white',\n                          stopwords=STOPWORDS,\n                          max_words=200,\n                          max_font_size=40, \n                          random_state=42\n                         ).generate(str(fake_df['text']))\n\nplt.imshow(wordcloud2)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T13:53:11.980995Z","iopub.execute_input":"2021-05-20T13:53:11.981341Z","iopub.status.idle":"2021-05-20T13:53:12.288277Z","shell.execute_reply.started":"2021-05-20T13:53:11.981309Z","shell.execute_reply":"2021-05-20T13:53:12.28729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Preprocessing","metadata":{}},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T13:53:12.289671Z","iopub.execute_input":"2021-05-20T13:53:12.290059Z","iopub.status.idle":"2021-05-20T13:53:12.294159Z","shell.execute_reply.started":"2021-05-20T13:53:12.290029Z","shell.execute_reply":"2021-05-20T13:53:12.293304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_article = combined_df.iloc[0,0]\ntest_article","metadata":{"execution":{"iopub.status.busy":"2021-05-20T13:53:12.295617Z","iopub.execute_input":"2021-05-20T13:53:12.29592Z","iopub.status.idle":"2021-05-20T13:53:12.314319Z","shell.execute_reply.started":"2021-05-20T13:53:12.29589Z","shell.execute_reply":"2021-05-20T13:53:12.313254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"contraction_map = {\n\"ain't\": \"is not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"I'd\": \"I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I will\",\n\"I'll've\": \"I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"i'd\": \"i would\",\n\"i'd've\": \"i would have\",\n\"i'll\": \"i will\",\n\"i'll've\": \"i will have\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as\",\n\"that'd\": \"that would\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what'll've\": \"what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"when's\": \"when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who will\",\n\"who'll've\": \"who will have\",\n\"who's\": \"who is\",\n\"who've\": \"who have\",\n\"why's\": \"why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you will\",\n\"you'll've\": \"you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\",\n}","metadata":{"execution":{"iopub.status.busy":"2021-05-20T13:53:12.315983Z","iopub.execute_input":"2021-05-20T13:53:12.316352Z","iopub.status.idle":"2021-05-20T13:53:12.341996Z","shell.execute_reply.started":"2021-05-20T13:53:12.31632Z","shell.execute_reply":"2021-05-20T13:53:12.340654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.1. Lowercasing","metadata":{}},{"cell_type":"code","source":"def uncapitalize(article):\n    return article.lower()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T13:53:12.343685Z","iopub.execute_input":"2021-05-20T13:53:12.343974Z","iopub.status.idle":"2021-05-20T13:53:12.361537Z","shell.execute_reply.started":"2021-05-20T13:53:12.343944Z","shell.execute_reply":"2021-05-20T13:53:12.360368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 Expanding abbreviations","metadata":{}},{"cell_type":"code","source":"def expand_abbr(article):\n    new_article = article\n    for item in contraction_map:\n        if item in article:\n            new_article = article.replace(item,contraction_map[item])\n    return new_article","metadata":{"execution":{"iopub.status.busy":"2021-05-20T13:53:12.363438Z","iopub.execute_input":"2021-05-20T13:53:12.363876Z","iopub.status.idle":"2021-05-20T13:53:12.379668Z","shell.execute_reply.started":"2021-05-20T13:53:12.363828Z","shell.execute_reply":"2021-05-20T13:53:12.378353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3 Removing stopwords","metadata":{}},{"cell_type":"code","source":"def remove_stopwords(article):\n    word_tokens = word_tokenize(article)\n    filtered_article = []\n    for tok in word_tokens:\n        if tok not in STOP_WORDS:\n            filtered_article.append(lemmatizer.lemmatize(tok))\n#     filtered_article = [w for w in word_tokens if not w in stop_words]\n    return filtered_article","metadata":{"execution":{"iopub.status.busy":"2021-05-20T13:53:12.381486Z","iopub.execute_input":"2021-05-20T13:53:12.381801Z","iopub.status.idle":"2021-05-20T13:53:12.394377Z","shell.execute_reply.started":"2021-05-20T13:53:12.381772Z","shell.execute_reply":"2021-05-20T13:53:12.393231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_punctuation = set(string.punctuation)\nall_punctuation.add(\"...\")\nall_punctuation.add('’')\nall_punctuation.add('-')\nall_punctuation.add('“')\nall_punctuation.add('[')\nall_punctuation.add(']')","metadata":{"execution":{"iopub.status.busy":"2021-05-20T13:53:12.395986Z","iopub.execute_input":"2021-05-20T13:53:12.396343Z","iopub.status.idle":"2021-05-20T13:53:12.409428Z","shell.execute_reply.started":"2021-05-20T13:53:12.396312Z","shell.execute_reply":"2021-05-20T13:53:12.408199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.4 Removing punctuation","metadata":{}},{"cell_type":"code","source":"def remove_punctuation(token_list):\n    new_list = []\n    for tok in token_list:\n        if tok not in all_punctuation:\n            new_list.append(tok)\n    return new_list","metadata":{"execution":{"iopub.status.busy":"2021-05-20T13:53:12.412587Z","iopub.execute_input":"2021-05-20T13:53:12.413065Z","iopub.status.idle":"2021-05-20T13:53:12.427777Z","shell.execute_reply.started":"2021-05-20T13:53:12.413032Z","shell.execute_reply":"2021-05-20T13:53:12.426545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_df[\"text\"] = combined_df[\"text\"].apply(uncapitalize)\ncombined_df[\"text\"] = combined_df[\"text\"].apply(expand_abbr)\ncombined_df[\"text\"] = combined_df[\"text\"].apply(remove_stopwords)\ncombined_df[\"text\"] = combined_df[\"text\"].apply(remove_punctuation)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T13:53:12.429481Z","iopub.execute_input":"2021-05-20T13:53:12.429906Z","iopub.status.idle":"2021-05-20T13:58:53.796374Z","shell.execute_reply.started":"2021-05-20T13:53:12.429872Z","shell.execute_reply":"2021-05-20T13:58:53.795268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_df[\"text\"] = combined_df[\"text\"].apply(lambda x:\" \".join(x))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T13:58:53.798266Z","iopub.execute_input":"2021-05-20T13:58:53.798804Z","iopub.status.idle":"2021-05-20T13:58:54.451111Z","shell.execute_reply.started":"2021-05-20T13:58:53.79875Z","shell.execute_reply":"2021-05-20T13:58:54.449978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Wordcloud for text after removing stopwords\")\nwordcloud2 = WordCloud(\n                          background_color='white',\n                          stopwords=STOPWORDS,\n                          max_words=200,\n                          max_font_size=40, \n                          random_state=42\n                         ).generate(str(combined_df['text']))\n\nplt.imshow(wordcloud2)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T13:58:54.452473Z","iopub.execute_input":"2021-05-20T13:58:54.452765Z","iopub.status.idle":"2021-05-20T13:58:54.648036Z","shell.execute_reply.started":"2021-05-20T13:58:54.452731Z","shell.execute_reply":"2021-05-20T13:58:54.646952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T13:58:54.650154Z","iopub.execute_input":"2021-05-20T13:58:54.6506Z","iopub.status.idle":"2021-05-20T13:58:54.663192Z","shell.execute_reply.started":"2021-05-20T13:58:54.650552Z","shell.execute_reply":"2021-05-20T13:58:54.661977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = combined_df[\"text\"]\ny = combined_df[\"label\"]","metadata":{"execution":{"iopub.status.busy":"2021-05-20T13:58:54.664782Z","iopub.execute_input":"2021-05-20T13:58:54.665201Z","iopub.status.idle":"2021-05-20T13:58:54.680386Z","shell.execute_reply.started":"2021-05-20T13:58:54.665168Z","shell.execute_reply":"2021-05-20T13:58:54.678963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Getting our train and test sets (TF-IDF vectors)","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nvectorizer = TfidfVectorizer(max_features=1000)\nX_train = vectorizer.fit_transform(X_train).toarray()\nX_test = vectorizer.transform(X_test).toarray()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T13:58:54.682178Z","iopub.execute_input":"2021-05-20T13:58:54.682503Z","iopub.status.idle":"2021-05-20T13:59:09.344703Z","shell.execute_reply.started":"2021-05-20T13:58:54.682474Z","shell.execute_reply":"2021-05-20T13:59:09.343697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. ML models and evaluation (F1-score, Accuracy, AUC)","metadata":{}},{"cell_type":"code","source":"AUC_dict = {}\nprint(\"**********Naive Bayes**********\")\nnbc = GaussianNB()\nnbc.fit(X_train, y_train)\ny_pred = nbc.predict(X_test)\naccuracy = accuracy_score(y_test,y_pred)\nf1score = f1_score(y_test,y_pred)\nprint(f\"F1 score: {f1score}\")\nprint(f\"Accuracy: {accuracy}\")\nnb_probs = nbc.predict_proba(X_test)[:,1]\nnb_auc = roc_auc_score(y_test,nb_probs)\nprint(f\"ROC AUC: {nb_auc}\")\nAUC_dict[\"Naive Bayes\"] = nb_auc\nprint(\"\\n\")\n\nprint(\"**********Logistic Regression**********\")\nlrc = LogisticRegression()\nlrc.fit(X_train, y_train)\ny_pred = lrc.predict(X_test)\naccuracy = accuracy_score(y_test,y_pred)\nf1score = f1_score(y_test,y_pred)\nprint(f\"F1 score: {f1score}\")\nprint(f\"Accuracy: {accuracy}\")\nlr_probs = lrc.predict_proba(X_test)[:,1]\nlr_auc = roc_auc_score(y_test,lr_probs)\nprint(f\"ROC AUC: {lr_auc}\")\nAUC_dict[\"Logistic Regression\"] = lr_auc\nprint(\"\\n\")\n\nprint(\"**********Random Forest Classifier**********\")\nrfc = RandomForestClassifier()\nrfc.fit(X_train, y_train)\ny_pred = rfc.predict(X_test)\naccuracy = accuracy_score(y_test,y_pred)\nf1score = f1_score(y_test,y_pred)\nprint(f\"F1 score: {f1score}\")\nprint(f\"Accuracy: {accuracy}\")\nrf_probs = rfc.predict_proba(X_test)[:,1]\nrf_auc = roc_auc_score(y_test,rf_probs)\nprint(f\"ROC AUC: {rf_auc}\")\nAUC_dict[\"Random Forest\"] = rf_auc\nprint(\"\\n\")\n\nprint(\"**********XGB Classifier**********\")\nxgbc = XGBClassifier()\nxgbc.fit(X_train, y_train)\ny_pred = xgbc.predict(X_test)\naccuracy = accuracy_score(y_test,y_pred)\nf1score = f1_score(y_test,y_pred)\nprint(f\"F1 score: {f1score}\")\nprint(f\"Accuracy: {accuracy}\")\nxgb_probs = xgbc.predict_proba(X_test)[:,1]\nxgb_auc = roc_auc_score(y_test,xgb_probs)\nprint(f\"ROC AUC: {xgb_auc}\")\nAUC_dict[\"XGBoost\"] = xgb_auc\nprint(\"\\n\")\n\nprint(\"**********Linear SVC**********\")\n\nsvc = LinearSVC()\nsvc_clf = CalibratedClassifierCV(svc)\nsvc.fit(X_train, y_train)\nsvc_clf.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\nsvc_probs = svc_clf.predict_proba(X_test)[:,1]\naccuracy = accuracy_score(y_test,y_pred)\nf1score = f1_score(y_test,y_pred)\nprint(f\"F1 score: {f1score}\")\nprint(f\"Accuracy: {accuracy}\")\nsvc_auc = roc_auc_score(y_test,svc_probs)\nprint(f\"ROC AUC: {svc_auc}\")\nAUC_dict[\"Linear SVC\"] = svc_auc\n","metadata":{"execution":{"iopub.status.busy":"2021-05-20T13:59:09.34646Z","iopub.execute_input":"2021-05-20T13:59:09.346823Z","iopub.status.idle":"2021-05-20T14:00:32.909075Z","shell.execute_reply.started":"2021-05-20T13:59:09.346792Z","shell.execute_reply":"2021-05-20T14:00:32.907922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Plotting ROC curve","metadata":{}},{"cell_type":"code","source":"ns_probs = [0 for _ in range(len(y_test))]\nns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\nnb_fpr, nb_tpr, _ = roc_curve(y_test, nb_probs)\nlr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\nrf_fpr, rf_tpr, _ = roc_curve(y_test, rf_probs)\nxgb_fpr, xgb_tpr, _ = roc_curve(y_test, xgb_probs)\nsvc_fpr, svc_tpr, _ = roc_curve(y_test, svc_probs)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:00:32.910583Z","iopub.execute_input":"2021-05-20T14:00:32.91102Z","iopub.status.idle":"2021-05-20T14:00:32.942144Z","shell.execute_reply.started":"2021-05-20T14:00:32.910986Z","shell.execute_reply":"2021-05-20T14:00:32.940868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.plot(ns_fpr, ns_tpr, linestyle='--', label=\"No Skill\")\nplt.plot(nb_fpr, nb_tpr, marker='.', label=\"Naive Bayes\")\nplt.plot(lr_fpr, lr_tpr, marker='.', label=\"Logistic Regression\")\nplt.plot(rf_fpr, rf_tpr, marker='.', label=\"Random Forest\")\nplt.plot(xgb_fpr, xgb_tpr, marker='.', label=\"XGBoost\")\nplt.plot(svc_fpr, svc_tpr, marker='.', label=\"Linear SVC\")\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:00:32.943996Z","iopub.execute_input":"2021-05-20T14:00:32.944431Z","iopub.status.idle":"2021-05-20T14:00:33.217254Z","shell.execute_reply.started":"2021-05-20T14:00:32.944386Z","shell.execute_reply":"2021-05-20T14:00:33.215904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Conclusion","metadata":{}},{"cell_type":"code","source":"auc_keys = AUC_dict.keys()\nauc_values = AUC_dict.values()\nplt.figure(figsize=(8,8))\nplt.xticks(rotation=45)\nplt.bar(auc_keys, auc_values)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:00:33.21898Z","iopub.execute_input":"2021-05-20T14:00:33.219358Z","iopub.status.idle":"2021-05-20T14:00:33.369922Z","shell.execute_reply.started":"2021-05-20T14:00:33.219326Z","shell.execute_reply":"2021-05-20T14:00:33.368895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUC_dict_sorted = {k: v for k, v in sorted(AUC_dict.items(), key=lambda item: item[1], reverse=True)}","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:00:33.373553Z","iopub.execute_input":"2021-05-20T14:00:33.373939Z","iopub.status.idle":"2021-05-20T14:00:33.379669Z","shell.execute_reply.started":"2021-05-20T14:00:33.373908Z","shell.execute_reply":"2021-05-20T14:00:33.378809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i=1\nfor k,v in AUC_dict_sorted.items():\n    print(f\"{i} place: {k} -> AUC = {v}\")\n    i+=1","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:00:33.381722Z","iopub.execute_input":"2021-05-20T14:00:33.382049Z","iopub.status.idle":"2021-05-20T14:00:33.398764Z","shell.execute_reply.started":"2021-05-20T14:00:33.38202Z","shell.execute_reply":"2021-05-20T14:00:33.397859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}