{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Multiple Classifier Robot Movement Classification\n\n* by [Ayo Ayibiowu](https://thehapyone.com)"},{"metadata":{},"cell_type":"markdown","source":"## Pattern Recognition\n\nIn this task, the goal is to be able to predict the position label of a moving robot based on the sensor readings from the robot during its movement.\n\nThe data is collected during the course of a robot navigating through a room following the wall in a clockwise direction, for 4 rounds. \n\nAlso, I attempted in trying out several models. The dataset (sensor reading) is non-linear in nature"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# Importing the libraries needed\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport scipy\nfrom sklearn.svm import SVC\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# importing the dataset\ndata = np.loadtxt(\"/kaggle/input/wall-following-robot/sensor_readings_24.csv\", delimiter=',', dtype=np.str)\n\nraw_data = pd.DataFrame(data[:,:24], dtype=np.float)\nraw_data = pd.concat([raw_data, pd.DataFrame(data[:, 24], columns=['Label'])], axis=1)\n                      \nprint(\"Data size - \", raw_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"Sample raw data\")\nraw_data.head(n=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Describe the dataset\nraw_data.describe()\n# from the nature of the data, it can be seen that we don't need to normalize it.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluating features (sensors) contribution towards the label\nfig = plt.figure(figsize=(15,5))\nax = sns.countplot(x='Label',data=raw_data,alpha=0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Observation\nFrom the plot above, it is evident of a class inblance happening. This inbalance might influence our result[](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# label count\nraw_data.groupby(['Label']).count()[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pair plot of the data\ncolss = raw_data.columns[0:24]\nsns.pairplot(raw_data, vars=colss, hue='Label')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection\nThe fisher score is a good discrimating function for finding features that contributes more towards the data. \nThe code below shows how to calculate it. You can find the latest fisher calculation code [here](https://gist.github.com/e55758727bce8c5acc7ca6785ad63a5f) - https://gist.github.com/e55758727bce8c5acc7ca6785ad63a5f"},{"metadata":{"trusted":true},"cell_type":"code","source":"# helper function for evalating the fisher ndex\ndef fisher_index_calc(trainingSet, labelSet):\n    (dim1_T, dim2_T) = trainingSet.shape\n    (dim1_L, dim2_L) = labelSet.shape\n\n    # create the fisher output variable - A vector of all the features\n    fisher_ratios = np.zeros((1, dim2_T), dtype=float).flatten()\n    # It's expected that the dim1_T and dim1_L be of the same size, else this input parameters is nulled.\n    if dim1_L != dim1_T:\n        return fisher_ratios\n\n    # First extract out the number of features available.\n    # grouped both data together, and create a pandas dataframe from it.\n    train1 = pd.DataFrame(trainingSet)\n    label1 = pd.DataFrame(labelSet, columns=['LABEL'])\n    grouped = pd.concat([train1, label1], axis=1)\n\n    # fetch the number of classes\n    (no_classes, demo) = grouped.groupby('LABEL').count()[[0]].shape\n    #print grouped\n\n    # loop through all features\n    for j in range(dim2_T):\n        # the variance of the feature j\n        j_variance = np.var(trainingSet[:,j])\n        j_mean = np.mean(trainingSet[:,j])\n        j_summation = 0\n        for k in range(no_classes):\n            output = grouped.groupby('LABEL').count()[[j]]\n            k_feature_count = output.iloc[k,0]\n            # mean for class k of feature j\n            output = grouped.groupby('LABEL').mean()[[j]]\n            k_feature_mean = output.iloc[k,0]\n            currentSum = k_feature_count * np.square((k_feature_mean - j_mean))\n            j_summation = j_summation + currentSum\n        fisher_ratios[j] = j_summation / np.square(j_variance)\n\n    return fisher_ratios","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_set = raw_data.iloc[:, :(raw_data.shape[1]-1)].values\nlabel_set = raw_data.iloc[:, (raw_data.shape[1]-1):].values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculates the fisher score of the sensors features\nfisher_scores = fisher_index_calc(training_set, label_set)\n\nfig= plt.figure(figsize=(23, 10))\ndf = pd.DataFrame({'Fisher Ratio For All Features': fisher_scores})\nax = df.plot.bar(figsize=(20,10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection.\nFisher Score is a good discrimintating attribute for features. Here, features with scores less than 300 are discarded. \n> **Pro - Tip**: The fisher based score helps to improve the performance of the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature selection based on fisher score\n# Fisher Index Ratio Filter - Remove features with low score\n# indices of features to remove based on fisher ratios\nto_remove = []\nfor i in range((len(fisher_scores))):\n    if fisher_scores[i] < 300:\n        # we mark for removal\n        to_remove.append(i)\n\n# remove features with low fisher score\ntraining_set_fisher = np.delete(training_set, to_remove, 1)\ntraining_set_fisher.shape\n# ihave about 18 features left.\n#print \"fisher - \", fisher_ratios","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encoding the label set with a label encoder\nfrom sklearn.preprocessing import LabelEncoder\n\nlabelEn = LabelEncoder()\nencoded_labels = labelEn.fit_transform(raw_data.iloc[:, 24].values)\nclass_names = labelEn.classes_\nclass_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# performaing PCA\n\nfrom sklearn.decomposition import PCA\n\npca = PCA()\n\npca_sets = pca.fit_transform(training_set)\nprint (100 * np.sum(pca.explained_variance_ratio_))\n\nx1 = np.arange(1, pca.explained_variance_ratio_.shape[0]+1, 1)\nfig = plt.figure(figsize=(20,5))\nsns.barplot(x = x1, y = 100 * pca.explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalizaling the data with standard scaler\n### The data doesn't require to be normalize, it already looks normalized\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscalled_set = scaler.fit_transform(training_set)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scalling and PCA Observation\nAfter testing several models based on the cross validation, doing PCA and Standard Scaler doesn't contribute any much difference to the model.\nMoreover, the dataset already looks scaled properly. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the data for Training and Testing\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\nX_train, X_test, y_train, y_test = train_test_split(training_set_fisher, encoded_labels, test_size=0.3, shuffle=False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Class Inbalance\nThe output class for this dataset is highly inbalance which has the chance of creating basis in the model. I created class weights to compensate for this inbalance. \nThis weight can be used in a model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.utils.testing import assert_almost_equal\n\n# this function creats a class weight\n\ndef create_class_weight(y):\n    classes = np.unique(y)\n    cw = compute_class_weight(\"balanced\", classes, y)\n    # evaluate if weights are truly balanced\n    # total effect of samples is preserved\n    class_counts = np.bincount(y)\n    # print (class_counts)\n    assert_almost_equal(np.dot(cw, class_counts), y.shape[0])\n    assert cw[0] < cw[1] < cw[2]   \n\n    return cw\n\nlabel_weights = create_class_weight(encoded_labels)\n# Convert class_weights to a dictionary to pass it to class_weight in model.fit\nlabel_weights = dict(enumerate(label_weights))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function for confusion matrix\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils.multiclass import unique_labels\n\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    print(cm)\n\n    fig, ax = plt.subplots(figsize=(10,10))\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression\nThis is the first classifier used here. The performance is quite poor. The task is non-linear"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross validation with Logistic Regression\n'''\n# Set the parameters by cross-validation\nc_range = np.arange(1,100,5)\nsolver = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n\n# Hyper Paremters to be used for Model 1\ntuned_parameters = {'C': c_range, 'solver': solver}\n\n# defining the model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, make_scorer\n\nlg_model = LogisticRegression(max_iter = 1000, multi_class = 'auto', n_jobs = -1, random_state = 0, class_weight= label_weights)\n\n# Creates a GridSearch Classifier using parameters for model 1\nlg_grid = GridSearchCV(lg_model, tuned_parameters, cv=10, scoring=make_scorer(accuracy_score), n_jobs=-1)\n\n# commence training - NOTE: It takes somes time to be complete\nlg_grid.fit(X_train, y_train)\n\nprint(\"Best parameters set found on development set:\")\nprint()\nprint(lg_grid.best_estimator_)\nprint (\"Best Params CV1 - \", lg_grid.best_params_)\nprint (\"Best score - \", lg_grid.best_score_)\nprint()\n\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining the model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, make_scorer\n\nlg_model = LogisticRegression(C= 86, solver='liblinear', max_iter = 1000, multi_class = 'auto', random_state = 0, class_weight= label_weights)\n\n# commence training -\nlg_model.fit(X_train, y_train)\n\n# predict the result\ny_pred = lg_model.predict(X_test)\nprint (\"Logisitic Regression Result Considering Class Inbalance\")\nprint (\"Performance - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n\n### Without considering the class inbalance\nlg_model = LogisticRegression(C= 86, solver='liblinear', max_iter = 1000, multi_class = 'auto', random_state = 0)\n\n# commence training -\nlg_model.fit(X_train, y_train)\n\n# predict the result\ny_pred = lg_model.predict(X_test)\nprint (\"Logisitic Regression Result Without considering Class Inbalance\")\nprint (\"Performance - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot non-normalized confusion matrix\n\nplot_confusion_matrix(y_test, y_pred, classes=class_names,\n                      title='Confusion matrix For MLP Model')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Multi-layer Perceptron\nMLP can handle both linear and non-linear task. MLP performed best here and it was able to handle the class inbalance case even without supplying a class weight."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross validation with MLP\n'''\n# Set the parameters by cross-validation\n# number of neurons to use for the hidden layers\nn_neurons_range = np.arange(5,80,5)\nn_neurons = [(5,5), (10,10), (20,20), (30,30), (40,40), (50,50), (60,60), (65,65), (40,40,40)]\na_param_range = 10.0 ** -np.arange(1, 7)\n# Hyper Paremters to be used for Model 1\ntuned_parameters = {'solver': ['lbfgs', 'adam'], 'activation': ['tanh', 'logistic', 'relu'], 'alpha': a_param_range,\n                     'hidden_layer_sizes': n_neurons_range}\n\n# Creates the MLP Classifier\nfrom sklearn.neural_network import MLPClassifier\n\nmlp = MLPClassifier(random_state=1, early_stopping=True, max_iter=1000)\n\n# Creates a GridSearch Classifier using parameters for model 1\nmlp_grid = GridSearchCV(mlp, tuned_parameters, cv=10, scoring=make_scorer(accuracy_score), n_jobs=-1)\n# commence training - NOTE: It takes hours to be complete\nmlp_grid.fit(X_train, y_train)\n\nprint(\"Best parameters set found on development set:\")\nprint()\nprint(mlp_grid.best_estimator_)\nprint (\"Best Params CV1 - \", mlp_grid.best_params_)\nprint (\"Best score - \", mlp_grid.best_score_)\nprint()\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creates the MLP Classifier\nfrom sklearn.neural_network import MLPClassifier\n\n# After training with Cross validation, this was derived as the best model.\nmlp = MLPClassifier(activation= 'logistic', alpha= 0.01, hidden_layer_sizes= (40,), solver= 'lbfgs', random_state=1, max_iter=1000)\n\n# commence training -\nmlp.fit(X_train, y_train)\n\n# predict the result\ny_pred = mlp.predict(X_test)\nprint (\"MLP Result Without considering Class Inbalance\")\nprint (\"Performance - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot non-normalized confusion matrix\n\nplot_confusion_matrix(y_test, y_pred, classes=class_names,\n                      title='Confusion matrix For MLP Model')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Support Vector Machine\nSVM result improved after using the fisher based feature selection."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross validation with Support Vector Machines\n'''\n# Set the parameters by cross-validation\n# number of neurons to use for the hidden layers\nc_range = [0.5, 1, 2, 5, 10, 15, 17, 19, 20, 22, 25, 30, 35, 40, 45, 60, 70, 100, 300]\n#c_range = [0.1, 0.2, 0.5, 1]\n\nkernel_sv = ['linear', 'poly', 'rbf']\ndegree_sv = np.arange(1, 5, 1)\n# Hyper Paremters to be used for Model 1\ntuned_parameters = {'C': c_range, 'kernel': kernel_sv, 'degree': degree_sv}\n\n# defining the SVC model\nfrom sklearn.metrics import accuracy_score, make_scorer\n\nsvc_model = SVC(gamma='auto')\n# Creates a GridSearch Classifier using parameters for model 1\nsvc_grid = GridSearchCV(svc_model, tuned_parameters, cv=10, scoring=make_scorer(accuracy_score), n_jobs=-1)\n\n# commence training - NOTE: It takes hours to be complete\nsvc_grid.fit(X_train, y_train)\n\nprint(\"Best parameters set found on development set:\")\nprint()\nprint(svc_grid.best_estimator_)\nprint (\"Best Params CV1 - \", svc_grid.best_params_)\nprint (\"Best score - \", svc_grid.best_score_)\nprint()\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining the SVC model\nfrom sklearn.metrics import accuracy_score\n\nsvc_model = SVC(C=26, degree=1, kernel='rbf', gamma='scale', class_weight=label_weights)\n\n# commence training\nsvc_model.fit(X_train, y_train)\n\n# predict the result\ny_pred = svc_model.predict(X_test)\nprint (\"Support Vector Machines Result Considering Class Inbalance\")\nprint (\"Performance - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n\nsvc_model = SVC(C=26, degree=1, kernel='rbf', gamma='scale')\n\n# commence training\nsvc_model.fit(X_train, y_train)\n\n# predict the result\ny_pred = svc_model.predict(X_test)\nprint (\"Support Vector Machines Result Without considering Class Inbalance\")\nprint (\"Performance - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot non-normalized confusion matrix\n\nplot_confusion_matrix(y_test, y_pred, classes=class_names,\n                      title='Confusion matrix For MLP Model')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}