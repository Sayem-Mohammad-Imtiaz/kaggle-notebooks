{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\n\npd.options.display.max_colwidth = 200\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames[:5]:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this notebook we introduce basics of neural networks.","metadata":{}},{"cell_type":"markdown","source":"# 1. Basics of ANN\n\nNeural networks are a set of algorithms, modeled loosely after the human brain, that are designed to recognize patterns. Neural networks are made of perceptrons.\n\nPerceptrons are cell units that are responsible for some mathematical computation.\n\n<img src=\"https://miro.medium.com/max/1400/1*ZoT34FHnrrPxWHsXv6mK8Q.jpeg\" width=\"60%\">\n\nIn Artificial neural network, we use multiple such perceptrons to learn different attributes of the dataset. Together, they can learn highly complex non linear structure from data.\n\nIn the above picture - \n\n(1) is the input features\n\n(2) are the weights of the hidden layer perceptrons\n\n(3) are the representations learned by each perceptron\n\n(4) is the activation function\n\n(5) is the output\n","metadata":{}},{"cell_type":"markdown","source":"## 1a. Neural Network Architecture\n\nDifferent types of cells are meant for different types of computations. e.g. - \n\n* Dense cell - these are meant for dense computations (basic perceptron)\n* RNN cell - recurrent neural network cell is used where we have to store some temporal information in the NN\n* Convolution cell - convolution cells are just like image filters, which are used to learn new features. Conv cells are mostly popular for image type of data.\n\nEach cell takes input of a fixed dimension and generates intermediate hidden output of fixed dimension. During building the neural network, we need to make sure of the cell inputs and outputs, so that the final layer output matches with the data output dimensions.\n\nIn deep neural networks, multiple layer of multiple such cells are stacked together. e.g. - in the above diagram, we have a simple 1-layer ANN with just dense cells.\n\nDuring the learning process, neural networks learn the weights of each of these cells. These are called the parameters of the neural network.","metadata":{}},{"cell_type":"markdown","source":"# 1b. Activation function\n\nActivation functions are mathematical functions that are used in the hidden layers and in the final layer. By tuning activation functions, we learn more complex and non linear structures from data. Few activation functions are - \n\n* Linear\n* Sigmoid (logistic)\n* ReLU (rectified linear unit)\n* Softmax\n* Tanh\n* Swish\n* Mish\n* ELU\n\n<img src=\"https://miro.medium.com/max/2000/1*4ZEDRpFuCIpUjNgjDdT2Lg.png\" width=\"50%\">\n\nReLU is widely used activation function for intermediate layers. Sigmoid is widely used in the final layer for binary classification task. For multiclass classification, softmax is used in the final layer. In regression tasks, mostly ReLU is used in the final layer.","metadata":{}},{"cell_type":"markdown","source":"## 1c.How does neural network learn\n\nIn neural networks, we also need to define a loss function that calculates the loss between the actual prediction and the predicted output by the neural network. Once, loss is calculated, neural network uses $\\textbf{back propagation}$ to learn the weights of the cells in it.\n\nForward propagation is the process of moving forward through the neural network (from inputs to the ultimate output or prediction). Backpropagation is the reverse. Except instead of signal, we are moving error backwards through our model.\n \n<img src=https://miro.medium.com/max/1400/1*UY4-RIrSVgfuhAkawKIr2w.jpeg>\n\n<img src=https://miro.medium.com/max/1400/1*0RIBu3Iz-aOOX9dyob_FHA.jpeg>\n\nBack propagation can be thought as a kind of feedback loop for the network.","metadata":{}},{"cell_type":"markdown","source":"## 1d. Cost function\n\nCost function is used to calculate loss between actual and predicted output by the network.\n\n<b> P.S. - although cost function and evaluation metric are very similar, there is a fine difference between two. cost function is used for backpropagation, due to which, it needs to differentiable. On the other hand, evaluation metric is used to evaluate a model's performance. </b> \n\nFor regression the most popular loss function is Mean Squared Error (MSE) or L2 loss.\n\n<img src=\"https://miro.medium.com/max/1026/1*SGhoeJ_BgcfqU06CmX41rw.png\" width=\"20%\">\n\nSimilarly, L1 loss or, Mean Absolute Error (MAE) is also used for regression task.\n\n<img src=\"https://miro.medium.com/max/1066/1*piCo0iDgPmESnQkHSwAK6A.png\" width=\"20%\">\n\nFor classification, cross entropy loss is used.\n\n<img src=\"https://miro.medium.com/max/1400/1*zi1wKAAGGt1Bn6mqo2MSFw.png\" width=\"40%\">\n\nOther loss function are -\n\n* KL divergence\n* Hinge loss\n* Triplet loss","metadata":{}},{"cell_type":"markdown","source":"## 1e. Optimization\n\nNow that we have the loss calculated, how does NNs learn the model parameters?\n\nAs we need to learn model parameters that minimizes the overall loss, we treat the learning process as an optimization technique which updates the parameters in order to find the local (global) minima.\n\nThe name for one commonly used optimization function that adjusts weights according to the error they caused is called <b> gradient descent </b>.\n\nGradient is another word for slope, and slope, in its typical form on an x-y graph, represents how two variables relate to each other: rise over run, the change in money over the change in time, etc. In this particular case, the slope we care about describes the relationship between the networkâ€™s error and a single weight; i.e. that is, how does the error vary as the weight is adjusted.\n\n\nThese gradients are used to understand how to update each parameter in order to get the local (global) minima. In the NN, parameters are initialized with random values and then updates using the gradients calculated during backpropagation.\n\n<img src=https://miro.medium.com/max/1201/1*VymEfQTf30evUczsTBiz4g.png width=400>\n\nDifferent optimization techniques are available in the literature. To name a few -\n\n* SGD (stochastic gradient descent)\n* Adam\n* RMSProp\n* Adagrad\n* Radam\n* Adadelta\n","metadata":{}},{"cell_type":"markdown","source":"## 1f. Regularization\n\nRegularization is a very useful technique which is used to generalize the performance of neural networks. To reduce overfitting, we need to restrict the weights of the neurons. There are different regularization parameters - L1, L2 are used to restrict the search space of paremeters.\n\nAnother idea is <b>dropout</b>. Using dropout, neural network drop randomly selected neurons during training process. \n\n<img src=\"https://miro.medium.com/max/1400/1*iWQzxhVlvadk6VAJjsgXgg.png\" width=\"50%\">","metadata":{}},{"cell_type":"markdown","source":"Now that we have learn the basics of neural networks, let us build a simple ANN (not from scratch though) using scikit learn. Our objective is to classify the processed clinical notes into different specialities.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/nlp-specialization-data/Cleaned_POS_Medical_Notes.csv') #for excel file use read_excel\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['label'].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_vector = TfidfVectorizer(lowercase=True, #this will convert all the tokens into lower case\n                         stop_words='english', #remove english stopwords from vocabulary. if we need the stopwords this value should be None\n                         analyzer='word', #tokens should be words. we can also use char for character tokens\n                         max_features=5000, #maximum vocabulary size to restrict too many features\n                         min_df = 5)\n\ntfidf_vectorized_corpus = tfidf_vector.fit_transform(df.clean_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (tfidf_vectorized_corpus.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_vectorized_corpus.toarray()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#y = keras.utils.to_categorical(df['label'].values, 5)\ny=pd.get_dummies(df['label']).values\ny","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using Tensorflow Keras instead of the original Keras\n\nfrom tensorflow import keras\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\n\nfrom sklearn import metrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(64,input_shape = (3843,), activation = 'relu'))\nmodel.add(Dense(16, activation = 'relu'))\nmodel.add(Dense(4, activation = 'relu'))\nmodel.add(Dense(5,activation='softmax'))\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss='categorical_crossentropy',metrics=['accuracy',])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(tfidf_vectorized_corpus.toarray(), y, epochs=20, batch_size=128, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_predict = model.predict(tfidf_vectorized_corpus.toarray())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(y_predict[0])\nprint(np.argmax(y_predict[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = []\nfor val in y_predict:\n    y_pred.append(np.argmax(val))\n    \ny_pred[0:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.crosstab(df['label'],pd.Series(y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our basic ANN achieved average F1 score of 74% on the cross validation, which is little worse than logistic and naive bayes. However, ANN performs better than random forest. Tree based methods are widely successful for tabular dataset, however, they fail to capture semantic information from text data.","metadata":{}},{"cell_type":"markdown","source":"### References\n\n1. https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23\n\n2. https://towardsdatascience.com/introducing-deep-learning-and-neural-networks-deep-learning-for-rookies-1-bd68f9cf5883\n\n3. https://pathmind.com/wiki/neural-network#forward\n\n4. https://towardsdatascience.com/complete-guide-of-activation-functions-34076e95d044\n\n5. https://towardsdatascience.com/understanding-neural-networks-19020b758230\n\n6. https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}