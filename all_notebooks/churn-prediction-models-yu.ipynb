{"cells":[{"metadata":{},"cell_type":"markdown","source":"# CHURN PREDICTION PROJECT\n\n\n------\n\n\nA bank is investigating a very high rate of customer leaving the bank. Here is a 10.000 records dataset to investigate and predict which of the customers are more likely to leave the bank soon.\n\n### Dataset\nhttps://www.kaggle.com/mathchi/churn-for-bank-customers\n\nAbout dataset\n\n**RowNumber** : Corresponds to the record (row) number and has no effect on the output.\n\n**CustomerId** :Contains random values and has no effect on customer leaving the bank.\n\n**Surname** : The surname of a customer has no impact on their decision to leave the bank.\n\n**CreditScore** : Can have an effect on customer churn, since a customer with a higher credit score is less likely to leave the bank.\n\n**Geography** : A customer’s location can affect their decision to leave the bank.\n\n**Gender** : It’s interesting to explore whether gender plays a role in a customer leaving the bank.\n\n**Age** : This is certainly relevant, since older customers are less likely to leave their bank than younger ones.\n\n**Tenure** : Refers to the number of years that the customer has been a client of the bank. Normally, older clients are more loyal and less likely to leave a bank.\n\n**Balance** : Also a very good indicator of customer churn, as people with a higher balance in their accounts are less likely to leave the bank compared to close with lower balances.\n\n**NumOfProducts** : Refers to the number of products that a customer has purchased through the bank.\n\n**HasCrCard** : Denotes whether or not a customer has a credit card. This column is also relevant, since people with a credit card are less likely to leave the bank.\n\n**IsActiveMember** : Active customers are less likely to leave the bank.\n\n**EstimatedSalary** : As with balance, people with lower salaries are more likely to leave the bank compared to those with higher salaries.\n\n**Exited** : Whether or not the customer left the bank.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Importing the libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Importing libraries\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np # linear algebra\nimport seaborn as sns #visualization\nimport matplotlib.pyplot as plt #visualization\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression  \nfrom sklearn.neighbors import KNeighborsClassifier  \nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.metrics import (plot_confusion_matrix, confusion_matrix, \n                             accuracy_score, mean_squared_error, r2_score, \n                             roc_auc_score, roc_curve, classification_report, \n                             precision_recall_curve, auc, f1_score, \n                             average_precision_score, precision_score, recall_score)\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import scale, StandardScaler, RobustScaler, MinMaxScaler\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nwarnings.filterwarnings(\"ignore\", category=FutureWarning) \nwarnings.filterwarnings(\"ignore\", category=UserWarning) \n\n%config InlineBackend.figure_format = 'retina'\n\n# to display all columns and rows:\npd.set_option('display.max_columns', None); pd.set_option('display.max_rows', None);  # to display all columns and rows\npd.set_option('display.float_format', lambda x: '%.2f' % x) # The number of numbers that will be shown after the comma.\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reading the dataset","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"churn = pd.read_csv(\"../input/churn-for-bank-customers/churn.csv\", index_col = 0)\nchurn.head() # first five row of the dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data overview","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking dataset\n\nprint (\"Rows     : \" ,churn.shape[0])\nprint (\"Columns  : \" ,churn.shape[1])\nprint (\"\\nFeatures : \\n\" ,churn.columns.tolist())\nprint (\"\\nMissing values :  \", churn.isnull().sum().values.sum())\nprint (\"\\nUnique values :  \\n\",churn.nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"churn.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"churn[\"Exited\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Separating churn and non churn customers\nexited     = churn[churn[\"Exited\"] == 1]\nnot_exited = churn[churn[\"Exited\"] == 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dropping Irrelevant Feature\nCustomerId and Surname are irrelivant, so we drop those features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = churn.drop(['CustomerId', 'Surname'], axis = 1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Visualization¶\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axarr = plt.subplots(2, 3, figsize=(18, 6))\nsns.countplot(x = 'Geography', hue = 'Exited',data = df, ax = axarr[0][0])\nsns.countplot(x = 'Gender', hue = 'Exited',data = df, ax = axarr[0][1])\nsns.countplot(x = 'HasCrCard', hue = 'Exited',data = df, ax = axarr[0][2])\nsns.countplot(x = 'IsActiveMember', hue = 'Exited',data = df, ax = axarr[1][0])\nsns.countplot(x = 'NumOfProducts', hue = 'Exited',data = df, ax = axarr[1][1])\nsns.countplot(x = 'Tenure', hue = 'Exited',data = df, ax = axarr[1][2])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Customer with 3 or 4 products are higher chances to Churn\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"_, ax = plt.subplots(1, 3, figsize=(18, 6))\nplt.subplots_adjust(wspace=0.3)\nsns.swarmplot(x = \"NumOfProducts\", y = \"Age\", hue=\"Exited\", data = df, ax= ax[0])\nsns.swarmplot(x = \"HasCrCard\", y = \"Age\", data = df, hue=\"Exited\", ax = ax[1])\nsns.swarmplot(x = \"IsActiveMember\", y = \"Age\", hue=\"Exited\", data = df, ax = ax[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"facet = sns.FacetGrid(df, hue = \"Exited\", aspect = 3)\nfacet.map(sns.kdeplot, \"Age\", shade = True)\nfacet.set(xlim = (0, df[\"Age\"].max()))\nfacet.add_legend()\n\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_, ax =  plt.subplots(1, 2, figsize = (15, 7))\ncmap = sns.cubehelix_palette(light = 1, as_cmap = True)\nsns.scatterplot(x = \"Age\", y = \"Balance\", hue = \"Exited\", cmap = cmap, sizes = (10, 200), data = df, ax = ax[0])\nsns.scatterplot(x = \"CreditScore\", y = \"Balance\", hue = \"Exited\", cmap = cmap, sizes = (10, 200), data = df, ax = ax[1]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- 40 to 65 years old customers are higher chances to churn\n- Customer with CreditScore less then 450 are higher chances to churn","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 10))\nsns.swarmplot(x = \"HasCrCard\", y = \"Age\", data = df, hue = \"Exited\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking Correlation\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df.corr()\ncorr.style.background_gradient(cmap = 'coolwarm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NumOfProducts variable is converted to string values.\nNumOfProd = []\nfor i in df['NumOfProducts']:\n    if i == 1:\n        NumOfProd.append('A')\n    elif i == 2:\n        NumOfProd.append('B')\n    elif i == 3:\n        NumOfProd.append('C')\n    else:\n        NumOfProd.append('D')\n        \ndf['NumOfProducts'] = NumOfProd\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummies = pd.get_dummies(df[['Geography', 'Gender', 'NumOfProducts']], drop_first = True) \nX_ = df.drop(['Geography', 'Gender', 'NumOfProducts'], axis = 1)\ndf_1 = pd.concat([X_, dummies], axis = 1)\ndf_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_1.Balance = df_1.Balance + 1 # To get rid of the problem of dividing by 0\ndf_1['SalBal'] = df_1.EstimatedSalary / df_1.Balance #The ratio of variables EstimatedSalary and Balance is assigned as a new variable\ndf_1.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Standardization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Standardization on four features\nX_s = pd.DataFrame(df_1[['CreditScore', 'Balance', 'EstimatedSalary', 'SalBal']], \n                   columns = ['CreditScore', 'Balance', 'EstimatedSalary', 'SalBal'])\n\nMinMax = MinMaxScaler(feature_range = (0, 1)).fit(X_s)\nX_s = MinMax.transform(X_s)\nX_st = pd.DataFrame(X_s, columns = ['CreditScore', 'Balance', 'EstimatedSalary', 'SalBal'])\nX_st.index = X_st.index + 1\nX_st.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We define the dataset with standardized variables as df_2.\ndf_2 = df_1.drop(['CreditScore', 'Balance', 'EstimatedSalary', 'SalBal'], axis = 1)\ndf_2 = pd.concat([df_2, X_st], axis = 1, ignore_index = False)\ndf_2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# credit scores are divided into 6 classes.\nCreditScoreClass = []\nfor cs in churn.CreditScore:\n    if 400 <= cs < 500:\n        CreditScoreClass.append(1)\n    elif 500 <= cs < 700:\n        CreditScoreClass.append(2)\n    elif  700 <= cs < 800:\n        CreditScoreClass.append(3)\n    elif  800 <=  cs < 850:\n        CreditScoreClass.append(4)\n    elif  850 <= cs: \n        CreditScoreClass.append(5)\n    elif 400 > cs :\n        CreditScoreClass.append(0)\n\ndf_2['CreditScoreClass'] = CreditScoreClass\ndf_2.drop('CreditScore', axis = 1, inplace = True)\ndf_2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Machine Learning:\n\n\nWe will train out data on different machine learning models and use different techniques on each model and then compare our finding at the end to determine which model is working best for out data.\n\n\n----- Model Performance and Comparison -----\n\nTo measure the performance of a model, we need several elements\nConfusion matrix : also known as the error matrix, allows visualization of the performance of an algorithm\n\n- True Positive (TP) : Exited correctly identified as exited\n- True Negative (TN) : Nonexited correctly identified as nonexited\n- False Positive (FP) : Nonexited incorrectly identified as exited\n- False Negative (FN) : Exited incorrectly identified as nonexited\n\n\nMetrics\n\n- Accuracy : (TP + TN) / (TP + TN + FP +FN)\n- Precision : TP / (TP + FP)\n- Recall : TP / (TP + FN)\n- F1 score : 2 x ((Precision x Recall) / (Precision + Recall))\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df_2['Exited']\nX = df_2.drop('Exited', axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 12345)\nmodels = [\n    LogisticRegression(),\n    KNeighborsClassifier(),\n    SVC(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    GradientBoostingClassifier(),\n    LGBMClassifier(),\n    XGBClassifier()]\n\nresult = []\nresults = pd.DataFrame(columns = [\"Models\",\"Accuracy\"])\n\nfor model in models:\n    names = model.__class__.__name__\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    acc = accuracy_score(y_test, y_pred)\n    scores = cross_val_score(model, X_test, y_test, cv = 10, scoring = 'accuracy')\n    result = pd.DataFrame([[names, acc * 100, \n                            np.mean(scores) * 100]], \n                          columns = [\"Models\", \"Accuracy\", \"Avg_Accuracy\"])\n    results = results.append(result)\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining variables to store the outputs.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_accuracies={}\naccuracies={}\nroc_auc={}\npr_auc={}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining function to calculate the Cross-Validation score.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def cv_score(name, model, folds):\n    scores = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n    avg_result = []\n    for sc in scores:\n        scores = cross_val_score(model, X_test, y_test, cv = folds, scoring = sc)\n        avg_result.append(np.average(scores))\n    df_avg_score = pd.DataFrame(avg_result)\n    df_avg_score = df_avg_score.rename(index={0: 'Accuracy',\n                                             1:'Precision',\n                                             2:'Recall',\n                                             3:'F1 score',\n                                             4:'Roc auc'}, columns = {0: 'Average'})\n    avg_accuracies[name] = np.round(df_avg_score.loc['Accuracy'] * 100, 2)\n    values = [np.round(df_avg_score.loc['Accuracy'] * 100, 2),\n            np.round(df_avg_score.loc['Precision'] * 100, 2),\n            np.round(df_avg_score.loc['Recall'] * 100, 2),\n            np.round(df_avg_score.loc['F1 score'] * 100, 2),\n            np.round(df_avg_score.loc['Roc auc'] * 100, 2)]\n    plt.figure(figsize = (15, 8))\n    sns.set_palette('mako')\n    ax = sns.barplot(x = ['Accuracy', 'Precision', 'Recall', 'F1 score', 'Roc auc'], y = values)\n    plt.yticks(np.arange(0, 100, 10))\n    plt.ylabel('Percentage %', labelpad = 10)\n    plt.xlabel('Scoring Parameters', labelpad = 10)\n    plt.title('Cross Validation ' + str(folds) + '-Folds Average Scores', pad = 20)\n    for p in ax.patches:\n        ax.annotate(str(p.get_height()), (p.get_x(), p.get_height()), xytext = (p.get_x() + 0.3, p.get_height() + 1.02))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining function to create Confusion Matrix.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def conf_matrix(ytest, pred):\n    plt.figure(figsize = (15, 8))\n    global cm1\n    cm1 = confusion_matrix(ytest, pred)\n    ax = sns.heatmap(cm1, annot = True, cmap = 'Blues')\n    plt.title('Confusion Matrix', pad = 30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining function to calculate the Metrics Scores.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def metrics_score(cm):\n    total = sum(sum(cm))\n    accuracy = (cm[0, 0] + cm[1, 1]) / total\n    precision = cm[1, 1] / (cm[0, 1] + cm[1, 1])\n    sensitivity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n    f1 = 2 * (precision * sensitivity) / (precision + sensitivity)\n    specificity = cm[0,0] / (cm[0, 1] + cm[0, 0])\n    values = [np.round(accuracy * 100, 2),\n            np.round(precision * 100, 2),\n            np.round(sensitivity * 100, 2),\n            np.round(f1 * 100, 2),\n            np.round(specificity * 100, 2)]\n    plt.figure(figsize = (15, 8))\n    sns.set_palette('magma')\n    ax = sns.barplot(x = ['Accuracy', 'Precision', 'Recall', 'F1 score', 'Specificity'], y = values)\n    plt.yticks(np.arange(0, 100, 10))\n    plt.ylabel('Percentage %', labelpad = 10)\n    plt.xlabel('Scoring Parameter', labelpad = 10)\n    plt.title('Metrics Scores', pad = 20)\n    for p in ax.patches:\n        ax.annotate(str(p.get_height()), (p.get_x(), p.get_height()), xytext = (p.get_x() + 0.3, p.get_height() + 1.02))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining function to plot ROC Curve.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_roc_curve(fpr, tpr):\n    plt.figure(figsize = (8, 6))\n    plt.plot(fpr, tpr, color = 'Orange', label = 'ROC')\n    plt.plot([0, 1], [0, 1], color = 'black', linestyle = '--')\n    plt.ylabel('True Positive Rate', labelpad = 10)\n    plt.xlabel('False Positive Rate', labelpad = 10)\n    plt.title('Receiver Operating Characteristic (ROC) Curve', pad = 20)\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining function to plot Precision-Recall Curve.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_precision_recall_curve(recall, precision):\n    plt.figure(figsize = (8,6))\n    plt.plot(recall, precision, color = 'orange', label = 'PRC')\n    plt.ylabel('Precision', labelpad = 10)\n    plt.xlabel('Recall', labelpad = 10)\n    plt.title('Precision Recall Curve', pad = 20)\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Logistic Regression Classifier:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df_2['Exited']\nX = df_2.drop('Exited', axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 12345)\nlog_model = LogisticRegression()\nlog_model.fit(X_train, y_train)\nprediction1 = log_model.predict(X_test)\naccuracy1 = log_model.score(X_test, y_test) \nprint ('Model Accuracy:',accuracy1 * 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Storing model accuracy to plot for comparison with other Machine Learning models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracies['Linear Regression'] = np.round(accuracy1 * 100, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Plotting Confusion Matrix to describe the performance of Linear Regression Classifier on a set of test data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_matrix(y_test, prediction1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting different metrics scores for the Linear Regression Classifier for evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics_score(cm1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Plotting the average of different metrics scores for further evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_score('Linear Regression', log_model, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Receiver Operating Characteristic (ROC) Curve, to illustrate the diagnostic ability of Linear Regression Classifier as its discrimination threshold is varied and showing the Area under the ROC Curve (AUC) value which will tell us how much our model is capable of distinguishing between churn und nonchurn customers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = log_model.predict_proba(X_test)\nprobs = probs[:, 1]\nauc1 = roc_auc_score(y_test, probs)\nroc_auc['Linear Regression'] = np.round(auc1, 2)\nprint('Area under the ROC Curve (AUC): %.2f' % auc1)\nfpr1, tpr1, _ = roc_curve(y_test, probs)\nplot_roc_curve(fpr1, tpr1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Precision-Recall Curve for different thresholds of precision and recall much like the ROC Curve and showing the Area under the Precision-Recall Curve (AUCPR), it gives the number summary of the information in the Precision-Recall Curve.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"precision1, recall1, _ = precision_recall_curve(y_test, probs)\nauc_score1 = auc(recall1, precision1)\npr_auc['Linear Regression'] = np.round(auc_score1, 2)\nprint('Area under the PR Curve (AUCPR): %.2f' % auc_score1)\nplot_precision_recall_curve(recall1, precision1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. KNNeighbors Classifier:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"KNN is a non-parametric, lazy learning algorithm. Its purpose is to use a database in which the data points are separated into several classes to predict the classification of a new sample point.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df_1['Exited']\nX = df_2.drop('Exited', axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 12345)\nKNN_model = KNeighborsClassifier()\nKNN_model.fit(X_train, y_train)\nprediction2 = KNN_model.predict(X_test)\naccuracy2 = KNN_model.score(X_test, y_test) \nprint ('Model Accuracy:',accuracy2 * 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Storing model accuracy to plot for comparison with other Machine Learning models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracies['KNeighbors Classifier'] = np.round(accuracy2 * 100, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Plotting Confusion Matrix to describe the performance of KNN Classifier on a set of test data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_matrix(y_test, prediction2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting different metrics scores for the KNN Classifier for evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics_score(cm1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Plotting the average of different metrics scores for further evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_score('KNeighbors Classifier', KNN_model, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Receiver Operating Characteristic (ROC) Curve, to illustrate the diagnostic ability of KNN Classifier as its discrimination threshold is varied and showing the Area under the ROC Curve (AUC) value which will tell us how much our model is capable of distinguishing between churn und nonchurn customers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = KNN_model.predict_proba(X_test)\nprobs = probs[:, 1]\nauc2 = roc_auc_score(y_test, probs)\nroc_auc['KNeighbors Classifier'] = np.round(auc2, 2)\nprint('Area under the ROC Curve (AUC): %.2f' % auc2)\nfpr2, tpr2, _ = roc_curve(y_test, probs)\nplot_roc_curve(fpr2, tpr2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Precision-Recall Curve for different thresholds of precision and recall much like the ROC Curve and showing the Area under the Precision-Recall Curve (AUCPR), it gives the number summary of the information in the Precision-Recall Curve.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"precision2, recall2, _ = precision_recall_curve(y_test, probs)\nauc_score2 = auc(recall2, precision2)\npr_auc['KNeighbors Classifier'] = np.round(auc_score2, 2)\nprint('Area under the PR Curve (AUCPR): %.2f' % auc_score2)\nplot_precision_recall_curve(recall2, precision2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Support Vector Machine Classifier:\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df_1['Exited']\nX = df_2.drop('Exited', axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 12345)\nSVC_model = SVC(probability = True)\nSVC_model.fit(X_train, y_train)\nprediction3 = SVC_model.predict(X_test)\naccuracy3 = SVC_model.score(X_test, y_test) \nprint ('Model Accuracy:',accuracy3 * 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Storing model accuracy to plot for comparison with other Machine Learning models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracies['Support Vector Machine Classifier'] = np.round(accuracy3 * 100, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Plotting Confusion Matrix to describe the performance of SVM Classifier on a set of test data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_matrix(y_test, prediction3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting different metrics scores for the SVM Classifier for evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics_score(cm1);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Plotting the average of different metrics scores for further evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_score('Support Vector Machine Classifier', SVC_model, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Receiver Operating Characteristic (ROC) Curve, to illustrate the diagnostic ability of SVM Classifier as its discrimination threshold is varied and showing the Area under the ROC Curve (AUC) value which will tell us how much our model is capable of distinguishing between churn und nonchurn customers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = SVC_model.predict_proba(X_test)\nprobs = probs[:, 1]\nauc3 = roc_auc_score(y_test, probs)\nroc_auc['Support Vector Machine Classifier'] = np.round(auc3, 2)\nprint('Area under the ROC Curve (AUC): %.2f' % auc3)\nfpr3, tpr3, _ = roc_curve(y_test, probs)\nplot_roc_curve(fpr3, tpr3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Precision-Recall Curve for different thresholds of precision and recall much like the ROC Curve and showing the Area under the Precision-Recall Curve (AUCPR), it gives the number summary of the information in the Precision-Recall Curve.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"precision3, recall3, _ = precision_recall_curve(y_test, probs)\nauc_score3 = auc(recall3, precision3)\npr_auc['Support Vector Machine Classifier'] = np.round(auc_score3, 2)\nprint('Area under the PR Curve (AUCPR): %.2f' % auc_score3)\nplot_precision_recall_curve(recall3, precision3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Classification and Regression Tree:\n\n\nDecision Trees are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df_1['Exited']\nX = df_2.drop('Exited', axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 12345)\nCART_model = DecisionTreeClassifier(max_depth = 10, min_samples_split = 50)\nCART_model.fit(X_train, y_train)\nprediction4 = CART_model.predict(X_test)\naccuracy4 = CART_model.score(X_test, y_test) \nprint ('Model Accuracy:',accuracy4 * 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Storing model accuracy to plot for comparison with other Machine Learning models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracies['Classification and Regression Tree'] = np.round(accuracy4 * 100, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Plotting Confusion Matrix to describe the performance of CART Classifier on a set of test data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_matrix(y_test, prediction4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting different metrics scores for the CART Classifier for evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics_score(cm1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Plotting the average of different metrics scores for further evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_score('Classification and Regression Tree', CART_model, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Receiver Operating Characteristic (ROC) Curve, to illustrate the diagnostic ability of CART Classifier as its discrimination threshold is varied and showing the Area under the ROC Curve (AUC) value which will tell us how much our model is capable of distinguishing between churn und nonchurn customers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = CART_model.predict_proba(X_test)\nprobs = probs[:, 1]\nauc4 = roc_auc_score(y_test, probs)\nroc_auc['Desicion Tree Classifier']=np.round(auc4, 2)\nprint('Area under the ROC Curve (AUC): %.2f' % auc4)\nfpr4, tpr4, _ = roc_curve(y_test, probs)\nplot_roc_curve(fpr4, tpr4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Precision-Recall Curve for different thresholds of precision and recall much like the ROC Curve and showing the Area under the Precision-Recall Curve (AUCPR), it gives the number summary of the information in the Precision-Recall Curve.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"precision4, recall4, _ = precision_recall_curve(y_test, probs)\nauc_score4 = auc(recall4, precision4)\npr_auc['Desicion Tree Classifier'] = np.round(auc_score4, 2)\nprint('Area under the PR Curve (AUCPR): %.2f' % auc_score4)\nplot_precision_recall_curve(recall4, precision4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Random Forests:\n\n\nA Random Forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df_1['Exited']\nX = df_2.drop('Exited', axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 12345)\nrf_model = RandomForestClassifier(max_features = 3, min_samples_split = 10, n_estimators = 200)\nrf_model.fit(X_train, y_train)\nprediction5 = rf_model.predict(X_test)\naccuracy5 = rf_model.score(X_test, y_test) \nprint ('Model Accuracy:',accuracy5 * 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#rf_params = {\"n_estimators\": [100, 200, 500, 1000], \"max_features\": [3, 5, 7, 8], \"min_samples_split\": [2, 5, 10, 20]}\n#rf_cv_model = GridSearchCV(rf, rf_params, cv = 10, n_jobs = -1, verbose = 2).fit(X_train, y_train)\n#rf_cv_model.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Storing model accuracy to plot for comparison with other Machine Learning models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracies['Random Forests'] = np.round(accuracy5 * 100, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Plotting Confusion Matrix to describe the performance of Random Forest Classifier on a set of test data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_matrix(y_test, prediction5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting different metrics scores for the Random Forest Classifier for evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics_score(cm1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Plotting the average of different metrics scores for further evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_score('Random Forests', rf_model, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Receiver Operating Characteristic (ROC) Curve, to illustrate the diagnostic ability of Random Forest Classifier as its discrimination threshold is varied and showing the Area under the ROC Curve (AUC) value which will tell us how much our model is capable of distinguishing between churn und nonchurn customers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = rf_model.predict_proba(X_test)\nprobs = probs[:, 1]\nauc5 = roc_auc_score(y_test, probs)\nroc_auc['Random Forests Classifier']=np.round(auc5, 2)\nprint('Area under the ROC Curve (AUC): %.2f' % auc5)\nfpr5, tpr5, _ = roc_curve(y_test, probs)\nplot_roc_curve(fpr5, tpr5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Precision-Recall Curve for different thresholds of precision and recall much like the ROC Curve and showing the Area under the Precision-Recall Curve (AUCPR), it gives the number summary of the information in the Precision-Recall Curve.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"precision5, recall5, _ = precision_recall_curve(y_test, probs)\nauc_score5 = auc(recall5, precision5)\npr_auc['Random Forests'] = np.round(auc_score5,3)\nprint('Area under the PR Curve (AUCPR): %.2f' % auc_score5)\nplot_precision_recall_curve(recall5, precision5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp = pd.Series(rf_model.feature_importances_,\n                        index = X_train.columns).sort_values(ascending = False)\n\nsns.barplot(x = feature_imp, y = feature_imp.index)\nplt.xlabel('Feature Important Scores')\nplt.ylabel('Features')\nplt.title(\"Feature Important Range\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Gradient Boosting Machines","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df_1['Exited']\nX = df_2.drop('Exited', axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 12345)\ngbm_model = GradientBoostingClassifier(learning_rate = 0.01, max_depth = 5, n_estimators = 300)\ngbm_model.fit(X_train, y_train)\nprediction6 = gbm_model.predict(X_test)\naccuracy6 = gbm_model.score(X_test, y_test) \nprint ('Model Accuracy:',accuracy6 * 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#gbm_params = {\"learning_rate\": [0.1, 0.01, 0.001, 0.05],\"n_estimators\": [100, 300, 500, 1000], \"max_depth\":[2, 3, 5, 8]}\n#gbm_cv_model= GridSearchCV(gbm_model, gbm_params, cv = 10, n_jobs = -1, verbose = 2).fit(X_train, y_train)\n#gbm_cv_model.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Storing model accuracy to plot for comparison with other Machine Learning models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracies['Gradient Boosting Machines'] = np.round(accuracy6 * 100, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Plotting Confusion Matrix to describe the performance of GBM Classifier on a set of test data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_matrix(y_test, prediction6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting different metrics scores for the GBM Classifier for evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics_score(cm1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Plotting the average of different metrics scores for further evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_score('Gradient Boosting Machines', gbm_model, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Receiver Operating Characteristic (ROC) Curve, to illustrate the diagnostic ability of GBM Classifier as its discrimination threshold is varied and showing the Area under the ROC Curve (AUC) value which will tell us how much our model is capable of distinguishing between churn und nonchurn customers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = gbm_model.predict_proba(X_test)\nprobs = probs[:, 1]\nauc6 = roc_auc_score(y_test, probs)\nroc_auc['Gradient Boosting Machine Classifier'] = np.round(auc6, 2)\nprint('Area under the ROC Curve (AUC): %.2f' % auc6)\nfpr6, tpr6, _ = roc_curve(y_test, probs)\nplot_roc_curve(fpr6, tpr6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Precision-Recall Curve for different thresholds of precision and recall much like the ROC Curve and showing the Area under the Precision-Recall Curve (AUCPR), it gives the number summary of the information in the Precision-Recall Curve.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"precision6, recall6, _ = precision_recall_curve(y_test, probs)\nauc_score6 = auc(recall6, precision6)\npr_auc['Gradient Boosting Machine Classifier'] = np.round(auc_score6, 2)\nprint('Area under the PR Curve (AUCPR): %.2f' % auc_score6)\nplot_precision_recall_curve(recall6, precision6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp = pd.Series(gbm_model.feature_importances_,\n                        index = X_train.columns).sort_values(ascending = False)\n\nsns.barplot(x = feature_imp, y = feature_imp.index)\nplt.xlabel('Feature Important Scores')\nplt.ylabel('Features')\nplt.title(\"Feature Important Range\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. XGBoost:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df_1['Exited']\nX = df_2.drop('Exited', axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 12345)\nxgb_model = XGBClassifier(learning_rate = 0.01, max_depth = 5, n_estimators = 1000, subsample = 0.8)\nxgb_model.fit(X_train, y_train)\nprediction7 = xgb_model.predict(X_test)\naccuracy7 = xgb_model.score(X_test, y_test) \nprint ('Model Accuracy:',accuracy7 * 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#xgb_params = {\"n_estimators\": [100, 500, 1000], \"subsample\":[0.5, 0.8 ,1], \"max_depth\":[3, 5, 7], \"learning_rate\":[0.1, 0.001, 0.01, 0.05]}\n#xgb_cv_model = GridSearchCV(xgb_model, xgb_params, cv = 10, n_jobs = -1, verbose = 2).fit(X_train, y_train)\n#xgb_cv_model.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Storing model accuracy to plot for comparison with other Machine Learning models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracies['XGBoost Classifier'] = np.round(accuracy7 * 100, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Plotting Confusion Matrix to describe the performance of XGBM Classifier on a set of test data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_matrix(y_test, prediction7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting different metrics scores for the XGBM Classifier for evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics_score(cm1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Plotting the average of different metrics scores for further evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_score('XGBoost Classifier', xgb_model, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Receiver Operating Characteristic (ROC) Curve, to illustrate the diagnostic ability of XGBM Classifier as its discrimination threshold is varied and showing the Area under the ROC Curve (AUC) value which will tell us how much our model is capable of distinguishing between churn und nonchurn customers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = xgb_model.predict_proba(X_test)\nprobs = probs[:, 1]\nauc7 = roc_auc_score(y_test, probs)\nroc_auc['XGB Machine Classifier']=np.round(auc7, 2)\nprint('Area under the ROC Curve (AUC): %.2f' % auc7)\nfpr7, tpr7, _ = roc_curve(y_test, probs)\nplot_roc_curve(fpr7, tpr7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Precision-Recall Curve for different thresholds of precision and recall much like the ROC Curve and showing the Area under the Precision-Recall Curve (AUCPR), it gives the number summary of the information in the Precision-Recall Curve.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"precision7, recall7, _ = precision_recall_curve(y_test, probs)\nauc_score7 = auc(recall7, precision7)\npr_auc['XGB Machine Classifier'] = np.round(auc_score7, 2)\nprint('Area under the PR Curve (AUCPR): %.2f' % auc_score7)\nplot_precision_recall_curve(recall7, precision7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp = pd.Series(gbm_model.feature_importances_,\n                        index = X_train.columns).sort_values(ascending = False)\n\nsns.barplot(x = feature_imp, y = feature_imp.index)\nplt.xlabel('Feature Important Scores')\nplt.ylabel('Features')\nplt.title(\"Feature Important Range\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8. Light GBM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df_1['Exited']\nX = df_2.drop('Exited', axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 12345)\nlgbm_model = LGBMClassifier(learning_rate = 0.1, max_depth = 2, n_estimators = 500)\nlgbm_model.fit(X_train, y_train)\nprediction8 = lgbm_model.predict(X_test)\naccuracy8 = lgbm_model.score(X_test, y_test) \nprint ('Model Accuracy:',accuracy8 * 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lgbm_params = {\"learning_rate\": [0.001, 0.01, 0.1], \"n_estimators\": [200, 500, 100], \"max_depth\":[1,2,5,8]}\n#lgbm_cv_model = GridSearchCV(lgbm_model,lgbm_params, cv = 10, n_jobs = -1, verbose = 2).fit(X_train, y_train)\n#lgbm_cv_model.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Storing model accuracy to plot for comparison with other Machine Learning models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracies['LightGBM Classifier'] = np.round(accuracy8 * 100, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Plotting Confusion Matrix to describe the performance of LGBM Classifier on a set of test data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_matrix(y_test, prediction8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting different metrics scores for the LGBM Classifier for evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics_score(cm1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Plotting the average of different metrics scores for further evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_score('LightGBM Classifier', lgbm_model, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Receiver Operating Characteristic (ROC) Curve, to illustrate the diagnostic ability of LGBM Classifier as its discrimination threshold is varied and showing the Area under the ROC Curve (AUC) value which will tell us how much our model is capable of distinguishing between churn und nonchurn customers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = lgbm_model.predict_proba(X_test)\nprobs = probs[:, 1]\nauc8 = roc_auc_score(y_test, probs)\nroc_auc['LightGBM Classifier'] = np.round(auc8, 2)\nprint('Area under the ROC Curve (AUC): %.2f' % auc8)\nfpr8, tpr8, _ = roc_curve(y_test, probs)\nplot_roc_curve(fpr8, tpr8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting Precision-Recall Curve for different thresholds of precision and recall much like the ROC Curve and showing the Area under the Precision-Recall Curve (AUCPR), it gives the number summary of the information in the Precision-Recall Curve.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"precision8, recall8, _ = precision_recall_curve(y_test, probs)\nauc_score8 = auc(recall8, precision8)\npr_auc['LightGBM Classifier'] = np.round(auc_score8, 2)\nprint('Area under the PR Curve (AUCPR): %.2f' % auc_score8)\nplot_precision_recall_curve(recall8, precision8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp = pd.Series(gbm_model.feature_importances_,\n                        index = X_train.columns).sort_values(ascending = False)\n\nsns.barplot(x = feature_imp, y = feature_imp.index)\nplt.xlabel('Feature Important Scores')\nplt.ylabel('Features')\nplt.title(\"Feature Important Range\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Performance Comparison\n\nPlotting the accuracy metric score of the machine learning models for comparison.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodels_tuned = [\n    log_model,\n    KNN_model,\n    SVC_model,\n    CART_model,\n    rf_model,\n    gbm_model,\n    lgbm_model,\n    xgb_model]\n\nresult = []\nresults = pd.DataFrame(columns = [\"Models\",\"Accuracy\"])\n\nfor model in models_tuned:\n    names = model.__class__.__name__\n    y_pred = model.predict(X_test)\n    acc = accuracy_score(y_test, y_pred)\n    scores = cross_val_score(model, X_test, y_test, cv = 10, scoring = 'accuracy')\n    result = pd.DataFrame([[names, acc * 100, \n                            np.mean(scores) * 100]], \n                          columns = [\"Models\", \"Accuracy\", \"Avg_Accuracy\"])\n    results = results.append(result)\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15, 8))\nsns.set_palette('cividis')\nax = sns.barplot(x = list(accuracies.keys()), y = list(accuracies.values()))\nplt.yticks(np.arange(0, 100, 10))\nplt.ylabel('Percentage %', labelpad = 10)\nplt.xlabel('Algorithms', labelpad = 10)\nplt.title('Accuracy Scores Comparison', pad = 20)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x(), p.get_height()), xytext = (p.get_x() + 0.3, p.get_height() + 1.02))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nPlotting the average accuracy metric score of the machine learning models for comparison.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15, 8))\nsns.set_palette('viridis')\nax=sns.barplot(x = list(avg_accuracies.keys()), y = list(avg_accuracies.values()))\nplt.yticks(np.arange(0, 100, 10))\nplt.ylabel('Percentage %', labelpad = 10)\nplt.xlabel('Algorithms', labelpad = 10)\nplt.title('Average Accuracy Scores Comparison', pad = 20)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x(), p.get_height()),xytext=(p.get_x() + 0.3, p.get_height() + 1.02))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nPlotting the ROC Curve of the machine learning models for comparison.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (8, 6))\nsns.set_palette('Set1')\nplt.plot(fpr1, tpr1, label = 'Linear Regression')\nplt.plot(fpr2, tpr2, label = 'KNeiihbors Classifier')\nplt.plot(fpr3, tpr3, label = 'SVM')\nplt.plot(fpr4, tpr4, label = 'Decision Tree')\nplt.plot(fpr5, tpr5, label = 'Random Forests')\nplt.plot(fpr6, tpr6, label = 'Gradient Boosting MachineC')\nplt.plot(fpr7, tpr7, label = 'XGBoost')\nplt.plot(fpr8, tpr8, label = 'LightGBM')\nplt.plot([0, 1], [0, 1], linestyle = '--')\nplt.ylabel('True Positive Rate', labelpad = 10)\nplt.xlabel('False Positive Rate', labelpad = 10)\nplt.title('Receiver Operating Characteristic (ROC) Curves', pad = 20)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting the AUC values of ROC Curve of the machine learning models for comparison.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15, 8))\nsns.set_palette('magma')\nax = sns.barplot(x = list(roc_auc.keys()), y = list(roc_auc.values()))\n#plt.yticks(np.arange(0,100,10))\nplt.ylabel('Score', labelpad = 10)\nplt.xlabel('Algorithms', labelpad = 10)\nplt.title('Area under the ROC Curves (AUC)', pad = 20)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x(), p.get_height()), xytext = (p.get_x() + 0.3, p.get_height() + 0.01))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nPlotting the PR Curve of the machine learning models for comparison.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (8, 6))\nsns.set_palette('Set1')\nplt.plot(recall1, precision1, label = 'Linear Regression PRC')\nplt.plot(recall2, precision2, label = 'KNN PRC')\nplt.plot(recall3, precision3, label = 'SVM PRC')\nplt.plot(recall4, precision4, label = 'CART PRC')\nplt.plot(recall5, precision5, label = 'Random Forests PRC')\nplt.plot(recall6, precision6, label = 'GBM PRC')\nplt.plot(recall7, precision7, label = 'XGB PRC')\nplt.plot(recall8, precision8, label = 'LGBM PRC')\nplt.ylabel('Precision', labelpad = 10)\nplt.xlabel('Recall', labelpad = 10)\nplt.title('Precision Recall Curves', pad = 20)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting the AUC values of PR Curve of the machine learning models for comparison.","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15, 8))\nsns.set_palette('mako')\nax = sns.barplot(x = list(pr_auc.keys()), y = list(pr_auc.values()))\nplt.ylabel('Score', labelpad = 10)\nplt.xlabel('Algorithms', labelpad = 10)\nplt.title('Area under the PR Curves (AUCPR)', pad = 20)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x(), p.get_height()), xytext = (p.get_x() + 0.3, p.get_height() + 0.01))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# - - - -  REPORTING  - - - -\n\n\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Our aim in this project was to develop a churn prediction model using machine learning algorithms.\n\n### There were 10000 rows in the data set and there were no missing values.\n\n### The dataset consisted of 13 variables.\n\n### The following conclusions came from the analysis on the features:\n\n* Most customers who using products 3 and 4 stopped working with the bank. In fact, all customers using product number 4 were gone.\n* Customers between the ages of 40 and 65 were more likely to quit the bank.\n* Those who had a credit score below 450 had high abandonment rates.\n* Predictions were made with a total of 8 classification models. The highest head was taken with Random Forests.\n* Accuracy and cross validation scores were calculated for each model and results were displayed.\n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}