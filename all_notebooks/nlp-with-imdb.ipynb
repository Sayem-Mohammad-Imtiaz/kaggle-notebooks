{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\n# Modules for data manipulation\nimport numpy as np\nimport pandas as pd\nimport re\n\n# Modules for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\n# Tools for preprocessing input data\nfrom bs4 import BeautifulSoup\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# Tools for creating ngrams and vectorizing input data\nfrom gensim.models import Word2Vec, Phrases\n\n# Tools for building a model\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout, Bidirectional\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing.sequence import pad_sequences\n\n# Tools for assessing the quality of model prediction\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nimport os\nfrom keras.datasets import imdb\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom keras.preprocessing.text import Tokenizer  \nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import models\nfrom keras import layers\nfrom keras import losses\nfrom keras import metrics\nfrom keras import optimizers\nfrom keras.utils import plot_model\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nfrom collections import Counter\nfrom pathlib import Path\nimport os\nimport numpy as np\nimport re\nimport string\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')\nfrom nltk.corpus import wordnet\nimport unicodedata\nimport html\nstop_words = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data  =pd.read_csv( '../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sentiment count\ndata['sentiment'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#mapping \nfrom sklearn import preprocessing \nlabel_enc  = preprocessing.LabelEncoder()\ndata['sentiment'] = label_enc.fit_transform(data['sentiment'])\nlabels = data['sentiment']\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now, let's see the average number of words per sample\nplt.figure(figsize=(10, 6))\nplt.hist([len(sample) for sample in list(data['review'])], 50)\nplt.xlabel('Length of samples')\nplt.ylabel('Number of samples')\nplt.title('Sample length distribution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_data = data['review']\ny_data = data['sentiment']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = set(stopwords.words(\"english\")) \nlemmatizer = WordNetLemmatizer()\n\n\ndef clean_text(text):\n    text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n    text = text.lower()\n    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n    text = [word for word in text if not word in stop_words]\n    text = \" \".join(text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Cleaning the data***"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.review.apply(lambda x: clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_special_characters(text, remove_digits=True):\n    pattern=r'[^a-zA-z0-9\\s]'\n    text=re.sub(pattern,'',text)\n    return text\n#Apply function on review column\ndata['review']=data['review'].apply(remove_special_characters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set stopwords to english\nstop=set(stopwords.words('english'))\nfrom nltk.tokenize.toktok import ToktokTokenizer\ntokenizer = ToktokTokenizer()\nstopword_list=nltk.corpus.stopwords.words('english')\n#removing the stopwords\ndef remove_stopwords(text, is_lower_case=False):\n    tokens = tokenizer.tokenize(text)\n    tokens = [token.strip() for token in tokens]\n    if is_lower_case:\n        filtered_tokens = [token for token in tokens if token not in stopword_list]\n    else:\n        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n    filtered_text = ' '.join(filtered_tokens)    \n    return filtered_text\n#Apply function on review column\ndata['review']=data['review'].apply(remove_stopwords)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"texts = data['review']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split the dataset  \n#train dataset\ntrain_reviews=data.review[:40000]\ntrain_sentiments=data.sentiment[:40000]\n#test dataset\ntest_reviews=data.review[40000:]\ntest_sentiments=data.sentiment[40000:]\nprint(train_reviews.shape,train_sentiments.shape)\nprint(test_reviews.shape,test_sentiments.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#start by using bag of words module by using keras \nvocab_sz = 10000\ntok = Tokenizer(num_words = vocab_sz,oov_token='UNK')\ntok.fit_on_texts(data['review'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#extract binary bag of words\nx_train = tok.texts_to_matrix(train_reviews, mode='binary')\nx_test = tok.texts_to_matrix(test_reviews, mode='binary')\ny_train = np.asarray(train_sentiments).astype('float32')\ny_test = np.asarray(test_sentiments).astype('float32')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.4, random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer=optimizers.RMSprop(lr=0.001),\n              loss=losses.binary_crossentropy,\n              metrics=[metrics.binary_accuracy])\n\nhistory = model.fit(x_train,\n                    y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(x_val, y_val))\nhistory_dict = history.history\nhistory_dict.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nacc = history.history['binary_accuracy']\nval_acc = history.history['val_binary_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"norm_test_reviews=data.review[40000:]\nnorm_test_reviews[40000+4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ntrn_texts,trn_labels = get_texts(PATH/'train')\ntst_texts,tst_labels = get_texts(PATH/'test')\n\ntrn_texts = normalize_corpus(trn_texts)\ntst_texts = normalize_corpus(tst_texts)\n\nvocab_sz = 10000\ntok = Tokenizer(num_words=vocab_sz, oov_token='UNK')\ntok.fit_on_texts(trn_texts + tst_texts)\n#vocab_sz = len(tok.word_index)\n'''\n# Extract binary BoW features\nx_train = tok.texts_to_matrix(train_reviews, mode='tfidf')\nx_test = tok.texts_to_matrix(test_reviews, mode='tfidf')\ny_train = np.asarray(train_sentiments).astype('float32')\ny_test = np.asarray(test_sentiments).astype('float32')\n\n\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.4, random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train[1].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer=optimizers.RMSprop(lr=0.001),\n              loss=losses.binary_crossentropy,\n              metrics=[metrics.binary_accuracy])\n\t\t\t  \nhistory = model.fit(x_train,\n                    y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(x_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history.history['binary_accuracy']\nval_acc = history.history['val_binary_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.clf()   # clear figure\nacc_values = history_dict['binary_accuracy']\nval_acc_values = history_dict['val_binary_accuracy']\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#by using Embedding layers\nmax_len = 50\nfrom keras.layers import Embedding\ne = Embedding(10000, 32, input_length=max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"easd = data['review'][0]\nlen(seq[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#new ideas\n# define documents\ndocs = ['Well done!',\n'Good work',\n'Great effort',\n'nice work',\n'Excellent!',\n'Weak',\n'Poor effort!',\n'not good',\n'poor work',\n'Could have done better.']\n# define class labels\nlabels = [1,1,1,1,1,0,0,0,0,0]\n# prepare tokenizer\nt = Tokenizer()\nt.fit_on_texts(docs)\nvocab_size = len(t.word_index) + 1\n# integer encode the documents\nencoded_docs = t.texts_to_sequences(docs)\nprint(encoded_docs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_length = 4\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\nprint(padded_docs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.datasets import imdb\nmax_features = 10000\nmaxlen = 20\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\nx_train = pad_sequences(x_train, maxlen=maxlen)\nx_test = pad_sequences(x_test, maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Flatten, Dense\nmodel = Sequential()\nmodel.add(Embedding(10000, 8, input_length=maxlen))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(x_train, y_train,\nepochs=10,\nbatch_size=32,\nvalidation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#by using glove embedding loading all embedding pakage into meomory\n#loop for all words in the dataset and compare each word to the embedding matrix\nimport numpy as np\nembeddings_index = dict()\nf = open('../input/glove6b300dtxt/glove.6B.300d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\nprint('Loaded %s word vectors.' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#new GLOVE****"},{"metadata":{"trusted":true},"cell_type":"code","source":"#i have 300 dimensional in glove","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tokenizing the data\nmaxlen = 300\ntraining_samples = 200\nvalidation_samples = 10000\nmax_words = 10000\n#######################\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['sentiment']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nvocab_size = 10000\n(x_train, y_train), (x_val, y_val) = imdb.load_data(num_words=vocab_size)\n\ny_train = np.asarray(y_train).astype('float32')\ny_val = np.asarray(y_val).astype('float32')\n\n\nmax_length = 300\nx_train = pad_sequences(x_train, maxlen=max_length, padding='post')\nx_val = pad_sequences(x_val, maxlen=max_length, padding='post')\n\n\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.4, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_val.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_val.shape)\nprint(x_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indices.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dim = 300\nembedding_matrix = np.zeros((max_words, embedding_dim))\nfor word, i in word_index.items():\n    if i < max_words:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Embedding, Flatten, Dense\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen))\nmodel.add(Flatten())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n              loss=losses.binary_crossentropy,\n              metrics=[metrics.binary_accuracy])\n\t\t\t  \nhistory = model.fit(x_train,\n                    y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(x_val, y_val))\nhistory_dict = history.history\nhistory_dict.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history.history['binary_accuracy']\nval_acc = history.history['val_binary_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.clf()   # clear figure\nacc_values = history_dict['binary_accuracy']\nval_acc_values = history_dict['val_binary_accuracy']\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.layers[0].set_weights([embedding_matrix])\nmodel.layers[0].trainable = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='rmsprop',\nloss='binary_crossentropy',\nmetrics=['acc'])\nhistory = model.fit(x_train, y_train,\nepochs=10,\nbatch_size=32,\nvalidation_data=(x_val, y_val))\nmodel.save_weights('pre_trained_glove_model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}