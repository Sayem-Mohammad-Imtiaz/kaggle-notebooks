{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Gradient Descent : **\n\nGradient descent is a calculus based method to find the least-squared fit for a given set of data. Its starts with arbitrarily chosen importance of multiple variables (usually [0, 0] if there is only one feature) and then adjusts iteratively in an effort to reach the global minima. \n\n![](https://miro.medium.com/max/1593/1*WGHn1L4NveQ85nn3o7Dd2g.png)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport sys","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"MaxIterations = 2000 #Number of the times thetas will change\nalpha = 0.01 \nCostArray = [] #This will store the cost of each set of thetas ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data:** \n     \nThe data I will use to write the gradient descent algorithm comes from andrew ng's famous course on machine learning on coursera. it's a housing database with first column denoting the area of the house while the second column denotes the population of the area. Last column denotes the price of the house which we will consider to be the dependent variable. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/ex1data2.csv', sep=\",\", header=None)\nnumberOfColumns = data.shape[0]\nthetas = [0]*len(data.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Hypothesis : **\nHypothesis is the line or plane that is proposed using the thetas (for a single variable it will be y = mx + c line in cartesian plane). "},{"metadata":{"trusted":true},"cell_type":"code","source":"    \ndef hypothesis(theta, Xaxis):\n    thetaArray = np.matrix(np.array(theta)) \n    Xaxis = np.matrix(Xaxis)\n    xtrans = np.transpose(Xaxis) \n    mat =  np.matmul(thetaArray, xtrans)\n    return mat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Cost Function: **\n\nCost fuction can tell us how far our solution is from the data points that we have. Formula for finding cost function would be: \n\n![](http://s0.wp.com/latex.php?zoom=1.5&latex=J%28%5Ctheta%29+%3D+%5Cfrac%7B1%7D%7B2m%7D%5Csum%7B%28h_%7B%5Ctheta%7D%28x%5E%7B%28i%29%7D%29+-+y%5E%7B%28i%29%7D%29%5E2%7D&bg=ffffff&fg=000&s=0)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def costFunction(thetas, Xaxis, Yaxis):\n    resultingMatrix = hypothesis(thetas, Xaxis) - np.matrix(Yaxis)\n    totalSum = np.sum(np.square(resultingMatrix))\n    totalCost = totalSum / (2*(numberOfColumns))\n    CostArray.append(totalCost)\n    return totalCost\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Gradient Descent**\n\nTaking the derivative of the cost function we will have to loop through many time to get to the point where cost function is minimized. Ideally we will get the derivative of cost function to be 0, which means that we will have reached a minima. However, a potential problem that can occur in this way of solving the problem is that we could reach a local minima and not the global minima. \n\n![](https://miro.medium.com/max/765/1*QKHtyn4Rr-0R-s0an1eSsA.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def updateThetas(theta):\n    temp = np.matrix(np.array(theta))\n    resultingMatrix = hypothesis(theta, Xaxis) - np.matrix(Yaxis)\n    X2 = np.matrix(Xaxis)\n    multiplier = np.matmul(resultingMatrix, X2)\n    temp = np.matrix(np.array(theta)) - ((alpha/(numberOfColumns))* multiplier)\n    global thetas\n    thetas = temp ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ones = pd.Series([1]*(data.shape[0]))\n\nXaxis = (data.iloc[:, :-1] - np.mean(data.iloc[:, :-1]))\nXaxis = Xaxis/np.std(data.iloc[:, :-1])\nXaxis = pd.concat([ones, Xaxis], axis=1)\nYaxis = data[data.columns[-1]]\n\nfor j in range(MaxIterations):\n    updateThetas(thetas)\n    CostArray.append(costFunction(thetas, Xaxis, Yaxis))\nprint(CostArray[1999])\nprint(thetas)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As noted from the results above the bias variable has the importance of 340412 which means that a house will atleast have that much minimum value without cosidering the other features. The second variable also has a strong positive impact (109447) whereas the last variable tends to depreciate the value of houses "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(CostArray)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can observe that cost had been minimized after 500 repetitions. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}