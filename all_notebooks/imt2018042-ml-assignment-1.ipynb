{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Instructions\n1. We will be conducting the entire assignment through this notebook. You will be entering your code in the cells provided, and any explanation and details asked in markdown cells. \n2. You are free to add more code and markdown cells for describing your answer, but make sure they are below the question asked and not somewhere else. \n3. The notebook needs to be submitted on LMS. You can find the submission link [here](https://lms.iiitb.ac.in/moodle/mod/assign/view.php?id=13932). \n4. The deadline for submission is **5th October, 2020 11:59PM**."},{"metadata":{},"cell_type":"markdown","source":"# Data import\nThe data required for this assignment can be downloaded from the following [link](https://www.kaggle.com/dataset/e7cff1a2c6e29e18684fe6b077d3e4c42f9a7ae6199e01463378c60fe4b4c0cc), it's hosted on kaggle. Do check directory paths on your local system.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"alcdata = pd.read_csv(\"../input/iiitb-ai511ml2020-assignment-1/Assignment/alcoholism/student-mat.csv\")\nfifadata = pd.read_csv(\"../input/iiitb-ai511ml2020-assignment-1/Assignment/fifa18/data.csv\")\naccidata1 = pd.read_csv(\"../input/iiitb-ai511ml2020-assignment-1/Assignment/accidents/accidents_2005_to_2007.csv\")\naccidata2 = pd.read_csv(\"../input/iiitb-ai511ml2020-assignment-1/Assignment/accidents/accidents_2009_to_2011.csv\")\naccidata3 = pd.read_csv(\"../input/iiitb-ai511ml2020-assignment-1/Assignment/accidents/accidents_2012_to_2014.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part - 1\n## Alcohol Consumption Data\nThe following data was obtained in a survey of students' math course in secondary school. It contains a lot of interesting social, gender and study information about students. \n"},{"metadata":{},"cell_type":"markdown","source":"### 1. Try to visualize correlations between various features and grades and see which features have a significant impact on grades. \nTry to engineer the three grade parameters (G1, G2 and G3) as one feature for such comparisons.\n\n"},{"metadata":{},"cell_type":"markdown","source":"#### Engineering Grades as G_avg which is mean of G1,G2 and G3, and dropping them."},{"metadata":{"trusted":true},"cell_type":"code","source":"G_avg=(alcdata.G1+alcdata.G2+alcdata.G3)/3\nalcdata[\"G_avg\"]=G_avg\nalcdata.drop([\"G1\",\"G2\",\"G3\"],axis=1,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plotting Correlation heatmap to study relatinship between non-categorical features between features and grades"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,10))\nsns.color_palette(\"ocean\")\nsns.heatmap(alcdata.corr(),annot=True,cmap=\"coolwarm\",vmin=-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### From the above matrix we see that no two features have a correlation of more than 0.95, so we dont have to drop any columns"},{"metadata":{},"cell_type":"markdown","source":"#### Studying impact of some features on grades"},{"metadata":{"trusted":true},"cell_type":"code","source":"alcdata.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sex vs Grades"},{"metadata":{},"cell_type":"markdown","source":"#### Plotting boxplot"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=alcdata[\"G_avg\"],y=alcdata[\"sex\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation:\n#### From the above plots we see that the grade distribution for males is towards the right, whereas for females its shifted to a little left. This implies that males tend to score a higher grade as compared to females, but the difference is not much.\n#### In the count plot also, on the left side of G_avg, female peaks are more, and on the right, male peaks are more."},{"metadata":{},"cell_type":"markdown","source":"### Romantic vs grades"},{"metadata":{},"cell_type":"markdown","source":"#### Plotting boxplot"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=alcdata[\"G_avg\"],y=alcdata[\"romantic\"],hue=alcdata[\"romantic\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation:\n#### From the above two plots, it is evident that students with no relationships perform better as their distribution is more towards the high side of grades as comapares to students with relationships, as seen in the boxplot above"},{"metadata":{},"cell_type":"markdown","source":"### Traveltime vs Grades"},{"metadata":{},"cell_type":"markdown","source":"* ### Plotting barplot as it is a better represention of the whole data "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x=alcdata[\"traveltime\"],y=alcdata[\"G_avg\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation:\n#### This clearly indicates that people whi have less traveltime perform better than students with more traveltime."},{"metadata":{},"cell_type":"markdown","source":"### 2. If there is a need for encoding some of the features,  how would you go  about it? \nWould you consider combining certain encodings together ?\n"},{"metadata":{},"cell_type":"markdown","source":"* ### I have used a mix of binary encoding (label encoding) for objects which have two unique values, and one-hot for other objeccts. I checked that there are no null values in the dataset, so we dont have to take care of that."},{"metadata":{"trusted":true},"cell_type":"code","source":"a=alcdata.select_dtypes(include=[\"object\"])\na=a.columns\nalcdata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\np=alcdata[a].nunique().apply(lambda x: x==2)\nmy_label=p[p].index\nmy_label=alcdata[my_label]\nmy_label=my_label.apply(LabelEncoder().fit_transform)\nk=alcdata[a].nunique().apply(lambda x: x!=2)\nmy_onehot=k[k].index\nmy_onehot=alcdata[my_onehot]\nmy_onehot","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### my_onehot contains encodings which had objects having >2 unique values and my_labels contains other object types which are binary encoded."},{"metadata":{},"cell_type":"markdown","source":"* ### Dropping all object types and adding their encoded forms."},{"metadata":{"trusted":true},"cell_type":"code","source":"#enter code/answer in this cell. You can add more code/markdown cells below for your answer. \n\ndumm_bin = pd.get_dummies(my_onehot)\ndumm_bin\nnew_alcdata=alcdata.drop(p.index,axis=1)\nnew_alcdata=pd.concat([new_alcdata,dumm_bin,my_label],axis=1)\n\nnew_alcdata\n# my_onehot\n# alcdata\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Now everything is numerical"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_alcdata.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n### 3. Try to find out how family relation(famrel) and parents cohabitation(Pstatus) affect grades of students. \n"},{"metadata":{},"cell_type":"markdown","source":"* ### Here, correlation matrix is not a good measure to find dependence as this is ordinal data, so I have used plots to view dependence."},{"metadata":{},"cell_type":"markdown","source":"* ### Plotting barplot as it represents mean of data"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x=alcdata[\"Pstatus\"],y=alcdata[\"G_avg\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ## Explanation:\n* ### Students whose parents are apart perform better on an average as compared to students whose parents live together.( Not as expected though :))"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x=alcdata[\"famrel\"],y=alcdata[\"G_avg\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Famrel does'nt really affect the grades much, but students' grades first decrease with increasing famrel( which means bad) and then again increase a bit."},{"metadata":{},"cell_type":"markdown","source":"\n### 4. Figure out which features in the data are skewed, and propose a way to remove skew from all such columns. "},{"metadata":{"trusted":true},"cell_type":"code","source":"alcdata.skew()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ## From the above table, it is evident that features like absences,failures, trevaeltime and Dalc are highly skewed.I have shown removing skew for absences, but similarly skew for other features can be removed.\n* ## For data with left skew, we can take sqrt or log, and for right, we can take any power of data."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(new_alcdata.absences,bins=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Its left skew, so we normalise and take sqrt to remove skew."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#enter code/answer in this cell. You can add more code/markdown cells below for your answer. \n\ndef min_max_transform(x):\n    return (x-x.min())/(x.max()-x.min())\nsns.distplot(min_max_transform(new_alcdata.absences)**0.5,bins=100)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part - 2\n## FIFA 2019  Data\n"},{"metadata":{},"cell_type":"markdown","source":"### 1. Which clubs are the most economical? How did you decide that?"},{"metadata":{},"cell_type":"markdown","source":"* ## My criteria to decide economical was adding release clause and value of a player, as these are contibuting factores, and subtracting wage as it is a deducting factor,and then divide by overall as it is a key in denoting performance of a club."},{"metadata":{},"cell_type":"markdown","source":"* ### Preprocessing wage, value and release clause to remove € sign and convert M and K to 10^6 and 10^3 respectively\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"fifadata[\"Release Clause\"]=fifadata[\"Release Clause\"].replace('[\\€,]', '', regex=True).replace('M','e06' , regex=True).replace('K','e03' , regex=True).astype(float)\nfifadata[\"Value\"]= fifadata[\"Value\"].replace('[\\€,]', '', regex=True).replace('M','e06' , regex=True).replace('K','e03' , regex=True).astype(float)\nfifadata[\"Wage\"]= fifadata[\"Wage\"].replace('[\\€,]', '', regex=True).replace('M','e06' , regex=True).replace('K','e03' , regex=True).astype(float)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Caluclating club economy, and sortng in decreasing order"},{"metadata":{"trusted":true},"cell_type":"code","source":"club_economy=fifadata[[\"Wage\",\"Value\",\"Club\",\"Release Clause\",\"Overall\"]].groupby([\"Club\"]).sum()\neconomical=(club_economy[\"Release Clause\"]+club_economy.Value-club_economy.Wage)/club_economy[\"Overall\"]\neconomical.sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### So, Real Madrid is the most economical club along with FC Barcelona, Juventus and Man City."},{"metadata":{},"cell_type":"markdown","source":"### 2. What is the relationship between age and individual potential of the player? How does age influence the players' value? At what age does the player exhibit peak pace ?"},{"metadata":{},"cell_type":"markdown","source":"* ### Plotting linegraph of Potential vs Age"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(fifadata.Age,fifadata.Potential)\n# sns.barplot(fifadata.Age,fifadata.Potential)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### This shows that Potential of the palyer decreases with age. The spike in the end is due to outliers, as data density is almost zero there."},{"metadata":{},"cell_type":"markdown","source":"* ### Plotting linegraph of Value vs Age"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(fifadata.Age,fifadata.Value)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### This shows that the Value of a player first increases till a certain age about 31, and then starts decreases, as expected in real life."},{"metadata":{},"cell_type":"markdown","source":"* ### Plotting linegraph of Pace(SprintSpeed) vs Age"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(fifadata.Age,fifadata.SprintSpeed)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Pace increases slightly till 26 years of age , then decreases with age as expected"},{"metadata":{},"cell_type":"markdown","source":"### 3. What skill sets are helpful in deciding a player's potential? How do the traits contribute to the players' potential? "},{"metadata":{"trusted":true},"cell_type":"code","source":"fifadata.corr()[\"Potential\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Looking at above table, Potential is not much correlated to anything, but when we plot graphs of traits and skills  like, penalties, strength,Heading Acuracy, etc, it is a posititvely sloped graph\n* ### I'll be plotting some of them\n### I have considered Skills- Penalties,HeadingAccuracy and Crossing.\n### Traits : Shotpower, Reactions, and Weak Foot.\n\n### There are many others, but I have considered some of them as I feel these are relatively more important in real football. "},{"metadata":{},"cell_type":"markdown","source":"* ### Contribution of Penalties to Potential"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(fifadata[\"Penalties\"],fifadata[\"Potential\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Potential is posotively related to Penalties"},{"metadata":{},"cell_type":"markdown","source":"* ### Contribution of HeadingAccuracy to Potential"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(fifadata[\"HeadingAccuracy\"],fifadata[\"Potential\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Potential is posotively related to HeadingAccuracy"},{"metadata":{},"cell_type":"markdown","source":"* ### Contribution of Crossing to Potential"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(fifadata[\"Crossing\"],fifadata[\"Potential\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Potential is posotively related to Crossing"},{"metadata":{},"cell_type":"markdown","source":"* ### Contribution of ShotPower to Potential"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(fifadata[\"ShotPower\"],fifadata[\"Potential\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Potential is posotively related to ShotPower"},{"metadata":{},"cell_type":"markdown","source":"* ### Reactions vs Potential"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(fifadata[\"Reactions\"],fifadata[\"Potential\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Potential is positively related to Reactions."},{"metadata":{},"cell_type":"markdown","source":"* ### Weak Foot vs Potential"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(fifadata[\"Weak Foot\"],fifadata[\"Potential\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear relationship between weak foot and Potential"},{"metadata":{},"cell_type":"markdown","source":"### 4. Which features directly contribute to the wages of the players?"},{"metadata":{"trusted":true},"cell_type":"code","source":"#enter code/answer in this cell. You can add more code/markdown cells below for your answer. \nfifadata.corr()[\"Wage\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Looking at above table, Wage is mostly related to Value, Overall, International Reputation, Potential and Release Clause"},{"metadata":{},"cell_type":"markdown","source":"* ### Plotting Line plot between Value and Wage"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(fifadata[\"Value\"],fifadata[\"Wage\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Plotting Line plot between Overall and Wage"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(fifadata[\"Overall\"],fifadata[\"Wage\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Plotting Line plot between International Reputation and Wage"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(fifadata[\"International Reputation\"],fifadata[\"Wage\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Plotting Line plot between Release Clause and Wage"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(fifadata[\"Release Clause\"],fifadata[\"Wage\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Wages  are positively related to all of the above mentioned features"},{"metadata":{},"cell_type":"markdown","source":"### 5. What is the age distribution in different clubs? Which club has most players young?"},{"metadata":{},"cell_type":"markdown","source":"* ### I have used mean of age of players in each club as a measure of youngness of a club, then sorted to get club with lowest average age."},{"metadata":{"trusted":true},"cell_type":"code","source":"tp1=fifadata[[\"Age\",\"Club\"]].groupby([\"Club\"]).describe()\ntp1.sort_values([('Age',  'mean')])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### FC Nordsjælland is the player with most young players"},{"metadata":{},"cell_type":"markdown","source":"# Part - 3\n## UK Road Accidents Data\n\n\nThe UK government amassed traffic data from 2000 and 2016, recording over 1.6 million accidents in the process and making this one of the most comprehensive traffic data sets out there. It's a huge picture of a country undergoing change."},{"metadata":{},"cell_type":"markdown","source":"### 1. The very first step should be to merge all the 3 subsets of the data."},{"metadata":{},"cell_type":"markdown","source":"* ### Concatenating datasets to make new dataset accidents"},{"metadata":{"trusted":true},"cell_type":"code","source":"accidents=pd.concat([accidata1,accidata2,accidata3],axis=0)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accidents","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. What are the number of casualties in each day of the week? Sort them in descending order. "},{"metadata":{},"cell_type":"markdown","source":"* ### Calculating Casualties on each day of week, using python  dictionaries"},{"metadata":{"trusted":true},"cell_type":"code","source":"#enter code/answer in this cell. You can add more code/markdown cells below for your answer. \nweek_casualty={\"Mon\":0,\"Tue\":0,\"Wed\":0,\"Thurs\":0,\"Fri\":0,\"Sat\":0,\"Sun\":0}\nweek_casualty[\"Mon\"]=accidents[accidents.Day_of_Week==1].Number_of_Casualties.sum()\nweek_casualty[\"Tue\"]=accidents[accidents.Day_of_Week==2].Number_of_Casualties.sum()\nweek_casualty[\"Wed\"]=accidents[accidents.Day_of_Week==3].Number_of_Casualties.sum()\nweek_casualty[\"Thurs\"]=accidents[accidents.Day_of_Week==4].Number_of_Casualties.sum()\nweek_casualty[\"Fri\"]=accidents[accidents.Day_of_Week==5].Number_of_Casualties.sum()\nweek_casualty[\"Sat\"]=accidents[accidents.Day_of_Week==6].Number_of_Casualties.sum()\nweek_casualty[\"Sun\"]=accidents[accidents.Day_of_Week==7].Number_of_Casualties.sum()\nweek_casualty","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Sorting the dictionary in descending order"},{"metadata":{"trusted":true},"cell_type":"code","source":"# {k: v for k, v in sorted(l.items(), key=lambda item: item[1])}\n# l=sorted(l,key=lambda item:item[1])\n{k: v for k, v in sorted(week_casualty.items(), key=lambda item: item[1],reverse=True)}\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. On each day of the week, what is the maximum and minimum speed limit on the roads the accidents happened?"},{"metadata":{"trusted":true},"cell_type":"code","source":"#enter code/answer in this cell. You can add more code/markdown cells below for your answer. \n\nday_speed={\"Mon\":(0,0),\"Tue\":(0,0),\"Wed\":(0,0),\"Thurs\":(0,0),\"Fri\":(0,0),\"Sat\":(0,0),\"Sun\":(0,0)}\nday_speed[\"Mon\"]=(accidents[accidents.Day_of_Week==1].Speed_limit.min(),accidents[accidents.Day_of_Week==1].Speed_limit.max())\nday_speed[\"Tue\"]=(accidents[accidents.Day_of_Week==2].Speed_limit.min(),accidents[accidents.Day_of_Week==1].Speed_limit.max())\nday_speed[\"Wed\"]=(accidents[accidents.Day_of_Week==3].Speed_limit.min(),accidents[accidents.Day_of_Week==1].Speed_limit.max())\nday_speed[\"Thurs\"]=(accidents[accidents.Day_of_Week==4].Speed_limit.min(),accidents[accidents.Day_of_Week==1].Speed_limit.max())\nday_speed[\"Fri\"]=(accidents[accidents.Day_of_Week==5].Speed_limit.min(),accidents[accidents.Day_of_Week==1].Speed_limit.max())\nday_speed[\"Sat\"]=(accidents[accidents.Day_of_Week==6].Speed_limit.min(),accidents[accidents.Day_of_Week==1].Speed_limit.max())\nday_speed[\"Sun\"]=(accidents[accidents.Day_of_Week==7].Speed_limit.min(),accidents[accidents.Day_of_Week==1].Speed_limit.max())\nday_speed","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Explanation: Each tuple indicates (min,max) pair"},{"metadata":{},"cell_type":"markdown","source":"### 4. What is the importance of Light and Weather conditions in predicting accident severity? What does your intuition say and what does the data portray?"},{"metadata":{},"cell_type":"markdown","source":"* ### Plotting LinePlot and barplot for Light Conditions vs Accident Severity and Weather Conditions vs Accident Severity"},{"metadata":{"trusted":true},"cell_type":"code","source":"#enter code/answer in this cell. You can add more code/markdown cells below for your answer. \nplt.figure(figsize=(16,10))\naccidents.Accident_Severity.value_counts()\nsns.barplot(x=accidents.Light_Conditions,y=accidents.Accident_Severity)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,10))\nsns.lineplot(x=accidents.Light_Conditions,y=accidents.Accident_Severity)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### No street lighting has high severity as expected and daylight has less severity as expected."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,10))\nsns.barplot(x=accidents.Weather_Conditions,y=accidents.Accident_Severity)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,10))\nsns.lineplot(x=accidents.Weather_Conditions,y=accidents.Accident_Severity)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Fog has high severity, and fine without wind has less severity as expected. Only discrepency I found was Raining has less severity, which should be high according to me."},{"metadata":{},"cell_type":"markdown","source":"* ### The above plots show that that Light and Weather Conditions do follow our intuition, and that accident severity is more ( smaller in value)  for Harsh conditions, and less for normal conditions."},{"metadata":{},"cell_type":"markdown","source":"### 5. To predict the severity of the accidents which columns do you think are unnecessary and should be dropped before implementing a regression model. Support your statement using relevant plots and hypotheses derived from them."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(accidents[\"1st_Road_Class\"],accidents[\"Accident_Severity\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"####  Keeping this as Accident_Severity depends linearly on 1st_Road_Class "},{"metadata":{},"cell_type":"markdown","source":"#### Converting date to lie in range [1,365]"},{"metadata":{"trusted":true},"cell_type":"code","source":"number_of_days = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30]\nfor i in range(1, len(number_of_days)):\n    number_of_days[i] += number_of_days[i-1]\n\nprint(number_of_days)\n\nmonth = accidents.Date.apply(lambda x: int(x[3:5]))\ndate = accidents.Date.apply(lambda x: int(x[:2]))\n\naccidents.Date = month.apply(lambda x: number_of_days[x-1])+date\naccidents.Date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(accidents[\"Date\"],accidents[\"Accident_Severity\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_time(time):\n    if pd.isnull(time):\n        return pd.NA\n    \n    hours, minutes = time.split(\":\")\n    hours, minutes = int(hours), int(minutes)\n    return int(hours + minutes/60)\n\naccidents.Time = accidents.Time.apply(convert_time)\naccidents.Time.fillna(accidents.Time.mean(), inplace=True)\naccidents.Time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(accidents[\"Time\"],accidents[\"Accident_Severity\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(accidents[\"Year\"],accidents[\"Accident_Severity\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(accidents[\"Day_of_Week\"],accidents[\"Accident_Severity\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ## Dropping Accident severity as it is our target label and saving it as Y. The new dataset is new_accidents"},{"metadata":{"trusted":true},"cell_type":"code","source":"#enter code/answer in this cell. You can add more code/markdown cells below for your answer. \nnew_accidents=accidents.drop(\"Accident_Severity\",axis=1)\ny=accidents[\"Accident_Severity\"]\nnew_accidents.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### I am dropping the following columns because:\n* ### Accident_Index - It is a unique value, and signifies nothing\n* ### Location_Easting_OSGR and Location_Northing_OSGR - Their correlation with Longitude and latitude respectively\n* ### Local_Authority_(District) - Correlation with Police force is very high\n* ### Junction_Detail - Full of Nan values\n* ### LSOA_of_Accident_Location - leaving scene of accident does not affect severity as its just an ID\n* ### Local_Authority_(Highway) - Too many unique values, doesnt help in classification much as shown below.\n* ### Year, Date and Time- They dont have a definite relationship with Accident Severity as shown in above graphs.\n* ### 1st_Road_number  and 2nd_Road_Number - it is a ambiguous and fluctuating graph."},{"metadata":{"trusted":true},"cell_type":"code","source":"new_accidents[\"LSOA_of_Accident_Location\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(new_accidents[\"1st_Road_Number\"],y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_accidents[\"2nd_Road_Number\"].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_accidents[\"1st_Road_Number\"].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(new_accidents[\"Local_Authority_(Highway)\"],y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Dropping this as its not a feasible relationship"},{"metadata":{},"cell_type":"markdown","source":"* ### Putting all object type in objects, and int and float in non_objects"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_accidents1 = new_accidents.drop([\"Accident_Index\",\"Location_Easting_OSGR\",\"Location_Northing_OSGR\",\"Local_Authority_(District)\",\"Junction_Detail\",\"Year\",\"Date\",\"Time\",\"LSOA_of_Accident_Location\",\"Local_Authority_(Highway)\",\"1st_Road_Number\",\"2nd_Road_Number\"],axis=1)\nobjects=new_accidents1.select_dtypes(include=[\"object\"])\nnot_objects=new_accidents1.select_dtypes(include=[\"float64\",\"int64\"])\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Filling NA in null values for objects"},{"metadata":{"trusted":true},"cell_type":"code","source":"objects=objects.fillna(\"NA\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Filling null values in int and float by mean"},{"metadata":{"trusted":true},"cell_type":"code","source":"not_objects[\"Longitude\"]=not_objects[\"Longitude\"].fillna(not_objects[\"Longitude\"].mean())\nnot_objects[\"Latitude\"]=not_objects[\"Latitude\"].fillna(not_objects[\"Latitude\"].mean())\nnot_objects.isnull().sum()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### One hot encoding object types, and then concatenating with non_objects to get dataset accidata1 for training."},{"metadata":{"trusted":true},"cell_type":"code","source":"objects1=pd.get_dummies(objects)\naccidata1=pd.concat([objects1,not_objects],axis=1)\naccidata1.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### No Null values present."},{"metadata":{"trusted":true},"cell_type":"code","source":"\naccidata1.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6. Implement a basic Logistic Regression Model using scikit learn with cross validation = 5, where you predict the severity of the accident (Accident_Severity). Note that here your goal is not to tune appropriate hyperparameters, but to figure out what features will be best to use."},{"metadata":{},"cell_type":"markdown","source":"* ### The idea that I used is, I created 3 models, one for predicting 1( Meaning output 1 for 1 and 0 for 2 and 3), one for predicting 2 and one for predicting 3. Then, while testing, whichever is one, will be output."},{"metadata":{},"cell_type":"markdown","source":"* ### In another method I used LogisticRegressionCV  to make a model to predict . I used the multi_class hyperparameter to use Logistic Regression to predict 3 classes."},{"metadata":{"trusted":true},"cell_type":"code","source":"#enter code/answer in this cell. You can add more code/markdown cells below for your answer. \nimport sklearn\nfrom sklearn.linear_model import LogisticRegressionCV,LogisticRegression\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1st Approach"},{"metadata":{"trusted":true},"cell_type":"code","source":"y1=[]\ny2=[]\ny3=[]\nfor i in y:\n    if i==1:\n        y1.append(1)\n        y2.append(0)\n        y3.append(0)\n    if i==2:\n         y1.append(0)\n         y2.append(1)\n         y3.append(0)\n    if i==3:\n         y1.append(0)\n         y2.append(0)\n         y3.append(1)\n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1=LogisticRegression(max_iter=1000)\nmodel2=LogisticRegression(max_iter=1000)\nmodel3=LogisticRegression(max_iter=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train1,X_test1,Y_train1,Y_test1=train_test_split(accidata1,y1,test_size=0.2)\nX_train2,X_test2,Y_train2,Y_test2=train_test_split(accidata1,y2,test_size=0.2)\nX_train3,X_test3,Y_train3,Y_test3=train_test_split(accidata1,y3,test_size=0.2)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.fit(X_train1,Y_train1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2.fit(X_train2,Y_train2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model3.fit(X_train3,Y_train3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred1=model1.predict(accidata1)\ny_pred2=model2.predict(accidata1)\ny_pred3=model3.predict(accidata1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=[]\nfor i in range(len(y_pred1)):\n    if y_pred1[i]==1:\n        y_pred.append(1)\n    elif y_pred2[i]==1:\n        y_pred.append(2)\n    else:\n        y_pred.append(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_pred,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Accuracy of 1st Model= 85%"},{"metadata":{},"cell_type":"markdown","source":"### 2nd Approach"},{"metadata":{},"cell_type":"markdown","source":"### n_jobs gives paralellism multinomial helps to predict 3 or more classes."},{"metadata":{"trusted":true},"cell_type":"code","source":"model=LogisticRegressionCV(cv=5,multi_class=\"multinomial\",n_jobs=-1,max_iter=1000)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Normalizing data for faster training"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler=StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accidata1=scaler.fit_transform(accidata1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accidata1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,Y_train,Y_test=train_test_split(accidata1,y,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_2=model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_pred_2,Y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Accuracy using LogisticRegressionCV = 85%."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}