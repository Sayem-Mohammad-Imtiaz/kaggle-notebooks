{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Libraries required","metadata":{"id":"GoL4spm4LxOj"}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.style.use('ggplot')\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nimport xgboost\nfrom sklearn.metrics import accuracy_score\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"id":"UNAM7uZKBxbQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading data","metadata":{"id":"9IAvwuWRL5l5"}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/heart-failure-prediction/heart.csv')\ndata.head()","metadata":{"id":"kLNC5d8nCLzE","outputId":"7a314867-3ee9-4c0a-ba5c-bf6712a95fa1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{"id":"rqkMscSTMDsX"}},{"cell_type":"code","source":"data.columns = data.columns.str.lower()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"id":"Hb2L1VcdQ6wL","outputId":"4316dc8f-1e48-4979-d47a-fe61a88f9a8c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"id":"HeW2uO0KCej6","outputId":"f5627753-9982-4b3e-981e-18a1dbf75dc2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"id":"pj5izkqaCr0H","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heart_prob = data[data['heartdisease'] == 1]\nplt.figure(figsize=(12,8))\ns = sns.countplot(heart_prob.sex)\nfor p in s.patches:\n    s.annotate(format(p.get_height(), '.1f'), \n               (p.get_x() + p.get_width() / 2., p.get_height()), \n                xytext = (0, 9), \n                textcoords = 'offset points'\n              )\nplt.show()","metadata":{"id":"VLH0trCIQCJF","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we come to know that there are more number of males than the females who are prone to heart failures.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.hist(heart_prob.age, histtype='step', color='black')\nplt.xlabel('age')\nplt.ylabel('count')\nplt.xticks(rotation=45)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By above graph, we get to know that most of heart risk is in between the age 50 to 60","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.distplot(heart_prob.age)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Age feature is asymptotically gaussian.","metadata":{}},{"cell_type":"markdown","source":"# Data Preparation","metadata":{"id":"0bk7K2zxLh5u"}},{"cell_type":"code","source":"data['sex'].replace({'M': 1, 'F': 0}, inplace=True)\ndata.chestpaintype.replace({'ASY': 0, 'NAP': 1, 'ATA': 2, 'TA': 3}, inplace=True)\ndata.restingecg.replace({'Normal': 0, 'LVH': 1, 'ST': 2}, inplace=True)\ndata['exerciseangina'].replace({'Y': 1, 'N': 0}, inplace=True)\ndata.st_slope.replace({'Flat': 0, 'Up': 1, 'Down': 2}, inplace=True)\ndata.head()","metadata":{"id":"zm0r5f4FCvRf","outputId":"fe9da1db-1d60-4042-e039-ab847993bac5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.title('Distribution plots of all features')\nfor cols in data.columns:\n    sns.distplot(data[cols])\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All columns are asymptotically guassian","metadata":{}},{"cell_type":"code","source":"X = data.drop('heartdisease', axis=1)\nY = data['heartdisease']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=999, test_size=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training and validation","metadata":{}},{"cell_type":"code","source":"models = [(\"Logistic Regression\", LogisticRegression(random_state=0, max_iter=1000)),\n          (\"Support vectors\", SVC(random_state=0)),\n          (\"Random Forest\", RandomForestClassifier(random_state=0)),\n          (\"Decision Trees\", DecisionTreeClassifier(random_state=0)),\n          (\"XGBoost\", xgboost.XGBClassifier(random_state=0)),\n          ('Gradient Boosting', GradientBoostingClassifier(random_state=0))\n         ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = []\nnames=[]\nfinalresults=[]\n\nfor name, model in models:\n    model.fit(X_train, Y_train)\n    model_results = model.predict(X_test)\n    score= accuracy_score(Y_test, model_results)\n    results.append(score)\n    names.append(name)\n    finalresults.append((name,score))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualising the accuracy score of each classification model\nplt.rcParams['figure.figsize']=15,8 \nplt.style.use('dark_background')\nax = sns.barplot(x=names, y=results, palette = \"rocket\", saturation =1.0)\nplt.xlabel(\"Classifier Models\", fontsize = 20 )\nplt.ylabel(\"% of Accuracy\", fontsize = 20)\nplt.title(\"Accuracy of different Classifier Models\", fontsize = 20)\nplt.xticks(fontsize = 13, horizontalalignment = 'center', rotation = 0)\nplt.yticks(fontsize = 13)\nfor p in ax.patches:\n    width, height = p.get_width(), p.get_height()\n    x, y = p.get_xy() \n    ax.annotate(f'{height:.2%}', (x + width/2, y + height*1.02), ha='center', fontsize = 'x-large')\n#     print(width)\n#     print(height)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index = results.index(max(results))\nprint(f\"{names[index]} has the highest accuracy of {max(results)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}