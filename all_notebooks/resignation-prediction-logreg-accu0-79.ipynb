{"cells":[{"metadata":{},"cell_type":"markdown","source":"Kaggle: https://www.kaggle.com/jiangzuo/hr-comma-sep"},{"metadata":{"trusted":true,"_uuid":"efd67033e66041adce5d8c7107831a4e2fcd4f11"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c6bf123db3ba7a84fc4fb2ebf9979e9ef875e12"},"cell_type":"code","source":"TRAIN_DIR = '../input/HR_comma_sep.csv'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explantory Data Analyisis"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_barchart_for_y_by_categorical_column(dataset, y_col, x_col):\n    assert len(dataset[x_col].unique()) <= 4, \\\n        \"Input x_col should be categorical column with unique values fewer than 5.'\"\n    \n    result_dict = {}\n    for x_col_val in dataset[x_col].unique():\n        result_dict[x_col_val] = dataset[y_col][dataset[x_col] == x_col_val].value_counts()\n        \n    df = pd.DataFrame(result_dict)\n    df.plot(kind='bar', stacked=True)\n    plt.title(f\"{y_col} by {x_col}\")\n    plt.xlabel(f\"{y_col}\") \n    plt.ylabel(\"Count\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_hist_for_numeric_column_by_y(dataset, y_col, x_col):\n    assert sorted(list(dataset[y_col].unique())) == [0, 1], \"Input y_col should be 0 or 1.'\"\n        \n    dataset[x_col].hist()  \n    plt.ylabel(y_col) \n    plt.xlabel(x_col) \n    plt.title(f'{x_col} Distribution')\n    plt.show() \n\n    dataset[dataset[y_col]==0][x_col].hist()  \n    plt.ylabel(y_col) \n    plt.xlabel(x_col) \n    plt.title(f'{x_col} Distribution, {y_col}=0')\n    plt.show()\n\n    dataset[dataset[y_col]==1][x_col].hist()  \n    plt.ylabel(y_col) \n    plt.xlabel(x_col) \n    plt.title(f'{x_col} Distribution, {y_col}=1')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1.Take a Glance at the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(TRAIN_DIR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2.Look at Categorical Columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use Bar Chart\nplot_barchart_for_y_by_categorical_column(train, y_col='left', x_col='salary')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use Cross Tab\nct = pd.crosstab(train['salary'], train['left'])\nct.div(ct.sum(axis=1), axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use Cross Tab\nct = pd.crosstab(train['salary'], train['left'])\nct.div(ct.sum(axis=1), axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation: Salary is a useful column. High salary means unlikely to resign."},{"metadata":{},"cell_type":"markdown","source":"3.Look at Numeric Columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_hist_for_numeric_column_by_y(train, y_col='left', x_col='satisfaction_level')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation: Satisfaction level is a useful column. High satisfaction means unlikely to resign."},{"metadata":{},"cell_type":"markdown","source":"# Prepare Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from patsy import dmatrices\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize_0_1(X):\n    _, num_col = X.shape\n    for i in range(1, num_col): # Don't normalize the first column (intercept).\n        col_min, col_max = X[:, i].min(), X[:, i].max()\n        X[:, i] = (X[:, i] - col_min) / (col_max - col_min)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1.Use DMATRICES to Create Training Set Easily (Use Dummy Variables for Categorical Variables)"},{"metadata":{"trusted":true},"cell_type":"code","source":"y, X = dmatrices('left~satisfaction_level+last_evaluation+number_project+average_montly_hours+time_spend_company+Work_accident+promotion_last_5years+C(sales)+C(salary)', train, return_type='dataframe')\nX = X.rename(columns = {\n    'C(sales)[T.RandD]': 'Department: Random',\n    'C(sales)[T.accounting]': 'Department: Accounting',\n    'C(sales)[T.hr]': 'Department: HR',\n    'C(sales)[T.management]': 'Department: Management',\n    'C(sales)[T.marketing]': 'Department: Marketing',\n    'C(sales)[T.product_mng]': 'Department: Product_Management',\n    'C(sales)[T.sales]': 'Department: Sales',\n    'C(sales)[T.support]': 'Department: Support',\n    'C(sales)[T.technical]': 'Department: Technical',\n    'C(salary)[T.low]': 'Salary: Low',\n    'C(salary)[T.medium]': 'Salary: Medium'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_np = np.asmatrix(X)\ny_np = np.ravel(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2.Normalize into 0~1"},{"metadata":{"trusted":true},"cell_type":"code","source":"normalize_0_1(X_np)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3.Training Set, Cross Validation Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X_np, y_np, test_size=0.2, random_state=0)\nprint(f'Training Set - X train shape: {X_train.shape}, y train shape: {y_train.shape}')\nprint(f'Validation Set - X val shape: {X_val.shape}, y val shape: {y_val.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 1: Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(conf_mtrx, classes, cmap=plt.cm.Blues):\n    num_class = conf_mtrx.shape[0]\n    \n    fig, ax = plt.subplots()\n    im = ax.imshow(conf_mtrx, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    ax.set(xticks=np.arange(num_class), yticks=np.arange(num_class),\n           xticklabels=classes, yticklabels=classes, \n           ylabel='True label', xlabel='Predicted label')\n\n    middle_threshold = conf_mtrx.max() / 2.\n    for row in range(num_class):\n        for col in range(num_class):\n            ax.text(col, row, format(conf_mtrx[row, col], '.0f'), ha=\"center\", va=\"center\",\n                    color=\"white\" if conf_mtrx[row, col] > middle_threshold else \"black\")\n    fig.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1.Fit"},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg = LogisticRegression(max_iter=10000)\nlog_reg.fit(X_train, y_train)\nlog_reg.score(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(list(zip(X.columns, np.transpose(log_reg.coef_))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2.10-fold Cross Validation to Verify"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(cross_val_score(LogisticRegression(max_iter=10000), X_np, y_np, scoring='accuracy', cv=10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3.Confusion Matrix of Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = log_reg.predict(X_val)\nprint('Accuracy: ', accuracy_score(y_val, y_pred))\nprint('Confusion Matrix')\nprint(confusion_matrix(y_val, y_pred))\nprint('Classification Report')\nprint(classification_report(y_val, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(confusion_matrix(y_val, y_pred),classes=range(2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 2: Logistic Regression (No Sklearn)"},{"metadata":{"trusted":true},"cell_type":"code","source":"class LogisticRegression:\n    \"\"\"Logistic Regression with both GradientDescent and Newton's Method.\n    Example usage:\n        > log_reg = LogisticRegression()\n        > log_reg.fit(x_train, y_train)\n        > log_reg.predict(x_eval)\n    \"\"\"\n    def __init__(self, learning_rate=0.1, max_iter=100, solver='GD', theta_0=None, verbose=True):\n        \"\"\"\n        Args:\n            learning_rate: Step size for iterative solvers only.\n            max_iter: Maximum number of iterations for the solver.\n            solver: 'GD' - Gradient Descent | 'Newton' - Newton's Method\n            theta_0: Initial guess for theta. If None, use the zero vector.\n            verbose: Print loss and accuracy values during training.\n        \"\"\"\n        assert solver == 'GD' or solver == 'Newton', 'Unknown solver'\n        \n        self.learning_rate = learning_rate\n        self.max_iter = max_iter\n        self.solver = solver          \n        self.theta = theta_0\n        self.verbose = verbose\n\n    def fit(self, x_train, y_train, x_val, y_val):\n        \"\"\"Minimize loss(theta) for logistic regression.\n        Args:\n            x_train: Training example inputs. Shape (n_examples, dim).\n            y_train: Training example labels. Shape (n_examples,).\n        \"\"\"\n        _, num_features = x_train.shape\n        self.theta = np.zeros(num_features) if self.theta is None else self.theta        \n        for i in range(0, self.max_iter+1):\n            if self.solver == 'GD':\n                self.__update_theta_via_gradient_descent(x_train, y_train)\n            else:\n                self.__update_theta_via_newton_method(x_train, y_train)\n            loss_train = self.__calculate_loss(x_train, y_train)  \n            accuracy_train = self.__calculate_accuracy(x_train, y_train)               \n            accuracy_val = self.__calculate_accuracy(x_val, y_val)                              \n            if self.verbose and i%10 == 0:\n                print(f'Iteration {i} : Loss {loss_train:.4f}  | ' + \\\n                      f'Train Accuarcy {accuracy_train:.4f} | '+\\\n                      f'Validation Accuarcy {accuracy_val:.4f}')\n                        \n    def predict(self, x):\n        \"\"\"Return predicted probabilities given new inputs x.\n        Args:\n            x: Inputs of shape (n_examples, dim).\n        Returns:\n            Output Shape (n_examples,).\n        \"\"\"\n        pred = 1. / (1+np.exp(-x.dot(self.theta)))\n        return self.__ravel_np_matrix(pred)\n    \n    def __update_theta_via_gradient_descent(self, x, y):\n        \"\"\"Update theta via gradient descent (only one step).\n        Args:\n            x: Inputs Shape (n_examples, dim).\n            y: Inputs Shape (n_examples,).\n        \"\"\"\n        num_examples, _ = x.shape     \n        y_pred = self.predict(x)\n        gradient = - (1/num_examples) * self.__ravel_np_matrix(x.T.dot(y - y_pred))\n        self.theta -= self.learning_rate * gradient\n        \n    def __update_theta_via_newton_method(self, x, y):\n        \"\"\"Update theta via gradient descent (only one step).\n        Args:\n            x: Inputs Shape (n_examples, dim).\n            y: Inputs Shape (n_examples,).\n        \"\"\"\n        num_examples, _ = x.shape     \n        y_pred = self.predict(x)\n        gradient = - (1/num_examples) * x.T.dot(y - y_pred)\n        hessian = (1/num_examples) * x.T.dot(np.diag(y_pred*(1-y_pred))).dot(x)\n        self.theta -= self.learning_rate * \\\n                 self.__ravel_np_matrix(np.linalg.inv(hessian).dot(gradient.T))\n        \n    def __calculate_loss(self, x, y):\n        \"\"\"Calculate loss based on dataset (x, y).\n        Args:\n            x: Inputs Shape (n_examples, dim).\n            y: Inputs Shape (n_examples,).\n        Returns:\n            Outputs Shape scalar.\n        \"\"\"\n        num_examples, _ = x.shape\n        y_pred = self.predict(x)\n        y_pred_and_y = list(zip(y_pred, y))\n        loss = - (1/num_examples) * \\\n               sum([np.log(y_pred) if y == 1 else np.log(1-y_pred) \\\n                    for y_pred, y in y_pred_and_y])\n        return loss\n    \n    def __calculate_accuracy(self, x, y):\n        \"\"\"Calculate accuracy based on dataset (x, y).\n        Args:\n            x: Inputs Shape (n_examples, dim).\n            y: Inputs Shape (n_examples,).\n        Returns:\n            Outputs Shape scalar.\n        \"\"\"\n        num_examples, _ = x.shape  \n        y_pred = self.predict(x)\n        y_pred_and_y = list(zip(y_pred, y))\n        accuracy = (1/num_examples) * \\\n                   sum([1 if (y_pred > 0.5 and y == 1) or (y_pred <= 0.5 and y == 0) else 0 \\\n                        for y_pred, y in y_pred_and_y])\n        return accuracy\n    \n    def __ravel_np_matrix(self, np_matrix):\n        \"\"\"Ravel a np.matrix (1, n) into (n,).\n        Args:\n            np_matrix: Inputs Shape (1, n).\n        Returns:\n            Output Shape (n,).\n        \"\"\"\n        return np.array(np_matrix).ravel()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1.Fit"},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg = LogisticRegression(learning_rate=0.5, solver='GD', max_iter=200)\nlog_reg.fit(X_train, y_train, X_val, y_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation: For solver as gradient descent, 90 iterations is enough. Otherwise, there will be overfitting."},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg = LogisticRegression(learning_rate=0.5, solver='Newton', max_iter=30)\nlog_reg.fit(X_train, y_train, X_val, y_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation: Newton method converges within fewer iterations. But each iteration takes longer (due to the calculation of Heissan matrix)."},{"metadata":{},"cell_type":"markdown","source":"2.Confusion Matrix of Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = log_reg.predict(X_val)\ny_pred = [1 if y > 0.5 else 0 for y in y_pred]\nprint('Accuracy: ', accuracy_score(y_val, y_pred))\nprint('Confusion Matrix')\nprint(confusion_matrix(y_val, y_pred))\nprint('Classification Report')\nprint(classification_report(y_val, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(confusion_matrix(y_val, y_pred),classes=range(2))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}