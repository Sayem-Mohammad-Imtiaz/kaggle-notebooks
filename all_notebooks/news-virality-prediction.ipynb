{"cells":[{"metadata":{},"cell_type":"markdown","source":"# REFERENCES:\n\n* http://archive.ics.uci.edu/ml/datasets/Online+News+Popularity\n* https://newspaper.readthedocs.io/en/latest/\n* https://www.geeksforgeeks.org/newspaper-article-scraping-curation-python/\n* https://machinelearningmastery.com/clean-text-machine-learning-python/\n* https://textblob.readthedocs.io/en/dev/quickstart.html#sentiment-analysis\n* https://scikit-learn.org/stable/index.html"},{"metadata":{},"cell_type":"markdown","source":"# Installing the newspaper3k library"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# !pip install newspaper3k","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.linear_model import RidgeCV\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\nfrom sklearn.metrics import mean_squared_error as RMSE\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\nfrom newspaper import Article\nimport requests\nfrom bs4 import BeautifulSoup","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing dataset: Online news popularity from UCI library"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/uci-online-news-popularity-data-set/OnlineNewsPopularity.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cleansing the column names"},{"metadata":{"trusted":true},"cell_type":"code","source":"def cleancols(cols):\n    x = [y.lower().strip() for y in cols]\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns = cleancols(data.columns)\ny_labels = data['shares'] #the feature to be predicted\ndf = data.drop(columns = ['url','timedelta','shares'],axis = 1) #url and timedelta are of no use to us","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scraping news articles from the web"},{"metadata":{},"cell_type":"markdown","source":"### We use the BBC News website for testing our model that would be trained on UCI dataset. We observe the feature that we find useful, here the specific 'class' attribute"},{"metadata":{"trusted":true},"cell_type":"code","source":"url = \"https://www.bbc.com/news/world\"\npage = requests.get(url)\nsoup = BeautifulSoup(page.content,'html.parser')\nimgnews = soup.findAll('a',attrs = {'class':'gs-c-promo-heading gs-o-faux-block-link__overlay-link gel-pica-bold nw-o-link-split__anchor'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating the list of news articles"},{"metadata":{"trusted":true},"cell_type":"code","source":"newsarts = []\nfor arts in imgnews:\n    newsarts.append(\"https://www.bbc.com\"+arts['href'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Before we move on to extraction of information, we need to figure out the features of the UCI Dataset that we would be creating, out the extracted news, for prediction purpose"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components = 12)\ndf_scaled = pd.DataFrame(preprocessing.scale(df),columns = df.columns)\ndf_red = pca.fit_transform(df_scaled) #applying PCA on the standardized data\nexplainedfeats = pd.DataFrame(pca.components_,index = ['PC-1','PC-2','PC-3','PC-4','PC-5','PC-6','PC-7','PC-8','PC-9','PC-10','PC-11','PC-12'],columns = df.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using XGBoost to find the absolute importance of each feature. This is done so that we can minimize our effort of making features while converting extracted news article in the format of the Online News Popularity Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBRegressor(max_depth = 10,random_state = 42)\nxgb.fit(df,y_labels)\nfig, ax = plt.subplots(1,1,figsize=(10,10))\nimpplot = plot_importance(xgb,ax = ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"impfeats = [impplot.get_yticklabels()[::-1][i].get_text() for i in range(0,20)]\nprint(impfeats)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Finding the top 20 important features: (DNC : did not consider as unable to figure out calculation steps for that feature )\n*   n_tokens_title : Number of words in the title\n*   n_tokens_content: Number of words in the content \n*   n_unique_tokens : Rate of unique words in the content ( #unique words / #words )\n*   average_token_length : Average length of the words in the content \n*   n_non_stop_unique_tokens : Rate of unique non-stop words in the content\n*   num_hrefs : Number of links\n*   global_subjectivity : subjectivity of article content\n*   avg_positive_polarity : Average polarity of positive words\n*   global_sentiment_polarity : Text sentiment polarity\n*   kw_max_avg : DNC\n*   kw_avg_avg : DNC\n*   kw_avg_max : DNC\n*   kw_max_min : DNC\n*   lda_03 : DNC\n*   self_reference_min_shares : DNC\n*   lda_00 : DNC\n*   lda_01 : DNC\n*   lda_04 : DNC\n*   kw_avg_min : DNC\n*   lda_02 : DNC"},{"metadata":{},"cell_type":"markdown","source":"### Out of these, let us try to construct the features which are defined clearly in the details of the UCI dataset website.\n### 9 features could be figured out."},{"metadata":{},"cell_type":"markdown","source":"# Importing libraries for analyzing the article text"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import word_tokenize \nfrom nltk.corpus import stopwords\nstopwords=set(stopwords.words('english'))\nfrom textblob import TextBlob #for subjectivity and polarity purpose","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining functions that would return feature values"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenizetext(text):\n    return word_tokenize(text)\ndef words(text):\n    l = [word for word in word_tokenize(text) if word.isalpha()]\n    return l\ndef unique_words(text):\n    return list(set(words(text)))\ndef rate_uni_words(text):\n    uni_words = len(unique_words(text))/len(words(text))\n    return uni_words\ndef avglengthtoken(text):\n    w = words(text)\n    sum = 0\n    for item in w:\n        sum+=len(item)\n    avglen = sum/len(w)\n    return avglen\ndef n_non_stop_unique_tokens(text):\n    uw = unique_words(text)\n    n_uw = [item for item in uw if item not in stopwords]\n    w = words(text)\n    n_w = [item for item in w if item not in stopwords]\n    rate_nsut = len(n_uw)/len(n_w)\n    return rate_nsut\ndef numlinks(article):\n    return len(BeautifulSoup(sampletext.html).findAll('link'))\ndef get_subjectivity(a_text):\n    return a_text.sentiment.subjectivity\ndef get_polarity(a_text):\n    return a_text.sentiment.polarity\ndef word_polarity(words):\n    pos_words = []\n    ppos_words = [] # polarity of pos words\n    neg_words = []\n    pneg_words = [] # polarity of negative words\n    neu_words = []\n    pneu_words = [] # polarity of neutral words\n    for w in words:\n        an_word = TextBlob(w)\n        val = an_word.sentiment.polarity\n        if val > 0:\n            pos_words.append(w)\n            ppos_words.append(val)\n        if val < 0:\n            neg_words.append(w)\n            pneg_words.append(val)\n        if val == 0 :\n            neu_words.append(w)\n            pneu_words.append(val)\n    return pos_words,ppos_words,neg_words,pneg_words,neu_words,pneu_words\ndef avg_pol_pw(text):    \n    totalwords = words(text)\n    res = word_polarity(totalwords)\n    return np.sum(res[1])/len(res[0])\ndef avg_pol_nw(text):    \n    totalwords = words(text)\n    res = word_polarity(totalwords)\n    return np.sum(res[3])/len(res[2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating dataset with news articles "},{"metadata":{"trusted":true},"cell_type":"code","source":"finrows = []\nfor article in newsarts[0:25]:\n    sampletext = Article(article, language = 'en')\n    sampletext.download()\n    sampletext.parse()\n    sampletext.nlp() \n    \n    row = {}\n    row['n_tokens_title'] = len(words(sampletext.title))\n    row['n_tokens_content'] = len(words(sampletext.text))\n    row['n_unique_tokens'] = len(unique_words(sampletext.text))\n    row['average_token_length'] = avglengthtoken(sampletext.text)\n    row['n_non_stop_unique_tokens'] = n_non_stop_unique_tokens(sampletext.text)\n    row['num_hrefs'] = numlinks(sampletext)\n    \n    analysed_text = TextBlob(sampletext.text)\n    row['global_subjectivity'] = get_subjectivity(analysed_text)\n    row['avg_positive_polarity'] = avg_pol_pw(sampletext.text)\n    row['global_sentiment_polarity'] = get_polarity(analysed_text)\n    finrows.append(row)\n#converting the list to a dataframe\nmasterdf = pd.DataFrame(finrows, columns = ['n_tokens_title','n_tokens_content','n_unique_tokens','average_token_length','n_non_stop_unique_tokens','num_hrefs','global_subjectivity',\n                                   'avg_positive_polarity','global_sentiment_polarity'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## This dataset will now be used in the end for prediction purpose"},{"metadata":{},"cell_type":"markdown","source":"# Converting UCI dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_reduced = df[masterdf.columns]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting the UCI dataset into training and testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain, xtest, ytrain, ytest = train_test_split(df_reduced, y_labels, test_size = 0.2, shuffle = True, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# REGRESSOR TESTING"},{"metadata":{},"cell_type":"markdown","source":"# 1. XGBoost "},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb2 = XGBRegressor(random_state = 42)\nparamsxgb = {'max_depth':[5,20,50,100]}\ngsc = GridSearchCV(estimator = xgb2,param_grid = paramsxgb, cv = 3, scoring = 'neg_root_mean_squared_error')\ngscres = gsc.fit(xtrain,ytrain)\ngscres.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb2.max_depth = gscres.best_params_['max_depth']\nboost = ['gbtree','gblinear']\nrmsescores = {}\nfor b in boost:\n    xgb2.booster = b\n    xgb2.fit(xtrain,ytrain)\n    predicted = xgb2.predict(xtest)\n    rmsescores['xgb-'+b] = RMSE(ytest,predicted,squared = False)\n    print(\"RMSE error with {} booster is {} :\".format(b,RMSE(ytest,predicted,squared = False)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Best depth is found to be 5 and on testing setting the booster to 'gblinear' gives lower RMSE score ~ 10900 compared to 'gbtree' with a RMSE score ~ 11430. Hence, I have set the booster to 'gblinear'"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb2.booster = 'gblinear'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. RandomForestRegressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestRegressor(random_state = 42)\nparamsrf = {'max_depth':[5,20,50,100]}\ngsc = GridSearchCV(estimator = rf,param_grid = paramsrf, cv = 3, scoring = 'neg_root_mean_squared_error')\ngscres = gsc.fit(xtrain,ytrain)\ngscres.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf.max_depth = gscres.best_params_['max_depth']\nrf.fit(xtrain,ytrain)\npredictedrf = rf.predict(xtest)\nrmsescores['rf'] = RMSE(ytest,predictedrf,squared = False)\nprint(\"RMSE error with Random Forest Regressor is {} :\".format(RMSE(ytest,predicted,squared = False)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Best depth is found to be 5 with RMSE score ~ 10900, similar to XGBoost with gblinear regressor"},{"metadata":{},"cell_type":"markdown","source":"# 3. Linear Regression (RidgeCV): linear regression with l2 regularization and in-built CV"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = RidgeCV(alphas = [0.001,0.1,1,5,10,100],scoring = 'neg_root_mean_squared_error', cv = None, store_cv_values = True)\nlr.fit(xtrain,ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictedlr = lr.predict(xtest)\nrmsescores['ridgecv'] = RMSE(ytest,predictedlr,squared = False)\nprint(\"RMSE error with Linear Regression via RidgeCV is {} :\".format(RMSE(ytest,predictedlr,squared = False)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## RMSE score for RidgeCV regression is found to be lowest compared to XGBoost and Random Forest Regressor"},{"metadata":{},"cell_type":"markdown","source":"# 4. CatBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"cb = CatBoostRegressor(verbose = 0,random_state = 42,eval_metric = 'RMSE')\nparamscb = {'iterations':[1,10,50,100],'learning_rate':[0.03,0.1,0.5,1],'depth':[3,5,8,10]}\ngsc = GridSearchCV(estimator = cb,param_grid = paramscb, cv = 3, scoring = 'neg_root_mean_squared_error')\ngscres = gsc.fit(xtrain,ytrain)\ngscres.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cb.iterations = gscres.best_params_['iterations']\ncb.learning_rate = gscres.best_params_['learning_rate']\ncb.depth = gscres.best_params_['depth']\ncb.fit(xtrain,ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictedcb = cb.predict(xtest)\nrmsescores['catboost'] = RMSE(ytest,predictedcb,squared = False)\nprint(\"RMSE error with CatBoost is {} :\".format(RMSE(ytest,predictedcb,squared = False)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Catboost gives the second worse RMSE ~ 11200 after xgboost-via-gbtree whose RMSE was ~11430"},{"metadata":{},"cell_type":"markdown","source":"# EVALUATION RESULTS"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.barplot(x = list(rmsescores.keys()),y = list(rmsescores.values()))\nax.set_ylim(10750,12000)\nax.set_xlabel('Regressors')\nax.set_ylabel('RMSE Scores')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## We would therefore prefer using RidgeCV for prediction."},{"metadata":{},"cell_type":"markdown","source":"# FINAL PREDICTION"},{"metadata":{},"cell_type":"markdown","source":"# However, before we predict we cannot ascertain if the prediction is right or not"},{"metadata":{"trusted":true},"cell_type":"code","source":"predlr = lr.predict(masterdf)\nlrdata = {'Links':list(newsarts[:25]),'Predicted Virality':list(predlr)}\npd.DataFrame(lrdata).reindex(np.arange(0,25,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predcb = cb.predict(masterdf)\ncbdata = {'Links':list(newsarts[0:25]),'Predicted Virality':list(predcb)}\npd.DataFrame(cbdata).reindex(np.arange(0,25,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predrf = rf.predict(masterdf)\nrfdata = {'Links':list(newsarts[0:25]),'Predicted Virality':list(predrf)}\npd.DataFrame(rfdata).reindex(np.arange(0,25,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predxgb = xgb2.predict(masterdf)\nxgbdata = {'Links':list(newsarts[0:25]),'Predicted Virality':list(predxgb)}\npd.DataFrame(xgbdata).reindex(np.arange(0,25,1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# OBSERVATION\n\n## On a whole, we observe that RidgeCV gives us predictions that are more than 10x the predictions by other models, on an average."},{"metadata":{},"cell_type":"markdown","source":"# FURTHER IMPROVEMENTS"},{"metadata":{},"cell_type":"markdown","source":"## 1. Other features, with low importance, could be experimented with.\n## 2. Used prediction models could be improvised with time"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}