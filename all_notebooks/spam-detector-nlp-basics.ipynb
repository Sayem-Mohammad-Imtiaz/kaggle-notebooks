{"cells":[{"metadata":{"_uuid":"4c4acbaab7c19ad81db9649ff19c0a7219474f49"},"cell_type":"markdown","source":"Reffered from [Spam or Ham: Introduction to Natural Language Processing Part 2](https://towardsdatascience.com/spam-or-ham-introduction-to-natural-language-processing-part-2-a0093185aebd)"},{"metadata":{"_uuid":"fb44d2e1e59fafc1ac55d65454764479531e20ff"},"cell_type":"markdown","source":"- In this part, we will go through an end to end walk through of building a very simple text classifier. \n- We will be using the SMS Spam Collection Dataset which tags 5,574 text messages based on whether they are “spam” or “ham” (not spam).\n- Our goal is to build a predictive model which will determine whether a text message is spam or ham. For the code, see here."},{"metadata":{"_uuid":"0da37ab620b1cc385ac13c1c390d8d58fea0ac3d"},"cell_type":"markdown","source":"### Let’s begin!\nAfter importing the data, I changed the column names to be more descriptive."},{"metadata":{"trusted":true,"_uuid":"ad4899e333779776afbd6caf8f7727d4287f544b"},"cell_type":"code","source":"import os\nos.listdir('../input/')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\ndata = pd.read_csv(\"../input/sms-spam-collection-dataset/spam.csv\", encoding = \"latin-1\")\ndata = data[['v1', 'v2']]\ndata = data.rename(columns = {'v1': 'label', 'v2': 'text'})","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a95605e0da0b4aae5df6d5a763b50bdff6d2768"},"cell_type":"code","source":"data.label.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d5ae7e6d454a4a934a62b135169c44e3b6b05ade"},"cell_type":"markdown","source":"From briefly exploring our data, we gain some insight into the text that we are working with: colloquial English. This particular data set also has 87% messages labelled as “ham” and 13% messages labelled as “spam”. The class imbalance will become important later when assessing the strength of our classifier."},{"metadata":{"_uuid":"adecf7f35e247d5c91d9ca7a1856676d5b69cf03"},"cell_type":"markdown","source":"### Data Cleaning\nCleaning textual data is a little different than regular data cleaning. There is a much heavier emphasis on text normalisation than removing outliers or leverage points."},{"metadata":{"_uuid":"e472d3da486f8f7eb0d2d19c1e7a807a4e6e34d6"},"cell_type":"markdown","source":"**As per Wikipedia:**\n\nText normalisation is the process of transforming text into a single canonical form that it might not have had before.\nWhen used correctly, it reduces noise, groups terms with similar semantic meanings and reduces computational costs by giving us a smaller matrix to work with.\n\nI will go through several common methods of normalisation, but keep in mind that it is not always a good idea to use them. Reserving the judgement for when to use what is the human component in data science.\n\n**[No Free Lunch Theorem:](https://en.wikipedia.org/wiki/No_free_lunch_theorem) ** there is never one solution that works well with everything. Try it with your data set to determine if it works for your special use case."},{"metadata":{"_uuid":"bb6a7c84d3b02e5bce012a2c146b2f42cfcf3cf7"},"cell_type":"markdown","source":"### Removing stop words\n\nSome commonly agreed upon stop words from the English language:\n\n- I\n- a\n- because\n- to\n\nThere is a lot of debate over when removing stop words is a good idea. This practice is used in many information retrieval tasks (such as search engine querying), but can be detrimental when syntactical understanding of language is required.\n\n### Removing punctuations, special symbols\n\nOnce again, we must consider the importance of punctuation and special symbols to our classifier’s predictive capabilities. We must also consider the importance of each symbol’s functionality. For example, the apostrophe allows us to define contractions and differentiate between words like it’s and its.\n\n### Lemmatising/Stemming\n\nBoth of these techniques reduce inflection forms to normalise words with the same lemma. The difference between lemmatising and stemming is that lemmatising performs this reduction by considering the context of the word while stemming does not. The drawback is that there is currently no lemmatiser or stemmer with a very high accuracy rate."},{"metadata":{"_uuid":"1f05940cb9476b92cd9791390e1e3f6db322d635"},"cell_type":"markdown","source":"Less common normalisation techniques include error correction, converting words to their parts of speech or mapping synonyms using a synonym dictionary. With the exception of error correction and synonym mapping, there are many pre-built tools for the other normalisation techniques, all of which can be found in the [nltk library.](https://www.nltk.org/)\n\nFor this particular classification problem, we will only use case normalisation. The rationale is that it will be hard to apply a stemmer or lemmatiser onto colloquial English and that since the text messages are so short, removing stop words might not leave us with much to work with."},{"metadata":{"trusted":true,"_uuid":"88a508a24a97cada68e38c5e7579a16050a5b9b8"},"cell_type":"code","source":"def review_messages(msg):\n    # converting messages to lowercase\n    msg = msg.lower()\n    return msg","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8020224f1ed7ce273270abbec46b393be32b645b"},"cell_type":"markdown","source":"**For reference, this function does case normalisation, removing stop words and lemmatising.**"},{"metadata":{"trusted":true,"_uuid":"6152af59d28d0f72caf89df4abe76584f246a0fd"},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nlemmatizer = WordNetLemmatizer()\nstopwords = set(stopwords.words('english'))\ndef alternative_review_messages(msg):\n    # converting messages to lowercase\n    msg = msg.lower()\n    # removing stopwords \n    msg = [word for word in msg.split() if word not in stopwords]\n    # using a lemmatized \n    msg = \" \".join([lemmatizer.lemmatize(word) for word in msg])\n    return msg","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3be07d79a1fa82bcc37ff8d3e4e94a8620beefad"},"cell_type":"markdown","source":"We apply the first function to normalise the text messages."},{"metadata":{"trusted":true,"_uuid":"704aed9b5730166959fcf0aab749b718ccb64694"},"cell_type":"code","source":"data['text'] = data['text'].apply(review_messages)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e37f487f2cc0eb50262ca0f2bf13ffc7040f166"},"cell_type":"markdown","source":"### Vectorizing the Text\n\nwe will use the TF-IDF vectorizer (Term Frequency — Inverse Document Frequency), a similar embedding technique which takes into account the importance of each term to document.\n\nWhile most vectorizers have their unique advantages, it is not always clear which one to use. In our case, the TF-IDF vectorizer was chosen for its simplicity and efficiency in vectorizing documents such as text messages.\n\nTF-IDF vectorizes documents by calculating a TF-IDF statistic between the document and each term in the vocabulary. The document vector is constructed by using each statistic as an element in the vector."},{"metadata":{"trusted":true,"_uuid":"f741a7184e2ef591d1027090fad15ed3eda74d7b"},"cell_type":"markdown","source":"After settling with TF-IDF, we must decide the granularity of our vectorizer. A popular alternative to assigning each word as its own term is to use a tokenizer. A tokenizer splits documents into tokens (thus assigning each token to its own term) based on white space and special characters.\n\nFor example, the phrase what’s going on might be split into what, ‘s, going, on.\n\nThe tokenizer is able to extract more information than word level analysers. However, tokenizers do not work well with colloquial English and may encounter issues splitting URLs or emails. As such, we will use a word level analyser, which assigns each word to its own term.\n\nBefore training the vectorizer, we split our data into a training set and a testing set. 10% of our data is allocated for testing."},{"metadata":{"trusted":true,"_uuid":"b420ca780a48c220792c6be5a649ab5635a55a6c"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size = 0.1, random_state = 1)\n# training the vectorizer \nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\nX_train = vectorizer.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a780e80985c6e55c734101c7ee1c969732a0a786"},"cell_type":"markdown","source":"### Building and Testing the Classifier\nThe next step is to select the type of classifier to use. Typically in this step, we will choose several candidate classifiers and evaluate them against the testing set to see which one works the best. However, for our purposes, we can assume that a Support Vector Machine works well enough.\n\nA larger value of C represents a smaller hyperplane. Parameters such as this can be precisely tuned via grid search."},{"metadata":{"trusted":true,"_uuid":"9101305f430d963c9672f6d7af16707ac39874ea"},"cell_type":"code","source":"from sklearn import svm\nsvm = svm.SVC(C=1000)\nsvm.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"370a12217065dce253a9b0985a412cfeeca7ae6e"},"cell_type":"markdown","source":"Now, let’s test it."},{"metadata":{"trusted":true,"_uuid":"a3d47f25bd1449e4a8793089d99897f10dc0264d"},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nX_test = vectorizer.transform(X_test)\ny_pred = svm.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1c8d5c3fb398327bbab282759acf5ba6a91f535"},"cell_type":"markdown","source":"The results aren’t bad at all! We have no false positives and around 15% false negatives.\n\nLet’s test it against a few new examples."},{"metadata":{"trusted":true,"_uuid":"be352eac9d403a3f31d8a9a750ad260cb7ceb9f9"},"cell_type":"code","source":"def pred(msg):\n    msg = vectorizer.transform([msg])\n    prediction = svm.predict(msg)\n    return prediction[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f358db6022b0d466b44d74a103f2a4ad3ddb138e"},"cell_type":"code","source":"pred(\"How are You ?\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86dd4eb920a3746372a337202db8b7d8c3952f46"},"cell_type":"code","source":"pred(\"You HAVE won 20 million CASH PRIZE\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1435c640dd8332d54829022a4d1d7312e58ef5db"},"cell_type":"markdown","source":"Looks like it works perfectly on the sample data set. Use your custom text and test the model"},{"metadata":{"trusted":true,"_uuid":"ad9d9c7020e6f95a7d5f678ba0957fda55102a8d"},"cell_type":"markdown","source":"## Happy Learning"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}