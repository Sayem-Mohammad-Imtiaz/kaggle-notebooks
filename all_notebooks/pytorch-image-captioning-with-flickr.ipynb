{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch.nn as nn\nimport torchvision.models as models\nimport torch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-10T11:40:06.87472Z","iopub.execute_input":"2021-06-10T11:40:06.875099Z","iopub.status.idle":"2021-06-10T11:40:06.879298Z","shell.execute_reply.started":"2021-06-10T11:40:06.875065Z","shell.execute_reply":"2021-06-10T11:40:06.87831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_model = models.inception_v3(pretrained=True, aux_logits=False)\nnew_model.fc","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:40:06.893201Z","iopub.execute_input":"2021-06-10T11:40:06.89344Z","iopub.status.idle":"2021-06-10T11:40:07.248297Z","shell.execute_reply.started":"2021-06-10T11:40:06.893418Z","shell.execute_reply":"2021-06-10T11:40:07.247345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Adapted from this wonderful tutorial:\n\nhttps://www.youtube.com/watch?v=y2BaTt1fxJU&list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz&index=21","metadata":{}},{"cell_type":"code","source":"class encoderCNN(nn.Module):\n    def __init__(self, embed_size, should_train=False):\n        super(encoderCNN, self).__init__()\n        self.should_train = should_train\n        self.inception = models.inception_v3(pretrained=True, aux_logits=False)\n        self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n        self.dropout= nn.Dropout(0.5)\n        self.relu = nn.ReLU()\n    \n    def forward(self, x):\n        features = self.inception(x)\n        \n        for name, param in self.inception.named_parameters():\n            param.requires_grad = False\n        \n#         for name, param in self.inception.named_parameters():\n#             if \"fc.weight\" in name or \"fc.bias\" in name:\n#                 param.requires_grad = True\n#             else:\n#                 param.required_grad = self.should_train\n        \n        \n        return self.dropout(self.relu(features))\n        ","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:40:07.249958Z","iopub.execute_input":"2021-06-10T11:40:07.250327Z","iopub.status.idle":"2021-06-10T11:40:07.257264Z","shell.execute_reply.started":"2021-06-10T11:40:07.25029Z","shell.execute_reply":"2021-06-10T11:40:07.256375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class decoderRNN(nn.Module):\n    def __init__(self, embed_size,vocab_size, hidden_size, num_layers):\n        super(decoderRNN, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n        self.linear = nn.Linear(hidden_size, vocab_size)\n        self.dropout = nn.Dropout(0.5)\n    \n    def forward(self, features, caption):\n        embeddings = self.dropout(self.embedding(caption))\n        embeddings = torch.cat((features.unsqueeze(0),embeddings), dim=0)\n        hiddens, _ = self.lstm(embeddings)\n        outputs = self.linear(hiddens)\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:40:07.259278Z","iopub.execute_input":"2021-06-10T11:40:07.259623Z","iopub.status.idle":"2021-06-10T11:40:07.270711Z","shell.execute_reply.started":"2021-06-10T11:40:07.259588Z","shell.execute_reply":"2021-06-10T11:40:07.269911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CNN2RNN(nn.Module):\n    def __init__(self, embed_size, vocab_size, hidden_size, num_layers):\n        super(CNN2RNN, self).__init__()\n        self.encoderCNN = encoderCNN(embed_size)\n        self.decoderRNN = decoderRNN(embed_size, vocab_size, hidden_size, num_layers)\n    \n    def forward(self, images, caption):\n        x = self.encoderCNN(images)\n        x = self.decoderRNN(x, caption)\n        return x\n    \n    def captionImage(self, image, vocabulary, maxlength=50):\n        result_caption = []\n        \n        with torch.no_grad():\n            x = self.encoderCNN(image).unsqueeze(0)\n            states = None\n            \n            for _ in range(maxlength):\n                hiddens, states = self.decoderRNN.lstm(x, states)\n                output = self.decoderRNN.linear(hiddens.squeeze(0))\n                predicted = output.argmax(1)\n                print(predicted.shape)\n                result_caption.append(predicted.item())\n                x = self.decoderRNN.embedding(output).unsqueeze(0)\n                \n                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n                    break\n        return [vocabulary.itos[i] for i in result_caption]","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:40:07.273326Z","iopub.execute_input":"2021-06-10T11:40:07.273561Z","iopub.status.idle":"2021-06-10T11:40:07.282894Z","shell.execute_reply.started":"2021-06-10T11:40:07.273539Z","shell.execute_reply":"2021-06-10T11:40:07.281887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class EncoderCNN(nn.Module):\n#     def __init__(self, embed_size, train_CNN=False):\n#         super(EncoderCNN, self).__init__()\n#         self.train_CNN = train_CNN\n#         self.inception = models.inception_v3(pretrained=True, aux_logits=False)\n#         self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n#         self.relu = nn.ReLU()\n#         self.times = []\n#         self.dropout = nn.Dropout(0.5)\n\n#     def forward(self, images):\n#         features = self.inception(images)\n#         return self.dropout(self.relu(features))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:40:07.284812Z","iopub.execute_input":"2021-06-10T11:40:07.285239Z","iopub.status.idle":"2021-06-10T11:40:07.296672Z","shell.execute_reply.started":"2021-06-10T11:40:07.285205Z","shell.execute_reply":"2021-06-10T11:40:07.295681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class DecoderRNN(nn.Module):\n#     def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n#         super(DecoderRNN, self).__init__()\n#         self.embed = nn.Embedding(vocab_size, embed_size)\n#         self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n#         self.linear = nn.Linear(hidden_size, vocab_size)\n#         self.dropout = nn.Dropout(0.5)\n\n#     def forward(self, features, captions):\n#         embeddings = self.dropout(self.embed(captions))\n#         embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)\n#         hiddens, _ = self.lstm(embeddings)\n#         outputs = self.linear(hiddens)\n#         return outputs\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:40:07.298069Z","iopub.execute_input":"2021-06-10T11:40:07.29831Z","iopub.status.idle":"2021-06-10T11:40:07.304701Z","shell.execute_reply.started":"2021-06-10T11:40:07.298287Z","shell.execute_reply":"2021-06-10T11:40:07.303907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class CNNtoRNN(nn.Module):\n#     def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n#         super(CNNtoRNN, self).__init__()\n#         self.encoderCNN = EncoderCNN(embed_size)\n#         self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n\n#     def forward(self, images, captions):\n#         features = self.encoderCNN(images)\n#         outputs = self.decoderRNN(features, captions)\n#         return outputs\n\n#     def caption_image(self, image, vocabulary, max_length=50):\n#         result_caption = []\n\n#         with torch.no_grad():\n#             x = self.encoderCNN(image).unsqueeze(0)\n#             states = None\n\n#             for _ in range(max_length):\n#                 hiddens, states = se/lf.decoderRNN.lstm(x, states)\n#                 output = self.decoderRNN.linear(hiddens.squeeze(0))\n#                 predicted = output.argmax(1)\n#                 result_caption.append(predicted.item())\n#                 x = self.decoderRNN.embed(predicted).unsqueeze(0)\n\n#                 if vocabulary.itos[predicted.item()] == \"<EOS>\":\n#                     break\n\n#         return [vocabulary.itos[idx] for idx in result_caption]","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:40:07.305974Z","iopub.execute_input":"2021-06-10T11:40:07.306412Z","iopub.status.idle":"2021-06-10T11:40:07.316337Z","shell.execute_reply.started":"2021-06-10T11:40:07.306378Z","shell.execute_reply":"2021-06-10T11:40:07.315536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Getting the dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas\nimport spacy\n\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom PIL import Image\nfrom torchvision.transforms import transforms","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:40:07.319251Z","iopub.execute_input":"2021-06-10T11:40:07.319589Z","iopub.status.idle":"2021-06-10T11:40:07.32598Z","shell.execute_reply.started":"2021-06-10T11:40:07.319562Z","shell.execute_reply":"2021-06-10T11:40:07.325262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spacy_eng = spacy.load(\"en\")","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:40:07.327964Z","iopub.execute_input":"2021-06-10T11:40:07.328425Z","iopub.status.idle":"2021-06-10T11:40:08.143751Z","shell.execute_reply.started":"2021-06-10T11:40:07.328389Z","shell.execute_reply":"2021-06-10T11:40:08.142636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Vocabulary:\n    def __init__(self, freq_threshold):\n        \n        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n        \n        self.freq_threshold = freq_threshold\n    \n    def __len__(self):\n        return len(self.itos)\n    \n    @staticmethod\n    def tokenizer_eng(text):\n        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n    \n    def build_vocabulary(self,sentences):\n        idx = 4\n        frequency = {}\n        \n        for sentence in sentences:\n            for word in self.tokenizer_eng(sentence):\n                if word not in frequency:\n                    frequency[word] = 1\n                else:\n                    frequency[word] += 1\n                \n                if (frequency[word] > self.freq_threshold-1):\n                    self.itos[idx] = word\n                    self.stoi[word] = idx\n                    idx += 1\n    \n    def numericalize(self,sentence):\n        tokenized_text = self.tokenizer_eng(sentence)\n        \n        return [self.stoi[word] if word in self.stoi else self.stoi[\"<UNK>\"] for word in tokenized_text ]\n                    \n        ","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:40:08.147349Z","iopub.execute_input":"2021-06-10T11:40:08.147641Z","iopub.status.idle":"2021-06-10T11:40:08.157001Z","shell.execute_reply.started":"2021-06-10T11:40:08.147611Z","shell.execute_reply":"2021-06-10T11:40:08.156041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotation = pandas.read_csv(\"../input/flickr8kimagescaptions/flickr8k/captions.txt\")\nannotation.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:40:08.1585Z","iopub.execute_input":"2021-06-10T11:40:08.159226Z","iopub.status.idle":"2021-06-10T11:40:08.222581Z","shell.execute_reply.started":"2021-06-10T11:40:08.159177Z","shell.execute_reply":"2021-06-10T11:40:08.221762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotation['caption'].tolist()[:2]","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:40:08.223806Z","iopub.execute_input":"2021-06-10T11:40:08.224139Z","iopub.status.idle":"2021-06-10T11:40:08.230551Z","shell.execute_reply.started":"2021-06-10T11:40:08.224105Z","shell.execute_reply":"2021-06-10T11:40:08.229616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FlickrDataset(Dataset):\n    def __init__(self, root_dir=\"../input/flickr8kimagescaptions/flickr8k/images\", caption_path=\"../input/flickr8kimagescaptions/flickr8k/captions.txt\", freq_threshold=5, transform=None, data_length=10000):\n        self.freq_threshold = freq_threshold\n        self.transform = transform\n        self.root_dir = root_dir\n    \n        self.df = pandas.read_csv(caption_path)[:data_length]\n        \n        self.captions = self.df['caption']\n        self.images = self.df['image']\n        \n        self.vocab = Vocabulary(freq_threshold)\n        \n        print(len(self.captions.tolist()))\n        self.vocab.build_vocabulary(self.captions.tolist())\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        caption = self.captions[index]\n        image = self.images[index]\n        \n        img = Image.open(os.path.join(self.root_dir,image)).convert(\"RGB\")\n        \n        if (self.transform):\n            img = self.transform(img)\n        \n        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n        \n        numericalized_caption += self.vocab.numericalize(caption)\n        \n        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n        \n        return img, torch.tensor(numericalized_caption)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:40:08.231937Z","iopub.execute_input":"2021-06-10T11:40:08.232404Z","iopub.status.idle":"2021-06-10T11:40:08.243673Z","shell.execute_reply.started":"2021-06-10T11:40:08.232368Z","shell.execute_reply":"2021-06-10T11:40:08.242701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyCollate:\n    def __init__(self, pad_value):\n        self.pad_value = pad_value\n    \n    def __call__(self,batch):\n        imgs = [item[0].unsqueeze(0) for item in batch]\n        img = torch.cat(imgs, dim=0)\n        targets = [item[1] for item in batch]\n        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_value)\n        \n        return img, targets","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:40:08.244836Z","iopub.execute_input":"2021-06-10T11:40:08.245215Z","iopub.status.idle":"2021-06-10T11:40:08.257772Z","shell.execute_reply.started":"2021-06-10T11:40:08.24518Z","shell.execute_reply":"2021-06-10T11:40:08.256945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# transform = transforms.Compose(\n#         [transforms.Resize((224, 224)), transforms.ToTensor(),]\n#     )\n\ntransform = transforms.Compose(\n        [\n            transforms.Resize((356, 356)),\n            transforms.RandomCrop((299, 299)),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n        ]\n    )","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:40:08.258731Z","iopub.execute_input":"2021-06-10T11:40:08.259012Z","iopub.status.idle":"2021-06-10T11:40:08.267791Z","shell.execute_reply.started":"2021-06-10T11:40:08.258988Z","shell.execute_reply":"2021-06-10T11:40:08.266981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_loader(root_dir=\"../input/flickr8kimagescaptions/flickr8k/images\", caption_path=\"../input/flickr8kimagescaptions/flickr8k/captions.txt\", transform=transform, batch_size=32, num_workers=8, shuffle=True, pin_memory=True):\n    dataset = FlickrDataset(root_dir=root_dir,caption_path=caption_path, transform=transform)\n    pad_value = dataset.vocab.stoi[\"<PAD>\"]\n    \n    loader = DataLoader(dataset=dataset, batch_size=32, num_workers=8, shuffle=True, pin_memory=True, collate_fn=MyCollate(pad_value))\n    \n    return loader, dataset","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:40:08.270458Z","iopub.execute_input":"2021-06-10T11:40:08.270769Z","iopub.status.idle":"2021-06-10T11:40:08.277383Z","shell.execute_reply.started":"2021-06-10T11:40:08.270739Z","shell.execute_reply":"2021-06-10T11:40:08.276584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" loader, dataset = get_loader()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:40:08.278503Z","iopub.execute_input":"2021-06-10T11:40:08.279111Z","iopub.status.idle":"2021-06-10T11:40:08.709633Z","shell.execute_reply.started":"2021-06-10T11:40:08.279075Z","shell.execute_reply":"2021-06-10T11:40:08.708874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets load a random example.","metadata":{}},{"cell_type":"code","source":"import random\nimport math\n\nx, y = dataset[math.floor(random.random() * len(dataset))]\nx.shape, y.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:40:08.710806Z","iopub.execute_input":"2021-06-10T11:40:08.711138Z","iopub.status.idle":"2021-06-10T11:40:08.735444Z","shell.execute_reply.started":"2021-06-10T11:40:08.711102Z","shell.execute_reply":"2021-06-10T11:40:08.734746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.imshow(x.permute(1,2,0))\nprint(y)\n\n# print(dataset.vocab.itos[1])\n\nfor i in y:\n    print(dataset.vocab.itos[int(i)],end=\" \")","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:40:08.738018Z","iopub.execute_input":"2021-06-10T11:40:08.738263Z","iopub.status.idle":"2021-06-10T11:40:08.889119Z","shell.execute_reply.started":"2021-06-10T11:40:08.738231Z","shell.execute_reply":"2021-06-10T11:40:08.888393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_checkpoint(state, filename = \"my_checkpoint.pth.tar\"):\n    print(\"saving checkpoint!\")\n    torch.save(state, filename)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:40:08.891642Z","iopub.execute_input":"2021-06-10T11:40:08.891909Z","iopub.status.idle":"2021-06-10T11:40:08.89768Z","shell.execute_reply.started":"2021-06-10T11:40:08.891884Z","shell.execute_reply":"2021-06-10T11:40:08.896718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_checkpoint(checkpoint, model, optimizer):\n    print(\"loading checkpoint!\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    model.load_state_optimizer(checkpoint[\"optimizer\"])\n    step = checkpoint[\"step\"]\n    return step","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:40:08.89901Z","iopub.execute_input":"2021-06-10T11:40:08.899491Z","iopub.status.idle":"2021-06-10T11:40:08.905718Z","shell.execute_reply.started":"2021-06-10T11:40:08.899452Z","shell.execute_reply":"2021-06-10T11:40:08.904933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training the model","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n# from torchvision.utils.tensorboard import SummaryWriter","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:40:08.906973Z","iopub.execute_input":"2021-06-10T11:40:08.907438Z","iopub.status.idle":"2021-06-10T11:40:08.914205Z","shell.execute_reply.started":"2021-06-10T11:40:08.907404Z","shell.execute_reply":"2021-06-10T11:40:08.913356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.backends.cudnn.benchmark = True\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nload_model = False\nsave_model=False\ntrain_CNN = False","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:40:08.917928Z","iopub.execute_input":"2021-06-10T11:40:08.91825Z","iopub.status.idle":"2021-06-10T11:40:08.923485Z","shell.execute_reply.started":"2021-06-10T11:40:08.91822Z","shell.execute_reply":"2021-06-10T11:40:08.92243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = CNN2RNN(embed_size=embed_size, hidden_size=hidden_size,vocab_size=vocab_size, num_layers=num_layers).to(device=device)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:40:08.925191Z","iopub.execute_input":"2021-06-10T11:40:08.925598Z","iopub.status.idle":"2021-06-10T11:40:08.932298Z","shell.execute_reply.started":"2021-06-10T11:40:08.925565Z","shell.execute_reply":"2021-06-10T11:40:08.931503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameters","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim \n\nstep = 0\nembed_size = 256\nhidden_size = 256\nnum_layers = 5\nnum_epochs = 5\nlearning_rate = 3e-4\nvocab_size = len(dataset.vocab)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:41:49.92261Z","iopub.execute_input":"2021-06-10T11:41:49.922952Z","iopub.status.idle":"2021-06-10T11:41:49.929676Z","shell.execute_reply.started":"2021-06-10T11:41:49.922913Z","shell.execute_reply":"2021-06-10T11:41:49.928929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CNN2RNN(embed_size=embed_size, hidden_size=hidden_size,vocab_size=vocab_size, num_layers=num_layers).to(device=device)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:41:49.937594Z","iopub.execute_input":"2021-06-10T11:41:49.937894Z","iopub.status.idle":"2021-06-10T11:41:50.668803Z","shell.execute_reply.started":"2021-06-10T11:41:49.937863Z","shell.execute_reply":"2021-06-10T11:41:50.667944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.decoderRNN","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:41:50.670386Z","iopub.execute_input":"2021-06-10T11:41:50.670731Z","iopub.status.idle":"2021-06-10T11:41:50.676759Z","shell.execute_reply.started":"2021-06-10T11:41:50.670691Z","shell.execute_reply":"2021-06-10T11:41:50.675762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\noptimizer = optim.Adam(model.parameters(), lr = learning_rate)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:41:50.678732Z","iopub.execute_input":"2021-06-10T11:41:50.679122Z","iopub.status.idle":"2021-06-10T11:41:50.690747Z","shell.execute_reply.started":"2021-06-10T11:41:50.679085Z","shell.execute_reply":"2021-06-10T11:41:50.689985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Only finetune the CNN\n# for name, param in model.encoderCNN.inception.named_parameters():\n#     if \"fc.weight\" in name or \"fc.bias\" in name:\n#         param.requires_grad = True\n#     else:\n#         param.requires_grad = train_CNN","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:41:50.692335Z","iopub.execute_input":"2021-06-10T11:41:50.692681Z","iopub.status.idle":"2021-06-10T11:41:50.699286Z","shell.execute_reply.started":"2021-06-10T11:41:50.692646Z","shell.execute_reply":"2021-06-10T11:41:50.698517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if load_model:\n    step = load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:41:50.700343Z","iopub.execute_input":"2021-06-10T11:41:50.700717Z","iopub.status.idle":"2021-06-10T11:41:50.711546Z","shell.execute_reply.started":"2021-06-10T11:41:50.700678Z","shell.execute_reply":"2021-06-10T11:41:50.71074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.train()\n\nfor epoch in range(num_epochs):\n    if save_model:\n        checkpoint = {\n            \"state_dict\": model.state_dict(),\n            \"optimizer\": model.state_dict(),\n            \"step\": step,\n        }\n        save_checkpoint(checkpoint)\n\n#     for idx, (imgs, captions) in tqdm(\n#         enumerate(loader), total=len(loader), leave=False\n#     ):\n    for idx, (imgs, captions) in enumerate(loader):\n        imgs = imgs.to(device)\n        captions = captions.to(device)\n        \n        score = model(imgs, captions[:-1])\n        \n#         print(score.shape, captions.shape)\n#         print(score.reshape(-1, score.shape[2]).shape, captions.reshape(-1).shape)\n#         print(\"why are we reshaping it here?\")\n        optimizer.zero_grad()\n        loss = loss_criterion(score.reshape(-1, score.shape[2]), captions.reshape(-1))\n        \n       \n        step += 1\n        \n        loss.backward()\n        optimizer.step()\n    print(f\"Loss for epoch {epoch}: {loss}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:41:50.714459Z","iopub.execute_input":"2021-06-10T11:41:50.71472Z","iopub.status.idle":"2021-06-10T11:50:54.614258Z","shell.execute_reply.started":"2021-06-10T11:41:50.714696Z","shell.execute_reply":"2021-06-10T11:50:54.612694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_path = \"../input/flickr8kimagescaptions/flickr8k/images/1032460886_4a598ed535.jpg\"\n\nimg = Image.open(image_path)\n\nimg = transform(img)\n\nplt.imshow(img.permute(1,2,0))\n\nimage_input = img.to(device=device) # check here\n\nprint(model.captionImage(image=image_input, vocabulary=dataset.vocab))\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-10T11:56:07.383178Z","iopub.execute_input":"2021-06-10T11:56:07.383735Z","iopub.status.idle":"2021-06-10T11:56:07.464525Z","shell.execute_reply.started":"2021-06-10T11:56:07.383622Z","shell.execute_reply":"2021-06-10T11:56:07.462551Z"},"trusted":true},"execution_count":null,"outputs":[]}]}