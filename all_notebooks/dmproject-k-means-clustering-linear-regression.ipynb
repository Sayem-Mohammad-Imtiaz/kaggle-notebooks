{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Simple K-means clustering on the Iris dataset","metadata":{"_cell_guid":"2e45ab38-0aab-08b5-c487-c15752d83b9d"}},{"cell_type":"code","source":"# DM Project","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n#importing the Iris dataset with pandas\ndataset = pd.read_csv('../input/datamining/DM_Project.csv')\ndataset = dataset.rename(columns={'Gross Margin': 'Gross_Margin', 'Job Type':'Job_Type'})\nprint(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data cleaning\ndataset.info()\ndataset = dataset.dropna()\ndataset.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nx = dataset[['Job_Type','Zip_','Income','Marketing Expense / order','Dummy_Recall','Dummy_Member','Total Technician Paid Time','Dummy_Estimate','Gross_Margin']]\nprint(x)\nprint(x.shape)\nsns.pairplot(x,hue = 'Zip_')\nsns.pairplot(x,hue = 'Job_Type')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#column selection\nx = dataset.iloc[:, [ 3, 4, 7, 9, 10, 12, 13]].values\n#x = dataset[['Income','Marketing Expense / order','Dummy_Recall','Dummy_Member','Total Technician Paid Time','Dummy_Estimate','Gross Margin']]\nx","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#scaling dataset\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(x)\nx","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Finding the optimum number of clusters for k-means classification\nfrom sklearn.cluster import KMeans\nwcss = []\n\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n    kmeans.fit(x)\n    wcss.append(kmeans.inertia_)\n    \n#Plotting the results onto a line graph, allowing us to observe 'The elbow'\nf = plt.figure()\nf.set_figwidth(16)\nf.set_figheight(9)\nplt.plot(range(1, 11), wcss)\nplt.title('The elbow method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS') #within cluster sum of squares\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Applying kmeans to the dataset / Creating the kmeans classifier\nkmeans = KMeans(n_clusters = 4, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\ny_kmeans = kmeans.fit_predict(x)\nprint(y_kmeans.shape)\nprint(len(y_kmeans))\ny_kmeans","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nplt.figure(figsize=(20, 20))\n#Visualising the clusters\nplt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1], s = 5, c = 'red', label = 'Cluster1')\nplt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1], s = 5, c = 'blue', label = 'Cluster2')\nplt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1], s = 5, c = 'green', label = 'Cluster3')\nplt.scatter(x[y_kmeans == 3, 0], x[y_kmeans == 3, 1], s = 5, c = 'black', label = 'Cluster4')\n\n#Plotting the centroids of the clusters\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], s = 30, c = 'yellow', label = 'Centroids')\n\nplt.legend()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Adding Clusters to DataFrame\ndataset['y_cluster'] = y_kmeans\ndataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset= dataset.drop(['Location Zip', 'Completion Date', 'Recall','Member Status','Estimate Accepted Online', 'Per Hr charge' ], axis=1)\ndataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"zipcode = pd.get_dummies(dataset.Zip_, drop_first=True)\njobtype = pd.get_dummies(dataset.Job_Type, drop_first=True)\nclusters = pd.get_dummies(dataset.y_cluster, drop_first=True, prefix=\"Cluster\")\n\nmerged = pd.concat([dataset,zipcode,jobtype,clusters],axis=1)\nfinal = merged.drop(['Job_Type','Zip_','y_cluster'],axis ='columns')\nfinal.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Feature selection with heatmap on One hot Encoded data\n#get correlations of each features in final\ncorrmat = final.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(final[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Regression on Final Dataset\nfrom sklearn.linear_model import LinearRegression \nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn import datasets\n\n\n# Input Data\nx = final.drop('Gross_Margin',axis='columns')\n# Output Data\ny = final.Gross_Margin\n\nX_train, X_test, y_train, y_test = train_test_split(x,y,test_size=.2, random_state=0)\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_predicted = model.predict(X_test)\nprint(model.coef_)\nprint(model.intercept_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Actual vs Predicted graph\nplt.scatter(y_predicted, y_test, edgecolors=(0, 0, 1))\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=3)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model evaluation for testing set\nfrom sklearn import metrics\nfrom sklearn import datasets\nmae = metrics.mean_absolute_error(y_test, y_predicted)\nmse = metrics.mean_squared_error(y_test, y_predicted)\nr2 = metrics.r2_score(y_test, y_predicted)\n\n\n\nprint(\"The model performance for testing set\")\nprint(\"--------------------------------------\")\nprint('MAE is {}'.format(mae))\nprint('MSE is {}'.format(mse))\nprint('R2 score is {}'.format(r2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have very high R2 which signifies overfitting.","metadata":{}},{"cell_type":"code","source":"#display adjusted R-squared\nimport statsmodels.api as sm\nx = sm.add_constant(x)\nmodel = sm.OLS(y, x).fit()\nprint(model.rsquared_adj)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport statsmodels.api as sm\nX_train = sm.add_constant(X_train)\nX_test = sm.add_constant(X_test)\n\nmodel = sm.OLS(y_train,X_train)\n\nresults = model.fit()\nresults.params","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(results.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### We have very high adjusted R2 which signifies overfitting.\nWe will be reducing the features based on correlation matrix.","metadata":{}},{"cell_type":"markdown","source":"# F- Test\n* If your Linear Regression model fit well then R-squared valued would be closer to 1.\n* Adjusted R-squared will penalies R-square value if you will keep on adding unecessary fetuares for building your model. \n* If Adjusted R-squared is much lesser than R-squared it's a sign that you are using a feature which has very lesser impact on the target.\n* F-Statistic or F-test is used to access the significance of overall Regression model.\n\n**It compares the existing model with multiple feature with Intercept only model(without feature). The Null hypothesis is that these 2 models are equal.**\nWhereas alternate Hypothesis is that Intercept only model is worse than our model.\nWe will get back a P-value(Prob (F-statistic)) and F-statistics value for whether to accept or reject the Null hypothesis.\n### If P-value(Prob (F-statistic)) < 0.05 and F-statistics > 1 or high indicates that good relationship amoung the target and features.\n\n# T-Test\n* T-test will take into account one feature at a time.\n* The Null hypothesis in this case is feature coefficient is equal to 0. And Alternate hypothesis is that feature coefficient not equal 0.\n\n* if  P>|t| value is 0 or near to 0 , it means you reject the Null hypothesis and accept the Alternate hypothesis.\n\n**Omnibus tests** are a kind of statistical test. They test whether the explained variance in a set of data is significantly greater than the unexplained variance, overall.\n\n\n**AIC and BIC** differ by the way they penalize the number of parameters of a model. More precisely, BIC criterion will induce a higher penalization for models with an intricate parametrization in comparison with AIC criterion.\n\n**Log-Likelihood**\n* Coefficients of a linear regression model can be estimated using a negative log-likelihood function from maximum likelihood estimation.\n* The negative log-likelihood function can be used to derive the least squares solution to linear regression.\n\n## Dublin Watson Test\n\nThe Durbin Watson Test is a measure of **autocorrelation (also called serial correlation)** in residuals from regression analysis. Autocorrelation is the similarity of a time series over successive time intervals. \n\n* It can lead to underestimates of the standard error and can cause you to think predictors are significant when they are not. The Durbin Watson test looks for a specific type of serial correlation, the AR(1) process.\n\nThe Hypotheses for the Durbin Watson test are:\n* H0 = no first order autocorrelation.\n* H1 = first order correlation exists.\n\n## Jarque-Bera Test\n\nThe Jarque-Bera Test, is a test for normality. \nSpecifically, the test matches the skewness and kurtosis of data to see if it matches a normal distribution. The data could take many forms, including:\n\n* Time Series Data.\n* Errors in a regression model.\n* Data in a Vector.\n\n\n**A normal distribution has a skew of zero (i.e. it’s perfectly symmetrical around the mean) and a kurtosis of three; kurtosis tells you how much data is in the tails and gives you an idea about how “peaked” the distribution is. It’s not necessary to know the mean or the standard deviation for the data in order to run the test.**","metadata":{}},{"cell_type":"code","source":"##Feature selection with heatmap on non Encoded data\n#get correlations of each features in final\ncorrmat = dataset.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(dataset[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dropping cols\ndataset.drop(['Zip_', 'Job_Type', 'y_cluster','Dummy_Member'], axis = 1,inplace=True)\ndataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Regression on Dataset with feature selection\n\n# Input Data\nx = dataset.drop('Gross_Margin',axis='columns')\n# Output Data\ny = dataset.Gross_Margin\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=42)\n\nimport statsmodels.api as sm\nX_train = sm.add_constant(X_train)\nX_test = sm.add_constant(X_test)\n\nmodel = sm.OLS(y_train,X_train)\n\nresults = model.fit()\nresults.params","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(results.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_predicted = results.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Actual vs Predicted graph\n#plt.figure(figsize=(20, 20))\nplt.scatter(y_predicted, y_test, edgecolors=(0, 0, 1))\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=3)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model evaluation for testing set\nfrom sklearn import metrics\nfrom sklearn import datasets\nmae = metrics.mean_absolute_error(y_test, y_predicted)\nmse = metrics.mean_squared_error(y_test, y_predicted)\nr2 = metrics.r2_score(y_test, y_predicted)\n\nprint(\"The model performance for testing set\")\nprint(\"--------------------------------------\")\nprint('MAE is {}'.format(mae))\nprint('MSE is {}'.format(mse))\nprint('R2 score is {}'.format(r2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have very high R2 which signifies overfitting.","metadata":{}},{"cell_type":"code","source":"#display adjusted R-squared\nimport statsmodels.api as sm\nx = sm.add_constant(X)\nmodel = sm.OLS(y, x).fit()\nprint(model.rsquared_adj)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### We have very high adjusted R2 which signifies overfitting.\nWe will be reducing the features based on correlation matrix.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Backup Code","metadata":{}},{"cell_type":"code","source":"#Backup Code","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}