{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Flight delay time exploratory data analysis\n\n\n**Scroll down to Part 3 for this week's work**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport glob\nimport seaborn as sns\nimport networkx as nx\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First we read in the input files. We can use the `glob` package with `*` as a wildcard to make a list of all the csv files, and then open and concatenate all the files in the list to get a single dataframe."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.concat([pd.read_csv(f) for f in glob.glob(\"/kaggle/input/historical-flight-and-weather-data/*.csv\") ])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, lets explore some basic characteristics of our data."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.hist(figsize=(20,20)); # Tip: put a semicolon at the end of the line to avoid printing a bunch of text output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So from the initial analysis above, we can see that we've got a database of 5.5 billion flights, with each record including information about the airline (\"carrier_code\"), origin and destination airport, date and time, and weather information. This dataset is not well documented, but we'll assume that `*_x` corresponds to weather at the origin airport and `*_y` corresponds to weather at the destination airport. There is also information about flight delays and cancellations.\n\nOur goal is always to do something useful. Some useful things we could do with this dataset could be to gain insight into what conditions are related to delayed and canceled flights, and potentially predict or avoid those delays in the future, so we will explore the dataset with that goal in mind.\n\nFirst, we'll look into the frequency of delays and cancellations:"},{"metadata":{"trusted":true},"cell_type":"code","source":"(df.arrival_delay > 0).sum() / df.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(df.arrival_delay > 30).sum() / df.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(df.arrival_delay > 60).sum() / df.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(df.departure_delay > 0).sum() / df.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"((df.arrival_delay > 0) & (df.departure_delay > 0)).sum() / df.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.cancelled_code.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(df.cancelled_code != \"N\").sum() / df.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above, we can see that 34% of flight arrivals are delayed, 12% are delayed by more than 30 minutes, and 7% are delayed by more than one hour. (We're assuming the times are in minutes. Hopefully the benefit of having a well-documented dataset is apparent here.)\n\nIf we assume that a cancelled code of \"N\" means not cancelled, and everything else is cancelled, then about 1.5% of flights are cancelled.\n\nWe can start out by looking at how conditions were different for flights that were canceled compared to other flights. One way to do this is to create two sets of histograms:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cancel = df[df.cancelled_code != \"N\"]\ndf_cancel.hist(figsize=(20,20)); ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_nocancel = df[df.cancelled_code == \"N\"]\ndf_nocancel.hist(figsize=(20,20)); ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One insight this gives us is that the max windspeed for non-canceled flights appears much higher than the max windspeed for flights that were canceled. TWe can investigate this further:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_cancel.HourlyWindSpeed_x.mean(), df_cancel.HourlyWindSpeed_x.median(), df_cancel.HourlyWindSpeed_x.max())\nprint(df_nocancel.HourlyWindSpeed_x.mean(), df_nocancel.HourlyWindSpeed_x.median(), df_nocancel.HourlyWindSpeed_x.max())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part 2: Network analysis\n\nLast week, we started an exploratory analysis of this dataset, treating it as tabular data. However there is also a graph or network aspect of this datasetâ€”it's a \"transportation network'. This week, we will explore that aspect.\n\nFirst, let's calculate the number of flights on each \"route\", which is the number of flights that share an origin and destination airport:"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_flights = df.groupby(by=[\"origin_airport\", \"destination_airport\"]).count()['flight_number']\n\nnum_flights.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_flights.reset_index().head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, let's create a directed graph of the different routes."},{"metadata":{"trusted":true},"cell_type":"code","source":"g = nx.DiGraph()\n\nfor _, edge in num_flights.reset_index().iterrows():\n    g.add_edge(edge['origin_airport'], edge['destination_airport'], weight=edge['flight_number'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can make a plot of the graph:"},{"metadata":{},"cell_type":"markdown","source":"Next, let's calculate the degree centrality and betweenness centrality of each airport and create a data frame that includes the columns `airport`, `deg_cen`, and `bet_cen`:"},{"metadata":{"trusted":true},"cell_type":"code","source":"deg_cen = nx.degree_centrality(g)\n\nairport, dc = [], []\nfor k in deg_cen:\n    airport.append(k)\n    dc.append(deg_cen[k])\n\ndata = {\"airport\": airport, \"deg_cen\": dc}\n    \ndf_deg_cen = pd.DataFrame(data)\ndf_deg_cen.set_index(\"airport\", inplace=True)\n\ndf_deg_cen.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bet_cen = nx.betweenness_centrality(g, weight=\"weight\")\n\nairport, bc = [], []\nfor k in bet_cen:\n    airport.append(k)\n    bc.append(bet_cen[k])\n\ndata = {\"airport\": airport, \"bet_cen\": bc}\n    \ndf_bet_cen = pd.DataFrame(data)\ndf_bet_cen.set_index(\"airport\", inplace=True)\n\ndf_bet_cen.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"net_stats = df_deg_cen\nnet_stats[\"bet_cen\"] = df_bet_cen.bet_cen\nnet_stats.reset_index(inplace=True)\n\nnet_stats.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's add our network statistics for each airport to data frame of flights, and see whether they are correlated with our \"departure delay\" dependent variable:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_net_stats = df.merge(net_stats, left_on=\"origin_airport\", right_on=\"airport\")\n\ndf_net_stats[\"origin_bet_cen\"] = df_net_stats[\"bet_cen\"]\ndf_net_stats[\"origin_deg_cen\"] = df_net_stats[\"deg_cen\"]\ndf_net_stats.drop([\"airport\", \"deg_cen\", \"bet_cen\"], inplace=True, axis=1)\n\ndf_net_stats.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_net_stats = df_net_stats.merge(net_stats, left_on=\"destination_airport\", right_on=\"airport\")\n\ndf_net_stats[\"destination_bet_cen\"] = df_net_stats[\"bet_cen\"]\ndf_net_stats[\"destination_deg_cen\"] = df_net_stats[\"deg_cen\"]\ndf_net_stats.drop([\"airport\", \"deg_cen\", \"bet_cen\"], inplace=True, axis=1)\n\ndf_net_stats.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_net_stats[[\"arrival_delay\", \"destination_bet_cen\",\"destination_deg_cen\", \"origin_bet_cen\",\"origin_deg_cen\"]].corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Can you conclude anything from these correlations?"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Part 3: Spatial analysis\n\nSo far, we have explored this dataset as tabular data and as netork data. However, this dataset also has a spatial component, which we will explore today.\n\nFirst, we will merge it with a dataset of airport locations and calculate the distance of each flight. For this, we will use data from: https://openflights.org/data.html."},{"metadata":{"trusted":true},"cell_type":"code","source":"! wget https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['Airport ID', #Unique OpenFlights identifier for this airport.\n'Name', # Name of airport. May or may not contain the City name.\n'City', # Main city served by airport. May be spelled differently from Name.\n'Country', # Country or territory where airport is located. See Countries to cross-reference to ISO 3166-1 codes.\n'IATA', # 3-letter IATA code. Null if not assigned/unknown.\n'ICAO', # 4-letter ICAO code. Null if not assigned.\n'Latitude', # Decimal degrees, usually to six significant digits. Negative is South, positive is North.\n'Longitude', # Decimal degrees, usually to six significant digits. Negative is West, positive is East.\n'Altitude', # In feet.\n'Timezone', # Hours offset from UTC. Fractional hours are expressed as decimals, eg. India is 5.5.\n'DST', # Daylight savings time. One of E (Europe), A (US/Canada), S (South America), O (Australia), Z (New Zealand), N (None) or U (Unknown). See also: Help: Time\n'Tz', # database time zone\tTimezone in \"tz\" (Olson) format, eg. \"America/Los_Angeles\".\n'Type', # Type of the airport. Value \"airport\" for air terminals, \"station\" for train stations, \"port\" for ferry terminals and \"unknown\" if not known. In airports.csv, only type=airport is included.\n'Source', # Source of this data. \"OurAirports\" for data sourced from OurAirports, \"Legacy\" for old data not matched to OurAirports (mostly DAFIF), \"User\" for unverified user contributions. In airports.csv, only source=OurAirports is included.\n]\n\nairports = pd.read_csv(\"airports.dat\", names=cols)\n\nairports.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, let's make a map of our routes. It could be useful to make a map of the routes between each airport, where the thickness of the line along each route is proportional to the number of flights (`flight_number` in the above dataframe). The first thing we can do is add the latitude and longitudes of the origin and destination airports to the `num_flights` dataframe we created last week."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_flights_spatial = num_flights.reset_index().merge(airports[[\"IATA\", \"Latitude\", \"Longitude\"]], how=\"inner\", left_on=\"origin_airport\", right_on=\"IATA\")\nnum_flights_spatial[\"lat_origin\"] = num_flights_spatial[\"Latitude\"]\nnum_flights_spatial[\"lon_origin\"] = num_flights_spatial[\"Longitude\"]\nnum_flights_spatial.drop(['IATA', 'Latitude', \"Longitude\"], inplace=True, axis=1)\n\nnum_flights_spatial = num_flights_spatial.merge(airports[[\"IATA\", \"Latitude\", \"Longitude\"]], how=\"inner\", left_on=\"destination_airport\", right_on=\"IATA\")\nnum_flights_spatial[\"lat_destination\"] = num_flights_spatial[\"Latitude\"]\nnum_flights_spatial[\"lon_destination\"] = num_flights_spatial[\"Longitude\"]\nnum_flights_spatial.drop(['IATA', 'Latitude', \"Longitude\"], inplace=True, axis=1)\n\n\nnum_flights_spatial.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we're ready to make our map. Let's try plotting the routes using arcs like are shown in the maps you see on airplanes next to the lunch menu, which we know are called \"great circle\" paths because (for this hour, at least) we're transportation engineers.\n\nHowever, making plots of great circle routes wasn't in the readings for this week, what should we do?\n\nTo do this, we can use one of the great secrets of IT professionals everywhere, which, when encountered with a problem you've never seen before, to Google `[tool] [verb] [noun]`, where `[tool]` is the programming language or software library you want to use, `[verb]` is what you want to do, and `[noun]` is what you want to do it to.\n\nIn this case, our tool is \"Geopandas\" (or \"Python\", but more specific is usually better), our verb is \"plot\" or \"map\", and our noun is \"great circle routes\". So we can put \"`geopandas plot great circle routes`\" or \"`geopandas map great circle routes`\" into google, and see what comes out. More often than not, there will be a similar question on stackoverflow.com or a blog post by an unemployed software engineer describing something very similar to what you're looking for, so you just have to adapt it to your use case.\n\n(As an aside, \"`[tool] [context (optional)] [error message]`\" is a good google pattern for debugging your code. {As a double aside, \"`what noise does [animal] make`\" is a good google pattern for occupying a toddler for an extended period of time.})"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_flights_spatial.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When you give a transportation engineer a map of great circle routes, they might wonder what difference is between the great circle distance traveled on these routes and the distance it looks like we would travel if we go in a straight line using the \"Conus Albers\" projection (`epsg=5070`). Lets `describe` that, assuming that the original data use the NAD83 coordinate reference system (`epsg=4269`).\n\nFirst let's calculate the great circle distance of each route. We need to do some auxilliary googling to figure out how to do that (e.g. \"`python great circle distance`\"), which I'll save you by telling you that it's [sklearn.metrics.pairwise.haversine_distances](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.haversine_distances.html). (There's an example at the bottom of that page.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics.pairwise import haversine_distances\nfrom math import radians\n\n\ndef great_circle(row):\n    d = haversine_distances([[radians(row.lat_origin), radians(row.lon_origin)], [radians(row.lat_destination), radians(row.lon_destination)]])\n    d = d * 6371000/1000\n    return d[0][1] # The haversine distances function returns a 2-d array for some reason.\n\nnum_flights_spatial[\"great_circle_km\"] = num_flights_spatial.apply(great_circle, axis=1)\n\nnum_flights_spatial[\"great_circle_km\"].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, that was enlightening.\n\nFinally, let's see if the our flight delay variable is correlated with the distance of the flight:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_spatial = df_net_stats.merge(num_flights_spatial, how=\"inner\", on=[\"origin_airport\",\"destination_airport\"])\n\ndf_spatial.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_spatial[[\"departure_delay\",\"arrival_delay\", \"great_circle_km\", \"flight_number_y\"]].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Part 4: Linear regression\n\nIn this class, we're going to make a model of this dataset using linear regression.\n\nFirst, let's refamiliarize ourselves with the variables we're working with:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_spatial.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's do some feature engineering. We'll choose some of the variables that we think are likely to be predictive of flight delays (based on our exploratory data analysis above) and prepare them for use in a model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_columns = []\nfeature_data = {}\n\ndef add_categorical_column(df, key):\n    feat_col = tf.feature_column.categorical_column_with_vocabulary_list(\n        key=key, vocabulary_list=df[key].unique())\n    feat_col = tf.feature_column.indicator_column(feat_col)\n    feature_columns.append(feat_col)\n    feature_data[key] = np.array(df[key])\n    \nadd_categorical_column(df_spatial, \"carrier_code\")\n\n# add more here...\n\n\nfeature_layer = tf.keras.layers.DenseFeatures(feature_columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](http://)What other columns can we add to the cell above?"},{"metadata":{},"cell_type":"markdown","source":"Finally, Let's define our label data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"label_data = np.array(df_spatial[\"arrival_delay\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Modeling\n\nNow, let's use these features to make a model of our data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# These functions are adapted from the machine learning crash course.\n\ndef create_model(learning_rate, feature_layer):\n  # Most simple tf.keras models are sequential.\n  model = tf.keras.models.Sequential()\n\n  # Add the layer containing the feature columns to the model.\n  model.add(feature_layer)\n    \n  # Add one linear layer to the model to yield a simple linear regressor.\n  model.add(tf.keras.layers.Dense(units=1, input_shape=(1,)))    \n    \n  # Construct the layers into a model that TensorFlow can execute.\n  model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=learning_rate),\n                loss=\"mean_squared_error\",\n                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n    \n  return model\n\ndef plot_the_loss_curve(epochs, rmse):\n  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n\n  plt.figure()\n  plt.xlabel(\"Epoch\")\n  plt.ylabel(\"Root Mean Squared Error\")\n\n  plt.plot(epochs, rmse, label=\"Loss\")\n  plt.legend()\n  plt.ylim([rmse.min()*0.94, rmse.max()* 1.05])\n  plt.show()  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The following variables are the hyperparameters.\nlearning_rate = 0.05\nepochs = 10\nbatch_size = 1000\n\n# Create and compile the model's topography.\nmodel = create_model(learning_rate, feature_layer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the model on the training set.\nhistory = model.fit(x=feature_data, y=label_data, batch_size=batch_size,\n                  epochs=epochs, shuffle=True, steps_per_epoch=100)\n# We wouldn't normally use the 'steps_per_epoch' argument, but we're using it\n# here so the training goes faster. (Basically we're only training on part of the data).\n\n# The list of epochs is stored separately from the rest of history.\nepochs = history.epoch\n\n# Isolate the mean absolute error for each epoch.\nhist = pd.DataFrame(history.history)\nrmse = hist[\"root_mean_squared_error\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_the_loss_curve(epochs, rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data = {key : feature_data[key][0:1000] for key in feature_data}\ny_prediction = model.predict(plot_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(label_data[0:1000], y_prediction);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}