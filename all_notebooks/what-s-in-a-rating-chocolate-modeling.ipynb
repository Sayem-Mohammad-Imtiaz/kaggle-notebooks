{"cells":[{"metadata":{"_cell_guid":"d5ff69f5-0524-4145-a3fe-e8bc5ab50334","_uuid":"93cfefa42bd2ee44af52095922d959e8aef1029b"},"cell_type":"markdown","source":"# For the Love of Chocolate\nChocolate is loved worldwide.Can we identify the universal traits of chocolate we all love? Let's find out!\n\n![](https://www.wtfclub.net/wp-content/uploads/2017/05/Chocolate-min.jpg)\nSince we're all data people here, check out [some fun chocolate facts](https://www.wtfclub.net/chocolate-facts/) to get you properly motivated!\n\nAs a motivating philosophy, remember that machine learning can be done for two different end goals, prediction or understanding, and those goals shape the modeling decisions you make. For prediction, you emphasize processing and complexity in order to get the best performance possible. For interpretation, however, you relax the model performance a bit in order to understand the hidden relationships in the data. \n\nIn this notebook, we are going to approach machine learning for understanding, and thus the actual processing of the data is going to be light. But, because we aren't doing heavy lifting, we should find interesting insights within the model itself!\n\nSo, do just enough processing to get the data into modeling format, then feed it to a lighter model. Our weapons of choice? Logistic Regression and Random Forests.\n\nOne thing that's interesting about this data is it's more descriptive than numerical. Not a lot of information on the contents of the bars themselves, just labels of where they were from. It's unlikely we can infer a lot about the taste preferences themselves, but we can discover what labels *contribute* to the taste preferences we have.\n\nSo, let's get started!","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"bc5a22ea-3192-40a5-8b9f-7b541359e515","_uuid":"a8dd86e337d89b67cd40c66eb7a6108da7cb35ff","collapsed":true,"trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4e7d283c-7e74-4128-a131-a595e74d0f77","_uuid":"a7c7a88d709ff0d5ca023cc666235a0c6c818869","trusted":false},"cell_type":"code","source":"df = pd.read_csv('../input/flavors_of_cacao.csv')\n\n#removing some of the unicode non printing characters for convenience\nfor c in df.columns.values:\n    if c != 'Rating':\n        df[c] = df[c].apply(lambda x: str(x).replace(u'\\xa0', ''))\n\n        \nprint(\"Number of records: \" + str(df.shape[0]))\nprint(\"Baseline Average Rating: \" + str(df['Rating'].mean()))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e2a55c20-db6c-47d1-a398-8cd9978ec788","_uuid":"b4225ed752c0ee3366bb52b2a8a6093fba0ee04a"},"cell_type":"markdown","source":"# Data Exploration\n\nIt's important to start off by getting a feel for the data. Who knows, we might get some ideas for valuable features to add!\n\n### Rating\nLet's start with the target. What does the rating variable actually look like? ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"037b77e4-ac68-45ed-ab30-797b7cce872c","_uuid":"fe5eef88af7f2b87edfe59611fdb4f1f95e6dbe6","trusted":false},"cell_type":"code","source":"sns.countplot(df['Rating']).set_title('Distribution Over Chocolate Ratings')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f0505d1d-ad08-46fc-8612-8c59aa9feae7","_uuid":"216ba8dea9961d21ae786093dd53a8b9dcc05537"},"cell_type":"markdown","source":"A lot of chocolates in the 2.5-3.5 range. Not suprising. But there's actually some chocolates at the tail ends. Which are the sad ones liked by none? And the ones loved by all?\n\nLooks like Belgium made some duds, but Italy made some hits!","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"f9e66652-5064-4b02-b31f-8302adfd8efd","_uuid":"04fbb1d03c1750fdef44e4a739d9ff7e20a03179","trusted":false},"cell_type":"code","source":"print(\"Least liked chocolates: \")\ndf[df['Rating'] == 1]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2e195c94-ddca-4a71-beb7-82c7e2a9587f","_uuid":"19aa13ac1aba56b49fe02587d84e5bffdc90fd17","trusted":false},"cell_type":"code","source":"print(\"Most liked chocolates: \")\ndf[df['Rating'] == 5]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"41fc1d43-1ba3-4b93-aadc-01210698b5fe","_uuid":"bb4e802eec4dae07b6f4046992d85b33dfe4e8ff"},"cell_type":"markdown","source":"### Review Date\nIs there more data in the past, or is the volume growing with time? Is recent chocolate better, or is it staying the same?","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"3b4ab916-c049-44e6-95a4-61075d14ae41","_uuid":"e9acf44e1875045e811da722309ae525cbb0710f","trusted":false},"cell_type":"code","source":"sns.countplot(df['Review\\nDate']).set_title('Rating Volume Over Time')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1edf61f3-3486-440f-ac60-978edab46f38","_uuid":"fe5274f75eeabc018521424cfdd2d4cc48e280bf","trusted":false},"cell_type":"code","source":"sns.set_style(\"darkgrid\")\n\n#fitting a linear regression line to a scatterplot\nsns.regplot(x=df['Review\\nDate'].apply(lambda x: float(x)), \n            y=df['Rating'].apply(lambda x: float(x)))\n\nplt.title('Rating Over Time')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c6b2db83-3124-4593-8127-5ef8f3bf7470","_uuid":"9baec6078bbf1c03bbdb6cc42c66aabeec5594dd"},"cell_type":"markdown","source":"On the whole, chocolate seems to be getting better with time. As a chocolate lover, that's good to hear!\n\nAlthough, it's more noteworthy that variation in ratings is getting smaller over time, given the funneled shape towards the right. \n\n### Cocoa Percent","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"22203d80-b39c-43ab-88c1-3c9bd5db442f","_uuid":"e69797a0448cf3920bc2ed2722499d7f6ccfea1e","trusted":false},"cell_type":"code","source":"print(\"Total unique Cocoa Percent: \" + str(len(df['Cocoa\\nPercent'].unique())))\ndf['Cocoa\\nPercent'] = df['Cocoa\\nPercent'].apply(lambda x: float(str(x).replace('%', '')))\n\nsns.regplot(x=df['Cocoa\\nPercent'], y=df['Rating'])\n\nplt.title('Rating By Cocoa Percentage')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"68e45f3b-df15-4ded-a9b5-426228f17706","_uuid":"c77d0b8e3df450ada953e893a29f0834fc8355c0"},"cell_type":"markdown","source":"On the other hand, rating seems to go down with the increase of cocoa. \n\nI guess people aren't as big a fan of dark chocolate. ¯\\_(ツ)_/¯\n\n### Non Numeric Columns","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"e8bf4cf4-39a0-4021-931c-a280709d97bf","_uuid":"0510ad9ea71c9a8f1e26e81a798cb1c208ba33c3","trusted":false},"cell_type":"code","source":"df.replace('', np.nan, ).isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"91c2f5a7-a333-4af5-ab11-d338c7e1c144","_uuid":"d660e40fa9b419466ac58a9f2559b877866659d1"},"cell_type":"markdown","source":"Most everything is filled, exept we're missing about half of the Bean Type, and a few Broad Bean Origin. Why is it missing for some? Is it an insightful variable? Does the Bean Type seem to matter much? ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"1c9f11d4-b87b-401b-91cb-5b4837c206c3","_uuid":"8e4c36ad45076bd463cd640467f6d13252909a84","trusted":false},"cell_type":"code","source":"df['Bean\\nType'].fillna(\"Uknown\", inplace = True)\n#for one pesky row that has \"nan\"\ndf['Bean\\nType'].replace(\"nan\", \"Uknown\", inplace = True)\n\nprint(\"Total unique bean types: \" + str(len(df['Bean\\nType'].unique())))\n\nf = {'Rating':['size','mean','std']}\ndf.groupby('Bean\\nType').agg(f)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"69f5edf6-e81a-46aa-9321-eac0f1f9456d","_uuid":"159893d86b98880e11202103e833de2297595475"},"cell_type":"markdown","source":"There seems to be three main players in the Bean cateory, Criollo, Forastero, and Trinitario, with some variations within the group.\n\nThe groups all seem to be around 3.1 on the whole, including the \"Uknown\" records, so maybe there's more than just the bean type when it comes to good taste!\n\n### Company Location\nWhat about countries? Are different parts of the world preferred for their style of chocolate?","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"fea1ef6e-d088-4004-89d5-c85f2802d4ab","_uuid":"f0fa3b5879d9a5ba3bd5ac9d789b945c14c8af9e","trusted":false},"cell_type":"code","source":"print(\"Total unique bean types: \" + str(len(df['Company\\nLocation'].unique())))\n\nf = {'Rating':['size','mean','std']}\ndf.groupby('Company\\nLocation').agg(f)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1b1c15e3-73be-4e5d-9886-fdddce2ecd83","_uuid":"7be1cb182272c1cb6382e07221a50f43f8d632ab"},"cell_type":"markdown","source":"Still a lot of twos and threes. Nothing to noteworthy. \n\nThe other columns have too many unique values to really visualize, so we will likely treat them as text later.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"892f972f-8c3a-4fe4-ab94-8feb214bf1b3","_uuid":"dcbaea1d1fcc69e3e7cce3bbe27fb6f828e6450c","trusted":false},"cell_type":"code","source":"print(\"Total unique Specific Bean Origin or Bar Name: \" + str(len(df['Specific Bean Origin\\nor Bar Name'].unique())))\nprint(\"Total unique REF: \" + str(len(df['REF'].unique())))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"95876132-3536-4e4b-8fc7-f87bb5797552","_uuid":"59fd90e99ba26c188690cf3db4c096e1ce021eb4"},"cell_type":"markdown","source":"----------------------------------------------------------------------------\n# Modeling Time!\n\nNow let's get to the good stuff, actual modeling! Of course, we start by preprocessing. With sklearn, it's simple.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"8a3b7ac9-bade-4edf-8622-774f30bc9e18","_uuid":"a9de51bfc3fef4f1bb6204c6c9808ca55026a7ad","trusted":false},"cell_type":"code","source":"X = df.drop('Rating', axis = 1)\ny = df['Rating']\n\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7fd6b9c5-9488-4629-b916-6ca402b0fb3a","_uuid":"90aa3bf3a7bd0574bcdf22054e4c789538dd7e68"},"cell_type":"markdown","source":"## Categorical Variables\n\nTo start off, I'm going to go ahead and one hot encode the categorical variables **before** I split them into training and test sets. \n\nYou can do it after you split them as well, but depending on how you do it, you may have mismatched columns in the two data sets, since some rarer values likely only appear in one data set or the other. Which means writing more code to fix it. To avoid that headache, I'm going to go ahead and do it now.\n\n### Company (Maker-if known)\n","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"d0f7bccf-4c2d-43a8-a465-b907ea23a859","_uuid":"668bef28fd07bbebf0ee8b5de4c42ed3bf75e0da","trusted":false},"cell_type":"code","source":"#using pd.get_dummies to create a one hot encoded matrix\ndummies = pd.get_dummies(X['Company\\xa0\\n(Maker-if known)'])\n#Adding the variable to the column names so I can keep track of which original variable it came from\ndummies.columns = ['Company_' + k for k in dummies.columns.values]\nX = pd.concat([X, dummies], axis=1)\n\n#dropping the original column \ndel X['Company\\xa0\\n(Maker-if known)']\n\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b816614b-f415-4af5-b02c-b889a64fbf0c","_uuid":"e0d558dd26b196cea4b32e86af9ed000417e1e70"},"cell_type":"markdown","source":"### Company Location","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"9c3f65b5-3ac4-4a07-8ded-04d398add9bc","_uuid":"3755104cf84aa50b50e859fcba3866449fffd6b3","trusted":false},"cell_type":"code","source":"#using pd.get_dummies to create a one hot encoded matrix\ndummies = pd.get_dummies(X['Company\\nLocation'])\n#Adding the variable to the column names so I can keep track of which original variable it came from\ndummies.columns = ['Company_Loc_' + k for k in dummies.columns.values]\n\nX = pd.concat([X, dummies], axis=1)\n\n#dropping the original column \ndel X['Company\\nLocation']\n\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ee0f6c1c-cf8a-4d7b-9af9-9e079c7b0340","_uuid":"5cd90d11e8fa60c43881dd0b4a05faa0cf090ed9","collapsed":true},"cell_type":"markdown","source":"### REF\nI'll be honest, I don't know exactly what REF represents, but I'm thinking it's likely a code for some other thing. Which makes me think treating it as a label instead of a number is more appropriate.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"bb1ed40b-9d5a-4970-8658-b9d7b29b358b","_uuid":"ad84d8392e011df52a88cb619cb09713cc394132","trusted":false},"cell_type":"code","source":"#using pd.get_dummies to create a one hot encoded matrix\ndummies = pd.get_dummies(X['REF'])\n#Adding the variable to the column names so I can keep track of which original variable it came from\ndummies.columns = ['REF_' + k for k in dummies.columns.values]\n\nX = pd.concat([X, dummies], axis=1)\n\n#dropping the original column \ndel X['REF']\n\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4be24ab7-e92d-4129-a415-94be109e1487","_uuid":"f2fee10034d1eb266948b3c6079cdea37b86fd59"},"cell_type":"markdown","source":"### Review Date\nDates can be tricky. On the one hand, we can bucket them into categories by the distinct year. On the other hand, we can convert it to a number, and treat it as \"time since...\" whatever we transform it as. In this dataset, we'd likely treat 0 as the first year, 1 as the second year, and so on. For now, I'm going to treat the years as buckets. ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"4c8819d5-e818-4aa7-9c3c-383113c49af5","_uuid":"d1eec1060b7d0b91d8121e62d28214a8efdef347","trusted":false},"cell_type":"code","source":"#using pd.get_dummies to create a one hot encoded matrix\ndummies = pd.get_dummies(X['Review\\nDate'])\n#Adding the variable to the column names so I can keep track of which original variable it came from\ndummies.columns = ['Date_' + k for k in dummies.columns.values]\n\nX = pd.concat([X, dummies], axis=1)\n\n#dropping the original column \ndel X['Review\\nDate']\n\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ceb06126-3c06-4f9a-b2bb-2c0e9985597c","_uuid":"147fd9869cd012ab015f5fabaeac7e9b561104f7","collapsed":true},"cell_type":"markdown","source":"## Splitting into Train and Test\nOne hot encoding is simple since it only looks at the row to change the representation, so it doesn't matter when you apply it. Other preprocessing looks at *the whole dataset* in order to determine how to change the data representation. Like for instance imputing a mean, you have to look at the whole column to figure out what the mean is. And if you're also using the test data to figure out how to change the data before you model with it, that is sort of cheating.\n\nTo avoid that, I need to split it into a training and test set, then fit transformations to the training data and apply the transformation to both data sets.  ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"99cfda32-9256-47ae-8db1-b7cbbf5d2247","_uuid":"2ca329164e58cf77bf20a73967858b0f0aa098ac","trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=7)\n\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"eb78072f-3b1e-4fe1-a816-97849b2024cc","_uuid":"c05efe45ede79a25e337958a5046be7dcc3078cf"},"cell_type":"markdown","source":"## Text Variables\n\n### Specific Bean Origin or Bar Name\n\nFor this column, I want to use key words, since I think it could be useful to pick up on several separate words in the text. Given the value \"Djakarta, Java and Ghana\", I'd want to consider it similar to other bars from Java, and separately be able to consider it similar to other bars from Ghana. For this reason, I'll text vectorize it. ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"084e6852-f7c9-4483-92a7-764b8b916eac","_uuid":"391014f8101b1109f4211b9716cb8d21da5558a3","trusted":false},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntf = TfidfVectorizer(ngram_range=(1,2))\n\n#fitting to the training data\ntf.fit(X_train['Specific Bean Origin\\nor Bar Name'])\n\n#transforming on both data set\ntrain_transformed = tf.transform(X_train['Specific Bean Origin\\nor Bar Name'])\ntest_transformed = tf.transform(X_test['Specific Bean Origin\\nor Bar Name'])\n\n#converting to a dataframe so we can see the column names easier later\ntrain_transformed = pd.DataFrame(data = train_transformed.todense(), \n                              index = X_train.index.values, \n                              columns = [\"Bar_Name_\" + k for k in tf.vocabulary_] )\ntest_transformed = pd.DataFrame(data = test_transformed.todense(), \n                              index = X_test.index.values, \n                              columns = [\"Bar_Name_\" + k for k in tf.vocabulary_] )\n\n#appending back to the original data\nX_train = pd.concat([X_train, train_transformed], axis=1)\nX_test = pd.concat([X_test, test_transformed], axis=1)\n\ndel X_train['Specific Bean Origin\\nor Bar Name']\ndel X_test['Specific Bean Origin\\nor Bar Name']\n\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"81a00d86-fb88-4404-b7eb-e83686932b54","_uuid":"69584f2ae05bebeaf3c92da3ccef6c63066fbd5d"},"cell_type":"markdown","source":"### Bean Type\nSimilarly, I want to detect key words from the bean type as well.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"5b4fe86b-a482-4c72-9d63-0961faf4b8ee","_uuid":"f06bd2c9bcaaac3e710ee37751bd562854c14341","trusted":false,"collapsed":true},"cell_type":"code","source":"tf = TfidfVectorizer(ngram_range=(1,2))\n\ntf.fit(X_train['Bean\\nType'])\n\ntrain_bean_name = tf.transform(X_train['Bean\\nType'])\ntest_bean_name = tf.transform(X_test['Bean\\nType'])\n\n#transforming on both data set\ntrain_transformed = tf.transform(X_train['Bean\\nType'])\ntest_transformed = tf.transform(X_test['Bean\\nType'])\n\n#converting to a dataframe so we can see the column names easier later\ntrain_transformed = pd.DataFrame(data = train_transformed.todense(), \n                              index = X_train.index.values, \n                              columns = [\"Bean_Type_\" + k for k in tf.vocabulary_] )\ntest_transformed = pd.DataFrame(data = test_transformed.todense(), \n                              index = X_test.index.values, \n                              columns = [\"Bean_Type_\" + k for k in tf.vocabulary_] )\n\n#appending back to the original data\nX_train = pd.concat([X_train, train_transformed], axis=1)\nX_test = pd.concat([X_test, test_transformed], axis=1)\n\ndel X_train['Bean\\nType']\ndel X_test['Bean\\nType']\n\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"62fe52f7-9b5b-4d91-a8ec-49c84fc831bb","_uuid":"ee6a39ce4dd024179cf3447ae44aef04e301cbc5","collapsed":true},"cell_type":"markdown","source":"### Broad Bean Origin\n\nLastly, I want to get keywords for the bean origin as well.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"618a8d1b-8709-4141-8ae1-0177ff95cbb1","_uuid":"d45213b969a1a3c1342ad9e6f6a8ef1b130e64a5","trusted":false,"collapsed":true},"cell_type":"code","source":"tf = TfidfVectorizer(\n    ngram_range=(1,2))\n\ntf.fit(X_train['Broad Bean\\nOrigin'])\n\n#transforming on both data set\ntrain_transformed = tf.transform(X_train['Broad Bean\\nOrigin'])\ntest_transformed = tf.transform(X_test['Broad Bean\\nOrigin'])\n\n#converting to a dataframe so we can see the column names easier later\ntrain_transformed = pd.DataFrame(data = train_transformed.todense(), \n                              index = X_train.index.values, \n                              columns = [\"Bean_Origin_\" + k for k in tf.vocabulary_] )\ntest_transformed = pd.DataFrame(data = test_transformed.todense(), \n                              index = X_test.index.values, \n                              columns = [\"Bean_Origin_\" + k for k in tf.vocabulary_] )\n\n#appending back to the original data\nX_train = pd.concat([X_train, train_transformed], axis=1)\nX_test = pd.concat([X_test, test_transformed], axis=1)\n\ndel X_train['Broad Bean\\nOrigin']\ndel X_test['Broad Bean\\nOrigin']\n\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7d9ca70e-e1d8-4d76-9b17-6b000431fb46","_uuid":"a7867337c1ddf11822667551fd41bfcfdffe7889"},"cell_type":"markdown","source":"Now for the last variable.\n\nI'm going to scale down the Cocoa Percent, since right now the values are so much bigger than any other column, and we don't want it to be unfairly influencing the model.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"6c697070-fc7a-4f3a-abb2-31da5e30085c","_uuid":"1bed59440129eae3208ce79da40198e293681ede","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nsc.fit(X_train['Cocoa\\nPercent'].values.reshape(-1, 1))\n\nX_train['Cocoa\\nPercent'] = sc.transform(X_train['Cocoa\\nPercent'].values.reshape(-1, 1))\nX_test['Cocoa\\nPercent'] = sc.transform(X_test['Cocoa\\nPercent'].values.reshape(-1, 1))\n\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"04dea0ca-fed9-4a07-9cbc-c2d097d79d20","_uuid":"bebaf52d05b71a2b8c826012d8ad21434904982f"},"cell_type":"markdown","source":"#  Modeling\n\nNow we get to the main goal, inference. There's a vast array of models you can choose from, but I'm going to stick to ones that allow for feature relationships to be easily inferred. And to increase the chances of finding interesting things, I'll make a model from two different families, one tree based model, and one linear model.\n\n## Random Forests\nI like Random Forests because it's easy to apply to both regression and classification. It also has a built in feature importance ranking, which we can use to infer which variables are correlated most strongly to the target variable.\n\nUnfortunately, we can't infer the ***direction*** of the relationship, but we can at least be certain that there is a relationship going on. So let's see what we find...\n\nTo build it, I'm going to use grid search to try out a couple different parameters and pick the model with the best cross validation code. The actual code to do so is quite straightforward:","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"12522559-9407-4806-9d1a-95faf78cea45","_uuid":"884f07bdd59134f9b492073ea0a9cb8374733fb0","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\n\n#parameter combinations to try\nparam_grid = {'n_estimators': [10, 30, 50, 90], \n              'max_depth': [5, 10, 20, None]\n             }\n\nregr = RandomForestRegressor()\n\n#fitting the model to each combination in the grid\nmodel = GridSearchCV(regr, param_grid)\n#fining the best parameters based on the search grid\nmodel.fit(np.matrix(X_train), y_train)\n\n#pulling the fitted model on the best settings so we can see the variable importances\nregr = model.best_estimator_\n\nprint(model.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b8217bd6-2caf-45c0-a254-07df6cb217dd","_uuid":"7e12532372b383b27c6e5d4c96653eb48362681b"},"cell_type":"markdown","source":"The score isn't too bad, an average error of about 0.17 per rating. Given the scale, that's a reasonable model. If it was really bad, the inference we might make from the model might not actually represent the true relationships and I would say we should throw the model away, but I feel comfortable using it to learn inferences. \n\nNow, let's get to what we're really interested in, the feature rankings. ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"39ffbead-ea59-4b42-98bc-be60480f33b9","_uuid":"99a126ac2c463877ad4e2b549e82852c60dee920","trusted":false,"collapsed":true},"cell_type":"code","source":"#finding the indices that would sort the array \nsorted_indices = np.argsort(regr.feature_importances_)\n\n#finding the most important features and associated importance\nvariables = regr.feature_importances_[sorted_indices]\nimportance_rating = X_train.columns.values[sorted_indices]\n\nimportances = pd.DataFrame({'variable':variables, 'importance':importance_rating})\nimportances.tail(10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ceafaece-4b05-4154-bf23-bcf8745268fe","_uuid":"629049c4b9ac062bc5dc7f89c04542b70696e2c6"},"cell_type":"markdown","source":"Interesting! The Cocoa Percentage is the strongest factor going into a ranking. People must really have a preference there.  ¯\\_(ツ)_/¯\nWe saw that trend early in the exploratory phase, so it's good validation that the model is doing something right. \n\nAlso, remember that variable importances assign a value for EVERY variable, I just pulled the top ten values for inference. At some point, you have to make a decision at what the cutoff value is for a true \"importance\". In this instance, the cocoa percentage is ***significantly*** farther from the second variable than the second variable is from the third, so I am very comfortable in the \"importance\" of cocoa percent, but for the other ones, I'd say there is a small trend.\n\nLooking into the honduras trend a little more, there doesn't seem to be anything super distinguishing there. So I would be less confident in the strength of the relationship for the other variables going forward.","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"27060e2171d12e898d569cb644a3384b5a8da4a3"},"cell_type":"code","source":"mask = df['Specific Bean Origin\\nor Bar Name'].apply(lambda x: 'honduras' in str(x).lower())\n\nprint(\"Number of bars for Honduras: \" + str(df[mask].shape[0]))\ndf[mask]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7d6ce69f39dabffa546cdc97ad55de07b660473"},"cell_type":"markdown","source":"Looking into the Soma trend, however, gives me a little more confidence. Not only does it have a lot of bars in the category, but they also score relatively higher! [Looking at their website](https://www.somachocolate.com/), they are quite fancy, so I guess they're worth a try if you're in Toronto!","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"322b0b06-4475-4d94-a455-1869c158dcdb","_uuid":"c1df0a144e6aa204fa8512eac5086220877eaa29","trusted":false,"collapsed":true},"cell_type":"code","source":"print(\"Number of bars for Soma: \" + str(df[df['Company\\xa0\\n(Maker-if known)'] == 'Soma'].shape[0]))\nprint(\"Average Soma bar rating: \" + str(df[df['Company\\xa0\\n(Maker-if known)'] == 'Soma']['Rating'].mean()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5300a8719d185a6de574b6a991f4c25c7eb01bbb"},"cell_type":"markdown","source":"On the other hand, looking at REF 887, there are actually only two bars in the category, and together they didn't do well. With such a small sample, it's hard to tell if that's really a negative correlation, or if getting more samples would change. So I wouldn't call this one a true trend just yet.","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"4f800730485982fcdfd07b146726bc71f45432b7"},"cell_type":"code","source":"print(\"Number of bars for 887: \" + str(df[df['REF'] == '887'].shape[0]))\nprint(\"Average bar rating: \" + str(df[df['REF'] == '887']['Rating'].mean()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7755d82914aa96ddf8e3d5e26a563fa3cb59aafd"},"cell_type":"markdown","source":"Similarly with the words Del Toro, not a whole lot there. So, it's probably going to be diminishing returns going down the list. Let's move on to another model.","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"47332bc92afc81e608cb34ee5c79e56dbd066c46"},"cell_type":"code","source":"mask = df['Specific Bean Origin\\nor Bar Name'].apply(lambda x: 'del toro' in str(x).lower())\n\nprint(\"Number of bars for del toro: \" + str(df[mask].shape[0]))\ndf[mask]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bd0168ba-e1db-4384-945d-009fa21989b5","_uuid":"f66536597eaea23db7d2382c22b3f6a004862cf0"},"cell_type":"markdown","source":"----------------------------------------\n## Ridge Regression\n\nFor another inference approach, we can generate a linear model to infer variable relationships. In this model, the coefficient of the model can be interpreted as the strength of the variable in relationship to the target variable, and the sign of the coefficient tells us the direction of the relationship. \n\nIn that regard, it's a little more powerful than a tree based model, but because they are from different families, both models have the ability to pick up on different nuances, so always try more than one!","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"f756cfdd-2880-4975-9c32-13cdf3c1e20d","_uuid":"0eff410e9b75fa3e85ead1c9195e3c348e66ec1b","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\n\nparam_grid = {'alpha': [0.001, 0.01, 1, 3, 5, 10]\n             }\n\nregr = Ridge()\n\nmodel = GridSearchCV(regr, param_grid)\n#fining the best parameters based on the search grid\nmodel.fit(np.matrix(X_train), y_train)\n\n#pulling the fitted model on the best settings so we can see the variable importances\nregr = model.best_estimator_\n\nprint(model.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ab12fa6b-d1d3-4c3b-89bf-60d7a1ac15fc","_uuid":"3177b645b828b0e5beba9bfc4e28687e01bdafce","trusted":false,"collapsed":true},"cell_type":"code","source":"#finding the indices that would sort the array \nsorted_indices = np.argsort(regr.coef_)\n\n#finding the most important features and associated importance\nvariables = regr.coef_[sorted_indices]\nimportance_rating = X_train.columns.values[sorted_indices]\n\nimportances = pd.DataFrame({'variable':variables, 'coefficient':importance_rating})\nprint(\"Total non zero coefficients: \" + str(len(importances[importances['coefficient'] != 0.0])))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0eeffbf385e769c5391816571617a2e1d9b0f5d"},"cell_type":"markdown","source":"That's a lot of variables! Again, the smaller coefficients might not be as strong of a relationship, so it's best to stick with the largest coefficients overall. ","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"44aa0aeffed82571fbd50e10acf128cf799b2a65"},"cell_type":"code","source":"importances.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5771bb9e035623ed68ff6db8a86e9306c251218a"},"cell_type":"markdown","source":"For negative correlations, we again see the REF 887, honduras, and del toro. But we already know it was a smaller sample. But what about Callebaut? It's a small sample as well. [Their website](https://www.callebaut.com/en-US/homepage) looks so tempting... Poor Callebaut. ","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ede8e27722379d5d853881d88ed0bbbd63a37336"},"cell_type":"code","source":"print(\"Number of bars for Callebaut: \" + str(df[df['Company\\xa0\\n(Maker-if known)'] == 'Callebaut'].shape[0]))\ndf[df['Company\\xa0\\n(Maker-if known)'] == 'Callebaut']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fada5eb81e8f3ff59986a7103aa51facb436e29b"},"cell_type":"markdown","source":"Now for the flip side, let's look at positive trends! ","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"2fec53ee643f30b5193b37f1dfe253e327229b05"},"cell_type":"code","source":"importances.tail()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4bd2068b9a748101af649536a80d31e9f8f76b0a"},"cell_type":"markdown","source":"[Amedei](http://www.amedei.it/en/) is looking good. They have a larger sample, and had several 4s.","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"9aacf8aa6c3b150ff49a29644698b2af46d644eb"},"cell_type":"code","source":"print(\"Number of bars for Amedei: \" + str(df[df['Company\\xa0\\n(Maker-if known)'] == 'Amedei'].shape[0]))\ndf[df['Company\\xa0\\n(Maker-if known)'] == 'Amedei']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad3b8c96b44b687dde767411b8ac8019ec1b7cd8"},"cell_type":"markdown","source":"111 is also consistently high. Still a smallish sample, but promising.","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"7602679a4a8b82c6dd62179a554349d5dcd50d38"},"cell_type":"code","source":"print(\"Number of bars for 111: \" + str(df[df['REF'] == '111'].shape[0]))\ndf[df['REF'] == '111']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac4ca76225014b4dcb3488e84c1a17ce81fb208a"},"cell_type":"markdown","source":"[Patric](http://patric-chocolate.com/) also had some 4s in their history. ","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"a4ce4372fd1ae26877cdcb354c3a5b24fca5cda9"},"cell_type":"code","source":"print(\"Number of bars for Patric: \" + str(df[df['Company\\xa0\\n(Maker-if known)'] == 'Patric'].shape[0]))\ndf[df['Company\\xa0\\n(Maker-if known)'] == 'Patric']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"694ecaf8e552ec31ede84d180ef82f6bdba62a9c"},"cell_type":"markdown","source":"[Cacao Sampaka](http://www.cacaosampaka.com/) is also doing well. They have a larger sample, and had several 4s.","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"64ef01672df556320c1a735010d053652dd473d6"},"cell_type":"code","source":"print(\"Number of bars for Cacao Sampaka: \" + str(df[df['Company\\xa0\\n(Maker-if known)'] == 'Cacao Sampaka'].shape[0]))\ndf[df['Company\\xa0\\n(Maker-if known)'] == 'Cacao Sampaka']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"00c0bb90-b47d-46a5-8e63-8ada385a2cb8","_uuid":"f5a895bf0e0e171c22ec1025999922648e6f7a54","collapsed":true},"cell_type":"markdown","source":"We can continue on, but I think you get the idea.\n\n# Wrapping Up\nOverall, this was an interesting exploration into the world of chocolates. Surprisingly, the most signficant trends the models were able to pick up were the companies themselves more than any other factor. We haven't discovered any groundbreaking discoveries in chocolate, but at least I've found a few new brands to add to my list of desserts to try!\n\nHave you found any interesting trends?","outputs":[],"execution_count":null}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}