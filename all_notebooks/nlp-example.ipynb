{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\n\nnp.random.seed(1337)\n\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Dense, LSTM, BatchNormalization\nfrom keras.layers import SpatialDropout1D\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chatbot = pd.read_csv(\"../input/deepnlp/Sheet_1.csv\",usecols=['response_id','class','response_text'],encoding='latin-1')\nresume = pd.read_csv(\"../input/deepnlp/Sheet_2.csv\",encoding='latin-1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chatbot.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resume.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_vect = CountVectorizer()\n\n\nx = chatbot['response_text']\ny = chatbot['class']\nx_train,x_test,y_train,y_test = train_test_split(x,y,random_state=1)\nX_train_counts = count_vect.fit_transform(x_train)\nX_test_counts = count_vect.transform(x_test)\ntfidf_transformer = TfidfTransformer()\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\nX_test_tfidf = tfidf_transformer.fit_transform(X_test_counts)\n\nprint(X_train_tfidf.shape)\nprint(X_test_tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.feature_extraction.text import TfidfTransformer\n# tfidf_transformer = TfidfTransformer()\n# X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n# X_train_tfidf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Naive bayesian"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nnaive = MultinomialNB().fit(X_train_tfidf, y_train)\npredicted = naive.predict(X_test_tfidf)\nnp.mean(predicted == y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SVM\nLet's write less code"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n\nsvm = Pipeline([('vect', CountVectorizer()),\n\t('tfidf', TfidfTransformer()),\n    ('clf-svm', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter_no_change=5, random_state=42))\n])\nsvm_fit = svm.fit(x_train, y_train)\nsvm_predict = svm.predict(x_test)\nnp.mean(svm_predict == y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"it's better!"},{"metadata":{},"cell_type":"markdown","source":"## LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(resume.shape)\n\nx_resume = resume['resume_text']\ny_resume = resume['class']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The maximum number of words to be used. (most frequent)\nMAX_NB_WORDS = 50000\n# Max number of words in each complaint.\nMAX_SEQUENCE_LENGTH = 300\n# This is fixed.\nEMBEDDING_DIM = 100\n\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(x_resume)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#truncate and pad sequence to have string representation of each size\nX = tokenizer.texts_to_sequences(x_resume)\nX = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', X.shape)\n\n#convert label to categorical values\nY = pd.get_dummies(y_resume).values\nprint('Shape of label tensor:', Y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X,Y, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nepochs = 8\n\nhistory = model.fit(X_train, Y_train, epochs=epochs,validation_split=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accr = model.evaluate(X_test,Y_test)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Loss')\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show();","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}