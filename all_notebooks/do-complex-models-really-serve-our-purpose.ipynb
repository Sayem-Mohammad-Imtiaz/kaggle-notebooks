{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-05T05:07:28.425217Z","iopub.execute_input":"2021-07-05T05:07:28.425869Z","iopub.status.idle":"2021-07-05T05:07:28.447048Z","shell.execute_reply.started":"2021-07-05T05:07:28.42574Z","shell.execute_reply":"2021-07-05T05:07:28.445888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# OBJECTIVE : \n\n[1. Check usability of Complex Models in a Production scenario.](https://www.kaggle.com/nishantrock/do-complex-models-really-serve-our-purpose/edit/run/66865909)\n\n[2. Check the effect of different metrics while refitting a model.](https://www.kaggle.com/nishantrock/do-complex-models-really-serve-our-purpose/edit/run/66865909)\n   ( Using Mathew's corelation coefficient and F1 score, to balance out TN and FP )\n   \n[3. Using Transformations to tweak individual columns and use them with complex models.](https://www.kaggle.com/nishantrock/do-complex-models-really-serve-our-purpose/edit/run/66865909)","metadata":{}},{"cell_type":"markdown","source":"### Few Observations : ","metadata":{}},{"cell_type":"markdown","source":"- I tried both Linear and Complex models. \nLinear models gave me a balanced score with reference to Confusion Matrix. \nComplex Models such as XGBoost and Random Forest , ensemble of simple models gave me a much skewed Prediction. \nEither the FP or TN prediction was increased significantly, but my score was considerably improved. \n\n- I tried refitting the models with respect to the metrics such as mathew correlation coefficient and f1 score, to balance the FP and TN predictions.\nIt gave me a better prediction but it did not improve my score\n\n- I transformed the individual features via pipelines but still was not able to achieve a good score via Linear Model. \n\nMy objective is to use the Linear Model since the Complex Model give me a skewed predictions. \n","metadata":{}},{"cell_type":"markdown","source":"# IMPORTANT THOUGHT : \n\nMetrics are for Humans to interpret the model. \nLoss functions are for Computers / Model to interpret how they are doing. \nTry factoring that in. ","metadata":{}},{"cell_type":"code","source":"pip install --upgrade scikit-learn\n","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:07:28.448811Z","iopub.execute_input":"2021-07-05T05:07:28.449106Z","iopub.status.idle":"2021-07-05T05:07:45.409737Z","shell.execute_reply.started":"2021-07-05T05:07:28.449076Z","shell.execute_reply":"2021-07-05T05:07:45.408189Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install sklego","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:07:45.411666Z","iopub.execute_input":"2021-07-05T05:07:45.41198Z","iopub.status.idle":"2021-07-05T05:07:53.76738Z","shell.execute_reply.started":"2021-07-05T05:07:45.411945Z","shell.execute_reply":"2021-07-05T05:07:53.766348Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import plot_confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:11:36.693655Z","iopub.execute_input":"2021-07-05T05:11:36.694231Z","iopub.status.idle":"2021-07-05T05:11:37.692727Z","shell.execute_reply.started":"2021-07-05T05:11:36.694179Z","shell.execute_reply":"2021-07-05T05:11:37.691599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/jobathon-may-2021-credit-card-lead-prediction/train.csv')\ntest = pd.read_csv('/kaggle/input/jobathon-may-2021-credit-card-lead-prediction/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:11:37.694318Z","iopub.execute_input":"2021-07-05T05:11:37.69465Z","iopub.status.idle":"2021-07-05T05:11:38.505794Z","shell.execute_reply.started":"2021-07-05T05:11:37.69461Z","shell.execute_reply":"2021-07-05T05:11:38.504697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns, test.columns","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:11:38.507898Z","iopub.execute_input":"2021-07-05T05:11:38.508283Z","iopub.status.idle":"2021-07-05T05:11:38.518325Z","shell.execute_reply.started":"2021-07-05T05:11:38.508246Z","shell.execute_reply":"2021-07-05T05:11:38.517216Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.nunique()","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:11:39.084295Z","iopub.execute_input":"2021-07-05T05:11:39.084717Z","iopub.status.idle":"2021-07-05T05:11:39.478768Z","shell.execute_reply.started":"2021-07-05T05:11:39.084679Z","shell.execute_reply":"2021-07-05T05:11:39.477712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas_profiling as pp\n\nprofile = pp.ProfileReport(train)\n# profile.to_file(\"Train.html\")\nprofile","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:11:39.903296Z","iopub.execute_input":"2021-07-05T05:11:39.903702Z","iopub.status.idle":"2021-07-05T05:12:18.35364Z","shell.execute_reply.started":"2021-07-05T05:11:39.903664Z","shell.execute_reply":"2021-07-05T05:12:18.352593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Let's check what are the corelated columns when our final output is '1' i.e. it predicts a Lead.","metadata":{}},{"cell_type":"markdown","source":"Also label encoding it , so that we can check with all the columns. ","metadata":{}},{"cell_type":"code","source":"train_1 = train.loc[train['Is_Lead'] == 1]\n\nfrom sklearn import preprocessing \n\nle = preprocessing.LabelEncoder()\n\nfor col in train.drop('ID', axis = 1).select_dtypes('object').columns:\n    le = preprocessing.LabelEncoder()\n    train_1[col] = le.fit_transform(train_1[col])\n    \n\nimport pandas_profiling as pp\n\nprofile_1 = pp.ProfileReport(train_1, title = \"Profiling the Is_Lead = 1 segment\", explorative = True)\n#profile_1 = pp.ProfileReport(interactions = {interactions.targets : train_1['Is_Lead']})\nprofile_1","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:14:53.179046Z","iopub.execute_input":"2021-07-05T05:14:53.179449Z","iopub.status.idle":"2021-07-05T05:15:15.293695Z","shell.execute_reply.started":"2021-07-05T05:14:53.179406Z","shell.execute_reply":"2021-07-05T05:15:15.29257Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4 columns have high Correlation with the target column when \" WE HAVE A LEAD \": \n1. Credit_Product\n2. Gender\n3. Channel_Code\n4. Is_Active\n5. Occupation","metadata":{}},{"cell_type":"code","source":"train['Is_Lead'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:15:15.29498Z","iopub.execute_input":"2021-07-05T05:15:15.295435Z","iopub.status.idle":"2021-07-05T05:15:15.305474Z","shell.execute_reply.started":"2021-07-05T05:15:15.2954Z","shell.execute_reply":"2021-07-05T05:15:15.30445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have a skewed target ","metadata":{}},{"cell_type":"code","source":"class_weight = int(train['Is_Lead'].value_counts()[0] / train['Is_Lead'].value_counts()[1])","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:15:42.28813Z","iopub.execute_input":"2021-07-05T05:15:42.288684Z","iopub.status.idle":"2021-07-05T05:15:42.298221Z","shell.execute_reply.started":"2021-07-05T05:15:42.288642Z","shell.execute_reply":"2021-07-05T05:15:42.297289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_weight","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:15:43.405402Z","iopub.execute_input":"2021-07-05T05:15:43.405988Z","iopub.status.idle":"2021-07-05T05:15:43.411001Z","shell.execute_reply.started":"2021-07-05T05:15:43.405936Z","shell.execute_reply":"2021-07-05T05:15:43.409986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Imbalance between the 2 target Classes . 0:1 Equivalent to 3:1","metadata":{}},{"cell_type":"code","source":"# Save the initial state of dataframe\ntrain_df = train.copy(deep = True)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:15:55.244117Z","iopub.execute_input":"2021-07-05T05:15:55.244635Z","iopub.status.idle":"2021-07-05T05:15:55.264848Z","shell.execute_reply.started":"2021-07-05T05:15:55.244596Z","shell.execute_reply":"2021-07-05T05:15:55.263619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Label Encoding both Train and Test Dataset\n\nfrom sklearn import preprocessing \n\nle = preprocessing.LabelEncoder()\n\ncat_columns = []\n\nfor col in train.drop('ID', axis = 1).select_dtypes('object').columns:\n    print('Train:',col)\n    cat_columns.append(col)\n    le = preprocessing.LabelEncoder()\n    train[col] = le.fit_transform(train[col])\n    test[col] = le.transform(test[col])\n    \ncat_features_index = [i for i, col in enumerate(train.columns) if col in cat_columns]\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:16:05.104995Z","iopub.execute_input":"2021-07-05T05:16:05.105393Z","iopub.status.idle":"2021-07-05T05:16:05.951494Z","shell.execute_reply.started":"2021-07-05T05:16:05.105358Z","shell.execute_reply":"2021-07-05T05:16:05.950325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head().style.background_gradient(cmap = \"Blues\")","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:16:08.546224Z","iopub.execute_input":"2021-07-05T05:16:08.546699Z","iopub.status.idle":"2021-07-05T05:16:08.582849Z","shell.execute_reply.started":"2021-07-05T05:16:08.546651Z","shell.execute_reply":"2021-07-05T05:16:08.581595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn import linear_model \n\n\nX = train.drop(['Is_Lead', 'ID'], axis = 1)\ny = train['Is_Lead']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42, stratify = y)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:16:23.700127Z","iopub.execute_input":"2021-07-05T05:16:23.700511Z","iopub.status.idle":"2021-07-05T05:16:23.951012Z","shell.execute_reply.started":"2021-07-05T05:16:23.700458Z","shell.execute_reply":"2021-07-05T05:16:23.949785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:16:28.517178Z","iopub.execute_input":"2021-07-05T05:16:28.517547Z","iopub.status.idle":"2021-07-05T05:16:28.5242Z","shell.execute_reply.started":"2021-07-05T05:16:28.517513Z","shell.execute_reply":"2021-07-05T05:16:28.523074Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## I'll be using Patsy later to generate Feature Interactions between correlated columns","metadata":{}},{"cell_type":"code","source":"from sklego.preprocessing import PatsyTransformer\n\npt = PatsyTransformer(\"(C(Credit_Product)+Gender+Channel_Code+Is_Active+Occupation)**2-1-(C(Credit_Product)+Gender+Channel_Code+Is_Active+Occupation) + Age + Region_Code + Vintage + np.log(Avg_Account_Balance )\")\npt.fit(X_train, y_train).transform(X_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:16:55.109982Z","iopub.execute_input":"2021-07-05T05:16:55.110335Z","iopub.status.idle":"2021-07-05T05:16:57.129926Z","shell.execute_reply.started":"2021-07-05T05:16:55.110304Z","shell.execute_reply":"2021-07-05T05:16:57.128875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from functools import wraps\nimport datetime as dt\n\ndef log_step(func):\n    \n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        \n        tic = dt.datetime.now()\n        result = func(*args, **kwargs)\n        time_taken = str(dt.datetime.now() - tic)\n        print(f\"just ran step {func.__name__} shape = {result.shape} took {time_taken}s\")\n        return result\n    return wrapper\n\n@log_step\ndef start_pipeline(dataf):\n    return dataf.copy() \n\n@log_step\ndef corelation_target(data, target):\n    \n    \"\"\"\n    Find Co-relation of different features with the \"Target\" column in Descending Order\n    \"\"\"\n    plt.figure(figsize = (8, 12))\n\n    heatmap = sns.heatmap(data.corr()[[target]].drop(index = target, axis = 0).sort_values(by = target, ascending = False),\n                         vmin = -1,\n                         vmax = 1, \n                         annot = True, \n                         cmap = 'BrBG')\n\n    heatmap.set_title(f\"Features Correlating with {target} column\", \n                      fontdict = {'fontsize':18}, pad = 16)\n    \n    return data\n\n\n@log_step\ndef corelation_horizontal_target(data, target):\n    \n    \"\"\"\n    Horizontal Bar Plot of the Co-relation of individual features with the Target Column \n    \"\"\"\n    plt.figure(figsize=(10, 12))\n\n    corr = data.corr()[[target]].drop(index = target, axis = 0) # Removes the 1st row i.e. Corelation of target with itself\n    plt.barh(corr.index, corr.reset_index(drop = True).to_numpy().ravel())\n    plt.title(\"Corelation with target\")\n    plt.figure(figsize=(12, 22))\n    plt.show()\n    \n    return data\n\n\n\n","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_d = (train\n           .pipe(start_pipeline)\n           .pipe(corelation_target, target = 'Is_Lead')\n           .pipe(corelation_horizontal_target, target = 'Is_Lead')\n          )","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generating Features using SKLEGO Patsy Transformer . \n\n#### The features that are generated will be using the 4 features that affect the 'Is_Lead = 1' as checked from Pandas Profiling report\n\n1. Credit_Product\n2. Gender\n3. Channel_Code\n4. Is_Active\n5. Occupation","metadata":{}},{"cell_type":"markdown","source":"# Using Basic and Complex Models","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn import linear_model \n\nX = train.drop(['Is_Lead', 'ID'], axis = 1)\ny = train['Is_Lead']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42, stratify = y)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:17:20.712893Z","iopub.execute_input":"2021-07-05T05:17:20.713352Z","iopub.status.idle":"2021-07-05T05:17:20.914737Z","shell.execute_reply.started":"2021-07-05T05:17:20.713308Z","shell.execute_reply":"2021-07-05T05:17:20.913499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_lr = linear_model.LogisticRegression(solver = 'liblinear',class_weight = {0:1, 1: class_weight}).fit(X_train, y_train)\nroc_auc_score(model_lr.fit(X_train, y_train).predict(X_test), y_test)\nplot_confusion_matrix(model_lr, X_test, y_test,\n                     display_labels = np.unique(y),\n                     cmap = plt.cm.Blues,\n                     normalize = 'true')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:17:26.964321Z","iopub.execute_input":"2021-07-05T05:17:26.964877Z","iopub.status.idle":"2021-07-05T05:17:31.078523Z","shell.execute_reply.started":"2021-07-05T05:17:26.964842Z","shell.execute_reply":"2021-07-05T05:17:31.077566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import linear_model\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklego.preprocessing import PatsyTransformer\n\npipe_lr = Pipeline([\n    (\"patsy\", PatsyTransformer(\"(C(Credit_Product)+Gender+Channel_Code+Is_Active+Occupation)**2-1-(C(Credit_Product)+Gender+Channel_Code+Is_Active+Occupation) + Age + Region_Code + Vintage + np.log(Avg_Account_Balance )\")),\n    (\"scale\", StandardScaler()),\n    (\"model\", linear_model.LogisticRegression(class_weight = {0:1, 1: class_weight})\n    )\n])\n\nroc_auc_score(pipe_lr.fit(X_train, y_train).predict(X_test), y_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:18:02.266092Z","iopub.execute_input":"2021-07-05T05:18:02.266451Z","iopub.status.idle":"2021-07-05T05:18:06.212154Z","shell.execute_reply.started":"2021-07-05T05:18:02.266419Z","shell.execute_reply":"2021-07-05T05:18:06.210888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(pipe_lr, X_test, y_test,\n                     display_labels = np.unique(y),\n                     cmap = plt.cm.Blues,\n                     normalize = 'true')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:18:06.214163Z","iopub.execute_input":"2021-07-05T05:18:06.2147Z","iopub.status.idle":"2021-07-05T05:18:06.959155Z","shell.execute_reply.started":"2021-07-05T05:18:06.214643Z","shell.execute_reply":"2021-07-05T05:18:06.958223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As per confusion matrix, on comparing with the other algorithms , I am getting the least number of NT (False Positives) and the maximum number of TN (True Negatives) that are classified incorrectly.\n\nPositive is classified incorrectly, while the negative is somewhat better classified as negative. ","metadata":{}},{"cell_type":"code","source":"from sklego.mixture import GMMClassifier\n\npipe_GM = Pipeline([\n    (\"patsy\", PatsyTransformer(\"(C(Credit_Product)+Gender+Channel_Code+Is_Active+Occupation+np.log(Avg_Account_Balance))**2\")),\n    (\"scale\", StandardScaler()),\n    (\"model\", GMMClassifier(n_components = 4)\n    )\n])\n\npred = pipe_GM.fit(X_train, y_train).predict(X_test)\n\nroc_auc_score(pred, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:18:14.0106Z","iopub.execute_input":"2021-07-05T05:18:14.010986Z","iopub.status.idle":"2021-07-05T05:18:22.216022Z","shell.execute_reply.started":"2021-07-05T05:18:14.010953Z","shell.execute_reply":"2021-07-05T05:18:22.214375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(pipe_GM, X_test, y_test,\n                     display_labels = np.unique(y),\n                     cmap = plt.cm.Blues,\n                     normalize = 'true')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:18:22.223386Z","iopub.execute_input":"2021-07-05T05:18:22.223946Z","iopub.status.idle":"2021-07-05T05:18:23.233549Z","shell.execute_reply.started":"2021-07-05T05:18:22.223894Z","shell.execute_reply":"2021-07-05T05:18:23.232253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Interesting. \n\n### Gausian Mixture Model ( SKLEGO ) gives a good extra bump for the score, but my Confusion Matrix is Skewed.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier\n\nestimators = [\n             ('Logistic Regression', linear_model.LogisticRegression(class_weight = {0:1, 1: class_weight}, solver = 'liblinear')),\n             ('GMM', GMMClassifier(n_components = 4))\n             ]\n\nmodel_stack = StackingClassifier(\n                            estimators = estimators, \n                            final_estimator = linear_model.SGDClassifier()\n                        )\npred = model_stack.fit(X_train, y_train).predict(X_test)\n\nroc_auc_score(pred, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:19:00.012204Z","iopub.execute_input":"2021-07-05T05:19:00.012594Z","iopub.status.idle":"2021-07-05T05:20:19.154997Z","shell.execute_reply.started":"2021-07-05T05:19:00.012557Z","shell.execute_reply":"2021-07-05T05:20:19.153898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(model_stack, X_test, y_test,\n                     display_labels = np.unique(y),\n                     cmap = plt.cm.Blues,\n                     normalize = 'true')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:20:19.156753Z","iopub.execute_input":"2021-07-05T05:20:19.157359Z","iopub.status.idle":"2021-07-05T05:20:19.685827Z","shell.execute_reply.started":"2021-07-05T05:20:19.157309Z","shell.execute_reply":"2021-07-05T05:20:19.68486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Predictions have improved a bit , and my roc_auc score has reduced","metadata":{}},{"cell_type":"code","source":"from sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn import ensemble\n\nmodel_h = ensemble.HistGradientBoostingClassifier(\n                                     scoring = 'roc_auc',\n                                     warm_start = True,\n                                     ).fit(X_train, y_train)\n\npred = model_h.predict(X_test)\n\nroc_auc_score(pred, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:21:09.78504Z","iopub.execute_input":"2021-07-05T05:21:09.785459Z","iopub.status.idle":"2021-07-05T05:21:12.431372Z","shell.execute_reply.started":"2021-07-05T05:21:09.785421Z","shell.execute_reply":"2021-07-05T05:21:12.430414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(model_h, X_test, y_test,\n                     display_labels = np.unique(y),\n                     cmap = plt.cm.Blues,\n                     normalize = 'true')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:21:12.432726Z","iopub.execute_input":"2021-07-05T05:21:12.433111Z","iopub.status.idle":"2021-07-05T05:21:12.859102Z","shell.execute_reply.started":"2021-07-05T05:21:12.433082Z","shell.execute_reply":"2021-07-05T05:21:12.857974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### A good bump with HistGradientBoostingClassifier on roc_auc score. \n\nHistGradientBoostingClassifier has been built wrt LGBM. \nMy 'roc_auc' score definitely increases but my predictions are more skewed. ","metadata":{}},{"cell_type":"code","source":"model_hs = Pipeline([\n   # (\"patsy\", PatsyTransformer(\"(C(Credit_Product)+Gender+Channel_Code+Is_Active+Occupation+np.log(Avg_Account_Balance))**2\")),\n    (\"scale\", StandardScaler()),\n    (\"model\", ensemble.HistGradientBoostingClassifier(\n                                     scoring = 'roc_auc',\n                                     warm_start = True,\n                                     ).fit(X_train, y_train)\n    )\n]).fit(X_train, y_train)\n\npred = model_hs.predict(X_test)\n\nroc_auc_score(pred, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:22:42.145177Z","iopub.execute_input":"2021-07-05T05:22:42.145599Z","iopub.status.idle":"2021-07-05T05:22:45.522066Z","shell.execute_reply.started":"2021-07-05T05:22:42.145558Z","shell.execute_reply":"2021-07-05T05:22:45.520979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(model_hs, X_test, y_test,\n                     display_labels = np.unique(y),\n                     cmap = plt.cm.Blues,\n                     normalize = 'true')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:22:45.523534Z","iopub.execute_input":"2021-07-05T05:22:45.523841Z","iopub.status.idle":"2021-07-05T05:22:45.947883Z","shell.execute_reply.started":"2021-07-05T05:22:45.523811Z","shell.execute_reply":"2021-07-05T05:22:45.946776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using Patsy roc_auc is 0.5 that is 50 %. \nWithout Patsy it still gives a good score. \n\n\nBut my predictions are still skewed.","metadata":{}},{"cell_type":"code","source":"from sklearn import neighbors\npipe_sgd = Pipeline([\n    (\"patsy\", PatsyTransformer(\"(C(Credit_Product)+Gender+Channel_Code+Is_Active+Occupation)**2\")),\n    (\"scale\", StandardScaler()),\n    (\"model\", linear_model.SGDClassifier(class_weight = {0:1, 1: class_weight})\n    )\n])\n\nroc_auc_score(pipe_sgd.fit(X_train, y_train).predict(X_test), y_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:23:18.047196Z","iopub.execute_input":"2021-07-05T05:23:18.047578Z","iopub.status.idle":"2021-07-05T05:23:21.675888Z","shell.execute_reply.started":"2021-07-05T05:23:18.047546Z","shell.execute_reply":"2021-07-05T05:23:21.674699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(pipe_sgd, X_test, y_test,\n                     display_labels = np.unique(y),\n                     cmap = plt.cm.Blues,\n                     normalize = 'true')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:23:21.677731Z","iopub.execute_input":"2021-07-05T05:23:21.678382Z","iopub.status.idle":"2021-07-05T05:23:22.412753Z","shell.execute_reply.started":"2021-07-05T05:23:21.678333Z","shell.execute_reply":"2021-07-05T05:23:22.411727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"They give a much better outcome as per Confusion Matrix , but lesser score as per ROC_AUC","metadata":{}},{"cell_type":"markdown","source":"## Random Forest Without Class-weight","metadata":{}},{"cell_type":"code","source":"from sklearn import ensemble\n\nclf_rf = ensemble.RandomForestClassifier(n_estimators = 300)\n\nclf_rf.fit(X_train, y_train)\npred = clf_rf.predict(X_test)\n\nroc_auc_score(pred, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:27:52.210769Z","iopub.execute_input":"2021-07-05T05:27:52.211131Z","iopub.status.idle":"2021-07-05T05:29:20.926455Z","shell.execute_reply.started":"2021-07-05T05:27:52.211099Z","shell.execute_reply":"2021-07-05T05:29:20.925314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(clf_rf, X_test, y_test,\n                     display_labels = np.unique(y),\n                     cmap = plt.cm.Blues,\n                     normalize = 'true')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:29:26.324095Z","iopub.execute_input":"2021-07-05T05:29:26.324462Z","iopub.status.idle":"2021-07-05T05:29:34.871133Z","shell.execute_reply.started":"2021-07-05T05:29:26.324428Z","shell.execute_reply":"2021-07-05T05:29:34.869955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest with Class-weight","metadata":{}},{"cell_type":"code","source":"clf_rfw = ensemble.RandomForestClassifier(n_estimators = 300, class_weight = {0:1, 1: class_weight})\n\nclf_rfw.fit(X_train, y_train)\n\npred = clf_rfw.predict(X_test)\n\nroc_auc_score(pred, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:29:51.132274Z","iopub.execute_input":"2021-07-05T05:29:51.132739Z","iopub.status.idle":"2021-07-05T05:31:18.305075Z","shell.execute_reply.started":"2021-07-05T05:29:51.132688Z","shell.execute_reply":"2021-07-05T05:31:18.303768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(clf_rfw, X_test, y_test,\n                     display_labels = np.unique(y),\n                     cmap = plt.cm.Blues,\n                     normalize = 'true')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:31:18.306725Z","iopub.execute_input":"2021-07-05T05:31:18.307039Z","iopub.status.idle":"2021-07-05T05:31:26.880394Z","shell.execute_reply.started":"2021-07-05T05:31:18.307006Z","shell.execute_reply":"2021-07-05T05:31:26.87945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Score bumps up by 0.001","metadata":{}},{"cell_type":"markdown","source":"## XGB without Class-weight","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:31:26.882306Z","iopub.execute_input":"2021-07-05T05:31:26.88267Z","iopub.status.idle":"2021-07-05T05:31:26.970687Z","shell.execute_reply.started":"2021-07-05T05:31:26.882623Z","shell.execute_reply":"2021-07-05T05:31:26.969693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb = XGBClassifier(seed = 42)\n\nxgb.fit(X_train, y_train)\n\npred = xgb.predict(X_test)\n\nroc_auc_score(pred, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:31:26.972653Z","iopub.execute_input":"2021-07-05T05:31:26.97297Z","iopub.status.idle":"2021-07-05T05:31:36.622193Z","shell.execute_reply.started":"2021-07-05T05:31:26.972937Z","shell.execute_reply":"2021-07-05T05:31:36.62011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(xgb, X_test, y_test,\n                     display_labels = np.unique(y),\n                     cmap = plt.cm.Blues,\n                     normalize = 'true')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:31:36.624192Z","iopub.execute_input":"2021-07-05T05:31:36.624669Z","iopub.status.idle":"2021-07-05T05:31:37.015638Z","shell.execute_reply.started":"2021-07-05T05:31:36.624618Z","shell.execute_reply":"2021-07-05T05:31:37.014531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGB with Class-Weight","metadata":{}},{"cell_type":"code","source":"xgb_w = XGBClassifier(scale_pos_weight = class_weight,seed = 42)\n\nxgb_w.fit(X_train, y_train)\n\npred = xgb_w.predict(X_test)\n\nroc_auc_score(pred, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:31:37.027945Z","iopub.execute_input":"2021-07-05T05:31:37.02824Z","iopub.status.idle":"2021-07-05T05:31:46.609788Z","shell.execute_reply.started":"2021-07-05T05:31:37.02821Z","shell.execute_reply":"2021-07-05T05:31:46.608733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(xgb_w, X_test, y_test,\n                     display_labels = np.unique(y),\n                     cmap = plt.cm.Blues,\n                     normalize = 'true')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:32:31.21763Z","iopub.execute_input":"2021-07-05T05:32:31.21799Z","iopub.status.idle":"2021-07-05T05:32:31.631267Z","shell.execute_reply.started":"2021-07-05T05:32:31.217959Z","shell.execute_reply":"2021-07-05T05:32:31.630127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ROC_AUC bumps down by 0.7 . Check why ","metadata":{}},{"cell_type":"markdown","source":"# Observation : \n\n## *******************\n\n1. As we start using complex models , the 'ROC_AUC' score definitely bumps up , but we find that it is incorrectly classifying negative data as positive. \n   This can eb seen in \n*    XGB without Class weight \n* Random Forest (Generally)\n* HistGradientBoostingClassifer ( Similar to LightGBM )\n* ensemble Classifier\n\n2. As we move to complex models 01 i.e. False-Positives are reduced substantially ,\n   but 10  i.e. True-Negative do increase. \n\n3. A simple Logistic Regression is the only model that gives us Balanced values between both 01 and 10 i.e. 0.23 and 0.24","metadata":{}},{"cell_type":"markdown","source":"# What do we do now ? ","metadata":{}},{"cell_type":"markdown","source":"![](https://media.giphy.com/media/lraXagM6W2ae23iiRC/giphy-downsized.gif)","metadata":{}},{"cell_type":"markdown","source":"## If these models are deployed in production , we would need extra tests to curb the 10 i.e. True-Negatives .\n\n### COMPLEX MODELS GIVE A GOOD SCORE BUT THEY ARE NOT USEFUL in Production\n\n**Is this due to imbalance ? That more True values are classified negatively**\n\nFor imbalanced classes , ROC_AUC score is not a useful metric","metadata":{}},{"cell_type":"markdown","source":"As per below discussion by CPMP, SMOTE does not provide a bump in the performance of the model. \nLabel and Prediction Smoothing does provide an improvement . \n\nhttps://www.kaggle.com/c/lish-moa/discussion/191545","metadata":{}},{"cell_type":"markdown","source":"Let's take it again from the beginning","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/jobathon-may-2021-credit-card-lead-prediction/train.csv')\ntest = pd.read_csv('/kaggle/input/jobathon-may-2021-credit-card-lead-prediction/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:32:42.954624Z","iopub.execute_input":"2021-07-05T05:32:42.95499Z","iopub.status.idle":"2021-07-05T05:32:43.538096Z","shell.execute_reply.started":"2021-07-05T05:32:42.954958Z","shell.execute_reply":"2021-07-05T05:32:43.536992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Label Encoding both Train and Test Dataset\n\nfrom sklearn import preprocessing \n\nle = preprocessing.LabelEncoder()\n\ncat_columns = []\n\nfor col in train.drop('ID', axis = 1).select_dtypes('object').columns:\n    print('Train:',col)\n    cat_columns.append(col)\n    le = preprocessing.LabelEncoder()\n    train[col] = le.fit_transform(train[col])\n    test[col] = le.transform(test[col])\n    \ncat_features_index = [i for i, col in enumerate(train.columns) if col in cat_columns]\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:32:43.792813Z","iopub.execute_input":"2021-07-05T05:32:43.793178Z","iopub.status.idle":"2021-07-05T05:32:44.663268Z","shell.execute_reply.started":"2021-07-05T05:32:43.793147Z","shell.execute_reply":"2021-07-05T05:32:44.662081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:32:44.702459Z","iopub.execute_input":"2021-07-05T05:32:44.702895Z","iopub.status.idle":"2021-07-05T05:32:44.709706Z","shell.execute_reply.started":"2021-07-05T05:32:44.702853Z","shell.execute_reply":"2021-07-05T05:32:44.708474Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.nunique()","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:32:46.354891Z","iopub.execute_input":"2021-07-05T05:32:46.355284Z","iopub.status.idle":"2021-07-05T05:32:46.519601Z","shell.execute_reply.started":"2021-07-05T05:32:46.355252Z","shell.execute_reply":"2021-07-05T05:32:46.518427Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nsns.pairplot(train.drop('ID', axis = 1), y_vars = 'Is_Lead')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:33:56.325807Z","iopub.execute_input":"2021-07-05T05:33:56.32625Z","iopub.status.idle":"2021-07-05T05:34:02.320359Z","shell.execute_reply.started":"2021-07-05T05:33:56.326204Z","shell.execute_reply":"2021-07-05T05:34:02.319132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# USEFUL TOOL\ntrain['Avg_Account_Balance'].value_counts(bins = 6)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:34:17.858086Z","iopub.execute_input":"2021-07-05T05:34:17.858466Z","iopub.status.idle":"2021-07-05T05:34:17.891072Z","shell.execute_reply.started":"2021-07-05T05:34:17.858432Z","shell.execute_reply":"2021-07-05T05:34:17.890323Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn import linear_model \n\nX = train.drop(['Is_Lead', 'ID'], axis = 1)\ny = train['Is_Lead']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 42, stratify = y)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:34:39.144838Z","iopub.execute_input":"2021-07-05T05:34:39.145497Z","iopub.status.idle":"2021-07-05T05:34:39.350969Z","shell.execute_reply.started":"2021-07-05T05:34:39.145443Z","shell.execute_reply":"2021-07-05T05:34:39.350162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generating Feature Interaction using PatsyTransformer","metadata":{}},{"cell_type":"code","source":"from sklearn import linear_model\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import preprocessing\nfrom sklego.preprocessing import PatsyTransformer\nfrom sklego.mixture import GMMClassifier\n\n\nclass_weight = 3\n\npipe_lrn = Pipeline([\n    (\"patsy\", PatsyTransformer(\"(C(Credit_Product)+Gender+Channel_Code+Is_Active+Occupation)**2-1-(C(Credit_Product)+Gender+Channel_Code+Is_Active+Occupation) + np.log(Age) + Region_Code + Vintage + np.log(Avg_Account_Balance )\")),\n   # (\"Interaction\", preprocessing.PolynomialFeatures()),  # Fit the attributes to a normal Distribution\n    (\"Normalize\", preprocessing.RobustScaler()),  # Robust to outliers\n    (\"model\", linear_model.LogisticRegression(class_weight = {0:1, 1: class_weight}, solver = 'liblinear', max_iter = 1000))\n])\n\npred = pipe_lrn.fit(X_train, y_train).predict(X_test)\nroc_auc_score(pred, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:34:58.476798Z","iopub.execute_input":"2021-07-05T05:34:58.477155Z","iopub.status.idle":"2021-07-05T05:35:01.982722Z","shell.execute_reply.started":"2021-07-05T05:34:58.477122Z","shell.execute_reply":"2021-07-05T05:35:01.98141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(pipe_lrn, X_test, y_test,\n                     display_labels = np.unique(y),\n                     cmap = plt.cm.Blues,\n                     normalize = 'true'\n                     )","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:37:30.206101Z","iopub.execute_input":"2021-07-05T05:37:30.206521Z","iopub.status.idle":"2021-07-05T05:37:31.049261Z","shell.execute_reply.started":"2021-07-05T05:37:30.206459Z","shell.execute_reply":"2021-07-05T05:37:31.047793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import set_config                      # to change the display\nfrom sklearn.utils import estimator_html_repr       # to save the diagram into HTML format\nfrom IPython.core.display import display, HTML      # to visualize pipeline\n\nset_config(display='diagram')\ndisplay(HTML(estimator_html_repr(pipe_lrn)))#","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:37:31.051195Z","iopub.execute_input":"2021-07-05T05:37:31.051593Z","iopub.status.idle":"2021-07-05T05:37:31.069887Z","shell.execute_reply.started":"2021-07-05T05:37:31.051554Z","shell.execute_reply":"2021-07-05T05:37:31.067381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Below are the Estimators we can use in Grid Search","metadata":{}},{"cell_type":"code","source":"pipe_lrn.get_params()","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:37:35.656455Z","iopub.execute_input":"2021-07-05T05:37:35.657034Z","iopub.status.idle":"2021-07-05T05:37:35.66872Z","shell.execute_reply.started":"2021-07-05T05:37:35.656982Z","shell.execute_reply":"2021-07-05T05:37:35.667693Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using different metrics for refitting to tune the model","metadata":{}},{"cell_type":"markdown","source":"**Using the mathew correlation coefficient as it balances the 4 predictions. [ TN,TP,FN,FP ]**","metadata":{}},{"cell_type":"code","source":"from sklearn import model_selection\nfrom sklearn.metrics import roc_auc_score, make_scorer\nfrom sklearn.experimental import enable_halving_search_cv\n\n\n\nparam_grid = dict(C = [100,10,1,0.1,0.01])\ngrid_search1 = model_selection.HalvingRandomSearchCV(pipe_lrn , \n                                           param_distributions = {\n                                                   #       'Interaction__degree' : [2,3],\n                                                          'model__C': [100, 10, 1.0, 0.1, 0.01],\n                                                          'model__warm_start':[False, True],\n                                               #           'Interaction__include_bias':[False,True],\n                                                    #      'Interaction__interaction_only':[True, False],\n                                                          'model__class_weight': [{0: 1, 1: v} for v in np.linspace(1,20, 30)]\n                                                         },\n                                           aggressive_elimination = True,\n                                           n_jobs = -1,\n                                           scoring = 'roc_auc',\n                                           refit = 'matthews_corrcoef',\n                                           verbose = 2,\n                                           cv = 3)\ngrid_search1.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:39:10.061675Z","iopub.execute_input":"2021-07-05T05:39:10.062041Z","iopub.status.idle":"2021-07-05T05:40:06.046903Z","shell.execute_reply.started":"2021-07-05T05:39:10.062011Z","shell.execute_reply":"2021-07-05T05:40:06.045715Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = grid_search1.predict(X_test)\nroc_auc_score(pred, y_test)\n\nplot_confusion_matrix(grid_search1, X_test, y_test,\n                     display_labels = np.unique(y),\n                     cmap = plt.cm.Blues,\n                     normalize = 'true'\n                     )","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:38:52.710883Z","iopub.execute_input":"2021-07-05T05:38:52.711166Z","iopub.status.idle":"2021-07-05T05:38:54.057101Z","shell.execute_reply.started":"2021-07-05T05:38:52.711136Z","shell.execute_reply":"2021-07-05T05:38:54.055673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roc_auc_score(pred, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:40:37.691166Z","iopub.execute_input":"2021-07-05T05:40:37.691545Z","iopub.status.idle":"2021-07-05T05:40:37.720164Z","shell.execute_reply.started":"2021-07-05T05:40:37.691507Z","shell.execute_reply":"2021-07-05T05:40:37.718956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ROC_AUC score has bumped down. ","metadata":{}},{"cell_type":"markdown","source":"### Considering the Predictions TP is damn good, but it also has a high prediction for TN . ","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(grid_search1.cv_results_)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:42:14.46666Z","iopub.execute_input":"2021-07-05T05:42:14.467055Z","iopub.status.idle":"2021-07-05T05:42:14.529786Z","shell.execute_reply.started":"2021-07-05T05:42:14.467021Z","shell.execute_reply":"2021-07-05T05:42:14.528645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"My training score is also pretty good. ","metadata":{}},{"cell_type":"markdown","source":"A bit of new trial, \n\nRefitting on 'ROC_AUC' and 'f1_score' to give a ","metadata":{}},{"cell_type":"code","source":"%%time\n\nparam_grid = dict(C = [100,10,1,0.1,0.01])\ngrid_search1 = model_selection.HalvingGridSearchCV(pipe_lrn , \n                                           param_grid = {\n                                                      #    'Interaction__degree' : [2,3],\n                                                          'model__C': [100, 10, 1.0, 0.1, 0.01],\n                                                          'model__warm_start':[False, True],\n                                               #           'Interaction__include_bias':[False,True],\n                                            #             'Interaction__interaction_only':[True, False],\n                                                          'model__class_weight': [{0: 1, 1: v} for v in np.linspace(1,20, 30)]\n                                                         },\n                                           aggressive_elimination = True,\n                                           n_jobs = -1,\n                                           scoring = 'roc_auc',\n                                           refit = {'matthews_corrcoef', 'f1_score'},\n                                           verbose = 2,\n                                           cv = 5)\ngrid_search1.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T06:42:52.949267Z","iopub.execute_input":"2021-07-05T06:42:52.949684Z","iopub.status.idle":"2021-07-05T06:44:24.529989Z","shell.execute_reply.started":"2021-07-05T06:42:52.949646Z","shell.execute_reply":"2021-07-05T06:44:24.528736Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = grid_search1.predict(X_test)\n\n\nplot_confusion_matrix(grid_search1, X_test, y_test,\n                     display_labels = np.unique(y),\n                     cmap = plt.cm.Blues,\n                     normalize = 'true'\n                     )","metadata":{"execution":{"iopub.status.busy":"2021-07-05T06:44:24.532197Z","iopub.execute_input":"2021-07-05T06:44:24.532557Z","iopub.status.idle":"2021-07-05T06:44:25.856278Z","shell.execute_reply.started":"2021-07-05T06:44:24.532523Z","shell.execute_reply":"2021-07-05T06:44:25.854872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"TN score has increased from 0.45 to 0.49","metadata":{}},{"cell_type":"code","source":"roc_auc_score(pred, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T06:44:25.858139Z","iopub.execute_input":"2021-07-05T06:44:25.858566Z","iopub.status.idle":"2021-07-05T06:44:25.887815Z","shell.execute_reply.started":"2021-07-05T06:44:25.858528Z","shell.execute_reply":"2021-07-05T06:44:25.886517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search1.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:52:08.417261Z","iopub.execute_input":"2021-07-05T05:52:08.41767Z","iopub.status.idle":"2021-07-05T05:52:08.423196Z","shell.execute_reply.started":"2021-07-05T05:52:08.417636Z","shell.execute_reply":"2021-07-05T05:52:08.422441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### One thing that I hope you notice is that the confusion matrix is not improving despite the various Grid Search and Random Search \n\n### Also if you take into consideration the training score it is pretty damn good.\n\n## Let's try tweaking the dataset. ","metadata":{}},{"cell_type":"markdown","source":"Trying Feature Interaction using Patsy.","metadata":{}},{"cell_type":"code","source":"pt = PatsyTransformer(\"(C(Credit_Product)+Gender+Channel_Code+Is_Active+Occupation)**2-1-(C(Credit_Product)+Gender+Channel_Code+Is_Active+Occupation) + np.log(Age) + Region_Code + Vintage + np.log(Avg_Account_Balance )\")\nps = pt.fit(X_train, y_train).transform(X_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:54:08.315127Z","iopub.execute_input":"2021-07-05T05:54:08.315559Z","iopub.status.idle":"2021-07-05T05:54:10.003686Z","shell.execute_reply.started":"2021-07-05T05:54:08.315517Z","shell.execute_reply":"2021-07-05T05:54:10.002439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**'Terms' are the Columns that are generated using Feature Interactions**","metadata":{}},{"cell_type":"code","source":"ps","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:54:11.96738Z","iopub.execute_input":"2021-07-05T05:54:11.967755Z","iopub.status.idle":"2021-07-05T05:54:11.974732Z","shell.execute_reply.started":"2021-07-05T05:54:11.967711Z","shell.execute_reply":"2021-07-05T05:54:11.973426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import linear_model\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import preprocessing\nfrom sklego.preprocessing import PatsyTransformer\nfrom sklego.mixture import GMMClassifier\n\n\nclass_weight = 3\n\npipe_lri = Pipeline([\n    (\"patsy\", PatsyTransformer(\"(C(Credit_Product)+Gender+Channel_Code+Is_Active+Occupation)**2-1-(C(Credit_Product)+Gender+Channel_Code+Is_Active+Occupation) + np.log(Age) + Region_Code + Vintage + np.log(Avg_Account_Balance )\")),\n   # (\"Interaction\", preprocessing.PolynomialFeatures()),  # Fit the attributes to a normal Distribution\n    (\"Normalize\", preprocessing.RobustScaler()),  # Robust to outliers\n    (\"model\", linear_model.LogisticRegression(class_weight = {0:1, 1: class_weight}, solver = 'liblinear', max_iter = 1000))\n])\n\npred = pipe_lri.fit(X_train, y_train).predict(X_test)\nroc_auc_score(pred, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:55:55.544333Z","iopub.execute_input":"2021-07-05T05:55:55.544697Z","iopub.status.idle":"2021-07-05T05:55:58.927017Z","shell.execute_reply.started":"2021-07-05T05:55:55.544665Z","shell.execute_reply":"2021-07-05T05:55:58.92557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(pipe_lri, X_test, y_test,\n                     display_labels = np.unique(y),\n                     cmap = plt.cm.Blues,\n                     normalize = 'true'\n                     )","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:56:25.946036Z","iopub.execute_input":"2021-07-05T05:56:25.946386Z","iopub.status.idle":"2021-07-05T05:56:26.742732Z","shell.execute_reply.started":"2021-07-05T05:56:25.946356Z","shell.execute_reply":"2021-07-05T05:56:26.741646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Back to square 1 I guess","metadata":{}},{"cell_type":"code","source":"%%time\n\nparam_grid = dict(C = [100,10,1,0.1,0.01])\ngrid_search1 = model_selection.HalvingGridSearchCV(pipe_lri , \n                                           param_grid = {\n                                                #          'Interaction__degree' : [2,3],\n                                                          'model__C': [100, 10, 1.0, 0.1, 0.01],\n                                                          'model__warm_start':[False, True],\n                                               #          'Interaction__include_bias':[False,True],\n                                               #          'Interaction__interaction_only':[True, False],\n                                                          'model__class_weight': [{0: 1, 1: v} for v in np.linspace(1,20, 30)]\n                                                         },\n                                           aggressive_elimination = True,\n                                           n_jobs = -1,\n                                           scoring = 'roc_auc',\n                                           refit = {'roc_auc','f1_score'},\n                                           verbose = 2,\n                                           cv = 5)\ngrid_search1.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:57:54.546589Z","iopub.execute_input":"2021-07-05T05:57:54.547128Z","iopub.status.idle":"2021-07-05T05:59:29.120674Z","shell.execute_reply.started":"2021-07-05T05:57:54.547082Z","shell.execute_reply":"2021-07-05T05:59:29.119503Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(grid_search1, X_test, y_test,\n                     display_labels = np.unique(y),\n                     cmap = plt.cm.Blues,\n                     normalize = 'true'\n                     )","metadata":{"execution":{"iopub.status.busy":"2021-07-05T05:59:29.122778Z","iopub.execute_input":"2021-07-05T05:59:29.123222Z","iopub.status.idle":"2021-07-05T05:59:29.950507Z","shell.execute_reply.started":"2021-07-05T05:59:29.12317Z","shell.execute_reply":"2021-07-05T05:59:29.949366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### That's a good model","metadata":{}},{"cell_type":"code","source":"from sklearn import linear_model\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import preprocessing\nfrom sklego.preprocessing import PatsyTransformer\nfrom sklego.mixture import GMMClassifier\n\n\nclass_weight = 3\n\npipe_lrp = Pipeline([\n    (\"patsy\", PatsyTransformer(\"(C(Credit_Product)+Gender+Channel_Code+Is_Active+Occupation)**2-1-(C(Credit_Product)+Gender+Channel_Code+Is_Active+Occupation) + np.log(Age) + Region_Code + Vintage + np.log(Avg_Account_Balance )\")),\n  #  (\"Interaction\", preprocessing.PolynomialFeatures()),  # Fit the attributes to a normal Distribution\n    (\"Normalize\", preprocessing.RobustScaler()),  # Robust to outliers\n    (\"model\", linear_model.LogisticRegression(class_weight = {0:1, 1: class_weight}, solver = 'liblinear', max_iter = 1000))\n])\n\npred = pipe_lri.fit(X_train, y_train).predict(X_test)\nroc_auc_score(pred, y_test)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n##   ACTIVATING BOTH INTERACTION FEATURES\n\nparam_grid = dict(C = [100,10,1,0.1,0.01])\ngrid_search1 = model_selection.HalvingGridSearchCV(pipe_lrp , \n                                           param_grid = {\n                                                     #     'Interaction__degree' : [2,3],\n                                                          'model__C': [100, 10, 1.0, 0.1, 0.01],\n                                                          'model__warm_start':[False, True],\n                                                      #    'Interaction__include_bias':[False,True],\n                                                      #    'Interaction__interaction_only':[True, False],\n                                                          'model__class_weight': [{0: 1, 1: v} for v in np.linspace(1,20, 30)]\n                                                         },\n                                           aggressive_elimination = True,\n                                           n_jobs = -1,\n                                           scoring = 'roc_auc',\n                                           refit = 'roc_auc',\n                                           verbose = 2,\n                                           cv = 10)\ngrid_search1.fit(X_train, y_train)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(grid_search1, X_test, y_test,\n                     display_labels = np.unique(y),\n                     cmap = plt.cm.Blues,\n                     normalize = 'true'\n                     )","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's try an ensemble of few basic classifiers","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.ensemble import VotingClassifier\n\nknn = KNeighborsClassifier()\ngmm = GMMClassifier(n_components = 4)\nrf = RandomForestClassifier(class_weight = {0:1, 1: class_weight})\nlr = linear_model.LogisticRegression(class_weight = {0:1, 1: class_weight}, solver = 'liblinear', max_iter = 1000)\n\nclassifiers = [('knn', knn),\n               ('gmm', gmm),\n               ('rf', rf),\n               ('lr', lr)]\n\nvc = VotingClassifier(estimators=classifiers, voting='hard')\n\npipe_lri = Pipeline([\n    (\"patsy\", PatsyTransformer(\"(C(Credit_Product)+Gender+Channel_Code+Is_Active+Occupation)**2-1-(C(Credit_Product)+Gender+Channel_Code+Is_Active+Occupation) + np.log(Age) + Region_Code + Vintage + np.log(Avg_Account_Balance )\")),\n   # (\"Interaction\", preprocessing.PolynomialFeatures()),  # Fit the attributes to a normal Distribution\n    (\"Normalize\", preprocessing.RobustScaler()),  # Robust to outliers\n    (\"model\", vc)\n])\n\npred = pipe_lri.fit(X_train, y_train).predict(X_test)\nroc_auc_score(pred, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T06:02:14.06477Z","iopub.execute_input":"2021-07-05T06:02:14.065156Z","iopub.status.idle":"2021-07-05T06:07:06.48783Z","shell.execute_reply.started":"2021-07-05T06:02:14.065123Z","shell.execute_reply":"2021-07-05T06:07:06.486461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(pipe_lri, X_test, y_test,\n                     display_labels = np.unique(y),\n                     cmap = plt.cm.Blues,\n                     normalize = 'true'\n                     )","metadata":{"execution":{"iopub.status.busy":"2021-07-05T06:07:06.490157Z","iopub.execute_input":"2021-07-05T06:07:06.49066Z","iopub.status.idle":"2021-07-05T06:11:30.934102Z","shell.execute_reply.started":"2021-07-05T06:07:06.490606Z","shell.execute_reply":"2021-07-05T06:11:30.933031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we are predicting the FP , with a high probability, \nand my TP prediction is very low .","metadata":{}},{"cell_type":"markdown","source":"# Using Transformations of individual features and testing them . ","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import FeatureUnion, Pipeline\nfrom sklearn.linear_model import LogisticRegression, Ridge\nfrom sklearn import linear_model, preprocessing\nfrom sklearn import ensemble\nfrom sklego.preprocessing import PatsyTransformer\n\nfrom sklego.meta import EstimatorTransformer\nfrom sklego.preprocessing import ColumnSelector\n\nfeature_pipeline = Pipeline([\n    (\"datagrab\", FeatureUnion([\n        (\"discrete\", Pipeline([\n            (\"Account_grab\", ColumnSelector([\"Avg_Account_Balance\"])),\n            (\"Quantile\", preprocessing.QuantileTransformer(random_state=0)),\n        ])),\n        (\"Normalize\", Pipeline([\n            (\"Age_grab\",ColumnSelector([\"Age\"])),\n            (\"Normalize\", preprocessing.Normalizer())\n        ])),\n\n        (\"Onehotencode_columns\", Pipeline([\n            (\"OneHot_grab\",ColumnSelector([\"Credit_Product\", \"Occupation\"])),\n            (\"OneHotEncode\", preprocessing.OneHotEncoder(categories = \"auto\", sparse = False))\n        ])),\n    (\"Interaction\", FeatureUnion([\n      (\"Interaction Pipeline\", Pipeline([\n          (\"Lead Correlated Columns\", ColumnSelector([\"Credit_Product\",\"Gender\",\"Channel_Code\",\"Is_Active\",\"Occupation\"])),\n            (\"interaction\", preprocessing.PolynomialFeatures(include_bias = False, interaction_only = True)),\n      ]))\n  ])),\n    ])),\n  (\"Standardization\", FeatureUnion([\n     (\"Standardize\", preprocessing.StandardScaler())\n  ])),\n#   (\"ml_features\", FeatureUnion([\n#        (\"XGBoostClassifier\", EstimatorTransformer(linear_model.LogisticRegression(class_weight = {0:1, 1: class_weight}, solver = 'liblinear', max_iter = 1000))),\n#        (\"Random Forest Classifier\", EstimatorTransformer(ensemble.RandomForestClassifier())),\n#        (\"GMM\", EstimatorTransformer(GMMClassifier(n_components = 4))),\n#        (\"KNN\",EstimatorTransformer(KNeighborsClassifier()))\n#    ])),\n# (\"Ridge\", EstimatorTransformer(linear_model.Ridge())),\n\n #   (\"patsy\", PatsyTransformer(\"(C(Credit_Product)+Gender+Channel_Code+Is_Active+Occupation)**2-1-(C(Credit_Product)+Gender+Channel_Code+Is_Active+Occupation) + np.log(Age) + Region_Code + Vintage + np.log(Avg_Account_Balance )\")),\n   \n])","metadata":{"execution":{"iopub.status.busy":"2021-07-05T06:13:59.333443Z","iopub.execute_input":"2021-07-05T06:13:59.333854Z","iopub.status.idle":"2021-07-05T06:13:59.346076Z","shell.execute_reply.started":"2021-07-05T06:13:59.333821Z","shell.execute_reply":"2021-07-05T06:13:59.344962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe = Pipeline([\n    (\"transform\", feature_pipeline),\n #   (\"Random Forest Classifier\", (ensemble.RandomForestClassifier())),\n#    (\"XGBoostClassifier\", EstimatorTransformer(XGBClassifier(seed = 42))),\n    (\"Logistic Regression\", (linear_model.LogisticRegression(class_weight = {0:1, 1: 3}, solver = 'liblinear', max_iter = 1000))),\n#    (\"Extra tree Classifier\", ensemble.ExtraTreesClassifier(class_weight = {0:1, 1: 3},n_estimators=100, random_state=0))\n])\n\npipe.fit(X_train, y_train)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-05T06:14:05.524112Z","iopub.execute_input":"2021-07-05T06:14:05.524508Z","iopub.status.idle":"2021-07-05T06:14:07.646392Z","shell.execute_reply.started":"2021-07-05T06:14:05.524457Z","shell.execute_reply":"2021-07-05T06:14:07.645056Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = pipe.predict(X_test)\nroc_auc_score(pred, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T06:14:19.680962Z","iopub.execute_input":"2021-07-05T06:14:19.681764Z","iopub.status.idle":"2021-07-05T06:14:19.90655Z","shell.execute_reply.started":"2021-07-05T06:14:19.681705Z","shell.execute_reply":"2021-07-05T06:14:19.904868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(pipe, X_test, y_test,\n                     display_labels = np.unique(y),\n                     cmap = plt.cm.Blues,\n                     normalize = 'true'\n                     )\n","metadata":{"execution":{"iopub.status.busy":"2021-07-05T06:14:20.827254Z","iopub.execute_input":"2021-07-05T06:14:20.827659Z","iopub.status.idle":"2021-07-05T06:14:21.39558Z","shell.execute_reply.started":"2021-07-05T06:14:20.827622Z","shell.execute_reply":"2021-07-05T06:14:21.394597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe.get_params()","metadata":{"execution":{"iopub.status.busy":"2021-07-05T06:14:26.713621Z","iopub.execute_input":"2021-07-05T06:14:26.714165Z","iopub.status.idle":"2021-07-05T06:14:26.939373Z","shell.execute_reply.started":"2021-07-05T06:14:26.714129Z","shell.execute_reply":"2021-07-05T06:14:26.938506Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nfrom sklearn import model_selection\nfrom sklearn.experimental import enable_halving_search_cv\n\n\ngrid_search1 = model_selection.HalvingGridSearchCV(pipe , \n                                           param_grid = {\n                                                          'transform__datagrab__Interaction__Interaction Pipeline__interaction__degree' : [2,3],\n                                                          'transform__datagrab__Interaction__Interaction Pipeline__interaction__interaction_only':[True, False],\n                                                          'Logistic Regression__C': [100, 10, 1.0, 0.1, 0.01],\n                                                          'Logistic Regression__warm_start':[False, True],\n                                                      #    'Interaction__include_bias':[False,True],\n                                                          'Logistic Regression__class_weight': [{0: 1, 1: v} for v in np.linspace(1,20, 30)]\n                                                         },\n                                           aggressive_elimination = True,\n                                           n_jobs = -1,\n                                           scoring = 'roc_auc',\n                                           refit = {'log_loss', 'matthews_corrcoef'},\n                                           verbose = 2,\n                                           cv = 5)\ngrid_search1.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T06:14:27.360705Z","iopub.execute_input":"2021-07-05T06:14:27.361292Z","iopub.status.idle":"2021-07-05T06:19:10.518198Z","shell.execute_reply.started":"2021-07-05T06:14:27.36124Z","shell.execute_reply":"2021-07-05T06:19:10.51675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roc_auc_score(grid_search1.predict(X_test), y_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T06:19:10.520406Z","iopub.execute_input":"2021-07-05T06:19:10.520758Z","iopub.status.idle":"2021-07-05T06:19:10.680502Z","shell.execute_reply.started":"2021-07-05T06:19:10.520723Z","shell.execute_reply":"2021-07-05T06:19:10.679318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(grid_search1, X_test, y_test,\n                     display_labels = np.unique(y),\n                     cmap = plt.cm.Blues,\n                     normalize = 'true'\n                     )","metadata":{"execution":{"iopub.status.busy":"2021-07-05T06:23:40.852067Z","iopub.execute_input":"2021-07-05T06:23:40.852537Z","iopub.status.idle":"2021-07-05T06:23:41.387335Z","shell.execute_reply.started":"2021-07-05T06:23:40.852463Z","shell.execute_reply":"2021-07-05T06:23:41.386087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ## Overfitting on 'True' value. \n\n## I guess it might be due to the feature interaction. \n## Let's tweak that. ","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(grid_search1.cv_results_)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T10:11:10.283081Z","iopub.execute_input":"2021-06-29T10:11:10.283469Z","iopub.status.idle":"2021-06-29T10:11:10.336034Z","shell.execute_reply.started":"2021-06-29T10:11:10.283428Z","shell.execute_reply":"2021-06-29T10:11:10.334952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.pipeline import FeatureUnion, Pipeline\nfrom sklearn.linear_model import LogisticRegression, Ridge\nfrom sklearn import linear_model, preprocessing\nfrom sklearn import ensemble\nfrom sklego.preprocessing import PatsyTransformer\n\nfrom sklego.meta import EstimatorTransformer\nfrom sklego.preprocessing import ColumnSelector\n\nfeature_pipeline = Pipeline([\n    (\"datagrab\", FeatureUnion([\n        (\"discrete\", Pipeline([\n            (\"Account_grab\", ColumnSelector([\"Avg_Account_Balance\"])),\n            (\"Quantile\", preprocessing.QuantileTransformer(random_state=0)),\n        ])),\n        (\"Normalize\", Pipeline([\n            (\"Age_grab\",ColumnSelector([\"Age\"])),\n            (\"Normalize\", preprocessing.Normalizer())\n        ])),\n\n        (\"Onehotencode_columns\", Pipeline([\n            (\"OneHot_grab\",ColumnSelector([\"Credit_Product\", \"Occupation\"])),\n            (\"OneHotEncode\", preprocessing.OneHotEncoder(categories = \"auto\", sparse = False))\n        ])),\n    \n    ])),\n  (\"Standardization\", FeatureUnion([\n     (\"Standardize\", preprocessing.StandardScaler())\n  ])),\n    (\"Interaction\", FeatureUnion([\n      (\"Interaction Pipeline\", Pipeline([\n         (\"interaction\", preprocessing.PolynomialFeatures(include_bias = False, interaction_only = True)),\n      ]))\n  ])),\n#   (\"ml_features\", FeatureUnion([\n#        (\"XGBoostClassifier\", EstimatorTransformer(linear_model.LogisticRegression(class_weight = {0:1, 1: class_weight}, solver = 'liblinear', max_iter = 1000))),\n#        (\"Random Forest Classifier\", EstimatorTransformer(ensemble.RandomForestClassifier())),\n#        (\"GMM\", EstimatorTransformer(GMMClassifier(n_components = 4))),\n#        (\"KNN\",EstimatorTransformer(KNeighborsClassifier()))\n#    ])),\n# (\"Ridge\", EstimatorTransformer(linear_model.Ridge())),\n\n #   (\"patsy\", PatsyTransformer(\"(C(Credit_Product)+Gender+Channel_Code+Is_Active+Occupation)**2-1-(C(Credit_Product)+Gender+Channel_Code+Is_Active+Occupation) + np.log(Age) + Region_Code + Vintage + np.log(Avg_Account_Balance )\")),\n   \n])","metadata":{"execution":{"iopub.status.busy":"2021-07-05T06:26:36.029376Z","iopub.execute_input":"2021-07-05T06:26:36.029865Z","iopub.status.idle":"2021-07-05T06:26:36.042365Z","shell.execute_reply.started":"2021-07-05T06:26:36.029828Z","shell.execute_reply":"2021-07-05T06:26:36.041088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe = Pipeline([\n    (\"transform\", feature_pipeline),\n #   (\"Random Forest Classifier\", (ensemble.RandomForestClassifier())),\n#    (\"XGBoostClassifier\", EstimatorTransformer(XGBClassifier(seed = 42))),\n    (\"Logistic Regression\", (linear_model.LogisticRegression(class_weight = {0:1, 1: 3}, solver = 'liblinear', max_iter = 1000))),\n#    (\"Extra tree Classifier\", ensemble.ExtraTreesClassifier(class_weight = {0:1, 1: 3},n_estimators=100, random_state=0))\n])\n\npipe.fit(X_train, y_train)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-05T06:27:10.260154Z","iopub.execute_input":"2021-07-05T06:27:10.260831Z","iopub.status.idle":"2021-07-05T06:27:13.013453Z","shell.execute_reply.started":"2021-07-05T06:27:10.260771Z","shell.execute_reply":"2021-07-05T06:27:13.012599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(pipe, X_test, y_test,\n                     display_labels = np.unique(y),\n                     cmap = plt.cm.Blues,\n                     normalize = 'true'\n                     )","metadata":{"execution":{"iopub.status.busy":"2021-07-05T06:27:42.383743Z","iopub.execute_input":"2021-07-05T06:27:42.384355Z","iopub.status.idle":"2021-07-05T06:27:42.918844Z","shell.execute_reply.started":"2021-07-05T06:27:42.384298Z","shell.execute_reply":"2021-07-05T06:27:42.917694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seems back to square 1 ","metadata":{}},{"cell_type":"code","source":"roc_auc_score(pipe.predict(X_test), y_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T06:28:23.608602Z","iopub.execute_input":"2021-07-05T06:28:23.609178Z","iopub.status.idle":"2021-07-05T06:28:23.783718Z","shell.execute_reply.started":"2021-07-05T06:28:23.609141Z","shell.execute_reply":"2021-07-05T06:28:23.782273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe.get_params()","metadata":{"execution":{"iopub.status.busy":"2021-07-05T06:28:38.579433Z","iopub.execute_input":"2021-07-05T06:28:38.57987Z","iopub.status.idle":"2021-07-05T06:28:38.728828Z","shell.execute_reply.started":"2021-07-05T06:28:38.579825Z","shell.execute_reply":"2021-07-05T06:28:38.727724Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nfrom sklearn import model_selection\nfrom sklearn.experimental import enable_halving_search_cv\n\n\ngrid_search1 = model_selection.HalvingGridSearchCV(pipe , \n                                           param_grid = {\n                                                          'transform__Interaction__Interaction Pipeline__interaction__degree' : [2,3],\n                                                          'transform__Interaction__Interaction Pipeline__interaction__interaction_only':[True, False],\n                                                          'Logistic Regression__C': [100, 10, 1.0, 0.1, 0.01],\n                                                          'Logistic Regression__warm_start':[False, True],\n                                                      #    'Interaction__include_bias':[False,True],\n                                                          'Logistic Regression__class_weight': [{0: 1, 1: v} for v in np.linspace(1,20, 30)]\n                                                         },\n                                           aggressive_elimination = True,\n                                           n_jobs = -1,\n                                           scoring = 'roc_auc',\n                                           refit = {'roc_auc', 'matthews_corrcoef'},\n                                           verbose = 2,\n                                           cv = 5)\ngrid_search1.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T06:36:25.488748Z","iopub.execute_input":"2021-07-05T06:36:25.48918Z","iopub.status.idle":"2021-07-05T06:40:28.554983Z","shell.execute_reply.started":"2021-07-05T06:36:25.489144Z","shell.execute_reply":"2021-07-05T06:40:28.553728Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(pipe, X_test, y_test,\n                     display_labels = np.unique(y),\n                     cmap = plt.cm.Blues,\n                     normalize = 'true'\n                     )","metadata":{"execution":{"iopub.status.busy":"2021-07-05T06:40:28.55706Z","iopub.execute_input":"2021-07-05T06:40:28.557535Z","iopub.status.idle":"2021-07-05T06:40:29.09257Z","shell.execute_reply.started":"2021-07-05T06:40:28.557466Z","shell.execute_reply":"2021-07-05T06:40:29.09156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roc_auc_score(pipe.predict(X_test), y_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T06:40:29.09432Z","iopub.execute_input":"2021-07-05T06:40:29.094673Z","iopub.status.idle":"2021-07-05T06:40:29.263417Z","shell.execute_reply.started":"2021-07-05T06:40:29.094638Z","shell.execute_reply":"2021-07-05T06:40:29.261947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Even with the pipeline , it's not improving","metadata":{}}]}