{"cells":[{"metadata":{"_uuid":"40efe85270c8cb0f69d1c9a53a88b3938d280b6a"},"cell_type":"markdown","source":"This notebook implements the **Linear Discriminant Ananlyis(LDA)** as explained in the book \"**Python Machine Learning**\" by **Sebastian Raschka** and **Vahid Mirjalili**.\n\nPrerequisites:\n\n* Python\n* pandas\n* numpy\n\n**Dataset:** Wine\n\n**Note:** Descriptive comments explain the code in a better way\n\n**Assumptions for LDA**: \n\n* Samples are normally distributed\n* Features are statistically independent\n* Classes have identical covariance matrices\n\n\nImport necessary packages:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nfrom matplotlib.colors import ListedColormap","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"Load the Wine dataset in a panda dataframe:\n\n"},{"metadata":{"trusted":true,"_uuid":"ba192c8855c286598d6f48fb20b7ca76eb821753"},"cell_type":"code","source":"df_wine = pd.read_csv('../input/Wine.csv');\ndf_wine.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e51e562b198dc81e0c5576ad2f1dbc5c1ed5f47"},"cell_type":"markdown","source":"Add headers in the data:"},{"metadata":{"trusted":true,"_uuid":"6e05bba3a2a30731a51c147e708d7923a10e2259"},"cell_type":"code","source":"df_wine.columns = [  'name'\n                 ,'alcohol'\n             \t,'malicAcid'\n             \t,'ash'\n            \t,'ashalcalinity'\n             \t,'magnesium'\n            \t,'totalPhenols'\n             \t,'flavanoids'\n             \t,'nonFlavanoidPhenols'\n             \t,'proanthocyanins'\n            \t,'colorIntensity'\n             \t,'hue'\n             \t,'od280_od315'\n             \t,'proline'\n                ]\ndf_wine.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1b36d2bc54a22b610c99385947a986ae7be446a"},"cell_type":"markdown","source":"Step 1 : Preprocess the data into train and test sets with 70%:30% ratio respectively and standardize the data as is a requirement for LDA to assign equal importance to each feature beforehand"},{"metadata":{"trusted":true,"_uuid":"b1f410015685757aa6c6b1f03749d8828d9efbc0"},"cell_type":"code","source":"#make train-test sets\nfrom sklearn.model_selection import train_test_split;\nX, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values;\n#print(np.unique(y))\n#split with stratify on y for equal proportion of classes in train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, stratify = y,random_state = 0);\n\n#standardize the features with same model on train and test sets\nfrom sklearn.preprocessing import StandardScaler;\nsc = StandardScaler();\nX_train_std = sc.fit_transform(X_train);\nX_test_sd = sc.transform(X_test);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f8c130b94fd39d79876a05a3e3b75754fd045dd4"},"cell_type":"markdown","source":"Step 2: Compute the mean vectors of the features for each class label\n\n>> mi = [feature1 feature2...featureN] , i belongs to classes"},{"metadata":{"trusted":true,"_uuid":"6fe250ecc5c23426d36c5d373b953076255f8331"},"cell_type":"code","source":"#set precision of the vectors\nnp.set_printoptions(precision = 4);\nmean_vecs = [];\n\n#for each of the label compute the mean vector \nfor label in range(1,4):\n    mean_vecs.append(np.mean(X_train_std[y_train == label],axis = 0));\n    print('Mean Vector %s: %s\\n' %(label, mean_vecs[label - 1]));","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48552d5ef53f434abf4a18c387b82a1a7ba8306e"},"cell_type":"markdown","source":"Step 3: Compute the between class and within class **Scatter Matrices** using the mean vectors\n\n**Within Class Scatter Matrix**:  Sum of scatter matrices of each class i.e. \n\n>> Sum(Si) where Si = Sum(x - mi)(x - mi)T\nand i belongs to classes and mi is class mean vector, T = Transpose"},{"metadata":{"trusted":true,"_uuid":"92eb9dbd0cd5a8089f01a92ba012825d55bc7e9e"},"cell_type":"code","source":"#define number of features\nd  = 13;\n#define the within class scatter matrix of dimension d x d\nS_W = np.zeros((d,d));\n\n# run through each class label and keep track of the corresponding mean vector\nfor label , mv in zip(range(1,4),mean_vecs):\n    #define class scatter matrix for each label of dimension d x d \n    class_scatter = np.zeros((d,d));\n    \n    #run through each row corresponding to a class label and compute the class scatter matrix\n    for row in X_train_std[y_train == label]:\n        #reshape to vectors of dimension d x 1\n        row, mv  = row.reshape(d,1), mv.reshape(d,1);\n        #sum for each row d x d dimensional class matrices\n        class_scatter += (row - mv).dot((row - mv).T);\n    S_W += class_scatter;\n# within class scatter matrix of dimension d x d\nprint(\"Within Class Scatter Matrix: %s x %s\" % (S_W.shape[0], S_W.shape[1]));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ad0b480c5d2cff47770f8e47a055319011be3ab"},"cell_type":"code","source":"print('Class label distribution: %s' % np.bincount(y_train)[1:])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa4f05f7ddcacfd2d8e7ca3fe7b567d95bc52afb"},"cell_type":"markdown","source":"As can be seen above, the classes are not normaly distributed so we need to scale the class scatter matrices before summing them to find the Within Class Scatter Matrix. Dividing the Sum by the number of classes is equivalent to finding the Covariance Matrix which is nothing but the normalized version of the Within Class Scatter Matrix i.e.  \n\n>> S_W scaled = Sum(Si scaled), where Si scaled= (1/n) * Sum(x - mi)(x - mi)T\n>>which is equal to Cov = (1/n) S_W "},{"metadata":{"trusted":true,"_uuid":"30b1d858866209b611be29837a2a1d9cecb19338"},"cell_type":"code","source":"S_W = np.zeros((d,d));\nfor label, mv in zip(range(1,4),mean_vecs):\n    class_scatter = np.cov(X_train_std[y_train == label].T);\n    S_W += class_scatter;\nprint('Scaled Within Class Scatter Matrix: %sx%s' % (S_W.shape[0], S_W.shape[1]));","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d57ac561044fa38246818bbde53ec114ede1715"},"cell_type":"markdown","source":"**Between Class Scatter Matrix**:  \n>>S_B = number sample of class i  * Sum( mi - m)(mi -m).T, i belongs to classes and m is the overall mean including samples from all classes"},{"metadata":{"trusted":true,"_uuid":"a89f624d9002531ba36913b37ba6e13859949ac0"},"cell_type":"code","source":"#calculate the overall mean vector\nmean_overall = np.mean(X_train_std,axis = 0);\n#define Between Class Scatter Matrix of dimension d x d\nS_B = np.zeros((d,d));\nfor i, mean_vec in enumerate(mean_vecs):\n    #find number of samples for each class\n    n = X_train[y_train == i + 1].shape[0];\n    mean_vec = mean_vec.reshape(d,1);\n    mean_overall = mean_overall.reshape(d,1);\n    #find the scatter matrix using the above equation\n    S_B += n * (mean_vec - mean_overall).dot((mean_vec - mean_overall).T);\nprint('Between Class Scatter Matrix: %sx%s' % (S_B.shape[0], S_B.shape[1]));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8db9be414b7d3547da7c0a2d7565c3ee9551c311"},"cell_type":"markdown","source":"Step 4: Decompose the Inverse(S_W ) * S_B into eigen-pairs and sort in descending order"},{"metadata":{"trusted":true,"_uuid":"b76039c85b43e8f027057f6603b2b9bc12639ba0"},"cell_type":"code","source":"eigen_vals, eigen_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B));\neigen_pairs  = [(np.abs(eigen_vals[i]),eigen_vecs[:,i]) for i in range(len(eigen_vals))];\neigen_pairs = sorted(eigen_pairs, key = lambda k: k[0], reverse = True);\nprint('Eigenvalues in descending order: \\n');\nfor eigen_val in eigen_pairs:\n    print(eigen_val[0]);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ffef84171ad83855ebb1f8ed0e960c03573bc201"},"cell_type":"markdown","source":"As can be seen from the above result that we get at most c-1 linear dicriminants, where c is the number of classes since the inner class scatter matrix S_B is the sum of c matirces with rank 1. The others are way less than zero just because of the numpy's floating point operations."},{"metadata":{"_uuid":"36428ff653dc0c36870cdbaad1c1c813335a3805"},"cell_type":"markdown","source":"Let's now plot the linear discriminants by decreasing eigenvalues to check how much class discriminatory information is captured."},{"metadata":{"trusted":true,"_uuid":"72de9fa0a161f6e7b730ab6f9a56f828515037bd"},"cell_type":"code","source":"tot = sum(eigen_vals.real)\ndiscr = [(i / tot) for i in sorted(eigen_vals.real,reverse=True)]\ncum_discr = np.cumsum(discr)\nplt.bar(range(1, 14), discr, alpha=0.5, align='center',label='individual \"discriminability\"')\nplt.step(range(1, 14), cum_discr, where='mid',label='cumulative \"discriminability\"')\nplt.ylabel('\"discriminability\" ratio')\nplt.xlabel('Linear Discriminants')\nplt.ylim([-0.1, 1.1])\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4499dd11d7d3f3a167889a398ca161432ccb392d"},"cell_type":"markdown","source":"Thus the above figure rightly shows that the first two linear dicriminants capture almost 100% of the class-discriminatory information."},{"metadata":{"_uuid":"5f9088f8eb5ccee4496d9bee47efa9ac1648111b"},"cell_type":"markdown","source":"Step 4: Construct the transformation matrix using the top 2 discriminants"},{"metadata":{"trusted":true,"_uuid":"50888c12622b39ab75262e78d844fd7a2a04b790"},"cell_type":"code","source":"#transformation matrix of dimension d x k i.e. 13 x 2 here\nw = np.hstack((eigen_pairs[0][1][:, np.newaxis].real,eigen_pairs[1][1][:, np.newaxis].real))\nprint('Matrix W:\\n', w)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d86fb48ce41e147033e0e371fe833de6ed118572"},"cell_type":"markdown","source":"Step 6: Project the samples onto the new feature sub-space using the transformation matrix"},{"metadata":{"trusted":true,"_uuid":"5d7dd9063403e464c708bfbd2d5a8b42f4d40670"},"cell_type":"code","source":"# Xnew = Xorig.W\nX_train_lda = X_train_std.dot(w)\ncolors = ['r', 'b', 'g']\nmarkers = ['s', 'x', 'o']\nfor l, c, m in zip(np.unique(y_train), colors, markers): plt.scatter(X_train_lda[y_train==l, 0],X_train_lda[y_train==l, 1] * (-1),c=c, label=l, marker=m)\nplt.xlabel('LD 1')\nplt.ylabel('LD 2')\nplt.legend(loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"29457e9f2d182ebaac8c122ee36fa3153e378cdd"},"cell_type":"markdown","source":"The above plot clearly makes the data linearly separable in the new feature subspace using a linear classifier."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}