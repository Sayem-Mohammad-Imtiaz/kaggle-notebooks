{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# HackerNews analysis with newspaper3k\n\n- This notebook is a continuation of our Hacker News analysis.\n- In the [previous installment](https://www.kaggle.com/michapaliski/hackernews-analysis-with-bigquery) we have shown how you can use BigQuery and basic SQL for retrieving HN stories and comments.\n- In this tutorial we build on that and present how you can **collect articles** to which HN stories are referring.\n- We will use a great python library called [newspaper3k](https://github.com/codelucas/newspaper) for **scraping the articles and their metadata**\n\n### First part presents how you can:\n- connect to BQ from the Kaggle kernel\n- run basic SQL queries against the HN dataset\n\n**output: Top30 domains - outlets publishing stories on online privacy which were found worth sharing by HN users**\n\n### Second part focuses on:\n- introducing basic features of the newspaper3k\n- collecting the most popular HN stories related to online privacy issues\n\n**output: collection of popular articles on online privacy and their metadata**\n\n---","metadata":{}},{"cell_type":"markdown","source":"### We use pip for installing newspaper3k. For more details on installation see: https://newspaper.readthedocs.io/en/latest/.","metadata":{}},{"cell_type":"code","source":"pip install newspaper3k","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Please import necessary packages\nimport numpy as np\nimport pandas as pd\nfrom time import sleep\nimport newspaper\nfrom newspaper import Article\nfrom google.cloud import bigquery\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom nltk import FreqDist\nfrom nltk import bigrams\nfrom nltk.stem import SnowballStemmer\nimport itertools\nimport collections","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Basic features of the newspaper3k","metadata":{}},{"cell_type":"code","source":"## lets collect most recent articles from the verge\n\npaper = newspaper.build('https://arstechnica.com/', memoize_articles=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(paper.articles)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"j=0\nfor article in paper.articles[:30]:\n    print(j, article.url)\n    j=j+1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Connect to BQ from the Kaggle kernel\n","metadata":{}},{"cell_type":"code","source":"# Client is needed for configuring API requests. Leaving it empty will initiate Kaggle's public dataset BigQuery integration.\nclient = bigquery.Client()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Construct a reference to the \"hacker_news\" dataset\ndataset_ref = client.dataset(\"hacker_news\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Basic SQL queries against the HN dataset\n\nWe start our analysis with investigating the top domains that HN users use as sources. \n\nSteps:\n1. Extract domains from the stories' urls using regexp. \n2. Exclude stories without urls\n3. Include stories published after '2018-01-01' containing word 'privacy' or 'Privacy' in their titles or texts.\n4. `COUNT `top 30 domains and store the results in the column `c` ","metadata":{}},{"cell_type":"code","source":"# Let's create our first SQL query on HN database. \n\nquery = \"\"\"\n    #standardSQL\n    SELECT REGEXP_EXTRACT(url, '//([^/]*)/?') domain, COUNT(*) c\n    FROM `bigquery-public-data.hacker_news.full`\n    WHERE url!='' AND (REGEXP_CONTAINS(text, r\"(p|P)rivacy\") OR REGEXP_CONTAINS(title, r\"(P|p)rivacy\")) AND timestamp > '2018-01-01' AND type='story' \n    GROUP BY domain ORDER BY c DESC LIMIT 30\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For more details on using BQ see: https://www.kaggle.com/michapaliski/hackernews-analysis-with-bigquery\n\n# Set up the query\nquery_job = client.query(query)\n# API request - run the query, and return a pandas DataFrame\ndf = query_job.to_dataframe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Top30 domains: outlets publishing stories on online privacy which were found worth sharing by HN users  ","metadata":{}},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We build a similar SQL query. But this time we collect all columns.","metadata":{}},{"cell_type":"code","source":"query = \"\"\"\n        SELECT *\n        FROM `bigquery-public-data.hacker_news.full`\n        WHERE (REGEXP_CONTAINS(text, r\"(p|P)rivacy\") OR REGEXP_CONTAINS(title, r\"(P|p)rivacy\")) AND timestamp > '2018-01-01' AND type='story'\n        \"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set up the query\nquery_job = client.query(query)\n# API request - run the query, and return a pandas DataFrame\ndf = query_job.to_dataframe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's see how many stories don't have urls\nprint(df.shape)\ndf=df[~df['url'].isna()]\nprint(df.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Some users might refer to the same story. We get rid of duplicates.\ndf=df.drop_duplicates(subset=['url'])\ndf.reset_index(inplace=True)\nlen(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# An example story\ndf['url'][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We point newspaper to the first story's url\narticle = Article(df['url'][0])\n# Next, we download the source code\narticle.download()\n# we parse the html\narticle.parse()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now we can access different elements of the article like:","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# publication date\narticle.publish_date","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# or article's body\narticle.text[:1000]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can also use some cool nlp features provided by the newspaper3k\narticle.nlp()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# e.g. we can retrieve the keywords\narticle.keywords","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# or use newspaper3k for creating an automated summary of the article for us\narticle.summary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The most popular HN stories related to online privacy issues","metadata":{}},{"cell_type":"markdown","source":"### Now when you are familiar with the basic capabilities of the newspaper3k library. Let's use it for a bulk request. In the following we will try to collect top 1k stories","metadata":{}},{"cell_type":"code","source":"# We sort stories by their score\ndf=df.sort_values('score',ascending=False)[:1000]\nli=df['url'].tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Collect articles and their metadata (authors, titles and publication dates)\n\n# date=[]\n# auths=[]\n# titles=[]\n# text=[]\n\n# for no, l in enumerate(li):\n#     article = Article(l)\n#     try:\n#         article.download()\n#         article.parse()\n#         date.append(article.publish_date)\n#         auths.append(article.authors)\n#         titles.append(article.title)\n#         text.append(article.text)\n#     except:\n#         date.append(np.nan)\n#         auths.append(np.nan)\n#         titles.append(np.nan)\n#         text.append(np.nan)\n#     if no%100==0:\n#         print(no)\n#     sleep(.5)\n\n# res={\n#     'title': titles,\n#     'link':li,\n#     'date':date,\n#     'authors':auths,\n#     'text':text\n#     }\n# df=pd.DataFrame(res)\n\n# df.to_csv('./hn_newspaper.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('../input/hn-newspaper/hn_newspaper(1).csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"We have retrieved texts of: \"+ str((len(df[~df['text'].isna()])/1000)*100)+'% of the stories \\n Not bad!')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data cleaning","metadata":{}},{"cell_type":"code","source":"text=df['text'][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text=word_tokenize(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text=[i.lower() for i in text]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stopword_list=stopwords.words('english')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stopword_list[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text=[i for i in text if i not in stopword_list]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"st = SnowballStemmer('english')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"st.stem('exciting')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text=[st.stem(i) for i in text]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=df.dropna(subset=['text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text_token']=df['text'].apply(lambda x: word_tokenize(x))\ndf['text_token']=df['text_token'].apply(lambda row: [i.lower() for i in row])\ndf['text_token']=df['text_token'].apply(lambda row: [i for i in row if i not in stopword_list and len(i)>1] )\ndf['text_token_st']=df['text_token'].apply(lambda row: [st.stem(i) for i in row ] )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_frequencies = Counter()\nfor i, row in df.iterrows():\n    counts=Counter(row['text_token_st'])\n    all_frequencies.update(counts)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_frequencies.most_common()[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp=[]\nfor i, row in df.iterrows():\n    for j in row['text_token_st']:\n        \n        temp.append(j)\nCounter(temp).most_common()[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}