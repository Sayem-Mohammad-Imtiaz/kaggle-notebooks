{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os\nimport seaborn as sns\nprint(os.listdir(\"/kaggle/input/zomato-bangalore-restaurants\"))\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport plotly.offline as py\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected=False)\nfrom wordcloud import WordCloud\nfrom geopy.geocoders import Nominatim\nfrom folium.plugins import HeatMap\nimport folium\nfrom tqdm import tqdm\nimport re\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom sklearn.model_selection import train_test_split\nfrom nltk import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport gensim\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport matplotlib.colors as mcolors\nfrom sklearn.manifold import TSNE\nfrom gensim.models import word2vec\nimport nltk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/zomato-bangalore-restaurants/zomato.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"dataset contains {} rows and {} columns\".format(df.shape[0],df.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,7))\nchains=df['name'].value_counts()[:10]\nsns.barplot(x=chains,y=chains.index,palette='deep')\nplt.title(\"Most famous restaurants chains in Bangaluru\")\nplt.xlabel(\"Number of outlets\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=df['online_order'].value_counts()\ncolors = ['#FEBFB3', '#E1396C']\n\ntrace=go.Pie(labels=x.index,values=x,textinfo=\"value\",\n            marker=dict(colors=colors, \n                           line=dict(color='#000000', width=2)))\nlayout=go.Layout(title=\"Accepting vs not accepting online orders\",width=500,height=500)\nfig=go.Figure(data=[trace],layout=layout)\npy.iplot(fig, filename='pie_chart_subplots')\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=df['book_table'].value_counts()\ncolors = ['#96D38C', '#D0F9B1']\n\ntrace=go.Pie(labels=x.index,values=x,textinfo=\"value\",\n            marker=dict(colors=colors, \n                           line=dict(color='#000000', width=2)))\nlayout=go.Layout(title=\"Table booking \",width=500,height=500)\nfig=go.Figure(data=[trace],layout=layout)\npy.iplot(fig, filename='pie_chart_subplots')\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6,5))\nrating=df['rate'].dropna().apply(lambda x : float(x.split('/')[0]) if (len(x)>3)  else np.nan ).dropna()\nsns.distplot(rating,bins=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cost_dist=df[['rate','approx_cost(for two people)','online_order']].dropna()\ncost_dist['rate']=cost_dist['rate'].apply(lambda x: float(x.split('/')[0]) if len(x)>3 else 0)\ncost_dist['approx_cost(for two people)']=cost_dist['approx_cost(for two people)'].apply(lambda x: int(x.replace(',','')))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,7))\nsns.scatterplot(x=\"rate\",y='approx_cost(for two people)',hue='online_order',data=cost_dist)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6,6))\nsns.distplot(cost_dist['approx_cost(for two people)'])\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"votes_yes=df[df['online_order']==\"Yes\"]['votes']\ntrace0=go.Box(y=votes_yes,name=\"accepting online orders\",\n              marker = dict(\n        color = 'rgb(214, 12, 140)',\n    ))\n\nvotes_no=df[df['online_order']==\"No\"]['votes']\ntrace1=go.Box(y=votes_no,name=\"Not accepting online orders\",\n              marker = dict(\n        color = 'rgb(0, 128, 128)',\n    ))\n\nlayout = go.Layout(\n    title = \"Box Plots of votes\",width=800,height=500\n)\n\ndata=[trace0,trace1]\nfig=go.Figure(data=data,layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(7,7))\nrest=df['rest_type'].value_counts()[:20]\nsns.barplot(rest,rest.index)\nplt.title(\"Restaurant types\")\nplt.xlabel(\"count\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trace0=go.Box(y=df['approx_cost(for two people)'],name=\"accepting online orders\",\n              marker = dict(\n        color = 'rgb(214, 12, 140)',\n    ))\ndata=[trace0]\nlayout=go.Layout(title=\"Box plot of approximate cost\",width=800,height=500,yaxis=dict(title=\"Price\"))\nfig=go.Figure(data=data,layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cost_dist=df[['rate','approx_cost(for two people)','location','name','rest_type']].dropna()\ncost_dist['rate']=cost_dist['rate'].apply(lambda x: float(x.split('/')[0]) if len(x)>3 else 0)\ncost_dist['approx_cost(for two people)']=cost_dist['approx_cost(for two people)'].apply(lambda x: int(x.replace(',','')))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def return_budget(location,rest):\n    budget=cost_dist[(cost_dist['approx_cost(for two people)']<=400) & (cost_dist['location']==location) & \n                     (cost_dist['rate']>4) & (cost_dist['rest_type']==rest)]\n    return(budget['name'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"return_budget('BTM',\"Quick Bites\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(7,7))\nRest_locations=df['location'].value_counts()[:20]\nsns.barplot(Rest_locations,Rest_locations.index,palette=\"rocket\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_1=df.groupby(['location','cuisines']).agg('count')\ndata=df_1.sort_values(['url'],ascending=False).groupby(['location'],\n                as_index=False).apply(lambda x : x.sort_values(by=\"url\",ascending=False).head(3))['url'].reset_index().rename(columns={'url':'count'})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"locations=pd.DataFrame({\"Name\":df['location'].unique()})\nlocations['Name']=locations['Name'].apply(lambda x: \"Bangalore \" + str(x))\nlat_lon=[]\ngeolocator=Nominatim(user_agent=\"app\")\nfor location in locations['Name']:\n    location = geolocator.geocode(location)\n    if location is None:\n        lat_lon.append(np.nan)\n    else:    \n        geo=(location.latitude,location.longitude)\n        lat_lon.append(geo)\n\n\nlocations['geo_loc']=lat_lon\nlocations.to_csv('locations.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"locations[\"Name\"]=locations['Name'].apply(lambda x :  x.replace(\"Bangalore\",\"\")[1:])\nlocations.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Rest_locations=pd.DataFrame(df['location'].value_counts().reset_index())\nRest_locations.columns=['Name','count']\nRest_locations=Rest_locations.merge(locations,on='Name',how=\"left\").dropna()\nRest_locations['count'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generateBaseMap(default_location=[12.97, 77.59], default_zoom_start=12):\n    base_map = folium.Map(location=default_location, control_scale=True, zoom_start=default_zoom_start)\n    return base_map","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lat,lon=zip(*np.array(Rest_locations['geo_loc']))\nRest_locations['lat']=lat\nRest_locations['lon']=lon\nbasemap=generateBaseMap()\nHeatMap(Rest_locations[['lat','lon','count']].values.tolist(),zoom=20,radius=15).add_to(basemap)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"basemap","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(7,7))\ncuisines=df['cuisines'].value_counts()[:10]\nsns.barplot(cuisines,cuisines.index)\nplt.xlabel('Count')\nplt.title(\"Most popular cuisines of Bangalore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def produce_data(col,name):\n    data= pd.DataFrame(df[df[col]==name].groupby(['location'],as_index=False)['url'].agg('count'))\n    data.columns=['Name','count']\n    print(data.head())\n    data=data.merge(locations,on=\"Name\",how='left').dropna()\n    data['lan'],data['lon']=zip(*data['geo_loc'].values)\n    return data.drop(['geo_loc'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"North_India=produce_data('cuisines','North Indian')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"basemap=generateBaseMap()\nHeatMap(North_India[['lan','lon','count']].values.tolist(),zoom=20,radius=15).add_to(basemap)\nbasemap","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"food=produce_data('cuisines','South Indian')\nbasemap=generateBaseMap()\nHeatMap(food[['lan','lon','count']].values.tolist(),zoom=20,radius=15).add_to(basemap)\nbasemap","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def produce_chains(name):\n    data_chain=pd.DataFrame(df[df[\"name\"]==name]['location'].value_counts().reset_index())\n    data_chain.columns=['Name','count']\n    data_chain=data_chain.merge(locations,on=\"Name\",how=\"left\").dropna()\n    data_chain['lan'],data_chain['lon']=zip(*data_chain['geo_loc'].values)\n    return data_chain[['Name','count','lan','lon']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_1=df.groupby(['rest_type','name']).agg('count')\ndatas=df_1.sort_values(['url'],ascending=False).groupby(['rest_type'],\n                as_index=False).apply(lambda x : x.sort_values(by=\"url\",ascending=False).head(3))['url'].reset_index().rename(columns={'url':'count'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mapbox_access_token=\"pk.eyJ1Ijoic2hhaHVsZXMiLCJhIjoiY2p4ZTE5NGloMDc2YjNyczBhcDBnZnA5aCJ9.psBECQ2nub0o25PgHcU88w\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"casual=datas[datas['rest_type']=='Casual Dining']\ncasual","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def produce_trace(data_chain,name):\n        data_chain['text']=data_chain['Name']+'<br>'+data_chain['count'].astype(str)\n        trace =  go.Scattermapbox(\n           \n                lat=data_chain['lan'],\n                lon=data_chain['lon'],\n                mode='markers',\n                marker=go.scattermapbox.Marker(\n                    size=data_chain['count']*4\n                ),\n                text=data_chain['text'],name=name\n            )\n        \n        return trace\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=[] \nfor row in casual['name']:\n    data_chain=produce_chains(row) \n    trace_0=produce_trace(data_chain,row)\n    data.append(trace_0)\n\n\n\nlayout = go.Layout(title=\"Casual Dining Restaurant chains locations around Banglore\",\n    autosize=True,\n    hovermode='closest',\n    mapbox=dict(\n        accesstoken=mapbox_access_token,\n        bearing=0,style=\"streets\",\n        center=dict(\n            lat=12.96,\n            lon=77.59\n        ),\n        pitch=0,\n        zoom=10\n    ),\n)\n\n\nfig = dict(data=data, layout=layout)\npy.iplot(fig, filename='Montreal Mapbox')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"quick=datas[datas['rest_type']=='Quick Bites']\nquick ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=[]  \nfor row in quick['name']:\n    data_chain=produce_chains(row) \n    trace_0=produce_trace(data_chain,row)\n    data.append(trace_0)\n\n\n\nlayout = go.Layout(title=\"Quick Bites Restaurant chains locations around Banglore\",\n    autosize=True,\n    hovermode='closest',\n    mapbox=dict(\n        accesstoken=mapbox_access_token,\n        bearing=0,style=\"streets\",\n        center=dict(\n            lat=12.96,\n            lon=77.59\n        ),\n        pitch=0,\n        zoom=10\n    ),\n)\n\n\nfig = dict(data=data, layout=layout)\npy.iplot(fig, filename='Montreal Mapbox')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cafe=datas[datas['rest_type']=='Cafe']\ncafe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=[]  \nfor row in cafe['name']:\n    data_chain=produce_chains(row) \n    trace_0=produce_trace(data_chain,row)\n    data.append(trace_0)\n\n\n\nlayout = go.Layout(title=\"Cafe Restaurant chains locations around Banglore\",\n    autosize=True,\n    hovermode='closest',\n    mapbox=dict(\n        accesstoken=mapbox_access_token,\n        bearing=0,style=\"streets\",\n        center=dict(\n            lat=12.96,\n            lon=77.59\n        ),\n        pitch=0,\n        zoom=10\n    ),\n)\n\n\nfig = dict(data=data, layout=layout)\npy.iplot(fig, filename='Montreal Mapbox')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['dish_liked']=df['dish_liked'].apply(lambda x : x.split(',') if type(x)==str else [''])\n#x=df.groupby('rest_type',as_index=False)['dish_liked'].agg('sum')\n#x['dish_liked']=x['dish_liked'].apply(lambda x : list(filter(lambda a : a!='',x)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rest=df['rest_type'].value_counts()[:9].index\ndef produce_wordcloud(rest):\n    \n    plt.figure(figsize=(20,30))\n    for i,r in enumerate(rest):\n        plt.subplot(3,3,i+1)\n        corpus=df[df['rest_type']==r]['dish_liked'].values.tolist()\n        corpus=','.join(x  for list_words in corpus for x in list_words)\n        wordcloud = WordCloud(max_font_size=None, background_color='white', collocations=False,\n                      width=1500, height=1500).generate(corpus)\n        plt.imshow(wordcloud)\n        plt.title(r)\n        plt.axis(\"off\")\n        \n\n        \n        \nproduce_wordcloud(rest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_ratings = []\n\nfor name,ratings in tqdm(zip(df['name'],df['reviews_list'])):\n    ratings = eval(ratings)\n    for score, doc in ratings:\n        if score:\n            score = score.strip(\"Rated\").strip()\n            doc = doc.strip('RATED').strip()\n            score = float(score)\n            all_ratings.append([name,score, doc])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rating_df=pd.DataFrame(all_ratings,columns=['name','rating','review'])\nrating_df['review']=rating_df['review'].apply(lambda x : re.sub('[^a-zA-Z0-9\\s]',\"\",x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rating_df.to_csv(\"Ratings.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rating_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rest=df['name'].value_counts()[:9].index\ndef produce_wordcloud(rest):\n    \n    plt.figure(figsize=(20,30))\n    for i,r in enumerate(rest):\n        plt.subplot(3,3,i+1)\n        corpus=rating_df[rating_df['name']==r]['review'].values.tolist()\n        corpus=' '.join(x  for x in corpus)\n        wordcloud = WordCloud(max_font_size=None, background_color='white', collocations=False,\n                      width=1500, height=1500).generate(corpus)\n        plt.imshow(wordcloud)\n        plt.title(r)\n        plt.axis(\"off\")\n        \n\n        \n        \nproduce_wordcloud(rest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(7,6))\nrating=rating_df['rating'].value_counts()\nsns.barplot(x=rating.index,y=rating)\nplt.xlabel(\"Ratings\")\nplt.ylabel('count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rating_df['sent']=rating_df['rating'].apply(lambda x: 1 if int(x)>2.5 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stops=stopwords.words('english')\nlem=WordNetLemmatizer()\ncorpus=' '.join(lem.lemmatize(x) for x in rating_df[rating_df['sent']==1]['review'][:3000] if x not in stops)\ntokens=word_tokenize(corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vect=TfidfVectorizer()\nvect_fit=vect.fit(tokens)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"id_map=dict((v,k) for k,v in vect.vocabulary_.items())\nvectorized_data=vect_fit.transform(tokens)\ngensim_corpus=gensim.matutils.Sparse2Corpus(vectorized_data,documents_columns=False)\nldamodel = gensim.models.ldamodel.LdaModel(gensim_corpus,id2word=id_map,num_topics=5,random_state=34,passes=25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counter=Counter(corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out=[]\ntopics=ldamodel.show_topics(formatted=False)\nfor i,topic in topics:\n    for word,weight in topic:\n        out.append([word,i,weight,counter[word]])\n\ndataframe = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n\n\n# Plot Word Count and Weights of Topic Keywords\nfig, axes = plt.subplots(2, 2, figsize=(8,6), sharey=True, dpi=160)\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\nfor i, ax in enumerate(axes.flatten()):\n    ax.bar(x='word', height=\"word_count\", data=dataframe.loc[dataframe.topic_id==i, :], color=cols[i], width=0.3, alpha=0.3, label='Word Count')\n    ax_twin = ax.twinx()\n    ax_twin.bar(x='word', height=\"importance\", data=dataframe.loc[dataframe.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n    ax.set_ylabel('Word Count', color=cols[i])\n    #ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 3500)\n    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=8)\n    ax.tick_params(axis='y', left=False)\n    ax.set_xticklabels(dataframe.loc[dataframe.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n\nfig.tight_layout(w_pad=2)    \nfig.suptitle('Word Count and Importance of Topic Keywords', fontsize=8, y=1.05)    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stops=stopwords.words('english')\nlem=WordNetLemmatizer()\ncorpus=' '.join(lem.lemmatize(x) for x in rating_df[rating_df['sent']==0]['review'][:3000] if x not in stops)\ntokens=word_tokenize(corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vect=TfidfVectorizer()\nvect_fit=vect.fit(tokens)\nid_map=dict((v,k) for k,v in vect.vocabulary_.items())\nvectorized_data=vect_fit.transform(tokens)\ngensim_corpus=gensim.matutils.Sparse2Corpus(vectorized_data,documents_columns=False)\nldamodel = gensim.models.ldamodel.LdaModel(gensim_corpus,id2word=id_map,num_topics=5,random_state=34,passes=25)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counter=Counter(corpus)\nout=[]\ntopics=ldamodel.show_topics(formatted=False)\nfor i,topic in topics:\n    for word,weight in topic:\n        out.append([word,i,weight,counter[word]])\n\ndataframe = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n\n\n# Plot Word Count and Weights of Topic Keywords\nfig, axes = plt.subplots(2, 2, figsize=(8,6), sharey=True, dpi=160)\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\nfor i, ax in enumerate(axes.flatten()):\n    ax.bar(x='word', height=\"word_count\", data=dataframe.loc[dataframe.topic_id==i, :], color=cols[i], width=0.3, alpha=0.3, label='Word Count')\n    ax_twin = ax.twinx()\n    ax_twin.bar(x='word', height=\"importance\", data=dataframe.loc[dataframe.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n    ax.set_ylabel('Word Count', color=cols[i])\n    #ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 3500)\n    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=8)\n    ax.tick_params(axis='y', left=False)\n    ax.set_xticklabels(dataframe.loc[dataframe.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n\nfig.tight_layout(w_pad=2)    \nfig.suptitle('Word Count and Importance of Topic Keywords', fontsize=8, y=1.05)    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stops=stopwords.words('english')\nlem=WordNetLemmatizer()\ncorpus=' '.join(lem.lemmatize(x) for x in rating_df[rating_df['sent']==0]['review'][:3000] if x not in stops)\ntokens=word_tokenize(corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vect=TfidfVectorizer()\nvect_fit=vect.fit(tokens)\nid_map=dict((v,k) for k,v in vect.vocabulary_.items())\nvectorized_data=vect_fit.transform(tokens)\ngensim_corpus=gensim.matutils.Sparse2Corpus(vectorized_data,documents_columns=False)\nldamodel = gensim.models.ldamodel.LdaModel(gensim_corpus,id2word=id_map,num_topics=5,random_state=34,passes=25)\n\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counter=Counter(corpus)\nout=[]\ntopics=ldamodel.show_topics(formatted=False)\nfor i,topic in topics:\n    for word,weight in topic:\n        out.append([word,i,weight,counter[word]])\n\ndataframe = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n\n\n# Plot Word Count and Weights of Topic Keywords\nfig, axes = plt.subplots(2, 2, figsize=(8,6), sharey=True, dpi=160)\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\nfor i, ax in enumerate(axes.flatten()):\n    ax.bar(x='word', height=\"word_count\", data=dataframe.loc[dataframe.topic_id==i, :], color=cols[i], width=0.3, alpha=0.3, label='Word Count')\n    ax_twin = ax.twinx()\n    ax_twin.bar(x='word', height=\"importance\", data=dataframe.loc[dataframe.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n    ax.set_ylabel('Word Count', color=cols[i])\n    #ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 3500)\n    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=8)\n    ax.tick_params(axis='y', left=False)\n    ax.set_xticklabels(dataframe.loc[dataframe.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n\nfig.tight_layout(w_pad=2)    \nfig.suptitle('Word Count and Importance of Topic Keywords', fontsize=8, y=1.05)    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stops=set(stopwords.words('english'))\nlem=WordNetLemmatizer()\ncorpus=[]\nfor review in tqdm(rating_df['review'][:10000]):\n    words=[]\n    for x in word_tokenize(review):\n        x=lem.lemmatize(x.lower())\n        if x not in stops:\n            words.append(x)\n            \n    corpus.append(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = word2vec.Word2Vec(corpus, size=100, window=20, min_count=200, workers=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tsne_plot(model):\n    \"Creates and TSNE model and plots it\"\n    labels = []\n    tokens = []\n\n    for word in model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=(10, 10)) \n    for i in range(len(x)):\n        plt.scatter(x[i],y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne_plot(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"postive=rating_df[rating_df['rating']>3]['review'][:2000]\nnegative=rating_df[rating_df['rating']<2.5]['review'][:2000]\n\ndef return_corpus(df):\n    corpus=[]\n    for review in df:\n        tagged=nltk.pos_tag(word_tokenize(review))\n        adj=[]\n        for x in tagged:\n            if x[1]=='JJ':\n                adj.append(x[0])\n        corpus.append(adj)\n    return corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus=return_corpus(postive)\nmodel = word2vec.Word2Vec(corpus, size=100, min_count=10,window=20, workers=4)\ntsne_plot(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus=return_corpus(negative)\nmodel = word2vec.Word2Vec(corpus, size=100, min_count=10,window=20, workers=4)\ntsne_plot(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rating_df['sent']=rating_df['rating'].apply(lambda x: 1 if int(x)>2.5 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features=3000\ntokenizer=Tokenizer(num_words=max_features,split=' ')\ntokenizer.fit_on_texts(rating_df['review'].values)\nX = tokenizer.texts_to_sequences(rating_df['review'].values)\nX = pad_sequences(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embed_dim = 32\nlstm_out = 32\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, embed_dim,input_length = X.shape[1]))\n#model.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(2,activation='softmax'))\n\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = pd.get_dummies(rating_df['sent'].astype(int)).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 3200\nmodel.fit(X_train, Y_train, epochs = 5, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_size = 1500\n\nX_validate = X_test[-validation_size:]\nY_validate = Y_test[-validation_size:]\nX_test = X_test[:-validation_size]\nY_test = Y_test[:-validation_size]\nscore,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\nprint(\"score: %.2f\" % (score))\nprint(\"acc: %.2f\" % (acc))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}