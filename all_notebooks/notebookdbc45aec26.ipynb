{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **The project**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"BANK LOANS DATA\nthe project objective is to perdeict the number of credit problems can occur for a subjective customer","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#import basic libraries to handle data set and plot graphs\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First step is reading the data file in a DATAFRAME to handle it easily in the program \nand check the data to get a close look of how data looks like and what it contains","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#read data set\nloans = pd.read_csv('../input/finance-banking-predication/credit_train.csv')\nloans.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"IF there is any rows full of N/A values delete them so they don't miss guide us\nThen,\nDisplay actual number of rows which contains Data and number of columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"loans.dropna(how='all',inplace=True)\nprint(\"NUmber of data rows are :    \",loans.shape[0])\nprint(\"NUmber of data columns are : \",loans.shape[1])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Display data info and from it we can know an over view about:\nThe data type of each column and its name\nIf there are missing values for each column\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(loans.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here I made a list of column names and their number to helpme through the program \"Not very proffessional can be easier with pd attribuite columns\"\n\nDisplay every column unique numbers and how many are there to better think about the features and their importance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"column_names=list(loans.columns)\ncolumn_len=len(column_names)\nprint('The column names are:',column_names )\nprint( \"Number of clolumns are: \", column_len)\n\nfor c in range(0,column_len):\n  U=loans[column_names[c]].unique()\n  n=len(loans[column_names[c]].unique())\n  print(\"there are \" ,n,  \"      unique value   \")\n  print('the unique values of:   ', column_names[c], \"        are           \", U)\n  print(\"-------------------------------------------------------------------------------------------------------------------\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"count of null values of each column\ndepending on this analysis we conclude to:\n1- Remove \"Months since last delinquent\" Since as obvious more than half data missing\n2- Remove rows that contains null values of \n\"Maximum Open Credit                 2\nBankruptcies                      204\nTax Liens                          10\"  as the missing values are to few as the row data are 100K so that will not affect","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"loans.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loans.dropna(subset=['Maximum Open Credit','Bankruptcies','Tax Liens'],inplace=True)\nloans.drop(['Months since last delinquent'], axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loans.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"TO handle first column \" Credit Score\" \nwe dislay the probability denesity function of the column to choose best value to fill missing data with","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nax = loans['Credit Score'].plot.kde(ind=range(int(loans['Credit Score'].min()),int(loans['Credit Score'].max())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loans['Number of Credit Problems']=loans['Number of Credit Problems'].replace([2,3,4,5,6,7,8,9,10,11,12,15],3)\nloans.boxplot(by ='Number of Credit Problems', column =['Credit Score'], grid = False, figsize=(30,30)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"from the graph we can see that most values are in the zone less than 1000 and the curve is very narrow there\nso I choosed to fill missing value with mode wich appearently in the same zone ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#loans['Credit Score']=loans['Credit Score'].fillna(value=loans['Credit Score'].mode()[0],inplace=True)\nfrom sklearn.impute import SimpleImputer\n\n\nimp = SimpleImputer(missing_values = np.nan, strategy ='most_frequent')\nloans['Credit Score'] = imp.fit_transform(pd.DataFrame(loans['Credit Score']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loans['Credit Score'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hnadle second column Annual income","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#ax = loans['Annual Income'].plot.kde(ind=range(int(loans['Annual Income'].min()),int(loans['Annual Income'].max())))\nz =loans['Annual Income'].value_counts()\nzd=pd.DataFrame(z)\nsns.distplot(loans['Annual Income']);\nprint(zd)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\n\nimp = SimpleImputer(missing_values = np.nan, strategy ='most_frequent')\nloans['Annual Income'] = imp.fit_transform(pd.DataFrame(loans['Annual Income']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loans['Years in current job']=loans['Years in current job'].str.extract('(\\d+)').astype(float)\nax = loans['Years in current job'].plot.kde(ind=range(int(loans['Years in current job'].min()),int(loans['Years in current job'].max())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\n\nimp = SimpleImputer(missing_values = np.nan, strategy ='most_frequent')\nloans['Years in current job'] = imp.fit_transform(pd.DataFrame(loans['Years in current job']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loans.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = loans.drop(['Number of Credit Problems','Customer ID','Loan ID'], axis=1, inplace=False)\ny = loans['Number of Credit Problems']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nobject_column=['Loan Status','Term','Home Ownership','Purpose']\nfor col in object_column:\n       le = preprocessing.LabelEncoder()\n       X[col] = le.fit_transform(X[col])\n       l=list(le.classes_)\n       print('classed found : ' , l)\n       print('Update data is : \\n' ,X[col] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"problems = y.value_counts()\np=pd.DataFrame(problems)\nprint(p)\nprint(y.unique())\np.plot.pie(y=\"Number of Credit Problems\",figsize=(7,7),autopct='%1.1f%%')\nprint(y.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print('Original X Shape is ' , X.shape)\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.feature_selection import chi2 , f_classif \nFeatureSelection = SelectPercentile(score_func = f_classif, percentile=20) # score_func can = f_classif\nX = FeatureSelection.fit_transform(X, y)\n\n#showing X Dimension \n#print('X Shape is ' , X.shape)\n#print('Selected Features are : ' , FeatureSelection.get_support())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler(copy=True, with_mean=True, with_std=True)\nX = scaler.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train , X_test , y_train , y_test = train_test_split(X , y , test_size = 0.3 , random_state = 44,shuffle =True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RandomForestClassifierModel = RandomForestClassifier(criterion = 'gini',n_estimators=100,max_depth=3,random_state=None) #criterion can be also : entropy \nRandomForestClassifierModel.fit(X_train, y_train)\n\n#Calculating Details\nprint('RandomForestClassifierModel Train Score is : ' , RandomForestClassifierModel.score(X_train, y_train))\nprint('RandomForestClassifierModel Test Score is : ' , RandomForestClassifierModel.score(X_test, y_test))\nprint('RandomForestClassifierModel features importances are : ' , RandomForestClassifierModel.feature_importances_)\nprint('----------------------------------------------------')\n#Calculating Prediction\ny_pred = RandomForestClassifierModel.predict(X_test)\ny_pred_prob = RandomForestClassifierModel.predict_proba(X_test)\nprint('Predicted Value for RandomForestClassifierModel is : ' , y_pred[:10])\nprint('Prediction Probabilities Value for RandomForestClassifierModel is : ' , y_pred_prob[:10])\n\n#Calculating Precision Score : (Specificity) #(TP / float(TP + FP))  \n# precision_score(y_true, y_pred, labels=None, pos_label=1, average=’binary’,sample_weight=None)\n\nPrecisionScore = precision_score(y_test, y_pred, average='micro') #it can be : binary,macro,weighted,samples\nprint('Precision Score is : ', PrecisionScore)\n\n#----------------------------------------------------\n#Calculating Recall Score : (Sensitivity) (TP / float(TP + FN))   1 / 1+2  \n# recall_score(y_true, y_pred, labels=None, pos_label=1, average=’binary’, sample_weight=None)\n\nRecallScore = recall_score(y_test, y_pred, average='micro') #it can be : binary,macro,weighted,samples\nprint('Recall Score is : ', RecallScore)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SelectedModel = RandomForestClassifier()\nSelectedParameters = { \n            \"n_estimators\"      : [100,200,300],\n            \"max_depth\"      : [1,2,3],\n            \"min_samples_split\" : [2,4,8],\n            \"bootstrap\": [True, False],\n            }\n\n# #=======================================================================\nGridSearchModel = GridSearchCV(SelectedModel,SelectedParameters, cv = 2,return_train_score=True)\nGridSearchModel.fit(X_train, y_train)\nsorted(GridSearchModel.cv_results_.keys())\nGridSearchResults = pd.DataFrame(GridSearchModel.cv_results_)[['mean_test_score', 'std_test_score', 'params' , 'rank_test_score' , 'mean_fit_time']]\n\n# Showing Results\nprint('All Results are :\\n', GridSearchResults )\nprint('Best Score is :', GridSearchModel.best_score_)\nprint('Best Parameters are :', GridSearchModel.best_params_)\nprint('Best Estimator is :', GridSearchModel.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SVCModel = SVC(kernel= 'poly',# it can be also linear,poly,sigmoid,precomputed\n               max_iter=-1,C=1.0,gamma='auto')\nSVCModel.fit(X_train, y_train)\n\n#Calculating Details\nprint('SVCModel Train Score is : ' , SVCModel.score(X_train, y_train))\nprint('SVCModel Test Score is : ' , SVCModel.score(X_test, y_test))\nprint('----------------------------------------------------')\n\n#Calculating Prediction\ny_pred = SVCModel.predict(X_test)\nprint('Predicted Value for SVCModel is : ' , y_pred[:10])\n#Calculating Precision Score : (Specificity) #(TP / float(TP + FP))  \n# precision_score(y_true, y_pred, labels=None, pos_label=1, average=’binary’,sample_weight=None)\n\nPrecisionScore = precision_score(y_test, y_pred, average='micro') #it can be : binary,macro,weighted,samples\nprint('Precision Score is : ', PrecisionScore)\n\n#----------------------------------------------------\n#Calculating Recall Score : (Sensitivity) (TP / float(TP + FN))   1 / 1+2  \n# recall_score(y_true, y_pred, labels=None, pos_label=1, average=’binary’, sample_weight=None)\n\nRecallScore = recall_score(y_test, y_pred, average='micro') #it can be : binary,macro,weighted,samples\nprint('Recall Score is : ', RecallScore)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"SelectedModel = SVC(gamma='auto')\nSelectedParameters = {'kernel':('poly', 'rbf'), 'C':[1,2,3,4,5]}\nGridSearchModel = GridSearchCV(SelectedModel,SelectedParameters, cv = 2,return_train_score=True)\nGridSearchModel.fit(X_train, y_train)\nsorted(GridSearchModel.cv_results_.keys())\nGridSearchResults = pd.DataFrame(GridSearchModel.cv_results_)[['mean_test_score', 'std_test_score', 'params' , 'rank_test_score' , 'mean_fit_time']]\n\n# Showing Results\nprint('All Results are :\\n', GridSearchResults )\nprint('Best Score is :', GridSearchModel.best_score_)\nprint('Best Parameters are :', GridSearchModel.best_params_)\nprint('Best Estimator is :', GridSearchModel.best_estimator_)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}