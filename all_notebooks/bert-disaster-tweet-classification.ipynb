{"cells":[{"metadata":{},"cell_type":"markdown","source":"## BERT Disaster Tweet Classification\nThis notebook illustrates the ease with which modern APIs (in this case Tensorflow) enable the use of SOTA models for down-stream NLP tasks."},{"metadata":{},"cell_type":"markdown","source":"### Less common installs for BERT pre-trained model"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -q -U tensorflow-text\n!pip install -q -U tf-models-official","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preamble"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow_hub as hub\nimport tensorflow_text as text\nfrom official.nlp import optimization  # to create AdamW optmizer\n\nimport matplotlib.pyplot as plt\n\ntf.get_logger().setLevel('ERROR')\n\nprint('TensorFlow:', tf.__version__)\n\nstrategy = tf.distribute.MirroredStrategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyperparameters and global variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 16\nepochs = 6\ninit_lr = 3e-5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import Tweet Dataset\nTo squeeze all the information we have from the tweet dataset, if the tweet has a location and keyword, that is concatenated at the beginning of the sequence with a semi-colon.\n\nWe split the data into 80% train, and 20% validation. Typically one would partition the training set in k-folds for very reliable results, it is often the case with these larger SOTA models that this is no longer computationally feasible.\n\nWe then apply various BERT models and fine-tune them."},{"metadata":{"trusted":true},"cell_type":"code","source":"AUTOTUNE = tf.data.AUTOTUNE\n\ntrain_ds = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ny = train_ds['target']\n\nx_len = len(train_ds['text'])\n\ntrain_ds[\"keyword\"] = train_ds[\"keyword\"] + \"; \"\ntrain_ds[\"location\"] = train_ds[\"location\"] + \"; \"\n\ntrain_ds[\"keyword\"] = train_ds[\"keyword\"].fillna(\"\")\ntrain_ds[\"location\"] = train_ds[\"location\"].fillna(\"\")\n\ntrain_ds['text'] = train_ds[\"keyword\"] + train_ds[\"location\"] + train_ds['text']\n\ntrain_len = int(0.8 * x_len)\nval_len = int(0.2 * x_len)\n\n\nfull_df = tf.data.Dataset.from_tensor_slices((train_ds['text'],y)).batch(batch_size).cache().prefetch(buffer_size=AUTOTUNE)\ntrain_df = full_df.take(int(train_len/batch_size)).shuffle(buffer_size=int(train_len/batch_size))\nval_df = full_df.skip(int(train_len/batch_size)).shuffle(buffer_size=int(val_len/batch_size))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## BERT Model Selection\nLuckily the people at tensorflow make it as easy as changing a key in a dictionary to select a SOTA pre-trained model"},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_model_name = 'talking-heads_base' \n\nmap_name_to_handle = {\n    'bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n    'bert_en_uncased_L-24_H-1024_A-16':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/3',\n    'bert_en_wwm_uncased_L-24_H-1024_A-16':\n        'https://tfhub.dev/tensorflow/bert_en_wwm_uncased_L-24_H-1024_A-16/3',\n    'bert_en_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n    'bert_en_cased_L-24_H-1024_A-16':\n        'https://tfhub.dev/tensorflow/bert_en_cased_L-24_H-1024_A-16/3',\n    'bert_en_wwm_cased_L-24_H-1024_A-16':\n        'https://tfhub.dev/tensorflow/bert_en_wwm_cased_L-24_H-1024_A-16/3',\n    'bert_multi_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n    'albert_en_base':\n        'https://tfhub.dev/tensorflow/albert_en_base/2',\n    'albert_en_large':\n        'https://tfhub.dev/tensorflow/albert_en_large/2',\n    'albert_en_xlarge':\n        'https://tfhub.dev/tensorflow/albert_en_xlarge/2',\n    'albert_en_xxlarge':\n        'https://tfhub.dev/tensorflow/albert_en_xxlarge/2',\n    'electra_small':\n        'https://tfhub.dev/google/electra_small/2',\n    'electra_base':\n        'https://tfhub.dev/google/electra_base/2',\n    'experts_pubmed':\n        'https://tfhub.dev/google/experts/bert/pubmed/2',\n    'experts_wiki_books':\n        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n    'talking-heads_base':\n        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n    'talking-heads_large':\n        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_large/1',\n}\n\nmap_model_to_preprocess = {\n    'bert_en_uncased_L-24_H-1024_A-16':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'bert_en_wwm_cased_L-24_H-1024_A-16':\n        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n    'bert_en_cased_L-24_H-1024_A-16':\n        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n    'bert_en_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n    'bert_en_wwm_uncased_L-24_H-1024_A-16':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'bert_multi_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n    'albert_en_base':\n        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n    'albert_en_large':\n        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n    'albert_en_xlarge':\n        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n    'albert_en_xxlarge':\n        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n    'electra_small':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'electra_base':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'experts_pubmed':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'experts_wiki_books':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'talking-heads_base':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'talking-heads_large':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n}\n\ntfhub_handle_encoder = map_name_to_handle[bert_model_name]\ntfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n\nprint(f'BERT model selected           : {tfhub_handle_encoder}')\nprint(f'Preprocessing model auto-selected: {tfhub_handle_preprocess}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets see what a pre-processed sequence looks like\nBasically this transforms word, or sub-word units into integer tokens, which are typically associated with an embedded vector within the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    bert_preprocess_model = hub.load(tfhub_handle_preprocess)\n\n    test_inp = [train_ds['text'].iloc[0]]\n    print(test_inp)\n    print(bert_preprocess_model.tokenize(test_inp))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build and run BERT"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_bert():\n    bert_preprocess_model = hub.load(tfhub_handle_preprocess)\n    \n    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n    encoder_inputs = preprocessing_layer(text_input)\n    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n    outputs = encoder(encoder_inputs)\n    net = outputs['pooled_output']\n    net = tf.keras.layers.Dropout(0.1)(net)\n    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n    return tf.keras.Model(text_input, net)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    bert = build_bert()\n\n    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n    metrics = [tf.metrics.BinaryAccuracy()]\n\n    steps_per_epoch = train_len\n    num_train_steps = steps_per_epoch * epochs\n    num_warmup_steps = int(0.2*num_train_steps)\n\n    optimizer = optimization.create_optimizer(init_lr=init_lr,\n                                              num_train_steps=num_train_steps,\n                                              num_warmup_steps=num_warmup_steps,\n                                              optimizer_type='adamw')\n\n    bert.compile(optimizer=optimizer,\n                            loss=loss,\n                            metrics=metrics)\n\nbert.summary()\n\nhistory = bert.fit(x=train_df,\n                  validation_data=val_df,\n                  epochs=epochs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dev Performances\n * **bert_multi_cased_L-12_H-768_A-12**\n   * val_loss: 0.4034 - val_binary_accuracy: 0.8239\n   * 16 batch, 1e-5 LR, 3/4 epochs\n * **bert_en_uncased_L-24_H-1024_A-16**\n   * val_loss: 0.4089 - val_binary_accuracy: 0.8311\n   * 16 batch, 1e-5 LR, 3/4 epochs\n * **bert_en_uncased_L-12_H-768_A-12**\n   * val_loss: 0.4176 - val_binary_accuracy: 0.8297\n   * 32 batch, 3-e5 LR, 4/4 epochs\n * **bert_en_cased_L-24_H-1024_A-16**\n   * val_loss: 0.3840 - val_binary_accuracy: 0.8350\n   * 16 batch, 1e-5 LR, 3/4 epochs\n * **electra_base**\n   * val_loss: 0.4147 - val_binary_accuracy: 0.8232\n   * 16 batch, 3e-5 LR, 2/4 epochs\n * **albert_en_xxlarge**\n   * val_loss: 0.4116 - val_binary_accuracy: 0.8393\n   * 8 batch, 3e-5 LR, 3/4 epochs\n * **albert_en large**\n   * val_loss: 0.4517 - val_binary_accuracy: 0.8258\n   * 16 batch, 2e-5 LR, 3/4 epochs\n * **talking_heads_large**\n   * val_loss: 0.4345 - val_binary_accuracy: 0.8420\n   * 8 batch, 2e-5 LR, 2/4 epochs\n\n * **talking_heads_base**\n   * val_loss: 0.3835 - val_binary_accuracy: 0.8369\n   * 32 batch, 2e-5 LR, 4/4 epochs\n   * val_loss: 0.3723 - val_binary_accuracy: 0.8382\n   * 32 batch, 3e-5 LR, 4/4 epochs\n   * val_loss: 0.3844 - val_binary_accuracy: 0.8343\n   * 32 batch, 2e-5 LR, 6/6 epochs"},{"metadata":{},"cell_type":"markdown","source":"### Train model on entire training set"},{"metadata":{"trusted":true},"cell_type":"code","source":"init_lr = 3e-5\nepochs = 3\n\nwith strategy.scope():\n    bert = build_bert()\n\n    steps_per_epoch = train_len + val_len\n    num_train_steps = steps_per_epoch * epochs\n    num_warmup_steps = int(0.2*num_train_steps)\n\n\n    optimizer = optimization.create_optimizer(init_lr=init_lr,\n                                              num_train_steps=num_train_steps,\n                                              num_warmup_steps=num_warmup_steps,\n                                              optimizer_type='adamw')\n\n    bert.compile(optimizer=optimizer,\n                            loss=loss,\n                            metrics=metrics)\n\n    bert.summary()\n\nhistory = bert.fit(x=full_df,epochs=epochs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test set prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ds = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\n\ntest_ds[\"keyword\"] = test_ds[\"keyword\"] + \"; \"\ntest_ds[\"location\"] = test_ds[\"location\"] + \"; \"\n\ntest_ds[\"keyword\"] = test_ds[\"keyword\"].fillna(\"\")\ntest_ds[\"location\"] = test_ds[\"location\"].fillna(\"\")\n\ntest_ds['text'] = test_ds[\"keyword\"] + test_ds[\"location\"] + test_ds['text']\n\ntest_pred = bert.predict(test_ds['text'])\n\ntest_id = test_ds['id']\n\nwith open(\"./bert_talking_heads_predicitions.csv\",\"w+\") as f:\n    f.write(\"id,target\\n\")\n    \n    for i,val in enumerate(test_pred):\n        f.write(\"%d,%d\\n\" % (test_id[i],round(tf.sigmoid(val).numpy()[0])))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}