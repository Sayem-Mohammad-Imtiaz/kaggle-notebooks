{"cells":[{"metadata":{},"cell_type":"markdown","source":" # Predicting Black Friday Sale Values"},{"metadata":{},"cell_type":"markdown","source":"From Kaggle:<br>\n\nA retail company “ABC Private Limited” wants to understand the customer purchase behaviour (specifically, purchase amount) against various products of different categories. They have shared purchase summary of various customers for selected high volume products from last month.<br>\nThe data set also contains customer demographics (age, gender, marital status, citytype, stayincurrentcity), product details (productid and product category) and Total purchaseamount from last month."},{"metadata":{},"cell_type":"markdown","source":"Now, they want to build a model to predict the purchase amount of customer against various products which will help them to create personalized offer for customers against different products"},{"metadata":{},"cell_type":"markdown","source":"## Import Packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings \nimport os\nwarnings.filterwarnings('ignore')\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train=pd.read_csv('../input/black-friday-sales-prediction/train_oSwQCTC (1)/train.csv')\ndata_test=pd.read_csv('../input/black-friday-sales-prediction/test_HujdGe7 (1)/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Due to memory limits of Kaggle notebooks I am forced to subset the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train=data_train[0:10000]\ndata_test=data_test[0:10000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Brief look at the data<br>\nWhat columns do we have? "},{"metadata":{"trusted":false},"cell_type":"code","source":"print(data_train.columns.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What types are they?"},{"metadata":{"trusted":false},"cell_type":"code","source":"print(data_train.dtypes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"View the data "},{"metadata":{"trusted":false},"cell_type":"code","source":"data_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Are there NAs in cols?"},{"metadata":{"trusted":false},"cell_type":"code","source":"data_train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only NAs in product category, but since products can have multiple categories this is okay."},{"metadata":{},"cell_type":"markdown","source":"Is the target variable normally distributed?"},{"metadata":{"trusted":false},"cell_type":"code","source":"import pylab \nimport scipy.stats as stats","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"stats.probplot(data_train['Purchase'], dist=\"norm\", plot=pylab)\npylab.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Histogram"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.hist(data_train['Purchase'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not very normal - let's log transform. "},{"metadata":{"trusted":false},"cell_type":"code","source":"data_train['Purchase_log']=np.log(data_train['Purchase'])\nstats.probplot(data_train['Purchase_log'], dist=\"norm\", plot=pylab)\npylab.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This did not improve the distribution. A boxcox transformation would be more appropriate here..."},{"metadata":{"trusted":false},"cell_type":"code","source":"data_train['Purchase'], lmbda = stats.boxcox(data_train['Purchase'])\nstats.probplot(data_train['Purchase_log'], dist=\"norm\", plot=pylab)\npylab.show()\nplt.hist(data_train['Purchase'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data now appears reasonably normal. We will need to remember to back-transform the outcome using inv boxcox later."},{"metadata":{},"cell_type":"markdown","source":" Formatting categories"},{"metadata":{"trusted":false},"cell_type":"code","source":"category_cols=['Gender', 'Age', 'Occupation', 'City_Category', 'Stay_In_Current_City_Years',\n               'Marital_Status', 'Product_Category_1', 'Product_Category_2', 'Product_Category_3']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for col in category_cols:\n    data_train[col] = pd.Categorical(data_train[col])\n    \nfor col in category_cols:\n    data_test[col] = pd.Categorical(data_test[col])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How many unique values are in each cat? If too many, we may not be able to grab dummies."},{"metadata":{"trusted":false},"cell_type":"code","source":"cat_uniques = pd.DataFrame([[i, len(data_train[i].unique())] for i in data_train[category_cols].columns], columns=['Variable', 'Unique Values']).set_index('Variable')\nprint(cat_uniques)\n    \n### Visualisation\nn=len(category_cols)\nfig,ax = plt.subplots(n,1, figsize=(6,n*2), sharex=True)\nfor i in range(n):\n    plt.sca(ax[i])\n    col = category_cols[i]\n    sns.barplot(x=col, y='Purchase', data=data_train)\n    \n## Encoding\nfrom sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for col in category_cols:\n    data_train[col] = le.fit_transform(data_train[col])\n    \nfor col in category_cols:\n    data_test[col] = le.fit_transform(data_test[col])\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split data"},{"metadata":{"trusted":false},"cell_type":"code","source":"X=data_train.drop('Purchase', axis=1).drop(['Purchase_log', 'User_ID', 'Product_ID'], axis=1)\ny=data_train['Purchase']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Baseline Regression Algo Testing<br>\nImport regressors"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import Lasso, Ridge","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"RMSE is required as the primary performance metric. We'll also define some others."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import r2_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def rmse(y_true, y_preds):\n    return np.sqrt(((y_preds - y_true) ** 2).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"reg_algos = [\n    RandomForestRegressor(),\n    SVR(kernel='rbf'),#\n    DecisionTreeRegressor(),\n    GradientBoostingRegressor(),\n    MLPRegressor(),\n    Ridge(),\n    Lasso()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for algo in reg_algos:\n    algo.fit(X_train, y_train)\n    name = algo.__class__.__name__\n    \n    print(\"_\"*30)\n    print(name)\n    \n    print('****Results****')\n    train_predictions = algo.predict(X_test)\n    \n    # calculate score\n    RMSE=rmse(y_test, train_predictions)\n    r2=r2_score(y_test, train_predictions)\n    MAPE=mean_absolute_percentage_error(y_test, train_predictions)\n    \n    print(\"RMSE: {:.4}\".format(RMSE))\n    print(\"R^2: {:.4}\".format(r2))\n    print(\"MAPE: {:.4}\".format(MAPE))\n    \nprint(\"_\"*30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The top performing algos are GB and RF. This is most likely because there are many interactions between variables which I did not feature engineer.<br>\nLets train and stack these."},{"metadata":{},"cell_type":"markdown","source":"# Tuning Random Forest"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"n_estimators=[100, 500, 1000, 1500, 2000] # Define params\nmax_features=['auto', 'sqrt']\nmax_depth=[1, 10,  50, 100]\nmax_depth.append(None)\nmin_samples_split=[2,5,10,20,50]\nmin_samples_leaf=[1,5,10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"grid_params={'n_estimators':n_estimators,\n             'max_features':max_features,\n             'max_depth':max_depth,\n             'min_samples_split':min_samples_split,\n             'min_samples_leaf':min_samples_leaf}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf=RandomForestRegressor(random_state=40) # Initiate base model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fit RF CV Search"},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_rand = RandomizedSearchCV(estimator=rf, \n                             param_distributions=grid_params, \n                             scoring='neg_root_mean_squared_error',\n                             n_iter=500,\n                             cv=3,\n                             random_state=40,\n                             verbose = 2,n_jobs=-1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_rand.fit(X_train, y_train)\nprint(\"Best parameter (CV score=:\",  rf_rand.best_score_*-1)\nprint(\"Best RF params:\")\nprint (rf_rand.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Make final rf object"},{"metadata":{"trusted":false},"cell_type":"code","source":"tuned_rf=RandomForestRegressor(**rf_rand.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tuning Gradient Boost"},{"metadata":{"trusted":false},"cell_type":"code","source":"learning_rate = [1, 0.5, 0.1, 0.05, 0.01, 0.001]\nn_estimators = [100, 500, 1000, 1500, 2000]\nmax_depths = [1, 10,  50, 100]\nmin_samples_splits = [2,5,10,20,50]\nmin_samples_leafs = [1,5,10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"grid_params={'n_estimators':n_estimators,\n             'learning_rate':learning_rate,\n             'max_depth':max_depth,\n             'min_samples_split':min_samples_split,\n             'min_samples_leaf':min_samples_leaf}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"gb=GradientBoostingRegressor(random_state=40)\ngb_rand = RandomizedSearchCV(estimator=gb, \n                             param_distributions=grid_params, \n                             scoring='neg_root_mean_squared_error',\n                             n_iter=500,\n                             cv=3,\n                             random_state=40,\n                             verbose = 2,n_jobs=-1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"gb_rand.fit(X_train, y_train)\nprint(\"Best parameter (CV score=:\",  gb_rand.best_score_*-1)\nprint(\"Best GB params:\")\nprint (gb_rand.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Make final gb object"},{"metadata":{"trusted":false},"cell_type":"code","source":"tuned_gb=GradientBoostingRegressor(**gb_rand.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Voting Regressor"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.ensemble import VotingRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ensemble_reg = VotingRegressor(estimators=[('tuned_rf',tuned_rf), \n               ('tuned_gb',tuned_gb)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets test how each of these perform on the hold out set"},{"metadata":{"trusted":false},"cell_type":"code","source":"final_algos = [\n    tuned_rf,\n    tuned_gb,\n    ensemble_reg]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"performance=[]\nfor algo in final_algos:\n    \n    algo.fit(X_train, y_train)\n    name = algo.__class__.__name__\n    \n    print(\"_\"*30)\n    print(name)\n    \n    print('****Results****')\n    train_predictions = algo.predict(X_test)\n    \n    # calculate score\n    RMSE=rmse(y_test, train_predictions)\n    r2=r2_score(y_test, train_predictions)\n    MAPE=mean_absolute_percentage_error(y_test, train_predictions)\n    \n    print(\"RMSE: {:.4}\".format(RMSE))\n    print(\"R^2: {:.4}\".format(r2))\n    print(\"MAPE: {:.4}\".format(MAPE))\n    \n    cols=[\"Algo\", \"RMSE\"]\n    performance_df = pd.DataFrame([[name, RMSE]], columns=cols)\n    performance.append(performance_df)\n    \nprint(\"_\"*30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"performance=pd.concat(performance, axis=0).sort_values(by='RMSE')\nbest_algo=performance['Algo'].values[0] # get best algo","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('The best performing algorithm is: ' , best_algo)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"if best_algo=='VotingRegressor':\n    print('Ensembling via voting improved performance')\nelse:\n    print('Ensembling via voting did not improve performance')\n    \n# Assign best algo for final predictions\nif best_algo=='VotingRegressor':\n    final_algo=ensemble_reg\nif best_algo=='GradientBoostingRegressor':\n    final_algo=tuned_gb\nelse:\n    final_algo=tuned_rf\n    \n### Final Predictions\n# Lastly, I will predict on the test data\nuser_ids=data_test['User_ID']\nproduct_ids=data_test['Product_ID']\ndata_test=data_test.drop(['User_ID', 'Product_ID'], axis=1)\nfinal_preds = final_algo.predict(data_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Inverse BoxCox"},{"metadata":{"trusted":false},"cell_type":"code","source":"from scipy.special import inv_boxcox\nfinal_preds=inv_boxcox(final_preds, lmbda)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Format for submission"},{"metadata":{"trusted":false},"cell_type":"code","source":"final_df=pd.DataFrame({'Purchase':final_preds,\n                      'User_ID':user_ids,\n                      'Product_ID':product_ids})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"final_df.to_csv('submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}