{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Анализ тональности отзывов. Часть 1 из 2. Подготовка данных."},{"metadata":{},"cell_type":"markdown","source":"# Постановка задачи"},{"metadata":{},"cell_type":"markdown","source":"В этом задании вам нужно воспользоваться опытом предыдущих недель, чтобы побить бейзлайн в соревновании по сентимент-анализу отзывов на товары на Kaggle Inclass:\n\nhttps://inclass.kaggle.com/c/product-reviews-sentiment-analysis-light (старый лидерборд)\n\nhttps://www.kaggle.com/c/simplesentiment"},{"metadata":{},"cell_type":"markdown","source":"#### Review criteria:\nВ качестве ответа в этом задании вам нужно загрузить ноутбук с решением и скриншот вашего результата на leaderboard.\n\nУбедитесь, что:\n\n1. ход вашего решения задокументирован достаточно подробно для того, чтобы ваши сокурсники поняли, что вы делали и почему,\n\n2. ваша команда в соревновании состоит только из вас и названа вашим логином на Сoursera, чтобы ваши сокурсники могли понять, что на скриншоте именно ваш результат"},{"metadata":{},"cell_type":"markdown","source":"# Загрузка пакетов и данных"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport json\nprint(np.__version__)\n\nimport os\nprint(os.listdir(\"../input\"))\nnp.random.seed(42)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import nltk\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom random import randint\nfrom time import sleep\nfrom itertools import tee","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Загрузим тренировочную и тестовую выбобрку в датафрейм, а затем объединим в один датафрейм"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/data-sentiment-analysis/products_sentiment_train.tsv', \n                       sep = '\\t', header = None, names = ['text', 'label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv('../input/data-sentiment-analysis/products_sentiment_test.tsv', sep = '\\t')\ndf_test.columns = ['Id', 'text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame({'text': pd.concat([df_train['text'], df_test['text']],axis = 0)})\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{},"cell_type":"markdown","source":"Посмотрим на распределение классов и количество слов в текстах"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df_train['label']);\nplt.title('Train: Target distribution');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Есть небольшой дисбаланс классов: с признаком 1 почти в два раза больше. Можно будет попробовать приемы по исправлению дисбаланса, например, oversampling."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (15, 5));\nax1 = fig.add_subplot(121);\ndf_train['text'].apply(lambda x: len(x.split())).hist(bins = 20);\nplt.title('Train: Number of words');\n\nax2 = fig.add_subplot(122);\ndf_test['text'].apply(lambda x: len(x.split())).hist(bins = 20);\nplt.title('Test: Number of words');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"В трейне и тесте распрделения более или менее похожи."},{"metadata":{},"cell_type":"markdown","source":"Посмотрим на другие характеристики обучающей и тестовой выборках"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Среднее кол-во слов в обучении и тесте:', \n      df_train['text'].apply(lambda x: len(x.split())).mean(), ' и ',\n      df_test['text'].apply(lambda x: len(x.split())).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Медианы кол-ва слов в обучении и тесте:', \n      df_train['text'].apply(lambda x: len(x.split())).median(), ' и ',\n      df_test['text'].apply(lambda x: len(x.split())).median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Мера разброса кол-ва слов в обучении и тесте:', \n      df_train['text'].apply(lambda x: len(x.split())).std(), ' и ',\n      df_test['text'].apply(lambda x: len(x.split())).std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Всего слов в трейновой выборке:', df_train['text'].apply(lambda x: len(x.split())).sum())\nprint('Всего слов в тестовой выборке:', df_test['text'].apply(lambda x: len(x.split())).sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Взглянем на несколько случайных примеров."},{"metadata":{"trusted":true},"cell_type":"code","source":"indices = np.random.randint(low = 0, high = len(df_train), size = 20)\ndf_train.iloc[indices]['text'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Все буквы приведены к нижнему регистру. <br>\nЕсть также небуквенные символы, которые будем обрабатывать в дальнейшем. <br>\nМожно также попробовать заметить опечатки, поэтому качество текстов можно будет попробовать улучшить пакетом, который сопоставляет правильные слова ошибочным, например pyspellchecker или autocorrect.spell"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessor(text):\n    text = re.sub('<[^>]*>', '', text)\n    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n    text = (re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', ''))\n    return text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Обогащение текстов\nОбогатим обучающую выборку комментариями полученными с помощью аугментации через API Google translator. А именно первым действие сделаем перевод с английского на испанский (или русский), а вторым действием полученное предложение переведем обратно на оригинальный язык."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n!pip install googletrans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from googletrans import Translator\ntranslator = Translator(service_urls=['translate.google.com',  'translate.google.es', 'translate.google.ru'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"У гугл переводчика есть особенности и он выдает ошибку на некоторые комбинации спецсимволов. Поэтому сделаем функцию по замене этих символов."},{"metadata":{"trusted":true},"cell_type":"code","source":"def substitute_special_symb(mystring):\n    return re.sub('&#', '', mystring)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Посмотрим на одном случайном примере как это работает:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Оригинальный вариант:', 'all in all i am pleased with the router .')\n\ntranslations_es = translator.translate(['all in all i am pleased with the router .'], dest='es')\ntranslations_en = translator.translate([d.text for d in translations_es], dest='en')\n\nfor translation in translations_en:\n    print('Аугментированный текст:', translation.text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Оригинальный вариант:', 'all in all i am pleased with the router .')\n\ntranslations_ru = translator.translate(['all in all i am pleased with the router .'], dest='ru')\ntranslations_en = translator.translate([d.text for d in translations_ru], dest='en')\n\nfor translation in translations_en:\n    print('Аугментированный текст:', translation.text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Сделаем запросы в гугл переводчик партиями (размером chunk_size) через искусственные паузы (sleep(randint(1, 2))), чтобы не перегружать их систему, а распределить нагрузку равномерно. <br>\nБудем использовать для преобразования испанский язык, т.к. он представляется наиболее представленным в гугл переводчике"},{"metadata":{"trusted":true},"cell_type":"code","source":"# augmented_train = []\n# chunk_size = 50\n\n# for i in range(38, np.int(len(df_train)/chunk_size)):\n#    translations_es = translator.translate(df_train['text'].apply(substitute_special_symb).tolist()[chunk_size*i:chunk_size*(i+1)], dest='es')\n#    sleep(randint(1, 5))\n#    translations_en = translator.translate([d.text for d in translations_es], dest='en')\n#    sleep(randint(1, 5))\n\n#    for translation in translations_en:\n#        augmented_train.append(translation.text)\n        \n#    print(i+1, ' iteration has been ended')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Эта процедура была проделана лишь раз. Результаты были сохранены в файлы и далее в коде будет подгружаться файлы. Это быстрее и не надо лишний раз обращаться к гугл транслейту и нагружать их ресурсы."},{"metadata":{},"cell_type":"markdown","source":"Сохраним аугментированный текст для дальнейшего использования без вызова google translate и в дальнейшем будем использовать только этот файл, чтобы не обращаться к гугл транслейт еще раз."},{"metadata":{"trusted":true},"cell_type":"code","source":"# with open('augmented_train.txt', 'w') as f:\n#     for item in augmented_train:\n#         f.write(\"%s\\n\" % item)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Сделаем тоже самое для тестовой выборки. Есть идея, что после преобразования гугл переводчиком опечатки будут исправлены и качество данных может быть улучшено и соответственно предсказания будут точнее."},{"metadata":{"trusted":true},"cell_type":"code","source":"# augmented_test = []\n# chunk_size = 50\n\n# for i in range(8, np.int(len(df_test)/chunk_size)):\n#     translations_es = translator.translate(df_test['text'].apply(substitute_special_symb).tolist()[chunk_size*i:chunk_size*(i+1)], dest='es')\n#     sleep(randint(1, 4))\n#     translations_en = translator.translate([d.text for d in translations_es], dest='en')\n#     sleep(randint(1, 4))\n\n#     for translation in translations_en:\n#         augmented_test.append(translation.text)\n        \n#     print(i+1, ' iteration has been ended')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# with open('augmented_test.txt', 'w') as f:\n#     for item in augmented_test:\n#         f.write(\"%s\\n\" % item)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Загружаем ранее аугментированный текст двумя порциями - обучающей и тестовой"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input/augmented-data\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_augmented = pd.read_csv('../input/augmented-data/augmented_train.txt', sep = '\\n', \n                              header = None, names = ['text'])\ndf_train_augmented['label'] = df_train.label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_augmented = pd.read_csv('../input/augmented-data/augmented_test.txt', sep = '\\n', \n                              header = None, names = ['text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Добавляем аугментированный текст в датафрейм"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text_translated'] = pd.DataFrame({'text': pd.concat([df_train_augmented['text'], df_test_augmented['text']], axis = 0)})\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Обогатим текст с помощью предобученных веторов word2vec\nДля каждого слова из нашего текста будем добавлять ближайшие синонимы (соседи) согласно косинусной меры между их векторным представлением"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n!pip install autocorrect","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n!pip install pyspellchecker","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import Word2Vec, KeyedVectors\nfrom autocorrect import spell, Speller\nfrom spellchecker import SpellChecker","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Загрузим word2vec вектора, обученные на твитах и гугл новостях"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n# path = '../input/gensim-word-vectors/'\nGLOVE_TWITTER = '../input/gensim-word-vectors/glove-twitter-100/glove-twitter-100'\ntwitter_model = KeyedVectors.load_word2vec_format(GLOVE_TWITTER)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\nGOOGLE_NEWS = '../input/googles-trained-word2vec-model-in-python/GoogleNews-vectors-negative300.bin.gz'\nnews_model = KeyedVectors.load_word2vec_format(GOOGLE_NEWS, binary=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Напишем функцию, которая получает на вход фразу и для каждого слова из фразы подбирает 5 ближайших соседей в векторном пространстве по косинусной мере. Если слово из входящей фразы не найдено в векторном пространстве, то пытаемся найти ошибку в написании и исправить пакетом autocorrect и далее по исправленному слову ищем соседей в векторном пространстве:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_similar_bag_of_words_ordered(phrase, topn = 5, model = twitter_model):  \n    bag_words = []\n    ordered_bag_words = []\n    # считаем вектор по каждому слову из фразы\n    for word in phrase.split():\n        try:\n            bag_words.append([item[0] for item in model.most_similar([word], topn = topn)]) \n        except KeyError:\n            try:\n                subbag = [item[0]  for item in model.most_similar([spell(word)], topn = topn-1)]\n                subbag.append(spell(word))\n                bag_words.append(subbag)\n            except KeyError:\n                continue\n                \n    for i in range(topn):\n        for w in bag_words:\n            ordered_bag_words.append(w[i])\n        \n    return ' '.join([w for w in ordered_bag_words])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_similar_bag_of_words_ordered('treee grow slowly')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Функция для обработки текстов\nPreprocessor - приводит к нижнему регистру, убирает знаки препинания и т.д."},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessor(text):\n    text = re.sub('<[^>]*>', '', text)\n    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n    text = (re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', ''))\n    return text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Применим наши функцию к текстам"},{"metadata":{},"cell_type":"markdown","source":"с моделью на основе твитов"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n%%capture\n\n# на основе твитов\ndf['text_w2v_twitter'] = df['text'].apply(preprocessor).apply(get_similar_bag_of_words_ordered)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"с моделью на основе новостей"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n%%capture\n\n# на основе новостей\ndef get_similar_bag_of_words_news_ordered(phrase):\n    return get_similar_bag_of_words_ordered(phrase = phrase, topn = 5, model = news_model)\n\ndf['text_w2v_news'] = df['text'].apply(preprocessor).apply(get_similar_bag_of_words_news_ordered)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Проверяем содержание полученного датафрейма"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Сохраняем полученный датафрейм в файл"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_csv('enriched_train_test_text.csv', index = 'false')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Идеи, которые не успел попробовать\n\n1. Расширить выборку train. Например, датасетом из imdb (from keras.datasets import imdb). \n\n2. Для обогащения выборки использовать модель word2vec обученную на биграмах - https://www.kaggle.com/s4sarath/word2vec-unigram-bigrams-"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}