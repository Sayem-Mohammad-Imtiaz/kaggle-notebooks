{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport itertools\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_circles\nfrom mlxtend.plotting import plot_decision_regions\nimport seaborn.apionly as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n%matplotlib notebook\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"rng = np.random.RandomState(0)\niris = pd.read_csv(\"../input/iris/Iris.csv\", index_col=[\"Id\"])\nX_circle, y_cicle = make_circles(n_samples=1000, random_state=123, noise=0.1, factor=0.2)\nX_regres = 100 * rng.rand(100, 1) + 10\nX_xor = rng.randn(300, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iris","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(iris, hue=\"Species\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = {\n    \"regresión\": {\n        \"X\": X_regres,\n        \"y\": 200 + 1500 * X_regres[:, 0] + rng.rand(X_regres.shape[0]) * 50000\n    },\n    \"regresión2\": {\n        \"X\": X_regres,\n        \"y\": 200 + X_regres[:, 0] ** 4 + rng.rand(X_regres.shape[0]) * 50000000\n    },\n    \"iris\": {\n        \"X\": iris.drop(\"Species\", axis=1).values[:, [1, 2]],\n        \"y\": iris.Species.astype(\"category\").cat.codes.values\n    },\n    \"xor\": {\n        \"X\": X_xor,\n        \"y\": np.array(np.logical_xor(X_xor[:, 0] > 0, X_xor[:, 1] > 0), dtype=int)\n    },\n    \"circulo\": {\n        \"X\": X_circle,\n        \"y\": y_cicle\n    }   \n\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Regresión Lineal\n\n### Tarea: sólo regresión\n\n### Modelo: $$\\hat{y} = \\sum_{i=0}^p{\\theta_i . x_i}$$\n\n### Costo: $$\\sum_{i=0}^n{(\\hat{y_i} - y_i)^2} $$"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nplt.rcParams['figure.figsize'] = (15, 12)\n\ntipo = \"regresión\"\nmodel = LinearRegression()\nmodel.fit(data[tipo][\"X\"], data[tipo][\"y\"])\nplt.scatter(data[tipo][\"X\"], data[tipo][\"y\"])\nplt.plot(np.linspace(0, 120), model.predict(np.linspace(0, 120)[:, None]))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tipo = \"regresión2\"\nmodel = LinearRegression()\nmodel.fit(data[tipo][\"X\"], data[tipo][\"y\"])\nplt.scatter(data[tipo][\"X\"], data[tipo][\"y\"])\nplt.plot(np.linspace(0, 120), model.predict(np.linspace(0, 120)[:, None]))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ejercicio: implementar una regresión lineal con mínimos cuadrados ordinarios\n\n$$ \\mathbf{\\theta} = \\left( \\mathbf{X}^{'}\\mathbf{X} \\right)^{-1} \\mathbf{X}^{'}\\mathbf{y} $$\n\ninversa: np.linalg.inv\n\ntraspuesta: .T\n\nproducto interno: .dot"},{"metadata":{"trusted":true},"cell_type":"code","source":"def getOLSCoef(X, y):\n    X = np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)\n    alpha = 0\n    beta = 0\n    return alpha, beta\n\ngetOLSCoef(data[\"regresión\"][\"X\"], data[\"regresión\"][\"y\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Regresión Logistica\n\n### Tarea: sólo clasificación\n\n### Modelo: $$\\hat{y} = \\frac{1}{1 + e^{-\\sum_{i=0}^p{\\theta_i . x_i}}}$$\n\n### Costo: $$\\sum_{i=0}^n{y_i . \\log{(\\hat{y_i} + \\epsilon)} + (1 - y_i) . \\log{(\\hat{1 - y_i + \\epsilon) }}} $$"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\ngs = gridspec.GridSpec(2, 2)\n\nfor tipo, grd  in zip([\"iris\", \"xor\", \"circulo\"], itertools.product([0, 1], repeat=2)):\n    clf = LogisticRegression()\n    clf.fit(data[tipo][\"X\"], data[tipo][\"y\"])\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=data[tipo][\"X\"], y=data[tipo][\"y\"], clf=clf, legend=2)\n    plt.title(tipo)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNN\n\n### Tareas: regresión y clasificación\n\n### Modelo: no hay, basado en memoria\n\n### Parámetros: cantidad de vecinos / radio del vecindario\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n\ngs = gridspec.GridSpec(2, 3)\n\nfor (tipo, clfClass), grd  in zip([(\"regresión\", KNeighborsRegressor),\n                                   (\"regresión2\", KNeighborsRegressor),\n                                   (\"iris\", KNeighborsClassifier),\n                                   (\"xor\", KNeighborsClassifier),\n                                   (\"circulo\", KNeighborsClassifier)], itertools.product([0, 1, 2], repeat=2)):\n    clf = clfClass()\n    clf.fit(data[tipo][\"X\"], data[tipo][\"y\"])\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    try:\n        fig = plot_decision_regions(X=data[tipo][\"X\"], y=data[tipo][\"y\"], clf=clf, legend=2)\n    except:\n        plt.scatter(data[tipo][\"X\"], data[tipo][\"y\"])\n        plt.plot(np.linspace(0, 120), clf.predict(np.linspace(0, 120)[:, None]))\n    plt.title(tipo)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Naive Bayes\n\n### Tarea: sólo clasficación\n\n### Modelo: Busca encontrar la probabilidad de la variable explicada condicionada a las variables explicativas, para ello parte del teorema de bayes\n\n$$P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) P(x_1, \\dots x_n \\mid y)}\n                                 {P(x_1, \\dots, x_n)}$$\n                                 \n### La parte \"Naive\" viene de asumir muy \"inocentemente\" que todas las variables explicativas son independientes entre si, ergo:\n\n$$P(x_i | y, x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_n) = P(x_i | y)$$\n\n$$P(x_1, \\dots x_n \\mid y) = \\prod_{i=1}^{n} P(x_i \\mid y)$$\n\n$$P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) \\prod_{i=1}^{n} P(x_i \\mid y)}\n                                 {P(x_1, \\dots, x_n)}$$"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\ngs = gridspec.GridSpec(2, 2)\n\nfor tipo, grd  in zip([\"iris\", \"xor\", \"circulo\"], itertools.product([0, 1], repeat=2)):\n    clf = GaussianNB()\n    clf.fit(data[tipo][\"X\"], data[tipo][\"y\"])\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=data[tipo][\"X\"], y=data[tipo][\"y\"], clf=clf, legend=2)\n    plt.title(tipo)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Machines\n\n### Tareas: regresión y clasificación\n\n### Busca maximizar los margenes entre clases, cuando no es posible una separación lineal, se puede usar el \"kernel trick\""},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC, LinearSVR\n\ngs = gridspec.GridSpec(2, 3)\n\nfor (tipo, clfClass), grd  in zip([(\"regresión\", LinearSVR),\n                                   (\"regresión2\", LinearSVR),\n                                   (\"iris\", LinearSVC),\n                                   (\"xor\", LinearSVC),\n                                   (\"circulo\", LinearSVC)], itertools.product([0, 1, 2], repeat=2)):\n    clf = clfClass()\n    clf.fit(data[tipo][\"X\"], data[tipo][\"y\"])\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    try:\n        fig = plot_decision_regions(X=data[tipo][\"X\"], y=data[tipo][\"y\"], clf=clf, legend=2)\n    except:\n        plt.scatter(data[tipo][\"X\"], data[tipo][\"y\"])\n        plt.plot(np.linspace(0, 120), clf.predict(np.linspace(0, 120)[:, None]))\n    plt.title(tipo)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC, SVR\n\ngs = gridspec.GridSpec(2, 3)\n\nfor (tipo, clfClass, params), grd  in zip([(\"regresión\", SVR, {\"kernel\": \"linear\"}),\n                                   (\"regresión2\", SVR, {\"kernel\": \"poly\"}),\n                                   (\"iris\", SVC, {}),\n                                   (\"xor\", SVC, {}),\n                                   (\"circulo\", SVC, {})], itertools.product([0, 1, 2], repeat=2)):\n    clf = clfClass(**params)\n    clf.fit(data[tipo][\"X\"], data[tipo][\"y\"])\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    try:\n        fig = plot_decision_regions(X=data[tipo][\"X\"], y=data[tipo][\"y\"], clf=clf, legend=2)\n    except:\n        plt.scatter(data[tipo][\"X\"], data[tipo][\"y\"])\n        plt.plot(np.linspace(0, 120), clf.predict(np.linspace(0, 120)[:, None]))\n    plt.title(tipo)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Árboles de decision\n\n### Tareas: regresión y clasificación\n\n### Son metodos de inducción no paramétricos (no hay modelo tipo ecuación). Se basan en la creación \"greedy\" de simples reglas de decisión que permitan modelar el problema."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n\ngs = gridspec.GridSpec(2, 3)\n\nfor (tipo, clfClass), grd  in zip([(\"regresión\", DecisionTreeRegressor),\n                                   (\"regresión2\", DecisionTreeRegressor),\n                                   (\"iris\", DecisionTreeClassifier),\n                                   (\"xor\", DecisionTreeClassifier),\n                                   (\"circulo\", DecisionTreeClassifier)], itertools.product([0, 1, 2], repeat=2)):\n    clf = clfClass()\n    clf.fit(data[tipo][\"X\"], data[tipo][\"y\"])\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    try:\n        fig = plot_decision_regions(X=data[tipo][\"X\"], y=data[tipo][\"y\"], clf=clf, legend=2)\n    except:\n        plt.scatter(data[tipo][\"X\"], data[tipo][\"y\"])\n        plt.plot(np.linspace(0, 120), clf.predict(np.linspace(0, 120)[:, None]))\n    plt.title(tipo)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensamble 1: Bagging\n\n### Tareas: regresión y clasificación\n\n### Consiste en promediar una serie de algortimos base, entreganos en un sub-set de casos y variables. Busca disminuir la varianza de la estimación\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble  import RandomForestClassifier, RandomForestRegressor\n\ngs = gridspec.GridSpec(2, 3)\n\nfor (tipo, clfClass), grd  in zip([(\"regresión\", RandomForestRegressor),\n                                   (\"regresión2\", RandomForestRegressor),\n                                   (\"iris\", RandomForestClassifier),\n                                   (\"xor\", RandomForestClassifier),\n                                   (\"circulo\", RandomForestClassifier)], itertools.product([0, 1, 2], repeat=2)):\n    clf = clfClass()\n    clf.fit(data[tipo][\"X\"], data[tipo][\"y\"])\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    try:\n        fig = plot_decision_regions(X=data[tipo][\"X\"], y=data[tipo][\"y\"], clf=clf, legend=2)\n    except:\n        plt.scatter(data[tipo][\"X\"], data[tipo][\"y\"])\n        plt.plot(np.linspace(0, 120), clf.predict(np.linspace(0, 120)[:, None]))\n    plt.title(tipo)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensamble 1: Boosting (Adaboosting & GraientBoosting)\n\n### Tareas: regresión y clasificación\n\n### Son metodos que también agregan clasificadores base, pero en lugar de promediarlos, van ajustando los pesos de los posteriores, basados en los erroes de los anteriores"},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier, LGBMRegressor\n\ngs = gridspec.GridSpec(2, 3)\n\nfor (tipo, clfClass), grd  in zip([(\"regresión\", LGBMRegressor),\n                                   (\"regresión2\", LGBMRegressor),\n                                   (\"iris\", LGBMClassifier),\n                                   (\"xor\", LGBMClassifier),\n                                   (\"circulo\", LGBMClassifier)], itertools.product([0, 1, 2], repeat=2)):\n    clf = clfClass()\n    clf.fit(data[tipo][\"X\"], data[tipo][\"y\"])\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    try:\n        fig = plot_decision_regions(X=data[tipo][\"X\"], y=data[tipo][\"y\"], clf=clf, legend=2)\n    except:\n        plt.scatter(data[tipo][\"X\"], data[tipo][\"y\"])\n        plt.plot(np.linspace(0, 120), clf.predict(np.linspace(0, 120)[:, None]))\n    plt.title(tipo)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bonus: auto-tuning y bagging\n\nUniendo las idea de cv, boosting con early stopping y bagging, se puede componer un clasificador de simple ejecución y muy gran poder de predicción, "},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/titanic/train.csv\", index_col=[\"PassengerId\"])\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in data.select_dtypes(\"O\"):\n    data[c] = data[c].astype(\"category\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"NumFam\"] = data.SibSp + data.Parch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(data.NumFam, data.Survived)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(data.NumFam, data.Survived).apply(lambda x: x / x.sum(), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(data.Pclass, data.Survived).apply(lambda x: x / x.sum(), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(data.NumFam, data.Pclass, values=data.Survived, aggfunc=len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(data.NumFam, data.Pclass, values=data.Survived, aggfunc=np.mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import roc_auc_score\n\nX_train, X_test, y_train, y_test = train_test_split(data.drop(\"Survived\", axis=1), \n                                                      data.Survived, test_size=0.1, random_state=2)\nkf = KFold(n_splits=5)\nfolds = [(X_train.iloc[train_idx].index, X_train.iloc[valid_idx].index)\n         for train_idx, valid_idx in kf.split(X_train)]\n\nnum_leaves = list(range(10, 39, 3))\n\nres = pd.DataFrame([], index=[str(d) for d in num_leaves],\n                   columns=[\"fold_\" + str(i) for i in range(len(folds))] + [\"ensamble\"])\n\nfor nl in num_leaves:\n    test_probs = []\n    for i, (train_idx, valid_idx) in enumerate(folds):\n        print(\"doing fold {0} of depth {1}\".format(i + 1, str(nl)))\n        Xt = X_train.loc[train_idx]\n        yt = y_train.loc[train_idx]\n\n        Xv = X_train.loc[valid_idx]\n        yv = y_train.loc[valid_idx]\n\n        learner = LGBMClassifier(n_estimators=10000, num_leaves=nl)\n        learner.fit(Xt, yt, early_stopping_rounds=10, eval_metric=\"auc\",\n                    eval_set=[(Xt, yt), (Xv, yv)], verbose=3)\n        probs = pd.Series(learner.predict_proba(X_test)[:, -1],\n                          index=X_test.index, name=\"fold_\" + str(i))\n        test_probs.append(probs)\n        res.loc[str(nl), \"fold_\" + str(i)] = roc_auc_score(y_test, probs)\n        \n    test_probs = pd.concat(test_probs, axis=1).mean(axis=1)\n    res.loc[str(nl), \"ensamble\"] = roc_auc_score(y_test, test_probs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res.var().sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(learner.feature_importances_, index=X_train.columns).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}