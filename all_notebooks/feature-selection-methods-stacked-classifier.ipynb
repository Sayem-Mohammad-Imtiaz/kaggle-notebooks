{"cells":[{"metadata":{},"cell_type":"markdown","source":"Author: Jaketuricchi","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Training a stacked Ensemble Model to recognise human activity","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The ability to continuous track activity through devices such as a smart phone largely inspires two jusxtaposed responses: one of ethical and\nmoral uncertainty (should this data be collected? and who has access to it?) and one of recognition of the huge potential such data\nmay have when combined with advanced machine learning techniques. Recent work in my PhD lab at The University of Leeds has been \nconcerned with the ability to classify activity and predict energy expenditure resulting in my interest in these kinds of data sets. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Already, it seems like other users are classifying activity with extraordinarily high accuracy (~95%) with very basic models. Next,<br>\nI aim to increase this by using a stacked ensemble model. Since model accuracy is the primary goal and EDA has already been done <br>\nextensively on Kaggle, I will skip EDA and move to","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Set up<br>\nImport, set, read, initial exploration","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Import packages","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport warnings \nimport sklearn\nimport seaborn as sns\nimport matplotlib as plt\n#%matplotlib inline\n#%matplotlib qt\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis,  QuadraticDiscriminantAnalysis\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, log_loss, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import train_test_split\nwarnings.simplefilter('ignore')\npd.set_option('display.max_rows', 1000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Read data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/human-activity-recognition-with-smartphones/train.csv')\ntest_df = pd.read_csv('../input/human-activity-recognition-with-smartphones/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Is the data set ready for modelling?","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"print(train_df.isna().sum()) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have no missing data in the df. For data of this kind this is unrealistic and implies some kind of previous preprocessing and<br>\nimputation.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"print(train_df.dtypes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All data types are numeric, with categorical for the target, so all is in order here.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Modelling - Feature selection","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"With >560 features we have a substantial data frame and it's likely that we can do without many of these. However, given that there<br>\nare a series of different processes that can be used to remove features, I will take multiple approaches and consider the agreement<br>\nbetween these before removal. I will use: \nRF importance;<br>\nGeneric Univariate Selection<br>\nand Model-based selection (i'll use a different algo from RF to differentiate between RF importance)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Note that I use guidance provided in a very useful kernel from Aleksey Bilogur (https://www.kaggle.com/residentmario/automated-feature-selection-with-sklearn)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"To prepare, lets split the data into features and labels.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"X = train_df.drop('Activity', axis=1).reset_index(drop=True)\ny = train_df['Activity']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RF importance","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This involves fitting an RF model and extracting the top n(/%) of the features. It will also give us a good idea of what a basic model<br>\ncan produce in terms of classification accuracy.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"rf = RandomForestClassifier(random_state = 1)\nrf_model_basic = rf.fit(X, y)\nrf_importance = pd.Series(rf_model_basic.feature_importances_, index=X.columns).sort_values(ascending=False)\nrf_importance.nlargest(560).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see the way in which there is an exponential drop in importance of variables. At around 75% there is a slight drop where<br>\nimportance becomes very limited. For this purpose we will remove 25% of the variables","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_variables = rf_importance.head(420).index.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generic Univariate Feature Selection<br>\nHere we must define an arbitrary or % of data to remove. Lets go with 50% of variables for now and see how it compares later.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.feature_selection import GenericUnivariateSelect","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"trans = GenericUnivariateSelect(score_func=lambda X, y: X.mean(axis=0), mode='percentile', param=75)\nchars_X_trans = trans.fit_transform(X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get columns to keep and create new dataframe with those only","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"cols = trans.get_support(indices=True)\nGUS_variables = X.iloc[:,cols].columns.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"By Generic Univariate Selection we keep {1} of our original {0} features\".format(X.shape[1], chars_X_trans.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Based Selection<br>\nHere we will try a couple of different types of classifiers. In order for the SelectFromModel fn to perform, the classifier algo either<br>\n(a) must have a built in feature importance, or we can conduct permutation feature importance <br>\n(see:https://scikit-learn.org/stable/modules/permutation_importance.html#:~:text=The%20permutation%20feature%20importance%20is,model%20depends%20on%20the%20feature. )<br>\nwhich will allow us to provide the feature importance from all clf models. Permutation is very computationally expensive so for the<br>\npresent purposes I will use only those with a built in output.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.inspection import permutation_importance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that model selection<br>\nWe can also do this similarly using an algo such as Decision Tree Classifier:","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"clf = DecisionTreeClassifier()\nselection = SelectFromModel(clf, threshold='3*median', max_features=420)\nvars_selected = selection.fit_transform(X, y)\ncols = selection.get_support(indices=True)\nDT_variables = X.iloc[:,cols].columns.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature selection comparison<br>\nLets begin by converting each of these to lists as it makes matching simpler.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_var_list=list(rf_variables)\ngus_var_list=list(GUS_variables)\ndt_var_list=list(DT_variables)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we have 3 lists of selected features generated by slightly different methods. How many are in common?","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"print(len(set(rf_var_list) & set(gus_var_list) & set (dt_var_list)))\nprint(len(set(gus_var_list) & set (dt_var_list)))\nprint(len(set(rf_var_list)  & set (dt_var_list)))\nprint(len(set(rf_var_list) & set(gus_var_list)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 284 in common of the ~420 selected. This is not driven down by a single method. The implication is that there is no <br>\nclear agreement on which are most important. One way to decide on the best is to test the selections in models.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data splitting","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"X1 = X.filter(items=rf_variables)\nX2 = X.filter(items=GUS_variables)\nX3 = X.filter(items=DT_variables)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X1_train, X1_test, y_train, y_test = train_test_split(X1, y, test_size = 0.2, random_state = 42)\nX2_train, X2_test, y_train, y_test = train_test_split(X2, y, test_size = 0.2, random_state = 42)\nX3_train, X3_test, y_train, y_test = train_test_split(X3, y, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling - Algorithm selection.<br>\nWe'll test a range of classification (clf) models to see which to use in our stacked model. We will avoid using any models too similar<br>\n(e.g. RF and decision trees). Lets start by listing what are (usually) the top performing clfs.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"classifiers = [\n    KNeighborsClassifier(3),\n    SVC(kernel=\"rbf\",probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis(),\n    MLPClassifier()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we'll run a loop over each basic model and store the results for comparison.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def fit_basic_clf (train_features, train_labels, test_features, test_labels):\n    \n    # Log results for performance vis\n    log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\n    log = pd.DataFrame(columns=log_cols)\n    for clf in classifiers:\n        clf.fit(train_features, train_labels)\n        name = clf.__class__.__name__\n        \n        print(\"=\"*30)\n        print(name)\n        \n        print('****Results****')\n        train_predictions = clf.predict(test_features)\n        acc = accuracy_score(test_labels, train_predictions)\n        \n        # calculate score\n        precision = precision_score(test_labels, train_predictions, average = 'macro') \n        recall = recall_score(test_labels, train_predictions, average = 'macro') \n        f_score = f1_score(test_labels, train_predictions, average = 'macro')\n        \n        \n        print(\"Precision: {:.4%}\".format(precision))\n        print(\"Recall: {:.4%}\".format(recall))\n        print(\"F-score: {:.4%}\".format(recall))\n        print(\"Accuracy: {:.4%}\".format(acc))\n        \n        train_predictions = clf.predict_proba(test_features)\n        ll = log_loss(test_labels, train_predictions)\n        print(\"Log Loss: {}\".format(ll))\n        \n        log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols)\n        log = log.append(log_entry)\n        print(\"=\"*30)\n        \n    # Plot results\n    sns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\")\n\n    sns.barplot(x='Log Loss', y='Classifier', data=log, color=\"g\")\n\n        \n# First we'll test on the complete data to see how much accuracy we lose when we drop features\nfit_basic_clf(X1_train, y_train, X1_test, y_test)   \n \n# Now test on subsets\nfit_basic_clf(X1_train, y_train, X1_test, y_test)    \nfit_basic_clf(X2_train, y_train, X2_test, y_test)    \nfit_basic_clf(X3_train, y_train, X3_test, y_test)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are getting extremely well performing models with less features. If the purpose was to have a computationally fast model,<br>\nwe could probably reduce the features by ~50%. Reducing by ~75% reduces accuracy of the best models by ~0.2-0.5%. However, since<br>\nI am aiming to get maximal accuracy here every fraction of a % matters. Therefore I will work with the full feature set.<br>\nIt is an option to tune some parameters but it would likely require an extensive grid search to get a fraction of a percent extra<br>\nso we'll see how stacking goes.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Stacking<br>\nNext, I will pick some different models and stack. I'll go for KNN, RF, NN with XGb as they meta-classifier the 3 top performing models. <br>\nI'll use package mlx, the guidance for which is very useful and can be found at:<br>\nhttp://rasbt.github.io/mlxtend/user_guide/","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from mlxtend.classifier import StackingCVClassifier\nfrom sklearn import model_selection","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"RANDOM_SEED = 42","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=500, random_state = 42)\nknn=KNeighborsClassifier(3)\nNN=MLPClassifier()\nxgb= GradientBoostingClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"stack = StackingCVClassifier(classifiers=[rf, knn, NN],\n                            use_probas=True,\n                            meta_classifier=xgb,\n                            random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we use_probas =T -- this will use probabilities of the 3 classifiers as meta-features in the meta-classifier model","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"print('3-fold cross validation:\\n')\nfor clf, label in zip([rf, knn, NN, stack],\n                      ['Random Forest',\n                       'KNearestNeighbours',\n                       'NeuralNetwork', \n                       'StackingClassifer']):\n    scores = model_selection.cross_val_score(clf, X_train, y_train, \n                                              cv=3, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" \n          % (scores.mean(), scores.std(), label))\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we see that the stacked classifier has improved the accuracy to >99% meaning the stacked model has improved estimates.\nWe can feed the stack grid searches for each classifier, however this would take a lot of time and we're already at >99%, so I'm happy with the final stacked model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Final predictions","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"stack.fit(X,y) # Fit to train","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_test = test_df.drop('Activity', axis=1).reset_index(drop=True)\ny_test = test_df['Activity']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Predict test data","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"final_preds = stack.predict(X_test)\n    \n# calculate final accuracy\nacc = accuracy_score(y_test, final_preds)\nprecision = precision_score(y_test, final_preds, average = 'macro') \nrecall = recall_score(y_test, final_preds, average = 'macro') \nf_score = f1_score(y_test, final_preds, average = 'macro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('The stacked performance on test data:')\nprint(\"Precision: {:.4%}\".format(precision))\nprint(\"Recall: {:.4%}\".format(recall))\nprint(\"F-score: {:.4%}\".format(recall))\nprint(\"Accuracy: {:.4%}\".format(acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Good scores, but could be better with some additional tuning next time round!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}