{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Comparison of Classifiers\n\n## Introduction\n\n__Classification__ is a supervised machine learning problem where the algorithm or model predicts the class of a sample from it's features. Here  the __target__ _y_ is a categorical variable rather than a real variable like in a Regression problem. Statistically, the problem is to build a **decision boundary** for each class which will **separate that class from the rest of the  other classes**. If a classification problem has only two classes, then it is called a __Binary Classification__ problem and in this scenario we only require a single decision boundary to separate both classes.\n\nBased on the nature of the decision boundary, there are mainly two types of classifiers:\n\n- **Linear Classifiers**: (Ex. Logistic Regression, Support Vector Machine with Linear Kernel) The decision boundary is a straight line separating the classes. These models are fairly straightforward and are very popular for their simplicity.\n\n- **Nonlinear Classifiers**: (Ex. k-NN, Decision Tree, Random Forest, Support Vector Machines with nonlinear kernels) This is a much broader and more complex category of classifiers. These models are used for solving complex machine learning problems and also when the data is not separable by a straight line.\n\nIn this Notebook, we will go over some of the widely used classifiers:\n\n1. Logistic Regression\n\n2. Support Vector Machines with Radial Basis Kernel\n\n3. k-Nearest Neighbors\n\n4. Decision Tree Classifier\n\n5. Random Forest Classifier\n\n6. Gaussian Na√Øve Bayes Classifier\n\n## Evaluation of Model\n\nOnce our model is trained, now a question arises, how to evaluate the model? how to compare two different classifiers? \n\nThere are numerus methods to evaluate and compare classification models but the in this notebook, we will be using:\n\n1. Confusion Matrix\n\n2. Sensitivity or Recall\n\n3. Specificity\n\n4. AUC - ROC Curve\n\n## Note\n\nThis notebook is available on:\n \n - [GitHub](https://github.com/arnabd64/classifier-comparison-1)\n - [Kaggle](https://www.kaggle.com/arnabdata/comparison-of-classification-models)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n# Preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n# Train-Test Split\nfrom sklearn.model_selection import train_test_split\n# Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n# For model evaulation\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import roc_curve, auc","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:39:16.522462Z","iopub.execute_input":"2021-06-21T05:39:16.522883Z","iopub.status.idle":"2021-06-21T05:39:17.905174Z","shell.execute_reply.started":"2021-06-21T05:39:16.522805Z","shell.execute_reply":"2021-06-21T05:39:17.904126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Dataset\n\n## About\n\nA car manufacturer wants to build a social media advertisement campaign for a new SUV. The company will use data from it's existing customer database to build a model and use it to target ads to users who have been predicted to purchase an SUV.\n\n## Features\n\nThe dataset contains 400 observations on the following features\n\n1. _User ID_: (`int`) User ID of customer\n2. _Gender_: (`str`) Male or Female\n3. _Age_: (`int`) Age of customer during purchase\n4. _EstimatedSalary_: (`int`) Predicted Salary in USD of customer during purchase\n5. _Purchased_: (`int`) If the customer purchased an SUV then the value is $1$ and $0$ otherwise\n\n__Note:__ The dataset does not contain missing or duplicate observations.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/suv-purchase-decision/SUV_Purchase.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:39:17.907253Z","iopub.execute_input":"2021-06-21T05:39:17.907727Z","iopub.status.idle":"2021-06-21T05:39:17.951442Z","shell.execute_reply.started":"2021-06-21T05:39:17.907682Z","shell.execute_reply":"2021-06-21T05:39:17.950571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:39:17.952846Z","iopub.execute_input":"2021-06-21T05:39:17.953125Z","iopub.status.idle":"2021-06-21T05:39:17.970951Z","shell.execute_reply.started":"2021-06-21T05:39:17.953099Z","shell.execute_reply":"2021-06-21T05:39:17.970052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing\n\n## Cleaning\n\n- _User ID_ does not contribute anything to the model\n- _Gender_ is in String format whoch needs to be transformed to integer","metadata":{}},{"cell_type":"code","source":"# Class Names\nclass_names = {'Not Purchased':0, 'Purchased':1}\n# Remove User ID\ndf.drop('User ID', axis = 1, inplace = True)\n# Encoding Gender\ndf.replace({'Male':1, 'Female':0}, inplace = True)\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:39:17.972232Z","iopub.execute_input":"2021-06-21T05:39:17.972672Z","iopub.status.idle":"2021-06-21T05:39:17.993189Z","shell.execute_reply.started":"2021-06-21T05:39:17.972625Z","shell.execute_reply":"2021-06-21T05:39:17.991958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Scaling\n\nSince classifiers like k-NN, SVM and Logistic Regression are sensitive to the variability of the Features, we must bring all the Features to a comparable scale","metadata":{}},{"cell_type":"code","source":"sc = StandardScaler()\ndf[['Age','EstimatedSalary']] = sc.fit_transform(df.loc[:, ['Age','EstimatedSalary']])\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:39:17.997548Z","iopub.execute_input":"2021-06-21T05:39:17.997917Z","iopub.status.idle":"2021-06-21T05:39:18.018664Z","shell.execute_reply.started":"2021-06-21T05:39:17.997886Z","shell.execute_reply":"2021-06-21T05:39:18.017181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train-Test Split\n\nWe are going to  split the dataset into training set and testing set with a test_size of $20\\%$","metadata":{}},{"cell_type":"code","source":"# variable Separation\nX = df.drop('Purchased', axis = 1).values\ny = df.loc[:, 'Purchased'].values\n# Train test Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n# results\nprint(\"X_train: \",np.shape(X_train))\nprint(\"y_train: \",np.shape(y_train))\nprint('X_test:',np.shape(X_test))\nprint('y_test:',np.shape(y_test))","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:39:18.021683Z","iopub.execute_input":"2021-06-21T05:39:18.022104Z","iopub.status.idle":"2021-06-21T05:39:18.035049Z","shell.execute_reply.started":"2021-06-21T05:39:18.022069Z","shell.execute_reply":"2021-06-21T05:39:18.033892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dimensionality Reduction\n\nHere we want to build a classifier using only 2 features but we have 3. Using __Principal Component Analysis__ we will reduce the 3 features into 2 principal components such that maximum variation is captured by the PCA.","metadata":{}},{"cell_type":"code","source":"# PCA for dimensionality reduction\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 2)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\n## results of \nprint('Variance Captured by PCA')\nfor i in np.arange(pca.n_components_):\n    print(\"PC{0:d}: {1:.1f}%\".format(i+1, 100*pca.explained_variance_ratio_[i]))","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:39:18.036336Z","iopub.execute_input":"2021-06-21T05:39:18.036734Z","iopub.status.idle":"2021-06-21T05:39:18.065557Z","shell.execute_reply.started":"2021-06-21T05:39:18.036702Z","shell.execute_reply":"2021-06-21T05:39:18.064689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining functions for Model evaluation\n\n## AUC-ROC Curve\n\nThe __Receiver Operating Characteristic Curve__ or __ROC Curve__ is visual tool which illustrates the performance of a Binary Classifier. It is a plot where the X-axis is __False Positive Rate (FPR)__ and Y-axis is __True Positive Rate (TPR)__. Thus in a sense it represents the tradeoff between Value (TPR) and Cost (FPR) of a model. Each classification model outputs a probability of given observation being positive, if this probability is greater than a threshold (say, $\\theta$) then the model labels that observation as Positive. If the probability is below $\\theta$ then model labels it as Negative. There can be four outcomes:\n\n1. __TP__: An observation labelled Positive is actually Positive\n2. __FP__: An observation labelled Positive is actually Negative\n3. __TN__: An observation labelled Negative is actually Negative\n4. __FN__: An observation labelled Negative is actually Positive\n\nNow TPR and FPR are defined as:\n\n- $ TPR = \\frac{TP}{TP + FP} $\n- $ FPR = \\frac{FP}{FP + TN} $\n\nWe want a threshold which minimizes FPR without minimizing TPR.","metadata":{}},{"cell_type":"code","source":"def AUC_ROC(model,X_train, X_test, y_train, y_test) :\n    # Training Data\n    y_train_prob = model.predict_proba(X_train)\n    fpr1, tpr1, thres1 = roc_curve(y_train, y_train_prob[:,1], pos_label = 1)\n    auc1 = auc(fpr1, tpr1)\n    # testing Data\n    y_test_prob = model.predict_proba(X_test)\n    fpr2, tpr2, thres2 = roc_curve(y_test, y_test_prob[:,1], pos_label = 1)\n    auc2= auc(fpr2, tpr2)\n    #plotting\n    plt.figure(figsize = (8,6))\n    plt.plot(fpr1, tpr1, label = \"Training: AUC-ROC = {0:.3f}\".format(auc1),\n             color = \"tab:orange\")\n    plt.plot(fpr2, tpr2, label = \"Testing: AUC-ROC = {0:.3f}\".format(auc2),\n             color = 'tab:blue')\n    plt.plot(fpr1, fpr1, label = \"FPR = TPR\", color = 'black', linewidth=0.5)\n    plt.xlabel('False Positive Rate (FPR)')\n    plt.ylabel('True Positive Rate (TPR)')\n    plt.title(\"ROC Curve for \" + model.__class__.__name__)\n    plt.legend(loc = \"lower right\")\n    plt.grid(which = 'both', axis = 'both')\n    plt.show()\n    pass","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:39:18.066765Z","iopub.execute_input":"2021-06-21T05:39:18.06725Z","iopub.status.idle":"2021-06-21T05:39:18.076407Z","shell.execute_reply.started":"2021-06-21T05:39:18.06721Z","shell.execute_reply":"2021-06-21T05:39:18.07555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Confusion Matrix\n\nThe __Confusion Matrix__ is a $2 \\times 2$ matrix which stores all the number of occurences of each of the four outcomes of the Model. The columns represent the actual class labels and the rows represent predcited class labels thus cell representing the count of that outcome. The diagonal elements are correct predictions whereas off-diagonal elements represent error predctions.","metadata":{}},{"cell_type":"code","source":"def cm_Heatmap(y_true, y_pred, class_name = np.unique(y_train)) :\n    # Confusion Matrix\n    cm = confusion_matrix(y_true, y_pred)\n    cm = pd.DataFrame(cm, columns = class_name, index = class_name)\n    # Plotting\n    plt.figure(figsize = (8,6))\n    sns.heatmap(cm, annot = True, annot_kws = {'size': 20}, fmt = 'd', cmap = 'Greens')\n    plt.xlabel('Observed Class')\n    plt.ylabel('Predicted Class')\n    plt.title('Confusion Matrix')\n    plt.show()\n    pass","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:39:18.077679Z","iopub.execute_input":"2021-06-21T05:39:18.077995Z","iopub.status.idle":"2021-06-21T05:39:18.094384Z","shell.execute_reply.started":"2021-06-21T05:39:18.077957Z","shell.execute_reply":"2021-06-21T05:39:18.093328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression\n\nThe logistic regression is called a regression and not classifier because it primarily predicts the probabilitiy of on observation being positive rather than the actual class. It is a linear binary classifier","metadata":{}},{"cell_type":"code","source":"# Training the Model\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train, y_train)\n# Training Reprto\nprint(classification_report(y_train, log_reg.predict(X_train), target_names = class_names))","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:39:18.095713Z","iopub.execute_input":"2021-06-21T05:39:18.096193Z","iopub.status.idle":"2021-06-21T05:39:18.12071Z","shell.execute_reply.started":"2021-06-21T05:39:18.096159Z","shell.execute_reply":"2021-06-21T05:39:18.119801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Confusion Matrix","metadata":{}},{"cell_type":"code","source":"# test data\ncm_Heatmap(y_test, log_reg.predict(X_test), class_names)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:39:18.12227Z","iopub.execute_input":"2021-06-21T05:39:18.122594Z","iopub.status.idle":"2021-06-21T05:39:18.352484Z","shell.execute_reply.started":"2021-06-21T05:39:18.122562Z","shell.execute_reply":"2021-06-21T05:39:18.351646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Classification Report","metadata":{}},{"cell_type":"code","source":"# Classification report\nprint(classification_report(y_test, log_reg.predict(X_test), target_names = class_names))","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:39:18.353812Z","iopub.execute_input":"2021-06-21T05:39:18.354131Z","iopub.status.idle":"2021-06-21T05:39:18.366095Z","shell.execute_reply.started":"2021-06-21T05:39:18.354101Z","shell.execute_reply":"2021-06-21T05:39:18.364817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. AUC-ROC Curve","metadata":{}},{"cell_type":"code","source":"AUC_ROC(log_reg, X_train, X_test, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:39:18.367496Z","iopub.execute_input":"2021-06-21T05:39:18.367933Z","iopub.status.idle":"2021-06-21T05:39:18.555265Z","shell.execute_reply.started":"2021-06-21T05:39:18.367897Z","shell.execute_reply":"2021-06-21T05:39:18.554238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Results (on Test data)\n\n- __Sensitivity__: $77\\%$\n- __Specificity__: $98\\%$\n- __AUC ROC__: $0.973$","metadata":{}},{"cell_type":"markdown","source":"# k-Nearest Neighbors\n\nIt is a non-parametric non-linear classifier. the k-NN model does not need training rather the real computation cost is when we make predictions. Lets say we want to predict the class of a new observation, the model search for the $k$ nearest observations, based on some similarity metric, count the number of observations belonging to each class and label the new observation as the class which has the highest occurence among $k$ observations.","metadata":{}},{"cell_type":"code","source":"# Training model\nknn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(X_train, y_train)\n# Training report\nprint(classification_report(y_train, knn.predict(X_train), target_names = class_names))","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:39:18.556582Z","iopub.execute_input":"2021-06-21T05:39:18.557178Z","iopub.status.idle":"2021-06-21T05:39:18.583979Z","shell.execute_reply.started":"2021-06-21T05:39:18.557127Z","shell.execute_reply":"2021-06-21T05:39:18.582747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Confusion Matrix","metadata":{}},{"cell_type":"code","source":"cm_Heatmap(y_test, knn.predict(X_test), class_names)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:39:18.585516Z","iopub.execute_input":"2021-06-21T05:39:18.585955Z","iopub.status.idle":"2021-06-21T05:39:18.778672Z","shell.execute_reply.started":"2021-06-21T05:39:18.585911Z","shell.execute_reply":"2021-06-21T05:39:18.777731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Classification Report","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test, knn.predict(X_test), target_names = class_names))","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:39:18.779848Z","iopub.execute_input":"2021-06-21T05:39:18.780107Z","iopub.status.idle":"2021-06-21T05:39:18.79624Z","shell.execute_reply.started":"2021-06-21T05:39:18.780083Z","shell.execute_reply":"2021-06-21T05:39:18.79524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. AUC-ROC Curve","metadata":{}},{"cell_type":"code","source":"AUC_ROC(knn, X_train, X_test, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:39:18.797578Z","iopub.execute_input":"2021-06-21T05:39:18.79796Z","iopub.status.idle":"2021-06-21T05:39:18.989944Z","shell.execute_reply.started":"2021-06-21T05:39:18.79793Z","shell.execute_reply":"2021-06-21T05:39:18.988788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Results (on Test data)\n\n- __Sensitivity__: $93\\%$\n- __Specificity__: $92\\%$\n- __AUC ROC__: $0.973$","metadata":{}},{"cell_type":"markdown","source":"# Decision Tree Classifier\n\nThe Decision tree Classifier is a tree based classifier. This model splits observations on the basis of their entropy. Every split is done so that entropy is maximized.","metadata":{}},{"cell_type":"code","source":"# Training model\ndtc = DecisionTreeClassifier(max_depth = 4)\ndtc.fit(X_train, y_train)\n## Trainign report\nprint(classification_report(y_train, dtc.predict(X_train), target_names = class_names))","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:39:18.991547Z","iopub.execute_input":"2021-06-21T05:39:18.991956Z","iopub.status.idle":"2021-06-21T05:39:19.007818Z","shell.execute_reply.started":"2021-06-21T05:39:18.991923Z","shell.execute_reply":"2021-06-21T05:39:19.006322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Confusion Matrix","metadata":{}},{"cell_type":"code","source":"# test data\ncm_Heatmap(y_test, dtc.predict(X_test), class_names)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:39:19.009475Z","iopub.execute_input":"2021-06-21T05:39:19.0098Z","iopub.status.idle":"2021-06-21T05:39:19.231671Z","shell.execute_reply.started":"2021-06-21T05:39:19.009771Z","shell.execute_reply":"2021-06-21T05:39:19.230619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Classification Report","metadata":{}},{"cell_type":"code","source":"# Classification report\nprint(classification_report(y_test, dtc.predict(X_test), target_names = class_names))","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:39:19.233387Z","iopub.execute_input":"2021-06-21T05:39:19.233853Z","iopub.status.idle":"2021-06-21T05:39:19.24785Z","shell.execute_reply.started":"2021-06-21T05:39:19.233807Z","shell.execute_reply":"2021-06-21T05:39:19.246416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. AUC-ROC","metadata":{}},{"cell_type":"code","source":"AUC_ROC(dtc, X_train, X_test, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:39:19.2493Z","iopub.execute_input":"2021-06-21T05:39:19.249596Z","iopub.status.idle":"2021-06-21T05:39:19.43866Z","shell.execute_reply.started":"2021-06-21T05:39:19.249568Z","shell.execute_reply":"2021-06-21T05:39:19.437612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Results (on Test data)\n\n- __Sensitivity__: $87\\%$\n- __Specificity__: $92\\%$\n- __AUC ROC__: $0.918$","metadata":{}},{"cell_type":"markdown","source":"# Random Forest Classifier\n\nRandom Forest Classifier is an ensemble learning algorithm which means that several simpler algorithms, Decision Tree Classifier, in this case are used to build the model. Generally Random Forest Classifiers are used for larger and complex datasets. Unlike in Decision Tree Classifier, here every tree has to work with a random subset of features rather than working with the complete set of features. This is done to avoid Overfitting.","metadata":{}},{"cell_type":"code","source":"# Training model\nrfc = RandomForestClassifier(n_estimators = 50, max_depth = 3)\nrfc.fit(X_train, y_train)\n# Training report\nprint(classification_report(y_train, rfc.predict(X_train), target_names = class_names))","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:39:19.4399Z","iopub.execute_input":"2021-06-21T05:39:19.440169Z","iopub.status.idle":"2021-06-21T05:39:19.548937Z","shell.execute_reply.started":"2021-06-21T05:39:19.440143Z","shell.execute_reply":"2021-06-21T05:39:19.547836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Confusion Matrix","metadata":{}},{"cell_type":"code","source":"cm_Heatmap(y_test, rfc.predict(X_test), class_names)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:39:19.550228Z","iopub.execute_input":"2021-06-21T05:39:19.550502Z","iopub.status.idle":"2021-06-21T05:39:19.855204Z","shell.execute_reply.started":"2021-06-21T05:39:19.550475Z","shell.execute_reply":"2021-06-21T05:39:19.853995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Classification Report","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test, rfc.predict(X_test), target_names = class_names))","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:39:19.85704Z","iopub.execute_input":"2021-06-21T05:39:19.857498Z","iopub.status.idle":"2021-06-21T05:39:19.878146Z","shell.execute_reply.started":"2021-06-21T05:39:19.857453Z","shell.execute_reply":"2021-06-21T05:39:19.877302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. AUC-ROC Curve","metadata":{}},{"cell_type":"code","source":"AUC_ROC(rfc, X_train, X_test, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:39:19.87908Z","iopub.execute_input":"2021-06-21T05:39:19.879375Z","iopub.status.idle":"2021-06-21T05:39:20.077533Z","shell.execute_reply.started":"2021-06-21T05:39:19.879346Z","shell.execute_reply":"2021-06-21T05:39:20.076666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Results (on Test data)\n\n- __Sensitivity__: $97\\%$\n- __Specificity__: $92\\%$\n- __AUC ROC__: $0.972$","metadata":{}},{"cell_type":"markdown","source":"# Naive Bayes Classifier\n\nThis is a Bayesian approach of classification. At we we have to assign certain probabilities to individual classes called the __prior probabilities__ and using the training data, we have to update these probabilities to obtain the __posterior probability__ distribution. Using this probability distribution we can classify a new observation.","metadata":{}},{"cell_type":"code","source":"# Training model\nnbc = GaussianNB(priors = np.array([0.5, 0.5]))\nnbc.fit(X_train, y_train)\n# Training report\nprint(classification_report(y_train, nbc.predict(X_train), target_names = class_names))","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:39:20.079012Z","iopub.execute_input":"2021-06-21T05:39:20.079421Z","iopub.status.idle":"2021-06-21T05:39:20.092287Z","shell.execute_reply.started":"2021-06-21T05:39:20.079376Z","shell.execute_reply":"2021-06-21T05:39:20.091415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Confusion Matrix","metadata":{}},{"cell_type":"code","source":"cm_Heatmap(y_test, nbc.predict(X_test), class_names)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:39:20.093487Z","iopub.execute_input":"2021-06-21T05:39:20.09382Z","iopub.status.idle":"2021-06-21T05:39:20.285581Z","shell.execute_reply.started":"2021-06-21T05:39:20.09379Z","shell.execute_reply":"2021-06-21T05:39:20.284754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Classification Report","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test, nbc.predict(X_test), target_names = class_names))","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:39:20.286873Z","iopub.execute_input":"2021-06-21T05:39:20.287339Z","iopub.status.idle":"2021-06-21T05:39:20.29926Z","shell.execute_reply.started":"2021-06-21T05:39:20.287309Z","shell.execute_reply":"2021-06-21T05:39:20.298109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. AUC-ROC Curve","metadata":{}},{"cell_type":"code","source":"AUC_ROC(nbc, X_train, X_test, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:39:20.300774Z","iopub.execute_input":"2021-06-21T05:39:20.301137Z","iopub.status.idle":"2021-06-21T05:39:20.491192Z","shell.execute_reply.started":"2021-06-21T05:39:20.301106Z","shell.execute_reply":"2021-06-21T05:39:20.490398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Results (on Test data)\n\n- __Sensitivity__: $100\\%$\n- __Specificity__: $90\\%$\n- __AUC ROC__: $0.985$","metadata":{}},{"cell_type":"markdown","source":"# Support Vector Machines\n\nSVMs are based on the idea of finding a hyperplane which separates the data. Hyperplane is a general term for a __line__ in 2-D space or a __plane__ in 3-D space","metadata":{}},{"cell_type":"code","source":"svmc = SVC(probability = True)\n# Training the Model\nsvmc.fit(X_train, y_train)\n# Training Report\nprint(classification_report(y_train, svmc.predict(X_train), target_names = class_names))","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:39:20.492516Z","iopub.execute_input":"2021-06-21T05:39:20.492804Z","iopub.status.idle":"2021-06-21T05:39:20.51665Z","shell.execute_reply.started":"2021-06-21T05:39:20.492778Z","shell.execute_reply":"2021-06-21T05:39:20.515378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Confusion Matrix","metadata":{}},{"cell_type":"code","source":"cm_Heatmap(y_test, svmc.predict(X_test), class_names)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:39:20.517719Z","iopub.execute_input":"2021-06-21T05:39:20.517979Z","iopub.status.idle":"2021-06-21T05:39:20.703437Z","shell.execute_reply.started":"2021-06-21T05:39:20.517953Z","shell.execute_reply":"2021-06-21T05:39:20.702456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Classification Report","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test, svmc.predict(X_test), target_names = class_names))","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:39:20.706974Z","iopub.execute_input":"2021-06-21T05:39:20.707276Z","iopub.status.idle":"2021-06-21T05:39:20.718462Z","shell.execute_reply.started":"2021-06-21T05:39:20.707248Z","shell.execute_reply":"2021-06-21T05:39:20.71739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. AUC-ROC Curve","metadata":{}},{"cell_type":"code","source":"AUC_ROC(svmc, X_train, X_test, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:39:20.720765Z","iopub.execute_input":"2021-06-21T05:39:20.721269Z","iopub.status.idle":"2021-06-21T05:39:20.909874Z","shell.execute_reply.started":"2021-06-21T05:39:20.721227Z","shell.execute_reply":"2021-06-21T05:39:20.908748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Results (on Test data)\n\n- __Sensitivity__: $100\\%$\n- __Specificity__: $90\\%$\n- __AUC ROC__: $0.971$","metadata":{}},{"cell_type":"markdown","source":"# Conclusion\n\n| Model Name               | Sensitivity | Specificity | AUC ROC |\n| ------------------------ | ----------- | ----------- | ------- |\n| Logistic Regression      | $77\\%$      | $98\\%$      | $0.973$ |\n| k-Nearest Neighbors      | $93\\%$      | $92\\%$      | $0.973$ |\n| Decision Tree Classifier | $87\\%$      | $92\\%$      | $0.918$ |\n| Random Forest Classifier | $97\\%$      | $92\\%$      | $0.972$ |\n| Naive Bayes              | $100\\%$     | $90\\%$      | $0.985$ |\n| Support Vector Machines  | $100\\%$     | $90\\%$      | $0.971$ |\n\n- It seems that Naive Bayes classifier is the optimum model for this dataset, with the highest AUC ROC value.\n- When detection of Not-Purchased is more important, the Logistic Regression is the best, with the highest Specificity.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}