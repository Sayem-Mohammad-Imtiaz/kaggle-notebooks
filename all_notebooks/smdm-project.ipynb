{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction\nThis project aims to analyze the sentiment of the people on this new law by studying the messages tweeted by them on this topic. All the messages tweeted with the hashtag of CAA, Citizenship Amendment Act etc are extracted using the Twitter APIs and used as input dataset for this project. These messages are then classified as positive, to indicate that they are supportive of the bill, negative, to indicate that they do not endorse the new law, and neutral. This classification is made by training a model, which is based on the Naive Bayes classifier algorithm, on a sample dataset whose classification is known beforehand. The model classifies an input record by calculating probability of the record being positive (p), negative (ng) and neutral (ne) and if p > ng and p > ne then it classifies the record as positive, if ng > p and ng > ne then it classifies as negative and otherwise it classifies as neutral. The model is applied to the extracted twitter dataset to classify them as positive, negative or neutral and a pie chart is drawn to present the analysis made on the input dataset. The analysis will show the general mood or opinion of the people expressed on this new amendment moved forward by the government of India."},{"metadata":{},"cell_type":"markdown","source":"## Importing Required Libraries"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport numpy as np \nimport os \nimport pandas as pd \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Required as we are using Kaggle notebook"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Distribution graphs (histogram/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n    filename = df.dataframeName\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Data Exploration and Visualization"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# specify 'None' if want to read whole file. For now we have considered 30000 data to build the model\nnRowsRead = 30000 \n# file.csv may have more rows in reality, but we are only loading/previewing the first 30000 rows\ndf1 = pd.read_csv('../input/caa-tweets-till-9012020/file.csv', delimiter=',',nrows = nRowsRead)\ndf1.dataframeName = 'file.csv'\nnRow, nCol = df1.shape\nprint(f'There are {nRow} rows and {nCol} columns')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a quick look at what the data looks like:"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"df1.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Distribution graphs (histogram/bar graph) of sampled columns:"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"df_plot = df1.head(nRowsRead)\nplotPerColumnDistribution(df_plot, 10, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_plot = df1.head(20000)\ntype(df_plot)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation matrix:"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"plotCorrelationMatrix(df1, 8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scatter and density plots:"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"plotScatterMatrix(df_plot, 20, 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Data Pre-processing\n**Dropping duplicate rows**\n\nThere are two types of duplicates-duplicates with same values for all columns(this duplication happens when same tweets are collected again by tweet-collector) and duplicates with the same text for tweets(This occurs when two or more users post the same tweet.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(df1.index))\nserlis=df1.duplicated().tolist()\nprint(serlis.count(True))\nserlis=df1.duplicated(['tweet']).tolist()\nprint(serlis.count(True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above results we can see that there are about '2819' records that are duplicated and we had to remove the duplicated rows as it was resulting in overfitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head(205)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Reindexing\n\nAs the deletion of duplicate rows causes in irregular indexing. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#df=df1.drop_duplicates(['tweet']).reset_index()\ndf= df1[~df1.index.duplicated(keep='first')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are the features available in the data set but we do not need all of these features to perform a sentimental analysis. So we are exploring the features which are 'NaN' and the features which actually have the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['id'].isna().sum())\nprint(df['conversation_id'].isna().sum())\nprint(df['created_at'].isna().sum())\nprint(df['date'].isna().sum())\nprint(df['time'].isna().sum())\nprint(df['timezone'].isna().sum())\nprint(df['user_id'].isna().sum())\nprint(df['username'].isna().sum())\nprint(df['name'].isna().sum())#9\nprint(df['retweet'].isna().sum())\nprint(df['tweet'].isna().sum())\nprint(df['mentions'].isna().sum())\nprint(df['urls'].isna().sum())\nprint(df['photos'].isna().sum())\nprint(df['replies_count'].isna().sum())\nprint(df['retweets_count'].isna().sum())\nprint(df['likes_count'].isna().sum())\nprint(df['hashtags'].isna().sum())\nprint(df['cashtags'].isna().sum())\nprint(df['reply_to'].isna().sum())\nprint(df['video'].isna().sum())\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['near'].isna().sum())\nprint(df['geo'].isna().sum())\nprint(df['source'].isna().sum())\nprint(df['user_rt_id'].isna().sum())\nprint(df['user_rt'].isna().sum())\nprint(df['retweet_id'].isna().sum())\nprint(df['retweet_date'].isna().sum())\nprint(df['translate'].isna().sum())\nprint(df['trans_src'].isna().sum())\nprint(df['trans_dest'].isna().sum())\nprint(df['place'].isna().sum())\nprint(df['quote_url'].isna().sum())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that all the above columns have almost '30000' rows which are 'Nan' and we need to drop these features"},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Dropping unnecessary columns\n\nFeatures like 'near','geo','source','user_rt_id','user_rt','retweet_id','retweet_date','translate','trans_src',           'trans_dest','place','quote_url','urls','link','id','conversation_id','user_id','photos','video','hashtags','cashtags' etc have more not-a-number(NaN) as their values."},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df.drop(['near','geo','source','user_rt_id','user_rt','retweet_id','retweet_date','translate','trans_src',\n            'trans_dest','place','quote_url','urls','link','id','conversation_id','user_id','photos','video','hashtags','cashtags'],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are also dropping other columns like 'id','conversation_id','user_id' as these features are unique and will not help us to perform a sentimental analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Cleaning the tweets\n\nIn the text of a tweet, there may be some unnecessary symbols which is not essential for our analysis.Lets explore by printing a tweet"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['tweet'][2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, some of the unnecessary text and symbols to be removed are — username_tags(like @News18Rajasthan,@zeerajasthan_), retweet symbol(RT), hashtags(like #CAARally, #CAA_NRC_Protests), URLs(like pic.twitter.com/6CJirGJ70o),numbers and punctuations .Some of the meaningful hashtags convey meaning and can have some sentiment in it after the word is segmented into useful parts (like #No #CAA). So, instead of removing all the words starting with hashtag symbols, only ‘#’ symbols are removed. We perform this text cleaning using re module in python. The re.sub() function searches for a pattern and replaces with the text we specify. We replace all these symbols with a whitespace character."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfor i in range(len(df)):\n    txt = df.loc[i][\"tweet\"]\n    txt = re.sub('pic.twitter.com/[A-Za-z0-9./]+','',txt)\n    txt=re.sub(r'@[A-Z0-9a-z_:]+','',txt)#replace username-tags\n    txt=re.sub(r'^[RT]+','',txt)#replace RT-tags\n    txt = re.sub('https?://[A-Za-z0-9./]+','',txt)#replace URLs\n    txt=re.sub(\"[^a-zA-Z]\", \" \",txt)#replace hashtags\n    df.at[i,\"tweet\"]=txt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we can see that our tweets appear clean.\n\nThis tweet is before the pre-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1['tweet'][2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is after cleaning the twitter data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['tweet'][2]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Sentimental Analysis"},{"metadata":{},"cell_type":"markdown","source":"## 3.1 POS-Tagging and Sentiment labeling\n\nWe are done with the basic cleaning part of text data.SentiWordNet is an enhanced lexical resource explicitly devised for supporting sentiment classification and opinion mining applications. It has a large corpus of POS-tagged English words along with their sentiment."},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nimport nltk\nfrom nltk.corpus import sentiwordnet as swn\nfrom nltk.tag import pos_tag,map_tag\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\n\npstem = PorterStemmer()\nlem = WordNetLemmatizer()\nstop_words = stopwords.words('english')\ndef pos_senti(df_copy):#takes\n    li_swn=[]\n    li_swn_pos=[]\n    li_swn_neg=[]\n    missing_words=[]\n    for i in range(len(df_copy.index)):\n        text = df_copy.loc[i]['tweet']\n        tokens = nltk.word_tokenize(text)\n        tagged_sent = pos_tag(tokens)\n        store_it = [(word, map_tag('en-ptb', 'universal', tag)) for word, tag in tagged_sent]\n        #print(\"Tagged Parts of Speech:\",store_it)\n\n        pos_total=0\n        neg_total=0\n        for word,tag in store_it:\n            if(tag=='NOUN'):\n                tag='n'\n            elif(tag=='VERB'):\n                tag='v'\n            elif(tag=='ADJ'):\n                tag='a'\n            elif(tag=='ADV'):\n                tag = 'r'\n            else:\n                tag='nothing'\n\n            if(tag!='nothing'):\n                concat = word+'.'+tag+'.01'\n                try:\n                    this_word_pos=swn.senti_synset(concat).pos_score()\n                    this_word_neg=swn.senti_synset(concat).neg_score()\n                    #print(word,tag,':',this_word_pos,this_word_neg)\n                except Exception as e:\n                    wor = lem.lemmatize(word)\n                    concat = wor+'.'+tag+'.01'\n                    # Checking if there's a possiblity of lemmatized word be accepted into SWN corpus\n                    try:\n                        this_word_pos=swn.senti_synset(concat).pos_score()\n                        this_word_neg=swn.senti_synset(concat).neg_score()\n                    except Exception as e:\n                        wor = pstem.stem(word)\n                        concat = wor+'.'+tag+'.01'\n                        # Checking if there's a possiblity of lemmatized word be accepted\n                        try:\n                            this_word_pos=swn.senti_synset(concat).pos_score()\n                            this_word_neg=swn.senti_synset(concat).neg_score()\n                        except:\n                            missing_words.append(word)\n                            continue\n                pos_total+=this_word_pos\n                neg_total+=this_word_neg\n        li_swn_pos.append(pos_total)\n        li_swn_neg.append(neg_total)\n\n        if(pos_total!=0 or neg_total!=0):\n            if(pos_total>neg_total):\n                li_swn.append(1)\n            else:\n                li_swn.append(-1)\n        else:\n            li_swn.append(0)\n    df_copy.insert(5,\"pos_score\",li_swn_pos,True)\n    df_copy.insert(6,\"neg_score\",li_swn_neg,True)\n    df_copy.insert(7,\"sent_score\",li_swn,True)\n    return df_copy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"st=time.time()\nprint('Start time: ' + str(st))\ndf_copy = pos_senti(df)\nend=time.time()\nprint('End time: ' + str(st))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_copy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(df_copy['sent_score'][0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Results by using SentiWordNet\n\nSWN gives pos_score and neg_score for each word as in the above case. The higher the pos_score, the more positive is the word. We use a simple linear summation of these scores(We add pos_score of all the words in a tweet to form a pos_total and in a similar way, we obtain neg_total. Then we add these two to obtain sent_total) and label a sentence as positive(1) if it(sent_total) is greater than 0, negative(-1) if it is less than 0 and neutral(0)"},{"metadata":{"trusted":true},"cell_type":"code","source":"count_pos_sen=0\ncount_neg_sen=0\ncount_neut_sen=0\n\nfor i in range(len(df_copy.index)):\n    if df_copy['sent_score'][i] >0:\n        count_pos_sen = count_pos_sen +1\n    elif df_copy['sent_score'][i] == 0:\n        count_neut_sen = count_neut_sen +1\n    else:\n        count_neg_sen = count_neg_sen +1\n\nprint(\"positive tweets:\",count_pos_sen)\nprint(\"negative tweets:\",count_neg_sen)\nprint(\"neutral tweets:\",count_neut_sen)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Results by using TextBlob\n\nTextBlob gives pos_score and neg_score for each word as in the above case."},{"metadata":{"trusted":true},"cell_type":"code","source":"#TextBlob SENTIMENT LABELING\nfrom textblob import TextBlob\ncount_total=0\ncount_pos=0\ncount_neg=0\ncount_neut=0\n\nli_tb = []\nfor i in range(len(df.index)):\n    sent = TextBlob(str(df.loc[i]['tweet']))\n    if(sent.sentiment.polarity>0):\n        count_pos=count_pos+1\n        count_total=count_total+1\n        li_tb.append(1)\n    elif(sent.sentiment.polarity<0):\n        count_neg=count_neg+1\n        count_total=count_total+1\n        li_tb.append(-1)\n    else:\n        li_tb.append(0)\n        count_neut+=1\n\n        count_total=count_total+1\n\n\n#         print(df.loc[i]['full_text'])\n#         print(sent.sentiment)\nprint(\"Total tweets:\",len(df.index))\nprint(\"Total tweets with sentiment:\",count_total)\nprint(\"positive tweets:\",count_pos)\nprint(\"negative tweets:\",count_neg)\nprint(\"neutral tweets:\",count_neut)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# GENERATE POSITIVE TEXT,NEGATIVE TEXT,NEUTRAL TEXT FOR DATA-VISUALISATION\npos_text=\"\"\nneg_text=\"\"\nneut_text=\"\"\n\nfor i in range(len(df_copy.index)):\n    if(df_copy.loc[i][\"sent_score\"]==1):\n        pos_text+=df_copy.loc[i][\"tweet\"]\n    elif(df_copy.loc[i][\"sent_score\"]==-1):\n        neg_text+=df_copy.loc[i][\"tweet\"]\n    else:\n        neut_text+=df_copy.loc[i][\"tweet\"]\n\nlist_text = [pos_text,neg_text,neut_text]\n\n\nfor txt in list_text:\n    word_cloud = WordCloud(width = 600,height = 600,max_font_size = 200).generate(txt)\n    plt.figure(figsize=(12,10))# create a new figure\n    plt.imshow(word_cloud,interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# GENERATE POSITIVE TEXT,NEGATIVE TEXT,NEUTRAL TEXT FOR DATA-VISUALISATION\npos_text=\"\"\nneg_text=\"\"\nneut_text=\"\"\n\nfor i in range(len(df.index)):\n    if(df.loc[i][\"sent_score\"]==1):\n        pos_text+=df.loc[i][\"tweet\"]\n    elif(df.loc[i][\"sent_score\"]==-1):\n        neg_text+=df.loc[i][\"tweet\"]\n    else:\n        neut_text+=df.loc[i][\"tweet\"]\n\nlist_text = [pos_text,neg_text,neut_text]\n\n\nfor txt in list_text:\n    word_cloud = WordCloud(width = 600,height = 600,max_font_size = 200).generate(txt)\n    plt.figure(figsize=(12,10))# create a new figure\n    plt.imshow(word_cloud,interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Results By Using Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n# Create the pandas DataFrame \ndata = pd.DataFrame(df_copy, columns = ['tweet', 'sent_score']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert the text data into sequence using text_to_sequence method\n\ndef text2seq(data):\n    max_fatures = 2000\n    tokenizer = Tokenizer(num_words = max_fatures, split=' ')\n    tokenizer.fit_on_texts(data['tweet'].values)\n    X = tokenizer.texts_to_sequences(data['tweet'].values)\n    X = pad_sequences(X)\n    return X\n\nX = text2seq(data)\nprint(\"Dimension of the input data after text_to_sequence method: \", X.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ndef Tf_idf(data):\n    tweet_data = data['tweet']\n    vectorizer = TfidfVectorizer()\n    X = vectorizer.fit_transform(tweet_data)\n    print(vectorizer.get_feature_names())\n    return X\n\nX = Tf_idf(data)\n#print(\"Dimension of input data after tf-idf vectorization:\", X.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ndef count_vectorizer(data):\n    tweet_data = data['tweet']\n    bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n\n    # bag-of-words feature matrix\n    bow = bow_vectorizer.fit_transform(tweet_data)\n\n    df_bow = pd.DataFrame(bow.todense())\n\n    return df_bow\nX = count_vectorizer(data)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# Y = pd.get_dummies(data['Label']).values\n\ndef split_train_test(X, Y):\n    Y=data[\"sent_score\"].values\n    print(Y)\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.20)\n    print(\"Dimension of training features and label: \", X_train.shape,Y_train.shape)\n    print(\"Dimension of testing features and label: \", X_test.shape,Y_test.shape)\n\n    return X_train, X_test, Y_train, Y_test\n\nY = data['sent_score']\nX_train, X_test, Y_train, Y_test = split_train_test(X, Y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.1 Accuracy of the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\ndef Logistic_regression_model(X_train, X_test, Y_train, Y_test):\n\n    logreg = LogisticRegression(C=100, max_iter=500)\n    logreg.fit(X_train, Y_train)\n    y_pred = logreg.predict(X_test)\n\n    print(\"accuracy score on the logistic regression model:\", accuracy_score(y_pred, Y_test))\n    return y_pred\ny_pred = Logistic_regression_model(X_train, X_test, Y_train, Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ndata['sent_score'].value_counts().sort_index().plot(kind='bar', title='Sentiment Count', color='seagreen')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}