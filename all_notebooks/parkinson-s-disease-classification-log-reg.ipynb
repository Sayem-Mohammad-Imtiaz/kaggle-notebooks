{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d482c6e199419394305ba0d74ef45d60b08f59f"},"cell_type":"markdown","source":"**Data Set Information:**\n\nThis dataset is composed of a range of biomedical voice measurements from 31 people, 23 with Parkinson's disease (PD). Each column in the table is a particular voice measure, and each row corresponds one of 195 voice recording. The main aim of the data is to discriminate healthy people from those with PD, according to \"status\" column which is set to 0 for healthy and 1 for PD."},{"metadata":{"_uuid":"a523c26b282a009f277e6a362de4e6d56605a19f"},"cell_type":"markdown","source":"**Attribute Information:**\n\nMatrix column entries (attributes):\nMDVP:Fo(Hz) - Average vocal fundamental frequency\nMDVP:Fhi(Hz) - Maximum vocal fundamental frequency\nMDVP:Flo(Hz) - Minimum vocal fundamental frequency\nMDVP:Jitter(%),MDVP:Jitter(Abs),MDVP:RAP,MDVP:PPQ,Jitter:DDP - Several \nmeasures of variation in fundamental frequency\nMDVP:Shimmer,MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA - Several measures of variation in amplitude\nNHR,HNR - Two measures of ratio of noise to tonal components in the voice\nstatus - Health status of the subject (one) - Parkinson's, (zero) - healthy\nRPDE,D2 - Two nonlinear dynamical complexity measures\nDFA - Signal fractal scaling exponent\nspread1,spread2,PPE - Three nonlinear measures of fundamental frequency variation "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data=pd.read_csv(\"../input/parkinsons2.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"540045691d85c81f40f5f6db02be1e9dd6fcc93e"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94d76b7c8bbd37f0f3523f610588a5c2c83caa8a"},"cell_type":"code","source":"data.info()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"872175f616f62078cf51cab8e04ab73aa8ebe16c"},"cell_type":"code","source":"#correlation data\nf,ax=plt.subplots(figsize=(14,14))\nsns.heatmap(data.corr(),annot=True,ax=ax,fmt=\".2f\")\nplt.xticks(rotation=90)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fbc7e2e60087b607b3da13f067d63308bddcd136"},"cell_type":"code","source":"#prepare of data\ny=data.status.values\nx_data=data.drop([\"status\"],axis=1)\n#normlizasyon\nx=(x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9fb2a104f8d2d73cb5b43ce5f00e422ccd7abbf4"},"cell_type":"code","source":"#split data for test%train\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,random_state=42,test_size=0.2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f436e4fd3df9fe022b0b7e4ae884fce84c0ba3c7"},"cell_type":"code","source":"#transpose of each \nx_train=x_train.T\nx_test=x_test.T\ny_train=y_train.T\ny_test=y_test.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d2664d811a192090c564d2698302c4be6fc0596"},"cell_type":"code","source":"#determination of initial values of weights and bias\ndef initialize_weights_and_bias(dimension):\n    b=0.0\n    w=np.full((dimension,1),0.01)\n    return w,b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"746a60ece751cd52575d8fd11f7008ac01e79f8b"},"cell_type":"code","source":"#the formula of the activation function\ndef sigmoid(z):\n    y_head=1/(1+np.exp(-z))\n    return y_head","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51a1f92cf4bd99194f2feef1c2d24d540b01f90b"},"cell_type":"code","source":"#calculate of z, forward and backward propagation\ndef forward_backward_propagation(w,b,x_train,y_train):\n#forward  propagation   \n    z=np.dot(w.T,x_train)+b\n    y_head=sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost=(np.sum(loss))/x_train.shape[1]    \n#backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    return cost,gradients","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15f548be198f07d00277fab96bada31c7541e34b"},"cell_type":"code","source":"#update parameter values to reduce cost value\ndef update(w,b,x_train,y_train,learning_rate,number_of_iteration):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    for i in range(number_of_iteration):\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dcb5b9fdb3cb72e1c7a88795b62d0f792841b2c9"},"cell_type":"code","source":"#prediction of test values\ndef predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction\ndef logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    dimension =  x_train.shape[0] \n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n\n    # Print test Errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a25fc18a337e3928510b0aa9f61ab6b91ca6027"},"cell_type":"code","source":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1.2, num_iterations = 400)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fdaa14509cc0a3ff7cb8f1b2ce96113cb8339ffd"},"cell_type":"markdown","source":"**There is also a short version of the above codes.**"},{"metadata":{"trusted":true,"_uuid":"66dcbb7a179bb9f34be55df25fc950b6e6fcc590"},"cell_type":"code","source":"#Logistic regression with sklearn \nfrom sklearn.linear_model import LogisticRegression\nlog=LogisticRegression()\nlog.fit(x_train.T,y_train.T)\nprint(\"test-accuracy : {} \".format(log.score(x_test.T,y_test.T)*100))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45dd1c306665f97f74a431aff6005f5fdb23c3ce"},"cell_type":"markdown","source":"\n**Logistic Regression  predicted correctly  the class of data at 89.74 percent.  I hope you like. Waiting for comments and suggestions.**\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}