{"cells":[{"metadata":{},"cell_type":"markdown","source":"## California Housing Price\n- predict median price per district\n- model: regression/labeled supervised learning\n- dataset: https://github.com/ageron/handson-ml2/tree/master/datasets/housing"},{"metadata":{},"cell_type":"markdown","source":"### 1. Read Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport tensorflow as tf\nfrom tensorflow import feature_column\nfrom tensorflow.keras import layers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"../input/california-housing-prices/housing.csv\")\ndf['median_house_value']/=1000\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()\n#total_badrooms 207/20640 is missing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df.dropna()\ndf.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Take away:\n- comparing to 75%, max for `total_rooms`, `population`, `households` need a further check.\n- abnormal data for target col `median_house_value`."},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df[(df.total_rooms<=5000)&(df.total_bedrooms<=1000)&(df.population<=2500)&(df.households<=1000)&\n      (df.median_income<=8)&(df.median_house_value<500)]\n\n#df.median_house_value.hist(bins=100)\n#df.total_bedrooms.hist(bins=100)\n#df.median_income.hist(bins=100)\n#df.total_rooms.hist(bins=100)\n#df.population.hist(bins=100)\n#df.households.hist(bins=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler=StandardScaler()\nscaled_df=scaler.fit_transform(df.loc[:,df.columns!='ocean_proximity']) \nscaled_df=pd.DataFrame(scaled_df,columns=df.columns.values[0:-1])\nscaled_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_df.hist(bins=100,figsize=(15,10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_columns = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#location\nresolution_in_degrees = 0.4 \n\nlatitude_num = tf.feature_column.numeric_column(\"latitude\")\nlatitude_bins = list(np.arange(int(min(scaled_df['latitude'])), int(max(scaled_df['latitude'])), resolution_in_degrees))\nlatitude = tf.feature_column.bucketized_column(latitude_num, latitude_bins)\n\nlongitude_num = tf.feature_column.numeric_column(\"longitude\")\nlongitude_bins = list(np.arange(int(min(scaled_df['longitude'])), int(max(scaled_df['longitude'])), resolution_in_degrees))\nlongitude = tf.feature_column.bucketized_column(longitude_num, longitude_bins)\n\nlat_x_lon = tf.feature_column.crossed_column([latitude, longitude], hash_bucket_size=100)\ncrossed_feature = tf.feature_column.indicator_column(lat_x_lon)\nfeature_columns.append(crossed_feature)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#demographic\nmed_income = tf.feature_column.numeric_column(\"median_income\")\nfeature_columns.append(med_income)\n\npopulation = tf.feature_column.numeric_column(\"population\")\nfeature_columns.append(population)\n\nhouseholds = tf.feature_column.numeric_column(\"households\")\nfeature_columns.append(households)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#house\nhouse_age=tf.feature_column.numeric_column(\"housing_median_age\")\nfeature_columns.append(house_age)\n\nttl_room=tf.feature_column.numeric_column(\"total_rooms\")\nfeature_columns.append(ttl_room)\n\nttl_bedroom=tf.feature_column.numeric_column(\"total_bedrooms\")\nfeature_columns.append(ttl_bedroom)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_layer = layers.DenseFeatures(feature_columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train/Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_df, test_df= train_test_split(scaled_df,test_size=0.22, random_state=123)\nprint(\"Total df size: %i\\n train_df size: %i \\n test_df size: %i\"\\\n%(df.shape[0],train_df.shape[0],test_df.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Modeling"},{"metadata":{},"cell_type":"markdown","source":"### 4.1 Define functions that build and train a model\n- build_model(learning_rate), which builds a randomly-initialized model.\n- train_model(model, feature, label, epochs), which trains the model from the examples (feature and label) you pass."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define the functions that build and train a model\ndef build_model(my_learning_rate, feature_layer):\n  # Create and compile a simple linear regression model.\n  model = tf.keras.models.Sequential() # Most simple tf.keras models are sequential.\n\n  # Describe the topography of the model.\n  model.add(feature_layer)\n  model.add(tf.keras.layers.Dense(units=1, input_shape=(1,))) #a single node in a single layer.\n\n  # Compile into TensorFlow. \n  model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate),\n                loss=\"mean_squared_error\",\n                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n\n  return model        \n\n#Train the model by feeding feature and label.\ndef train_model(model, df, epochs, batch_size, label_name):\n    \n  features = {name:np.array(value) for name, value in df.items()}\n  label = np.array(features.pop(label_name))\n    \n  history = model.fit(x=features,y=label,\n                      batch_size=batch_size,epochs=epochs,\n                      shuffle=True) # specified number of epochs. \n\n  # Gather the trained model's weight and bias.\n  #trained_weight = model.get_weights()[0]\n  #trained_bias = model.get_weights()[1]\n\n  epochs = history.epoch # The list of epochs is stored separately from the rest of history.\n  \n  hist = pd.DataFrame(history.history) # Isolate the error for each epoch.\n  rmse = hist[\"root_mean_squared_error\"] # Take a snapshot of the model's root mean squared error at each epoch. \n\n  return epochs, rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2 Define plotting functions\n- a loss curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot the trained model against 200 random training examples.\ndef plot_the_model(trained_weight, trained_bias, feature, label):\n  \n  plt.xlabel(feature)\n  plt.ylabel(label)\n\n  random_examples = training_df.sample(n=200)\n  plt.scatter(random_examples[feature], random_examples[label])\n\n  # Create a red line starts at coordinates (x0, y0) and ends at coordinates (x1, y1).\n  x0 = 0\n  y0 = trained_bias\n  x1 = 10000\n  y1 = trained_bias + (trained_weight * x1)\n  plt.plot([x0, x1], [y0, y1], c='r')\n\n  plt.show()\n\n#Plot a curve of loss vs. epoch.\ndef plot_the_loss_curve(epochs, rmse):\n  plt.figure()\n  plt.xlabel(\"Epoch\")\n  plt.ylabel(\"Root Mean Squared Error\")\n\n  plt.plot(epochs, rmse, label=\"Loss\")\n  plt.legend()\n  plt.ylim([rmse.min()*0.94, rmse.max()* 1.05])\n  plt.show()  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3 Call the model functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameters:\nlearning_rate = 0.05\nepochs = 200\nbatch_size = 100\n\nlabel_name=\"median_house_value\" \n\n# Invoke the functions.\nmy_model = build_model(learning_rate,feature_layer)\nepochs, rmse = train_model(my_model, train_df, \n                           epochs, batch_size, label_name)\n\nplot_the_loss_curve(epochs, rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\n: Evaluate the new model on the test set:\")\ntest_features = {name:np.array(value) for name, value in test_df.items()}\ntest_label = np.array(test_features.pop(label_name))\nmy_model.evaluate(x=test_features, y=test_label, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}