{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n\n\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import SimpleRNN\nfrom keras.layers import Embedding\nfrom keras.layers import Bidirectional\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Dropout\n\nfrom keras.utils import to_categorical\nfrom sklearn.utils import shuffle\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\", names=['comment', 'label'], header=0, encoding='utf-8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset is balanced"},{"metadata":{"trusted":true},"cell_type":"code","source":"document_lenghts = list(map(len, df.comment.values))\nprint(np.max(document_lenghts))\nprint(np.min(document_lenghts))\nprint('mean_size:',np.mean(document_lenghts))\nprint('median_size:',np.median(document_lenghts))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.label.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dictionary_length = 1000\ninput_length = 100\n\ntokenizer = Tokenizer(num_words=dictionary_length)\ntokenizer.fit_on_texts(df.comment.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"post_seq = tokenizer.texts_to_sequences(df.comment.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(post_seq))\nprint(post_seq[0])\nprint(len(post_seq[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"post_seq_padded = pad_sequences(post_seq, maxlen=input_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(post_seq_padded))\nprint(post_seq_padded[0])\nprint(len(post_seq_padded[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_original = post_seq_padded\nx_original = np.array(x_original)\n\ny_original = df['label'].values\ny_original = 1*(y_original=='positive')\ny_original = np.array(y_original)\n\n\n\n\nx, y = shuffle(x_original, y_original, random_state=23)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.3, random_state=23)\nx_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=23)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"train set:\", x_train.shape)\nprint(\"validation set:\", x_val.shape)\nprint(\"test set:\", x_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(dictionary_length, 2, input_length=input_length))\nmodel.add(Dense(32,activation=\"relu\"))\nmodel.add(Dense(32,activation=\"relu\"))\nmodel.add(Bidirectional(SimpleRNN(16, return_sequences=True)))\nmodel.add(Bidirectional(SimpleRNN(16, return_sequences=True)))\nmodel.add(Bidirectional(SimpleRNN(16, return_sequences=False)))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(x=x_train, y=y_train, batch_size=256, verbose=1, epochs=5, validation_data=(x_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_dict = history.history\n\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.figure(figsize=(8,5))\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(8,5))\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim((0.5,1))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bad_comment = tokenizer.texts_to_sequences([\"Wow, this is the worst film I ever seen. This film is really bad\"])\ngood_comment = tokenizer.texts_to_sequences([\"Not so bad, it is not a masterpiece but I liked it\"])\n\nbad_comment = pad_sequences(bad_comment, maxlen=input_length)\ngood_comment = pad_sequences(good_comment, maxlen=input_length)\n\n#print(bad_comment)\nprint(model.predict(bad_comment))\nprint(model.predict(good_comment))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"e = model.layers[0]\nweights = e.get_weights()[0]\nprint(weights.shape)\nplt.figure(figsize=(12,9))\nplt.plot(weights[:,0], weights[:,1], 'bo')\nplt.title('Word embedding')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}