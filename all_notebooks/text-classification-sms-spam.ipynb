{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import nltk #NLTK provides  common NLP function\nfrom nltk.corpus import stopwords\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport string\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import fbeta_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/sms-spam-collection-dataset/spam.csv\", encoding = 'latin-1')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Text Analysis**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data=data.drop([\"Unnamed: 2\",\"Unnamed: 3\",\"Unnamed: 4\"], axis = 1)\ndata = data.rename(columns={\"v1\":\"label\", \"v2\":\"text\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby(\"label\").describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.label.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"label\"].value_counts().plot(kind = 'pie', explode = [0, 0.1], figsize = (6, 6), autopct = '%1.1f%%', shadow = True)\nplt.ylabel(\"Spam vs Ham\")\nplt.legend([\"Ham\", \"Spam\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.label.value_counts().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"string.punctuation\nfrom nltk.stem import SnowballStemmer\nstemmer = SnowballStemmer(\"english\")\n\n#def cleanText(message):\n  #  message = message.translate(str.maketrans('', '', string.punctuation))\n  #  print(message.split())\n   # words = [stemmer.stem(word) for word in message.split() if word.lower() not in stopwords.words(\"english\")]\n   # return \" \".join(words)\n\n#data[\"text\"] = data[\"text\"].apply(cleanText)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In Short, Stemming is typically faster as it simply chops off the end of the word, without understanding the context of the word. Lemmatizing is slower and more accurate as it takes an informed analysis with the context of the word in mind."},{"metadata":{"trusted":true},"cell_type":"code","source":"def rx(text):\n    # Applying Regular Expression\n    #Replace email addresses with 'emailaddr'\n    #Replace URLs with 'httpaddr'\n    #Replace money symbols with 'moneysymb'\n    #Replace phone numbers with 'phonenumbr'\n    #Replace numbers with 'numbr'\n    print(char for char in txt_no_stop_words)\n    msg = re.sub('\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b', 'emailaddr', text)\n    msg = re.sub('(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)', 'httpaddr', text)\n    msg = re.sub('Â£|\\$', 'moneysymb', text)\n    msg = re.sub('\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b', 'phonenumbr', text)\n    return msg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a dataframe from a word matrix\ndef wm2df(wm, feat_names):\n    # create an index for each row\n    doc_names = ['Doc{:d}'.format(idx) for idx, _ in enumerate(wm)]\n    df = pd.DataFrame(data=wm.toarray(), index=doc_names,columns=feat_names)\n    return(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cleanText(text):\n   #remove punctuation\n    txt_no_punctuation=[char for char in text if char not in string.punctuation]\n    txt_no_punctuation = \"\".join(txt_no_punctuation).split()\n    #remove stop_words\n    txt_no_stop_words=[char.lower() for char in txt_no_punctuation if char.lower() not in stopwords.words(\"english\")]\n    # using stemming\n    ps=nltk.PorterStemmer()\n    clean_text=[ps.stem(word) for word in txt_no_stop_words]\n    # using lemmatization\n    ws=nltk.WordNetLemmatizer()\n    clean_text=[ws.lemmatize(word) for word in txt_no_stop_words]\n    return clean_text\n\ndata[\"text_clean\"]=data[\"text\"].apply(lambda x:cleanText(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Bag of Words (CountVectorizer)\ncount_vect=CountVectorizer(analyzer=cleanText)\n# convert the documents into a document-term matrix\nwm=count_vect.fit_transform(data['text'])\n# retrieve the terms found in the corpora\ntokens = count_vect.get_feature_names()\n# create a dataframe from the matrix\nwm2df(wm, tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_counts.shape)\nprint(count_vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Bag of Words (CountVectorizer with n_gram)\n#ngram_vect=CountVectorizer(ngram_range=(2,2),analyzer=cleanText)\n#X_counts=ngram_vect.fit_transform(data['text'])\n#print(X_counts.shape)\n#print(ngram_vect.get_feature_names())\n\n#Bag of Words (tf idf)\ntfidf_vect=TfidfVectorizer(analyzer=cleanText)\nX_counts=tfidf_vect.fit_transform(data['text'])\ntfidf_vect._validate_vocabulary()\nprint(X_counts.shape)\nprint(tfidf_vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature Engineering: Feature Creation**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"text_len\"]=data['text'].apply(len)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bins=np.linspace(0,200,40)\nplt.hist(data[data['label']=='spam']['text_len'],bins,alpha=0.5,normed=True,label='spam')\nplt.hist(data[data['label']=='ham']['text_len'],bins,alpha=0.5,normed=True,label='ham')\nplt.legend(loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Building ML Classifiers: Model selection**"},{"metadata":{},"cell_type":"markdown","source":"**Encoding Labels** "},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X_counts.toarray()\ny = data['label']\nle = LabelEncoder()\ny = le.fit_transform(y)\ny = y.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Applying Guassian Naive Bayes***"},{"metadata":{"trusted":true},"cell_type":"code","source":"bayes_classifier = GaussianNB()\nbayes_classifier.fit(X_train, y_train)\n#Predicting\ny_pred = bayes_classifier.predict(X_test)\n# Evaluating\ncm = confusion_matrix(y_test, y_pred)\ncm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"Accuracy : %0.5f \\n\\n\" % accuracy_score(ytest, bayes_classifier.predict(xtest)))\nprint (classification_report(ytest, bayes_classifier.predict(xtest)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gaussianNb = MultinomialNB()\ngaussianNb.fit(X_train, y_train)\ny_pred = gaussianNb.predict(X_test)\nprint (\"Accuracy : %0.5f \\n\\n\" % accuracy_score(y_test, gaussianNb.predict(X_test)))\nprint(fbeta_score(y_test, y_pred, beta = 0.5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}