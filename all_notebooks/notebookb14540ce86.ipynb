{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Data Cleaning"},{"metadata":{},"cell_type":"markdown","source":"Importing useful libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Loading in the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/bitcoin-historical-data/bitstampUSD_1-min_data_2012-01-01_to_2020-09-14.csv')\nprint(df.shape)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bitcoin_df = df.copy() # making a copy of the dataset to work with","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"time_col = pd.to_datetime(bitcoin_df['Timestamp'], unit='s') # converting the 'Timestamp' column to \n                                                             # datetime object\n\nbitcoin_df.drop('Timestamp', axis=1, inplace=True) # drops the 'Timestamp' column\nbitcoin_df['Timestamp'] = time_col # creates a new 'Timestamp' column with datetime dtype\nbitcoin_df.set_index('Timestamp', inplace=True) # makes 'Timestamp' the index\n\nbitcoin_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_val_mask = bitcoin_df.isnull() # creating a dataframe with boolean values indicating the presence of \n                                       # missing value\nmissing_val_mask.sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check if all columns have Nan values for each row with any Nan value. "},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_val_mask.sum(axis=1).sum() == len(bitcoin_df.columns) * missing_val_mask.sum().loc['Open']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confirmed!"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_missing_val = missing_val_mask.sum().loc['Open'] # number of rows with missing values\nprint('There are {} rows with missing values which make up {}% of the total number of rows'.format( \\\n                                            n_missing_val, round(100*n_missing_val / bitcoin_df.shape[0], 3)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### From domain knowledge of Bitcoin trading, Weighted Price = Volume(Currency) / Volume(BTC). Let's check if this is true of our data. First, let's see what happens when Volume(BTC) = 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"bitcoin_df[bitcoin_df['Volume_(BTC)'] == 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### It seems when Volume(BTC) = 0, OHLC values are the same and Weighted_Price = OHLC constant values. Also, Volume(Currency) = 0. It looks like the situation that will occur when there are no tradings and it gives us a hint on how we will impute missing values.\n\n### So, we expect that the difference between (Volume(Currency) / Volume(BTC)) and Weighted_Price should be equal to (Total number of observations) - (total number of Nan values) - (number of observations where Volume(BTC) = 0). Let;s check it out!"},{"metadata":{"trusted":true},"cell_type":"code","source":"(np.round(np.abs(bitcoin_df['Volume_(Currency)'] / bitcoin_df['Volume_(BTC)'] - bitcoin_df['Weighted_Price']), \n    5) == 0).sum() == bitcoin_df.shape[0] - n_missing_val - bitcoin_df[bitcoin_df['Volume_(BTC)'] == 0].shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confirmed!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating a dataframe that contains only rows with missing values\nmissing_val_df = bitcoin_df[missing_val_mask.sum(axis=1) == 7].copy()\n\n# asserting that the number of rows in 'missing_val_df' is same as the number of rows with missing values in 'df'\nassert missing_val_df.shape[0] == n_missing_val ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating a 'date' column that contains only the date\nmissing_val_df['date'] = pd.Series(missing_val_df.index).apply(lambda x: x.date()).values\nmissing_val_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the duration of Nan values per day in minutes"},{"metadata":{"trusted":true},"cell_type":"code","source":"daily_Nan_minute = missing_val_df.groupby('date').apply(lambda x: x.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert daily_Nan_minute.sum() == n_missing_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"daily_Nan_minute.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('a day has {} minutes'.format(60*24))\nprint('maximum duration of Nan values in minute is {}mins which is {}% of the day'.format( \\\n                daily_Nan_minute.max(), round(100*daily_Nan_minute.max() / (60*24), 3)))\nprint('minimum duration of Nan values in minute is {}mins which is {}% of the day'.format( \\\n                daily_Nan_minute.min(), round(100*daily_Nan_minute.min() / (60*24), 3)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_val_df.tail(1) # checking the last time with Nan value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('1st quartile of timestamp with Nan value is {}'.format(missing_val_df.index[len(missing_val_df)//4]))\nprint('median of timestamp with Nan value is {}'.format(missing_val_df.index[len(missing_val_df)//2]))\nprint('3rd quartile of timestamp with Nan value is {}'.format(missing_val_df.index[3*(len(missing_val_df)//4)]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = plt.hist(missing_val_df.index) # checking the distribution of the Nan values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we see that the Nan values are skewed to the left. Nan values are heavily present in past years than earlier years."},{"metadata":{},"cell_type":"markdown","source":"### Let's see the reason for the missing values as stated by Zielak the provider of the dataset so we know how to handle them.\n\n> *CSV files for select bitcoin exchanges for the time period of Jan 2012 to September 2020, with minute to minute updates of OHLC (Open, High, Low, Close), Volume in BTC and indicated currency, and weighted bitcoin price. Timestamps are in Unix time. **TIMESTAMPS WITHOUT ANY TRADES OR ACTIVITY HAVE THEIR DATA FIELDS FILLED WITH NANS**...*"},{"metadata":{},"cell_type":"markdown","source":"### Zielak also told us about jumps in timestamps and here is the reason:\n> ***...IF A TIMESTAMP IS MISSING, OR IF THERE ARE JUMPS, THIS MAY BE BECAUSE THE EXCHANGE (OR ITS API) WAS DOWN, THE EXCHANGE (OR ITS API) DID NOT EXIST, OR SOME OTHER UNFORESEEN TECHNICAL ERROR IN THE DATA REPORTING OR GATHERING***. *All effort has been made to deduplicate entries and verify the contents are correct and complete to the best of my ability, but obviously trust at your own risk.*"},{"metadata":{},"cell_type":"markdown","source":"So, let's check for time jumps and contraction. Jump if any of the sampling rate is greater than a minute and contraction if it is less than a minute."},{"metadata":{"trusted":true},"cell_type":"code","source":"bitcoin_df['time_col'] = bitcoin_df.index\n# taking the first order difference to get the duration of Nan values\n\ntime_jump = (bitcoin_df['time_col'] - bitcoin_df['time_col'].shift()).dropna() \ndelta_time = time_jump.apply(lambda x: x.total_seconds()) # converts the duration to seconds\n\ndelta_time.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert delta_time.shape[0] + 1 == bitcoin_df.shape[0] # confirming the number of rows","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('There is {} time jump'.format((delta_time > 60).sum()))\nprint('There is {} time contraction'.format((delta_time < 60).sum()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So there is no time contraction, let's now find where exactly the time jump is."},{"metadata":{"trusted":true},"cell_type":"code","source":"jump_duration = delta_time[delta_time > 60].values[0]\nprint('time jump duration is {} minutes'.format(round(jump_duration / 60, 3)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ind_of_time_jump = delta_time.index.get_loc(delta_time[delta_time > 60].index[0])\ntime_jump_df = bitcoin_df.iloc[ind_of_time_jump: ind_of_time_jump+2]\ntime_jump_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total time jump in days is {}'.format(time_jump_df.index[-1] - time_jump_df.index[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although, there are more than one reasons for jumps in 'Timestamps' as stated by Zielak, one of worth noting is that \"it may be the exchange or its API was down or didn't exist. A jump of 4days 11hrs 53mins was observed which is 6,473mins (6,473 jumps). This jump is huge and we can conclude that it may not be due to error in data gathering rather, it may be due to a downtime in API or exchanges that don't exist. "},{"metadata":{},"cell_type":"markdown","source":"Let's check if there is any duplicate in 'Timestamp' values"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(bitcoin_df['time_col'].unique()) == bitcoin_df.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, we conclude that there are no duplicates in 'Timestamp' values!"},{"metadata":{},"cell_type":"markdown","source":"Let's check the total span of the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('the dataset spans for {}'.format(bitcoin_df.index[-1] - bitcoin_df.index[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now go back to the Nan values. this time, we will be handling them. since the cause of missingness is \"No trading activity\", it will be biased to use imputation methods like EMA or filling with a particular value since there was no trading activity in real sense.\n\nThere are two best ways to handle missing values of this nature:\n1. By filling 'Volume_(BTC)', and 'Volume_(Currency)' columns with zeros since there was no trading activity. Then, we do forward fill for 'Close' column and fill 'Open', 'High', 'Low' and 'Weighted_Price' columns with the values of the 'Close' column to make it a constant (horizontal) line of that peroid of inactivity.\n2. Dropping the rows with Nan values because in actual sense, there was no trade for those periods.\n\nWE WILL BE USING THE FIRST APPROACH TO BUILD OUR MODEL"},{"metadata":{},"cell_type":"markdown","source":"### Imputation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating a function to impute a given dataframe\ndef impute(df):\n    df_imputed = df.copy() # creating a copy of df to use for imputing\n    \n    # forward filling the 'Close' column\n    df_imputed['Close'].fillna(method='ffill', inplace=True)\n    \n    # imputing 'Volume_(BTC)' and 'Volume_(Currency)', with zeros\n    df_imputed[['Volume_(BTC)', 'Volume_(Currency)']] = \\\n    df_imputed[['Volume_(BTC)', 'Volume_(Currency)']].fillna(0)\n    \n    # copying the values of 'Close' column to the four columns of 'Open', 'High', 'Low' and 'Weighted_Price' \n    # that would later be used for imputing.\n    impute_cols = ['Open', 'High', 'Low', 'Weighted_Price']\n    impute_df = pd.DataFrame({k: df_imputed['Close'] for k in impute_cols})\n    \n    # imputing 'OHL' and 'Weighted_Price'\n    df_imputed[impute_cols] = df_imputed[impute_cols].fillna(impute_df)\n    \n    return df_imputed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bitcoin_df_imputed = impute(bitcoin_df) # imputes 'bitcoin_df'\nbitcoin_df_imputed.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(bitcoin_df_imputed.shape)\nbitcoin_df_imputed.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# making a list of all columns apart from 'time_col' column and'Weighted_Price' column\ncols = list(bitcoin_df_imputed.columns[:-2])\n\n# creating the appropriate subplot indices for the cols to be used for making plots\nind = list(range(1, 7))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create a function that would plot the columns with 'Timestamp' for a given dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_series_of_each_column(d_f, cols=cols):\n    fig = plt.figure(figsize=(18, 12))\n    for i, col in zip(ind, cols):\n        a = fig.add_subplot(4, 2, i)\n        plt.xlabel('year')\n        plt.ylabel(col)\n        plt.plot(d_f.index, d_f[col], '-')\n        plt.grid()\n    \n    a = fig.add_subplot(4, 1, 4)\n    plt.xlabel('year')\n    plt.ylabel('Weighted_Price')\n    plt.plot(d_f.index, d_f['Weighted_Price'], '-')\n    plt.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_series_of_each_column(bitcoin_df_imputed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Resampling"},{"metadata":{},"cell_type":"markdown","source":"### resampling to a daily time frame"},{"metadata":{"trusted":true},"cell_type":"code","source":"daily_Open = bitcoin_df_imputed['Open'].resample('D').first()\ndaily_High = bitcoin_df_imputed['High'].resample('D').max()\ndaily_Low = bitcoin_df_imputed['Low'].resample('D').min()\ndaily_Close = bitcoin_df_imputed['Close'].resample('D').last()\ndaily_Volume_BTC = bitcoin_df_imputed['Volume_(BTC)'].resample('D').sum()\ndaily_Volume_Currency = bitcoin_df_imputed['Volume_(Currency)'].resample('D').sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"daily_df = pd.DataFrame({'Open': daily_Open, 'High': daily_High, 'Low': daily_Low, 'Close': daily_Close, \\\n                         'Volume_(BTC)': daily_Volume_BTC, 'Volume_(Currency)': daily_Volume_Currency})\ndaily_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(daily_df['Volume_(BTC)'] == 0).sum() # checks if daily_Volume_BTC = 0 so we could calculate daily_Weighted_Price","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### checking fo Nan values"},{"metadata":{"trusted":true},"cell_type":"code","source":"daily_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"daily_df[daily_df.isnull().sum(axis=1) == 4] # pulling out the Nan values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"daily_df[daily_df[['Volume_(BTC)', 'Volume_(Currency)']].sum(axis=1) == 0] # pulling out rows where 'Volume(BTC)' \n                                                                           # and Volume(Currency)'s values are 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### It appeared that 'Volume(BTC) and Volume(Currency)'s values are zero where the other columns have Nan values and it as a result of the time jump in the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculates 'Weighted_Price' for the daily time frame\ndaily_df['Weighted_Price'] = daily_df['Volume_(Currency)'] / daily_df['Volume_(BTC)']\ndaily_df['Weighted_Price'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"daily_df[daily_df['Volume_(BTC)'] == 0] # checks what happens to 'Weighted_Price' when 'Volume_(BTC)' = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We see that Weighted_Price has Nan values where Volume_(BTC) = 0 in the daily time frame. So, we will repeat our imputation process."},{"metadata":{"trusted":true},"cell_type":"code","source":"daily_df_imputed = impute(daily_df)\ndaily_df_imputed.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert daily_df_imputed.isnull().sum().sum() == 0 # assert that Nan values have been handled.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('bitcoin_df_imputed has been downsampled from a minute timeframe of {}'.format(bitcoin_df_imputed.shape[0])\\\n      + ' observations to a daily timeframe of {} observations'.format(daily_df_imputed.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bitcoin_df.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Since the actual data ended on 2020-09-14 00:00:00, we will do away with the last observation because that day has only one observation in the minute time frame."},{"metadata":{"trusted":true},"cell_type":"code","source":"daily_df_imputed = daily_df_imputed.iloc[:-1].copy()\ndaily_df_imputed.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plots of the daily resampled series time series"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_series_of_each_column(daily_df_imputed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"daily_df_imputed.corr() # correlation dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlation heatmap\nmask = np.triu(daily_df_imputed.corr())\nplt.figure(figsize=(7, 7)) \nsns.heatmap(daily_df_imputed.corr(), mask=mask, xticklabels=True, yticklabels=True, cmap='coolwarm', annot=True)\n#xticklabels and yticklabels are set to True to display all columns in the heatmap\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### From the correlation heatmap, we see that OHLC and Weighted price are highly correlated"},{"metadata":{"trusted":true},"cell_type":"code","source":"daily_OHLCWp = daily_df_imputed.iloc[:, [0, 1, 2, 3, 6]].copy() # we will use a daily time frame for the rest \n                                    # series. Here, we make a dataframe of the remaining 5 series","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"daily_OHLCWp.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### OHLCWp stands for ````Open````, ````High````, ````Low````, ````Close````, and ````Weighted_Price````"},{"metadata":{},"cell_type":"markdown","source":"## Test for stationarity using the ADF TEST"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining a function to print out adf results\ndef get_adf_results(df):\n    results_cols = ['ADF_Stat', 'p_value', 'n_lags', 'n_observations', '1%', '5%', '10%']\n    results = {k: [] for k in results_cols}\n    \n    for col in df:\n    # for each column,\n        adf_result = adfuller(df[col]) # computes the adf result\n        \n        for i in range(len(results_cols)):\n            if i < 4:\n                results[results_cols[i]].append(adf_result[i])\n            else:\n                results[results_cols[i]].append(adf_result[4][results_cols[i]])\n                \n    return pd.DataFrame(results, index=df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_adf_results(daily_OHLCWp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### from the results, we see that daily OHLC and Weighted_Price are not stationary"},{"metadata":{},"cell_type":"markdown","source":"## Cointegration Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.vector_ar.vecm import coint_johansen","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining a function that will print out cointegration results\ndef get_coint_results(df, det_order=-1, k_ar_diff=1):\n    cj = coint_johansen(df, det_order, k_ar_diff)\n    \n    critical_trace = cj.trace_stat_crit_vals\n    critical_max_eigen = cj.max_eig_stat_crit_vals\n    eigen_vector = cj.evec[:, np.argmax(cj.eig)]\n    \n    results = {'Trace_stat': cj.lr1, '90% Trace': critical_trace[:, 0], '95% Trace': critical_trace[:, 1], \n               '99% Trace': critical_trace[:, 2], 'Max_eigen_stat': cj.lr2, '90% eigen': critical_max_eigen[:, 0], \n                '95% eigen': critical_max_eigen[:, 1], '99% eigen': critical_max_eigen[:, 2]}\n    \n    ind = ['r=0'] + ['r<='+str(i) for i in range(1, df.shape[1])]\n    \n    return pd.DataFrame(results, index=ind), eigen_vector","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is a [link](https://en.wikipedia.org/wiki/Vector_autoregression) to know more about cointegration"},{"metadata":{"trusted":true},"cell_type":"code","source":"coint_result_OHLCWp = get_coint_results(daily_OHLCWp) # gets cointegration results of OHLCWp\ncoint_result_OHLCWp[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### from the results, we see that OHLCWp are not cointegrating even to a threshold of 90%"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_adf_results(np.log(daily_OHLCWp)) # checks if the log transform of OHLCWp is stationary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### from the results, we see that the log transform of OHLCWp is not stationary also"},{"metadata":{"trusted":true},"cell_type":"code","source":"coint_result_log_OHLCWp = get_coint_results(np.log(daily_OHLCWp)) # checks if the log transform of OHLCWp are \n                                                              # cointegrating\ncoint_result_log_OHLCWp[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### from the results, we see that OHLCWp are cointegrating more than a threshold of 95%. So, there exist at least 5 cointegrating vectors(eigen vectors) when operated on the non-stationary OHLCWp, will make them stationary. Let's check them out!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# operating the non-stationary OHLC on the eigen vectors\nstationary_OHLCWp = np.matmul(np.array(np.log(daily_OHLCWp)), coint_result_log_OHLCWp[1].reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(daily_OHLCWp.index, stationary_OHLCWp) # checks the plots of after the operation in the eigen vectors\nplt.xlabel('years')\nplt.ylabel('Co-integrated OHLCWp')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### from the plot, we see that that although the log of OHLCWp themselves aren't stationary, they will become stationary after operation on the eigen vectors. Let's further confirm this by seeing the results of ADF"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_adf_results(pd.DataFrame({'OHLCWp': stationary_OHLCWp.flatten()}, index=daily_df_imputed.index))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### we now see that after operating the log of OHLCWp on the eigen vectors, they are will become stationary. We conclude that the log of OHLCWp are indeed cointegrated. We won't conduct cointegration test for the weekly btc and currency because they didn't meet the pre-requisite of cointegration. Btc according to ADF test is I(0) while Currency currency is I(0) after taking log transform. So, we proceed to applying differencing on the log of btc and currency."},{"metadata":{},"cell_type":"markdown","source":"## Causation Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import grangercausalitytests\nfrom statsmodels.tsa.vector_ar.var_model import VAR","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_grangers_causation_results(df, maxlag, test='ssr_chi2test'):    \n    results = []\n    \n    for row in df.columns:\n        row_result = []\n        \n        for col in df.columns:\n            test_result = grangercausalitytests(df[[col, row]], maxlag=maxlag, verbose=False)\n            p_values = [round(test_result[i+1][0][test][1], 4) for i in range(maxlag)]\n            min_p_value = np.min(p_values)\n            row_result.append(min_p_value)\n            \n        results.append(row_result)\n        \n    ind = [i + '_x' for i in df.columns]\n    cols = [i + '_y' for i in df.columns]\n    \n    return pd.DataFrame(np.array(results), index=ind, columns=cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating a time series split function\ndef ts_train_test_split(df, test_size):\n    len_test = int(df.shape[0]*test_size)\n    split_point = df.index[-len_test]\n    \n    train, test = df[df.index < split_point], df[df.index >= split_point]\n    \n    return train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting OHLCWp into train and test set\nOHLCWp_train, OHLCWp_test = ts_train_test_split(np.log(daily_OHLCWp), 0.25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_ohlcwp = VAR(OHLCWp_train) # initializes VAR object\nmodel_ohlcwp.select_order(maxlags=20).summary() # gets the summary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### from the AIC score, the best lag order to use for OHLCWp is 11"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_grangers_causation_results(np.log(daily_OHLCWp), 11)        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### From the causation test, we see clearly that almost all the p-values of OHLCWp are less than the 0.05 significant threshold, while just two of them are greater than 0.05 but less than 0.1 significant threshold. SO, we can conclude that the causation amongst OHLCWp is high."},{"metadata":{},"cell_type":"markdown","source":"## Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating a walk forward fuction that will split a time series into training and testing, and return a tuple of\n# the test set and the predicited test set\ndef walk_forward_VAR(df, test_size, lag_order):\n    \n    train, test = ts_train_test_split(df, test_size) # splits the data into train and test set\n        \n    history = [list(x) for x in np.array(train)] # makes a list of lists from the train set\n    forecasts = [] # empty list to store our forecasts\n    \n    for t in range(test.shape[0]): # for each test set,\n        model = VAR(np.array(history)) # initialize a VAR model on the train set,              \n        model_fit = model.fit(lag_order) # fit the model\n        yhat = model_fit.forecast(model.y, steps=1) # forecast just the next time step\n        forecasts.append(list(yhat[0])) # add the forecast to the forecasts list\n        history.append(list(np.array(test)[t])) # add the actual test value as the last observation of the \n                                                # train set \n            \n    # return a tuple of the test set and the predicited test set as a tuple of dataframes \n    return test, pd.DataFrame(np.array(forecasts), index=test.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# OHLC walk forward\nOHLCWp_VAR_results_test, OHLCWp_VAR_results_pred = walk_forward_VAR(np.log(daily_OHLCWp), 0.25, 11)\nOHLCWp_VAR_results = np.exp(OHLCWp_VAR_results_test), np.exp(OHLCWp_VAR_results_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing mean squared error metric\nfrom sklearn.metrics import mean_squared_error as mse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining mean absolute percentage error metric\ndef mape(df_true, df_pred, epsilon=1e-4):\n    df_true[df_true == 0] = epsilon # replaces every occurence of zero in the true values with epsilon\n    \n    error = df_true - df_pred\n    \n    return 100 * np.mean(np.abs(error/df_true), axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining a function to return both rmse and mape scores\ndef get_rmse_and_mape(df_true, df_pred):\n    results = {'RMSE': mse(np.array(df_true), np.array(df_pred), multioutput='raw_values', squared=False), \n               'MAPE (%)': mape(np.array(df_true), np.array(df_pred))}\n    \n    return pd.DataFrame(results, index=df_true.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_rmse_and_mape(OHLCWp_VAR_results[0], OHLCWp_VAR_results[1]) # gets the rmse and mape scores of OHLC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plots of our predictions and the true values for OHLCWp with VAR model\nfig = plt.figure(figsize=(18, 8))\nfor i, col_test, col_pred in zip(range(1, 6), OHLCWp_VAR_results[0], OHLCWp_VAR_results[1]):\n    if i == 5:\n        a = fig.add_subplot(3, 1, 3)\n        plt.plot(OHLCWp_VAR_results[0][col_test])\n        plt.plot(OHLCWp_VAR_results[1].iloc[:, col_pred])\n        plt.legend(['true', 'pred'])\n        plt.xlabel('days')\n        plt.ylabel(col_test)\n        \n    else:\n        a = fig.add_subplot(3, 2, i)\n        plt.plot(OHLCWp_VAR_results[0][col_test])\n        plt.plot(OHLCWp_VAR_results[1].iloc[:, col_pred])\n        plt.legend(['true', 'pred'])\n        plt.xlabel('days')\n        plt.ylabel(col_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plots of our predictions and the true values for OHLCWp with VAR model\nfig = plt.figure(figsize=(18, 8))\nfor i, col_test, col_pred in zip(range(1, 6), OHLCWp_VAR_results[0], OHLCWp_VAR_results[1]):\n    if i == 5:\n        a = fig.add_subplot(3, 1, 3)\n        plt.plot(OHLCWp_VAR_results[0][col_test] - OHLCWp_VAR_results[1].iloc[:, col_pred])\n        plt.xlabel('days')\n        plt.ylabel(col_test)\n        \n    else:\n        a = fig.add_subplot(3, 2, i)\n        plt.plot(OHLCWp_VAR_results[0][col_test] - OHLCWp_VAR_results[1].iloc[:, col_pred])\n        plt.xlabel('days')\n        plt.ylabel(col_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plots of our predictions and the true values for OHLCWp with VAR model\nfig = plt.figure(figsize=(18, 8))\nfor i, col_test, col_pred in zip(range(1, 6), OHLCWp_VAR_results[0], OHLCWp_VAR_results[1]):\n    if i == 5:\n        a = fig.add_subplot(3, 1, 3)\n        plt.hist(OHLCWp_VAR_results[0][col_test] - OHLCWp_VAR_results[1].iloc[:, col_pred])\n        plt.xlabel('days')\n        plt.ylabel(col_test)\n        \n    else:\n        a = fig.add_subplot(3, 2, i)\n        plt.hist(OHLCWp_VAR_results[0][col_test] - OHLCWp_VAR_results[1].iloc[:, col_pred])\n        plt.xlabel('days')\n        plt.ylabel(col_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}