{"nbformat_minor":1,"nbformat":4,"metadata":{"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py","mimetype":"text/x-python","name":"python","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.3"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"cells":[{"metadata":{"_uuid":"6b8662d87a2448618b0dff71e7169002199e3731","_cell_guid":"b298f6f5-f2c2-4ea1-ae6c-ef244e512598"},"cell_type":"markdown","source":"# Feature Selection/Ranking Methods\n\nSelecting good features may lead to\nless overfitting by reducing the number of parameters\nless computation time\nimproved accuracy (especially on high dimensional data)\n\nWe will explore important features of this 'Gender Recognition by Voice' dataset using various methods.\n1. Model Based Ranking\n2. Regularization (L1 and L2)\n3. Univariate Feature Selection\n4. Recursive Feature Elimination\n5. Random Forest Feature Importance\n6. Stability Selection\n\n\nWe won't cover dimensionality reduction techniques like Principal Component Analysis (PCA) or Linear Discriminant Analysis (LDA) and focus on common feature selection techniques.\n"},{"execution_count":null,"metadata":{"_uuid":"3274536d60e558dfda7c0d3bc8129e24dc9cbba6","_cell_guid":"92830fb1-3f9a-4c4f-81dd-d3cf65d737ec","collapsed":true},"outputs":[],"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"metadata":{"_uuid":"b823c7cfd241cef6e14df1d2918743307d3d7e75","_cell_guid":"eed5a5c4-b7ad-4789-9eef-1e92a35634a2"},"cell_type":"markdown","source":"**Data preprocessing**"},{"execution_count":null,"metadata":{"_uuid":"9e211857b9bd8e7ec961cef8b611bc2e8e09d0be","_cell_guid":"35b65098-5f3c-4868-9fdb-dec26ee13c72","collapsed":true},"outputs":[],"cell_type":"code","source":"data = pd.read_csv('../input/voice.csv')"},{"execution_count":null,"metadata":{"_uuid":"cdd510ee467bb28aaebc75e85e007efcc2496de9","_cell_guid":"07864cca-7f8f-462f-bcf6-a9621431bab5","collapsed":true},"outputs":[],"cell_type":"code","source":"data.sample(5)"},{"execution_count":null,"metadata":{"_uuid":"a49165fd3be718eb6348c95ccff7485f2f4cd409","_cell_guid":"5001315b-4852-42d3-ac1e-46fb6b62c94e","collapsed":true},"outputs":[],"cell_type":"code","source":"sns.heatmap(data.corr())"},{"execution_count":null,"metadata":{"_uuid":"99e864d586d42e36adc66fc83ccfcef08a09c007","_cell_guid":"1f7ad85a-3d12-4815-9934-b3a381a926e4","collapsed":true},"outputs":[],"cell_type":"code","source":"data.info()"},{"execution_count":null,"metadata":{"_uuid":"51695aa31e9bace81dd45b39425bbdeebeae3b98","_cell_guid":"1676266a-b21d-42f5-a2b0-fb5a675bb658","collapsed":true},"outputs":[],"cell_type":"code","source":"data.isnull().any()"},{"execution_count":null,"metadata":{"_uuid":"a8059b7a1b762ec214e9e27aa790d042b7ea1f5c","_cell_guid":"888f7dde-c177-4c05-a373-b6ad622925f0","collapsed":true},"outputs":[],"cell_type":"code","source":"data.label.value_counts()"},{"metadata":{"_uuid":"d5172011010dbb7aec303fd5170f0681dc8aa256","_cell_guid":"109757fc-6239-494b-b50d-1825bf5abe94"},"cell_type":"markdown","source":"We see that there are 3168 instances, 20 features and a class label. \nThere are no missing data values in any columns.\nThere are equal number of females and males at 1584 instances each, "},{"execution_count":null,"metadata":{"_uuid":"f744e3a1c41806b22d63657d358b59a21f6dd5d3","_cell_guid":"909a2b91-56d6-4614-af0a-6b9b8200a202","collapsed":true},"outputs":[],"cell_type":"code","source":"# convert class label into binary number, 1: female, 0: male\ndata.label = np.where(data.label.values == 'female', 1, 0)\ndata.label.value_counts()"},{"execution_count":null,"metadata":{"_uuid":"debe61ecf7e93c50875cfd8b7e48805815af11df","_cell_guid":"8f8b1fd8-14d5-48c3-96ec-320cbdbdb677","collapsed":true},"outputs":[],"cell_type":"code","source":"X = data.drop('label', axis = 1)\ny = data.label"},{"execution_count":null,"metadata":{"_uuid":"587d02a417b74b1cb924c3cf1ef30bd7e51c95ae","_cell_guid":"fe5b178c-9004-4382-95ce-2e5aee679611","collapsed":true},"outputs":[],"cell_type":"code","source":"X.shape, y.shape"},{"execution_count":null,"metadata":{"_uuid":"b73eefca2042cf571fad6dfbadba6bb29168bc17","_cell_guid":"6271b332-f097-46e7-b0e5-62076e258b5f","collapsed":true},"outputs":[],"cell_type":"code","source":"# from sklearn.model_selection import train_test_split\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=333)"},{"metadata":{"_uuid":"66342cf85318a5f7974babf56d7f5276a560957b","_cell_guid":"114d8089-2e22-4f1a-a6f8-c22877dd405b"},"cell_type":"markdown","source":"# 1. Model Based Ranking\n\nWe can fit a classfier to each feature and rank the predictive power.\nThis method selects the most powerful features individually but ignores the predictive power when features are combined.\n\nRandom Forest Classifier is used in this case because it is robust, nonlinear, and doesn't require scaling."},{"execution_count":null,"metadata":{"_uuid":"b4b2e0d4389918d6537e235859c37d4435bef7e2","_cell_guid":"2a7fde04-dc89-4bff-8b7c-f332c8d82412","collapsed":true},"outputs":[],"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\nclf = RandomForestClassifier(n_estimators = 50, max_depth = 4)\n\nscores = []\nnum_features = len(X.columns)\nfor i in range(num_features):\n    col = X.columns[i]\n    score = np.mean(cross_val_score(clf, X[col].values.reshape(-1,1), y, cv=10))\n    scores.append((int(score*100), col))\n\nprint(sorted(scores, reverse = True))\n\n"},{"execution_count":null,"metadata":{"_uuid":"f4186e2649691ed7548865216f00647c72a4e96b","_cell_guid":"faa0ecfd-18a9-4846-9eea-94dc1b632c37","collapsed":true},"outputs":[],"cell_type":"code","source":"def print_best_worst (scores):\n    scores = sorted(scores, reverse = True)\n    \n    print(\"The 5 best features selected by this method are :\")\n    for i in range(5):\n        print(scores[i][1])\n    \n    print (\"The 5 worst features selected by this method are :\")\n    for i in range(5):\n        print(scores[len(scores)-1-i][1])"},{"metadata":{"_uuid":"917ef540eb63551086d6259d9d000ebc8485dd55","_cell_guid":"432638ee-e466-4cd7-aa0a-5a6278d22b65"},"cell_type":"markdown","source":"'meanfun', 'IQR', 'Q25', 'sd' are among the most important features, and \n'modindx', 'minfun', 'maxfun', 'Q75' are among the least important features."},{"metadata":{"_uuid":"6dd11e6845943a86c957f94e3ea5bfa68d86b825","_cell_guid":"c1dcf656-7df5-40b2-aa86-0a8fad631e7e"},"cell_type":"markdown","source":"# 2-1 L1-regularization (Lasso)\n\nL1-regularization adds L1 penalty to the parameters which forces many parameters to be zero as regularization strength increases. Thus weak features should have zero as coefficients \n"},{"execution_count":null,"metadata":{"_uuid":"7d959a171ee917cf0086fccc79b2a26f64e91ff3","_cell_guid":"8a48a6a2-b182-45a7-b3b6-d2b60ea3a839","collapsed":true},"outputs":[],"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n# {'logisticregression__C': [1, 10, 100, 1000]\nparam_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10, 100]}\npipe = make_pipeline(StandardScaler(), LogisticRegression(penalty = 'l1'))     \ngrid = GridSearchCV(pipe, param_grid, cv = 10)\ngrid.fit(X, y)\nprint(grid.best_params_)"},{"metadata":{"_uuid":"12b3e151ba824abb8555e1ef7005fba99f5f3f9e","_cell_guid":"2818f9b0-f71d-4ff4-9ceb-1d0fe8533882"},"cell_type":"markdown","source":"Using pipeline to avoid data leakage during scaling, grid search was used to find the best C.\nHigh C means low regularization strength and low C means high regularization strength.\nThe best C chosen by the grid search was 0.1."},{"execution_count":null,"metadata":{"_uuid":"e4d256f8220605ca226313b877105fb780e9f79f","_cell_guid":"16f22325-5da7-4f23-98c7-1b338e5db054","collapsed":true},"outputs":[],"cell_type":"code","source":"X_scaled = StandardScaler().fit_transform(X)\nclf = LogisticRegression(penalty = 'l1', C = 0.1)\nclf.fit(X_scaled,y)"},{"execution_count":null,"metadata":{"_uuid":"862555b6c71eb561a58a1a57ed2d49677b40dd1f","_cell_guid":"4d1b8c4d-bf4b-4b4a-82ef-a13d4b71c728","collapsed":true},"outputs":[],"cell_type":"code","source":"zero_feat = []\nnonzero_feat = []\n# type(clf.coef_)\nfor i in range(num_features):\n    coef = clf.coef_[0,i]\n    if coef == 0:\n        zero_feat.append(X.columns[i])\n    else:\n        nonzero_feat.append((coef, X.columns[i]))\n        \nprint ('Features that have coeffcient of 0 are: ', zero_feat)\nprint ('Features that have non-zero coefficients are:')\nprint (sorted(nonzero_feat, reverse = True))\n        "},{"metadata":{"_uuid":"ebd924c61a83f29eb574828bf9b96ec9ac772a13","_cell_guid":"7fb7898f-0168-4af9-9350-097de80edef0"},"cell_type":"markdown","source":"Features 'meanfreq', 'sd', 'median', 'Q25', 'kurt', 'sp.ent', 'centroid', 'maxfun', 'meandom', 'mindom', 'maxdom', 'dfrange' were zeroed out by lasso.\n\nFeatures 'meanfun', 'skew', 'sfm', 'modindx', 'mode', 'Q75', 'minfun', 'IQR' survived."},{"metadata":{"_uuid":"e4da3f1c11fde367853f3daa9380bfef54ee3673","_cell_guid":"166b7ddf-9e08-4718-a607-f048c84aeedc"},"cell_type":"markdown","source":"# 2-2 L2-regularization (Ridge)\n\nL2-regularization is numerically more stable and produces more consistent coefficients than L1, however, it does not cause sparsity (coefficients do not get zeroed out)."},{"execution_count":null,"metadata":{"_uuid":"715a5b860c46b2b6edf85ec7a1cda9670bbfbbd6","_cell_guid":"287b0bfe-4ab3-4470-8db6-7f106d991c3d","collapsed":true},"outputs":[],"cell_type":"code","source":"param_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10, 100]}\npipe = make_pipeline(StandardScaler(), LogisticRegression(penalty = 'l2'))     \ngrid = GridSearchCV(pipe, param_grid, cv = 10)\ngrid.fit(X, y)\nprint(grid.best_params_)"},{"metadata":{"_uuid":"12e666b761bef80e6daf22a0918edf359fa9e023","_cell_guid":"41ad780c-72e1-43c0-b244-3e8fe0bba0bf"},"cell_type":"markdown","source":"The best C chosen by the grid search was 1."},{"execution_count":null,"metadata":{"_uuid":"0a9611a1f90de373fc60297b1467344d918fe323","_cell_guid":"e2b18dc6-300d-46d7-b6ea-fc0575b99534","collapsed":true},"outputs":[],"cell_type":"code","source":"X_scaled = StandardScaler().fit_transform(X)\nclf = LogisticRegression(penalty = 'l2', C = 1)\nclf.fit(X_scaled,y)"},{"execution_count":null,"metadata":{"_uuid":"d8d8df6c2e5d743f37bedbec3ceb46e283380267","_cell_guid":"7d447413-8d46-4dcf-af9a-a7d70f8623ed","collapsed":true},"outputs":[],"cell_type":"code","source":"abs_feat = []\nfor i in range(num_features):\n    coef = clf.coef_[0,i]\n    abs_feat.append((abs(coef), X.columns[i]))\n        \nprint (sorted(abs_feat, reverse = True))"},{"execution_count":null,"metadata":{"_uuid":"bf5811c6c60a600dafc17cb66be2f5736560e51e","_cell_guid":"ca686a86-9aaa-462d-adfa-4c0d09aa8812","collapsed":true},"outputs":[],"cell_type":"code","source":"print_best_worst(abs_feat)"},{"metadata":{"_uuid":"55e3b21573362dcc17f9b96c32c48f36481a0ddd","_cell_guid":"db221eee-c0d9-49e6-b78d-45a89f70d452"},"cell_type":"markdown","source":"# 3. Univariate Feature Selection\n\nUnivariate feature selection selects the best features by running univariate statistical tests like chi-squared test, F-1 test, and mutual information methods.\n\n\n"},{"execution_count":null,"metadata":{"_uuid":"e129fcb08b490171c03479aff2f165b0c1951827","_cell_guid":"e979b9dd-cf5c-4fd0-b96b-51e39c7f44d3","collapsed":true},"outputs":[],"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2, mutual_info_classif\n\ntest = SelectKBest(score_func=chi2, k=2)\ntest.fit(X, y)"},{"execution_count":null,"metadata":{"_uuid":"12dca4b0814b1a944be99eaba90bfb5cf85f513e","_cell_guid":"92a29290-77bb-429a-82af-a6396975e7f3","collapsed":true},"outputs":[],"cell_type":"code","source":"scores = []\nfor i in range(num_features):\n    score = test.scores_[i]\n    scores.append((score, X.columns[i]))\n        \nprint (sorted(scores, reverse = True))"},{"execution_count":null,"metadata":{"_uuid":"b638700cae15552bee9aa8124e51a5d18fe7fa06","_cell_guid":"16476c3a-12d1-4b0f-960a-7b29b4b22e9c","collapsed":true},"outputs":[],"cell_type":"code","source":"print_best_worst(scores)"},{"execution_count":null,"metadata":{"_uuid":"a69a24392fb7d2a09dcd3df146e84b7b2dea4271","_cell_guid":"a8d3d53e-2c59-494d-9230-f70d3eb11bac","collapsed":true},"outputs":[],"cell_type":"code","source":"test = SelectKBest(score_func = mutual_info_classif, k=2)\ntest.fit(X, y)"},{"execution_count":null,"metadata":{"_uuid":"29c59eca78ecccfb7adc4ca8ea9adad84e3b0d8c","_cell_guid":"6d2f15f6-2bad-490e-8920-1a72a9f5b4ae","collapsed":true},"outputs":[],"cell_type":"code","source":"scores = []\nfor i in range(num_features):\n    score = test.scores_[i]\n    scores.append((score, X.columns[i]))\n        \nprint (sorted(scores, reverse = True))"},{"execution_count":null,"metadata":{"_uuid":"cc54b325d86d48430402be086e6dab07f9851608","_cell_guid":"83fe296b-281f-4532-b6b9-8cd438d3368c","collapsed":true},"outputs":[],"cell_type":"code","source":"print_best_worst(scores)"},{"metadata":{"_uuid":"5e325cd40aaf90d85a43cda89c0ff63db662a38d","_cell_guid":"8d5bccfc-3059-4daf-b38b-a33416d5002b"},"cell_type":"markdown","source":"# 4. Recursive Feature Elimination (RFE)\n\nRecursive Feature Elimination (RFE) recursively selects important subsets of features based on built-in attributes like coefficients or feature importance of a given estimator. Hence RFE heavily depends on which estimator we are using.\n\n"},{"execution_count":null,"metadata":{"_uuid":"668600701f78046e1496f2d9813cfb187a3b95ea","_cell_guid":"7e44980a-66c5-4b98-91f5-6044d78e71c4","collapsed":true},"outputs":[],"cell_type":"code","source":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\n\nrfe = RFE(LogisticRegression(), n_features_to_select=1)\nrfe.fit(X,y)"},{"execution_count":null,"metadata":{"_uuid":"7b58bdf2739d6591d81f7bdbb650f8b0ceb12499","_cell_guid":"35e53103-5e73-4d44-b3ed-7e23f8f5d873","collapsed":true},"outputs":[],"cell_type":"code","source":"scores = []\nfor i in range(num_features):\n    scores.append((rfe.ranking_[i],X.columns[i]))\n    \nprint_best_worst(scores)"},{"execution_count":null,"metadata":{"_uuid":"174ecac6c53c258bb06909f447116edf0b2b0568","_cell_guid":"de031063-9299-45af-b337-ac4ae6b798b7","collapsed":true},"outputs":[],"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrfe = RFE(RandomForestClassifier(), n_features_to_select = 1)\nrfe.fit(X,y)"},{"execution_count":null,"metadata":{"_uuid":"6f228c8ad3101fc4d462f9f0dd53f8f233dede43","_cell_guid":"47e6c2a0-ec6c-44d6-9b7e-9f72f02b357a","collapsed":true},"outputs":[],"cell_type":"code","source":"scores = []\nfor i in range(num_features):\n    scores.append((rfe.ranking_[i],X.columns[i]))\n    \nprint_best_worst(scores)"},{"metadata":{"_uuid":"e68e072ec53a922277c7cdf885e35b2d54dc686d","_cell_guid":"2eebc60d-3808-438b-bf6f-6011fd843443"},"cell_type":"markdown","source":"# 5. Random Forest Feature Importance\n\n\n# 5-1. Mean Decrease Impurity\n\nEmsembled tree classifiers like random forest or extra trees that use bagging have multiple decision trees that are grown by decreasing impurities based on measures like information gain or Gini impurity. When a node in a tree splits, the impurity of the tree is decreased by drawing a decision line on one of the features. The impurity decrease of each feature for a forest then can be averaged over many trees.\n"},{"execution_count":null,"metadata":{"_uuid":"6b38e184aea3e7888e322aa835589a7b59c7703c","_cell_guid":"c0f2b97a-c549-48d8-8098-97d147a38fec","collapsed":true},"outputs":[],"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier()\nclf.fit(X,y)\n"},{"execution_count":null,"metadata":{"_uuid":"0bb99e7b6d76f2322a5cba988576070a49f1f4bc","_cell_guid":"9a49c342-e164-43eb-ac5b-694599cab364","collapsed":true},"outputs":[],"cell_type":"code","source":"scores = []\nfor i in range(num_features):\n    scores.append((clf.feature_importances_[i],X.columns[i]))\n        \nprint_best_worst(scores)"},{"metadata":{"_uuid":"d22851cf85855892da52ea374db046a17e44d9dc","_cell_guid":"e6e7897c-f4a0-425c-872c-bd5623cb7963"},"cell_type":"markdown","source":"# 5-2: Mean Decrease Accuracy\n\nFor mean decrease accuracy, we measure the decline in accuracy when we shuffle or exclude a single feature.\nShuffling or exlucding an important feature should result in a drop in accuracy.\n"},{"execution_count":null,"metadata":{"_uuid":"c49d9325a819fe4ad4d7d91a51802f2f3e352ce2","_cell_guid":"b9e2f841-579c-48c4-8037-5670c83db24a","collapsed":true},"outputs":[],"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nscores = []\nclf = RandomForestClassifier()\nscore_normal = np.mean(cross_val_score(clf, X, y, cv = 10))\n\n# X_shuffled = X.copy()\n# np.random.shuffle(X_shuffled[X.columns[i]])\n\n# X_shuffled.meanfreq\nfor i in range(num_features):\n    X_shuffled = X.copy()\n    scores_shuffle = []\n    for j in range(3):\n        np.random.seed(j*3)\n        np.random.shuffle(X_shuffled[X.columns[i]])\n        score = np.mean(cross_val_score(clf, X_shuffled, y, cv = 10))\n        scores_shuffle.append(score)\n        \n    scores.append((score_normal - np.mean(scores_shuffle), X.columns[i]))\n    "},{"execution_count":null,"metadata":{"_uuid":"89ad1a71c794de370c1c3bdaef13207588532d04","_cell_guid":"9d6965e6-ebdd-4d70-882c-e7cc4e0308cb","collapsed":true},"outputs":[],"cell_type":"code","source":"scores,score_normal"},{"execution_count":null,"metadata":{"_uuid":"af545d76b6da7eabf058836b30d2e0755409985d","_cell_guid":"ce92cb99-c844-464f-91af-3bd4e2b3da60","collapsed":true},"outputs":[],"cell_type":"code","source":"print_best_worst(scores)"},{"metadata":{"_uuid":"2fd995a49631a45e9752a9def69768d1c973a021","_cell_guid":"1cae962e-01d7-4b34-ac51-4515f41b4e23"},"cell_type":"markdown","source":"# 6. Stability Selection\n\nStability selection method uses randomized lasso for regression and randomized logistic regression for classification.\nIt randomly subsamples instances and features, selects good features on each subset and aggregates the results.\nIt is straightforward to implement.\n"},{"metadata":{"_uuid":"f9adbf39b2a2eac660c323b4b4daa896774f737d","_cell_guid":"61cb9651-8781-46eb-b90f-1845a0a07817"},"cell_type":"markdown","source":""},{"execution_count":null,"metadata":{"_uuid":"d5ebc84a8df8a186f385e9ef6ad2eaeed9ccc75f","_cell_guid":"062ae383-0ce0-4986-826d-12473e9fb39c","collapsed":true},"outputs":[],"cell_type":"code","source":"from sklearn.linear_model import RandomizedLogisticRegression\n\nclf = RandomizedLogisticRegression()\nclf.fit(X,y)\n"},{"execution_count":null,"metadata":{"_uuid":"a6b698a13c3295b76760033c0388e50d95237e25","_cell_guid":"45777cda-4ae2-4f7c-8bf5-fee2a6f0854e","collapsed":true},"outputs":[],"cell_type":"code","source":"zero_feat = []\nnonzero_feat = []\n# type(clf.coef_)\nfor i in range(num_features):\n    coef = clf.scores_[i]\n    if coef == 0:\n        zero_feat.append(X.columns[i])\n    else:\n        nonzero_feat.append((coef, X.columns[i]))\n        \nprint ('Features that have coeffcient of 0 are: ', zero_feat)\nprint ('Features that have non-zero coefficients are:')\nprint (sorted(nonzero_feat, reverse = True))"},{"metadata":{"_uuid":"0cee69b13e11e46d25c496e91e9d34437c8ff46e","_cell_guid":"00ec22fe-7880-4bf8-bc9a-40e759f097e8"},"cell_type":"markdown","source":"# Conclusion\n\n\nFor feature selection, incorporating some of these techniques in combination and cross-validating should give reliable results.\nFor feature ranking, it is important to take each method with caution. It is recommended to take multiple subsets of your data and aggregate the results to ensure stability of the outcomes.\n"}]}