{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = '/kaggle/input/titanic-dataset-from-kaggle/train.csv'\n\ndata = pd.read_csv(data) \ndata.head() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Drop the columns that are unnecessary for Machine learning process"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = data.drop(['Name', 'Ticket','Cabin'],axis=1) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#No. of columns and rows\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reduce null values\nprint(df.isnull().sum()) \ndf.dropna(inplace = True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Since Sexal and embarked columns is filled with object data type, so we need to convert it into numerical form"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"df.head()\nsex = pd.get_dummies(df.Sex) \n\nsex.drop('female', inplace = True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Sex'] = sex\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embarked=pd.get_dummies(df.Embarked)\nembarked.head() \n\nembarked.drop('S',axis=1,inplace=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Adding embarked dataFrame with df\nembarked.head()\n\ndf = pd.concat([df, embarked],axis=1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()\ndf.drop('Embarked', axis=1, inplace=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.head()) \n\nX = df.drop('Survived', axis=1) \ny = df.Survived\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#feature data.\nX.head() \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning process\n\n1. split data into train "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\nlg_model = LogisticRegression(max_iter=40) \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1) \n\nlg_model.fit(X_train,y_train) \n\nprediction = lg_model.predict(X_test) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#evaluating the accuracy of the model\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, prediction) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check how much the predicted data and actual data are matched"},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.head(10) \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cross Validation score"},{"metadata":{},"cell_type":"markdown","source":"# Using different methods to increase the accuracy of the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\navg_score = cross_val_score(LogisticRegression(solver='liblinear', \n                                   multi_class='auto', \n                                   max_iter=45), X_train, y_train, cv=5) \navg_score.mean() \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# define the function for avoiding the repitation while coding"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVC\ndef get_score(model, X_train, X_test, y_train, y_test):\n    model.fit(X_train, y_train) \n    return model.score(X_test, y_test) \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_lg = get_score(LogisticRegression(solver='liblinear',max_iter=45, multi_class='auto'), \n    X_train, X_test, y_train, \n    y_test)\nscore_rg = get_score(RandomForestRegressor(), \n            X_train,X_test,y_train,y_test)         \nscore_svm = get_score(SVC(kernel='rbf'), X_train, X_test, \n                      y_train, y_test) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(score_lg) \nprint(score_rg) \nprint(score_svm) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lg_score = cross_val_score(LogisticRegression( solver = 'liblinear',max_iter=45, multi_class='auto'),X_train, y_train, cv=5) \nRfr_score = cross_val_score(RandomForestRegressor(max_leaf_nodes=100),X_train, y_train, cv=5) \nsvm_score = cross_val_score(SVC(),X_train, y_train, cv=5) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lg_score\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating the dataFrame for the result of cross_value score\nmy_data = pd.DataFrame({'Lg_score':lg_score,'Rfr_score': Rfr_score,'SVM_score':svm_score}) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_data.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define function to calculate cross val score and try to evaluate the result using hyper parameter tuning concept\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\ndef score_value(n_estimators):\n    \n    result = RandomForestRegressor(n_estimators, random_state =0) \n    score = -1* cross_val_score(result, X_train, y_train, cv=5, scoring='neg_mean_absolute_error') \n    return score.mean() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Here, we are calculating the mean absolute error of the model rather than score"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(score_value(50)) \n#using dictionary to fit the value\nresults={}\nfor i in range(50, 401,50):\n    results[i]=score_value(i) \n   # print(results) \n    \nprint(results) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for key, value in results.items():\n    print(key, value) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGboost - Xtreme Gradient Boosting, For optimizing MAE(Mean Absolute Error) at minimum Point\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\n\nmy_model = XGBRegressor(n_estimators=1000,learning_rate= 0.001) \nmy_model.fit(X_train, y_train) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\n\npreds = my_model.predict(X_test) \nMAE = mean_absolute_error(y_test, preds) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAE\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = my_model.score(X_test, y_test) \nscore\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### In this case getting proper results using GBosst regressor seems to be difficult than approaches used above"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}