{"cells":[{"cell_type":"markdown","source":"## Breast Cancer ( 1 = Malignant(M) vs 0 = Benign(B) ) ","metadata":{"_cell_guid":"6eb81f8b-dd22-4407-9d4b-a2678afeddfd","_uuid":"fb8c29a3901ab25efbddc0ce0afbbfe0430d249b"}},{"execution_count":null,"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n%matplotlib inline","metadata":{"collapsed":true,"_cell_guid":"3f095a6e-c0ef-496b-9fe4-675284fe8ab9","_uuid":"d7b66ad4aec8f36e5dc9ce980a002be7ab80a0f0"},"outputs":[]},{"cell_type":"markdown","source":"### Breast cancer dataset \nThis dataset consists of 10 features whose values are obtained in terms of Mean, Standard error and worst case.\nFirst two columns gives us the information about ID and The output itself (Maliganat or Benign).\n\n**Instruction for data-preprocessing**\n- First load the data.csv file to dataframe.\n- Then divide it to test and train dataset in 80:20.\n- Normalize the data i.e. feature normalization.","metadata":{"_cell_guid":"6b4a63ab-81c4-475c-9201-6bc6bd1d93f8","_uuid":"8eb6a30d72f883bc3c42b8de7a71599a007a71db"}},{"execution_count":null,"cell_type":"code","source":"#LOADING THE DATA From data.csv\ndf = pd.read_csv('../input/data.csv')\ndf['diagnosis'] = df['diagnosis'].map({'M':1,'B':0})\n\n#Complete data set:\nX = df[df.columns[2:32]]\nY = df['diagnosis']\nY = Y.values.reshape(Y.shape[0],1)\n\n\n#train set (80%):\ntrain_X = X.loc[0:454,X.columns[0:]]\ntrain_Y = Y[0:455]\n\n#test set (20%):\ntest_X = X.loc[0:143,X.columns[0:]]\ntest_Y = Y[0:144]\n","metadata":{"collapsed":true,"_cell_guid":"d1f5c8a4-7969-4478-b35b-95b4d85057fa","scrolled":false,"_uuid":"33284c97ebabc08208d566eb4838df799108cb3d"},"outputs":[]},{"cell_type":"markdown","source":"### Normalization\nLet's find out the mean and standered deviation for each column and substract each element from its mean and divide this by its standered deviation.","metadata":{"_cell_guid":"26292a72-7f50-48ab-ba74-d2cc386e2791","_uuid":"5756b1fa88fd1a47c3bf45c2ec59fdde14d5b7db"}},{"execution_count":null,"cell_type":"code","source":"#training set:\n\nmean = train_X.mean()\nstd_error = train_X.std()\ntrain_X = (train_X - mean)/std_error\n\n#test set:\nmean = test_X.mean()\nstd_error = test_X.std()\ntest_X = (test_X - mean)/std_error","metadata":{"collapsed":true,"_cell_guid":"34f597e4-90e7-4312-a790-e25867a7af14","scrolled":false,"_uuid":"20c52fdf8c2dd65156a7fb6fb0222a9c9e1f870e"},"outputs":[]},{"cell_type":"markdown","source":"### Shapes of all the sets\n**Shapes**\n- train_X\n- test_X\n- train_Y\n- test_Y","metadata":{"_cell_guid":"d0087018-ad8a-4797-a5c7-65792b4179d4","_uuid":"2fc411f90563b76b1d4f156ee9dce11d51ee3440"}},{"execution_count":null,"cell_type":"code","source":"print(\"Shape of train_X\",train_X.shape)\nprint(\"Shape of test_X\",test_X.shape)\nprint(\"Shape of train_Y\",train_Y.shape)\nprint(\"Shape of test_Y\",test_Y.shape)","metadata":{"_cell_guid":"ef41ca8e-43af-4210-8226-53e5566a8f52","_uuid":"fc03d37223dc0dfd6086943c42b6551c8fef1aab"},"outputs":[]},{"cell_type":"markdown","source":"### Algorithm for logistic regression\n**Steps**\n- random initialization of w and b\n- Forward propogation\n- Backward propogation\n- gradient descent","metadata":{"_cell_guid":"e16f6a0a-7068-4a59-984b-9a32f2045ebc","_uuid":"79ac4e53083019509ce8a54ebd32f137c9c99d45"}},{"cell_type":"markdown","source":"### Sigmoid function\nThis function is required for calculating the hypothesis i.e. y = a = sigmoid(z)\nwhere z = w^TX + b","metadata":{"_cell_guid":"46a72d7b-b99c-4be4-94b2-9094fa597497","_uuid":"732b65f72dc0c560415bacd5b5984ba4ff6dc567"}},{"execution_count":null,"cell_type":"code","source":"def sigmoid(z):\n    return 1/(1+np.exp(-z))","metadata":{"collapsed":true,"_cell_guid":"009a9f66-b8f6-40ed-bd7c-209e014ac817","_uuid":"44025b5154133c17830719f1f57bbe9f5df47e6b"},"outputs":[]},{"execution_count":null,"cell_type":"code","source":"sigmoid(np.array([1,2,3,3]))","metadata":{"_cell_guid":"8e652fb6-077b-429c-8183-874ab0032d49","_uuid":"9cdcafc7393e500da247ddd3d0ad8097f0b09f2d"},"outputs":[]},{"cell_type":"markdown","source":"\n### Random initialization of w and b","metadata":{"_cell_guid":"683a4366-a939-4466-959c-db5fd2327683","_uuid":"d83784f5f0470b6b9ffe75c18cfaf72185b75086"}},{"execution_count":null,"cell_type":"code","source":"def random_init(dim):\n    w = np.zeros((dim,1))\n    b = 0\n    \n    return w,b\n","metadata":{"collapsed":true,"_cell_guid":"b3e0347c-dc81-46e1-a046-0de2403be283","_uuid":"5f5e15ab92e844567763255e2aebe12ad70b527b"},"outputs":[]},{"execution_count":null,"cell_type":"code","source":"(random_init(train_X.shape[1]))","metadata":{"scrolled":true,"_cell_guid":"d13552e1-9191-4792-91fa-3894b53b1e5f","_uuid":"f4951f866c7cf5867038f45d001a5707c857e8bd"},"outputs":[]},{"cell_type":"markdown","source":"### Forward and backward propogation\nhypothesis in logistic regression is y = a = sigmoid(z) = sigmoid(w^TX + b)","metadata":{"_cell_guid":"257ca44c-c393-4582-a2b6-8da5236b6d83","_uuid":"e00eebe1b5ba6b5c587eba1b6c9f209bf4aa67aa"}},{"execution_count":null,"cell_type":"code","source":"def propo(w,b,X,Y):\n    \n    m = X.shape[0]\n    \n    #forward propogation\n    z = np.dot(X,w) + b\n    a = sigmoid(z)\n    cost = -np.sum(Y*np.log(a) - (1-Y)*np.log(1-a))/m\n    \n    \n    #backpropogation:\n    dz = a-Y\n    dw  = np.dot(np.transpose(X),dz)/m\n    db = np.sum(dz)/m\n    \n    grad = {\n        \"dw\":dw,\n        \"db\":db\n    }\n    return grad,cost","metadata":{"collapsed":true,"_cell_guid":"dada35e7-1d9c-4a4c-b34b-7c2958ddf69d","_uuid":"0c47a1d2e333082dd154842cea4fec0b4c887809"},"outputs":[]},{"cell_type":"markdown","source":"### Gradient descent over number of iteration","metadata":{"_cell_guid":"e0296caf-a302-44b0-84ef-acab4d25364d","_uuid":"60f703dbe6352646da7fb781e7785b128e1d19bb"}},{"execution_count":null,"cell_type":"code","source":"def optim(w,b,X,Y,learning_rate,num_iteration):\n    costs = []\n    \n    for i in range(num_iteration):\n        grads, cost=propo(w,b,X,Y)\n        \n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n        \n        #updating w and b\n        w  = w - learning_rate*dw\n        b  = b - learning_rate*db\n          \n        if(i%100==0):\n            costs.append(cost)\n        \n    params= {\n        \"w\":w,\n        \"b\":b\n    }\n    grads = {\n        \"dw\":dw,\n        \"db\":db\n    }\n    return params,grads,costs","metadata":{"collapsed":true,"_cell_guid":"23bf4f93-55f6-40b2-8cf6-72ce741309b1","_uuid":"310795928727c37322328ab015caa63d39fbef3d"},"outputs":[]},{"execution_count":null,"cell_type":"code","source":"#random init of w,b\nw,b = random_init(train_X.shape[1])\n\n#forward, backward & grad. descent:\n\nparams,grads,costs = optim(w,b,train_X,train_Y,0.01,2000)\n\nprint(params)\nprint(grads)\nprint(costs)","metadata":{"_cell_guid":"6ca00606-d8a4-4cdc-9ead-91392e9a3ad4","_uuid":"6a4184ea2b2ea74bc8a4caf1adb8fc04f166deb1"},"outputs":[]},{"cell_type":"markdown","source":"### Cost vs iteration graph\nFor checking learning rate","metadata":{"_cell_guid":"885f929d-0392-4744-871d-4794c0195b37","_uuid":"363690295f01ee15cc4f5293bfb9bb866c7aa7d7"}},{"execution_count":null,"cell_type":"code","source":"# plt.plot(cost_all,range(len(cost_all)))\ncosts = np.squeeze(costs)\nplt.plot(costs)\nplt.xlabel('No. of iteration')\nplt.ylabel('Cost')\nplt.show()","metadata":{"_cell_guid":"56e00435-2473-4de6-b2a1-fc8674804216","_uuid":"dedd885e2d59dfe30f7b6cea7934dffcf2d72b66"},"outputs":[]},{"execution_count":null,"cell_type":"code","source":"def predict(w,b,X):\n    a = sigmoid(np.dot(X,w) + b)\n    return a","metadata":{"collapsed":true,"_cell_guid":"a1e7e7cb-3d82-4539-8980-ff03ce07184b","_uuid":"4cff90f10d15e7849c6f77a3d2b8e1bf8551078b"},"outputs":[]},{"execution_count":null,"cell_type":"code","source":"def oneORzero(x):\n    if(x>=0.5):\n        return 1\n    elif(x<0.5):\n        return 0","metadata":{"collapsed":true,"_cell_guid":"b38f37bd-e6c5-405d-98ca-0feeaf598ede","_uuid":"f090277052107bb45e21f534c563a1f541c9e4ba"},"outputs":[]},{"cell_type":"markdown","source":"\n### Prediction accuracy for Train and test set","metadata":{"_cell_guid":"2563176e-8e15-46c6-8d5f-dccfcbbc1210","_uuid":"fb1b996aa3e1fb8d779582cbd659452d0da55e9b"}},{"execution_count":null,"cell_type":"code","source":"# Accuracy for training set:\ntemp = predict(params[\"w\"],params[\"b\"],train_X)\ntrain_prediction = np.array(list(map(oneORzero,temp)))\ntrain_prediction = train_prediction.reshape((train_prediction.shape[0],1))\n\n# Accuracy for test set:\ntemp = predict(params[\"w\"],params[\"b\"],test_X)\ntest_prediction = np.array(list(map(oneORzero,temp)))\ntest_prediction = test_prediction.reshape((test_prediction.shape[0],1))\n\nprint(\"Training set accuracy = \",(100 - np.mean(np.abs(train_prediction - train_Y))*100))\nprint(\"Test set accuracy = \",(100 - np.mean(np.abs(test_prediction - test_Y))*100))","metadata":{"_cell_guid":"db5e1bae-0efc-474d-9cb2-1451fcdb4a78","_uuid":"8a913401ae630d2933fa7f250118c11ad26e6572"},"outputs":[]}],"metadata":{"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","version":"3.6.4","nbconvert_exporter":"python","mimetype":"text/x-python","pygments_lexer":"ipython3","name":"python"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat_minor":1,"nbformat":4}