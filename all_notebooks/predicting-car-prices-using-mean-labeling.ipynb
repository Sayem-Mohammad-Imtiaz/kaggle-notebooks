{"cells":[{"metadata":{"_uuid":"71aa8634a9a974edb52f7aad5c23cbdd1e25a1ad"},"cell_type":"markdown","source":"The main purpose of this kernel is to represent a way of converting the categorical features in the feature set of car price prediction (like model, make, and such) into numerical features. This is usually done by labeling or using dummy features. However, a linear labeling method will not work as it will not be able to capture the correlation difference between various categories effectively. For instance, if I'm labeling Ferrari cars as 1 and Mazda cars as 2, it would not make sense for the difference between their lables (2 - 1) to be equal to the difference between Mazda and Toyota assuming Toyota was labeled as 3.\nAs for Dummy features, that conversion method is great for categorical data but it falls short when there are many categories to consider. For instance, there are 50 states which means we need at least 49 features just to represent the State. This causes problems because of the curse of dimensionality.\nIn order to overcome both problems, we went with a labeling method that isn't linear. Instead, we are processing the entire training set to collect means per category. Since we are predicting the price, we collected the mean of price per car make and used that as a label for all cars of that make. Same is done separately for both Model and State. This overcomes the dimensionality problem caused by the dummy features and also allows our model to better capture the variance of the categorical data in their labeled representaiton.\nWe call this method Mean Labeling."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"#Loading libraries \nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (10.0, 8.0)\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm","execution_count":1,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"edc8a90015b45b0ef4f26bee7ea006ddb0d3e9c5"},"cell_type":"code","source":"train = pd.read_csv(\"../input/car-prices/data.csv\") #Load the clean training data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6bea24acca3d39e019304f7a5df28871528f4477"},"cell_type":"code","source":"print ('The train data has {0} rows and {1} columns'.format(train.shape[0],train.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8db521f5e974177a060bb3ee57601b1cb49f9b39"},"cell_type":"code","source":"#check missing values\ntrain.columns[train.isnull().any()]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"901cdd727b6ac3d35660db89319592e57ea9fcfd"},"cell_type":"markdown","source":"Since there aren't any missing values, we don't have to worry about accounting for nulls in our data."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ba60d3a57b7f673d1d5452a1c55af9f358b95680"},"cell_type":"code","source":"numeric_data = train.select_dtypes(include=[np.number])\ncat_data = train.select_dtypes(exclude=[np.number])\nprint(\"There are {} numeric and {} categorical columns in train data\".format(numeric_data.shape[1],cat_data.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bd5070508da93457d07aa77d8c988a353d37bdbd"},"cell_type":"code","source":"#create numeric plots\nnum = [f for f in train.columns if train.dtypes[f] != 'object']\nnd = pd.melt(train, value_vars = num)\nn1 = sns.FacetGrid (nd, col='variable', col_wrap=4, sharex=False, sharey = False)\nn1 = n1.map(sns.distplot, 'value')\n\n#correlation plot\ncorr = numeric_data.corr()\nsns.heatmap(corr)\nprint (corr['Price'].sort_values(ascending=False), '\\n')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf1b1c3ec2cee174dfb78e4a67e638800ee8304f"},"cell_type":"markdown","source":"By splitting the data into numerical and categorical, we can further process them separately to see their effects on the price."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"acff62f3525261dc6661b7b1ce16f40ba740394a"},"cell_type":"code","source":"cat_data.describe()\nsp_pivot = train.pivot_table(index='Make', values='Price', aggfunc=np.mean).sort_values(by='Price') #Get mean price per make\nsp_pivot.plot(kind='bar',color='blue')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11957b860e1d1ce8791087bdb9bc059200a8fe5a"},"cell_type":"markdown","source":"Now we can see that the make does somehow correlate with the price. though the correlation isn't very obvious since the make is categorical and not numerical. However if we somehow can replace the categorical data with a numerical replacement, we can possibly express this correlation in a way that a regressor can identify and gain some knowledge about the data from. The same can be done for State and Model. We are skipping City as it's correlation is not likely to be very beneficial with such scattered data."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8fb2c765ffb7740d76e1db478919205dfb8e9d75"},"cell_type":"code","source":"#GrLivArea variable\nsns.jointplot(x=np.log(train['Mileage']), y=np.log(train['Price']))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ef6122ded44346ccea2105dcd3177a2fa84c538"},"cell_type":"markdown","source":"From this graph showing Mileage vs Price, we can start looking at outliers that may adversly affect our model and preemptively remove them. After processing, we decided to remove all entries with Mileage below 5000, All Models that have less than 5 entries in the training data and all entries with price over 60000$"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1058265c2f879c34f6e41fc261e4e9da1eda9a62"},"cell_type":"code","source":"X = train[['Year', 'Mileage', 'Make', 'State']] #Model emitted as there are thousands of models and XGB will not converge with that many features\nY = train.Price\nX = pd.get_dummies(data=X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"589613c9c0e7d007aeffd2200852fa98252b549a"},"cell_type":"code","source":"from sklearn.cross_validation import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = .20, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"61df088c8ebc191234412866e328356b219b150a"},"cell_type":"markdown","source":"XGBoost Regression with categorical data converted using the one hot encoding technique."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e7e900c0e10bdc5c15e4ec38ad2438ad5323922a"},"cell_type":"code","source":"#gradient booster with one hot conversion\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import cross_val_score\n\ngbr = GradientBoostingRegressor(loss ='ls', max_depth=6)\ngbr.fit (X_train, Y_train)\npredicted = gbr.predict(X_test)\nerr = Y_test - predicted\n\nplt.scatter(Y_test, err , color ='teal')\nplt.xlabel('Actual Price',fontsize=25)\nplt.ylabel('Error',fontsize=25)\n\nplt.show()\n\nfrom sklearn.metrics import mean_squared_error\nrmse = np.sqrt(mean_squared_error(Y_test, predicted))\nprint('RMSE:')\nprint(rmse)\n\nfrom sklearn.metrics import r2_score\nprint('Variance score: %.2f' % r2_score(Y_test, predicted))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"714ec793fd7a8ea337acd44aa0957914361fdd23"},"cell_type":"markdown","source":"As we can see, this training takes a very long time. It also isn't very accurate and the variance score is quite bad. We can definitely do better."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"The data contains categorical features like the City, State, Model, and Make information. While it's possible to use one hot encoding to convert it, this causes dimensionality problems as the number of features increases dramatically. So what we did was to group the training data by each of those categorical data separately and computing average of prices, then replacing the categorical data with the average computed. This maintains the correlation between the categorical data and the price as what we care about the most is the price."},{"metadata":{"trusted":true,"_uuid":"6ec4eaba5357426466aae30e1d8a95993827cd2e","collapsed":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/carpredictiondata/dataProc.csv\") #Load the processed data. Categorical features are converted to numerical ones\n                                                              #, and outliers are removed","execution_count":3,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f5030ca8dc162fc41f96077f02e69f00c73a34cf"},"cell_type":"code","source":"#plotting corr of the data we plan to use\nvisData = data[['Year', 'Mileage', 'MakeNum', 'StateNum', 'ModelNum', 'Price']] \ncorr = visData.corr()\nsns.heatmap(corr)\nprint (corr['Price'].sort_values(ascending=False), '\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3850f19cbf5c56e97bfce914f75dde4395866f87","collapsed":true},"cell_type":"code","source":"X = data[['Year', 'Mileage', 'MakeNum', 'StateNum', 'ModelNum']]\nY = data.Price\n\n#Split into training and test data\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = .20, random_state = 42)\nX_test.info","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"a91d44170bbb6c8aa2ff8381f5238a791c111414"},"cell_type":"markdown","source":"KNN Regression"},{"metadata":{"trusted":true,"_uuid":"f26b754d474c32c7919a37ff07635be3c452996d","collapsed":true},"cell_type":"code","source":"from sklearn import neighbors\nknn = neighbors.KNeighborsRegressor(n_neighbors=6)\nknn.fit(X_train, Y_train)\n\npredicted = knn.predict(X_test)\nerr = Y_test - predicted\n\nplt.scatter(Y_test, err , color ='teal')\nplt.xlabel('Actual Price',fontsize=25)\nplt.ylabel('Error',fontsize=25)\n\nplt.show()\n\nfrom sklearn.metrics import mean_squared_error\nrmse = np.sqrt(mean_squared_error(Y_test, predicted))\nprint('RMSE:')\nprint(rmse)\n\nfrom sklearn.metrics import r2_score\nprint('Variance score: %.2f' % r2_score(Y_test, predicted))\n\n# Uncomment the next 2 lines to produce a model file that you can use later\n#from sklearn.externals import joblib\n#joblib.dump(knn, 'model.pkl')","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"392116fc317bd898c734625781408acf6ac62630"},"cell_type":"markdown","source":"Decision Tree Regression"},{"metadata":{"trusted":true,"_uuid":"55d4f8f7e360909cdc0eb97d99eb4ac77e1af412","collapsed":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\ndtr = DecisionTreeRegressor(max_features='auto')\ndtr.fit(X_train, Y_train)\npredicted = dtr.predict(X_test)\nerr = Y_test - predicted\n\nplt.scatter(Y_test, err , color ='teal')\nplt.xlabel('Actual Price',fontsize=25)\nplt.ylabel('Error',fontsize=25)\n\n\nplt.show()\n\nfrom sklearn.metrics import mean_squared_error\nrmse = np.sqrt(mean_squared_error(Y_test, predicted))\nprint('RMSE:')\nprint(rmse)\n\nfrom sklearn.metrics import r2_score\nprint('Variance score: %.2f' % r2_score(Y_test, predicted))","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"68f0158a5464a97f175715dc212a1b670365af2f"},"cell_type":"markdown","source":"Linear Regression"},{"metadata":{"trusted":true,"_uuid":"4f090708345a950523eeffb43bb77e18de01bfb5","collapsed":true},"cell_type":"code","source":"from sklearn import linear_model\n\nregr = linear_model.LinearRegression()\nregr.fit(X_train, Y_train)\n\npredicted = regr.predict(X_test)\nerr = Y_test - predicted\n\nplt.scatter(Y_test, err , color ='teal')\nplt.xlabel('Actual Price',fontsize=25)\nplt.ylabel('Error',fontsize=25)\n\nplt.show()\n\nfrom sklearn.metrics import mean_squared_error\nrmse = np.sqrt(mean_squared_error(Y_test, predicted))\nprint('RMSE:')\nprint(rmse)\n\nfrom sklearn.metrics import r2_score\nprint('Variance score: %.2f' % r2_score(Y_test, predicted))","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"d1fd8f39e096bfc0df84f0d34f98247f710c8c18"},"cell_type":"markdown","source":"XGBoost Regression"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"de29c274f7eb98e9d7a953d362f84655bf750d0c"},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import cross_val_score\n\ngbr = GradientBoostingRegressor(loss ='huber', max_depth=6)\ngbr.fit (X_train, Y_train)\npredicted = gbr.predict(X_test)\nerr = Y_test - predicted\n\nplt.scatter(Y_test, err , color ='teal')\nplt.xlabel('Actual Price',fontsize=25)\nplt.ylabel('Error',fontsize=25)\n\nplt.show()\n\nfrom sklearn.metrics import mean_squared_error\nrmse = np.sqrt(mean_squared_error(Y_test, predicted))\nprint('RMSE:')\nprint(rmse)\n\nfrom sklearn.metrics import r2_score\nprint('Variance score: %.2f' % r2_score(Y_test, predicted))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa51b9df9465675a5bf5320b357de3a734334c7c"},"cell_type":"markdown","source":"One thing we notice about the graphs is the format of the error is very cylindrical. We looked for ways to reduce that and decided to look at the graphs of the Price and the negative correlated features (Mileage)"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7d3960619f73f88e930980d4e7616fb9808e32de"},"cell_type":"code","source":"sns.distplot(data.Price)\nsns.distplot(data.Mileage)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5ebf53952bd8b9aa8d0066e83609495a918cf28"},"cell_type":"markdown","source":"The graphs show that the data is very skewed. We can simply use the log function to convert it into a more uniform distribution"},{"metadata":{"trusted":true,"_uuid":"eb7c23f8e48ea7ee4c788c936e03e0c6c0b5fe19","collapsed":true},"cell_type":"code","source":"X = data[['Year', 'Mileage', 'MakeNum', 'StateNum', 'ModelNum']] #CityNum is state, misnamed in data\nY = np.log(data.Price)\nX['Mileage'] = np.log(X['Mileage'])\nX['Mileage'] = 0.9 * X['Mileage'] + 0.1\n\n#Split into training and test data\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = .20, random_state = 42)","execution_count":39,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a44c1ee3208800f309aed77dc3edd360660beb3e","collapsed":true},"cell_type":"code","source":"sns.distplot(Y)\nsns.distplot(X['Mileage'])","execution_count":40,"outputs":[]},{"metadata":{"_uuid":"e960952fa6c606cceb57e2205b7281c7562b3d35"},"cell_type":"markdown","source":"KNN Regression after normalization"},{"metadata":{"trusted":true,"_uuid":"47a20b116ccc93265bdc3297306276cdfa1eaee1","collapsed":true},"cell_type":"code","source":"from sklearn import neighbors\nknn = neighbors.KNeighborsRegressor(n_neighbors=6)\nknn.fit(X_train, Y_train)\n\npredicted = knn.predict(X_test)\nerr = Y_test - predicted\n\np2 = knn.predict(X_train)\nerr2 = Y_train - p2\n\nplt.scatter(Y_test, err , color ='teal')\nplt.xlabel('Actual Price',fontsize=25)\nplt.ylabel('Error',fontsize=25)\n\nplt.show()\n\nfrom sklearn.metrics import mean_squared_error\nrmse = np.sqrt(mean_squared_error(Y_test, predicted))\nprint('RMSE:')\nprint(rmse)\n\nfrom sklearn.metrics import r2_score\nprint('Variance score: %.2f' % r2_score(Y_test, predicted))","execution_count":42,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"d14adf305d293236e19c33e31ac774ed1f333820","collapsed":true},"cell_type":"code","source":"#Compare error of training data vs error of test data. We need to get the exp of the data since we used log before\nxperr = np.exp(Y_test) - np.exp(predicted)\nxperr2 = np.exp(Y_train) - np.exp(p2)\nsns.distplot(xperr)\nsns.distplot(xperr2)\n\nfrom sklearn.metrics import r2_score\nprint('Variance score: %.2f' % r2_score(Y_train, p2))","execution_count":45,"outputs":[]},{"metadata":{"_uuid":"59aa651fabca099313f6bcfc23a0590d726429b3"},"cell_type":"markdown","source":"XGBoost"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"2c30af9309bb2c0c2554f4cc8b6352c2d3cd861b","collapsed":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import cross_val_score\n\ngbr = GradientBoostingRegressor(loss ='huber', max_depth=6)\ngbr.fit (X_train, Y_train)\npredicted = gbr.predict(X_test)\nerr = Y_test - predicted\n\nplt.scatter(Y_test, err , color ='teal')\nplt.xlabel('Actual Price',fontsize=25)\nplt.ylabel('Error',fontsize=25)\n\nplt.show()\n\nfrom sklearn.metrics import mean_squared_error\nrmse = np.sqrt(mean_squared_error(Y_test, predicted))\nprint('RMSE:')\nprint(rmse)\n\nfrom sklearn.metrics import r2_score\nprint('Variance score: %.2f' % r2_score(Y_test, predicted))","execution_count":48,"outputs":[]},{"metadata":{"_uuid":"883da086f083e3419df7c50f01cb40daf42817c1"},"cell_type":"markdown","source":"As we can see, our score improved after normalization as well. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}