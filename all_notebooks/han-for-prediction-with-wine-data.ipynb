{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from __future__ import print_function, division\n\nimport os\nimport re\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n# import tensorflow as tf2\nfrom bs4 import BeautifulSoup\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.compat.v1.keras.layers import GRUCell\nfrom tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_sentences = 100\nmax_words = 20000\nmaxlen = 300\nembedding_dim = 100\nvalidation_split = 0.2\nhidden_size = 150\nattention_size = 50\nkeepprob = 0.8\nbatch_size = 256\nnum_epochs = 15\nloss_delta = 0.5\nmodel_path = './model'\nglove_dir = \"../input/glove6b100dtxt/\"\nreviews = []\nlabels = []\ntexts = []\nembeddings_index = {}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def attention(inputs, att_size, time_major=False, return_alphas=False):\n    \"\"\"\n    Attention mechanism layer which reduces RNN/Bi-RNN outputs with Attention vector.\n    \"\"\"\n    if isinstance(inputs, tuple):\n        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n        inputs = tf.concat(inputs, 2)\n\n    if time_major:\n        # (T,B,D) => (B,T,D)\n        inputs = tf.array_ops.transpose(inputs, [1, 0, 2])\n\n    hiddensize = inputs.shape[2].value  # D value - hidden size of the RNN layer\n\n    # Trainable parameters\n    w_omega = tf.Variable(tf.random_normal([hiddensize, att_size], stddev=0.1))\n    b_omega = tf.Variable(tf.random_normal([att_size], stddev=0.1))\n    u_omega = tf.Variable(tf.random_normal([att_size], stddev=0.1))\n\n    with tf.name_scope('v'):\n        # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n        #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n        v = tf.tanh(tf.tensordot(inputs, w_omega, axes=1) + b_omega)\n\n    # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n    vu = tf.tensordot(v, u_omega, axes=1, name='vu')  # (B,T) shape\n    alphas = tf.nn.softmax(vu, name='alphas')  # (B,T) shape\n\n    # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)\n\n    if not return_alphas:\n        return output\n    else:\n        return output, alphas\n\n\ndef batch_generator(X, y, batchsize):\n    size = X.shape[0]\n    x_copy = X.copy()\n    y_copy = y.copy()\n    ind = np.arange(size)\n    np.random.shuffle(ind)\n    x_copy = x_copy[ind]\n    y_copy = y_copy[ind]\n    i = 0\n    while True:\n        if i + batchsize <= size:\n            yield x_copy[i:i + batchsize], y_copy[i:i + batchsize]\n            i += batchsize\n        else:\n            i = 0\n            ind = np.arange(size)\n            np.random.shuffle(ind)\n            x_copy = x_copy[ind]\n            y_copy = y_copy[ind]\n            continue\n\n\ndef remove_html(str_a):\n    p = re.compile(r'<.*?>')\n    return p.sub('', str_a)\n\n\n# replace all non-ASCII (\\x00-\\x7F) characters with a space\ndef replace_non_ascii(str_a):\n    return re.sub(r'[^\\x00-\\x7f]', r'', str_a)\n\n\n# Tokenization/string cleaning for dataset\ndef clean_str(string):\n    string = re.sub(r\"\\\\\", \"\", string)\n    string = re.sub(r\"\\'\", \"\", string)\n    string = re.sub(r\"\\\"\", \"\", string)\n    return string.strip().lower()\n\n\n# input_data = pd.read_csv('../input/kumarmanoj-bag-of-words-meets-bags-of-popcorn/labeledTrainData.tsv', sep='\\t')\ninput_data = pd.read_csv(\"../input/wine-reviews/winemag-data-130k-v2.csv\")\n\ndf = input_data[['description', 'points']]\nclaps = df['points']\ntext = df['description']\ntext = np.array(text)\n\n# for idx in range(input_data.review.shape[0]):\n#     text = BeautifulSoup(input_data.review[idx], features=\"html5lib\").decode('utf-8')\n#     text = clean_str(text)\n#     texts.append(text)\n#     labels.append(input_data.sentiment[idx])\n\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(text)\nsequences = tokenizer.texts_to_sequences(text)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\ndata = pad_sequences(sequences, maxlen=maxlen)\n\n# labels = to_categorical(np.asarray(labels))\nclaps = np.array(claps)\nprint('Shape of reviews (data) tensor:', data.shape)\nprint('Shape of sentiment (label) tensor:', claps.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"indices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = claps[indices]\nnb_validation_samples = int(validation_split * data.shape[0])\n\nx_train = data[:-nb_validation_samples]\ny_train = labels[:-nb_validation_samples]\nx_val = data[-nb_validation_samples:]\ny_val = labels[-nb_validation_samples:]\n\nprint(\"X train shape\", x_train.shape)\nprint(\"Y train shape\", y_train.shape)\n\nprint('Number of positive and negative reviews in training and validation set')\nprint(y_train.sum(axis=0))\nprint(y_val.sum(axis=0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Total %s word vectors.' % len(embeddings_index))\n\n# building Hierachical Attention network\nembedding_matrix = np.random.random((len(word_index) + 1, embedding_dim))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n\n# Different placeholders\nwith tf.name_scope('Input_layer'):\n    input_x = tf.placeholder(tf.int32, [None, maxlen], name='input_x')\n    output_y = tf.placeholder(tf.float32, [None], name='output_y')\n    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n\n# Embedding layer\nwith tf.name_scope('Embedding_layer'):\n    embeddings_var = tf.Variable(tf.random_uniform([len(word_index) + 1, embedding_dim], -1.0, 1.0), trainable=True)\n    tf.summary.histogram('embeddings_var', embeddings_var)\n    batch_embedded = tf.nn.embedding_lookup(embeddings_var, input_x)\n\n# BiDirectional RNN Layer\nrnn_outputs, _ = bi_rnn(GRUCell(hidden_size), GRUCell(hidden_size), inputs=batch_embedded, dtype=tf.float32)\ntf.summary.histogram('RNN_outputs', rnn_outputs)\n\n# Attention layer\nwith tf.name_scope('Attention_layer'):\n    attention_output, alphas = attention(rnn_outputs, attention_size, return_alphas=True)\n    tf.summary.histogram('alphas', alphas)\n\n# Dropout for attention layer\ndrop = tf.nn.dropout(attention_output, keep_prob)\n\n# Fully connected layer\nwith tf.name_scope('Fully_connected_layer'):\n    W = tf.Variable(tf.truncated_normal([hidden_size * 2, 1], stddev=0.1))  # Hidden size is multiplied by 2 for Bi-RNN\n    b = tf.Variable(tf.constant(0., shape=[1]))\n    y_hat = tf.nn.xw_plus_b(drop, W, b)\n    y_hat = tf.squeeze(y_hat)\n    tf.summary.histogram('W', W)\n\nwith tf.name_scope('Metrics'):\n    # Cross-entropy loss and optimizer initialization\n    loss = tf.reduce_mean(tf.keras.losses.MSE(y_hat, output_y))\n    tf.summary.scalar('loss', loss)\n    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss)\n\n    # Accuracy metric\n#     accuracy = tf.reduce_mean(tf.keras.losses.MAE(y_hat, output_y))\n\n    total_error = tf.reduce_sum(tf.square(tf.subtract(y_hat, tf.reduce_mean(y_hat))))\n    unexplained_error = tf.reduce_sum(tf.square(tf.subtract(y_hat, output_y)))\n    accuracy = tf.subtract(1.0, tf.math.divide(unexplained_error, total_error))\n    tf.summary.scalar('R-squared', accuracy)\n\nmerged = tf.summary.merge_all()\n\n# Batch generators\ntrain_batch_generator = batch_generator(x_train, y_train, batch_size)\ntest_batch_generator = batch_generator(x_val, y_val, batch_size)\n\ntrain_writer = tf.summary.FileWriter('./logdir/train', accuracy.graph)\ntest_writer = tf.summary.FileWriter('./logdir/test', accuracy.graph)\n\nsession_conf = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n\nsaver = tf.train.Saver()\n\nif __name__ == \"__main__\":\n    with tf.Session(config=session_conf) as sess:\n        sess.run(tf.global_variables_initializer())\n        print(\"Begin training...\")\n        for epoch in range(num_epochs):\n            loss_train = 0\n            loss_test = 0\n            accuracy_train = 0\n            accuracy_test = 0\n\n            print(\"epoch: {}\\t\".format(epoch), end=\"\")\n\n            # Training in batches\n            num_batches = x_train.shape[0] // batch_size\n            for b in tqdm(range(num_batches)):\n                x_batch, y_batch = next(train_batch_generator)\n\n                loss_tr, acc, _, summary = sess.run([loss, accuracy, optimizer, merged],\n                                                    feed_dict={input_x: x_batch, output_y: y_batch,\n                                                               keep_prob: keepprob})\n                accuracy_train += acc\n                loss_train = loss_tr * loss_delta + loss_train * (1 - loss_delta)\n                train_writer.add_summary(summary, b + num_batches * epoch)\n            accuracy_train /= num_batches\n\n            print(\"Training complete...\")\n            # Testing\n            num_batches = x_val.shape[0] // batch_size\n            for b in tqdm(range(num_batches)):\n                x_batch, y_batch = next(test_batch_generator)\n\n                loss_test_batch, acc, summary = sess.run([loss, accuracy, merged],\n                                                         feed_dict={input_x: x_batch, output_y: y_batch,\n                                                                    keep_prob: 1.0})\n                accuracy_test += acc\n                loss_test += loss_test_batch\n                test_writer.add_summary(summary, b + num_batches * epoch)\n            accuracy_test /= num_batches\n            loss_test /= num_batches\n\n            print(\"loss: {:.3f}, val_loss: {:.3f}, acc: {:.3f}, val_acc: {:.3f}\".format(loss_train, loss_test,\n                                                                                        accuracy_train, accuracy_test))\n        train_writer.close()\n        test_writer.close()\n        saver.save(sess, model_path)\n        print(\"Run 'tensorboard --logdir=./logdir' to checkout tensorboard logs.\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}