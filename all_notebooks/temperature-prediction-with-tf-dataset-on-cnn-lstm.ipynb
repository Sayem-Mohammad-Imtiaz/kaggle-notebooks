{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nIn this notebook, I am creating a tensorflow based timeseries forecasting model using CNN & LSTM.\n\n## Acknowledgments:\nThis notebook is inspired by the course 4 of TensorFlow in Practice Specialization which is [Sequences, Time Series and Prediction](https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction) by Laurence Moroney.\n\nI used this course to prepare for the tensorflow speciality examination, and I am using the methods and codes that Mr. Moroney used in the course. \n\n\n## Sections:\n1. Introduction\n2. Importing and exploring the dataset\n3. Imputting missing values\n4. Naive forecast\n5. Moving average forecast\n6. Preparing a pre-fetched tensorflow dataset\n7. Creating a CNN-LSTM based model\n8. Model metrics\n9. Conclusion\n10. References\n\n\nIf you find this notebook helpful for you, please upvote!"},{"metadata":{},"cell_type":"markdown","source":"I always like to import the libraries in the alphabetical order so that is it easy to review when needed"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 20)\n\nimport tensorflow as tf\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing and exploring the dataset"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/daily-temperature-of-major-cities/city_temperature.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking if all the cities has the data for a full range"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['City'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I wanted to develop a timeseries model for a single city. For this purpouse, I am taking the city Chennai (previously known as Madras), from Tamil Nadu, India. The city where I reside.\n\nChennai generally has only two season. It is hot for almost throughout the year, and rains in November/December months."},{"metadata":{"trusted":true},"cell_type":"code","source":"chennai = data[data[\"City\"] == \"Chennai (Madras)\"]\nchennai.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking if all the year has complete records"},{"metadata":{"trusted":true},"cell_type":"code","source":"chennai[\"Year\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Imputing missing values\n\nThe dataset has recorded missing values with the number -99. The chennai dataset has missing values close to 29 records. \n\nI will use forward fill method to impute the missing values for the dataset. That is, we will take the previously non missing value and fill it in the place of the missing value.\n\nFirst replacing -99 with np.nan"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"-99 is put in place of missing values. \nWe will have to forward fill with the last non missing value before -99\n\"\"\"\nchennai[\"AvgTemperature\"] = np.where(chennai[\"AvgTemperature\"] == -99, np.nan, chennai[\"AvgTemperature\"])\nchennai.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now using ffill() method to fill the np.nan that we created"},{"metadata":{"trusted":true},"cell_type":"code","source":"chennai[\"AvgTemperature\"] = chennai[\"AvgTemperature\"].ffill()\nchennai.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since there is no single column that contains the date, creating a new column called Time_steps to combine the year month and date fields"},{"metadata":{"trusted":true},"cell_type":"code","source":"chennai.dtypes\nchennai[\"Time_steps\"] = pd.to_datetime((chennai.Year*10000 + chennai.Month*100 + chennai.Day).apply(str),format='%Y%m%d')\nchennai.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_series(time, series, format=\"-\", start=0, end=None):\n    \"\"\"to plot the series\"\"\"\n    plt.plot(time[start:end], series[start:end], format)\n    plt.xlabel(\"Year\")\n    plt.ylabel(\"Temprature\")\n    plt.grid(True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting the timeseries for the entire duration"},{"metadata":{"trusted":true},"cell_type":"code","source":"time_step = chennai[\"Time_steps\"].tolist()\ntemprature = chennai[\"AvgTemperature\"].tolist()\n\nseries = np.array(temprature)\ntime = np.array(time_step)\nplt.figure(figsize=(10, 6))\nplot_series(time, series)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting for recent one year only"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplot_series(time[-365:], series[-365:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are totally 9,266 records on the dataset. We will keep 8000 records for training (85%) and keep remaining 15% for testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"split_time = 8000\ntime_train = time[:split_time]\nx_train = series[:split_time]\ntime_valid = time[split_time:]\nx_valid = series[split_time:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Naive forecast\n\nIn naive forecast, we will take the record in month - 1 (the month previously) and assume that it will be carried forward for the next observation also."},{"metadata":{"trusted":true},"cell_type":"code","source":"naive_forecast = series[split_time - 1:-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, naive_forecast)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the plot above is so crowded, we will take for a small section of the dataset and visualize it."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Zoom in and see only few points\nplt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid, start=0, end=150)\nplot_series(time_valid, naive_forecast, start=1, end=151)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tf.keras.metrics.mean_squared_error(x_valid, naive_forecast).numpy())\nprint(tf.keras.metrics.mean_absolute_error(x_valid, naive_forecast).numpy())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Moving average forecast\n\nIn moving average forecast, we will take the value of average for the previous window period and take it as the prediction for the next period."},{"metadata":{"trusted":true},"cell_type":"code","source":"def moving_average_forecast(series, window_size):\n    \"\"\"Forecasts the mean of the last few values.\n     If window_size=1, then this is equivalent to naive forecast\"\"\"\n    forecast = []\n    for time in range(len(series) - window_size):\n        forecast.append(series[time:time + window_size].mean())\n    return np.array(forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"moving_avg = moving_average_forecast(series, 30)[split_time - 30:]\n\nplt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, moving_avg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tf.keras.metrics.mean_squared_error(x_valid, moving_avg).numpy())\nprint(tf.keras.metrics.mean_absolute_error(x_valid, moving_avg).numpy())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Differencing\n\nWe will use a technique called differencing to remove the trend and seasonality from the data. \nHere we difference the data between what the value was 365 days (1 year back). The differencing should always follow the seasonal pattern. "},{"metadata":{"trusted":true},"cell_type":"code","source":"diff_series = (series[365:] - series[:-365])\ndiff_time = time[365:]\n\nplt.figure(figsize=(10, 6))\nplot_series(diff_time, diff_series)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diff_moving_avg = moving_average_forecast(diff_series, 50)[split_time - 365 - 50:]\n\nplt.figure(figsize=(10, 6))\nplot_series(time_valid, diff_series[split_time - 365:])\nplot_series(time_valid, diff_moving_avg)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Restoring trend and seasonality\nBut these are just the forecast of the differenced timeseries. To get the value for the original timeseries, we have to add back the value of t-365"},{"metadata":{"trusted":true},"cell_type":"code","source":"diff_moving_avg_plus_past = series[split_time - 365:-365] + diff_moving_avg\n\nplt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, diff_moving_avg_plus_past)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tf.keras.metrics.mean_squared_error(x_valid, diff_moving_avg_plus_past).numpy())\nprint(tf.keras.metrics.mean_absolute_error(x_valid, diff_moving_avg_plus_past).numpy())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Smoothing with moving average again\n\nThe above plot has a lot of noise. To smooth it again, we do a moving average on that"},{"metadata":{"trusted":true},"cell_type":"code","source":"diff_moving_avg_plus_smooth_past = moving_average_forecast(series[split_time - 370:-360], 10) + diff_moving_avg\n\nplt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, diff_moving_avg_plus_smooth_past)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tf.keras.metrics.mean_squared_error(x_valid, diff_moving_avg_plus_smooth_past).numpy())\nprint(tf.keras.metrics.mean_absolute_error(x_valid, diff_moving_avg_plus_smooth_past).numpy())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# How to prepare a window dataset?\n\nA window dataset is used in the dataset prepration of the tensorflow. It yields a prefetched dataset with the x and y variables as tensors. \n\n### Step 1: Converting the numpy array into a tensor using tensor_slices"},{"metadata":{"trusted":true},"cell_type":"code","source":"series1 = tf.expand_dims(series, axis=-1)\nds = tf.data.Dataset.from_tensor_slices(series1[:20])\nfor val in ds:\n    print(val.numpy())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 2: tf window option groups 5 (window size) into a single line\n\nBut for the last observations for which there are no observations to group will be kept as remaining as in the outupt of this cell"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndataset = ds.window(5, shift=1)\nfor window_dataset in dataset:\n    for val in window_dataset:\n        print(val.numpy(), end=\" \")\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 3: Drop reminder set to True will drop the variables which are not having the grouping"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = ds.window(5, shift=1, drop_remainder=True)\nfor window_dataset in dataset:\n    for val in window_dataset:\n        print(val.numpy(), end=\" \")\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 4: flat map option will group the 5 observation in a single tensor variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = ds.window(5, shift=1, drop_remainder=True)\ndataset = dataset.flat_map(lambda window: window.batch(5))\nfor window in dataset:\n    print(window.numpy())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 5: map option will split the variables into X and y variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = ds.window(5, shift=1, drop_remainder=True)\ndataset = dataset.flat_map(lambda window: window.batch(5))\ndataset = dataset.map(lambda window: (window[:-1], window[-1:]))\nfor x,y in dataset:\n    print(x.numpy(), y.numpy())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 6: shuffle option will shuffle the dataset into random order.\n\nTill the previous step, the observation would have been in the correct order. the shuffle will ensure that the data are randomly mixed up"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = ds.window(5, shift=1, drop_remainder=True)\ndataset = dataset.flat_map(lambda window: window.batch(5))\ndataset = dataset.map(lambda window: (window[:-1], window[-1:]))\ndataset = dataset.shuffle(buffer_size=10)\nfor x,y in dataset:\n    print(x.numpy(), y.numpy())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 7: Batch option will put the variables into mini-batches suitable for training. It will group both X and y into mini batches"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = ds.window(5, shift=1, drop_remainder=True)\ndataset = dataset.flat_map(lambda window: window.batch(5))\ndataset = dataset.map(lambda window: (window[:-1], window[-1:]))\ndataset = dataset.shuffle(buffer_size=10)\ndataset = dataset.batch(2).prefetch(1)\nfor x,y in dataset:\n    print(\"x = \", x.numpy())\n    print(\"y = \", y.numpy())\n    print(\"*\"*25)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Window size is how many observations in the past do you want to see before making a prediction.\nBatch size is similar to mini-batches set while training the neural network"},{"metadata":{"trusted":true},"cell_type":"code","source":"window_size = 60\nbatch_size = 32\nshuffle_buffer_size = 1000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n    \"\"\"\n    To create a window dataset given a numpy as input\n    \n    Returns: A prefetched tensorflow dataset\n    \"\"\"\n    series = tf.expand_dims(series, axis=-1)\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n    ds = ds.shuffle(shuffle_buffer)\n    ds = ds.map(lambda w: (w[:-1], w[1:]))\n    return ds.batch(batch_size).prefetch(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Finding the correct learning rate\n\nUsing a call back for LearningRateScheduler(). For every epoch this just changes the learning rate a little so that the learning rate varies from 1e-8 to 1e-6\n\nAlso a new loss function Huber() is introduced which is less sensitive to outliers."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\nwindow_size = 64\nbatch_size = 256\ntrain_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\nprint(train_set)\nprint(x_train.shape)\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Conv1D(filters=32, kernel_size=5,\n                      strides=1, padding=\"causal\",\n                      activation=\"relu\",\n                      input_shape=[None, 1]),\n  tf.keras.layers.LSTM(64, return_sequences=True),\n  tf.keras.layers.LSTM(64, return_sequences=True),\n  tf.keras.layers.Dense(30, activation=\"relu\"),\n  tf.keras.layers.Dense(10, activation=\"relu\"),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 400)\n])\n\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8 * 10**(epoch / 20))\n\noptimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\nmodel.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])\nhistory = model.fit(train_set, epochs=100, callbacks=[lr_schedule])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We plot this on a semilog axis"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\nplt.axis([1e-8, 1e-4, 0, 60])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We take the step where the learning rate drops the steepest to train our neural network."},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\ntrain_set = windowed_dataset(x_train, window_size=60, batch_size=100, shuffle_buffer=shuffle_buffer_size)\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Conv1D(filters=60, kernel_size=5,\n                      strides=1, padding=\"causal\",\n                      activation=\"relu\",\n                      input_shape=[None, 1]),\n  tf.keras.layers.LSTM(60, return_sequences=True),\n  tf.keras.layers.LSTM(60, return_sequences=True),\n  tf.keras.layers.Dense(30, activation=\"relu\"),\n  tf.keras.layers.Dense(10, activation=\"relu\"),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 400)\n])\n\n\noptimizer = tf.keras.optimizers.SGD(lr=1e-7, momentum=0.9)\nmodel.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])\nhistory = model.fit(train_set,epochs=500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_forecast(model, series, window_size):\n    \"\"\"\n    Given a model object and a series for it to predict, this function will return the prediction\n    \"\"\"\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size))\n    ds = ds.batch(32).prefetch(1)\n    forecast = model.predict(ds)\n    return forecast","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)\nrnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, rnn_forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss=history.history['loss']\n\nepochs=range(len(loss)) # Get number of epochs\n\n\n#------------------------------------------------\n# Plot training and validation loss per epoch\n#------------------------------------------------\nplt.plot(epochs, loss, 'r')\nplt.title('Training loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend([\"Loss\"])\n\nplt.figure()\n\nzoomed_loss = loss[200:]\nzoomed_epochs = range(200,500)\n\n\n#------------------------------------------------\n# Plot training and validation loss per epoch\n#------------------------------------------------\nplt.plot(zoomed_epochs, zoomed_loss, 'r')\nplt.title('Training loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend([\"Loss\"])\n\nplt.figure()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\nI have demonstrated in this notebook how to use naive forecast, moving average forecast and build a model in CNN and LSTM using tensorflow dataset prepration.\n\nIf you find this notebook helpful for you, please upvote!"},{"metadata":{},"cell_type":"markdown","source":"# References:\n1. https://github.com/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%204%20-%20S%2BP/S%2BP%20Week%204%20Lesson%205.ipynb\n2. https://thispointer.com/python-pandas-how-to-display-full-dataframe-i-e-print-all-rows-columns-without-truncation/\n3. https://stackoverflow.com/questions/19350806/how-to-convert-columns-into-one-datetime-column-in-pandas"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}