{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Let's enjoy <font color=\"Blue\">Chinese charactors' dataset</font>!","metadata":{}},{"cell_type":"code","source":"#Follow Kaggle's way to load datasets.\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Import libraries","metadata":{}},{"cell_type":"code","source":"#import libraries\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras as kr\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.layers import Dense, Activation, Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.utils import np_utils\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Read the dataset as Padas DataFrame","metadata":{}},{"cell_type":"code","source":"#change the dataset into Pandas dataframe\nimport pandas as pd\ndataset = pd.read_csv(\"/kaggle/input/chinese-mnist-digit-recognizer/chineseMNIST.csv\")\ndataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### I see :-) The true label isn't the first column but the the last column.\n### I should do labelencoding in the last column.\n","metadata":{}},{"cell_type":"markdown","source":"## 3. Let's start labelencoding.etc!","metadata":{}},{"cell_type":"code","source":"#Let's start labelencoding!\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nencoded = le.fit_transform(dataset['character'].values)\ndecoded = le.inverse_transform(encoded)\ndataset['character2'] = encoded\nprint('This dataset has following true labels, ', le.classes_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now I know this dataset has 15 classifications.","metadata":{}},{"cell_type":"markdown","source":"## 4.Let's make train data and test data!","metadata":{}},{"cell_type":"code","source":"#Then, make a dictionary. \ndic={0:'一',1:'七',2:'万',3:'三',4:'九',5:'二',6:'五',7:'亿',8:'八',9:'六',10:'十',11:'千',12:'四',13:'百',14:'零'}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Shuffle data :-)\ndataset_shuffled = dataset.sample(frac=1, random_state=0)\n#split the shuffled dataset into explanation data and target data.\nx =dataset_shuffled.iloc[:,0:4096]\ny =dataset_shuffled.iloc[:,[4098]]\n#Let's use sklearn's train_test_split function\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#normalizes data from 1 to 0. \nx_train = x_train.astype('float32')/255\ny_train = y_train.astype('float32')\nx_test = x_test.astype('float32')/255\ny_test = y_test.astype('float32')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Check the shape of dataset' images","metadata":{}},{"cell_type":"code","source":"#Check the shape of dataset' images\nimport numpy as np\nnp.sqrt(4096)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now I know this dataset has 64*64 images.","metadata":{}},{"cell_type":"markdown","source":"<HR>\nBy the way, Let's check what kinda Chinese charactors the dataset has!\n<HR>","metadata":{}},{"cell_type":"markdown","source":"## 6. Let's check real images of the dataset.","metadata":{}},{"cell_type":"code","source":"#import numpy and give a seed. \nimport numpy as np\nnp.random.seed(31)\n#Show 3 letter at random and convert them into gray scale letters. \nfor i in range(3):\n    plt.imshow(x_train.iloc[np.random.randint(0,15000)].values.reshape(64,64),cmap='Greys')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#change data into numpy array\nx_train= x_train.to_numpy()\ny_train= y_train.to_numpy()\nx_test= x_test.to_numpy()\ny_test= y_test.to_numpy()\n\n#reshape train data and test data into 64 * 64 * 1channel\nx_train = x_train.reshape(x_train.shape[0], 64, 64, 1).astype('float32')\nx_test = x_test.reshape(x_test.shape[0], 64, 64, 1).astype('float32')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#convert y_train and y_test into 15 categories\ny_train = kr.utils.to_categorical(y_train, 15)\ny_test = kr.utils.to_categorical(y_test, 15)\nnum_classes = y_train.shape[1]\nnum_classes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Build an ordinary \"Deep Learning\" model with CNN and maxpooling by using TensorFlow Keras.","metadata":{}},{"cell_type":"code","source":"#Build an ordinary \"Deep Learning\" model with CNN and maxpooling by using Keras.\nmodel = Sequential()\nmodel.add(Conv2D(32, (5, 5), input_shape=(64, 64, 1), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\n#Choose an optimizer and compile the model.\nmodel.compile(optimizer = Adam(learning_rate = 0.01), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n#And print the summary of the model.\nprint(model.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7.Model Fitting","metadata":{}},{"cell_type":"code","source":"#model fitting\nmodel1 = model.fit(x_train, y_train,batch_size=128, epochs=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8. Check this model by using two metrics, loss and accuracy.","metadata":{}},{"cell_type":"code","source":"#Check this model by using two metrics, loss and accuracy.\nmetrics = ['loss', 'accuracy']\n#show the evaluation result by using matoplot.\nplt.figure(figsize=(10, 5))\n#Use \"For Loop\".\nfor i in range(len(metrics)):\n    metric = metrics[i]\n    #set subplots to show the result\n    plt.subplot(1, 2, i+1)\n    #Titles of subplots are \"loss\" and \"accuracy\"\n    plt.title(metric) \n    plt_train1 = model1.history[metric] \n\n    #plot them all\n    plt.plot(plt_train1, label='train1') \n    plt.legend() \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=+3> That's enough! :-) </font>","metadata":{}},{"cell_type":"markdown","source":"## 9. Let's make use of the trained model and predict one case :-)","metadata":{}},{"cell_type":"code","source":"i=100\n#Here is the prediction sample.\nplt.imshow(x_test[[i]].reshape(64,64),cmap='Greys')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's predict.\nprediction=model.predict(x_test[[i]]) \nprediction","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 10. Let's check the result.","metadata":{}},{"cell_type":"code","source":"#Let's check the result.\nprint(\"The answer is\",dic[np.argmax(prediction)],\". :-)\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=+2>Correct! :-) </font>","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}