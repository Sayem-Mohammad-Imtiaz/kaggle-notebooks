{"cells":[{"metadata":{},"cell_type":"markdown","source":"## \"Tags\" is an interesting columnm in this review dataset. I wondered if there is any relationship between reiviews and type of travel(triptyle),\n> Lots of tags maybe tagged by reviewer, some interesting informations about the trip in this \"Tags\" column. \nFor example, triptype(Leisure/Business trip), how many people in this trip(solo, family,...)\n\nSo, this is a binary classification task. Each review was labeled into \"Business Trip\" or \"Leisure Trip\". And all the features we need will come from with the pure text."},{"metadata":{"trusted":false},"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('../input/Hotel_Reviews.csv')\ndf['Tags'].head()\ncol = ['Negative_Review', 'Positive_Review', 'Tags'] \ndf = df[col]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"P.S. Every single review have two-sided reivew(two fields), but not all of the reviews have both.\n\n\"No Negative\"/\"No Positive\" means the review without negative/positive "},{"metadata":{"trusted":false},"cell_type":"code","source":"nr = df.Negative_Review\npr = df.Positive_Review\nnr.replace({'No Negative' : ''}, inplace = True)\npr.replace({'No Positive' : ''}, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"lt = df['Tags'].str.contains(' Leisure trip ')\nbt = df['Tags'].str.contains(' Business trip ')\ndenull = (lt != bt)\n#i hav checked that no review tagged 'leisure trip' and 'business trip' at the same time.","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data = pd.DataFrame({'Review': nr + pr, 'TripType': lt})\ndata = data[denull].reset_index(drop = True)\ndata.replace({True: 'Leisure', False: 'Business'}, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data[10:15]\n#500717 reviews","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step1 Preprocessing"},{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np\nimport nltk\ndata.dropna(inplace = True)\ndata.reset_index(drop = True, inplace = True)\n\n#Tokenization: \nfrom nltk.tokenize import word_tokenize\nword_tokenized = data.Review.apply(word_tokenize)\ndata.insert(0, \"WordToken\", word_tokenized)\n\n\n#Anomaly, Weird records: Drop the empty review(or we could remove the review with less than n words?)\nword_count = data.WordToken.apply(lambda x: len(x))  \n#maybe word count could be took as a feature?\n#data.insert(0, 'WordCount', data.WordToken.apply(lambda x: len(x)))\nfilter_count = (word_count >= 1) #if 5, 462350 remained \ndata = data[filter_count]\ndata.reset_index(drop = True, inplace = True)\n#500487 remained.\n\n\n#StopWordRemoval: Remove the NLTK build-in stopwords.\n#from nltk.corpus import stopwords\n#stop_words = set(stopwords.words('english'))\n#wosw = data.WordToken.apply(lambda x:  [item for item in x if item not in stop_words])\n\n#Lemmatization\n#from nltk.stem import WordNetLemmatizer\n\n#wnl = WordNetLemmatizer()\n#def lemmatize_text(text):\n#    return [wnl.lemmatize(w) for w in text]\n#data.insert(0, 'Lemmatized',data.WordToken.apply(lemmatize_text))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step2 Split into training/testing set"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Split randomly with the test size 0.33 (same distribution of classes)\nfrom sklearn.model_selection import train_test_split\ntrain, test = train_test_split(data, stratify = data['TripType'], test_size = 0.33, random_state = 1)\n\ntrain.reset_index(drop = True, inplace = True)\ntest.reset_index(drop = True, inplace = True)\n\n#335K v.s. 165K\n#with 0.83 of Leisure Trip\n\n#Labeling\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nlbl_train, lbl_test = le.fit_transform(train.TripType), le.transform(test.TripType)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step3 Feature engineering\n\nA. Origianl Framework(baseline)\n> 1. TFIDF\n> 2. W2V "},{"metadata":{"trusted":false},"cell_type":"code","source":"# A-1: TFIDF\ndef dum(doc):\n    return doc\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(\n                             ngram_range = (1,1),\n                             tokenizer = dum, \n                             preprocessor = dum,\n                             min_df = 0.01)\n\n#x_train = vectorizer.fit_transform(train.WordToken).toarray()\n#x_test = vectorizer.transform(test.WordToken).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#from sklearn.preprocessing import StandardScaler\n#idf_dict = vectorizer.idf_[:, np.newaxis]\n#ss = StandardScaler()\n#weight = ss.fit_transform(idf_dict)\n#dictionary = dict(zip(vectorizer.get_feature_names(), weight.flatten()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# A-2: W2V, with using the mean of word vectors to represent a review.\n\nfrom gensim.models import Word2Vec, KeyedVectors\nfrom gensim.test.utils import common_texts, get_tmpfile\nmodel = Word2Vec(size = 300, window = 2, min_count = 1)\n\n#Building the dictionary.\nmodel.build_vocab(train.WordToken)\n\n#Train the w2v model with WordTokens\nmodel.train(train.WordToken, total_examples = len(train.WordToken), epochs = 3)\nmodel.init_sims(replace = True)\n\n#To lower the memory usage, save/load in KV model is necessary...\nfpath = get_tmpfile(\"w2v.kv\")\nmodel.wv.save(fpath)\ndel model\nwv = KeyedVectors.load(fpath, mmap='r')\n\n#Document vector: By simply get the average of the wordvec in a review.\ndef doc_vec(doc, mean = np.zeros(wv.vector_size)):  \n    doc = [word for word in doc] # target input list of words\n    try:\n        return np.mean(wv[doc], axis = 0)\n       # return np.mean(np.multiply(wv[doc] , np.array(idfweight(doc))[:, np.newaxis]), axis = 0)\n    except:\n        return mean  #13168/165161 is zero vector\n\n#x_train = np.vstack(train.WordToken.apply(doc_vec))  #not sure abou whether a more efficient way\n#x_test = np.vstack(test.WordToken.apply(lambda x : doc_vec(x, np.mean(x_train, axis = 0)) ))  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"B. Experiment\n> 1. One-class fitting: minority class, Business. (1st, now)\n>2. Add a feature: WordCount included(bad)\n>3. POS filtering(nothing helpful)\n>4. Combination: TFIDF + W2V (better than fittin on all the instances)\n>5. more: https://docs.google.com/spreadsheets/d/1sYVVqwnPrhTWFtEl3k4xBjvTRO3fccQcOHQkYHB-_Vs/edit#gid=936173861"},{"metadata":{},"cell_type":"markdown","source":"## B-1: Fit on Business-Labeled data only with TFIDF(1st now)"},{"metadata":{"trusted":false},"cell_type":"code","source":"def dum(doc):\n    return doc\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(\n                             ngram_range = (1,2),  # uni-to-bigram perform better, but out of memory on kaggle's notebook.\n                             tokenizer = dum, \n                             preprocessor = dum,\n                             min_df = 0.001,\n                             max_df = 0.4)  \n\nbt = train[train.TripType == 'Business']  \nvectorizer.fit(bt.WordToken)\n#fit only the instances with Business only.\nx_train = vectorizer.transform(train.WordToken).toarray()\nx_test = vectorizer.transform(test.WordToken).toarray()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## B-2: Add the WordCount feature on each representation method.(x)"},{"metadata":{"trusted":false},"cell_type":"code","source":"'''\nfrom sklearn.preprocessing import StandardScaler, KBinsDiscretizer\n\n#a = scaler.fit_transform(train.WordCount[:, np.newaxis])  # fit does nothing.\n#b = scaler.transform(test.WordCount[:, np.newaxis])\na = np.array(train.WordCount).reshape(len(train.WordCount), 1) # fit does nothing.\nb = np.array(test.WordCount).reshape(len(test.WordCount), 1)\n\nkbin = KBinsDiscretizer(n_bins = 17, encode = 'onehot-dense', strategy = 'quantile')\na = kbin.fit_transform(a) \nb = kbin.transform(b)\n\n#Cobiine the wordcount in to be treated as the feature.\nx_train = np.append(x_train, a, axis = 1)\nx_test = np.append(x_test, b , axis = 1)\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## B-3: Part of speech filtering.(x)"},{"metadata":{"trusted":false},"cell_type":"code","source":"#POS filter: filtered the other word without given POS tag.\n#Warning: It takes time.\n\ndef pos_tagging(sent):\n    tagfilter = {'NOUN'}  # e.g. 'NOUN', 'VERB'...\n    target = [item for (item, tag) in nltk.pos_tag(sent, tagset = 'universal') if tag in tagfilter]\n    return target\n\ndata.insert(0, 'POS', data.Lemmatized.apply(pos_tagging))\n\n#then apply on the TFIDF or W2V later.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## B-4: Combination of two frameworks.(2nd)"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Execute each of the vectorization approach to two variables\n#Assign the new feature vectors(combination) to x_train, x_test\n\n#x_train_t, x_test_t = ...(A-1)\n#x_train_w, x_test_t = ...(A-2)\nx_train = np.append(x_train_t, x_train_w, axis = 1)\nx_test = np.append(x_test_t, x_test_w, axis = 1)\n\n#Dimentionality is 300+434 = 734","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## C. Feature Selection\n>1. Supervised Approach:f_classif"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Feature Selection: Fitting the ANOVA stats on training data.\nfrom sklearn.feature_selection import f_classif, SelectKBest, VarianceThreshold, chi2\n\ndef FeatureSelect_f(feature, target, d):\n    fs = SelectKBest(f_classif, k = d).fit(feature, target)\n    return fs\n\n#Supervised approach\nfs = FeatureSelect_f(x_train, lbl_train, 500)\n#Transform both training/testing set into new dimensionality.\nx_train = fs.transform(x_train)\nx_test = fs.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step4: Build NB classifier\nIn order to compare each feature frameworks, chose the simplest classifier, NB."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nclf_gnb = GaussianNB()\nclf_gnb.fit(x_train, lbl_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 4: Evaluation"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Evaluation function:\nfrom sklearn.metrics import precision_recall_curve, auc, confusion_matrix, accuracy_score, classification_report\n#from imblearn.metrics import classification_report_imbalanced\nimport matplotlib.pyplot as plt\ndef evaluating(truth, pred, ax=object):\n  \n    print(accuracy_score(truth, pred))\n    print(classification_report(truth, pred))    \n    print(confusion_matrix(truth, pred))\n    precision, recall, threshold = precision_recall_curve(truth, pred)\n\n    ax.step(recall, precision, color='b', alpha=1, where='post')\n    ax.fill_between(recall, precision, step='post', alpha=0.5, color='b')\n    ax.set_xlabel('Recall')\n    ax.set_ylabel('Precision')\n    ax.set_ylim([0.0, 1.05])\n    ax.set_xlim([0.0, 1.0])\n    ax.set_title('Precision-Recall curve')\n    return ax\n#ref: https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"f, (ax1) = plt.subplots(1, 1, figsize=(10,10))\n\npred_gnb = clf_gnb.predict(x_test)\nevaluating(lbl_test, pred_gnb, ax1)\n\n#Take as an imbalanced Problem??","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":1}