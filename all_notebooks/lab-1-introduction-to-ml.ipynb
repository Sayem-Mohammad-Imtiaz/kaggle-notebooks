{"cells":[{"metadata":{"_uuid":"dfff8d55bdca8f061ca99e04653bfa73c7600f6a"},"cell_type":"markdown","source":"# Creators\n* Morgan Dally - 1313361\n* Reece Breebaart - 1314828"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0059bbcc527affa1904301ea9e1d306fdcb87c22"},"cell_type":"code","source":"def print_predicted_stats(predicted, y_test):\n    '''\n    This is just a dumb version of classification report.\n    '''\n    hit = 0\n    miss = 0\n    for actual, prediction in zip(y_test, predicted):\n        if actual == prediction:\n            hit += 1\n            continue\n        miss += 1\n    percent = 100 - ((miss/hit) * 100)\n    print('missed: %d\\nhit: %d\\n%d%s' % (miss, hit, percent, '% accuracy'))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":false},"cell_type":"code","source":"input_csv_loc = '../input/wisconsin_breast_cancer.csv'\nbccf = pd.read_csv(input_csv_loc)\n\n# get rid of NaN entries\nbccf_clean = bccf.dropna()\nbccf_clean.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"02dab7925c5c7e8e930636c749aa4425f969a527"},"cell_type":"code","source":"import seaborn as sns\nsns.pairplot(data=bccf_clean, hue='thickness', palette='Set2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0b00791ec8c59cbc421338acdba57385e5382aa"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# select all entries, then get from col1 and select up until the end, exlcuding it\nx_dat = bccf_clean.iloc[:, 1:-1]\n# select all entries, then get col 10\ny_dat = bccf_clean.iloc[:, 10]\n\nx_train, x_test, y_train, y_test = train_test_split(x_dat, y_dat, test_size=0.2, random_state=342)\n\nprint('x_train: %s\\ny_train: %s\\nx_test: %s\\ny_test: %s' % (\n    x_train.shape, y_train.shape,\n    x_test.shape, y_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6231945f2cffa661d92664e27445efa6d06b8d77"},"cell_type":"code","source":"from sklearn.svm import SVC\nmodel = SVC()\nmodel.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87e348c99d4eebda217e7655764fadcc93d183be"},"cell_type":"code","source":"predicted = model.predict(x_test)\nprint_predicted_stats(predicted, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90a7703de95cae6f73dd2b8f2f17fda9811c6e28"},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(y_test, predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"977ee51ee2f26311794b7aeaa73d5cb2219ed8ae"},"cell_type":"code","source":"print(classification_report(y_test, predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"7c0bf0f05240d6dbe53ea143104e0609a3740db5"},"cell_type":"code","source":"x_test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e7d7b163f9275b3a8b5eec667ca4225c04765ae"},"cell_type":"markdown","source":"# xsub train\n## Our xsub col choices:\n* nuclei\n* size or shape (these two will be related features so try either or)\n* adhesion\n* single\n\n## Notes / Discussion:\n* Class 0 looks to be easier to distinguish from class 1\n* We therefore want to select in a way that makes class 1 features more prominent\n* We experimented and eyeballed the accuaracy\n* tried to determine which classes were detremental to perfomance\n* it seemed that selecting columns with a dominant class provided the largest accuracy\n* in the end we were able to make a subset that contradicted any results we found trying to find any patterns\n* we were not able to determine good or bad individual columns\n* we ended up with [5, 2, 3, 4] as they improved accuracy\n\n#### max vals\n*  1: 139\n*  2: 373 <- 5\n*  3: 346\n*  4: 393 <- 3\n*  5: 376 <- 4 (seems to influence results badly) // contradicted as we made a good subset with it improving performance\n*  6: 402 <- 2\n*  7: 161\n*  8: 432 <- 1"},{"metadata":{"trusted":true,"_uuid":"d8ee988767c70b8f7a5cfbe17ae8a8eb1a28d66a"},"cell_type":"code","source":"bccf_clean.iloc[:, 8].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ffa74b0bb9d757097e7478da1d534cd36b6d5f9","scrolled":true},"cell_type":"code","source":"\"\"\"\ncols:\n    thickness: 0,\n    size: 1,\n    shape: 2,\n    adhesion: 3,\n    single: 4,\n    nuclei: 5,\n    chromatin: 6,\n    nucleoli: 7,\n    mitosis: 8\nmy choice: [nuclei, shape, adhesion, single]\n\"\"\"\n# selecting rows to increase accuracy - eyeballed 96% accuracy\ncols = [5, 2, 3, 4]\n# experimenting getting bad accuracy, came out with 92%\n# cols = [5, 4, 8, 1]\nxsub_train = x_train.iloc[:, cols]\nxsub_test = x_test.iloc[:, cols]\nxsub_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43799c7321598aec1e662779b65babe718222e86"},"cell_type":"code","source":"second_model = SVC()\nsecond_model.fit(xsub_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"2eec02a5dc3a1f11db38ffd7446728d1890c8f31"},"cell_type":"code","source":"sub_predicted = second_model.predict(xsub_test)\nprint_predicted_stats(sub_predicted, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"986d5530b84b0ccf0e5c3238c644d62fdfeb27ea"},"cell_type":"code","source":"print(confusion_matrix(y_test, sub_predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"3dcad4ff91fea514e83704444d117236a05305db"},"cell_type":"code","source":"print(classification_report(y_test, sub_predicted))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ac527d775f2665f2616e8ab4dd3ebb76a5d5b37"},"cell_type":"markdown","source":"# kNN Tests"},{"metadata":{"trusted":true,"_uuid":"fac715c8ea7e70606dc9d5fcd693cc9605428b5a"},"cell_type":"code","source":"import sklearn.neighbors\n#when k value is 1\nknn_1_model = sklearn.neighbors.KNeighborsRegressor(n_neighbors=1)\nknn_1_model.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb8a19ef92b61356b9a9a3141d71355219cfdc53"},"cell_type":"code","source":"knn_1_predicted = knn_1_model.predict(x_test)\nprint_predicted_stats(knn_1_predicted, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab1a7bda8ca801cde92ff057b63510c53659b3ee"},"cell_type":"code","source":"#when k value is 1\nknn_5_model = sklearn.neighbors.KNeighborsClassifier(n_neighbors=5)\nknn_5_model.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"8868ad9b45947dab8352a00c69d546b56fc0e95e"},"cell_type":"code","source":"knn_5_predicted = knn_5_model.predict(x_test)\nprint_predicted_stats(knn_5_predicted, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5472ef7348d59faf6793e83c4de9e09473b4c42d"},"cell_type":"code","source":"y_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ccc29ed663171784859e808d1568a59bf7a52af7"},"cell_type":"code","source":"print(classification_report(y_test, knn_1_predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4bed218ef942365e9bf74923c11b2df875c2358","scrolled":false},"cell_type":"code","source":"print(classification_report(y_test, knn_5_predicted))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9daca75093114c06183eaa66759e4e19291013b"},"cell_type":"markdown","source":"# Discussion\nFrom the above tests, our results were:\n* x_train:\n  * 100% accuracy classifying class 0\n  * 85% accuracy classifying class 1\n* xsub_train:\n  * 100% accuracy classifying class 0\n  * 92% accuracy classifying class 1\n* Nearest neighbor model, k = 1:\n  * 97% accuracy classifying class 0\n  * 95% accuracy classifying class 1\n* Nearest neighbor model, k = 5:\n  * 99% accuracy classifying class 0\n  * 98% accuracy classifying class 1\n  \n## All cols and subsetting\n\nModifying the test and train set to only use a subset of columns saw varying results. In general it seems that with this dataset, subsetting the data can improve accuracy. However it was possible to also degrade the performance of our model. This can be seen with the two subsets 5 code blocks up (one is commented out). In this case subsetting was able to improve the accuracy of classifying class 1 from 85% to 92%, a significant difference.\n\nThe largest accuracy difference was actually found by modifying the random_state value when initially splitting into the train and test sets. For example at one point the initial split saw 98% accuracy. After reloading the page the accuracy then dropped to around 95%. To compensate for this the random state used for splitting data has been pinned. In both cases subsetting the data could improve the accuracy of classifying the classes, moreso classifying class 1.\n\nFor more conclusive results we would need to test with a larger dataset. However it can be said that subsetting improved our performance with a 'good' subset.\n\n## kNN Results\n* k = 1:\n  * 97% accuracy classifying class 0\n  * 95% accuracy classifying class 1\n* k = 5:\n  * 99% accuracy classifying class 0\n  * 98% accuracy classifying class 1\n\nUsing the neighbor classifier model improved the performance over the original SVC model used. When k = 1, it could be seen that the accuracy improve for classifying class 1 (92% -> 95%), however the accuracy for finding class 0 dropped (100% -> 97%). When setting k = 5 however or accuracy improved greatly. Class 0 was now classified 99% of the time and class 1 was classified 98% of the time.\n\nIn conclusion it looks like a nearest neighbor model with k = 5 is the best approach for the provided dataset.\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}