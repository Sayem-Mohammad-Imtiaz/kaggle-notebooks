{"cells":[{"metadata":{"_uuid":"70f7b4814065164f591096d927fbcc0ba64ffdea"},"cell_type":"markdown","source":"**Introduction**\n\nAs a beginner, I purpose to analyse orthopedic patients bones with using some regression algorithms for this Kernel. \nFirstly, I will analyze my data; according to results, try to apply some regression models, and  lastly I will conclude my report.\n1. Linear regression\n1. Logistic Regression\n1. KNN Algorithm"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns  # visualization tool\nfrom sklearn.linear_model import LinearRegression\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":true},"cell_type":"code","source":"#read data\ndata = pd.read_csv(\"../input/column_2C_weka.csv\")\ndata.info()\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc36713bcebf4d1924fdae39cb34ab6b947f4b0c"},"cell_type":"markdown","source":"After info() funciton, there are no null value therefore I dont need to apply any filling up function, I can use direclt my data.\n\n*First 6 rows* ==> float values(my features for analyzing my data)\n\n*Last row'class' * ==> string value and has **\"Normal\" and \"Abnormal\"**\n\nTherefore I can use my \"Class\" feature as a binary classification, and try to understand my features characteristics and effects on that feature."},{"metadata":{"trusted":true,"_uuid":"aa2c42a50dd207c0b0efacc28cc9bebfc7b877b0","scrolled":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aeaec58922b4ae348415961868f5067a29db2adf"},"cell_type":"markdown","source":"**Analyze data**\n\nTo understand relaitonhip between features, \n\n*Firstly,* I want to divide my data on binary classificaiton point; means Noemal and Abnormal.In order to analyze those classes I want to plot count of those class values.\n    \n*Secondly, *I want to learn correlation between my features.\nNote: Correlation matrix is a very useful plot for the first look up;  intersection of features seems approximately 1 it means those 2 features are very related with each other and they have big correlation."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"40a8df25f783acda4b6526a2f56950bc6a2dd755"},"cell_type":"code","source":"sns.countplot(x=\"class\", data=data)\ndata.loc[:,'class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b2e9bd9e6ea53788e036191ebc2884b08e89c5f2"},"cell_type":"markdown","source":"From the figure, as it seems Normal class values are smaller than Abnormal values; therefore i need to narrow my perspective with selecting some correlated features. Lets look at correlation matrix of all features:"},{"metadata":{"trusted":true,"_uuid":"fbc54b67ad6e0304ba7235b7f0309bada898bebb","scrolled":true},"cell_type":"code","source":"#Correlation map\ndata.corr() \nf, ax = plt.subplots(figsize = (8,8))\nsns.heatmap(data.corr(), annot = True, linewidths=.5, fmt = \".2f\", ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"16e29bc1b1a878ae9ceb13711d98869d405a3e58"},"cell_type":"markdown","source":"As it seems from the figure,  **pelvic_incidence** and **sacral_slope** has the big correlation among the other features.\nHence, I want to focus on those features while analyzing my data with respect to \"Normal\" and \"Abnormal\" class feature."},{"metadata":{"_uuid":"e3b112c825786652f09b3a9eeff0c4657346f157"},"cell_type":"markdown","source":" ** 1. \n LINEAR REGRESSION ANALYSIS**\n\nIn that part of the analysis, I want to give some brief information about Linear Regression;\n\nIt is a mathematical representation on data. \n> Y = B0 + B1.X + ... BN.X\n\n**B0:** it is a bias for our data\n**B1:** coefficient for input data\n**Y: **result\n"},{"metadata":{"trusted":true,"_uuid":"f250d56054caf8825be56593c9447653ee6d928a"},"cell_type":"code","source":"#Dividing my data as Normal and Abnormal class\ndata_a = data[data[\"class\"] == \"Abnormal\"]\ndata_n = data[data[\"class\"] == \"Normal\"]\n\n#For Abnormal data\nx_a = np.array(data_a.loc[:,'pelvic_incidence']).reshape(-1,1)\ny_a = np.array(data_a.loc[:,'sacral_slope']).reshape(-1,1)\n#For Normal data\nx_n = np.array(data_n.loc[:,'pelvic_incidence']).reshape(-1,1)\ny_n = np.array(data_n.loc[:,'sacral_slope']).reshape(-1,1)\n\n# Scatter plot\nplt.scatter(x = x_a, y = y_a, color = \"red\", label = \"Abnormal\", alpha = 0.5)\nplt.scatter(x = x_n, y = y_n, color = \"green\", label = \"Normal\", alpha = 0.5)\nplt.xlabel('pelvic_incidence')\nplt.ylabel('sacral_slope')\nplt.legend()\nplt.title(\"pelvic_incidence and sacral_slope for Normal and Abnormal class\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"0e908fe0c5259db803f56c6c91a2571db778a30c"},"cell_type":"code","source":"#Linear Regression and drawing line and calculate accuracy\nlr = LinearRegression()\nlr.fit(x_a, y_a)\npredicted_a = np.linspace(min(x_a), max(x_a)).reshape(-1,1)\ny_a_head = lr.predict(predicted_a)\nprint('Accuracy for Abnormal: ',lr.score(x_a, y_a))\n\nlr2 = LinearRegression()\nlr2.fit(x_n, y_n)\npredicted_n = np.linspace(min(x_n), max(x_n)).reshape(-1,1)\ny_n_head = lr2.predict(predicted_n)\nprint('Accuracy for Normal: ',lr2.score(x_n, y_n))\n\n# Plot regression line and scatter\nplt.figure(figsize=[10,8])\nplt.plot(predicted_a, y_a_head, color='black', linewidth=3, label = \"Abnormal\", linestyle='dashed')\nplt.plot(predicted_n, y_n_head, color='black', linewidth=3, label = \"Normal\")\nplt.scatter(x = x_a, y = y_a, color = \"red\", label = \"Abnormal\", alpha = 0.5)\nplt.scatter(x = x_n, y = y_n, color = \"green\", label = \"Normal\", alpha = 0.5)\nplt.xlabel('pelvic_incidence')\nplt.ylabel('sacral_slope')\nplt.legend()\nplt.title(\"pelvic_incidence and sacral_slope for Normal and Abnormal class\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ed26076f91d492ffc4545597164b546071d56b8"},"cell_type":"markdown","source":" **2. LOGISTIC REGRESSION**\n\n"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"c9bf8edcf66956d5d9e427ac882a0e18522185ee"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b7a6ab91cd00a56b310cbd4776cb0a0499807f4"},"cell_type":"code","source":"data.columns = data.columns.str.replace('class','bone_type')#I chance class because it has some refer problem\n\n#Convert Abnormal to 1 Normal 0 for binary representation\ndata.bone_type = [1 if each == \"Abnormal\" else 0 for each in data.bone_type]\n\ny = data.bone_type.values #take all values into y \nx_data = data.drop(['bone_type'], axis = 1)\n\n#Normalization for all values into x between 0 and 1 for calculation\n#  (x- min(x)) / (max(x) - min(x))\nx = (x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data)).values\n\n#Split data for train and test and test size is assumed as 20%\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n\nprint(\"x_train: \", x_train.shape,\"y_train: \", y_train.shape,\" \\nx_test: \", x_test.shape, \" y_test: \", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62481d7f0951b7e62c8f1b009828e5a5a7850187","scrolled":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlor = LogisticRegression()\nlor.fit(x_train,y_train)\n\ny_predict = lor.predict(x_test)\n\ndef find_accuracy(y_test, y_predict):\n    count = 0\n    if(len(y_test) == len(y_predict)):\n        for i in range(len(y_test)):\n            if(y_test[i] == y_predict[i]): \n                count += 1\n        acc = count / len(y_test) * 100\n        print(\"Accuracy -->\", acc, \"%\")\n    else:\n        print(\"Your test and predicted data set are not equal accuracy will not be calculated\")\n\nfind_accuracy(y_test, y_predict)\n#lor.score(x_test, y_test)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4209e5007c4d23c534f83c525f215ad6754389ce"},"cell_type":"markdown","source":"**COMPARISION**\n\nIt is a comparision between Linear and Logistic Regression with plot"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"6f7c4b16fab01dc2a2e5f530aea9842a62f26da1"},"cell_type":"code","source":"x_1 = np.array(x_data.pelvic_incidence).reshape(-1,1)\ny_1 = np.array(data.bone_type.values).reshape(-1,1)\n\n#Linear Regression\nx_n = (x_1 - min(x_1)) / (max(x_1) - min(x_1))\nlr = LinearRegression()\nlr.fit(x_n,y_1)\nlp = np.linspace(min(x_n), max(x_n)).reshape(-1,1)\ny_head = lr.predict(lp)\nprint('Accuracy for Linear Regression: ',lr.score(x_n, y_1))\n\n#Logistic Regression\nlgr = LogisticRegression()\nlgr.fit(x_n, y_1)\nlp = np.linspace(min(x_n), max(x_n)).reshape(-1,1)\ny_head2 = lgr.predict(lp)\nprint('Accuracy Logistic Regression: ',lgr.score(x_n, y_1))\n\n# Plot regression line and scatter\nplt.plot(lp, y_head, color='black', linewidth=3, label = \"Linear\")\nplt.plot(lp, y_head2, color='red', linewidth=3, label = \"Logistic\")\nplt.scatter(x = x_n, y = y_1)\nplt.xlabel(\"Pelvic Incidence\")\nplt.ylabel(\"Type(Normal || Abnormal)\")\nplt.legend()\nplt.title(\"Linear and Logistic Regression\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8dd9b2f79419dde27f07d38e56630588d83ead02"},"cell_type":"markdown","source":"**3. KNN ALGORITHM**\n\nK-Neighbor algorithm is classification algorithm to find k neighbors and then according to neighbors classificaiton find test point class.\n\nI will use divided classes as abnormal and normal in Linear Regression secion for this algorithm:\n\n+Abnormal class --> a_data (value 1)\n\n+Normal class --> n_data (value 0)\n\n"},{"metadata":{"trusted":true,"_uuid":"b740ed553ef216caf14258534ba285b9fe132ae8"},"cell_type":"code","source":"#For plotting I need to change column[class] name because class is defined word.\n#Then I will represent my class attribitues as binary representation \na_data = data[data[\"bone_type\"] == 1]\nn_data = data[data[\"bone_type\"] == 0]\n\nplt.scatter(a_data.degree_spondylolisthesis, a_data.lumbar_lordosis_angle, color = \"red\", label = \"Abnormal\", alpha=0.5)\nplt.scatter(n_data.degree_spondylolisthesis, n_data.lumbar_lordosis_angle, color = \"green\", label = \"Normal\", alpha = 0.5)\nplt.xlabel(\"Pelvic Incidence\")\nplt.ylabel(\"Pelvic Radius\")\nplt.legend()\nplt.title(\"Pelvic Incidence and Radius for Normal and Abnormal Bone Type\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d27b53cbd04163a4b1933fba5917b00d084ba9d"},"cell_type":"code","source":"x_all = data.drop([\"bone_type\"], axis = 1) # all values except bone type\ny = data.bone_type.values #label 0 or 1\n\n#Normalization\nx_norm = (x_all - np.min(x_all))/(np.max(x_all)-np.min(x_all))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f3874a71d556c1c9198ea8d85640316b5a6de97"},"cell_type":"code","source":"#Train-Test Splitting\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_norm, y, test_size = 0.3, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb92646f7807875f929fb501dca3092c432f52ef"},"cell_type":"code","source":"#KNN Algorithm application\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 5) # give number of neighbor as 5\nknn.fit(x_train, y_train)\nprediction = knn.predict(x_test)\nprint(\"KNN Accuracy: \", knn.score(x_test, y_test))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27288fc27e6f2e2e2f597de9a33d42a7170f9e67"},"cell_type":"code","source":"#Find best neighbor number for this situation and then plot it\naccuracy_list = []\nfor each in range (1,30):\n    knn2 = KNeighborsClassifier(n_neighbors = each)\n    knn2.fit(x_train, y_train)\n    accuracy_list.append(knn2.score(x_test, y_test))\n\n#Plot with line diagram\nplt.plot(range(1,30),accuracy_list, label = \"Accuracy\", linewidth = 3)\nplt.grid()\nplt.xlabel(\"K Neighbors numbers\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy graph according to Neighbors numbers\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"67106ae719fe26e37b4405cb8007d9b330189c7c"},"cell_type":"markdown","source":"As you can see from the above figure K = 15 has the best accuracy.\nAccording to that good neighbor number, I would like to plot decision boundaries because my accuracy is very accurate."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}