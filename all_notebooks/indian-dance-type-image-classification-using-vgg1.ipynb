{"cells":[{"metadata":{"_uuid":"859b7833-2585-45c2-ae11-83978d9c17a3","_cell_guid":"8db1e9b5-f55f-43b0-981c-7e2e1824cd94","trusted":true},"cell_type":"markdown","source":"# HackerEarth Deep Learning challenge: Identify the dance form\nThis International Dance Day, an event management company organized an evening of Indian classical dance performances to celebrate the rich, eloquent, and elegant art of dance. Post the event, the company planned to create a microsite to promote and raise awareness among the public about these dance forms. However, identifying them from images is a tough nut to crack.\nYou have been appointed as a Machine Learning Engineer for this project. Build an image tagging Deep Learning model that can help the company classify these images into eight categories of Indian classical dance.\n\n### Dataset\nThe dataset consists of 364 images belonging to 8 categories, namely manipuri, bharatanatyam, odissi, kathakali, kathak, sattriya, kuchipudi, and mohiniyattam.\nThe benefits of practicing this problem by using Machine Learning/Deep Learning techniques are as follows:\nThis challenge will encourage you to apply your Machine Learning skills to build models that classify images into multiple categories\nThis challenge will help you enhance your knowledge of classification actively. It is one of the basic building blocks of Machine Learning and Deep Learning\nWe challenge you to build a model that auto-tags images and classifies them into various categories of Indian classical dance forms.\n\nThe data folder consists of two folders and two .csv files. The details are as follows:\ntrain: Contains 364 images for 8 classes \n* manipuri,\n* bharatanatyam\n* odissi\n* kathakali\n* kathak\n* sattriya\n* kuchipudi\n* mohiniyattam\n\ntest: Contains 156 images\ntrain.csv: 364 x 2\ntest.csv: 156 x 1\n\nData description\nThis data set consists of the following two columns:\n\n| Column Name | Description |\n|-------------|-------------|\n| Image       | Name of Image| \n|target       |Category of Image  ['manipuri','bharatanatyam','odissi','kathakali','kathak','sattriya','kuchipudi','mohiniyattam'] |","execution_count":null},{"metadata":{"_uuid":"f7a50e5e-d96e-448e-a6fa-1a6995100936","_cell_guid":"1e5c1e4c-78ce-436e-953d-cc6f035a9a62","trusted":true},"cell_type":"markdown","source":"In this file we are using Transfer Learning concept to classify Indian dance form. Transfer Learning used when we have very less training data. In image processing, training with less data does not give good results. So we are using Transfer Learning to get weights. This notebook use *tensorflow* *VGG16* and the purpose of this notbook to show how to use Transfer Learning for image classification","execution_count":null},{"metadata":{"_uuid":"df237a26-f0bb-4a9d-be38-0d2e1a95ca2d","_cell_guid":"36e8a212-f1c5-46c9-b520-d853776914d4","trusted":true},"cell_type":"code","source":"# import required libraries \nimport os\nimport pandas as pd\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.layers import Flatten,Dense,Dropout, Conv2D, MaxPooling2D\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.utils import to_categorical\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.optimizers import Adam\n\nfrom sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0baf49a1-9425-4821-84dd-70bdcbf7896d","_cell_guid":"cb5a14ea-c419-4171-9272-efd5f2e13f41","trusted":true},"cell_type":"code","source":"# Load train and test csv file for image class\ntrain = pd.read_csv('/kaggle/input/identifythedanceform/train.csv')\ntest = pd.read_csv('/kaggle/input/identifythedanceform/test.csv')\n\nprint(train.head())\nprint(test.head())\nprint(train['target'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6dd3c466-462d-4ce0-9b3d-15199ee88aa4","_cell_guid":"40fdd209-c01a-4f17-94fa-45d5284edbe7","trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5650e417-aa0a-4fc9-ac71-5d09f53d1bdc","_cell_guid":"17007148-d625-498f-995d-a7b093fc4d69","trusted":true},"cell_type":"markdown","source":"Basic Histrogram plot to check number of training data for each dance form.","execution_count":null},{"metadata":{"_uuid":"d41ed2cc-8951-4153-b7a1-47dc38cc2fce","_cell_guid":"e99ab92d-88bd-4df2-9b99-4b1b143ec877","trusted":true},"cell_type":"code","source":"#Histogram chart for target\ntrain['target'].value_counts().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ebd87e34-6c92-4831-9056-5cd90f7f9a4b","_cell_guid":"77271e90-0944-4bcf-b7e7-20b18f9517d7","trusted":true},"cell_type":"code","source":"base='/kaggle/input/identifythedanceform'\ntrain_dir = os.path.join(str(base)+ '/train/')\ntest_dir = os.path.join(str(base)+'/test/')\n\ntrain_fnames = os.listdir(train_dir)\ntest_fnames = os.listdir(test_dir)\n\nprint(train_fnames[:9])\nprint(test_fnames[:9])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e031bfc-9264-408f-8680-b77a058abc34","_cell_guid":"bc55d935-0f93-4cd6-a32a-2fcd53b084b8","trusted":true},"cell_type":"code","source":"# Images might be in different size. In this section I assigning all image at same size of 224*224\nimg_width = 224\nimg_height = 224","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00e131ff-ee43-4b0a-8f94-0ed059332c37","_cell_guid":"f588f28e-1fb9-4e79-9296-cbed32a4402d","trusted":true},"cell_type":"markdown","source":"Below two section used for data preprocessing. We are reading image data using OpenCV and converting into numeric formate.","execution_count":null},{"metadata":{"_uuid":"47fc34cc-4920-4e3b-95fc-8d73a53a0a7a","_cell_guid":"6a602528-9768-4315-bea1-690bab0e3e12","trusted":true},"cell_type":"code","source":"# this function reads image from the disk,train file for image and class maping and returning output in numpy array formate\n# for input and target data\ndef train_data_preparation(list_of_images, train, train_dir):\n    \"\"\"\n    Returns two arrays: \n        train_data is an array of resized images\n        train_label is an array of labels\n    \"\"\"\n    train_data = [] \n    train_label = [] \n    for image in list_of_images:\n        train_data.append(cv2.resize(cv2.imread(train_dir+image), (img_width,img_height), interpolation=cv2.INTER_CUBIC))\n        if image in list(train['Image']):\n            train_label.append(train.loc[train['Image'] == image, 'target'].values[0])\n    \n            \n    return train_data, train_label","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e59c48eb-e710-4b16-8266-b80a6b030f91","_cell_guid":"c818dbd1-8256-476e-b307-78d3bb24fa4c","trusted":true},"cell_type":"code","source":"def test_data_prepare(list_of_images, test_dir):\n    \"\"\"\n    Returns: \n        x is an array of resized images\n    \"\"\"\n    test_data = [] \n    \n    for image in list_of_images:\n        test_data.append(cv2.resize(cv2.imread(test_dir+image), (img_width,img_height), interpolation=cv2.INTER_CUBIC)) \n            \n    return test_data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86c57720-24a9-42b3-bc05-ce65ca7bd9ab","_cell_guid":"73a8efe1-2312-4839-bbed-bdda8c525781","trusted":true},"cell_type":"code","source":"training_data, training_labels = train_data_preparation(train_fnames, train, train_dir)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c3b452c-15b7-4b48-9af2-2316a14f03ca","_cell_guid":"572ada12-6aa7-4017-9a2f-1b5661756e1f","trusted":true},"cell_type":"code","source":"training_labels[:10]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1cead0f3-eb35-4893-bcf5-487c389459e9","_cell_guid":"c7d5b26b-dc66-4a2c-be25-d0cb44f94142","trusted":true},"cell_type":"code","source":"training_data[1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"beb69628-a16d-4e4c-8532-deba79d21ae0","_cell_guid":"06be15a6-07f6-426c-92c4-2573bf8e3bc6","trusted":true},"cell_type":"code","source":"\ndef show_batch(image_batch, label_batch):\n    plt.figure(figsize=(12,12))\n    for n in range(25):\n        ax = plt.subplot(5,5,n+1)\n        plt.imshow(image_batch[n])\n        plt.title(label_batch[n].title())\n        plt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41c84d76-d96e-4120-8bed-7f8ca841d682","_cell_guid":"b16f98bf-2d4f-4687-9dca-4b9525121d96","trusted":true},"cell_type":"markdown","source":"Just showing loaded data for first 25 image","execution_count":null},{"metadata":{"_uuid":"27f4994f-c20b-43e9-a477-166de8450708","_cell_guid":"fc00c4e4-b35a-4e7e-9c39-955ee4f44667","trusted":true},"cell_type":"code","source":"show_batch(training_data, training_labels)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea78d05f-175e-4288-af1d-263017566f48","_cell_guid":"180186d5-b73f-480e-92f4-931681c68f7c","trusted":true},"cell_type":"code","source":"testing_data = test_data_prepare(test_fnames, test_dir)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d5a0752-5e97-4d41-89ae-0a23de06c165","_cell_guid":"d647c23a-b386-4d9d-be52-d2f46c765c75","trusted":true},"cell_type":"markdown","source":"Using label incoder converting target class to numeric format","execution_count":null},{"metadata":{"_uuid":"77bd9273-0015-4fec-a655-1b42789483ce","_cell_guid":"2d83c8cd-90c7-4cd9-9088-eec2676452d6","trusted":true},"cell_type":"code","source":"le =LabelEncoder()\ntraining_labels=le.fit_transform(training_labels)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5653e43-1a32-4c3e-bfe9-d81aba323467","_cell_guid":"b710a55d-b3ad-4617-8d60-973219053e0c","trusted":true},"cell_type":"markdown","source":"In this section I am using ougumentation techniques to generate more data for given input","execution_count":null},{"metadata":{"_uuid":"ed450aec-3d15-4733-9ad7-988873729c14","_cell_guid":"21133d4f-ffb1-474e-95e3-349b2df69cd4","trusted":true},"cell_type":"code","source":"training_labels[:10]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15271c96-1ab0-4f1a-b6a0-85e6c5ed71f2","_cell_guid":"48f9d778-c26b-4dd5-ab1a-b68ad67ffae4","trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(training_data, training_labels, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81ee480c-7c3b-485e-b863-a2628877d605","_cell_guid":"d6de8fbe-c013-48c5-ba78-7dc0241dd07e","trusted":true},"cell_type":"code","source":"train_datagenerator = ImageDataGenerator(\n        rescale=1. / 255,\n        featurewise_center=False,  \n        samplewise_center=False,  \n        featurewise_std_normalization=False,  \n        samplewise_std_normalization=False,  \n        rotation_range=40,  \n        zoom_range = 0.20,  \n        width_shift_range=0.10,  \n        height_shift_range=0.10,  \n        horizontal_flip=True,  \n        vertical_flip=False) \n\n\nval_datagenerator=ImageDataGenerator(\n        rescale=1. / 255\n)\n\ntrain_datagenerator.fit(X_train)\nval_datagenerator.fit(X_val)\nX_train=np.array(X_train)\nX_val=np.array(X_val)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a95d6a20-fcc1-46b7-9ba3-ea0c23e1ab2f","_cell_guid":"2a3aff97-312f-425d-93a2-c90ef51d9f4d","trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(y_train.shape)\nprint(X_val.shape)\nprint(y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ac77a4c-52fa-400c-974e-9e8ed23363a8","_cell_guid":"ca54899d-c4ed-4465-ad2f-1e64f806627b","trusted":true},"cell_type":"markdown","source":"In below code we are loading *VGG16* weights for image classifier using transfer learning","execution_count":null},{"metadata":{"_uuid":"44e5e52c-7621-4ff2-8340-7388770a0c9c","_cell_guid":"16ff63a2-7868-433a-a146-3a2031063703","trusted":true},"cell_type":"code","source":"# traing using transfer learning\n\nvggmodel =VGG16(weights='imagenet', include_top=False, input_shape = (224, 224, 3),pooling='max')\n\n # Print the model summary\nvggmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83ba0502-2955-480b-b26d-a346768fc7c8","_cell_guid":"79953bcb-7d2e-44f9-9ce0-784c06ed3ae7","trusted":true},"cell_type":"markdown","source":"Using already trained model for our task and bulding 2 fully connected layer with *softmax* activation function","execution_count":null},{"metadata":{"_uuid":"545156cd-eefb-49fb-9b40-5a5d636cb9b9","_cell_guid":"6710a7f7-a597-40d8-9bb6-fa34f7f8628b","trusted":true},"cell_type":"code","source":"vggmodel.trainable = False\nmodel = Sequential([\n  vggmodel, \n  Dense(1024, activation='relu'),\n  Dropout(0.15),\n  Dense(256, activation='relu'),\n  Dropout(0.15),\n  Dense(8, activation='softmax'),\n])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98699991-ad27-450a-9860-8d89cd09a9b7","_cell_guid":"3951ed85-fd07-4b58-9967-e468a2cab336","trusted":true},"cell_type":"code","source":"\nreduce_learning_rate = ReduceLROnPlateau(monitor='loss',\n                                         factor=0.1,\n                                         patience=2,\n                                         cooldown=2,\n                                         min_lr=0.00001,\n                                         verbose=1)\n\ncallbacks = [reduce_learning_rate]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"813cd62a-cdc7-4841-9d92-5d3da5c4cb90","_cell_guid":"e99f5993-f795-4f16-adf6-8c10fe4ae517","trusted":true},"cell_type":"markdown","source":"In the below code we are compiling and traing our image data","execution_count":null},{"metadata":{"_uuid":"1a11859e-8f3f-442c-a328-3f87319756f3","_cell_guid":"a97c8020-70f3-40ea-a246-14e73230355c","trusted":true},"cell_type":"code","source":"model.compile( optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\nhistory =model.fit_generator(\n    train_datagenerator.flow(X_train, to_categorical(y_train,8), batch_size=16),\n    validation_data=val_datagenerator.flow(X_val, to_categorical(y_val,8), batch_size=16),\n    verbose=2,\n    epochs=30,\n    callbacks=callbacks\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history.history['val_accuracy']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3bcb0e56-5562-487e-b44a-231e803d6768","_cell_guid":"cf94ed48-75ce-480f-b528-05de283f5f85","trusted":true},"cell_type":"code","source":"import matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\n\nacc      = history.history['accuracy']\nval_acc  = history.history[ 'val_accuracy' ]\nloss     = history.history[ 'loss' ]\nval_loss = history.history['val_loss' ]\n\nepochs   = range(len(acc)) # Get number of epochs\n\n#------------------------------------------------\n# Plot training and validation accuracy per epoch\n#------------------------------------------------\nplt.plot( epochs, acc )\nplt.plot( epochs, val_acc )\nplt.title('Training and validation accuracy')\nplt.figure()\n\n#------------------------------------------------\n# Plot training and validation loss per epoch\n#------------------------------------------------\nplt.plot  ( epochs,     loss )\nplt.plot  ( epochs, val_loss )\nplt.title ('Training and validation loss'   )","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}