{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport warnings\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"papers = pd.read_csv(\"../input/papers.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"403c8a38acfdf80a28ef17d456a7812ad67a490d"},"cell_type":"code","source":"papers.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7dc5bef3c034fa24191cfca0547ef3814f22a6eb"},"cell_type":"markdown","source":" # We find that:\n*  id\n*  event_type, and \n*  pdf_name\nare redundant and not important for prediction.\n# So we prepare the data by droping these columns in a new copied Data Frame"},{"metadata":{"trusted":true,"_uuid":"d38568c70d95a23df799280cd1bb4835d9d564f3"},"cell_type":"code","source":"papers_copy = papers.copy()\npapers_copy.drop(columns = ['id','event_type','pdf_name'],inplace=True)\npapers_copy.head(3)\n# We can now go on and prepare the daraset for modelling","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a34895f76c32ece06b1bc56d1ca3b22c5699ae1"},"cell_type":"markdown","source":"## Visualize the number of publications per year"},{"metadata":{"trusted":true,"_uuid":"65309cd898f084ecb06b6bd00d55b04dae18248d"},"cell_type":"code","source":"groups = papers_copy.groupby(\"year\")\ncounts = groups.size()\ncounts.plot(kind='bar',figsize=(9,7));\nplt.title(\"ML Publications per year\");\nplt.xlabel(\"Years..!!\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d323d28d19531e94566bacad21bfc521f8fd045"},"cell_type":"markdown","source":"# We see that between 2007 to 2017...the publications got almost tripled while between 1987 to 2007, there was a small significant change"},{"metadata":{"trusted":true,"_uuid":"0b4e4cf847ddc5b23835f82ab58a7e44a82aa453"},"cell_type":"code","source":"#Preprocessing the Text Data\n#1. Remove any punctutations\n#2. Perform LowerCasing\n#3. Print title before and after modifications\nimport re\nprint(\"Before MODIFICATION:\")\nprint(papers_copy['title'].head())\npapers_copy['title_processed'] = papers_copy['title'].map(lambda x: re.sub('[,\\.!?]','',x))\npapers_copy['title_processed'] = papers_copy['title_processed'].map(lambda x:x.lower())\nprint(\"After MODIFICATION\")\nprint(papers_copy['title_processed'].head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1ca68deacea0c050f81e5323690b238d499d4fa"},"cell_type":"markdown","source":"# Now we will use Word Cloud to Visualize the preprocessed text data:\n"},{"metadata":{"trusted":true,"_uuid":"03f0e636d8e15f238e59f5ce13f36123f28cf34a"},"cell_type":"code","source":"import wordcloud\nlong_string = \"\".join(papers_copy.title_processed)\nwordcloud = wordcloud.WordCloud()\nwordcloud.generate(long_string)\nwordcloud.to_image()\n\n#Word cloud tends out to be a very helpful tool to visualize the mmost used words in the particular feature.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c700406d9e1518e2395bbdab113bf5e7ddd46f5f"},"cell_type":"markdown","source":"# Prepare the Text for LDA Ananlysis:\n## LDA: Latent Dirichlet Allocation\n### It performs the \"topic detection\" for the large data sets, determining main \"topics\" that are in a large unlabelled set of texts"},{"metadata":{"trusted":true,"_uuid":"d5743308006cb2f5e53d7bc644a4ade5a79c7478"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ndef plot_10_most_common_words(count_data,count_vectorizer):\n    words = count_vectorizer.get_feature_names()\n    total_counts = np.zeros(len(words))\n    for t in count_data:\n        total_counts +=t.toarray()[0]\n    count_dict = (zip(words,total_counts))\n    count_dict= sorted(count_dict,key=lambda x:x[1],reverse = True)[0:10]\n    words = [w[0] for w in count_dict]\n    counts = [w[1] for w in count_dict]\n    x_pos = np.arange(len(words))\n    \n    plt.bar(x_pos,counts,align='center')\n    plt.xticks(x_pos,words,rotation=90)\n    plt.ylabel('counts')\n    plt.xlabel('words')\n    plt.title('10 Most Common Words..!!')\n    plt.show()\n    \n#Initialize the count vectorizer with the English Stop Words:\ncount_vectorizer = CountVectorizer(stop_words = 'english')\n#Fit and transform the processed titles\ncount_data = count_vectorizer.fit_transform(papers_copy['title_processed'])\n#Visualize the most 10-most common words\nplot_10_most_common_words(count_data,count_vectorizer)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c18d8c422cf1a5326b4aff6f0ee43d8492f86df1"},"cell_type":"markdown","source":"# Analyzing trends with LDA"},{"metadata":{"trusted":true,"_uuid":"ea43faea20bb5e6f59494c8b422db45db69ec8a4"},"cell_type":"code","source":"warnings.simplefilter(\"ignore\",DeprecationWarning)\nfrom sklearn.decomposition import LatentDirichletAllocation as LDA\n#Load the LDA model from sklearn\ndef print_topics(model,count_vectorizer,n_top_words):\n    words = count_vectorizer.get_feature_names()\n    for topic_idx, topic in enumerate(model.components_):\n        print(\"\\nTopic #%d:  \"% topic_idx)\n        print(\" \".join([words[i] for i in topic.argsort()[:- n_top_words -1: -1]]))\n#Tweak the two parameters below:\nnumber_topics =10\nnumber_words=6\n#Create and fit the LDA Model\nlda = LDA(n_components = number_topics)\nlda.fit(count_data)\n#Print the topics found by the LDA Model:\nprint(\"Topics Found: \")\nprint_topics(lda,count_vectorizer,number_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1218df142b80f6608fdc7b72aaacdfcf504c5258"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}