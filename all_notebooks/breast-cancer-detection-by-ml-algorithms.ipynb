{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # visualization\nimport seaborn as sns #for eda process\n\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score\nfrom sklearn import metrics\nfrom sklearn import preprocessing\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"cancer =pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cancer.head(100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*  Data contains; <br>\n\n*  ID number\n*  Diagnosis (M = malignant, B = benign) <br> \n\n\n* Ten real-valued features are computed for each cell nucleus:\n\n* a) radius (mean of distances from center to points on the perimeter) <br>\n* b) texture (standard deviation of gray-scale values) <br>\n* c) perimeter <br>\n* d) area <br>\n* e) smoothness (local variation in radius lengths) <br>\n* f) compactness (perimeter^2 / area - 1.0) <br>\n* g) concavity (severity of concave portions of the contour) <br>\n* h) concave points (number of concave portions of the contour) <br>\n* i) symmetry <br>\n* j) fractal dimension (\"coastline approximation\" - 1) <br>\n\n\n* All feature values are recoded with four significant digits.\n* Class distribution: 357 benign, 212 malignant <br>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**DATA MINING**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cancer.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Drop the columns ::\nUnnamed:32 and id.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncancer = cancer.drop(['Unnamed: 32'],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cancer = cancer.drop(['id'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Varaible Transformation by using LeabelEncoder","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cancer.diagnosis    = le.fit_transform(cancer.diagnosis)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cancer.diagnosis.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Data Exploration","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(5,5))\nplt.title('Class Distribution')\ncancer['diagnosis'].value_counts().plot(kind = 'pie',autopct = '%1.0f%%')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(cancer['diagnosis'],label=\"Count\")\nplt.title('Class Distribution')\nplt.xlabel('Class')\nplt.ylabel('Frequency')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation plot\ncancer_num = cancer.select_dtypes(include=['int64','float64' ])\ncorrelation = cancer.corr()\n\nplt.figure(figsize=(20,10))\nsns.heatmap(correlation,xticklabels=correlation.columns,yticklabels=correlation.columns,annot=True, annot_kws={\"size\": 8})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nplt.title('Distribution of Loan Amount')\nsns.distplot(cancer['diagnosis'], color='b')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.figure(figsize=(12,5))\nplt.title('Distribution of concave points_mean')\nsns.distplot(cancer['concave points_mean'], color='r')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.figure(figsize=(12,5))\nplt.title('Distribution of perimeter_worst')\nsns.distplot(cancer['perimeter_worst'], color='k')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.figure(figsize=(12,5))\nplt.title('Distribution of concave points_worst')\nsns.distplot(cancer['concave points_worst'], color='k')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cancer.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cancer.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Divide the target variable and independent variable into two sepearete object named as cancer_x(contains all Independent Varibale) and cancer_y(has only Target Variable)**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cancer_x = cancer.iloc[:,1:31] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cancer_x.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cancer_y = cancer.iloc[:,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cancer_y.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sampling","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Divide the data into model training dataset and test dataset**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn \nfrom sklearn.model_selection import train_test_split  #for spliiting the data\n\n\nx_train, x_test, y_train, y_test = train_test_split(cancer_x, cancer_y, test_size=0.33, random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_train.shape, y_train.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_test.shape , y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# # Model Building, testing and validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Creating Model for Logistic Regression <br>\n\n* We can use sklearn library .","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression #To build logistic regression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Prediction for model\npred = log.predict(x_test)\npred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusionmatrix_log = pd.crosstab(pred, y_test, rownames=['predicted'], colnames= ['Actual'], margins=False)\nconfusionmatrix_log","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tab_log = confusion_matrix(pred, y_test)\nprint(tab_log)\nprint('\\n')\nprint(classification_report(pred, y_test))\n\nprint('\\n')\naccuracy_log = tab_log.diagonal().sum() / tab_log.sum()*100\naccuracy_log","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color=\"#0FBBAE\">Test Accuracy of Logistic Regression is: <font color=\"red\">95.74%</font></font>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Creating Model for Support Vector Machine <br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_model = LinearSVC()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_model.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_svm = svm_model.predict(x_test)\npredict_svm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tab_svm = confusion_matrix(predict_svm, y_test)\nprint(tab_svm)\n\nprint('\\n')\n    \nprint(classification_report(predict_svm, y_test))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_svm = tab_svm.diagonal().sum() / tab_svm.sum()*100\naccuracy_svm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color=\"magenta\">Accuracy of Support Vector Machine is: <font color=\"red\">93.61%</font></font>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Model for Random_Forest <br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier  #To build Random_Forest\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf.fit(x_train,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_rf = rf.predict(x_test)\npred_rf\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tab_rf = confusion_matrix(pred_rf, y_test)\nprint(tab_rf)\nprint('\\n')\n    \nprint(classification_report(pred_rf, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_rf = tab_rf.diagonal().sum() / tab_rf.sum()*100\naccuracy_rf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ## <font color=\"#CFC60E\">Test Accuracy of Random Forest Algorithm is : <font color=\"red\">95.74%</font></font>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n# STORING (FEATURE SELECTION AND COLUMN NAME IN A DATAFRAME)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_feature_score = pd.DataFrame({'Importance':rf.feature_importances_ ,'Variables_Name':x_train.columns})\nrf_feature_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n* # SORTING IN ASCENDING ORDER , TO UNDERSTAND WHICH FEATURE/COLUMN HAS MORE IMPORTANCE","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nrf_feature_score.sort_values(['Importance'],ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Model for Decision Tree <br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.tree import DecisionTreeClassifier  #To build Decision_Tree","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtree = DecisionTreeClassifier()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtree.fit(x_train,y_train)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_dtree = dtree.predict(x_test)\npred_dtree","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tab_dtree = confusion_matrix(pred_dtree,y_test)\n\nprint(tab_dtree)\n\n\nprint(classification_report(pred_dtree,y_test))\n\naccuracy_dtree = tab_dtree.diagonal().sum() / tab_dtree.sum()*100\naccuracy_dtree","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color=\"green\">Test Accuracy of Decision Tree Algorithm is <font color=\"red\">94.14%</font></font>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dtree_feature_score = pd.DataFrame({'Importance':dtree.feature_importances_ ,'Variables_Name':x_train.columns})\ndtree_feature_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtree_feature_score.sort_values(['Importance'],ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Model for Naive Bayes <br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB   #To build navie_bayes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nnaive_bay = MultinomialNB()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"naive_bay.fit(x_train,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_nb = naive_bay.predict(x_test)\npredictions_nb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tab_nb = confusion_matrix(predictions_nb,y_test)\n\nprint(tab_nb)\n\n\nprint(classification_report(predictions_nb,y_test))\n\naccuracy_nb = tab_nb.diagonal().sum() / tab_nb.sum()*100\naccuracy_nb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## <font color=\"orange\">Test Accuracy of Naive Bayes Algorithm is <font color=\"red\"> 92.02%</font></font>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Compairing accuracy for different algorithms","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracies = ({\"Logistic Regression\":accuracy_log,\"Decision Tree\":accuracy_dtree,\"Random Forest\":accuracy_rf,\"Naive Bayes\":accuracy_nb,'SVM':accuracy_svm})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracies)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colors = [\"purple\", \"green\", \"orange\", \"magenta\",\"#CFC60E\"]\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,5))\nplt.yticks(np.arange(0,100,10))\nplt.ylabel(\"Accuracy %\")\nplt.xlabel(\"Algorithms\")\nsns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()), palette=colors)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(24,12))\n\nplt.suptitle(\"Confusion Matrixes\",fontsize=24)\nplt.subplots_adjust(wspace = 0.4, hspace= 0.4)\n\nplt.subplot(2,3,1)\nplt.title(\"Logistic Regression Confusion Matrix\")\nsns.heatmap(tab_log,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,2)\nplt.title(\"Random Forest Confusion Matrix\")\nsns.heatmap(tab_rf,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,3)\nplt.title(\"Support Vector Machine Confusion Matrix\")\nsns.heatmap(tab_svm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,4)\nplt.title(\"Decision Tree Classifier Confusion Matrix\")\nsns.heatmap(tab_dtree,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,5)\nplt.title(\"Naive Bayes Confusion Matrix\")\nsns.heatmap(tab_nb,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n**I am new with data science. Please comment me your feedbacks to help me improve myself. Thanks for your time.**\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}