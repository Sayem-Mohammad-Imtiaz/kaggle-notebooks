{"cells":[{"metadata":{},"cell_type":"markdown","source":"**STYLE TRANSFER LEARNING**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from __future__ import print_function\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torchvision.transforms as transforms\nimport torchvision.models as models\n\nimport copy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Image Loader Function \ndef image_loader(img_path, max_size=600, shape=None):\n    image = Image.open(img_path).convert('RGB')\n    \n    if max(image.size) > max_size:\n        imgsize = max_size\n    else:\n        imgsize = max(image.size)\n        \n    if shape is not None:\n        size = shape\n        \n    in_transform = transforms.Compose([\n        transforms.Resize((imgsize, int(1.5*imgsize))),\n        transforms.ToTensor(), #convert to tensor\n        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n    \n    image = in_transform(image)[:3, :, :].unsqueeze(0)\n    \n    return image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load Style Image\nstyle = image_loader(\"../input/picadylan/pics.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def im_show(tensor):  #function to show image\n    image = tensor.to(\"cpu\").clone().detach() #clone to not do changes on it\n    image = image.numpy().squeeze() #remove fake batch dimension\n    image = image.transpose(1, 2, 0)\n    image = image * np.array((0.229, 0.224, 0.225)) + np.array(\n    (0.485, 0.456, 0.406))\n    image = image.clip(0, 1)\n    \n    return image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature Extraction Function\ndef get_features(image, model, layers=None):\n    if layers is None:\n        layers = {'0': 'conv1_1','5': 'conv2_1',\n                  '10': 'conv3_1',\n                  '19': 'conv4_1',\n                  '21': 'conv4_2',  ## content layer\n                  '28': 'conv5_1'}\n        \n    features = {}\n    x = image\n    for name, layer in enumerate(model.features):\n        x = layer(x)\n        if str(name) in layers:\n            features[layers[str(name)]] = x\n            \n    return features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#gram marix function\n#gram matrix = mat * tranpose(mat)\n\ndef gram_matrix(tensor):\n    _,n_filters,h,w = tensor.size() #abs(=1)\n    #b = number of feature maps\n    #(c,d) = dimensions of a f. map (N=c*d)\n    tensor = tensor.view(n_filters, h * w)\n    G = torch.mm(tensor, tensor.t()) #gram matrix\n    return G","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**IMPORTING THE MODEL**"},{"metadata":{"trusted":true},"cell_type":"code","source":"cnn = models.vgg19(pretrained=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for param in cnn.parameters():\n    param.requires_grad_(False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#AvgPool2d instead of MaxPool2d for better results\nfor i, layer in enumerate(cnn.features):\n    if isinstance(layer, torch.nn.MaxPool2d):\n        cnn.features[i] = torch.nn.AvgPool2d(kernel_size=2, stride=2, padding=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check CUDA\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ncnn.to(device).eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#load content image\ncontent = image_loader(\"../input/picadylan/dylan.jpg\").to(device)\nstyle = style.to(device)\n\ncontent_features = get_features(content, cnn)\nstyle_features = get_features(style, cnn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"style_grams = {\n    layer: gram_matrix(style_features[layer]) for layer in style_features}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#target image\ntarget = content.clone().requires_grad_(True).to(device) #clone content image for target\n#for target with random white noise use the line below\n#target = torch.randn_like(content).requires_grad_(True).to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Style weights for different layers\nstyle_weights = {'conv1_1': 0.75,\n                 'conv2_1': 0.5,\n                 'conv3_1': 0.2,\n                 'conv4_1': 0.2,\n                 'conv5_1': 0.2}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#default weights\ncontent_weight = 1e4\nstyle_weight = 1e2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#optimizer LBFGS\noptimizer = optim.LBFGS([target.requires_grad_()])\nnum_iterations = 400","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Style Transfer Function\ndef styl_trans():\n    i = [0]\n    while i[0] <= num_iterations:\n        def closure():\n            optimizer.zero_grad()\n            target_features = get_features(target, cnn)\n    \n            content_loss = torch.mean((target_features['conv4_2'] -\n                             content_features['conv4_2']) ** 2)\n  \n            style_loss = 0\n            for layer in style_weights:\n                target_feature = target_features[layer]\n                target_gram = gram_matrix(target_feature)\n                _, d, h, w = target_feature.shape\n                style_gram = style_grams[layer]\n                layer_style_loss = style_weights[layer] * torch.mean(\n                    (target_gram - style_gram) ** 2)\n                style_loss += layer_style_loss / (d * h * w)\n    \n                style_score = style_weight * style_loss\n                content_score = content_weight * content_loss\n                total_loss = content_weight * content_loss + style_weight * style_loss\n                total_loss.backward(retain_graph=True)\n        \n            i[0] += 1\n            if i[0] % 50 == 0:\n                content_fraction = round(\n                    content_weight*content_loss.item()/total_loss.item(), 2)\n                style_fraction = round(\n                    style_weight*style_loss.item()/total_loss.item(), 2)\n                print('Iteration {}, (content-loss: {}, style-loss {})'.format(\n                    i, content_fraction, style_fraction))\n            return style_score + content_score\n        optimizer.step(closure)\n    final_img = im_show(target)\n    return final_img\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#OUTPUT\noutput = styl_trans()\nfig = plt.figure()\nplt.imshow(output)\nplt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}