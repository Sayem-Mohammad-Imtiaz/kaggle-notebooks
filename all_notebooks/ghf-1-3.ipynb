{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Univ.AI Hackathon ","metadata":{}},{"cell_type":"markdown","source":"## Hello and Welcome to the start of Ayan's Attempt to Geofferey Hinton's Fellowship \n\n## I am honored to participate in such an event and present my work. \n\n## Let's begin with the notebook then. ","metadata":{}},{"cell_type":"markdown","source":"# Contents \n\n## 1. [Libraries](#libraries)\n## 2. [Data Visualization](#data_visualization)\n## 3. [Feature Engineering](#feature_engineering)\n## 4. [Data Modelling](#data_modelling)\n## 5. [Output](#output)","metadata":{}},{"cell_type":"markdown","source":"# Libraries <a id='libraries'></a>\nLet's start with all the imports and get done with the boring part ","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport re\nfrom category_encoders import LeaveOneOutEncoder\nfrom sklearn.preprocessing import OrdinalEncoder # Not Needed Right now\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\n\nimport optuna\nimport lightgbm\nfrom sklearn.metrics import roc_auc_score,roc_curve\nfrom sklearn.model_selection import StratifiedKFold\nfrom imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state = 2)\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/ghf1-hackathon/Training Data.csv')\ntest=pd.read_csv('/kaggle/input/ghf1-hackathon/Test Data.csv')\nsub=pd.read_csv('/kaggle/input/ghf1-hackathon/Sample Prediction Dataset.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Visualization <a id='data_visualization'></a>\n### Let's look with what data we will be working with in this Problem ","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### So the data has 2.5 lakh entires, that's a lot of credit card data\n\n### Let's see what is the distribution of the target column in this data ","metadata":{}},{"cell_type":"code","source":"train['risk_flag'].value_counts()/len(train)*100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Looks like the data is highly skewed. \n\n### Check for null values in train and test set ","metadata":{}},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### So no null values, the next step is to go for getting info from train set\n\n### As see if we can find anything solid ","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Id column looks a bit redundant here, dropping it will be the best thing to do ","metadata":{}},{"cell_type":"code","source":"train.drop(columns=['Id'],inplace=True)\ntest.drop(columns=['id'],inplace=True) # Note:- Keep in mind the 'id' for test is different, i is lower-case in this case, so case in point :-)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_columns=['age', 'experience', 'married', 'house_ownership','car_ownership', 'profession', 'city',\n               'state', 'current_job_years','current_house_years',]\n\nfor i in total_columns:\n    print(f'Column :{i} New Elements :{set(test[i].unique())-set(train[i].unique())}')\n    print(f'Number of New Elements {len(set(test[i].unique())-set(train[i].unique()))}')\n    print('')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### This is interesting, we can see that we have a lot of entries with '-' , 'upper case' , '_' or anything else creating a new entry\n\n### Before we proceed any further, let's fix this error","metadata":{}},{"cell_type":"code","source":"def convert_profession(x):\n    prof = x\n    prof = re.sub(r'\\-',' ',prof)\n    prof = re.sub(r'\\_',' ',prof)\n    prof = re.sub(r'\\[[0-9]\\]',' ',prof)\n    prof = prof.split()\n    prof = ' '.join(prof)\n    \n    prof = prof.lower()\n    return prof\n\ndef convert_state(x):\n    state = x\n    state = re.sub(r'\\-',' ',state)\n    state = re.sub(r'\\_',' ',state)\n    state = re.sub(r'\\[[0-9]\\]',' ',state)\n    state = state.split()\n    state = ' '.join(state)\n    \n    state = state.lower()\n    return state\n\ndef convert_city(x):\n    city = x\n    city = re.sub(r'\\-',' ',city)\n    city = re.sub(r'\\_',' ',city)\n    city = re.sub(r'\\[[0-9][0-9]*\\]',' ',city)\n    city = city.split()\n    city = ' '.join(city)\n    \n    city = city.lower()\n    return city\n\ntrain['profession'] = train['profession'].apply(convert_profession)\ntest['profession'] = test['profession'].apply(convert_profession)\n\ntrain['state'] = train['state'].apply(convert_state)\ntest['state'] = test['state'].apply(convert_state)\n\ntrain['city'] = train['city'].apply(convert_city)\ntest['city'] = test['city'].apply(convert_city)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Confirm it for surety, sice being extra carefull is never bad ","metadata":{}},{"cell_type":"code","source":"total_columns=['age', 'experience', 'married', 'house_ownership','car_ownership', 'profession', 'city',\n               'state', 'current_job_years','current_house_years',]\n\nfor i in total_columns:\n    print(f'Column :{i} New Elements :{set(test[i].unique())-set(train[i].unique())}')\n    print(f'Number of New Elements {len(set(test[i].unique())-set(train[i].unique()))}')\n    print('')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Done and Done\n\n### Moving on now, some basic visualisations for inferences ","metadata":{}},{"cell_type":"code","source":"sns.distplot(train['income'],bins=1000,norm_hist=False,kde=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(train['age'],kde=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x=train['experience'],y=train['age'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.barplot(x='experience',y='age',data=train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.barplot(x='experience',y='income',hue='married',data=train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Looks like married people are paid less as they gather experience \n\n### TAKE THAT SOCIETY AND SOCIAL NORMS!","metadata":{}},{"cell_type":"code","source":"train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.barplot(x='house_ownership',y='income',data=train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.groupby('experience')['age'].median()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.groupby('experience')['income'].median()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now, let's be honest with ourselves \n\n### This looks very beautiful but not a lot can actually be inferred from it, so let's jump right into feature engineering ","metadata":{}},{"cell_type":"markdown","source":"## Feature Engineering <a id='feature_engineering'></a>\n\n### Time to get our hands dirty and play with the data \n\n### First things first let's check for duplicate entries","metadata":{}},{"cell_type":"code","source":"df = train[train.duplicated()]\nlen(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Wow, that's a lot \n\n### Drop duplicates to get a better model ","metadata":{}},{"cell_type":"code","source":"train.drop_duplicates(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating new column <a id='profession_state'></a>\n### This part is important but it will be explained later ","metadata":{}},{"cell_type":"code","source":"def profession_state(x):\n    prof = x[0]\n    state = x[1]\n    \n    new = prof + '_' + state\n    return new \n        \ntrain['profession_state']=train[['profession','state']].apply(profession_state,axis=1)\ntest['profession_state']=test[['profession','state']].apply(profession_state,axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dummie_columns = ['married','house_ownership','car_ownership']\ntarget_columns = ['profession','city','state','profession_state']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dividing the categorical columns into Dummy and Target Encoded columns\n\n### Converting Columns with few values into dummies and others into target encoding cause of curse of dimensionality ","metadata":{}},{"cell_type":"code","source":"train = pd.get_dummies(train, columns=dummie_columns, drop_first=True)\ntest = pd.get_dummies(test, columns=dummie_columns,drop_first=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loo = LeaveOneOutEncoder()\ntrain[target_columns] = loo.fit_transform(train[target_columns],train['risk_flag'])\ntest[target_columns] = loo.transform(test[target_columns])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## VIF(Variance Inflation Factor) <a id='vif'></a>\n### VIF factor an important statistical concept playing a vital role in this question","metadata":{}},{"cell_type":"code","source":"X=train.drop(columns=['risk_flag'])\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = X.columns\n  \n# calculating VIF for each feature\nvif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n                          for i in range(len(X.columns))]\n  \nprint(vif_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Important thing to note here is we remove profession and state because of their evident high vif factor and inplace introduce a new column of [profession_state](#profession_state)","metadata":{}},{"cell_type":"markdown","source":"### Binning age to age_bins for a better model and robust model ","metadata":{}},{"cell_type":"code","source":"labels=[1,2,3,4,5]\ntrain['age_bin'] = pd.cut(train['age'], bins=5, labels=labels).astype('uint8')\ntest['age_bin'] = pd.cut(test['age'], bins=5, labels=labels).astype('uint8')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dropping 'age','profession' and 'state' based on their high [VIF factor](#vif) and to remove multi-colinearity effect","metadata":{}},{"cell_type":"code","source":"train.drop(columns=['age','profession','state'],inplace=True)\ntest.drop(columns=['age','profession','state'],inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's just check our VIF factor again, cause there is nothing like being too careful :-)","metadata":{}},{"cell_type":"code","source":"X=train.drop(columns=['risk_flag'])\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = X.columns\n  \n# calculating VIF for each feature\nvif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n                          for i in range(len(X.columns))]\n  \nprint(vif_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### I am kind of satisfied with it cause any changes just lowers the roc-auc score of the model. ","metadata":{}},{"cell_type":"markdown","source":"## Data Modelling <a id='data_modelling'></a>\n\n### Time to be setup your favourite movie cause it's modelling time \n\n### I tried it with Boosting approach, in particular LGBMClassifier, XGBClassifier and CatBoost\n\n### Here, LGBMClassifier fetched the best result so I am going forward with it for now \n\n### For feature tunning i have performed Optuna Optimization the code for the same is provided below","metadata":{}},{"cell_type":"code","source":"features = ['income', 'experience', 'city', 'current_job_years',\n       'current_house_years', 'profession_state','married_single',\n        'house_ownership_owned', 'house_ownership_rented','car_ownership_yes', 'age_bin']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=train.drop(columns=['risk_flag'])\ntarget=train['risk_flag']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Optuna <a id='optuna'></a>\n\n### The functioning of optuna can be understood through it's [documentation](https://github.com/optuna/optuna)","metadata":{}},{"cell_type":"code","source":"# def objective(trial , data = data , target = target):\n#     train_x , test_x , train_y , test_y = train_test_split(data , target , \\\n#             test_size = 0.28059109276941666 , random_state = 42)\n    \n#     params = {\n#         'reg_alpha' : trial.suggest_loguniform('reg_alpha' , 1e-5 , 10),\n#         'reg_lambda' : trial.suggest_loguniform('reg_lambda' , 1e-5 , 10),\n#         'num_leaves' : trial.suggest_int('num_leaves' , 11 , 300),\n#         'learning_rate' : trial.suggest_uniform('learning_rate' , 0 , 0.1),\n#         'max_depth' : trial.suggest_int('max_depth' , 1 , 20),\n#         'n_estimators' : trial.suggest_int('n_estimators' , 1 , 9999),\n#         'min_child_samples' : trial.suggest_int('min_child_samples' , 1 , 100),\n#         'min_child_weight' : trial.suggest_loguniform('min_child_weight' , 1e-5 , 1),\n#         'subsample' : trial.suggest_uniform('subsample' , 0 , 1.0),\n#         'colsample_bytree' : trial.suggest_loguniform('colsample_bytree' , 1e-5 , 1),\n#         'random_state' : trial.suggest_categorical('random_state' , [0,42,2021,555]),\n#         'metric' : 'auc',\n#         'device_type' : 'gpu',\n#     }\n#     model = lightgbm.LGBMClassifier(**params)\n#     model.fit(train_x , train_y , eval_set = [(test_x , test_y)] , early_stopping_rounds = 200 , \\\n#              verbose = False)\n#     preds = model.predict_proba(test_x)[:,1]\n#     auc = roc_auc_score(test_y , preds)\n#     return auc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# study = optuna.create_study(direction = 'maximize' , study_name = 'lgbm')\n# study.optimize(objective , n_trials = 50)\n# print('numbers of the finished trials:' , len(study.trials))\n# print('the best params:' , study.best_trial.params)\n# print('the best value:' , study.best_value)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using [Optuna](#optuna) result from above, we can model our lgbm model \n\n### The block given below looks terrifying but let me just break it down for you, \n\n### First we are taking stratified k-fold to divide the entire data into small packets of samples which will later be taken into train and test set. Since, the data is highly skewed and Binary classification problem stratified k-fold is adopted to balance out the majority and minority classes in all the samples. \n\n### Then, data is split and model is prepared. \n\n### Here LGBM model is used for training. \n\n### The parameters for LGBM can be seen from it's [documentation](https://lightgbm.readthedocs.io/en/latest/Parameters.html)","metadata":{}},{"cell_type":"code","source":"preds=np.zeros(test.shape[0])\noof_predictions=np.zeros(len(data))\nkf=StratifiedKFold(n_splits=20,random_state=42,shuffle=True)\nroc=[]\nn=0\nfor trn_idx,val_idx in kf.split(data,target):\n    train_x = data.iloc[trn_idx]\n    train_y = target.iloc[trn_idx]\n    val_x = data.iloc[val_idx]\n    val_y = target.iloc[val_idx]\n    \n    lgb_model = LGBMClassifier(\n        random_state=42,\n        cat_l2=25.999876242730252,\n        cat_smooth=89.2699690675538,\n        colsample_bytree=0.37861069156854954,\n        early_stopping_round=200,\n        learning_rate=0.015559547182527743,\n        max_bin=788,\n        max_depth=16,\n        metric=\"auc\",\n        min_data_per_group=177,\n        n_estimators=9482,\n        n_jobs=-1,\n        num_leaves=244,\n        reg_alpha=1.8401832133621817e-05,\n        reg_lambda=0.11900311978519747,\n        subsample=0.9753842843857927,\n        subsample_freq=1,\n        verbose=-1,\n    )\n    lgb_model.fit(train_x,train_y,eval_set=[(val_x,val_y)],early_stopping_rounds=100,verbose=False)\n    preds+=lgb_model.predict_proba(test[features])[:,1]/kf.n_splits\n    oof_predictions += lgb_model.predict_proba(data[features])[:,1]/kf.n_splits\n    roc.append(roc_auc_score( val_y , lgb_model.predict_proba(val_x)[:,1]))\n    print(f\"ROC {n+1}: {roc[n]}\")\n    n=n+1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Output <a id='output'></a>\n### The only thing left now is to get the test predictions and upload the csv file for submission","metadata":{}},{"cell_type":"code","source":"np.mean(roc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['risk_flag'] = preds\nsub.to_csv('lgb_1.csv' , index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_new = [1 if x>0.5 else 0 for x in preds]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['risk_flag']=preds_new\nsub.to_csv('lgb_2.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}