{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This notebook contains Exploratory Data Analysis and some data visualisation using plotly package. The data used here is the Ecommerce behaviour data for a medium cosmetics online store and I have chosen only January data for the analysis.\n    \nLink to the dataset -> [https://www.kaggle.com/mkechinov/ecommerce-events-history-in-cosmetics-shop](http://)\n\nLets begin.\n    ","metadata":{}},{"cell_type":"markdown","source":"# Table of Contents","metadata":{}},{"cell_type":"markdown","source":"* [Import Packages](#1)\n* [Data Cleaning and EDA](#2)\n    1. [Reading Data](#3)\n    2. [Parsing Datetime and Creating necessary Datetime columns](#4)\n    3. [Checking datatypes and range for filtering unrelavent data](#5)\n    4. [Checking Null data](#6)\n* [Data Analysis and Data Visualisation](#7)\n    1. [Checking the Unique Values](#8)\n    2. [Customer Purchase Funnel](#9)\n        1. [Data Prep](#10)\n        2. [Funnel Visualisation](#11)\n    3. [Hourly Website Traffic](#12)\n        1. [Data Prep](#13)\n        2. [Hourly Website Traffic Visualisation](#14)\n    4. [Daily Sales,Ticket Size and Number of Orders](#15)\n        1. [Data Prep](#16)\n        2. [Visualising Daily sales,Ticket Size and # of Orders](#17)\n    5. [Hourly sales During Jan - Heatmap Vs Countour](#18)\n        1. [Data Prep](#19)\n        2. [Data Visualisation - Daily Sales (by Hour)](#20)\n        3. [Data Visualisation - Hourly Sales (by Week)](#21)\n    6. [Summary Statistics for # of Orders and Ticket size](#22)\n        1. [Data Prep](#23)\n        2. [Visualising summary statistics](#24)\n    ","metadata":{}},{"cell_type":"markdown","source":"# Import Packages\n<a id=\"1\"></a>\n**Lets import all the necessary packages that we are going to use**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport plotly.express as px \nimport os\nimport matplotlib.pyplot as plt\nimport missingno as msno\nimport datetime\nfrom plotly import graph_objects as go\nfrom plotly.subplots import make_subplots","metadata":{"execution":{"iopub.status.busy":"2021-07-08T07:26:59.091115Z","iopub.execute_input":"2021-07-08T07:26:59.091541Z","iopub.status.idle":"2021-07-08T07:27:01.497645Z","shell.execute_reply.started":"2021-07-08T07:26:59.091508Z","shell.execute_reply":"2021-07-08T07:27:01.49659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This notebook consists of some Exploratory data analysis, Basic web analysis and data visualisation. For simplicity, will be using only Jan data.","metadata":{}},{"cell_type":"markdown","source":"# Data Cleaning and EDA\n<a id=\"2\"></a>","metadata":{}},{"cell_type":"markdown","source":"## 1.Reading Data\n<a id=\"3\"></a>","metadata":{}},{"cell_type":"code","source":"data=pd.read_csv('../input/ecommerce-events-history-in-cosmetics-shop/2020-Jan.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-08T07:27:01.499346Z","iopub.execute_input":"2021-07-08T07:27:01.499742Z","iopub.status.idle":"2021-07-08T07:27:14.52578Z","shell.execute_reply.started":"2021-07-08T07:27:01.499705Z","shell.execute_reply":"2021-07-08T07:27:14.52468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T07:27:14.52806Z","iopub.execute_input":"2021-07-08T07:27:14.528417Z","iopub.status.idle":"2021-07-08T07:27:14.560989Z","shell.execute_reply.started":"2021-07-08T07:27:14.528385Z","shell.execute_reply":"2021-07-08T07:27:14.559806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.Parsing Datetime and Creating necessary Datetime columns\n<a id=\"4\"></a>","metadata":{}},{"cell_type":"markdown","source":"Event_time column has string \"UTC\" in it. So split the actual datetime and UTC seperate. Then convert the Event_time column into datetime datatype. Also, create new columns called date,time,hour,weekday,weeknum from the \"event_time\" column. Lets also change the weekday format from number to string like mon,tues,etc..","metadata":{}},{"cell_type":"code","source":"#seperating timezone\ndata[\"timezone\"]= data[\"event_time\"].str.rsplit(\" \", n=1,expand = True)[1]\ndata[\"event_time\"]= data[\"event_time\"].str.rsplit(\" \", n=1,expand = True)[0]\ndata[\"event_time\"]=pd.to_datetime(data[\"event_time\"])\n\n#creating date,time,hours,weekday,weeknum columns\ndata[\"date\"]=data['event_time'].dt.date\ndata[\"time\"]=data['event_time'].dt.time\ndata[\"hours\"]=data['event_time'].dt.hour\ndata[\"weekday\"]=data['event_time'].dt.weekday\ndata['weeknum']=data['event_time'].dt.isocalendar().week\n\n#changing weekday to string and adding 'week_' prefix to weeknum\ndata['weeknum'] = 'week_' + data['weeknum'].astype(str)\ndata['weekday']= data['weekday'].replace({0:'Mon',1:'Tues',2:'Wed',3:'Thurs',4:'Fri',5:'Sat',6:'Sun'})","metadata":{"execution":{"iopub.status.busy":"2021-07-08T07:27:14.563471Z","iopub.execute_input":"2021-07-08T07:27:14.563975Z","iopub.status.idle":"2021-07-08T07:27:55.262085Z","shell.execute_reply.started":"2021-07-08T07:27:14.563925Z","shell.execute_reply":"2021-07-08T07:27:55.261267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-08T07:27:55.263349Z","iopub.execute_input":"2021-07-08T07:27:55.263965Z","iopub.status.idle":"2021-07-08T07:27:55.286564Z","shell.execute_reply.started":"2021-07-08T07:27:55.263919Z","shell.execute_reply":"2021-07-08T07:27:55.285259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.Checking datatypes and range for filtering unrelavent data\n<a id=\"5\"></a>","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T07:27:55.288377Z","iopub.execute_input":"2021-07-08T07:27:55.28881Z","iopub.status.idle":"2021-07-08T07:27:55.313916Z","shell.execute_reply.started":"2021-07-08T07:27:55.288775Z","shell.execute_reply":"2021-07-08T07:27:55.313035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see from above info() method, all the datatypes are correctly available. Hours is available as int64. Since hours is only used for grouping and other data analysis related works, its okay to have it in int64 format. Lets quickly check the minimum, average, maximum and other statics related to the numerical columns.","metadata":{}},{"cell_type":"code","source":"data.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T07:27:55.315256Z","iopub.execute_input":"2021-07-08T07:27:55.31574Z","iopub.status.idle":"2021-07-08T07:27:57.715816Z","shell.execute_reply.started":"2021-07-08T07:27:55.315709Z","shell.execute_reply":"2021-07-08T07:27:57.714756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that there are some entries which have negative prices. This might be a return orders. Lets just try to see how many orders are returned.","metadata":{}},{"cell_type":"code","source":"returned_orders=data[data['price']<0]['price'].count()\nreturned_orders_perc=returned_orders/(data['price'].count())\n\nprint(\"There are %2d returned orders which is %.5f of total orders.\" %(returned_orders,round(returned_orders_perc,5)))","metadata":{"execution":{"iopub.status.busy":"2021-07-08T07:27:57.71723Z","iopub.execute_input":"2021-07-08T07:27:57.71758Z","iopub.status.idle":"2021-07-08T07:27:57.753686Z","shell.execute_reply.started":"2021-07-08T07:27:57.717545Z","shell.execute_reply":"2021-07-08T07:27:57.752716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"0.00001% almost negligible.Might be result of poor data mining. Lets remove those records and save our dataset.","metadata":{}},{"cell_type":"markdown","source":"## 4.Checking Null data\n<a id=\"6\"></a>","metadata":{}},{"cell_type":"code","source":"data=data[data['price']>=0]\n\n#Checkinng how much missing values are present in the dataset.\n\n# Calculate the Percentage of missing values in All columns\nperc=data.isnull().sum() * 100 / len(data)\nprint(round(perc,2))","metadata":{"execution":{"iopub.status.busy":"2021-07-08T07:27:57.756373Z","iopub.execute_input":"2021-07-08T07:27:57.756676Z","iopub.status.idle":"2021-07-08T07:28:01.958887Z","shell.execute_reply.started":"2021-07-08T07:27:57.756647Z","shell.execute_reply":"2021-07-08T07:28:01.957863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can clearly see that Category_code and brand has majority of missing values. user_session has very minimal missing value. Lets visually see the missing value in the dataframe as follows:","metadata":{}},{"cell_type":"code","source":"#Visualising in matrix form\nmsno.matrix(data)","metadata":{"execution":{"iopub.status.busy":"2021-07-08T07:28:01.96078Z","iopub.execute_input":"2021-07-08T07:28:01.961086Z","iopub.status.idle":"2021-07-08T07:28:29.23246Z","shell.execute_reply.started":"2021-07-08T07:28:01.961055Z","shell.execute_reply":"2021-07-08T07:28:29.231119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualising as bar graph\nmsno.bar(data)","metadata":{"execution":{"iopub.status.busy":"2021-07-08T07:28:29.234483Z","iopub.execute_input":"2021-07-08T07:28:29.23497Z","iopub.status.idle":"2021-07-08T07:28:33.944357Z","shell.execute_reply.started":"2021-07-08T07:28:29.234911Z","shell.execute_reply":"2021-07-08T07:28:33.943237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Analysis and Data Visualisation\n<a id=\"7\"></a>","metadata":{}},{"cell_type":"markdown","source":"## 1.Checking the Unique Values\n<a id=\"8\"></a>","metadata":{}},{"cell_type":"markdown","source":"Lets check the number of Unique values in the dataframe","metadata":{}},{"cell_type":"code","source":"#checking number of unique values in dataframe\ndata.nunique()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T07:28:33.94603Z","iopub.execute_input":"2021-07-08T07:28:33.946492Z","iopub.status.idle":"2021-07-08T07:28:42.715903Z","shell.execute_reply.started":"2021-07-08T07:28:33.946445Z","shell.execute_reply":"2021-07-08T07:28:42.714747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.Customer Purchase Funnel\n<a id=\"9\"></a>","metadata":{}},{"cell_type":"markdown","source":"### A.Data Prep\n<a id=\"10\"></a>","metadata":{}},{"cell_type":"markdown","source":"We can see that there are only 4 types of event_types in the data. So lets check the differnt types and will create a data funnel as how customers will go thorugh the purchase funnel.","metadata":{}},{"cell_type":"code","source":"data.event_type.unique()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T07:28:42.717521Z","iopub.execute_input":"2021-07-08T07:28:42.717954Z","iopub.status.idle":"2021-07-08T07:28:43.065817Z","shell.execute_reply.started":"2021-07-08T07:28:42.717909Z","shell.execute_reply":"2021-07-08T07:28:43.064562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see there are four event_types out of which \"Remove_from_cart\" is a event which all users might not go through during their journey. So lets remove that particular event before we group and make our data ready for visualising the funnel","metadata":{}},{"cell_type":"code","source":"#grouping and preparing data for funnel visualisation\n\ndata_funnel=data[data['event_type']!='remove_from_cart'].groupby(['event_type'],as_index=False)['event_time'].count()\ndata_funnel.columns=['event_type','# events']\ndata_funnel.sort_values('# events', inplace=True,ascending=False)\ndata_funnel.reset_index(drop=True,inplace=True)\ndata_funnel['percent']=data_funnel['# events']/(data_funnel['# events'][0].sum())*100\ndata_funnel","metadata":{"execution":{"iopub.status.busy":"2021-07-08T07:28:43.067306Z","iopub.execute_input":"2021-07-08T07:28:43.067656Z","iopub.status.idle":"2021-07-08T07:28:44.718411Z","shell.execute_reply.started":"2021-07-08T07:28:43.067625Z","shell.execute_reply":"2021-07-08T07:28:44.717286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### B. Funnel Visualisation<a id=\"11\"></a>","metadata":{}},{"cell_type":"markdown","source":"Now lets use plotly too visualise the customer funnel","metadata":{}},{"cell_type":"code","source":"#plotly to visualise funnel\nfig = go.Figure(go.Funnel(\n    y = data_funnel[\"event_type\"],\n    x = data_funnel[\"# events\"],\n    customdata=data_funnel[\"percent\"],\n    texttemplate= \"<b>%{label}: </B>%{value:.2s}\"+\"<br><b>% of Total:</b> %{customdata:.2f}%\",\n    textposition='inside',\n    marker = {\"color\": [\"lightyellow\", \"lightsalmon\", \"tan\"]}\n    ))\nfig.update_yaxes(visible=False)\nfig.update_layout(template='simple_white',     \n                  title={'xanchor': 'center',\n                         'yanchor': 'top',        \n                         'y':0.9,\n                         'x':0.5,\n                         'text':\"Customer Funnel for Purchase Journey\"})\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T07:28:44.720042Z","iopub.execute_input":"2021-07-08T07:28:44.720497Z","iopub.status.idle":"2021-07-08T07:28:45.699322Z","shell.execute_reply.started":"2021-07-08T07:28:44.720452Z","shell.execute_reply":"2021-07-08T07:28:45.698299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.Hourly Website Traffic<a id=\"12\"></a>","metadata":{}},{"cell_type":"markdown","source":"###  A. Data Prep<a id=\"13\"></a>","metadata":{}},{"cell_type":"code","source":"datahour=data.groupby(['hours','weeknum'],as_index=False)['price'].count()\ndatahour.columns=['hours','weeknum','price']","metadata":{"execution":{"iopub.status.busy":"2021-07-08T07:28:45.700603Z","iopub.execute_input":"2021-07-08T07:28:45.700913Z","iopub.status.idle":"2021-07-08T07:28:46.336564Z","shell.execute_reply.started":"2021-07-08T07:28:45.700883Z","shell.execute_reply":"2021-07-08T07:28:46.335549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### B. Hourly Website Traffic Visualisation<a id=\"14\"></a>","metadata":{}},{"cell_type":"code","source":"#Visualisation\nfig = px.area(datahour, x='hours', y=\"price\",color='weeknum')\nfig.update_layout(template='simple_white',     \n                title={'xanchor': 'center',\n                         'yanchor': 'top',        \n                         'y':0.9,\n                         'x':0.5,\n                         'text':\"Customer's Hourly Website Views\"},\n                xaxis = dict(\n                    title_text='hours',\n                    tickmode = 'linear',\n                    tick0 = 0,\n                    dtick = 2),\n                 yaxis = dict(\n                    title_text='Visitors'))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T07:28:46.337991Z","iopub.execute_input":"2021-07-08T07:28:46.338395Z","iopub.status.idle":"2021-07-08T07:28:46.616029Z","shell.execute_reply.started":"2021-07-08T07:28:46.338341Z","shell.execute_reply":"2021-07-08T07:28:46.615155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see from the above graph, there are usually two peaks in a day which happens around 10 AM to 2PM and then again the peak starts from 6PM to 8PM. Knowing the marketing spent through out the day, and conversion rate arouund these hours, we can target campaigns (especially conversion campaigns) to run specifically targeting highly converting hours.\n\nThis peak hours generally translates to lunch break, and post work. Thus having high traffic at these hours makes sense.","metadata":{}},{"cell_type":"markdown","source":"## 4.Daily Sales,Ticket Size and Number of Orders<a id=\"15\"></a>","metadata":{}},{"cell_type":"markdown","source":"###  A. Data Prep<a id=\"16\"></a>","metadata":{}},{"cell_type":"code","source":"datadate=data[data['event_type']=='purchase'].groupby(['date'],as_index=False)['price'].sum()\ndatadateh=data[data['event_type']=='purchase'].groupby(['date'],as_index=False)['price'].count()\ndatadateh['avg_ticket']=datadate['price']/datadateh['price']\ndatadate.columns=['date','price']","metadata":{"execution":{"iopub.status.busy":"2021-07-08T07:28:46.617272Z","iopub.execute_input":"2021-07-08T07:28:46.61755Z","iopub.status.idle":"2021-07-08T07:28:48.140697Z","shell.execute_reply.started":"2021-07-08T07:28:46.617523Z","shell.execute_reply":"2021-07-08T07:28:48.139536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### B. Visualising Daily sales,Ticket Size and # of Orders<a id=\"17\"></a>","metadata":{}},{"cell_type":"code","source":"#Visualisation\nfig = make_subplots(\n    rows=2, cols=1,\n    column_widths=[1.0],\n    row_heights=[0.5, 0.5],\n    specs=[[{\"type\": \"Bar\"}],\n           [{\"type\": \"Scatter\"}]])\n\nfig.add_trace(go.Bar(x=datadate['date'], y=datadate[\"price\"],name='Sales'),\n             row=1,col=1)\nfig.add_trace(go.Scatter(x=datadateh['date'], y=datadateh['price'],\n                    mode='lines+markers',\n                    name='No of Purchases'),\n              row=1,col=1)\nfig.add_trace(go.Scatter(x=datadateh['date'], y=datadateh['avg_ticket'],\n                    mode='lines+markers',\n                    name='Avg Ticket Size'),\n              row=2,col=1)\nfig.update_layout(template='simple_white',     \n                title={'xanchor': 'center',\n                         'yanchor': 'top',        \n                         'y':0.9,\n                         'x':0.5,\n                         'text':\"Daily sales\"}\n                 )\nfig.update_yaxes(title_text='Sales/No of Purchases',ticks=\"inside\", row=1)\nfig.update_yaxes(title_text='Avg Ticket Size',ticks=\"inside\", row=2)\nfig.update_xaxes(title_text='Date',ticks=\"inside\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T07:28:48.142086Z","iopub.execute_input":"2021-07-08T07:28:48.142442Z","iopub.status.idle":"2021-07-08T07:28:48.234043Z","shell.execute_reply.started":"2021-07-08T07:28:48.142409Z","shell.execute_reply":"2021-07-08T07:28:48.233012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see from above graph, there are two peaks when sales were high. This happened on Jan 27th and Jan 28th,2020. This might be due to some specific promotional campaigns.The number of sales is also high on these days but the average ticket size is not that low by which we can infer that,even if there is any promotional campaigns running on Jan 27 and Jan 28, the discount is not that low affecting the average ticket size.","metadata":{}},{"cell_type":"markdown","source":"## 5.Hourly sales During Jan - Heatmap Vs Countour<a id=\"18\"></a>","metadata":{}},{"cell_type":"markdown","source":"###  A. Data Prep<a id=\"19\"></a>","metadata":{}},{"cell_type":"code","source":"#preprare data for further Visualisation\ndatadatehour=data[data['event_type']=='purchase'].groupby(['date','hours'],as_index=False)['price'].sum()\ndatadatehour.columns=['date','hours','price']\ndatadatehour['hours']=datadatehour['hours'].astype(str)\ndatadatehour['date']=datadatehour['date'].astype(str)","metadata":{"execution":{"iopub.status.busy":"2021-07-08T07:28:48.235347Z","iopub.execute_input":"2021-07-08T07:28:48.235645Z","iopub.status.idle":"2021-07-08T07:28:49.009387Z","shell.execute_reply.started":"2021-07-08T07:28:48.235614Z","shell.execute_reply":"2021-07-08T07:28:49.008084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### B. Data Visualisation - Daily Sales (by Hour)<a id=\"20\"></a>","metadata":{}},{"cell_type":"code","source":"fig = make_subplots(\n    rows=2, cols=1,\n    column_widths=[1.0],\n    row_heights=[0.5, 0.5],\n    specs=[[{\"type\": \"histogram2d\"}],\n           [{\"type\": \"histogram2dcontour\"}]])\nfig.add_trace(\n    go.Histogram2d(\n        x = datadatehour[\"date\"],\n        y = datadatehour[\"hours\"],\n        z = datadatehour[\"price\"],\n        colorbar=dict(len=0.5, y=0.8,title=\"Overall Sales\"),\n        histfunc = \"sum\",\n        colorscale = \"cividis\",\n        nbinsx = 31,\n        nbinsy=24),\n    row=1,col=1)\nfig.add_trace(\n    go.Histogram2dContour(\n        x = datadatehour[\"date\"],\n        y = datadatehour[\"hours\"],\n        z = datadatehour[\"price\"],\n        colorbar=dict(len=0.5, y=0.25,title=\"Overall Sales\",tickmode=\"array\",\n        tickvals=[20000,40000,60000,80000,100000,120000],),\n        histfunc = \"sum\",\n        showlegend=False,\n        colorscale = \"cividis\",\n        contours = dict(\n            showlabels = True),\n        nbinsx=31,\n        nbinsy=24),\n    row=2,col=1)\n\n\nfig.update_layout(\n    template=\"simple_white\",\n    margin=dict(r=50, t=50, b=50, l=50),\n    height=600,\n    showlegend=False,\n    title={'xanchor': 'center',\n           'yanchor': 'top',        \n            'y':1,\n            'x':0.5,\n           'text':\"Heatmap vs Contourplot</br></br></br>Hourly sales during Jan\"}\n)\nfig.update_yaxes(title_text='Hours',ticks=\"inside\")\nfig.update_xaxes(title_text='Date',ticks=\"inside\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T07:28:49.010795Z","iopub.execute_input":"2021-07-08T07:28:49.011109Z","iopub.status.idle":"2021-07-08T07:28:49.170728Z","shell.execute_reply.started":"2021-07-08T07:28:49.011078Z","shell.execute_reply":"2021-07-08T07:28:49.169924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can confirm the same high volume of sales on Jan 27th and 28th in above countour or heatmap as well. From above visualisation, it is also clear that generally the sales happen around 9AM to 1PM. But in last week, there are sales happening in the evening as well.","metadata":{}},{"cell_type":"markdown","source":"### C. Data Visualisation - Hourly Sales (by Week)<a id=\"21\"></a>","metadata":{}},{"cell_type":"code","source":"fig = make_subplots(\n    rows=2, cols=1,\n    column_widths=[1.0],\n    row_heights=[0.5, 0.5],\n    specs=[[{\"type\": \"histogram2d\"}],\n           [{\"type\": \"histogram2dcontour\"}]])\nfig.add_trace(\n    go.Histogram2d(\n        x = datahour[\"hours\"],\n        y = datahour[\"weeknum\"],\n        z = datahour[\"price\"],\n        colorbar=dict(len=0.5, y=0.8,title=\"Overall Sales\"),\n        histfunc = \"sum\",\n        colorscale='cividis',\n        nbinsx = 31,\n        nbinsy=7\n    ),\n    row=1,col=1)\nfig.add_trace(\n    go.Histogram2dContour(\n        x = datahour[\"hours\"],\n        y = datahour[\"weeknum\"],\n        z = datahour[\"price\"],\n        colorbar=dict(len=0.5, y=0.20,title=\"Overall Sales\",tickmode=\"array\",\n        tickvals=[20000,40000,60000],\n                     ),\n        histfunc = \"sum\",\n        showlegend=False,\n        colorscale='cividis',\n        contours = dict(\n            showlabels = True),\n        nbinsx=31,\n        nbinsy=7\n    ),\n    row=2,col=1)\n\n\nfig.update_layout(\n    template=\"simple_white\",\n    margin=dict(r=50, t=50, b=50, l=50),\n    height=600,\n    showlegend=False,\n    title={'xanchor': 'center',\n           'yanchor': 'top',        \n            'y':0.999,\n            'x':0.5,\n           'text':\"Heatmap vs Contourplot</br></br></br>Hourly sales by weekdays\"}\n)\nfig.update_yaxes(title_text='Weekdays',ticks=\"inside\")\nfig.update_xaxes(title_text='Hours',ticks=\"inside\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T07:28:49.171968Z","iopub.execute_input":"2021-07-08T07:28:49.17229Z","iopub.status.idle":"2021-07-08T07:28:49.259779Z","shell.execute_reply.started":"2021-07-08T07:28:49.172259Z","shell.execute_reply":"2021-07-08T07:28:49.258785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When we combine sales weekwise and visualise, as we already predicted sales happen around 9AM to 2PM in the morning and in evening from 4PM To 8PM (mmajorly from 6PM to 8PM)\n\nWhen combining weekly sales, majority of sales happened in week3 and week4 ( because in January 2020, there are 4 days in week1 and 6days in week5 and rest of the week has 7 days. If we compensate the one missing day in week5, we will have higher sales in last week as well.","metadata":{}},{"cell_type":"markdown","source":"## 6.Summary Statistics for # of Orders and Ticket size <a id=\"22\"></a>","metadata":{}},{"cell_type":"markdown","source":"###  A. Data Prep<a id=\"23\"></a>","metadata":{}},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T07:28:49.261345Z","iopub.execute_input":"2021-07-08T07:28:49.261775Z","iopub.status.idle":"2021-07-08T07:28:49.285777Z","shell.execute_reply.started":"2021-07-08T07:28:49.26173Z","shell.execute_reply":"2021-07-08T07:28:49.284627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to find the average ticket size, averge number of purchases by user, lets group the data by user_id.","metadata":{}},{"cell_type":"code","source":"#grouping based on user_id,date,hours,weekday,weeknum\ndata_user=data[data['event_type']=='purchase'].groupby(['user_id','date','hours','weekday','weeknum']).agg({'price':['sum','count']}).reset_index()\n\n#converting columns from multi index to single index\ndata_user.columns=data_user.columns.to_flat_index()\ndata_user=data_user.rename(columns={('price', 'sum'):'purchased_value',('price', 'count'):'no_of_purchases',('user_id',''):'user_id',\n                          ('date',''):'date',('hours',''):'hours',('weekday',''):'weekday',('weeknum',''):'weeknum'})\n\n#checking whether columns are updated correctly\ndata_user.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T07:28:49.287619Z","iopub.execute_input":"2021-07-08T07:28:49.288079Z","iopub.status.idle":"2021-07-08T07:28:50.270766Z","shell.execute_reply.started":"2021-07-08T07:28:49.288031Z","shell.execute_reply":"2021-07-08T07:28:50.26999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### B. Visualising summary statistics<a id=\"24\"></a>","metadata":{}},{"cell_type":"code","source":"fig = make_subplots(\n    rows=1, cols=2,\n    column_widths=[0.5, 0.5],\n    row_heights=[1.0],\n    specs=[[{\"type\": \"Violin\"},\n           {\"type\": \"Violin\"}]])\nfig = fig.add_trace(go.Violin(y=data_user['no_of_purchases'],\n                            name='No of Purchases',\n                            box_visible=True,\n                            meanline_visible=True),\n                       row=1,col=1)\nfig = fig.add_trace(go.Violin(y=data_user['purchased_value'],\n                            name='Ticked Size',\n                            box_visible=True,\n                            meanline_visible=True),\n                       row=1,col=2)\nfig.update_layout(\n    template=\"simple_white\",\n    margin=dict(r=50, t=50, b=50, l=50),\n    height=600,\n    showlegend=False,\n    yaxis_title=\"Count\",\n    title={'xanchor': 'center',\n           'yanchor': 'top',        \n            'y':0.95,\n            'x':0.5,\n           'text':\"Summary Statistics for Number of Purchases and Ticket size of purchases\"}\n)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T07:28:50.272017Z","iopub.execute_input":"2021-07-08T07:28:50.272543Z","iopub.status.idle":"2021-07-08T07:28:50.421033Z","shell.execute_reply.started":"2021-07-08T07:28:50.272485Z","shell.execute_reply":"2021-07-08T07:28:50.420209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see from above graph, on average 8 products have been purchased where as 50% of people have purchased 6 products in the month of January. 25%(since INR Q3 is 10) of people have atleast purchased 10 products. Similarly, on average, the ticket size is 40.7 𝑎𝑛𝑑50 . 25%(since INR Q3 is 10) of people have spent minimum of 50.1$ as ticket size.","metadata":{}},{"cell_type":"markdown","source":"\n***This notebook I have created to practice Explarotary data analysis and to practise some data visualisation. Feel free to share your feedbacks with me. If you like my work,Please upvote. This will help me be motivated***","metadata":{}}]}