{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center\">Pima Diabetes EDA for Inference</p>\nThe main motive of this notebook is to educate learners that we don't require **Machine Learning** always.  \nMany problem statemets can be solved easily with **inference** only.  \nChoosing the right plot is what matters.  ","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center\">Import Data and Libraries</p>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center\">Preprocessing</p>\nThe describe function gives and idea of values in the dataset.  \nAs you see there are many outliers.  \n\nFor example 0 blood pressure is technically dead.  \nAlso number of pregnancies have been limited to 6 as 17 is highly unlikely ","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# changing irrelevant values\ndf.loc[ df.Pregnancies > 6, 'Pregnancies' ] = 6\ndf.loc[ df.Glucose < 70 , 'Glucose' ] = 70     \ndf.loc[ df.BloodPressure < 60 , 'BloodPressure' ] = 60 \ndf.loc[ df.BMI < 18 , 'BMI' ] = 18 \ndf.loc[ df.BMI > 40 , 'BMI' ] = 40\ndf.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center\">Preprocessing</p>","metadata":{}},{"cell_type":"code","source":"plt.title('Ratio of Healthy and Diabetic Patients in Dataset')\nplt.pie(df['Outcome'].value_counts(),autopct='%.2f')\nplt.legend(['Healthy','Diabetic'],)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot(s):\n    sns.kdeplot(df.loc[ df.Outcome == 0, s] )\n    sns.kdeplot(df.loc[ df.Outcome == 1, s] )    \n    plt.legend(['Healthy','Diabetic'])\n    plt.ylabel('Number of Patients')\n    plt.yticks([])\n    plt.show()    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in df.columns[:-1]:\n    plot(i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The inference here is just for undersatnding purposes\nThe kdeplot the relative number of people corresponding to the column.  \nThe intersection of the two lines can vaguely estimate the chances of being diabetic.  \n\nFor example, numer of people with glucose level above 125 are more in diabetic category than healthy.  \nHowever no such conclusive result can be drawn from Skin thickness.  ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.heatmap(df.corr(),annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### From this plot we know that Glucose has highest correlation with being diabetic followed by BMI and AGE","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center\">Test Hypothesis with ML</p>","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import *\nfrom sklearn.tree import *\nfrom sklearn.metrics import *","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.drop('Outcome',axis=1).values\ny = df['Outcome'].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=42,test_size=0.2)\n\nclf = DecisionTreeClassifier()\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint(\"F1 Score = \",f1_score(y_true = y_test , y_pred = y_pred))\nprint(\"Accuracy = \",accuracy_score(y_true = y_test , y_pred = y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For a sample so small the F1 score and accuracy are relatively good.  \nLets plot the series of decisions the algorithm took to classify","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplot_tree(clf,feature_names=df.drop('Outcome',axis=1).columns,filled=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lets observe it carefullly","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplot_tree(clf,feature_names=df.drop('Outcome',axis=1).columns,max_depth=2,filled=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you see the first decision is taken on glucose level 127.5,  which is roughly the intersecton of the lines in the plot.  \nSimilar series of decisions are taken to categorise people. Observe BMI and Age intersection which are also close to values estimated by Decision tree.  \n\nVisualizations are always helpful in undersatnding the data.  \nThe score can be improved with more preprocesing and so can the visualisations.  \n\nHappy Learning ahead :)","metadata":{}}]}