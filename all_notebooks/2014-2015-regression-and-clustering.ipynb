{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install seaborn\n!pip install dmba\n\n%matplotlib inline\n\n\nfrom pathlib import Path\n\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.metrics import pairwise\nfrom scipy.cluster.hierarchy import dendrogram, linkage, fcluster\nimport scipy.cluster.hierarchy as shc\nfrom sklearn.cluster import KMeans\nimport matplotlib.pylab as plt\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas.plotting import parallel_coordinates\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, LassoCV, BayesianRidge\nimport statsmodels.formula.api as sm\n\n\nfrom dmba import regressionSummary, exhaustive_search\nfrom dmba import backward_elimination, forward_selection, stepwise_selection\nfrom dmba import adjusted_r2_score, AIC_score, BIC_score\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom dmba import classificationSummary\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"details_df = pd.read_csv('/kaggle/input/nba-players-stats-20142015/players_stats.csv')\ndetails_df.set_index('Name', inplace=True)\ndetails_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stats_df = details_df[['EFF', 'Games Played', 'MIN', 'PTS', 'FGM', 'FGA', 'FG%', 'FTM', 'FTA', 'FT%', \n                       '3PM', '3PA', '3P%',\n                       'OREB', 'DREB', 'REB', 'AST',\n                       'STL', 'BLK', 'TOV', 'PF']]\nstats_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stats_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The df.describe() above presents the descriptive analysis of the whole NBA season. Out of the 490 records, this is the descriptive analysis focusing on Efficiency (EFF):\nMean: 564.33\nStd: 464.43\nMin: -3\nMax: 2202\nThe output gave us an acceptance range of Efficiency score, guiding us in recognizing a strong contender versus a weaker one. It also gave us indicators on where the player stands based on other stats compared to others (total points, rebounds, steals, to name a few stats recognizing skill level). This is a great reference to compare players in the league.\nIt's interesting to see the average points scored per player every season, it give each of them a quick reflection on where they stand in the league."},{"metadata":{"trusted":true},"cell_type":"code","source":"stats_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"FGA/FGM, FTM/FTA, and 3PM/3PA, have high correlation themselves and may have a difficult time distinguishing their effects on Efficiency. Yes, multicollinearity apperas in this dataset but does not affect the accuracy of the model since these correlation doesn't mean causation.\n\nThe dataset used were mostly numerical values, dummies are not needed. The categorical values were dropped since they did not pertain to our approach."},{"metadata":{"trusted":true},"cell_type":"code","source":"stats_df.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yes, need to normalize the data first to get a cleaner result. Normalizing will give us a common scale without a large difference in ranges. Not normalizing first will result in big distances because some columns have 1000's (eg. MIN) in values while some (eg. OREB) have only 10's.\nBelow is new dataframe k_stats_df then normalized it. The variables used on k_stats_df dataframe to run clustering analysis were chosen based on factors that affect Efficiency rating. These variables are shown below. Below we also changed the data type to float64, before normalizing it.[](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"k_stats_df = details_df[['EFF', 'Games Played', 'MIN', 'PTS', 'FGM', 'FGA', 'FTM', 'FTA', '3PM', '3PA',\n                       'OREB', 'DREB', 'REB', 'AST',\n                       'STL', 'BLK', 'TOV', 'PF']]\nk_stats_df = k_stats_df.apply(lambda x: x.astype('float64'))\nk_stats_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k_stats_df_norm = k_stats_df.apply(preprocessing.scale, axis=0)\nk_stats_df_norm.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pairwise Distance using Euclidean metric - it measures the distance between 2 players."},{"metadata":{"trusted":true},"cell_type":"code","source":"d = pairwise.pairwise_distances(k_stats_df_norm, metric='euclidean')\npd.DataFrame(d, columns=k_stats_df_norm.index, index=k_stats_df_norm.index).head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"K Clustering, we chose K=15. There are 30 teams in the NBA, and we expect 15 clusters can give us a good representation of the players. Even though we had 490 players, 15 clusters should give us a nice spread of members per cluster."},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters=15, random_state=1).fit(k_stats_df_norm)\nmemb = pd.Series(kmeans.labels_, index=k_stats_df_norm.index)\nfor key, item in memb.groupby(memb):\n    print(key, ': ', ', '.join(item.index))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cluster 11, containing only 2 players (James Harden and Russell Westbrook), scored high on 10 out of 18 columns.\nAlthough this could be considered an outlier, we chose to keep it since the dataframe is based on player's performance. We cannot treat great players (or bad players) as outliers, as mentioned before.\nAlthough we didn't mind looking over the players listed in clusters, we found the centroids measurement much easier to analyze. Higher values per cluster/column meant that those group of players score higher on that specific stat.\nThis is especially important for NBA organizations to get a quick look on a smaller list of players who can contribute a particular skill on the team. They can analyze the clusters list below to find which cluster can offer the most improvement in their team, and refer to the list above for players on that cluster."},{"metadata":{"trusted":true},"cell_type":"code","source":"centroids = pd.DataFrame(kmeans.cluster_centers_, columns=k_stats_df_norm.columns)\npd.set_option('precision', 3)\nprint(centroids)\npd.set_option('precision', 6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A good data set should be clustered together; it indicates strong cohesiveness.\nAnd to measure cohesion, we used Within Cluster Sum of Square distances"},{"metadata":{"trusted":true},"cell_type":"code","source":"withinClusterSS = [0] * 15\nclusterCount = [0] * 15\nfor cluster, distance in zip(kmeans.labels_, kmeans.transform(k_stats_df_norm)):\n    withinClusterSS[cluster] += distance[cluster]**2\n    clusterCount[cluster] += 1\nfor cluster, withClustSS in enumerate(withinClusterSS):\n    print('Cluster {} ({} members): {:5.2f} within cluster'.format(cluster, \n        clusterCount[cluster], withinClusterSS[cluster]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"centroids['cluster'] = ['Cluster {}'.format(i) for i in centroids.index]\n\nfig = plt.figure(figsize=(20,30))\nfig.subplots_adjust(right=3)\nax = parallel_coordinates(centroids, class_column='cluster', colormap='Dark2', linewidth=5)\nplt.legend(loc='upper right', bbox_to_anchor=(0.95, 0.5))\nplt.xlim(-0.5,7.5)\ncentroids","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above (centroids), is the average value per cluster per column. Since we are focusing on EFF, Cluster 7 was second in highest EFF at 2.492 (after Harden and Westbrook's 3.158). Other notable clusters were Cluster 4- high Games Played (0.978), Cluster 6- high 3 Points Made (2.271), and Cluster 7- high Rebounds (3.152)"},{"metadata":{"trusted":true},"cell_type":"code","source":"inertia = []\nfor n_clusters in range(1, 8):\n    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(k_stats_df_norm)\n    inertia.append(kmeans.inertia_ / n_clusters)\ninertias = pd.DataFrame({'n_clusters': range(1, 8), 'inertia': inertia})\nax = inertias.plot(x='n_clusters', y='inertia')\nplt.xlabel('Number of clusters(k)')\nplt.ylabel('Average Within-Cluster Squared Distances')\nplt.ylim((0, 1.1 * inertias.inertia.max()))\nax.legend().set_visible(False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The elbow chart suggests that the k=4 is the optimum amount of clusters.\nAfter analyzing this problem for some time, we came to conclusion that elbow chart or KMeans clustering are not the best method in calculating our dataset. Although it's fast at partitioning for clustering, it doesn't do a great job scaling a big data set.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"scatter_df = details_df[['PTS', 'FGM', 'FGA', 'FTM', 'FTA', '3PM', '3PA',\n                         'REB', 'AST', 'STL', 'BLK', 'TOV',\n                         'EFF','MIN']]\nscatter_df.info()\n#review the data once again","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\n\ncluster = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')\ncluster.fit_predict(scatter_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 7))\nplt.title(\"Player Dendrograms\")\ndend = shc.dendrogram(shc.linkage(scatter_df, method='ward'))\n#dendrogram of players","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eff_min = scatter_df.iloc[:, 12:14].values\n\nplt.figure(figsize=(10, 7))\nplt.scatter(eff_min[:,0], eff_min[:,1], c=cluster.labels_, cmap='rainbow')\nplt.title('Minutes vs Efficiency')\nplt.xlabel('Minutes')\nplt.ylabel('Efficiency')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above is a scatter of EFF vs MIN. We assumed the more players stay on the floor the more effective they can be, and this diagram confirms that.\nBelow is a representation of Assists and Steals. Position such as Point Guard and Shooting Guard are known to handle the ball more and quick on the hands"},{"metadata":{"trusted":true},"cell_type":"code","source":"ast_stl = scatter_df.iloc[:, 10:18].values\nplt.figure(figsize=(10, 7))\nplt.scatter(ast_stl[:,0], ast_stl[:,1], c=cluster.labels_, cmap='rainbow')\nplt.title('Assists vs Steals')\nplt.xlabel('Assist')\nplt.ylabel('Steal')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"blk_3pm = scatter_df[['BLK', '3PM']].values\nplt.figure(figsize=(10, 7))\nplt.scatter(blk_3pm[:,0], blk_3pm[:,1], c=cluster.labels_, cmap='rainbow')\nplt.title('Block vs 3 Point Made')\nplt.xlabel('3 Point Made')\nplt.ylabel('Blocks')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lastly, above we have Blocks and 3 Point Made. We were expecting to see a better scatter diagram, but this was the result. We assumed that Center players are tall and don't make 3 point shots often.\n\nBelow is a heirarchical clustering dendrogram using \"Single Linkage\", which we found less meaningful. Next code was dendrogram using \"Average Linkage\" which we found more useful"},{"metadata":{"trusted":true},"cell_type":"code","source":"Z = linkage(k_stats_df_norm, method='single')\n\nfig = plt.figure(figsize=(20, 6))\nfig.subplots_adjust(bottom=0.23)\nplt.title('NBA - Single Linkage Dendrogram')\nplt.xlabel('Player')\ndendrogram(Z, labels=k_stats_df_norm.index, color_threshold=2.75)\nplt.axhline(y=2.65, color='black', linewidth=0.5, linestyle='dashed')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Z = linkage(k_stats_df_norm, method='average')\n\nfig = plt.figure(figsize=(50, 12))\nfig.subplots_adjust(bottom=0.23)\nplt.title('NBA - Average Linkage Dendrogram')\nplt.xlabel('Player')\ndendrogram(Z, labels=k_stats_df_norm.index, color_threshold=3.6)\nplt.axhline(y=3.3, color='black', linewidth=0.5, linestyle='dashed')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above is a dendrogram of players and below is the list of players the algorithm split the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"memb = fcluster(linkage(k_stats_df_norm, 'average'), 17, criterion='maxclust')\nmemb = pd.Series(memb, index=k_stats_df_norm.index)\nfor key, item in memb.groupby(memb):\n    print(key, ': ', ', '.join(item.index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k_stats_df.index = ['{}: {}'.format(cluster, state) for cluster, state in zip(memb, k_stats_df_norm.index)]\nsns.clustermap(k_stats_df_norm, method='average', col_cluster=False,  cmap=\"mako_r\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above is a dendrogram with heatmap.\n\nThere is a large difference in number of players per cluster and that was against our expectations.\nWe found that KMeans clustering at k=15 was the best model. Besides Cluster 11 (Harden and Westbrook), results after partitioning made more sense than single/average linkage model. The label on clusters can be referred above on 10.1 Clustering.\nAlthough there are better visualizations than dendrograms, there is one lesson we can learn from this data. When estimating the player's salary, NBA organizations can take these clusters into consideration to avoid undervaluing or overvaluing potential recruits. Since players are paid for their skills and potential contribution, the players who stand alone in a cluster can set the bar for their salary, while those with members with, let's say, 5 players, creates a range NBA organizations can refer to.\n\nWe came to a conclusion that KMeans cluster analysis is not the best model to approach this data. Although it tries to calculate the best partitioning of the given data quickly, the drawback is that it struggles to find clusters with stronger cohesion. We searched online for various ways to approach this data (eg. visualization), tried new codes multiple times and got errors after errors, and only when we finally get the code to work is when we realized that the output was meaningless (scatter plots and dendrograms). We also realized that maybe other neighbor classifier or other clustering methods are more suitable on our dataset. Unfortunately time would not allow us to explore other clustering methods."},{"metadata":{},"cell_type":"markdown","source":"**Regression Modeling**"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors = ['MIN', 'PTS', 'FG%', 'FT%', '3P%',\n                       'REB', 'AST',\n                       'STL', 'BLK', 'TOV']\noutcome = 'EFF'\n\nX = stats_df[predictors]\ny = stats_df[outcome]\nprint(X.shape)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#partition data\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.5, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"car_lm = LinearRegression()\ncar_lm.fit(train_X, train_y)\n#train the model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Shown below, the coefficients are being printed, as well as the performance measures, which include the mean error, root mean squaed error, and mean absolute error.\n\nprint('intercept ', car_lm.intercept_)\nprint(pd.DataFrame({'Predictor': X.columns, 'coefficient': car_lm.coef_}))\n\nregressionSummary(train_y, car_lm.predict(train_X))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the code below, we are using the training set to test the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_y = car_lm.predict(train_X)\n\nprint('adjusted r2 : ', adjusted_r2_score(train_y, pred_y, car_lm))\nprint('AIC : ', AIC_score(train_y, pred_y, car_lm))\nprint('BIC : ', BIC_score(train_y, pred_y, car_lm))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are using the validation set to test the model. In addition to that, the code is making predictions on a new set. Shown below are the first 20 predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"car_lm_pred = car_lm.predict(valid_X)\n\nresult = pd.DataFrame({'Predicted': car_lm_pred, 'Actual': valid_y,\n                       'Residual': valid_y - car_lm_pred})\n\nprint(result.head(20))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Forward Selection:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(variables):\n    if len(variables) == 0:\n        return None\n    model = LinearRegression()\n    model.fit(train_X[variables], train_y)\n    return model\n\ndef score_model(model, variables):\n    if len(variables) == 0:\n        return AIC_score(train_y, [train_y.mean()] * len(train_y), model, df=1)\n    return AIC_score(train_y, model.predict(train_X[variables]), model)\n\nbest_model, best_variables = forward_selection(train_X.columns, train_model, score_model, verbose=True)\n\nprint(best_variables)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressionSummary(valid_y, best_model.predict(valid_X[best_variables]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below, is a histogram of residuals."},{"metadata":{"trusted":true},"cell_type":"code","source":"car_lm_pred = car_lm.predict(valid_X)\nall_residuals = valid_y - car_lm_pred\n\nprint(len(all_residuals[(all_residuals > -50) & (all_residuals < 50)]) / len(all_residuals))\n\nax = pd.DataFrame({'Residuals': all_residuals}).hist(bins=25)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As it can be seen, 87% of the efficiency scores are within 50 of the actual efficiency score. This means that less than 13% are more off than 50 points. Based on the residuals, there are very few numbers that are very large, specifcally around the values of -150 and 150. Overall, a majority are between -100 and 100, which is not a suprise. In addition to that, it is pretty symmetrical around 0, which means that there is not exactly underestimating or overestimating. In a way, it is pretty well balanced.\n\nListed below, is both the predictors and the outcome. In addition to that, the MLPClassifier is created with one hidden layer of a size of 3 nodes."},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors = ['MIN', 'PTS', 'FG%', 'FT%', '3P%',\n                       'REB', 'AST',\n                       'STL', 'BLK', 'TOV']\noutcome = 'EFF'\n\nA = stats_df[predictors]\nB = stats_df[outcome]\nclasses = sorted(y.unique())\n\nclf = MLPClassifier(hidden_layer_sizes=(3), activation='logistic', solver='lbfgs', random_state=1)\nclf.fit(A, B)\nclf.predict(A)\n\nprint('Intercepts')\nprint(clf.intercepts_)\n\nprint('Weights')\nprint(clf.coefs_)\n\nprint(pd.concat([\n    stats_df,\n    pd.DataFrame(clf.predict_proba(X), columns=classes)\n], axis=1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The code below partitions the data. In addition to that, the clf trains the neural network with only two hidden nodes. It also shows the training performaqnce, as well as the validation performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"A = stats_df[predictors]\nb = stats_df[outcome]\ntrain_A, valid_A, train_b, valid_b = train_test_split(A, b, test_size=0.4, random_state=1)\n\nscaler = StandardScaler()\ntrain_A = scaler.fit_transform(train_A)\nvalid_A = scaler.transform(valid_A)\n\n# train neural network with 2 hidden nodes\nclf = MLPClassifier(hidden_layer_sizes=(2), activation='logistic', solver='lbfgs',\n                    random_state=1)\nclf.fit(train_A, train_b.values)\n\nclassificationSummary(train_b, clf.predict(train_A))\n\nclassificationSummary(valid_b, clf.predict(valid_A))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below, is the performance measures being printed, such as mean error, root mean squared error, and mean absolute error.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"regressionSummary(train_b, car_lm.predict(train_A))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, (weights, intercepts) in enumerate(zip(clf.coefs_, clf.intercepts_)):\n    print('Hidden layer' if i == 0 else 'Output layer', '{0[0]} => {0[1]}'.format(weights.shape))\n    print(' Intercepts:\\n ', intercepts)\n    print(' Weights:')\n    for weight in weights:\n        print(' ', weight)\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below, is the confusion matrix being printed."},{"metadata":{"trusted":true},"cell_type":"code","source":"classificationSummary(b, clf.predict(A), class_names=classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressionSummary(train_b, car_lm.predict(train_A))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below, is a histogram of residuals."},{"metadata":{"trusted":true},"cell_type":"code","source":"car_lm_pred = car_lm.predict(valid_A)\nall_residuals = valid_b - car_lm_pred\n\nprint(len(all_residuals[(all_residuals > -50) & (all_residuals < 50)]) / len(all_residuals))\n\nax = pd.DataFrame({'Residuals': all_residuals}).hist(bins=25)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After running the regression analysis, the conclusions that can be drawn are that James Harden has the highest efficiency of 2202. Our prediction that the player with the highest efficiency would be either a point guard or a shooting guard was correct. James Harden is a shooting guard and his points were significantly higher than all the other players, which gave him a huge increase in efficiency. Vice versa, the player who had the lowest efficiency of -3 was Julius Randle. The highest effiency score of 2202 and the lowest efficiency score of -3 was an insanely large difference that was quite surprising."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}