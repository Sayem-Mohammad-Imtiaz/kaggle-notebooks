{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Code dependancies"},{"metadata":{"_uuid":"f80900ff-d042-4721-b23a-b7f113b7cadb","_cell_guid":"f98879dd-5ad9-47a7-a158-fe748ef2a725","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport numpy as np\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Import+Edit our dataset\n"},{"metadata":{},"cell_type":"markdown","source":"We will import the data, clean it up(get rid of links, tags, hashtags and so on, so that we're left with clean data.\nAfter that we'll split it into train and test, and we'll get some more information about it."},{"metadata":{"trusted":true},"cell_type":"code","source":"file_path = '/kaggle/input/the-social-dilemma-tweets/TheSocialDilemma.csv'\ndata = pd.read_csv(file_path)\ndata = data[['text', 'Sentiment']]\ndata.head()\n\ndef clean_text(text):\n  text = text.lower()\n  text = re.sub('\\[.*?\\]', '', text)\n  text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n  text = re.sub('\\n', '', text)\n  text = \" \".join(filter(lambda x:x[0]!=\"@\", text.split()))\n  return text\ndata['text'] = data['text'].apply(lambda x: clean_text(x))\n\nX = data['text']\ny = data['Sentiment'].map({'Negative':0, 'Neutral':1, 'Positive':2})\n\ntrain_size = int(len(data)*0.8)\nX_train, y_train = X[:train_size], y[:train_size]\nX_test, y_test = X[train_size:], y[train_size:]\nprint(\"X_train shape:\", X_train.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"X_test shape:\", X_test.shape)\nprint(\"y_test shape:\", y_test.shape)\n\nprint ('Length of text: {} characters'.format(len(X_train)))\n\nprint(\"Max tweet length:\", X.map(len).max())\nprint(\"Min tweet length:\", X.map(len).min())\nprint(\"Average tweet length:\", X.map(len).mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Chose a vocab size for our model, embedding dim(can be 16,32,64... play around with it), and then fit it to train data only(if we were to fit it on all data, then validation accuracy would not be accurate, as we'd already have vocabulary data from both train and test..).\nGet sequences of train and test separately, and then pad them to a fixed amount that we predefined."},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = 8000\nembedding_dim = 32\nmax_length = 90\ntokenizer = Tokenizer(vocab_size)\ntokenizer.fit_on_texts(X_train)\nword_index = tokenizer.word_index\n\ntrain_sequences = tokenizer.texts_to_sequences(X_train)\ntrain_padded = pad_sequences(train_sequences, maxlen=max_length, padding='pre', truncating='pre')\ntest_sequences = tokenizer.texts_to_sequences(X_test)\ntest_padded = pad_sequences(test_sequences, maxlen=max_length, padding='pre', truncating='pre')\n\nprint(\"Shape of train_padded:\", train_padded.shape)\nprint(\"Shape of test_padded:\", test_padded.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{},"cell_type":"markdown","source":"I've built a simple model with a couple of layers, nothing too crazy. You can see that I've also decided to use a learning rate that exponentially drops, which can also be helpful when training our models for a longer time."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential([\n                    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n                    tf.keras.layers.LSTM(100),\n                    tf.keras.layers.Dense(max_length/2, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n                    tf.keras.layers.Dropout(0.4),\n                    tf.keras.layers.Dense(3, activation='softmax')\n])\nmodel.summary()\n\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    0.01,\n    decay_steps=10000,\n    decay_rate=0.95,\n    staircase=True\n)\n\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate = lr_schedule),\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nhistory = model.fit(train_padded, y_train, epochs=30, validation_data=(test_padded, y_test))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Test the data**"},{"metadata":{},"cell_type":"markdown","source":"We define get_encode function, that prepares any textual data we pass in to a format the model can take. Make no mistakes, the text sentence you pass over must be a list(so it must be in [] brackets)."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef get_encode(x):\n  #x = clean_text(x)\n  x = tokenizer.texts_to_sequences(x)\n  x = tf.keras.preprocessing.sequence.pad_sequences(x, maxlen=max_length, padding='pre', truncating='pre')\n  return x\n\ntest_comment = ['This movie depicted the current society issues so well, I loved it so much']\n\nseq = tokenizer.texts_to_sequences(test_comment)\npadded = pad_sequences(seq, maxlen=max_length, padding='pre', truncating='pre')\nprint(padded.shape)\ny_pred = model.predict(padded).round()\nprint(y_pred)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}