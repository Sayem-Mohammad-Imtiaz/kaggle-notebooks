{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport scipy.stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndavis_df = pd.read_csv(\"/kaggle/input/davis-data-set/Davis.csv\")\n\n#Get rid of impossible values\ndavis_df = davis_df[davis_df.height > 140].dropna()\n\n#Separate df to test and train saving proportions\ndavis_train, davis_test = train_test_split(davis_df, train_size=0.5, stratify=davis_df.sex)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot the data\nsns.scatterplot(data=davis_train, x='weight', y='height', hue='sex')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Different hists\noverall_weight_df = davis_df.weight\nmale_weight_df = davis_df[davis_df.sex == 'M'].weight\nfemale_weight_df = davis_df[davis_df.sex == 'F'].weight\n\noverall_height_df = davis_df.height\nmale_height_df = davis_df[davis_df.sex == 'M'].height\nfemale_height_df = davis_df[davis_df.sex == 'F'].height\n\noverall_weight_lie_df = davis_df.repwt - davis_df.weight\nmale_weight_lie_df = davis_df[davis_df.sex == 'M'].repwt - davis_df[davis_df.sex == 'M'].weight\nfemale_weight_lie_df = davis_df[davis_df.sex == 'F'].repwt -davis_df[davis_df.sex == 'F'].weight\n\noverall_height_lie_df = davis_df.repht - davis_df.height\nmale_height_lie_df = davis_df[davis_df.sex == 'M'].repht - davis_df[davis_df.sex == 'M'].height\nfemale_height_lie_df = davis_df[davis_df.sex == 'F'].repht -davis_df[davis_df.sex == 'F'].height\n\nfig, axes = plt.subplots(4, 3, figsize=(15,15))\n\naxes[0,0].set_title(\"Overall weight\")\naxes[0,1].set_title(\"Male weight\")\naxes[0,2].set_title(\"Female weight\")\n\naxes[1,0].set_title(\"Overall height\")\naxes[1,1].set_title(\"Male height\")\naxes[1,2].set_title(\"Female height\")\n\naxes[2,0].set_title(\"Overall repwt - weight\")\naxes[2,1].set_title(\"Male repwt - weight\")\naxes[2,2].set_title(\"Female repwt - weight\")\n\naxes[3,0].set_title(\"Overall repht - height\")\naxes[3,1].set_title(\"Male repht - height\")\naxes[3,2].set_title(\"Female repht - height\")\n\noverall_weight_df.hist(bins=30, ax=axes[0,0])\nmale_weight_df.hist(bins=30, ax=axes[0,1])\nfemale_weight_df.hist(bins=30, ax=axes[0,2])\n\noverall_height_df.hist(bins=30, ax=axes[1,0])\nmale_height_df.hist(bins=30, ax=axes[1,1])\nfemale_height_df.hist(bins=30, ax=axes[1,2])\n\noverall_weight_lie_df.hist(bins=15, ax=axes[2,0])\nmale_weight_lie_df.hist(bins=15, ax=axes[2,1])\nfemale_weight_lie_df.hist(bins=15, ax=axes[2,2])\n\noverall_height_lie_df.hist(bins=15, ax=axes[3,0])\nmale_height_lie_df.hist(bins=15, ax=axes[3,1])\nfemale_height_lie_df.hist(bins=15, ax=axes[3,2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class HumanGenerator:\n    def __init__(self):\n        self.p_female = None;\n        self.mean_height = {\n            \"female\": None,\n            \"male\": None\n        }\n        self.mean_weight = {\n            \"female\": None,\n            \"male\": None\n        }\n        self.covariance = {\n            \"female\": None,\n            \"male\": None\n        }\n    \n    def fit(self, X):\n        female_samples = X[X.sex == \"F\"]\n        male_samples = X[X.sex == \"M\"]\n        \n        self.p_female = len(female_samples)/len(X)\n        \n        self.mean_height[\"female\"] = np.mean(female_samples.height)\n        self.mean_height[\"male\"] = np.mean(male_samples.height)\n\n        self.mean_weight[\"female\"] = np.mean(female_samples.weight)\n        self.mean_weight[\"male\"] = np.mean(male_samples.weight)\n\n        self.covariance[\"female\"]  = np.cov(female_samples.weight, female_samples.height)\n        self.covariance[\"male\"]  = np.cov(male_samples.weight, male_samples.height)\n        \n    def generate_samples(self, n):\n        result = np.empty((n, 2))\n        result_sex = np.empty(n, dtype = object)\n        for i in range(n):\n            sex = np.random.choice([\"female\", \"male\"], p=[self.p_female, 1 - self.p_female])\n            if(sex == \"female\"):\n                result_sex[i] = \"F\"\n            else:\n                result_sex[i] = \"M\"\n            result[i] = np.random.multivariate_normal(mean=[self.mean_weight[sex], self.mean_height[sex]], cov=self.covariance[sex])\n        return pd.DataFrame(dict(sex = result_sex.ravel(), weight = result[:, 0].ravel(), height = result[:, 1].ravel()))\n    \n    def log_likehood(self, X):\n        female_samples = X[X.sex == \"F\"][[\"weight\", \"height\"]]\n        male_samples = X[X.sex == \"M\"][[\"weight\", \"height\"]]\n        \n        female_log_likehood = np.log(self.p_female *\n            scipy.stats.multivariate_normal.pdf(\n                female_samples, \n                mean=[self.mean_weight[\"female\"],self.mean_height[\"female\"]], \n                cov=self.covariance[\"female\"]))\n        male_log_likehood = np.log((1 - self.p_female) *\n            scipy.stats.multivariate_normal.pdf(\n                male_samples, \n                mean=[self.mean_weight[\"male\"],self.mean_height[\"male\"]], \n                cov=self.covariance[\"male\"]))\n        \n        return np.append(female_log_likehood, male_log_likehood)\n    \n    def mean_log_likehood(self, X):\n        return self.log_likehood(X).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Test generator\ngenerator = HumanGenerator()\ngenerator.fit(davis_train)\n\nnew_samples = generator.generate_samples(1000)\nprint(f\"Train: {generator.mean_log_likehood(davis_train)}\")\nprint(f\"Test: {generator.mean_log_likehood(davis_test)}\")\nprint(f\"Generated: {generator.mean_log_likehood(new_samples)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Scatter original data and generated one\nfig, axis = plt.subplots(1, 2, figsize=(15,3))\n\naxis[0].set_title(\"Original data\")\naxis[1].set_title(\"Generated data\")\n\nsns.scatterplot(data=davis_df, x='weight', y='height', hue='sex', ax=axis[0])\nsns.scatterplot(data=new_samples, x='weight', y='height', hue='sex', ax=axis[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Gradient descent\ny = davis_train['weight']\nx = davis_train['height'].to_numpy()\n\nw = 1\nb = 0\nalpha = 0.0001\nold_w = []\n\nmax_norm = 5\nfor t in range(10000):\n    l = ((1/len(x)) * np.sum((y - w * x - b)**2))\n    d_yh = (w * x + b - y)\n    \n    d_w = np.mean(d_yh * x)\n    d_b = np.mean(d_yh)\n    \n    norm = np.sqrt(d_w**2 + d_b**2)\n\n    if norm > max_norm:\n        reciprocal = norm / max_norm\n        d_w /= reciprocal\n        d_b /= reciprocal\n    w = w - alpha * d_w\n    b = b - alpha * d_b\n    \n    if t % 500 == 0:\n        old_w.append(np.array((w, b)))\n        \nprint(w, b)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Normalize data before gradient descent\ny_norm = (davis_train.weight - davis_train.weight.mean()) / davis_train.weight.std()\nx_norm = (davis_train.height - davis_train.height.mean()) / davis_train.height.std()\n\nw_norm = 1\nb_norm = 0\nold_w_norm = []\n\nmax_norm = 5\nfor t in range(10000):\n    l_norm = ((1/len(x_norm)) * np.sum((y_norm - w_norm * x_norm - b_norm)**2))\n    d_yh = (w_norm * x_norm + b_norm - y_norm)\n    \n    d_w = np.mean(d_yh * x_norm)\n    d_b = np.mean(d_yh)\n    \n    norm = np.sqrt(d_w**2 + d_b**2)\n\n    if norm > max_norm:\n        reciprocal = norm / max_norm\n        d_w /= reciprocal\n        d_b /= reciprocal\n    w_norm = w_norm - alpha * d_w\n    b_norm = b_norm - alpha * d_b\n    \n    if t % 500 == 0:\n        old_w_norm.append(np.array((w_norm, b_norm)))\n        \nw_norm = w_norm * davis_train.weight.std() / davis_train.height.std()\nb_norm = davis_train.weight.mean() + b_norm * davis_train.weight.std() - w_norm * davis_train.height.mean()\n\nprint(w_norm, b_norm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compare gradient descent with linear regression\nfrom sklearn.linear_model import LinearRegression\n\nlreg = LinearRegression()\n\nx_train = davis_train[[\"height\"]].values.astype(np.float)\ny_train = davis_train[\"weight\"].values.astype(np.float)\n\nlreg.fit(x_train, y_train)\n\nx_to_pred = np.linspace(x_train.min(), x_train.max(),30).reshape(-1,1)\nlreg_pred = lreg.predict(x_to_pred)\ngrad_pred = w * x_to_pred + b\ngrad_pred_norm = w_norm * x_to_pred + b_norm\n\nplt.scatter(davis_train[\"height\"], davis_train[\"weight\"])\nplt.plot(x_to_pred[:,0], lreg_pred,label=\"Linear regression\")\nplt.plot(x_to_pred[:,0], grad_pred, label=\"Gradient descent\")\nplt.plot(x_to_pred[:,0], grad_pred_norm, label=\"Gradient descent normalized\")\nplt.xlabel(\"Height\")\nplt.ylabel(\"Weight\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting gradient descent path\nall_ws = np.array(old_w)\nww,bb = np.mgrid[0:1.2:100j, -0.25:0.05:100j]\nL = []\nheights = davis_train['height'].values.astype(np.float64)\nweights = davis_train['weight'].values.astype(np.float64)\nfor w, b in zip(ww.ravel(),bb.ravel()):\n    L.append(1/len(heights) * np.sum((weights - w * heights - b)**2))\nL = np.array(L)\nplt.figure(figsize=(10, 8))\nplt.title(\"Contour Plot of Gradient Descent\")\nplt.xlabel(\"w\")\nplt.ylabel(\"b\")\nplt.contourf(ww,bb,L.reshape(ww.shape),levels=50)\n\nfor i in range(len(old_w) - 1):\n    plt.annotate('', xy=all_ws[i + 1, :], xytext=all_ws[i, :],\n                 arrowprops={'arrowstyle': '->', 'color': 'r', 'lw': 1},\n                 va='center', ha='center')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}