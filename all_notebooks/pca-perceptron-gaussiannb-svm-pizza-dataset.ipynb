{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Indice contenuti\n- [Analisi esplorativa del dataset](#Analisi-esplorativa-del-dataset)\n    - [Descrizione del dataset](#Descrizione-del-dataset)\n    - [Caricamento in memoria del dataset](#Caricamento-in-memoria-del-dataset)\n    - [Preprocessing](#Preprocessing)\n    - [Matrice di correlazione di Pearson](#Matrice-di-correlazione-di-Pearson)\n    - [Distribuzione dei dati per feature \"brand\"](#Distribuzione-dei-dati-per-feature-\"brand\")\n- [Grafici](#Grafici)\n    - [Boxplot](#Boxplot)\n    - [Jointplot](#Jointplot)\n    - [Pairplot](#Pairplot)\n- [PCA (Principal Component Analysis)](#PCA-(Principal-Component-Analysis))\n     - [Dichiarazione della feature target](#Dichiarazione-della-feature-target)\n     - [Standardizzazione dei dati](#Standardizzazione-dei-dati)\n     - [Split del dataset](#Split-del-dataset)\n     - [Applicazione della PCA](#Applicazione-della-PCA)\n- [Plot di Accuracy](#Plot-di-Accuracy)\n    - [Grid Search](#Grid-Search)\n    - [Random Search](#Random-Search)\n- [Percettrone](#Percettrone)\n- [Linear Discriminant Analysis](#Linear-Discriminant-Analysis)\n- [Regressione Lineare](#Regressione-Lineare)\n- [Regressione Polinomiale](#Regressione-Polinomiale)\n- [Regressione Logistica](#Regressione-Logistica)\n- [Regressione mediante Random Forest](#Regressione-mediante-Random-forest)\n- [Utilizzo di metodi regolarizzati per la regressione](#Utilizzo-di-metodi-regolarizzati-per-la-regressione)\n    - [Ridge Regression](#Ridge-Regression)\n    - [Lasso Regression](#Lasso-Regression)\n    - [Elastic Net](#Elastic-Net)\n- [Decision Tree](#Decision-Tree)\n- [Random Forest](#Random-Forest)\n- [K-nearest Neighbor](#K-nearest-Neighbor)\n- [Gaussian Naive Bayes](#Gaussian-Naive-Bayes)\n- [Support Vector Machine](#Support-Vector-Machine)\n- [Matrice di confusione per K-nn](#Matrice-di-confusione-per-K-nn)\n     \n<hr>"},{"metadata":{},"cell_type":"markdown","source":"## Analisi esplorativa del dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Import delle librerie richieste per il funzionamento\nimport itertools\nimport pandas as pd  \nimport numpy as np  \nimport matplotlib.pyplot as plt  \nimport seaborn as seabornInstance \nfrom sklearn import metrics\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Descrizione del dataset"},{"metadata":{},"cell_type":"markdown","source":"Il dataset _Pizza.csv_ contiene informazioni circa la quantità di componenti che devono essere rispettate per rendere la pizza gustosa.</br>In particolare, le features di input presenti sono le seguenti:\n- <b>id</b>: Campioni analizzati\n- <b>mois</b>: Quantità di acqua per 100 grammi nel campione\n- <b>prot</b>: Quantità di proteine per 100 grammi nel campione\n- <b>fat</b>: Quantità di grasso per 100 grammi nel campione\n- <b>ash</b>: Quantità di cenere per 100 grammi nel campione\n- <b>sodium</b>: Quantità di sodio per 100 grammi nel campione\n- <b>carb</b>: Quantità di carboidrati per 100 grammi nel campione\n- <b>cal</b>: Quantità di calorie per 100 grammi nel campione\n \nCome unica feature di output, invece, si ritrova la seguente feature:\n - <b>brand</b> Pizza brand (class label)"},{"metadata":{},"cell_type":"markdown","source":"### Caricamento in memoria del dataset"},{"metadata":{},"cell_type":"markdown","source":"Con il seguente comando si effettua il caricamento di quanto contenuto nel dataset _'Pizza.csv'_:"},{"metadata":{"trusted":false},"cell_type":"code","source":"dataset = pd.read_csv('../input/nutrient-analysis-of-pizzas/Pizza.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"Dalla semplice descrizione del dataset è emersa la necessità di effettuare attività di preprocessing sul dataset, tra cui:\n1. Rimozione del campo _id_ dal DataFrame\n2. Trasformazione della campo stringa _brand_ in una feature numerica\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"dataset.drop(['id'], axis = 1, inplace = True)\n\n#L'attributo brand da stringa diventa intero -> Categorico\ndataset.brand = pd.factorize(dataset.brand)[0] + 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Per condurre una prima fase di analisi esplorativa e comprendere la natura dei dati a disposizione, si stampano di seguito le prime cinque righe del dataset:"},{"metadata":{"trusted":false},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Si prosegue, dunque, alla stampa della lista dell'intestazione del DataFrame caricato:"},{"metadata":{"trusted":false},"cell_type":"code","source":"dataset.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Successivamente, al fine di comprendere le dimensioni (in termini di esempi e di features a disposizione), mediante apposito attributo si stampano il numero di righe e di colonne del DataFrame:"},{"metadata":{"trusted":false},"cell_type":"code","source":"dataset.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Per ottenere informazioni statistiche inerenti ciascuna feature a disposizione, mediante il metodo _describe()_ si è provveduto al calcolo delle seguenti informazioni:\n- <b>count</b>: conteggio del numero di esempi per la feature selezionata\n- <b>mean</b>: media aritmetica per la feature selezionata\n- <b>std</b>: deviazione standard per la feature selezionata\n- <b>min</b>: valore minimo presentato dagli esempi per la feature selezionata\n- <b>25%</b>: primo quartile calcolato sugli esempi per la feature selezionata\n- <b>50%</b>: secondo quartile calcolato sugli esempi per la feature selezionata\n- <b>75%</b>: terzo quartile calcolato sugli esempi per la feature selezionata\n- <b>max</b>: valore massimo presentato dagli esempi per la feature selezionata"},{"metadata":{"trusted":false},"cell_type":"code","source":"dataset.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Per controllare che effettivamente nel DataFrame non vi siano feature con valore mancante, il seguente metodo ritorna _False_ se tutti gli esempi per ogni feature sono completi, _True_ altrimenti:"},{"metadata":{"trusted":false},"cell_type":"code","source":"dataset.isnull().any().any() #Ritorna False se tutte gli esempi per ogni feature sono completi","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Al fine di ottenere una descrizione complessiva del Dataframe (e dunque del relativo dataset) caricato, mediante il metodo _info()_ si sono ottenute le seguenti informazioni:\n- <b>#</b>: numero di feature presente nel DataFrame\n- <b>Column</b>: intestazione delle features nel DataFrame\n- <b>Non-Null Count</b>: contatore di valori non nulli per ogni feature presente nel DataFrame\n- <b>Dtype</b>: tipo di dato memorizzato per ogni feature presente nel DataFrame"},{"metadata":{"trusted":false},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dopo aver controllato l'assenza di valori nulli, attraverso il metodo appositamente realizzato, sono stati analizzati il numero dei valori unici (non ripetuti) per ogni feature presente nel DataFrame:"},{"metadata":{"trusted":false},"cell_type":"code","source":"for col in dataset.columns.values:\n    print(f\"Il numero dei valori unici (non ripetuti) di {col} è: {dataset[col].nunique()}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Matrice di correlazione di Pearson"},{"metadata":{},"cell_type":"markdown","source":"Al fine di condurre una analisi completa, di seguito è riportata la matrice di correlazione di Pearson che prende in considerazione le differenti features presenti all'interno del DataFrame.\n\nI valori presenti all'interno della matrice di correlazione saranno espressi mediante valore decimale nell'intervallo [-1,+1] che rispettivamente indicherà la presenza di una correlazione inversa oppure una correlazione diretta.\n\nNel caso in cui il valore calcolato di correlazione sia vicino al valore 0, non è possibile definire la presenza di correlazione tra le features considerate."},{"metadata":{"trusted":false},"cell_type":"code","source":"correlations = dataset.corr()\nnames = dataset.columns\n\n# plot correlation matrix\nimport seaborn as sns\n#Using Pearson Correlation\nplt.figure(figsize=(10,10))\ncor = dataset.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.BuGn)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distribuzione dei dati per feature \"brand\""},{"metadata":{},"cell_type":"markdown","source":"Poichè la feature _'brand'_ è la variabile target per questo dataset, è stata condotta una analisi per tale feature.\n\nAnalizzando quanto segue, si evince che, per ogni classe, il numero di esempi a disposizione è più o meno equo, il che implica un buon bilanciamento degli esempi presenti all'interno del dataset.\n\nPertanto, qualora si desiderasse utilizzare questo dataset per un task di classificazione, si prevede che il modello possa classificare per bene tutte le differenti classi, in quanto aventi lo stesso numero di esempi, anche se esiguo."},{"metadata":{"trusted":false},"cell_type":"code","source":"dataset['brand'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Per mostrare visivamente il bilanciamento del dataset, di seguito è riportato un diagramma a torta che compara il numero di esempi per le categorie presenti."},{"metadata":{"trusted":false},"cell_type":"code","source":"#Plot dei vini per classe\nimport matplotlib.pyplot as plt\n\n# Data to plot\nlabels = dataset['brand'].unique()\nsizes = dataset['brand'].value_counts()\n\n# Plot\nplt.pie(sizes, labels=labels,autopct='%1.1f%%', shadow=True, startangle=140, textprops={'color':\"w\"}) #Con tema bianco, rimuovere l'attributo textprops\n\nplt.axis('equal')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Stessa informazione è stata ottenuta realizzando un istogramma dove, sull'asse delle ascisse sono presenti le differenti classi mentre sulle ordinate il numero di esempi presenti."},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.hist(x=dataset['brand'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Grafici"},{"metadata":{},"cell_type":"markdown","source":"### Boxplot"},{"metadata":{},"cell_type":"markdown","source":"Al fine di far emergere la distribuzione dei dati per ogni feature presente all'interno del dataset confrontata con la feature target (_'brand'_), di seguito sono riportati i relativi boxplot che mettono in risalto grafico le seguenti informazioni:\n- valore minimo per la feature\n- primo quartile per la feature\n- mediana per la feature\n- terzo quartile per la feature\n- valore massimo per la feature\n- presenza di valori outlier"},{"metadata":{"trusted":false},"cell_type":"code","source":"features = list(dataset.columns)\nfeatures.remove('brand')\nfor column in features:\n    sns.boxplot(data = dataset, x = 'brand', y=column)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Jointplot"},{"metadata":{},"cell_type":"markdown","source":"Per ottenere un rapido confronto grafico tra la feature target ('_brand_') e le feature di input, di seguito sono riportati una serie di jointplot che mettono in risalto la distribuzione degli esempi, suddivisi per qualità, rispetto alla feature di input selezionata:"},{"metadata":{"trusted":false},"cell_type":"code","source":"features = list(dataset.columns)\nfeatures.remove('brand')\nfor column in features:\n    sns.jointplot(data = dataset, x = 'brand' , y=column)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pairplot"},{"metadata":{},"cell_type":"markdown","source":"Con il pairplot, invece, si è provveduto ad ottenere in un unico grafico la relazione esistente tra le coppie di features presenti all'interno del dataset.\n\nAl fine di plottare i dati in tre dimensioni, sfruttando l'attributo _hue_ è stato possibile diversificare mediante colori contrastanti la classe di appartenenza del relativo punto plottato."},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.pairplot(data=dataset, diag_kind=\"kde\", hue=\"brand\", corner=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PCA (Principal Component Analysis)"},{"metadata":{},"cell_type":"markdown","source":"L'analisi delle componenti principali (PCA) è una tecnica che consente l'estrazione di informazioni rilevanti da un insieme di dati numerici. Consente, inoltre, di rivelare l'esistenza di relazioni lineari (nascoste) in dati multidimensionali.</br>\nLa PCA, inoltre, consente di effettuare la _low rank approximization_ di una matrice di dati.\n\nPCA, in presenza di dataset ad alta dimensionalità, consente di ridurre quest'ultima in, generalmente, 2 o 3 dimensioni consentendo la rappresentazione di dati multivariati su grafici 2D o 3D.\n\nUn open problem relativo a questa tecnica è l'individuazione del numero di componenti principali da considerare: solitamente, attraverso uno scree plot, si individua il gomito del plotting e quest'ultimo indicherà il numero delle componenti principali da considerare. Alternativamente, è possibile definire una percentuale di varianza spiegata dei dati [70%-90%] che verrà raggiunta sommando le varianze di ogni componente principale identificata, poste in ordine decrescente.\n\nUlteriore metodologia, invece, è rappresentata dal cerchio delle correlazioni che è una rappresentazione grafica utile per avere una idea delle variabili che contribuiscono positivamente o negativamente a spiegare la varianza della PC1 e PC2. Valori di loadings grandi associati a specifiche variabili indicano il contributo di queste ultime rispetto alle PC's.\n\nMediante il metodo di Kaiser, invece, è possibile ottenere euristicamente la stima del numero delle PC's da considerare prendendo in considerazioni gli autovalori che presentano un valore numerico maggiore di 1."},{"metadata":{},"cell_type":"markdown","source":"### Dichiarazione della feature target"},{"metadata":{},"cell_type":"markdown","source":"Una volta caricato il dataset, al fine di poter applicare la PCA, si identifica con _y_ la feature target, ovvero _brand_, mentre X rappresenterà tutte le restanti features presenti all'interno del dataset."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ny = dataset['brand']\ndataset.drop(['brand'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Standardizzazione dei dati"},{"metadata":{},"cell_type":"markdown","source":"Onde evitare problemi circa la presenza di dati espressi su un diverso range numerico, come fase di pre-processing è stata introdotta la standardizzazione, mediante apposito metodo _StandardScaler()_ applicata su X."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nx = dataset\n\nsc_x = StandardScaler()\nx = sc_x.fit_transform(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split del dataset"},{"metadata":{},"cell_type":"markdown","source":"Per valutare le prestazioni del modello sfruttando i dati a disposizione, il dataset di partenza viene suddiviso nel seguente modo:\n- 70%: dataset di training\n- 30%: dataset di test\n\nPoichè è stata già definita la feature target (_y_), tale suddivisione avviene anche per tale vettore.\n\nPer evitare, inoltre, un possibile overfitting del modello basato sulla disposizione ingenua dei dati all'interno del dataset, viene effettuato uno shuffle randomico, impostando il parametro _shuffle_=True."},{"metadata":{"trusted":false},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.3, random_state=42, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Applicazione della PCA"},{"metadata":{},"cell_type":"markdown","source":"Per l'applicazione della PCA, si è provveduto ad usare il rispettivo metodo _fit()_ che consente il fitting del modello mediante gli esempi contenuti nella matrice X di training, identificata con il nome di _x\\_train_."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA()\npca.fit(x_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Per ottenere il grafico relativo alla varianza spiegata da ogni componente è stato sviluppato il seguente codice che consente, a fronte del modello fittato, di ottenere in forma grafica la varianza per ciascuna componente (per identificare il \"gomito\") mentre, a destra, è riportato il plot della varianza cumulativa spiegata.\n\nCome è possibile notare dal primo grafico, seppure il \"gomito principale\" sia presente sulla terza componente, per le ulteriori vi è la presenza di una varianza spiegata non indifferente. Pertanto, mediante prove analitiche svolte, si è preferito prendere in considerazione le prime 3 componenti principali che, in ogni caso, vanno a ridurre la dimensionalità del dataset di partenza.</br>"},{"metadata":{"trusted":false},"cell_type":"code","source":"asse_y = pca.explained_variance_ratio_\nasse_x = np.arange(len(asse_y)) + 1\n\nplt.figure(figsize=(15,5))\nplt.subplot(1, 2, 1)\nplt.bar(asse_x, asse_y)\nplt.plot(asse_x, asse_y, \"ro-\")\nplt.title(\"Variance for each component\")\nplt.xticks(asse_x, [\"Comp.\"+str(i) for i in asse_x], rotation=90)\nplt.ylabel(\"Variance\")\n\ny2 = np.cumsum((pca.explained_variance_ratio_))\n\nplt.subplot(1, 2, 2)\nplt.bar(asse_x,y2)\nplt.plot(asse_x, y2, \"ro-\")\nplt.xticks(asse_x, [\"Comp.\"+str(i) for i in asse_x], rotation=90)\nplt.title(\"Cumulative Variance\")\nplt.ylabel(\"Variance\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ottenuto il grafico delle varianze spiegate per ogni singola componente, il numero delle componenti principali da prendere in considerazione è fissato a 3 (per le motivazioni sopra riportate).\n\nFissato il numero delle componenti, si effettua il fitting del modello e la trasformazione della matrice X di train (identificata con _x\\_train_) e della matrice X di test (_x\\_test_)\n\nSi procede, successivamente alla stampa delle seguenti informazioni:\n- Componenti identificate dalla PCA\n- Varianza spiegata da ciascuna componente\n- Varianza spiegata in percentuale da ciascuna componente\n- Valori singolari identificati"},{"metadata":{"trusted":false},"cell_type":"code","source":"pca = PCA(n_components=3)\npca.fit(x_train)\nx_train = pca.transform(x_train)\nx_test = pca.transform(x_test)\n\n#Stampa dei valori della PCA\nprint(pca.components_)\nprint(pca.explained_variance_)\nprint(pca.explained_variance_ratio_)\nprint(pca.singular_values_)\nprint(pca.singular_values_>=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot di Accuracy"},{"metadata":{},"cell_type":"markdown","source":"### Grid Search"},{"metadata":{},"cell_type":"markdown","source":"Grid search realizza una ricerca esaustiva dei parametri specificati per un modello.\nI parametri vengono specificati manualmente dall'utente, e la grid search addestra il modello in questione con ogni combinazione possibile di parametri, restituendo l'insieme che registra la migliore accuracy."},{"metadata":{},"cell_type":"markdown","source":"### Random Search\nLa Random Search è sorprendentemente efficiente rispetto alla Grid Search. Sebbene la Grid Search alla fine troverà il valore ottimale degli iperparametri (supponendo che siano nella griglia), la Random Search di solito troverà un valore \"abbastanza vicino\" in molte meno iterazioni.\n\nLa ricerca Grid Search troppo tempo a valutare regioni poco promettenti dello spazio di ricerca dell'iperparametro perché deve valutare ogni singola combinazione nella griglia. La Random Search, al contrario, fa un lavoro migliore nell'esplorare lo spazio di ricerca e quindi di solito può trovare una buona combinazione di iperparametri in molte meno iterazioni.\n\nLa Random Search dovrebbe probabilmente essere il primo metodo di ottimizzazione degli iperparametri provato per la sua efficacia. Anche se si tratta di un metodo che non si basa sui risultati delle valutazioni precedenti, la Random Search di solito può ancora trovare valori migliori rispetto a quella predefinita ed è semplice da eseguire."},{"metadata":{"trusted":false},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\n\ndef plot_accuracy(used_model, best_config_gs, best_config_random_search):\n    model = used_model(**best_config_gs)\n\n    #Suddivisione delle dimensioni del training_set e raccoglimento del training_score e test_score\n    #Impostando il valore di cross_validation = 10\n\n    #Train_sizes: controlla il numero assoluto o relativo di esempi di training utilizzati per generare le curve di apprendimento. \n    #train_sizes = np.linspace (0.1, 1.0, 10) per usare 10 punti equidistanti per le dimensioni del set di dati di allenamento.\n    train_sizes, train_scores, test_scores =\\\n                    learning_curve(estimator=model,\n                                   X=x_train,\n                                   y=y_train,\n                                   train_sizes=np.linspace(0.1, 1.0, 10),\n                                   cv=10,\n                                   n_jobs=-1)\n\n    #Calcolo delle medie e delle deviazioni standard sul training e test\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n\n    #Plot dei valori della GridSearch() sul grafico\n    plt.figure(figsize=(15,5))\n    plt.subplot(1, 2, 1)\n    \n    plt.plot(train_sizes, train_mean,\n             color='blue', marker='o',\n             markersize=5, label='Training accuracy')\n\n    plt.fill_between(train_sizes,\n                     train_mean + train_std,\n                     train_mean - train_std,\n                     alpha=0.15, color='blue')\n\n    plt.plot(train_sizes, test_mean,\n             color='green', linestyle='--',\n             marker='s', markersize=5,\n             label='Validation accuracy')\n\n    #la funzione fill_between indica la varianza della stima colorando la regione di piano\n    plt.fill_between(train_sizes,\n                     test_mean + test_std,\n                     test_mean - test_std,\n                     alpha=0.15, color='green')\n\n    plt.grid()\n    plt.xlabel('Number of training examples')\n    plt.ylabel('Accuracy')\n    plt.legend(loc='lower right')\n    plt.ylim([0.3, 1.03])\n    plt.tight_layout()\n    \n    #RandomSearch e relativo grafico\n    model = used_model(**best_config_random_search)\n\n    #Suddivisione delle dimensioni del training_set e raccoglimento del training_score e test_score\n    #Impostando il valore di cross_validation = 10\n\n    #Train_sizes: controlla il numero assoluto o relativo di esempi di training utilizzati per generare le curve di apprendimento. \n    #train_sizes = np.linspace (0.1, 1.0, 10) per usare 10 punti equidistanti per le dimensioni del set di dati di allenamento.\n    train_sizes, train_scores, test_scores =\\\n                    learning_curve(estimator=model,\n                                   X=x_train,\n                                   y=y_train,\n                                   train_sizes=np.linspace(0.1, 1.0, 10),\n                                   cv=10,\n                                   n_jobs=-1)\n\n    #Calcolo delle medie e delle deviazioni standard sul training e test\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n    \n    plt.subplot(1, 2, 2)\n    #Plot dei valori della GridSearch() sul grafico\n    plt.plot(train_sizes, train_mean,\n             color='blue', marker='o',\n             markersize=5, label='Training accuracy')\n\n    plt.fill_between(train_sizes,\n                     train_mean + train_std,\n                     train_mean - train_std,\n                     alpha=0.15, color='blue')\n\n    plt.plot(train_sizes, test_mean,\n             color='green', linestyle='--',\n             marker='s', markersize=5,\n             label='Validation accuracy')\n\n    #la funzione fill_between indica la varianza della stima colorando la regione di piano\n    plt.fill_between(train_sizes,\n                     test_mean + test_std,\n                     test_mean - test_std,\n                     alpha=0.15, color='green')\n\n    plt.grid()\n    plt.xlabel('Number of training examples')\n    plt.ylabel('Accuracy')\n    plt.legend(loc='lower right')\n    plt.ylim([0.3, 1.03])\n    plt.tight_layout()\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Percettrone"},{"metadata":{},"cell_type":"markdown","source":"Implementa un modello che prevede l'utilizzo di un percettrone, il concetto alla base delle reti neurali.\nTale percettrone in fase di training viene inizializzato con dei pesi, i quali vengono utilizzati per fare predizione.\nIn base ai risultati ottenuti, ad ogni iterazione viene calcolata una perdita (loss), che indica il tasso di errore che si sta commettendo nella predizione.\nIn base a tale metrica, vengono poi modificati i pesi in base alla funzione di attivazione che si sta utilizzando, realizzando una tecnica nota come back-propagation.\n\nL'apprendimento termina quando si raggiunge una soglia minima di errore pre-impostata, o quando si raggiunge un numero massimo di iterazioni.\n\nParametri testati:\n- <b>tol</b>: Criterio di stop. Le iterazioni terminano quando loss > previous_loss - tol.\n- <b>eta0</b>: Costante con la quale gli updates sono moltiplicati\n- <b>penality</b>: termine di regolarizzazione da usare:\n    - <b>l2</b>\n    - <b>l1</b>\n    - <b>elasticnet</b>"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nimport numpy as np\n\ngrid={'tol':[0.001,0.01,0.1,1], 'eta0': [0.001,0.01,0.1,1], 'penalty': ['l2','l1','elasticnet']}\n\nppn = Perceptron(random_state=1, n_jobs = -1, max_iter=1000)\nppn_cv=GridSearchCV(ppn,grid,cv=10)\nppn_cv.fit(x_train,y_train)\n\n#Stampa degli attributi best_score sul modello e i parametri con cui quel 'best_score' è stato ottenuto\nprint(\"GridSearch():\\n\")\ncombinazioni = 1\nfor x in grid.values():\n    combinazioni *= len(x)\nprint('Per l\\'applicazione della GridSearch ci sono {} combinazioni'.format(combinazioni))\nprint(\"Migliore configurazione: \",ppn_cv.best_params_)\nbest_config_gs = ppn_cv.best_params_\nprint(\"Accuracy CV:\",ppn_cv.best_score_)\nppn_cv = ppn_cv.best_estimator_\nprint('Test accuracy: %.3f' % ppn_cv.score(x_test, y_test))\n\n\n#RandomizedSearch\nn_iter_search = 20\nrandom_search = RandomizedSearchCV(ppn, param_distributions=grid, n_iter=n_iter_search, cv=10, n_jobs =-1)\nrandom_search.fit(x_train,y_train)\n\n#Stampa degli attributi best_score sul modello e i parametri con cui quel 'best_score' è stato ottenuto\nprint(\"\\n\\nRandomizedSearch():\\n\")\nprint(\"Migliore configurazione: \",random_search.best_params_)\nbest_config_random_search = random_search.best_params_\nprint(\"Accuracy CV:\",random_search.best_score_)\n\nrandom_search = random_search.best_estimator_\nprint('Test accuracy: %.3f' % random_search.score(x_test, y_test))\n\nplot_accuracy(Perceptron, best_config_gs, best_config_random_search) #Stampo il grafico dell'accuratezza","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear Discriminant Analysis"},{"metadata":{},"cell_type":"markdown","source":"Un classificatore lineare con un confine di decisione generato dal fitting delle densità condizionali ai dati usando la regola di Bayes.\n\nIl modello fitta una densità Gaussiana per ogni classe, assumendo che tutte le classi condividono la stessa matrice di covarianza.\n\nIl modello fittato può anche essere usato per ridurre la dimensionalità dell'input proiettando le sue direzioni più discriminative, utilizzando il metodo transform.\n\nAttraverso LDA si sfrutta la conoscenza delle classi (variabile target del problema). In particolare, si sfrutta la matrice X delle features e le informazioni che si hanno sulle Y.\nSi deve risolvere un problema di classificazione. L'obiettivo è di porre un altro vincolo: la proiezione deve tenere le classi separate tra loro.\nL’obiettivo è di trovare la direzione w che vada a massimizzare la distanza (favorendo la separazione tra le due classi), minimizzando la varianza (presente al denominatore, massimizzando la coesione all'interno della classe.)\n•  m1 (la proiezione) si ottiene da w e dalla media dell'intera classe 1. \n•  Idem anche su m2\n•  Il denominatore è calcolato mediante gli scarti quadratici med\n\nIn contrasto con PCA, LDA tenta di trovare un sottospazio di features che massimizzi la separabilità delle classi.\nLDA fa ipotesi su classi distribuite normalmente e covarianze di classi uguali.\n(La PCA tende a produrre risultati di classificazione migliori in un'attività di riconoscimento delle immagini se il numero di campioni per una data classe era relativamente piccolo.)\n\nParametri testati:\n- <b>solver</b>: risolutore da utilizzare. Si può scegliere tra:\n    - <b>svd (default)</b>:  singular value decomposition. Non viene calcolata la matrice di covarianza, tuttavia è necessario modificare questo parametro quando si è in presenza di un gran numero di features.\n    - <b>lsqr</b>: soluzione ai minimi quadrati\n    - <b>eigen</b>: decomposizione di autovalori\n- <b>tol</b>: soglia in valore assoluto per i valori singolari da considerare significanti in X. Le dimensioni i cui valori singolari non sono significanti sono scartati. Può essere utilizzato solo con svd.\n- <b>n_components</b>: numero di componenti da tenere in considerazione per la riduzione della dimensionalità."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nimport numpy as np\n\nx_train_lda = x_train\ny_train_lda = y_train\n\n\ngrid={'n_components':[x for x in range(1,4)], 'solver': ['svd', 'lsqr', 'eigen'], 'tol':[0.001,0.01,0.1,1]}\n\nlda = LDA()\nlda_cv=GridSearchCV(lda,grid,cv=10)\nlda_cv.fit(x_train,y_train)\n\n#Stampa degli attributi best_score sul modello e i parametri con cui quel 'best_score' è stato ottenuto\nprint(\"GridSearch():\\n\")\ncombinazioni = 1\nfor x in grid.values():\n    combinazioni *= len(x)\nprint('Per l\\'applicazione della GridSearch ci sono {} combinazioni'.format(combinazioni))\nprint(\"Migliore configurazione: \",lda_cv.best_params_)\nbest_config_gs = lda_cv.best_params_\nprint(\"Accuracy CV:\",lda_cv.best_score_)\nlda_cv = lda_cv.best_estimator_\nprint('Test accuracy: %.3f' % lda_cv.score(x_test, y_test))\n\n#RandomizedSearch\nn_iter_search = 20\nrandom_search = RandomizedSearchCV(lda, param_distributions=grid, n_iter=n_iter_search, cv=10, n_jobs =-1)\nrandom_search.fit(x_train,y_train)\n\n#Stampa degli attributi best_score sul modello e i parametri con cui quel 'best_score' è stato ottenuto\nprint(\"\\n\\nRandomizedSearch():\\n\")\nprint(\"Migliore configurazione: \",random_search.best_params_)\nbest_config_random_search = random_search.best_params_\nprint(\"Accuracy CV:\",random_search.best_score_)\n\nrandom_search = random_search.best_estimator_\nprint('Test accuracy: %.3f' % random_search.score(x_test, y_test))\n\nplot_accuracy(LDA, best_config_gs, best_config_random_search) #Stampo il grafico dell'accuratezza","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Regressione Lineare"},{"metadata":{},"cell_type":"markdown","source":"La regressione lineare è una tecnica di analisi predittiva di base che utilizza dati raccolti per prevedere una variabile di output. </br>L'idea alla base della regressione lineare è che se risulta possibile adattare un modello di regressione lineare ai casi osservati, allora è possibile utilizzare tale modello per prevedere eventuali valori futuri.\n\nPer semplicità si descrive il funzionamento di un modello di regressione lineare semplice:\n\nIn un modello di regressione lineare esistono due tipi di variabili:\n- <b>Variabile di input</b>: sono le variabili che aiutano a prevedere il valore della variabile di output. Viene comunemente indicata con X\n- <b>Variabile di output</b>: è la variabile che si vuole prevedere. Viene comunemente indicata con Y.\n\nPer stimare Y, usando la regressione lineare, si sfrutta la seguente equazione: ${Y\\_predicted = a0+a1*X}$.</br>L'obiettivo è trovare valori statisticamente significativi dei parametri $a0$ e $a1$ che minimizzino la differenza tra $Y$ e $Y\\_predicted$ sfruttando una tecnica di minimizzazione dell'errore.</br>\nSe si è in grado di determinare i valori ottimali per tali parametri, allora si otterrà la retta di _best fit_ che è possibile utilizzare per prevedere i valori di Y dato X."},{"metadata":{},"cell_type":"markdown","source":"Nel seguente snippet di codice viene utilizzato il modello LinearRegressione della libreria sklearn per trovare la migliore retta di regressione, minimizzando i quadrati dei residui.\nIn questo caso, non si tratta di regressione lineare semplice perchè i dati in input sono multidimensionali."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn import linear_model\n\nregr = linear_model.LinearRegression(n_jobs = -1)\nregr.fit(x_train, y_train)\n\nprint('Accuracy of Linear regression classifier on training set: {:.2f}'\n     .format(regr.score(x_train, y_train)))\nprint('Accuracy of Linear regression classifier on test set: {:.2f}'\n     .format(regr.score(x_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Regressione Polinomiale"},{"metadata":{},"cell_type":"markdown","source":"La regressione polinomiale utilizza lo stesso metodo della regressione lineare, ma assume che la funzione che meglio descrive l’andamento dei dati non sia una retta (lineare), ma un polinomio (curva). Quindi è adatta quando lo scatterplot di una relazione bivariata, ad esempio, mostra una forma diversa da quella della retta, ad esempio una curva.\n\nPer adempiere alla realizzazione del task di regressione polinomiale è stato realizzato un apposito snippet:"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\npolynomial_features= PolynomialFeatures(degree=3)\nx_poly_train = polynomial_features.fit_transform(x_train)\nx_poly_test = polynomial_features.fit_transform(x_test)\n\nmodel = LinearRegression(n_jobs = -1)\nmodel.fit(x_poly_train, y_train)\n\nprint('Accuracy of Linear regression classifier on training set: {:.2f}'\n     .format(model.score(x_poly_train, y_train)))\nprint('Accuracy of Linear regression classifier on test set: {:.2f}'\n     .format(model.score(x_poly_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Regressione Logistica"},{"metadata":{},"cell_type":"markdown","source":"La regressione logistica è uno degli algoritmi di apprendimento automatico più semplici e comunemente utilizzati per la classificazione a due classi. È facile da implementare e può essere utilizzata come base per qualsiasi problema di classificazione binaria.\n\nLo scopo di questo algoritmo è quello di descrivere e stimare la probabilità di una variabile dipendente categorica da una o più variabili indipendenti. La variabile dipendente è una variabile binaria che contiene dati codificati come 1 (sì, successo, ecc.) o 0 (no, errore, ecc.). In altre parole, il modello di regressione logistica prevede P (Y = 1) come funzione di X.\n\n\nNel caso rappresentato in seguito, si è in presenza di Regressione Logistica Multinomiale, in quanto le variabili da predire non sono dicotomiche, ma possono assumere più di due valori.\nPer testare i molteplici parametri del modello _logreg_ è stata utilizzata la GridSearchCV() per ottenere la configurazione migliore del settaggio dei parametri da utilizzare.\n\nParametri testati:\n- <b>solvers</b>: indica l'algoritmo da utilizzare nel problema di ottimizzazione\n- <b>multi_class</b>:\n    - <b>ovr</b>: viene fittato un problema binario per ogni etichetta.\n    - <b>multinomial</b>: la loss minimizzata è la loss multinomiale fittata su tutta la distribuzione di probabilitù, anche quando i dati sono binari.\n    - <b>auto (default)</b>: viene selezionato il miglior parametro automaticamente\n- <b>C</b>: Inverso della potenza di regolarizzazione, inteso come float positivo. Valori più piccoli indicano una regolarizzazione più forte."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Grid search cross validation\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\nsolvers = ['newton-cg', 'lbfgs', 'sag', 'saga']\nmulticlass_values = ['auto', 'ovr', 'multinomial']\nC = [x for x in range(10,100+1,10)]\n\ngrid={'solver':solvers, 'multi_class':multiclass_values, 'C':C}\nlogreg=LogisticRegression(random_state=1, max_iter=5000)\nlogreg_cv=GridSearchCV(logreg,grid,cv=10)\nlogreg_cv.fit(x_train,y_train)\n\n#Stampa degli attributi best_score sul modello e i parametri con cui quel 'best_score' è stato ottenuto\nprint(\"GridSearch():\\n\")\ncombinazioni = 1\nfor x in grid.values():\n    combinazioni *= len(x)\nprint('Per l\\'applicazione della GridSearch ci sono {} combinazioni'.format(combinazioni))\nprint(\"Migliore configurazione: \",logreg_cv.best_params_)\nbest_config_gs = logreg_cv.best_params_\nprint(\"Accuracy CV:\",logreg_cv.best_score_)\nlogreg_be = logreg_cv.best_estimator_\nprint('Test accuracy: %.3f' % logreg_be.score(x_test, y_test))\n\n#RandomizedSearch\nn_iter_search = 20\nrandom_search = RandomizedSearchCV(logreg, param_distributions=grid, n_iter=n_iter_search, cv=10, n_jobs =-1)\nrandom_search.fit(x_train,y_train)\n\n#Stampa degli attributi best_score sul modello e i parametri con cui quel 'best_score' è stato ottenuto\nprint(\"\\n\\nRandomizedSearch():\\n\")\nprint(\"Migliore configurazione: \",random_search.best_params_)\nbest_config_random_search = random_search.best_params_\nprint(\"Accuracy CV:\",random_search.best_score_)\n\nrandom_search = random_search.best_estimator_\nprint('Test accuracy: %.3f' % random_search.score(x_test, y_test))\n\nplot_accuracy(LogisticRegression, best_config_gs, best_config_random_search) #Stampo il grafico dell'accuratezza","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Regressione mediante Random forest\n\nUna Random Forest è un meta stimatore che si adatta ad una serie di alberi decisionali su vari sottocampioni del dataset e utilizza la media per migliorare l'accuratezza predittiva e il controllo dell'overfitting.\n\nLa differenza tra RandomForestClassifier e RandomForestRegressor, sta nel fatto che nel primo caso, viene utilizzato un classificatore per predire un insieme specificato di labels.\n\nNel secondo caso invece, viene utilizzato un regressore per predire valori reali, che possono variare e non appartengono ad un insieme limitato di valori che si possono assumere.\n\nParametri testati:\n- <b>n_estimators</b>: numero di alberi nella foresta\n- <b>criterion</b>: funzione per misurare la qualità di una divisione. I criteri supportati sono \"mse\" (errore quadratico medio) e \"mae\" (errore medio assoluto)\n- <b>max_features</b>: numero di features da considerare quando si cerca la migliore suddivisione."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Grid search cross validation\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\n\ngrid={'n_estimators':[x for x in range(1000,2000+1,500)], 'criterion':['mae','mse'], 'max_features':['auto', 'sqrt', 'log2']}\nforest = RandomForestRegressor(random_state=1, n_jobs=-1)\nforest_cv=GridSearchCV(forest,grid,cv=10)\nforest_cv.fit(x_train,y_train)\n\n#Stampa degli attributi best_score sul modello e i parametri con cui quel 'best_score' è stato ottenuto\nprint(\"GridSearch():\\n\")\ncombinazioni = 1\nfor x in grid.values():\n    combinazioni *= len(x)\nprint('Per l\\'applicazione della GridSearch ci sono {} combinazioni'.format(combinazioni))\nprint(\"Migliore configurazione: \",forest_cv.best_params_)\nbest_config_gs = forest_cv.best_params_\nprint(\"Accuracy CV:\",forest_cv.best_score_)\nforest_be = forest_cv.best_estimator_\nprint('Test accuracy: %.3f' % forest_be.score(x_test, y_test))\n\n#RandomizedSearch\nn_iter_search = 10\nrandom_search = RandomizedSearchCV(forest, param_distributions=grid, n_iter=n_iter_search, cv=10, n_jobs =-1)\nrandom_search.fit(x_train,y_train)\n\n#Stampa degli attributi best_score sul modello e i parametri con cui quel 'best_score' è stato ottenuto\nprint(\"\\n\\nRandomizedSearch():\\n\")\nprint(\"Migliore configurazione: \",random_search.best_params_)\nbest_config_random_search = random_search.best_params_\nprint(\"Accuracy CV:\",random_search.best_score_)\n\nrandom_search = random_search.best_estimator_\nprint('Test accuracy: %.3f' % random_search.score(x_test, y_test))\n\nplot_accuracy(RandomForestRegressor, best_config_gs, best_config_random_search) #Stampo il grafico dell'accuratezza","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Utilizzo di metodi regolarizzati per la regressione"},{"metadata":{},"cell_type":"markdown","source":"Gli approcci popolari per i modelli lineare di regressione regolarizzata sono: Ridge Regression, least absolute shrinkage and selection operator (LASSO), and elastic Net.\n\n![](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1543418451/tradeoff_sevifm.png)"},{"metadata":{},"cell_type":"markdown","source":"### Ridge Regression"},{"metadata":{},"cell_type":"markdown","source":"Questo modello è conosciuto anche come regressione della cresta o regolarizzazione di Tikhonov. Questo stimatore ha un supporto integrato per la regressione multi-variata, cioè quando y è un array multi-dimensionale.\n\nE' un ottima soluzione da adottare quando si è in presenza multi-collinearità tra i vari esempi.\n\nDunque, Ridge aggiunge un fattore di penalizzazione (norma l2) alla cost function. Ciò determina la perdita di importanza del valore di una feature, che a seconda della penalità può essere più o meno accentuata. La forza della penalità è modificabile e controllata da un iperparametro.\n\nParametro utilizzato:\n- <b>alpha</b>: forza di regolarizzazione. Migliora il condizionamento del problema. Valori più grandi specificano una regolarizzazione più forte.\n\n![](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1543418449/eq7_ylxudw.png)"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nridge = Ridge(alpha=1.0)\n\nridge.fit(x_train, y_train)\ny_train_pred = ridge.predict(x_train)\ny_test_pred = ridge.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\nprint('MSE train: %.3f, test: %.3f' % (\n        mean_squared_error(y_train, y_train_pred),\n        mean_squared_error(y_test, y_test_pred)))\nprint('R^2 train: %.3f, test: %.3f' % (\n        r2_score(y_train, y_train_pred),\n        r2_score(y_test, y_test_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lasso Regression"},{"metadata":{},"cell_type":"markdown","source":"Una caratteristica fondamentale della Lasso regression riguarda la gestione delle feature di importanza minore.\nContrariamente alla Ridge regression, che minimizzando il peso di alcune feature ne riduce la contribuzione al modello, la lasso regression effettua una vera e propria feature selection (prendendo in considerazione solo le variabili indipendenti) portando le restanti a zero attraverso un opportuno valore di peso associato generando quindi un modello sparso.\n\nA differenza di Ridge, Lasso penalizza la somma dei valori assoluti, ovvero la L1.\n\nParametro utilizzato:\n- <b>alpha</b>: costante che moltiplica il termine l1.\n\n![](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1543418448/eq11_ij4mms.png)"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import Lasso\nlasso = Lasso(alpha=1.0)\n\nlasso.fit(x_train, y_train)\ny_train_pred = lasso.predict(x_train)\ny_test_pred = lasso.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\nprint('MSE train: %.3f, test: %.3f' % (\n        mean_squared_error(y_train, y_train_pred),\n        mean_squared_error(y_test, y_test_pred)))\nprint('R^2 train: %.3f, test: %.3f' % (\n        r2_score(y_train, y_train_pred),\n        r2_score(y_test, y_test_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Elastic Net"},{"metadata":{},"cell_type":"markdown","source":"Elastic Net è emerso per la prima volta a seguito di una critica alla Lasso Regression, la cui feature selection potrebbe essere troppo dipendente dai dati, rendendo il modello instabile.\n\nLa soluzione è quella di combinare le penalità della regressione Ridge e Lasso per ottenere i vantaggi da entrambi i modelli.\n\nSi utilizzano quindi due parametri regolarizzatori, che corrispondono rispettivamente alle funzioni di regolarizzazione di Ridge e Lasso regression.\n\nParametri utilizzati:\n- <b>alpha</b>: costante che moltiplica il termine di penalità.\n- <b>l1_ratio</b>: parametro di mix di elasticNet. Per l1_ratio=0 la penalità è la norma l2. Per l1_ratio=1 la penalità è la norma l1, altrimenti è una combinazione di l1 e l2.\n\n![](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1543418448/eq12_vh6ilt.png)"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import ElasticNet\nelanet = ElasticNet(alpha=1.0, l1_ratio=0.5)\n\nelanet.fit(x_train, y_train)\ny_train_pred = elanet.predict(x_train)\ny_test_pred = elanet.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\nprint('MSE train: %.3f, test: %.3f' % (\n        mean_squared_error(y_train, y_train_pred),\n        mean_squared_error(y_test, y_test_pred)))\nprint('R^2 train: %.3f, test: %.3f' % (\n        r2_score(y_train, y_train_pred),\n        r2_score(y_test, y_test_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree"},{"metadata":{},"cell_type":"markdown","source":"Gli alberi decisionali (DT) sono un metodo di apprendimento supervisionato non parametrico utilizzato per la classificazione e la regressione.\nL'obiettivo è creare un modello che preveda il valore di una variabile target apprendendo semplici regole decisionali dedotte dalle caratteristiche dei dati.\n\nAlcuni vantaggi degli alberi decisionali sono:\n- Semplice da capire e da interpretare. Gli alberi possono essere visualizzati.\n- Potrebbe richiedere poca preparazione dei dati. Altre tecniche spesso richiedono la normalizzazione dei dati, la creazione di variabili fittizie e la rimozione dei valori vuoti. Si noti tuttavia che questo modulo non supporta i valori mancanti.\n- Il costo dell'utilizzo dell'albero (ovvero la previsione dei dati) è logaritmico nel numero di punti dati utilizzati per addestrare l'albero.\n- In grado di gestire dati sia numerici che categoriali. Altre tecniche sono solitamente specializzate nell'analisi di set di dati che hanno un solo tipo di variabile. Vedi algoritmi per maggiori informazioni.\n- In grado di gestire problemi con più output.\n- Se una data situazione è osservabile in un modello, la spiegazione della condizione è facilmente spiegabile dalla logica booleana.\n- Possibilità di validare un modello utilizzando test statistici. Ciò consente di tenere conto dell'affidabilità del modello.\n\nGli svantaggi degli alberi decisionali includono:\n- Gli studenti che apprendono l'albero decisionale possono creare alberi troppo complessi che non generalizzano bene i dati. Questo si chiama overfitting. Per evitare questo problema sono necessari meccanismi come la potatura, l'impostazione del numero minimo di campioni richiesto su un nodo fogliare o l'impostazione della profondità massima dell'albero.\n- Gli alberi decisionali possono essere instabili perché piccole variazioni nei dati potrebbero comportare la generazione di un albero completamente diverso. Questo problema viene mitigato utilizzando alberi decisionali all'interno di un insieme.\n- Il problema dell'apprendimento di un albero decisionale ottimale è noto per essere NP-completo sotto diversi aspetti dell'ottimalità e anche per concetti semplici. Di conseguenza, gli algoritmi pratici di apprendimento dell'albero decisionale sono basati su algoritmi euristici come l'algoritmo greedy in cui vengono prese decisioni ottimali a livello locale in ogni nodo. Tali algoritmi non possono garantire la restituzione dell'albero decisionale ottimale a livello globale. Ciò può essere mitigato addestrando più alberi in un gruppo di studenti, in cui le caratteristiche e i campioni vengono campionati in modo casuale con la sostituzione.\n- Ci sono concetti che sono difficili da imparare perché gli alberi decisionali non li esprimono facilmente, come XOR, parità o problemi di multiplexer.\n- Si potrebbero creare alberi sbilanciati. Si consiglia pertanto di bilanciare il set di dati prima di adattarlo all'albero decisionale.\n\nParametri testati:\n- <b>criterion</b>: indica la funzione da utilizzare per la misurazione della qualità di uno split. I criteri supportati sono:\n    - <b>gini</b>: misura di impurità\n    - <b>entropy</b>: information gain\n- <b>splitter</b>: la strategia usata per scegliere lo split ad ogni nodo. Le strategie supportate sono:\n    - <b>best</b>: seleziona il miglior split\n    - <b>random</b>: sceglie in modo casuale"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Grid search cross validation\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\n\n\ngrid={'criterion':['gini','entropy'], 'splitter':['best', 'random']}\n\nclf = DecisionTreeClassifier()\nclf_cv=GridSearchCV(clf,grid,cv=10)\nclf_cv.fit(x_train,y_train)\n\n#Stampa degli attributi best_score sul modello e i parametri con cui quel 'best_score' è stato ottenuto\nprint(\"GridSearch():\\n\")\ncombinazioni = 1\nfor x in grid.values():\n    combinazioni *= len(x)\nprint('Per l\\'applicazione della GridSearch ci sono {} combinazioni'.format(combinazioni))\nprint(\"Migliore configurazione: \",clf_cv.best_params_)\nbest_config_gs = clf_cv.best_params_\nprint(\"Accuracy CV:\",clf_cv.best_score_)\nclf_be = clf_cv.best_estimator_\nprint('Test accuracy: %.3f' % clf_be.score(x_test, y_test))\n\n#RandomizedSearch\nn_iter_search = 20\nrandom_search = RandomizedSearchCV(clf, param_distributions=grid, n_iter=n_iter_search, cv=10, n_jobs =-1)\nrandom_search.fit(x_train,y_train)\n\n#Stampa degli attributi best_score sul modello e i parametri con cui quel 'best_score' è stato ottenuto\nprint(\"\\n\\nRandomizedSearch():\\n\")\nprint(\"Migliore configurazione: \",random_search.best_params_)\nbest_config_random_search = random_search.best_params_\nprint(\"Accuracy CV:\",random_search.best_score_)\n\nrandom_search = random_search.best_estimator_\nprint('Test accuracy: %.3f' % random_search.score(x_test, y_test))\n\nplot_accuracy(DecisionTreeClassifier, best_config_gs, best_config_random_search) #Stampo il grafico dell'accuratezza","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{},"cell_type":"markdown","source":"Una foresta casuale (in inglese: random forest) è un classificatore d'insieme ottenuto dall'aggregazione tramite bagging di alberi di decisione.\nLe foreste casuali si pongono come soluzione che minimizza l'overfitting del training set rispetto agli alberi di decisione.\n\nParametri testati:\n- <b>n_estimators</b>: numero di alberi decisionali da utilizzare nella random forest\n- <b>criterion</b>: indica la funzione da utilizzare per la misurazione della qualità di uno split. I criteri supportati sono:\n    - <b>gini</b>: misura di impurità\n    - <b>entropy</b>: information gain"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Grid search cross validation\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\nestimators = [x for x in range(5,50+1,5)]\ncriterion_list = ['gini','entropy']\ngrid={'n_estimators':estimators, 'criterion':criterion_list}\n\nforest = RandomForestClassifier(random_state=1, n_jobs=-1)\nforest_cv=GridSearchCV(forest,grid,cv=10)\nforest_cv.fit(x_train,y_train)\n\n#Stampa degli attributi best_score sul modello e i parametri con cui quel 'best_score' è stato ottenuto\nprint(\"GridSearch():\\n\")\ncombinazioni = 1\nfor x in grid.values():\n    combinazioni *= len(x)\nprint('Per l\\'applicazione della GridSearch ci sono {} combinazioni'.format(combinazioni))\nprint(\"Migliore configurazione: \",forest_cv.best_params_)\nbest_config_gs = forest_cv.best_params_\nprint(\"Accuracy CV:\",forest_cv.best_score_)\nforest_cv = forest_cv.best_estimator_\nprint('Test accuracy: %.3f' % forest_cv.score(x_test, y_test))\n\n#RandomizedSearch\nn_iter_search = 20\nrandom_search = RandomizedSearchCV(forest, param_distributions=grid, n_iter=n_iter_search, cv=10, n_jobs =-1)\nrandom_search.fit(x_train,y_train)\n\n#Stampa degli attributi best_score sul modello e i parametri con cui quel 'best_score' è stato ottenuto\nprint(\"\\n\\nRandomizedSearch():\\n\")\nprint(\"Migliore configurazione: \",random_search.best_params_)\nbest_config_random_search = random_search.best_params_\nprint(\"Accuracy CV:\",random_search.best_score_)\n\nrandom_search = random_search.best_estimator_\nprint('Test accuracy: %.3f' % random_search.score(x_test, y_test))\n\nplot_accuracy(RandomForestClassifier, best_config_gs, best_config_random_search) #Stampo il grafico dell'accuratezza","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## K-nearest Neighbor"},{"metadata":{},"cell_type":"markdown","source":"Il k-nearest neighbors (k-NN) è un algoritmo che di solito viene utilizzato nel riconoscimento di pattern per la classificazione di oggetti basandosi sulle caratteristiche degli oggetti vicini a quello considerato.\n\nUn oggetto è classificato in base alla maggioranza dei voti dei suoi k vicini. k è un intero positivo tipicamente non molto grande. In un contesto binario in cui sono presenti esclusivamente due classi è opportuno scegliere k dispari per evitare di ritrovarsi in situazioni di parità.\nConsiderando solo i voti dei k oggetti vicini c'è l'inconveniente dovuto alla predominanza delle classi con più oggetti. In questo caso può risultare utile pesare i contributi dei vicini in modo da dare, nel calcolo della media, maggior importanza in base alla distanza dall'oggetto considerato.\n\nLa scelta di k dipende dalle caratteristiche dei dati. Generalmente all'aumentare di k si riduce il rumore che compromette la classificazione, ma il criterio di scelta per la classe diventa più labile. La scelta può esser presa attraverso tecniche euristiche, come ad esempio la cross validation.\n\nParametri testati:\n- <b>algorithm</b>: algoritmo utilizzato per computare i migliori k vicini\n    - <b>auto</b>: seleziona il miglior algoritmo automaticamente\n    - <b>ball_tree</b>: utilizza un ballTree\n    - <b>kd_tree</b>: utilizza un albero kd\n    - <b>brute</b>: ricerca brute force\n- <b>weights</b>: funzione di peso da utilizzare per la predizione\n    - <b>uniform</b>: ciascun nodo ha lo stesso peso\n    - <b>distance</b>: ciascun nodo viene pesato in base all'inverso della distanza con i suoi vicini\n- <b>n_neighbors</b>: numero di vicini da tenere in considerazione"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Grid search cross validation\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\ngrid={'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'], 'weights':['uniform', 'distance'], 'n_neighbors':[x for x in range(1,20+1)]}\n\nknn = KNeighborsClassifier(n_jobs = -1)\nknn_cv = GridSearchCV(knn,grid,cv=10)\nknn_cv.fit(x_train,y_train)\n\n#Stampa degli attributi best_score sul modello e i parametri con cui quel 'best_score' è stato ottenuto\nprint(\"GridSearch():\\n\")\ncombinazioni = 1\nfor x in grid.values():\n    combinazioni *= len(x)\nprint('Per l\\'applicazione della GridSearch ci sono {} combinazioni'.format(combinazioni))\nprint(\"Migliore configurazione: \",knn_cv.best_params_)\nbest_config_gs = knn_cv.best_params_\nprint(\"Accuracy CV:\",knn_cv.best_score_)\nknn_cv = knn_cv.best_estimator_\nprint('Test accuracy: %.3f' % knn_cv.score(x_test, y_test))\n\n#RandomizedSearch\nn_iter_search = 20\nrandom_search = RandomizedSearchCV(knn, param_distributions=grid, n_iter=n_iter_search, cv=10, n_jobs =-1)\nrandom_search.fit(x_train,y_train)\n\n#Stampa degli attributi best_score sul modello e i parametri con cui quel 'best_score' è stato ottenuto\nprint(\"\\n\\nRandomizedSearch():\\n\")\nprint(\"Migliore configurazione: \",random_search.best_params_)\nbest_config_random_search = random_search.best_params_\nprint(\"Accuracy CV:\",random_search.best_score_)\n\nrandom_search = random_search.best_estimator_\nprint('Test accuracy: %.3f' % random_search.score(x_test, y_test))\n\nplot_accuracy(KNeighborsClassifier, best_config_gs, best_config_random_search) #Stampo il grafico dell'accuratezza","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gaussian Naive Bayes"},{"metadata":{},"cell_type":"markdown","source":"Il teorema di Bayes proposto da Thomas Bayes, è utilizzato per calcolare la probabilità condizionata di un evento A, sapendo che si è verificato un evento B, a partire dalla conoscenza delle probabilità a priori degli eventi A e B e della probabilità condizionata di B noto A.\nIl teorema di Bayes è utilizzato in molti campi, come nella diagnosi medica per calcolare la probabilità che un individuo sia affetto da una malattia sapendo che presenti determinati sintomi.\n\nConsiderando un insieme di variabili aleatorie indipendenti $A_1, ..., A_n$ che partizionano l'insieme degli eventi $\\Omega$, la probabilità condizionata è definita come:\n$$P(A_i|B) = \\frac{P(B|A_i)P(A_i)}{P(B)} = \\frac{P(B|A_i)P(A_i)}{\\sum_{j=1}^nP(B|A_j)P(A_j)}$$\n\nDove:\n\n- $P(A_i|B)$: probabilità condizionata di $A_i$ noto B. E' anche conosciuta come probabilità a posteriori visto che dipende dallo specifico valore di B.\n- $P(B|A_i)$: probabilità condizionata di B noto $A_i$\n- $P(A_i)$: probabilità a priori di $A_i$.\n- $P(B)$: probabilità a priori di B, e funge da costante di normalizzazione, che permette di ottenre $P(A_i|B) = 1$ al variare di $i$.\n\nL'espressione Naive Bayes (\"ingenuo\") indica il fatto che si fanno forti assunzioni di indipendenza nel modello. Si assume infatti che data una certa classe, ciascuna delle variabili aleatorie (features) siano indipendenti.\nFormalmente quindi dato un vettore $B=(B_1, ..., B_n)$ si assume che sia verificata:\n$$P(B_1, ..., B_n|A) = \\prod_{i=1}^{n}P(B_i|A)$$\nQuesta assunzione di indipendenza però non sempre è verificata nella realtà, motivo per il quale è detto \"Naive Bayes\". Nonostante ciò, i modelli che utilizzano Naive Bayes funzionano sorprendentemente bene.\n\nQuesta particolarità sta a significare che la probabilità a posteriori:\n$$P(A|B_1, ..., B_n) \\propto P(A)P(B_1|A)...P(B_n|A)$$\n\nIl teorema può essere rivisto in diverse maniere a seconda della distribuzione di probabilità delle varie variabili aleatorie.\n\n\nNel caso esposto nello snippet seguente, si utilizza Gaussian Naive Bayes, perchè si assume che le feature di cui si dispone seguano una distribuzione Gaussiana.\n\nPer questo motivo, la probabilità a posteriori può essere calcolata di volta in volta mediante l'uso dell'equazione Gaussiana."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(x_train, y_train)\n\nprint('Accuracy of GNB classifier on training set: {:.2f}'\n     .format(gnb.score(x_train, y_train)))\nprint('Accuracy of GNB classifier on test set: {:.2f}'\n     .format(gnb.score(x_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Machine"},{"metadata":{},"cell_type":"markdown","source":"Support Vector Machine (SVM), sono dei modelli di apprendimento supervisionato associati ad algoritmi di apprendimento per la regressione e la classificazione. Dato un insieme di esempi per l'addestramento, ognuno dei quali etichettato con la classe di appartenenza fra le due possibili classi, un algoritmo di addestramento per le SVM costruisce un modello che assegna i nuovi esempi a una delle due classi, ottenendo quindi un classificatore lineare binario non probabilistico. Un modello SVM è una rappresentazione degli esempi come punti nello spazio, mappati in modo tale che gli esempi appartenenti alle due diverse categorie siano chiaramente separati da uno spazio il più possibile ampio. I nuovi esempi sono quindi mappati nello stesso spazio e la predizione della categoria alla quale appartengono viene fatta sulla base del lato nel quale ricade.\n\nOltre alla classificazione lineare è possibile fare uso delle SVM per svolgere efficacemente la classificazione non lineare utilizzando il metodo kernel, mappando implicitamente i loro ingressi in uno spazio delle features multi-dimensionale.\n\nQuando gli esempi non sono etichettati è impossibile addestrare in modo supervisionato e si rende necessario l'apprendimento non supervisionato: questo approccio cerca d'identificare i naturali gruppi in cui si raggruppano i dati, mappando successivamente i nuovi dati nei gruppi ottenuti. L'algoritmo di raggruppamento a vettori di supporto, applica le statistiche dei vettori di supporto, sviluppate negli algoritmi delle SVM, per classificare dati non etichettati, ed è uno degli algoritmi di raggruppamento maggiormente utilizzato nelle applicazioni industriali.\n\nAl fine di testare i differenti Kernel disponibili per SVC, è stata implementata una GridSearch con l'ausilio di una 10-Fold Cross Validation.\n\nParametri testati:\n- <b>kernel</b>: specifica il tipo di kernel da utilizzare nell'algoritmo. I possibili valori sono:\n    - <b>linear</b>\n    - <b>poly</b>\n    - <b>rbg (default)</b>\n    - <b>sigmoid</b>\n    - <b>precoumputed</b>"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Grid search cross validation\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\nkernel_list = ['linear', 'poly', 'rbf', 'sigmoid']\n\ngrid={'kernel':kernel_list}\nsvm=SVC()\nsvm_cv=GridSearchCV(svm,grid,cv=10)\nsvm_cv.fit(x_train,y_train)\n\n#Stampa degli attributi best_score sul modello e i parametri con cui quel 'best_score' è stato ottenuto\nprint(\"GridSearch():\\n\")\ncombinazioni = 1\nfor x in grid.values():\n    combinazioni *= len(x)\nprint('Per l\\'applicazione della GridSearch ci sono {} combinazioni'.format(combinazioni))\nprint(\"Migliore configurazione: \",svm_cv.best_params_)\nbest_config_gs = svm_cv.best_params_\nprint(\"Accuracy CV:\",svm_cv.best_score_)\nsvm_be = svm_cv.best_estimator_\nprint('Test accuracy: %.3f' % svm_be.score(x_test, y_test))\n\n#RandomizedSearch\nn_iter_search = 20\nrandom_search = RandomizedSearchCV(svm, param_distributions=grid, n_iter=n_iter_search, cv=10, n_jobs =-1)\nrandom_search.fit(x_train,y_train)\n\n#Stampa degli attributi best_score sul modello e i parametri con cui quel 'best_score' è stato ottenuto\nprint(\"\\n\\nRandomizedSearch():\\n\")\nprint(\"Migliore configurazione: \",random_search.best_params_)\nbest_config_random_search = random_search.best_params_\nprint(\"Accuracy CV:\",random_search.best_score_)\n\nrandom_search = random_search.best_estimator_\nprint('Test accuracy: %.3f' % random_search.score(x_test, y_test))\n\nplot_accuracy(SVC, best_config_gs, best_config_random_search) #Stampo il grafico dell'accuratezza","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Matrice di confusione per K-nn"},{"metadata":{},"cell_type":"markdown","source":"Dopo aver visionato l'accuratezza di ciascun classificatore, è possibile affermare che il K-nearest Neighbor ha restituito la maggiore accuratezza rispetto a tutti i modelli di classificazione testati.\n\nPertanto, per esso è definita la matrice di confusione con le relative metriche di valutazione del modello."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\npred = knn_cv.predict(x_test)\n\nprint(confusion_matrix(y_test, pred))\nprint(classification_report(y_test, pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}