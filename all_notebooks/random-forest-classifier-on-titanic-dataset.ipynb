{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"All explanations are written from scikit website (https://scikit-learn.org/stable/index.html)","metadata":{}},{"cell_type":"markdown","source":"### Table of Contents\n##### 1. Let's look at the data\n##### 2. Replacing Null values\n##### 3. Deriving new parameters\n##### 4. Correlation plot\n##### 5. Comparision between different classifiers\n##### 6. Parameter Tuning\n##### 7. Conclusion","metadata":{}},{"cell_type":"markdown","source":"## Let's look at the data","metadata":{}},{"cell_type":"code","source":"#Importing basic libraries\nimport pandas as pd\nimport numpy as np\nimport re","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/titanic/train.csv\")\ntest_df = pd.read_csv(\"../input/titanic/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_df.columns.values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have 12 columns in the dataset","metadata":{}},{"cell_type":"code","source":"print(train_df.shape)\ntrain_df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_df.shape)\ntest_df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's get some information regarding our data","metadata":{}},{"cell_type":"code","source":"train_df.info()\nprint('_'*40)\ntest_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Following columns have missing values (number in brackets indicates no. of missing values):\n\nTrain dataset - Age(177), Cabin(687), Embarked(2)\n\nTest dataset - Age(86), Fare(1), Cabin(327)","metadata":{}},{"cell_type":"code","source":"train_df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let's create new variables for further use\ny_train = train_df['Survived']\nx_test = test_df\nx_train = train_df\nfull_data = [x_train, x_test]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Replacing Null values","metadata":{}},{"cell_type":"code","source":"#Replaceing 1 missing Fare value with median\nfor dataset in full_data:\n    dataset['Fare'] = dataset['Fare'].fillna(x_train['Fare'].median())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now we replace 2 missing Embarked values\nx_train[x_train['Embarked'].isnull()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We create a box plot to see the distribution of Embarked w.r.t Fare","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_style('darkgrid')\nfig, ax = plt.subplots(figsize=(16,12),ncols=2)\nax1 = sns.boxplot(x=\"Embarked\", y=\"Fare\", hue=\"Pclass\", data=x_train, ax = ax[0]);\nax2 = sns.boxplot(x=\"Embarked\", y=\"Fare\", hue=\"Pclass\", data=x_test, ax = ax[1]);\nax1.set_title(\"Training Set\", fontsize = 18)\nax2.set_title('Test Set',  fontsize = 18)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can observe that for Fare 80 the median of Embarked 'C' is closest","metadata":{}},{"cell_type":"code","source":"# Replacing the null values in the Embarked column with the median. \nx_train.Embarked.fillna(\"C\", inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We calculate the mean age and fill na values with values around one std deviation from the mean\nfor dataset in full_data:\n    age_avg = dataset['Age'].mean()\n    age_std = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's derive few new parameters","metadata":{}},{"cell_type":"code","source":"# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n# Create new feature IsAlone from FamilySize\nfor dataset in full_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dataset in full_data:\n    dataset['Has_Cabin'] = dataset[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define function to extract titles from passenger names\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n# Create a new feature Title, containing the titles of passenger names\nfor dataset in full_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)\n# Group all non-common titles into one single grouping \"Rare\"\nfor dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \n    # Mapping titles\n    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mapping Name length\nfor dataset in full_data:\n    dataset['Name_length'] = dataset['Name'].apply(len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Seperating Ticket Numbers and Letters\nfor dataset in full_data:\n    dataset['TicketNumbers'] = dataset.Ticket.apply(lambda x:int(x) if x.isnumeric() else 0 if x == 'LINE' else int(x.split(' ')[-1]))\n    dataset['TicketLetters'] = dataset.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.', '').replace('/', '').lower()  if len(x.split(' ')[:-1]) > 0 else x.lower() if x == 'LINE' else 'none')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mapping Embarked\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].map({'S': 0, 'C': 1, 'Q': 2}).astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mapping Sex\nfor dataset in full_data:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1}).astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_id = x_test['PassengerId']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's drop columns which are not required","metadata":{}},{"cell_type":"code","source":"drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin']\nfor dataset in full_data:\n    dataset.drop(drop_elements, axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlation plot between different parameters","metadata":{}},{"cell_type":"code","source":"x_train_plot = x_train.drop('TicketLetters', axis = 1)\ncolormap = plt.cm.RdBu\nplt.figure(figsize = (14, 12))\nplt.title('Pearson correlation of features', y = 1.05, size = 15)\nsns.heatmap(x_train_plot.astype(float).corr(), linewidths = 0.1, vmax = 1.0, square = True, cmap = colormap, linecolor = 'white', annot = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_test_cleaning = pd.concat([x_train, x_test], keys = ['train', 'test'], axis = 0)\ntrain_test_cleaning","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_test_cleaning = pd.get_dummies(train_test_cleaning)\ntrain_test_cleaning","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_test_cleaning.drop('Survived', axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = train_test_cleaning.loc['train']\nx_test = train_test_cleaning.loc['test']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Classification ","metadata":{}},{"cell_type":"markdown","source":"We test on 6 classification models and see which performs best on train dataset. Since our dataset consists of both categorical and numeric data, we use Ensemble models which are known to perform well on such a dataset. If you don't know what Ensemble models are, here's some info...","metadata":{}},{"cell_type":"markdown","source":"##### Ensemble Method of Classification\nThe goal of \"ensemble methods\" is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.\n\nTwo families of ensemble methods are usually distinguished:\n\nI. In \"averaging methods\", the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.\n\nExamples: Bagging methods, Forests of randomized trees, …\n\nII. By contrast, in \"boosting methods\", base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.\n\nExamples: AdaBoost, Gradient Tree Boosting, …","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score, cross_validate\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\n\n\nfrom sklearn.model_selection import GridSearchCV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = LogisticRegression()\ndt = DecisionTreeClassifier(random_state = 1)\nrf = RandomForestClassifier(random_state = 1)\nsvc = make_pipeline(StandardScaler(), SVC(probability = True))\nknn = make_pipeline(StandardScaler(), KNeighborsClassifier())\nexttree = ExtraTreesClassifier(random_state=1)\n\n\nestimators = [lr, dt, rf, svc, knn, exttree]\nlabels = ['Linear Regression', \n            'Decision Tree', \n            'Random Forest Classifier', \n            'SVC', \n            'k Nearest Neighbour',\n            'Extra Tree Classifier']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def estimate(x_train, y_train, estimators, labels):\n    df_result = pd.DataFrame()\n    \n    row_index = 0\n    for est, est_name in zip(estimators, labels):\n        cv_results = cross_validate(est, x_train, y_train, n_jobs = -1, cv = 10)\n        df_result.loc[row_index, 'Model name'] = est_name\n        df_result.loc[row_index, 'Test_accuracy'] = cv_results['test_score'].mean()\n        df_result.loc[row_index, 'Standard Deviation'] = cv_results['test_score'].std()\n        df_result.loc[row_index, 'Fit_time'] = cv_results['fit_time'].mean()\n        \n        row_index +=1\n        \n    df_result.sort_values(by=['Test_accuracy'], ascending = False, inplace = True, ignore_index = True)\n    \n    return df_result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"estimate(x_train, y_train, estimators, labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Random Forest performs the best among all classifiers. Here's some info on Random Forest Classifier --","metadata":{}},{"cell_type":"markdown","source":"#### Random Forest Classifier\nIn random forests, each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set.\n\nFurthermore, when splitting each node during the construction of a tree, the best split is found either from all input features or a random subset of size max_features.\n\nThe purpose of these two sources of randomness is to decrease the variance of the forest estimator. Indeed, individual decision trees typically exhibit high variance and tend to overfit. The injected randomness in forests yield decision trees with somewhat decoupled prediction errors. By taking an average of those predictions, some errors can cancel out. Random forests achieve a reduced variance by combining diverse trees, sometimes at the cost of a slight increase in bias. In practice the variance reduction is often significant hence yielding an overall better model.","metadata":{}},{"cell_type":"markdown","source":"### Parameter Tuning","metadata":{}},{"cell_type":"markdown","source":"We will tune the parameters with GridSearchCV. It performs an Exhaustive cross validation over the specified parameters and return the best parameters which maximize the score during Cross Validation. I ran my GridSearch on several values before nailing it down to the ones you can see.","metadata":{}},{"cell_type":"code","source":"rf_params = {'random_state': [1],\n             'max_depth': [16, 17, 18],\n             'max_features': [19, 20, 21],\n             'min_samples_leaf': [1,2],\n             'min_samples_split': [2, 3, 4, 5],\n             'n_estimators': [42,43,44]}\n\ngrid = GridSearchCV(rf, \n                    rf_params,\n                    cv = 10,   \n                    n_jobs = -1)\n\ngrid.fit(x_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf = RandomForestClassifier(**grid.best_params_)\n\ncv_results = cross_val_score(rf, x_train, y_train, n_jobs = -1, cv = 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'All results: {cv_results} \\n\\n' +\n      f'Mean: {cv_results.mean()} \\n\\n' +\n      f'Std: {cv_results.std()}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf.fit(x_train, y_train)\npredictions = rf.predict(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({'PassengerId': test_id,\n                           'Survived': predictions})\nsubmission.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion","metadata":{}},{"cell_type":"markdown","source":"There's still a lot I have in my mind which might increase the accuracy and the visualization. I will be updating this notebook in future.\n\nAppreciate any comments you might have which might help me make this better. Also if someone wants to team up with me feel free to drop me an email on keyurpethad1996@gmail.com.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}