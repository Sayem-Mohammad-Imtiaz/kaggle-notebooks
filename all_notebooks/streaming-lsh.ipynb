{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* > # EE226 - Coding 2\n## Streaming algorithm & Locality Sensitive Hashing","metadata":{}},{"cell_type":"markdown","source":"### Streaming: DGIM","metadata":{}},{"cell_type":"markdown","source":"DGIM is an efficient algorithm in processing large streams. When it's infeasible to store the flowing binary stream, DGIM can estimate the number of 1-bits in the window. In this coding, you're given the *stream_data.txt* (binary stream), and you need to implement the DGIM algorithm to count the number of 1-bits. Write code and ask the problems below.","metadata":{}},{"cell_type":"markdown","source":"### Your task","metadata":{}},{"cell_type":"markdown","source":"1. Set the window size to 1000, and count the number of 1-bits in the current window.","metadata":{}},{"cell_type":"code","source":"# Your code here, you can add cells if necessary\nimport time","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bucket_n=[]#桶的列表\n\nwindow = 1000\ntime_location = 5000\n#time_location = eval(input(\"请输入时刻：\"))\nmax_bucket = 2\n\ndef Is_due(time_now,bucket_n):\n    if len(bucket_n) > 0 and time_now - window == bucket_n[0]['timestamp']:\n        del bucket_n[0]\n    \ndef Merge(bucket_n):\n    for i in range(len(bucket_n)-1,max_bucket-1,-1):\n        if bucket_n[i]['bit_sum'] == bucket_n[i - max_bucket]['bit_sum']:\n            bucket_n[i - max_bucket]['bit_sum'] += bucket_n[i - max_bucket + 1]['bit_sum']\n            bucket_n[i - max_bucket]['timestamp'] = bucket_n[i - max_bucket + 1]['timestamp']\n            del bucket_n[i-max_bucket+1]\n\ndef Count_DGIM(bucket_n):\n    bit_sum = 0\n\n    with open(\"../input/coding2/stream_data.txt\",\"rt\") as f:\n        for i in range(time_location):\n            tmp = f.read(2)[0]\n            if tmp:\n                Is_due(i+1,bucket_n)#判断是否有桶到期\n                if tmp == '1':\n                    bucket={\"timestamp\":i+1,\"bit_sum\":1}#桶的结构\n                    bucket_n.append(bucket)\n                    Merge(bucket_n)#合并大小相同的桶\n    for i in range(len(bucket_n)):\n        bit_sum += bucket_n[i]['bit_sum']\n    bit_sum -= bucket_n[0]['bit_sum']/2\n    return bit_sum if len(bucket_n)>0 else 0\n\n#开始时间 \nstart_time = time.time()\n#程序运行\nbit_sum=Count_DGIM(bucket_n)\n#结束时间\nend_time = time.time()\n#计算运行时间\nrun_time = end_time - start_time \n\nprint(bit_sum,run_time)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Write a function that accurately counts the number of 1-bits in the current window, and compare the difference between its running time and space and the DGIM algorithm.","metadata":{}},{"cell_type":"code","source":"# Your code here, you can add cells if necessary\ndef Count():\n    bit_sum = 0\n    if time_location - window < 0:\n        start_location = 0\n    else:\n        start_location = time_location - window\n    with open(\"../input/coding2/stream_data.txt\",\"rt\") as f:\n        for i in range(time_location):\n            tmp = f.read(2)[0]\n            if i >= start_location and tmp == '1':\n                bit_sum += 1\n    return bit_sum\n\n    \n\n#开始时间 \nstart_time = time.time()\n#程序运行\nbit_sum=Count()\n#结束时间\nend_time = time.time()\n#计算运行时间\nrun_time = end_time - start_time \n\nprint(bit_sum,run_time)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Locality Sensitive Hashing","metadata":{}},{"cell_type":"markdown","source":"The locality sensitive hashing (LSH) algorithm is efficient in near-duplicate document detection. In this coding, you're given the *docs_for_lsh.csv*, where the documents are processed into set of k-shingles (k = 8, 9, 10). *docs_for_lsh.csv* contains 201 columns, where column 'doc_id' represents the unique id of each document, and from column '0' to column '199', each column represents a unique shingle. If a document contains a shingle ordered with **i**, then the corresponding row will have value 1 in column **'i'**, otherwise it's 0. You need to implement the LSH algorithm and ask the problems below.","metadata":{}},{"cell_type":"markdown","source":"### Your task","metadata":{}},{"cell_type":"markdown","source":"Use minhash algoirthm to create signature of each document, and find 'the most similar' documents under Jaccard similarity. \nParameters you need to determine:\n1) Length of signature (number of distinct minhash functions) *n*. Recommanded value: n > 20.\n\n2) Number of bands that divide the signature matrix *b*. Recommanded value: b > n // 10.","metadata":{}},{"cell_type":"markdown","source":"### read the data","metadata":{}},{"cell_type":"code","source":"# Your code here, you can add cells if necessary\nimport numpy as np\nimport pandas as pd\nfilepath = '../input/coding2/docs_for_lsh.csv'\ndata = pd.read_csv(filepath)\ndata = np.array(data)\ndata = data[:,1:]\nprint(data.shape)\nprint(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### get the Permutation π","metadata":{}},{"cell_type":"code","source":"# Length of signature (number of distinct minhash functions) n\nN = 100\n# Number of bands that divide the signature matrix b. \nb = 100\n# number of doc and col\ndoc = data.shape[0]\ncol = data.shape[1]\n# define the sequence [1:1000000]\nseq = np.arange(col)\n#define the h\npermutation_matrix = np.zeros((b,col))\nfor i in range(b):\n    permutation_matrix[i,:] = np.random.permutation(seq)\n\nprint(permutation_matrix)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Signature matrix M","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n# M 行为 hash函数的个数，列为文件数\nM = np.ones((b,doc))\nM = -1*M\n\n# 对于第i个hash序列，第j篇文章进行操作\n# 遍历col个k-shingles，第k个如果等于1，更新min_index\nfor i in tqdm(range(b)):\n    hi = permutation_matrix[i,:]\n    for j in range(doc):\n#        min_index = 300\n#         for k in range(col):\n#             if data[j,k] == 1:\n#                 if min_index > permutation_matrix[i,k]:\n#                     min_index = permutation_matrix[i,k]\n        data_j = data[j,:]\n        min_index = min(hi[data_j==1])\n        M[i,j] = min_index\nprint(M)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"the next code cell is reference to \n> https://github.com/guoziqingbupt/Locality-sensitive-hashing","metadata":{}},{"cell_type":"code","source":"import random\nimport hashlib\n\ndef minHash(sigMatrix, b, r):\n\n    hashBuckets = {}\n    \n    # begin and end of band row\n    begin, end = 0, r\n        \n    # count the number of band level\n    count = 0\n    doc = sigMatrix.shape[1]\n    while end <= sigMatrix.shape[0]:\n        print(end)\n        # traverse the column of sig matrix\n        for colNum in range(doc):\n\n            # generate the hash object, we used md5\n            hashObj = hashlib.md5()\n\n            # calculate the hash value\n            band = str(sigMatrix[begin: begin + r, colNum])\n            hashObj.update(band.encode())\n\n            # use hash value as bucket tag\n            tag = hashObj.hexdigest()\n\n            # update the dictionary\n            if tag not in hashBuckets:\n                hashBuckets[tag] = [colNum]\n            elif colNum not in hashBuckets[tag]:\n                hashBuckets[tag].append(colNum)\n        begin += r\n        end += r\n\n\n    # return a dictionary\n    return hashBuckets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"band = 20\nrow = 5\nhashBucket = minHash(M,band,row)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(hashBucket))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Problem: For document 0 (the one with id '0'), list the **30** most similar document ids (except document 0 itself). You can valid your results with the [sklearn.metrics.jaccard_score()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html) function.\n\nTips: You can adjust your parameters to hash the documents with similarity *s > 0.8* into the same bucket.","metadata":{}},{"cell_type":"code","source":"# Your code here, you can add cells if necessary\nfrom sklearn.metrics import jaccard_score\n\n#cnt表示中第i个元素表示id = i的文件和id = 0的文件中被哈希到相同bucket的band个数\ncnt = np.zeros(doc)\nprint(cnt.shape)\nqueryCol = 0\n#检查每一个哈希值，如果bucket中的hash值对应的集合中含有“0”，那么将该集合中的其他文章的计数+1\nfor key in hashBucket:\n    if queryCol in hashBucket[key]:\n        for i in hashBucket[key]:\n            cnt[i] += 1\n\n#排序找到cnt最大30个数的索引位置\nsorted_cnt = np.argsort(cnt)\n\n#sim_doc中保存文件id，sim中保存相似度\nsim_doc = []\nsim = []\n\nfor i in range(30):\n    index = sorted_cnt[doc-i-1]\n    tmp = jaccard_score(data[index],data[0])\n    sim_doc.append(index)\n    sim.append(tmp)\n    \nprint(sim_doc)\nprint(sim)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}