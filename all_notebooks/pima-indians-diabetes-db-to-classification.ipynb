{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"from mlxtend.plotting import plot_decision_regions\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly as py\nfrom plotly.offline import init_notebook_mode, iplot, plot\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a>\n# INTRODUCTION\n\n- Our aim in this kernel is to estimate whether a patient has diabetes using the data set. By guessing with Logistic Regression, we will reach 0 or 1 class labels.\n\n\n    Bu kenelde, pima-indians-diabetes data seti ile bazı sınıfandırma algoritmaları gerçekleştirilmiştir.\n    Pregnancies: Gebelik Sayısı\n    Glucose: Oral glukoz tolerans testinde glikoz konsantrasyonu değeri.\n    BloogPressure: Kan Değeri(mm Hg)\n    SkinThickness: Cilt Kalınlığı(mm)\n    Insulin: 2 saatlik serum insulini(mu U/ml)\n    BMI:  Vücut Kütle indeksi\n    DiabetesPedigreeFunction: Diyabet soyağacı işlevi\n    Age: Yaş\n    Outcome: Diyabet olup olmaması 1 veya 0\n    \n<b> Content: </b>\n\n1. [Data Reading and Data Pre-Processing](#1) <br>\n    1.1. [Exploratory Data Analysis (EDA)](#1.1) <br>\n    1.2. [Data Cleaning](#1.2) <br> \n    1.3.  [Data Normalization](#1.3) <br>\n2. [Train Test Split Data](#2) <br>\n3. [Machine Learning Classifiers and Model Performance Analysis](#3)<br>\n    3.1. [Confusion Matrix and Classification Report ?](#3.1) <br>\n    3.2. [K Neighhbors Classifier](#3.2) <br>\n    3.3. [Decision Tree Classifier](#3.3) <br>\n    3.4. [Random Forest Classifier](#3.4) <br>\n    3.5. [Support Vector Machine Classifier](#3.5) <br>\n    \n    *3.5.1. [Linear SVM](#3.5.1) <br>\n    *3.5.2. [Polynomial Kernel SVM](#3.5.2) <br>\n    *3.5.3. [Gaussian (Rbf) Kernel SVM](#3.5.3) <br>\n    *3.5.4. [Sigmoid Kernel SVM](#3.5.4)    \n    \n   3.6. [Naive Bayes Classifier](#3.6) <br>\n   3.7. [Logistic Regression Classifier](#3.7) - For More (https://www.kaggle.com/kursatkaragoz29/pima-indians-diabetes-db-to-logistic-regression) <br>\n4. [Hyperparameter Tuning and Cross Validation](#4)<br>\n    4.1. [Hyper Parameter](#4.1)<br>\n    4.2. [Cross Validation - K Fold Cv](#4.2)<br>\n    4.3. [Cross Validation - LOO CV](#4.3)<br>\n    4.4. [Grid Search - CV - KNN](#4.4)<br>\n    4.5. [Grid Search - CV - Decision Tree](#4.5)<br>\n    4.6. [Grid Search - CV - Random Forest](#4.6)<br>\n    4.7. [Grid Search - CV - Linear SVM](#4.7)<br>\n    4.8. [Grid Search - CV - Rbf (Gaussian) SVM](#4.8)<br>\n    4.9. [Grid Search - CV - Polynomial SVM](#4.9)<br>\n    4.10. [Grid Search - CV - Sigmoid SVM](#4.10)<br>\n5. [Conclusion](#5)\n\n"},{"metadata":{},"cell_type":"markdown","source":"<a id='1'></a>\n# 1. Data Reading and Data Pre-Processing\n\n\n\n This phase includes operations such as reading the data set, checking the missing value, checking the type of properties, the properties of the records in the data set, clearing the missing values ​​in the data set, and several visualizations to better interpret the data set.\n\n--------------\n                                                                            TR\n---------\nBu aşama veri setinin okunması, kayıp değer kontrolü, özelliklerin tür kontrolü, veri seti içerisindeki kayıtların özellikleri, veri setinde bulunan kayıp değerlerin temizlenmesi ve veri setini daha iyi yorumlamak için birkaç görselleştirme gibi işlemler içermektedir."},{"metadata":{},"cell_type":"markdown","source":"<a id=1.1></a>\n## 1.1. Exploratory Data Analysis (EDA)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#data read\ndata = pd.read_csv(\"/kaggle/input/pima-indians-diabetes-database/diabetes.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape # sample,feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndata.head() #first 5 data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n        missing value does not appear in the data.\n        It consists of 768 sample and 9 features. \n        The class label of the dataset is the feature Outcome.\n        The forecasting process we will do will be on Outcome."},{"metadata":{},"cell_type":"markdown","source":"## describe()\n\n\n* How many records belong to each attribute (count),\n* Mean value of the records in each attribute (mean),\n* standard deviations, i.e. divergence value of each value,\n* It is the method that outputs 25%, 50% (median), 75% percentiles (percentile).\n\n* 25% percentile, 25% and less of the values ​​in Pregnancies have a value of 1 and below. It allows us to comment.\n* Similarly, 50% and less of the Glucose features of the dataset have a value of 117 or less. It also corresponds to the median value of the data set with 50% percentile. In other words, it is the value that divides the data set into two in a ordered manner.\n* With the same logic, 75% and less of the records of the BloodPressure feature have 32 and lower values.\n-------------------------------------------------------------------------------------\n*     Her bir özniteliğe ait kaç kayıt yer almakta (count),\n*     Her bir öznitelikte yer alan kayıtların ortalama değeri(mean),\n*     standart sapmaları, yani her bir değerin ortalamadan uzaklaşma değeri,\n*     %25,%50(median),%75 lik yüzdelikler(percentile) gibi çıktıları veren methoddur.\n\n*     %25 percentile, Pregnancies deki değerlerin %25 ve daha azı 1 değeri ve aşağısına sahiptir. Yorumunu yapmamızı sağlar.\n*     Aynı şekilde veri setinin Glucose featuresinin %50 ve daha azı 117 ve daha az değere sahiptir. Aynı zamanda %50 percentile veri setinin ortanca değerine(median) denk gelmektedir. Yani veri setini sıralanmış bir biçimde ikiye ayıran değerdir.\n*     Yine aynı mantıkla BloodPressure featuresine ait kayıtların %75 ve daha azı 32 ve aşağı değere sahiptir.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe().T # Changed rows and columns. (T)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    \n    When we examine the data.desribe () output, it is seen that the minimum values ​​of each feature are 0.\n    This may be a coincidence, but it is likely that null data is entered as 0. What we need to do is find the number of zeros in each attribute and fill in these 0 values ​​with another value (mean, median, etc.) depending on the number of numbers.\n    \n    Obviously, when we look at the percentages (percentile (25.50)), a large number of 0s appear in feathers.\n    We have to fix them.\n    --------------------------------------------------TR-------------------------------------------\n    data.desribe() çıktısını incelediğimizde her featurenin minimum değerlerinin 0 olduğu görülmektedir.\n    Bunun bir tesadüf olması yüksek olabileceği gibi null verilerin 0 olarak girilmiş olmasıda muhtemeldir. Yapmamız gereken işlem her bir öznitelikteki sıfır sayısını bulup gerektiği taktirde, sayının çokluğuna göre bu 0 değerlerini başka bir değerle (mean,median vb.) doldurmamızdır.\n    \n    Açıkcası yüzdeliklere baktığımızda(percentile(25,50)) featurelerde fazla sayıda 0 görünmektedir.\n    Bunları düzeltmeliyiz."},{"metadata":{},"cell_type":"markdown","source":"<a id=1.2></a>\n## 1.2. Data Cleaning\n\n* Missing Value significantly affects analysis results and success rates of models. These lost values ​​must be eliminated.\n* Some methods I can count to eliminate lost values ​​are:\n   * With a constant value for all null values.\n   * completely remove the relevant sheet from the dataset\n   * With the mean value of the corresponding feature\n   * With the median value of the relevant fetur\n   * Estimation of lost values ​​can be provided by establishing a regression or decision tree model for missing values.\n* By looking at the histogram of each featurer, we tried to conclude how to replace null values.\n* Since the distribution of BloodPressure and Glucose feathers seemed \"Normal\" and close to normal, I chose to fill each null value with the mean value of the relevant featurer.\n* I chose to fill the median values ​​of the related feathers for the Null values ​​of the other feathers.\n    \n------------------------------------------------------------------------------------------------------------------------------------------------\n\n                                                                TR\n* Kayıp değerler (Missing Value), analiz sonuçlarını ve modellerin başarı oranlarını ciddi miktarda etkiler. Bu kayıp değerlerin giderilmesi gerekir.\n* Kayıp değerleri gidermek için sayabileceğim bazı yöntemler şunlardır:\n  *  Bütün null değerleri için bir sabit değer ile.\n  *  ilgili samplenin tamamen veri setinden çıkarılması\n  *  İlgili Featurenin mean değeri ile\n  *  İlgili feturenin median değeri ile\n  *   Kayıp değerler için bir regresyon yada karar ağacı modeli kurularak kayıp değerlerin tahmini sağlanabilir. \n* Her bir featurenin histogramına bakarak, null değerleri nasıl replace edeceğimiz hakkında bir kanıya varmaya çalıştık\n* BloodPressure ve Glucose featurelerinin dağılımı \"Normal\" ve normale yakın göründüğü için her bir null değeri ilgili featurenin mean değeri ile doldurmayı tercih ettim.\n* Diğer featurelerin Null değerleri için ilgili featurelerin median değerleri doldurmayı tercih ettim. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of rows with Glucose == 0 => {}\".format((data.Glucose==0).sum()))\nprint(\"Number of rows with BloodPressure == 0 => {}\".format((data.BloodPressure==0).sum()))\nprint(\"Number of rows with SkinThickness == 0 => {}\".format((data.SkinThickness==0).sum()))\nprint(\"Number of rows with Insulin == 0 => {}\".format((data.Insulin==0).sum()))\nprint(\"Number of rows with BMI == 0 => {}\".format((data.BMI==0).sum()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    Featurelerde yer alan \"0\" değerlerini \"NAN (null)\" olarak kabül ettik.\n    0 değerleri null olarak replace edildi.\n\n    0 values in features are accepted as null values ​​and 0 values ​​are replaced with null"},{"metadata":{"trusted":true},"cell_type":"code","source":"#datayı bozmadan kopyaladık ve bağlılığı kaldırdık.\ndata_copy = data.copy(deep=True) \n\n#Featurelerdeki  0 valueleri Nan value yapalım.\ndata_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']]= data_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)\n\nprint(data_copy.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_copy.hist(figsize=(15,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Featurelerdeki missing valueleri mean ve median değerleri ile doldurduk.\n#We filled missing values in features with mean and median values\n\ndata_copy.BloodPressure.fillna(data_copy.BloodPressure.mean(),inplace=True)\ndata_copy.Glucose.fillna(data_copy.Glucose.mean(),inplace=True)\ndata_copy.SkinThickness.fillna(data_copy.SkinThickness.median(),inplace=True)\ndata_copy.Insulin.fillna(data_copy.Insulin.median(),inplace=True)\ndata_copy.BMI.fillna(data_copy.BMI.median(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_copy.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_copy.hist(figsize=(15,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=1.3></a>\n## 1.3. Data Normalization (X={0...1})\n    \n    The distance between the values ​​in the data is important. Many different values ​​can destroy added values.\n    Very small or very large values ​​are accepted as outliers. It affects system performance.\n    Considering two different features, one can contain values ​​from 1 to 10, while the other feature can contain values ​​from 10,000 and above.\n    In this case, the feature with large values ​​can manipulate small-value feaures and decrease the number of floors.\n    This causes poor performance.\n    By normalizing all the data, it is possible to draw, reduce, convert to the range of 0 to 1.\n    \n    -------------------------------------------------------------------------------------------------------------------------------------------\n                                                                TR\n    \n    Veri içerisindeki değerlerin birbirlerine uzaklığı önemlidir. Birbirinden çok farklı değerler katma değerlerini yok edebilir.\n    Çok küçük veya çok büyük değerler aykırı değerler olarak kabül edilir. Sistem performansını etkilemesi söz konusudur.\n    İki farklı featureyi düşünürsek birisi 1 ila 10 arasında değerler barındırırken, diğer feature 10.000 ve yukarısı değer barındırabilir.\n    Bu durumda büyük değerlerin yer aldığı feature küçük değerli feaureyi manipule edebilir, kat sayısını düşürebilir. \n    Bu durum performans düşüklüğüne yol açar.\n    Bütün verileri normalize ederek 0 ila 1 aralığına çekmek,indirgemek,dönüştürmek mümkündür.\n    \n    A = [5,10,100,250,50] bir feature düşünelim.\n    A featuresini normalize etmek istediğimiz izleyeceğimiz formül aşağıdadır.\n    A[i] Normalize değer = (gerçek değer) - (featurenin minimum değeri)         /  (featurenin maksimum değeri)-(featurenin minimum değeri)\n    A[0] = (5-5) / 250-5 = 0\n    A[1] = (10-5) / (250-5) = 0.020\n    A[2] = (100-5) / (250-5) = 0.380\n    A[3] = (250-5) / (250-5) = 1\n    A[4] = (50-5) / (25-5) = 0.14\n    \n    A(norm)= [0,0.020,0.38,1,0.14]\n    "},{"metadata":{},"cell_type":"markdown","source":"* Formula of Min-Max Normalization \n\n![Min-Max Normalization](https://miro.medium.com/max/506/1*Dl3P3Rrzto258X0Ales9Xw.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_copy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Normalize\n#Yöntem 1(Medhod 1)\n#from sklearn.preprocessing import MinMaxScaler\n#norm = MinMaxScaler()\n#x = pd.DataFrame(norm.fit_transform(data_copy.drop(['Outcome'],axis=1)),\n#                 columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n#       'BMI', 'DiabetesPedigreeFunction', 'Age'])\n#x.head()\n\n#Yöntem 2(Method 2)\ny=  data_copy['Outcome'].values.reshape(-1,1)  #Dependent Features (Class)\nx_data =data_copy.drop(['Outcome'],axis=1)     #Independent Features\nx = (x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data))   #normalized\nx.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=2></a>\n# 2. Train Test Split\n\n    The train_test_split method is a method in the model_selection module.\n    Two data sets are required to be used in the development and testing of models.\n    These datasets can be derived from the dataset that they are intended to use in the development of the model or they may be a different dataset.\n    With the train_test_split method, a data set can be separated into a train and test data set at the desired rate.\n    X, which includes the previously separated samples, y, which indicates which classes the sample belongs to, is given as a parameter in this method.\n    According to the percentage given to the test_size parameter, this method performs partitioning.\n    \n    x_train: x data to be used in education (70%)  y_train: y data to be used in education (class labels of x_train)\n    x_test: x data to be used in the test (30%)    y_test: y data to be used in the test (class labels of x_test)\n    \n    test_size = 0.3: We said that 30% of the data set is used for training and 70% for the test.\n    \n    The random_state parameter is: The measurement of the performance of the models requires constant processing with the same data.\n    Otherwise, the result for any model will give a different result when the same model is run again. Because train_test_split\n    The method takes the percentile given to the test_size parameter from different parts of the data set each time.\n    Here is the random_state parameter that will make this partitioning process from a fixed part.\n    The value given to this parameter is not important. This method stores the parameter value in its memory and when the model is run,\n    value is the random_stte parameter. If the parameter matches, it processes with the same data set.\n\n------------------------------------------------------------------------------------------------------------------------\n\n    train_test_split methodu model_selection modulü içerisinde yer alan bir methoddur.\n    Modellerin geliştirilmesinde ve test edilmesinde kullanılmak üzere iki veri seti gerekir.\n    Bu veri setleri modelin geliştirilmesinde kullanması hedeflenen veri setinden türetilebileceği gibi farklı bir veri seti de olabilir.\n    train_test_split methodu ile bir veri seti istenilen oranda train ve test veri seti olmak üzere ayrılabilir.\n    Daha önceden ayrılmış örneklemleri içeren x, örneklemin hangi sınıflara ait olduğunu ifade eden y, bu methoda parametre olarak verilir.\n    test_size parametresine verilen yüzdeliğe göre bu method bölümleme işlemi yapar.\n    \n    x_train: Eğitimde kullanılacak x verisi (%70)         y_train: Eğitimde kullanılacak y verisi (x_train'in sınıf etiketleri)\n    x_test : Testte kullanılacak x verisi (%30)           y_test: Testte kullanılacak y verisi (x_test'in sınıf etiketleri)\n    \n    test_size =0.3 : Veri setinin %30'unu test içn %70 ini eğitim için kullan demiş olduk.\n    \n    random_state parametresi ise: Modellerin performanslarının ölçümünde sabit olarak aynı veriler ile işlem yapılması gerekir.\n    Aksi taktirde herhangi bir model için alınan sonuç tekrar aynı model koşturulduğunda farklı sonucu verecektir. Çünkü train_test_split\n    methodu her seferinde test_size parametresine verilen yüzdelik bölümü, veri setinin farklı kısımlarından almaktadır.\n    İşte bu bölümleme işlemini sabit bir kısımdan yapılmasını sağlayacak olan ise random_state parametresidir.\n    Bu parametreye verilen değerin önemi yoktur. Bu method parametre değerini hafızasında saklar ve model koşturulduğunda ilk baktığı\n    değer random_stte parametresidir. Parametrenin eşleşmesi halinde aynı veri seti ile işlem yapar."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=29)\nx_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=3></a>\n# 3. Machine Learning Classifiers and Model Performance Analysis\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=3.1></a>\n## 3.1. Confusion Matrix and Classification Report\n    \n* Confusion matrix is used to summarize the examination of the classification algorithm\n* Unlike MSE, MAE, RMAE techniques, detailed error matrix is obtained and analyzed better.\n\n* Well, it is a performance measurement for machine learning classification problem where output can be two or more classes. It is a table with 4 different combinations of predicted and actual values.    \n\n*Karışıklık Matrisi, herhangi bir sınıflandırma algoritmasının performansını özetlemek için kullanılan bir tekniktir.\n*MSE,MAE,RMAE gibi tekniklerden farklı olarak detaylı hata matrisi elde edilir ve daha iyi analiz gerçekleştirilir. \n\n![](https://i.hizliresim.com/3c7NdN.png)\n\n\n    True Positive: You predicted positive and it’s true.\n    True Positive: You predicted negative and it’s true.\n    False Positive(Type 1 Error): You predicted positive and it’s false.\n    False Negative(Type 2 Error): You predicted negative and it’s false.\n\n** If 0 negative 1 positive inside **:\n\n<center>![](https://i.hizliresim.com/noxyWA.png)</center>\n\n**Recall, Precision, Acc?**\n\n- Acc: Accuracy rate\n- Recall: How many were known in the test? (Actually, the rate of correctly identifying those who have Diabetes?)\n- Precision: How many of the existing elements did he know? (How many of our diabetes patients are sick?)\n----------------------------------------------\n- Acc : Doğruluk oranı\n- Recall: Gerçekleşen testte, kaçta kaçı bilindi ?     (Gerçekte Diyabet Hasta olanları doğru tespit etme oranı ?)\n- Precision: Var olan elemanlarda kaçta kaçını bildi ? (Diyabet hastası dediklerimizin gerçekten kaçı hasta ?)\n\n\n**Why use Confusion Matrix ?**\n\n\n    Classification successful high accuracy.\n    It is among the data set. In the dataset,\n    All 99000 cats were correctly estimated but none of the 1000 dogs were correctly predicted, so the accuracy was 99%.\n    But as he seems, he has not guessed any dog correctly!\n\n    Sınıflandırma sonucunda yüksek başarı oranı her zaman başarılı bir modeli doğrulamaz.\n    Veri seti içerisinde verilerin dağılımı da önemlidir. Örneğin kedi ve köpeklerin sınıf etiketi olduğu bir veri setinde,\n    99000 kedinin hepsi doğru tahmin edilmiş fakat 1000 köpeğin hiçbiri doğru tahmin edilmemiş bu durumda başarı oranı %99 çıkabilir.\n    Fakat göründüğü gibi hiç bir köpeği doğru tahmin edememiş !\n\n<center>![](https://i.hizliresim.com/eLEhbN.png)</center>\n\n\n**Classification Report**\n\n\n![](https://i.hizliresim.com/QRECj7.png)\n\n\n![](https://i.hizliresim.com/LRn3AR.png)\n\n\n*  **For Reference : https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62** \n*  **https://www.udemy.com/user/datai-team/**"},{"metadata":{},"cell_type":"markdown","source":"<a id=3.2></a>\n# 3.2. Classification with KNN (K Nearest Neighborhood)\n\n**KNN Algorithm Steps:**\n1. Determine the K parameter.\n2. Calculate the distance between the point (x_test) to which the class is to be determined and all points (x_train).\n3. Sort the calculated distances and choose the smallest k.\n4. Look at the class labels of the selected k observations, the most repeated class category is the new class category.\n------------------------------------------------------------------------------------------------------------------------\n**KNN Algoritması Adımları:**\n1. K parametresini belirle.\n2. Sınıfı belirlenmesi istenen nokta (x_test) ile tüm noktalar(x_train) arasındaki uzaklığı hesapla.\n3. Hesaplanan uzaklıkları sırala ve en küçük k tanesini seç.\n4. Seçilen k tane gözlemin sınıf kategorilerine (class label) bak, en çok tekrarlanan sınıf kategorisi yeni sınıf kategorisidir.\n\n\n<center>![](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1531424125/KNN_final1_ibdm8a.png)</center>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\ntrain_score_list=[]\ntest_score_list=[]\n\nfor each in range (2,31):\n    knn=KNeighborsClassifier(n_neighbors=each)\n    knn.fit(x_train,y_train)\n    test_score_list.append(knn.score(x_test,y_test)*100)\n    train_score_list.append(knn.score(x_train,y_train)*100)\n\n    \nprint(\"Best accuracy(test) is {:.3f} % with K = {}\".format(np.max(test_score_list),test_score_list.index(np.max(test_score_list))+ 2))\nprint(\"Best accuracy(train) is {:.3f}% with K = {}\".format(np.max(train_score_list),train_score_list.index(np.max(train_score_list))+2))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Result Visualization\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objs as go\n\narange=np.arange(2,31)\ntrace1=go.Scatter(\n    x=arange,\n    y=train_score_list,\n    mode=\"lines + markers\",\n    name=\"Train_Score\",\n    marker=dict(color = 'rgba(16, 112, 2, 0.8)'),\n    \n)\ntrace2=go.Scatter(\n    x=arange,\n    y=test_score_list,\n    mode=\"lines + markers\",\n    name=\"Test_Score\",\n    marker=dict(color = 'rgba(80, 26, 80, 0.8)'),\n)\ndata=[trace1,trace2]\nlayout = dict(title = '-value VS Accuracy',\n              xaxis= dict(title= 'K Value',ticklen= 5,zeroline= False),\n              yaxis= dict(title= 'Score',ticklen= 5,zeroline= False)\n             )\nfig = dict(data = data, layout = layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2. Knn Model Performance Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import confusion_matrix\nfrom sklearn.metrics import confusion_matrix\n\nk_value =test_score_list.index(np.max(test_score_list))+ 2   # K paramer\nknn2 = KNeighborsClassifier(n_neighbors=k_value)\nknn2.fit(x_train,y_train)\ny_pred = knn2.predict(x_test)\ny_true = y_test\n\n# 0 value is negative\n# 1 value is positive\ncmatrix = confusion_matrix(y_true,y_pred,labels=[0,1])\n\nf,ax = plt.subplots(figsize=(8,4))\nsns.heatmap(cmatrix,annot=True,linewidths=0.5,cbar=True,linecolor=\"white\",fmt='.0f',ax=ax)\nplt.title(\"Model's Confusion Matrix\")\nplt.xlabel(\"y_predict (Predicted Label)\")\nplt.ylabel(\"y_true (Actual Label)\")\nplt.show()\n\n#1=\"Pozitif\"\n#0=\"Negatif\"\n\nprint('True negative = ', cmatrix[0][0])\nprint('False positive = ', cmatrix[0][1])\nprint('False negative = ', cmatrix[1][0])\nprint('True positive = ', cmatrix[1][1])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n- In reality, the number of patients without diabetes is 149. He estimated 16 of them as diabetes, and 133 of them correctly predicted.\n- The number of patients with diabetes is 82. He predicted 48 correctly, 34 incorrectly (not diabetic)\n- The rate of knowing those with diabetes is higher than the rate of knowing non-patients.\n- So they actually have a higher rate than they know 0 and 0, actually 1 and 1\n-------------------------------------------------------------------------------------------------------\n- 149 diyabet olmayan hastanın 133'ünü doğru bilmiş 16'sını yanlış bilmiş yani diyabetmiş gibi bulmuş.\n- 82 diyabet hastasının 34 tanesini diyabet değil olarak bilmiş ve 48 tanesini doğru bilmiş.\n- Yapılacak çıkarım şudur ki diyabet hastası olanları bilme oranı, hasta olmayanları bilme oranına göre daha yüksektir.\n- Yani gerçekte 0 olup 0 bildikleri, gerçekte 1 olup 1 bildiklerinden daha yüksek orana sahiptir"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_true,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> Precision </b>\n\n- 80% of what we estimate as \"not diabetic\" is not really sick.                                        ==> p0\n- 75% of what we estimate as \"diabetic\" is really sick.                                                ==> p1\n\n        NOTE: It allows us to think carefully before calling someone sick.\n\n- We actually estimated 89% of those who were not sick. In other words, we said 89% of them \"not sick\".==> R0\n- We actually estimated 59% of those who were sick. So we said 59% of you are \"sick\". ==> R1\n\n<b> Recall </b>\n\n        NOTE: It is more important to correctly detect some anomaly cases than to produce false alarms. In other words, false negative is more critical than false positivity. Rather than being able to detect someone with cancer and cause death, it is more acceptable to make a false estimate and call him to the hospital.\n        \n<b>F1 SCORE:</b>\n\n- As you can see, recall and precision have two important metrics and a trade-off between them. To cope with this, the F1-score is used. The F1-score uses the harmonic average instead of the arithmetic mean to punish extreme situations.        \n\n--------------------------------------------------------------------------------------\n--------------------------------------------------------------------------------------\n<b> TR </b>\n\n<b> Precision (Kesinlik) </b>\n\n- \"Diyabet hastası değil\" olarak tahmin ettiklerimizin %80'i gerçekten hasta değil.                     ==>p0\n- \"Diyabet hastası\" olarak tahmin ettiklerimizin %75'i gerçekten hasta.                                 ==>p1\n\n         NOT:Birine hasta demeden önce iyice düşünüp taşınmamızı sağlıyor.\n         \n<b>  Recision </b>\n\n- Gerçekte hasta olmayanların %89'unu doğru tahmin ettik. Yani %89'una \"hasta değil\" dedik.             ==>R0\n- Gerçekte hasta olanların %59'unu doğru tahmin ettik. Yani %59'una \"hastasın\" dedik.                   ==>R1\n\n        NOT :Bazı anomali vakalarını doğru tespit etmek yanlış alarm üretmekten daha önemli. Diğer bir deyişle false negative false positiveden daha kritik. Kanserli birini tespit edemeyip ölümüne neden olmaktansa kanser olmayan biri için yanlış tahmin yapıp onu hastaneye çağırmak daha kabul edilebilir.\n\n\n<b>F1 SCORE:</b>\n\n\n- As you can see, recall and precision have two important metrics and a trade-off between them. To cope with this, the F1-score is used. The F1-score uses the harmonic average instead of the arithmetic mean to punish extreme situations.\n------------------------------------------------------------------------------------------------------\n- Gördüğünüz gibi recall ve precision iki önemli metrik ve aralarında bir trade-off var. Bununla baş edebilmek için F1-skoru kullanılıyor. F1-skoru ekstrem durumları cezalandırmak için aritmetik ortalama yerine harmonik ortalamayı kullanıyor.\n"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<a id=3.3></a>\n# 3.3. Classification with Decision Tree\n\n* Each attribute is represented by a node.\nThe branches and leaves are the tree structure.\nThe last structure is called \"leaf\", the top structure is called \"root\" and the structures between them are called \"branch\".\n\n* Algorithms such as Twoing and Gini have been developed for Classification and Regression trees.\nMany algorithms have been developed by Qinlan for classification processes with decision trees.\nSome of these are the ID3 and C4.5 algorithms.\n\n* The measure of uncertainty in a system is called \"entropy\". Let S be a resource. Let's say that this resource can produce n messages: {m1, m2, ... mn}.\nAll messages are independent of each other and the probability that mi messages are produced is pi. The entropy H (S) of the S source that produces the messages with probability distribution P = {p1, p2 ... pn} is as follows.\n\n![Entropi](https://i.hizliresim.com/lHKfya.png)\n\n\n### Entropy, Feature Selection and Gain criteria?\n\n* If we consider the values ​​that the class variable (T) can take in a data set, let's assume that the record set is {C1, C2, C3..CK}.\nIn other words, there are k types of values ​​(class label, class category) in the class variable.\nIn this case, the Entropy of the T class is calculated as follows.\n\n* First, P (T), that is, the probability distribution of classes must be calculated.\n\n![olaslık dağılımı](https://i.hizliresim.com/GKLial.png)\n\n- examle\n\n        risk = { blue,blue,blue,purple,blue,purple,purple,blue,blue,purple}\n        \n        Probability distribution for these attribute values: C1 = blue, C2 = purple.\n            [C1] = 6\n            [C2] = 4\n            probabilities are calculated as p1 = 6/10, p2 = 4/10.\n\n            So the probability distribution = P (risk) = (6/10, 4/10).\n\n            Entropy => H (risk) = - [6 / 10.log2.6 / 10 + 4 / 10.log2.4 / 10] = 0.97.\n            \n** Selecting Attributes for Branching and Boiler Criterion **\n\n* If T, which expresses the target attribute, is divided into T1, T2 .. Tn al clusters depending on the value of an X attribute without a target attribute (i.e. not a class attribute),\nThe information required to determine the class of an element of T is considered the weighted average of the information required to determine the class of an element of Ti.\nSo, depending on this definition, the information required to determine the class of an element of T is calculated as follows.\n\n![](https://i.hizliresim.com/n69rYU.png)\n<br>\n\n* In order to measure the information obtained as a result of dividing the T set by X test, the expression called \"gain criterion\" is used.\nThe gain criterion is calculated as follows.\n\n** Gain (X, T) = H (T) - H (X, T) **\n\n** It is aimed to maximize (maximize) the value of earnings while making the separation process in decision trees.\nThe X attribute is selected, which provides the highest information gain, that is, to maximize the gain. **\n\n\n        ------------------------------------------------------------------------------------------------------------------------------\n<center> TR </center><br>\n\n\n\n* Her bir öznitelik bir düğüm tarafından temsil edilir.\nDallar ve yapraklar ağaç yapısının elemanlarıdır.\nEn son yapı \"yaprak\" olarak, en üst yapı\"kök\" ve bunların arasında kalan yapılar ise \"dal\" olarak isimlendirilir.\n\n* Sınıflandırma ve Regresyon ağaçları konusunda Twoing, Gini gibi algoritmalar geliştirilmiştir.\nKarar ağaçları ile sınıflandırma işlemleri için Qinlan tarafından birçok algoritma geliştirilmiştir.\nBunlar arasında yer alan ID3 ve C4.5 algoritmalarından bazılarıdır.\n\n* Bir sistemdeki belirsizliğin öl.üsüne \"entropi\" adı verilir. S bir kaynak olsun. Bu kaynağın {m1,m2,...mn} olmak üzere n mesaj üretebildiğini varsayalım. \nTüm mesajlar birbirinden bağımsızdır ve mi mesajlarının üretilme olasılıkları pi'dir. P={p1,p2...pn} olasılık dağılımına sahip mesajları üreten S kaynağın entropisi H(S) şu şekildedir.\n\n![Entropi](https://i.hizliresim.com/lHKfya.png)\n\n### Entropi, Öznitelik Seçilmesi ve Kazanç ölçütü ?\n\n* Bir veri setinde sınıf değişkeninin (T) alabileceği değerleri düşünürsek,kayıt kümesinin {C1,C2,C3..CK} şeklinde olduğunu varsayalım.\nYani sınıf değişkeninde k türde değer (class label, class category) mevcut olsun.\nBu durumda T sınıfının Entropisi şu şekilde hesaplanır.\n\n* İlk olarak P(T) yani sınıfların olasılık dağılımının hesaplanması gerekir.\n\n![olaslık dağılımı](https://i.hizliresim.com/GKLial.png)\n\n* [Ci] ifadesi Ci kümesindeki elemanların sayısını vermektedir. Burada örneğin Pi = |Ci| / |T| olasılığını ifade etmektedir.\nO halde başka bir deyişle T içn ortalama bilgi miktarı yani entropi şu şekilde ifade edilir.\n\n![entropi T](https://i.hizliresim.com/ebEAUJ.png)\n\nörnek\n\n    risk = { mavi,mavi,mavi,mor,mavi,mor,mor,mavi,mavi,mor}\n    \n    Bu öznitelik değerleri için olasılıklar dağılımı: C1=Mavi, C2=Mor olsun.\n    [C1] = 6\n    [C2] = 4\n    olduğuna göre, olasılıklar p1=6/10, p2=4/10 biçiminde hesaplanır.\n    \n    O halde olasılıklar dağılımı = P(risk) = (6/10, 4/10).\n    \n    Entropi =>  H(risk) = -[ 6/10.log2.6/10  +  4/10.log2.4/10] = 0.97.\n\n** Dallanma için Özniteliklerin Seçilmesi ve Kazan Ölçütü **\n\n* Hedef niteliğini ifade eden T, hedef niteliği olmayan (yani sınıf niteliği olmayan) bir X niteliğinin değerine bağlı olarak T1,T2 .. Tn al kümelerine ayrılırsa,\nT nin bir elemanının sınıfını belirlemek için gerekli bilgi Ti nin bir elemanının sınıfının belirlenmesinde gerekli olan bilginin ağırlıklı ortalaması olarak kabul edilir.\nO halde, bu tanıma bağlı olarak T nin bir elemanının sınıfını belirlemek için gerekli bilgi şu şekilde hesaplanır.\n\n![](https://i.hizliresim.com/n69rYU.png)\n<br><br>\n\n* T kümesinin X testine göre bölünmesi sonucunda elde edilen bilgiyi ölçmek için \"kazanç ölçütü\" adı verilen ifadeye başvurulur.\nKazanç ölçütü şu şekilde hesaplanır.\n\nKazanç(X,T) = H(T) - H(X,T)\n\n** Karar ağaçlarında ayırma işlemi yapılırken kazanç değerini en çoklama (maximize) amaçlanır.\nEn yüksek bilgi kazancını sağlayan, yani kazanı maksimize edebilecek X özniteliği seçilir. **<br><br><br>\n\n\n\n<center>![](https://i.hizliresim.com/347M52.png)</center>"},{"metadata":{},"cell_type":"markdown","source":"<b> Decision Tree Some Parameters </b>\n\n- criterion{“gini”, “entropy”}, default=”gini”<br>\nThe function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain.\n\n- splitter{“best”, “random”}, default=”best”<br>\nThe strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and “random” to choose the best random split.\n\n- max_depth: int, default=None<br>\nThe maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n\n- min_samples_leafint or float, default=1 <br>\nThe minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n\nIf int, then consider min_samples_leaf as the minimum number.\n\nIf float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.\n\n[For Reference](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree\ndt = tree.DecisionTreeClassifier(random_state=1,criterion='entropy') # criterion default = gini\ndt.fit(x_train,y_train)\n\nprint(\"Train Score: \",dt.score(x_train,y_train))\nprint(\"Test Score: \",dt.score(x_test,y_test))\n\nprint(\"\\n\\nDecision Tree default parameters : \",dt.get_params)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree\ndt = tree.DecisionTreeClassifier(random_state=1,criterion='gini')\ndt.fit(x_train,y_train)\n\nprint(\"Train Score: \",dt.score(x_train,y_train))\nprint(\"Test Score: \",dt.score(x_test,y_test))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Result Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Criterion gini visualization\nimport graphviz \nfrom graphviz import Source\nplt.figure(figsize=(40,20))  \n_ = tree.plot_tree(dt, feature_names = x.columns, \n             filled=True, fontsize=10, rounded = True)\nplt.savefig('diabetes.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree Performance Analysis\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#confusion matrix for Criterion gini\ny_pred = dt.predict(x_test)\ny_true = y_test\n\n# 0 value is negative\n# 1 value is positive\ncmatrix2 = confusion_matrix(y_true,y_pred,labels=[0,1])\n\nf,ax = plt.subplots(figsize=(8,4))\nsns.heatmap(cmatrix2,annot=True,linewidths=0.5,cbar=True,linecolor=\"white\",fmt='.0f',ax=ax)\nplt.title(\"Model's Confusion Matrix\")\nplt.xlabel(\"y_predict (Predicted Label)\")\nplt.ylabel(\"y_true (Actual Label)\")\nplt.show()\n\n#1=\"Pozitif\"\n#0=\"Negatif\"\n\nprint('True negative = ', cmatrix2[0][0])\nprint('False positive = ', cmatrix2[0][1])\nprint('False negative = ', cmatrix2[1][0])\nprint('True positive = ', cmatrix2[1][1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_true,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=3.4></a>\n# 3.4. Classification with Random Forest\n*Random Forests combine multiple decision tree models to convey their predictions to a decision mechanism\nand this decision mechanism is determined by the majority of votes and the final classification result. *<br>\n<center> ![](https://i.hizliresim.com/2NnVpM.png) </center> \n\n1. The whole data set is divided into two parts according to the determined rate (train and test). When dividing into pieces, the pieces that will correspond to the percentiles are randomly divided. There are now two data sets available: the Learning data set and the Test data set.\n2. Learning data sets are randomly divided into n sub-set for N decision tree models.\n3. N decision tree models perform the learning process with the learning data assigned to them.\n4. N decision tree models are tested using the test data set and N prediction results are obtained.\n5. These estimation results conveyed to the decision making mechanism are subject to the majority of votes.\n6. The result determined by the majority of votes from the decision making mechanism is the final result. Classification process is completed\n\n----------------------------------------------------------------------------------------------------\n                                                                TR\n----------------------------------------------------------------------------\n\n*Rassal Ormanlar birden fazla karar ağacı modelinin biraraya gelerek yaptıkları tahminleri bir karar mekanizmasına iletmesi\nve bu karar mekanizmasında oy çokluğu ile nihai sınıflandırma sonucunun belirlenmesi şeklinde gerçekleşir.*\n\n\n1. Tüm veri seti belirlenen orana göre (öğrenme ve test) iki parçaya bölünür. Parçaya bölünürken yüzdelik dilimlere denk gelecek parçalar rastgele bölümlenir. Artık elde iki veri seti bulunmaktadır: Öğrenme veri seti ve Test veri seti.\n2. Öğrenme veri setleri N adet karar ağacı modeli için n adet alt parçaya yine rastgele olacak şekilde bölünür.\n3. N adet karar ağacı modelleri kendilerine atanan öğrenme verileri ile öğrenme işlemini gerçekleştirir.\n4. Test veri seti kullanılarak N adet karar ağacı modeli test edilir ve N adet tahmin sonucu elde edilir.\n5. Karar verme mekanizmasına iletilen bu tahmin sonuçları, oy çokluğuna tabi tutulur.\n6. Karar verme mekanizmasından oy çokluğu ile belirlenen sonuç nihai sonuçtur. Sınıflandırma işlemi tamamlanır.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n#*n_estimators: number of tree models to be used\n#max_depth: The maximum depth of the tree. If None, \n#then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n\nrf = RandomForestClassifier(n_estimators=10,max_depth= 5, random_state=2, criterion=\"gini\")\nrf.fit(x_train,y_train)\n\nprint(\"Random Forest Score: \",rf.score(x_test,y_test))\n\nprint(\"Random Forest Parameters: \",rf.get_params)\n# n_ estimators default = 100\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> Importance coefficients of features"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.concat((pd.DataFrame(x_train.columns, columns = ['variable']), \n           pd.DataFrame(rf.feature_importances_, columns = ['importance'])), \n          axis = 1).sort_values(by='importance', ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Result Visualization"},{"metadata":{},"cell_type":"markdown","source":"### Best Estimator"},{"metadata":{"trusted":true},"cell_type":"code","source":"score_estimators=[]\nfor i in range(0,len(rf.estimators_)):\n    estimator=rf.estimators_[i].score(x_test,y_test)\n    print(\"{}. estimator score is {}\".format(i+1,estimator))\n    score_estimators.append(estimator)\n    \nprint(\"\\nBest Accuracy(test) is {:.3f}% with estimator number = {}\".format(np.max(score_estimators)*100,score_estimators.index(np.max(score_estimators))+1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### decision tree structure according to best estimator"},{"metadata":{"trusted":true},"cell_type":"code","source":"# decision tree structure according to best estimator\n\nbest_estimator_number = score_estimators.index(np.max(score_estimators))+1\nmodel_estimator = rf.estimators_[best_estimator_number]\n\nplt.figure(figsize=(18,10))  \n_ = tree.plot_tree(model_estimator, feature_names = x.columns, \n             filled=True, fontsize=10, rounded = True)\nplt.savefig('diabetes.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import timeit\nstart = timeit.default_timer()\ntrain_score_list=[]\ntest_score_list=[]\n\nfor i in range(1,101):\n    rf2 = RandomForestClassifier(n_estimators=i,max_depth= 5, random_state=2, criterion=\"gini\")\n    rf2.fit(x_train,y_train)\n    train_score_list.append(rf2.score(x_test,y_test))\n    test_score_list.append(rf2.score(x_train,y_train))\n    \nplt.figure(figsize=(12,5))\np1=sns.lineplot(x=range(1,101),y=train_score_list,color='red',label=\"Train Scores\")\np1=sns.lineplot(x=range(1,101),y=test_score_list,color='lime',label=\"Test Scores\")\nplt.legend()\nplt.title('N-Estimator vs Accuracy')\nplt.xlabel(\"Estimators\")\nplt.ylabel(\"Accuracy\")\nplt.show()\n\nstop = timeit.default_timer()\nprint('Run Time: ', stop - start) \nprint(\"\\nBest Accuracy(test): {:.3f} with n_estimators: {} \".format(np.max(test_score_list)*100,test_score_list.index(np.max(test_score_list))+1))\nprint(\"\\nBest Accuracy(train): {:.3f} with n_estimators: {} \".format(np.max(train_score_list)*100,train_score_list.index(np.max(train_score_list))+1))\nbest_n_estimators_parameter = test_score_list.index(np.max(test_score_list))+1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Performance Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf3 = RandomForestClassifier(n_estimators=best_n_estimators_parameter,max_depth= 5, random_state=2, criterion=\"gini\")\nrf3.fit(x_train,y_train)\ny_true = y_test\ny_pred = rf3.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 0 value is negative\n# 1 value is positive\nfrom sklearn.metrics import confusion_matrix\ncmatrix3 = confusion_matrix(y_true,y_pred,labels=[0,1])\n\nf,ax = plt.subplots(figsize=(8,4))\nsns.heatmap(cmatrix3,annot=True,linewidths=0.5,cbar=True,linecolor=\"white\",fmt='.0f',ax=ax)\nplt.title(\"Model's Confusion Matrix\")\nplt.xlabel(\"y_predict (Predicted Label)\")\nplt.ylabel(\"y_true (Actual Label)\")\nplt.show()\n\n#1=\"Pozitif\"\n#0=\"Negatif\"\n\nprint('True negative = ', cmatrix3[0][0])\nprint('False positive = ', cmatrix3[0][1])\nprint('False negative = ', cmatrix3[1][0])\nprint('True positive = ', cmatrix3[1][1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_true,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=3.5></a>\n# 3.5. Classification with Support Vector Machine\n1. Linear Support Vector Machine\n       \n        Linear SVM can be used in the linearly distributed sample space. \n        In datasets that are not distributed as truths what will we do?\n        In this case, SVMs cannot draw a linear hyper plane. \"Core Cheats\" are used.\n        The kernel method greatly increases machine learning in the nonlinear machine.\n        -----------------------------------------TR---------------------------------------------------     \n        Doğrusal olarak dağılan örneklem uzayında Linear SVM kullanılabilir fakat. \n        Doğrusalar olarak dağılmayan veri kümelerinde ne yapacağız ? \n        Bu durumda SVMler doğrusal bir hiper düzlem çizemez. Bu nedenle çekirdek numarası olarak adlandırılan \"Kernel Trick\" ler kullanılır.\n        Çekirdek yöntemi, doğrusal olmayan verilerde makine öğrenimini yüksek oranda artırmaktadır.\n\n2.  Kernel Trick (None Linear SVM) <br>\n\n    2.1  (Kernel=Polynomial) Support Vector Machine <br>\n    2.2. (Kernel=Rbf) Support Vector Machine (Gaussian Kernel(Radial Basis Function))<br>\n    2.3. (Kernel=sigmoid) Support Vector Machine <br>\n\n\n* Default Parameters: {C(cost):1.0, kernel:rbf, degree(for poly):3, gamma(for rbf,poly,sigmoid)\n* [For More Details and Reference](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)"},{"metadata":{},"cell_type":"markdown","source":"<a id=3.5.1></a>\n## 3.5.1. Linear Support Vector Machine\n\n<center>![SVM](https://i.hizliresim.com/ZQX62g.png)</center>\n\n- For Reference [Data Iteam-Machine Learning](https://www.udemy.com/course/machine-learning-ve-python-adan-zye-makine-ogrenmesi-4/)\n\n- Linear SVM is parameterless classifier. Parameter C, as with all kernels, is used only as a loose parameter to determine the imbalance in the classifier error.\n\n- Linear SVM parametresiz bir svm sınıflandırıcısıdır. C parametresi bütün çekirdeklerde olduğu gibi sadece sınıflandırıcı hatasındaki dengesizliği belirlemek için gevşek parametre olarak kullanılır.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nc_range=np.arange(0.1,1.5,0.1)\n\nfor i in c_range:\n    svm_linear = SVC(kernel=\"linear\",C=i,random_state=29)\n    svm_linear.fit(x_train,y_train)\n    print(\"accuracy of svm(linear): {:.4f} with C(cost) parameter: {:.2f}\".format(svm_linear.score(x_test,y_test),i))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear svm performance analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"#confusion matrix for Criterion gini\ny_pred = svm_linear.predict(x_test)\ny_true = y_test\n\n# 0 value is negative\n# 1 value is positive\ncmatrix_linear = confusion_matrix(y_true,y_pred,labels=[0,1])\n\nf,ax = plt.subplots(figsize=(8,4))\nsns.heatmap(cmatrix_linear,annot=True,linewidths=0.5,cbar=True,linecolor=\"white\",fmt='.0f',ax=ax)\nplt.title(\"Model's Confusion Matrix\")\nplt.xlabel(\"y_predict (Predicted Label)\")\nplt.ylabel(\"y_true (Actual Label)\")\nplt.show()\n\n#1=\"Pozitif\"\n#0=\"Negatif\"\n\nprint('True negative = ', cmatrix_linear[0][0])\nprint('False positive = ', cmatrix_linear[0][1])\nprint('False negative = ', cmatrix_linear[1][0])\nprint('True positive = ', cmatrix_linear[1][1])\n\n\n\n\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_true,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Kernel Trick (None Linear SVM)\n\n\n-It aims to find the most appropriate decision line by adding z-axis as a third axis (3rd dimension) in the classification of the Svm graphic linearly.\n\n-Svm verileri doğrusal olarak sınıflandıramadığında üçüncü bir eksen (3.boyut) olarak z ekseni ekleyerek en optimal karar doğrusunu bulmayı hedefler.\n\n<center>![Kernel Trick](https://i.hizliresim.com/mSa98Y.png)</center>\n"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<a id=3.5.2></a>\n## 3.5.2. Polynomial Support Vector Machine\n\n    - gamma parameter default value is 1/(n_features * x_train.var())\n    - if gamma parameter value is \"auto\", it is calculated as 1/n_features \n    -Polynomial: (gamma*u'*v + coef0)^degree (using libsvm's nomenclature) Degree is the main parameter here, but you can also vary gamma & coef0 to make the kernel non-symmetric.\n   \n   \n    - Polinom: (gamma * u '* v + coef0) ^ degree (libsvm terminolojisini kullanarak) Derecesi burada ana parametredir ancak çekirdeği simetrik olmayan yapmak için gamma & coef0'ı da değiştirebilirsiniz.\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"### Let's understand svm from the visual (Where to use polynomial)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import make_moons\n# create spiral dataset\nX, Y = make_moons(n_samples=1000, noise=0.15, random_state=42)\nplt.figure(figsize=(10,5))\nax = plt.axes()\nax.scatter(X[:,0],X[:,1],c=Y);\nplt.xlabel(\"1\");\nplt.ylabel(\"2\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=(X - np.min(X)) / (np.max(X) - np.min(X))\nfrom sklearn.svm import SVC\nsvm_poly = SVC(kernel=\"poly\",C=1.0,degree=3,random_state=29)\nsvm_poly.fit(X,Y)\n\nplt.figure(figsize=(15,5))\nplot_decision_regions(X,Y,svm_poly)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.5.2. (Polynomial) support vector machine with sklearn"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvm_polynomial = SVC(kernel=\"poly\",C=1.0,degree=2.0,gamma='scale')\nsvm_polynomial.fit(x_train,y_train)\nprint(\"Accuracy of svm(poly)%: \",svm_polynomial.score(x_test,y_test)*100)\nprint(\"\\n\\nOther Default Parameters: \",svm_polynomial.get_params())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Polynomial SVM Perfrmance Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"#confusion matrix for Criterion gini\ny_pred = svm_polynomial.predict(x_test)\ny_true = y_test\n\n# 0 value is negative\n# 1 value is positive\ncmatrix_polynomial = confusion_matrix(y_true,y_pred,labels=[0,1])\n\nf,ax = plt.subplots(figsize=(8,4))\nsns.heatmap(cmatrix_polynomial,annot=True,linewidths=0.5,cbar=True,linecolor=\"white\",fmt='.0f',ax=ax)\nplt.title(\"Model's Confusion Matrix\")\nplt.xlabel(\"y_predict (Predicted Label)\")\nplt.ylabel(\"y_true (Actual Label)\")\nplt.show()\n\n#1=\"Pozitif\"\n#0=\"Negatif\"\n\nprint('True negative = ', cmatrix_polynomial[0][0])\nprint('False positive = ', cmatrix_polynomial[0][1])\nprint('False negative = ', cmatrix_polynomial[1][0])\nprint('True positive = ', cmatrix_polynomial[1][1])\nprint(\"\\n\\n\")\n\n\n\n\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_true,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=3.5.3></a>\n## 3.5.3. Gaussian Rbf (Radial Basis Function) Kernel\n\n    - Gamma is the main parameter.\n                                             Parameter Gamma and C\n    - It is generally used in the sample space and it has a spiral distribution.\n    - Since the data is not distributed linearly, then it is impossible to draw a logical decision line (maximum margin).\n    - It is possible to create a decision line (max margin) by adding a new dimension (z) to a 2D dataset.\n    - The gamma parameter used in Rbf kernel is directly proportional to the euclidean distance. That is, the closer the two vectors, the lower the gamma            value.\n    - But lowering Gamma too much may make the process generalizing, that is, giving bad results,\n    - Keeping it high may cause excessive curve fitting and increase processing cost.\n    - Gamma should be kept neither too little nor too much. \n    - Because the best, most optimal decision line(margin classifier) is the equidistant and max decision line on both sides.\n    \n    C is the slack parameter for SVM, regardless of kernel chosen, it has nothing to do with the kernel. It determines the tradeoff between a wide margin and classifier error. Large C means you are allowing little slack and the model will fit tighter to your data, small C means you are allowing a lot of slack and the model will have more error on the training set, but be less sensitive to noise.\n  \n--------------------------------------------------------------------------------------------------------------------------------------\n                                               Gamma ve C parametreleri\n    - Genelde örneklem uzayındaki verilerin spiral şeklinde dağılıma sahip olduğu durumlarda kullanılır.\n    - Verilerin doğrusal olarak dağılmadığı durumlarda veriler için mantıklı bir karar doğrusu (max margin) çizmek imkansızdır.\n    - Bu durumlarda aşağıdaki örnekte görüldüğü gibi 2 boyutlu bir veri kümesine yeni bir boyut(z) ekleyerek, karar doğrusu (max margin) oluşturmak olasıdır.\n    - Rbf kernel'da kullanılan gamma parametresi, öklid uzaklığı ile doğru orantılıdır. Yani iki vektör nekadar yakınsa gamma değeri o kadar düşecektir.\n    - Fakat Gammanın fazla düşürülmesi işlemin genelleme yapmasını yani kötü sonuç vermesini sağlayabileceği gibi,\n    - yüksek tutulmasıda eğri uydurmada aşırıya sebebiyet verebilir ve işlem maliyetini arttırır.\n    - Gamma ne az, ne de çok tutulmalıdır. Çünkü en iyi, en optimal karar çizgisi iki tarafada eşit uzaklıkta ve maksimum olan karar çizgisidir.\n    \n    C, seçilen çekirdeğe bakılmaksızın, SVM'nin gevşek parametresidir, çekirdek ile ilgisi yoktur. Geniş bir kenar boşluğu ve sınıflandırıcı hatası arasındaki dengeyi belirler. Büyük C, biraz gevşekliğe izin verdiğiniz anlamına gelir ve model verilerinize daha sıkı sığar,\n    küçük C, çok fazla gevşekliğe izin verdiğiniz anlamına gelir ve modelin eğitim setinde daha fazla hatası olacaktır, ancak gürültüye daha az duyarlı olacaktır.\n\n    \n- [For Reference](https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets.samples_generator import make_circles\nX,Y = make_circles(90, factor=0.2, noise=0.1) \n#noise = standard deviation of Gaussian noise added in data. \n#factor = scale factor between the two circles\nplt.figure(figsize=(8,5))\nplt.scatter(X[:,0],X[:,1], c=Y, s=50, cmap='seismic')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=(X - np.min(X)) / (np.max(X) - np.min(X))\nfrom sklearn.svm import SVC\nimport time\n\ngamma_values=[0.1,1,10,100]\nfor i in gamma_values:\n    start=time.time()\n    svm_gausian = SVC(kernel=\"rbf\",C=1.0,degree=3,gamma=i,random_state=29)\n    svm_gausian.fit(X,Y)\n    finish=time.time()\n\n    plt.figure(figsize=(15,5))\n    plt.title(\"Gamma = {}, Time Cost= {:.4f}\".format(i,(finish-start)))\n    plt.xlabel(\"x feature\")\n    plt.ylabel(\"y feature\")\n    plot_decision_regions(X,Y,svm_gausian)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Rbf (Gaussian) Support Vector Machine with Sklearn"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvm_rbf = SVC(kernel=\"rbf\",C=0.5,gamma=\"scale\")\nsvm_rbf.fit(x_train,y_train)\nprint(\"Accuracy of svm(poly): %\",svm_rbf.score(x_test,y_test)*100)\nprint(svm_rbf.get_params())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n### RBF SVM Performance Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"#confusion matrix for Criterion gini\ny_pred = svm_rbf.predict(x_test)\ny_true = y_test\n\n# 0 value is negative\n# 1 value is positive\ncmatrix_rbf = confusion_matrix(y_true,y_pred,labels=[0,1])\n\nf,ax = plt.subplots(figsize=(8,4))\nsns.heatmap(cmatrix_rbf,annot=True,linewidths=0.5,cbar=True,linecolor=\"white\",fmt='.0f',ax=ax)\nplt.title(\"Model's Confusion Matrix\")\nplt.xlabel(\"y_predict (Predicted Label)\")\nplt.ylabel(\"y_true (Actual Label)\")\nplt.show()\n\n#1=\"Pozitif\"\n#0=\"Negatif\"\n\nprint('True negative = ', cmatrix_rbf[0][0])\nprint('False positive = ', cmatrix_rbf[0][1])\nprint('False negative = ', cmatrix_rbf[1][0])\nprint('True positive = ', cmatrix_rbf[1][1])\nprint(\"\\n\\n\")\n\n\n\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_true,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=3.5.4></a>\n## 3.5.4. Sigmoid Support Vector Machine\n\n- Gamma and Coef0 önemli rol oynar.\n- Gamma ana parametredir."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvm_sigmoid = SVC(kernel=\"sigmoid\",gamma=0.5)\nsvm_sigmoid.fit(x_train,y_train)\nprint(\"Accuracy of svm(sigmoid): %\",svm_sigmoid.score(x_test,y_test)*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sigmoid SVM Performance Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = svm_sigmoid.predict(x_test)\ny_true = y_test\n\n# 0 value is negative\n# 1 value is positive\ncmatrix_sigmoid  = confusion_matrix(y_true,y_pred,labels=[0,1])\n\nf,ax = plt.subplots(figsize=(8,4))\nsns.heatmap(cmatrix_sigmoid,annot=True,linewidths=0.5,cbar=True,linecolor=\"white\",fmt='.0f',ax=ax)\nplt.title(\"Model's Confusion Matrix\")\nplt.xlabel(\"y_predict (Predicted Label)\")\nplt.ylabel(\"y_true (Actual Label)\")\nplt.show()\n\n#1=\"Pozitif\"\n#0=\"Negatif\"\n\nprint('True negative = ', cmatrix_sigmoid[0][0])\nprint('False positive = ', cmatrix_sigmoid[0][1])\nprint('False negative = ', cmatrix_sigmoid[1][0])\nprint('True positive = ', cmatrix_sigmoid[1][1])\nprint(\"\\n\\n\")\n\n\n\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_true,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=3.6></a>\n# 3.6. Classification with Naive Bayes\n\n- The Naive Bayes classification algorithm performs a learning, possibly with the data provided to it.\n- The learning result is for guessing, it calculates according to its values ​​as it was created during the learning phase and makes an estimate.\n------------------------------------------------------\n- Naive Bayes sınıflandırma algoritması, ona sunulan veriler ile olasılıksak olarak bir öğrenme gerçekleştirir.\n- Öğrenme sonucu, tahmin edilmesi istenen test verilerini öğrenme aşamasında oluşturmuş olduğu olasılık değerlerine göre hesaplar ve bir tahminde bulunur.\n\n\n![](https://i.hizliresim.com/SKvKZl.png)\n![](https://i.hizliresim.com/ZJBgPx.png)\n\n\n        Let's consider a data set with deneyim and Maas.\n        Purple colors represent physics, red colors represent mathematics. (6 maths, 5 physics (fiz))\n        Is our problem math or physics that will be added to the sample?\n\n        Calculating as possible:\n        P (Mathematics | X) => probability that the x point is math? required for:\n\n        1. P (Math) => Probability of mathematics (Except Black Point (x)) = 6/11\n        2. P (X) ==> Probability of points within the similarity range in X = 4/11 (3 purple + 1 red) / total data\n        3. P (X | Math) => Probability of x according to mathematics within similarity range = 3/6\n\n        P (Mathematics | X) = 3/6 * 6/11 / 4/11 = 75%\n        p (Fiz | X) = 1/5 * 5/11 / 4/11 = 25%\n\n        Result => Blackhead = Math\n        \n------------------------------------------------------------------------------------------------------------------\n                                    TR\n        Deneyim ve Maas featurelerinden oluşan bir veri setini  düşünelim.\n        Mor renkler fizik, kırmızı renkler matematiği temsil eder. ( 6 tane matematik, 5 tane fizik)\n        Problemimiz örnekleme yeni katılacak olan değer (siyah nokta) matematik mi yoksa fizik mi ?\n        \n        Olasılıksak olarak hesaplanması:\n        P(Math|X) => x noktasının matematik olma olasılığı ? için adımlar:\n        \n            1. P(Math) => Matematiğin olma olasılığı (Siyah Nokta(x) Hariç) = 6/11\n            2. P(X) ==>   X'in belirlenen benzerlik aralığı içindeki noktalardan olma olasılığı  =4/11    (3 mor + 1 kırmızı) / toplam data\n            3. P(X|MATH) => Benzerlik aralığı içinde matematiğe göre x'in olma olasılığı = 3/6   \n            \n            P(Math|X) =  3/6 * 6/11  / 4/11  = %75\n            p(Fiz|X)  =  1/5 * 5/11  / 4/11  = %25\n            \n            Sonuç => Siyah Nokta = Math\n       \n        \n        \n        \n        \n [For Reference: DATAITEAM - Machine Learning](https://www.udemy.com/course/machine-learning-ve-python-adan-zye-makine-ogrenmesi-4/)\n        \n\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train, y_train)\nprint(\"print accuracy of naive bayes: % \",nb.score(x_test, y_test)*100)\nprint(svm_rbf.get_params())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Naive Bayes Model Performace Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"#confusion matrix for Criterion gini\ny_pred = nb.predict(x_test)\ny_true = y_test\n\n# 0 value is negative\n# 1 value is positive\ncmatrix_nb = confusion_matrix(y_true,y_pred,labels=[0,1])\n\nf,ax = plt.subplots(figsize=(8,4))\nsns.heatmap(cmatrix_nb,annot=True,linewidths=0.5,cbar=True,linecolor=\"white\",fmt='.0f',ax=ax)\nplt.title(\"Model's Confusion Matrix\")\nplt.xlabel(\"y_predict (Predicted Label)\")\nplt.ylabel(\"y_true (Actual Label)\")\nplt.show()\n\n#1=\"Pozitif\"\n#0=\"Negatif\"\n\nprint('True negative = ', cmatrix_nb[0][0])\nprint('False positive = ', cmatrix_nb[0][1])\nprint('False negative = ', cmatrix_nb[1][0])\nprint('True positive = ', cmatrix_nb[1][1])\nprint(\"\\n\\n\")\n\n\n\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_true,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"<a id=3.7></a>\n# 3.7. Classification with Logistic Regression\n\n- for detailed information : [My Detailed Logistic Regression Kernel](https://www.kaggle.com/kursatkaragoz29/pima-indians-diabetes-db-to-logistic-regression#Data-Reading-and-Data-Pre-Processing)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\nprint(\"Test Accuracy : %{}\".format(lr.score(x_test,y_test)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import linear_model\ntrain_score_list=[]\ntest_score_list=[]\narange=np.arange(1,20)\nfor each in range (1,len(arange)+1):\n    logreg = linear_model.LogisticRegression(random_state = 29,max_iter= each)\n    logreg.fit(x_train,y_train)\n    train_score_list.append(logreg.score(x_train,y_train))\n    test_score_list.append(logreg.score(x_test,y_test))    \nprint(\"Best accuracy(test) is {} with itereration n. = {}\".format(np.max(test_score_list),test_score_list.index(np.max(test_score_list))+ 1))\nprint(\"Best accuracy(train) is {} with iteration n. = {}\".format(np.max(train_score_list),train_score_list.index(np.max(train_score_list))+ 1))\nbest_max_iter_parameter = test_score_list.index(np.max(test_score_list))+ 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg2 = linear_model.LogisticRegression(random_state = 29,max_iter= best_max_iter_parameter)\nlogreg2.fit(x_train,y_train)\ny_pred = logreg2.predict(x_test)\ny_true = y_test\n\nprint(\"Score: \",logreg2.score(x_train,y_train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression Model Performance Analysis\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 0 value is negative\n# 1 value is positive\ncmatrix_logreg = confusion_matrix(y_true,y_pred,labels=[0,1])\n\nf,ax = plt.subplots(figsize=(8,4))\nsns.heatmap(cmatrix_logreg,annot=True,linewidths=0.5,cbar=True,linecolor=\"white\",fmt='.0f',ax=ax)\nplt.title(\"Model's Confusion Matrix\")\nplt.xlabel(\"y_predict (Predicted Label)\")\nplt.ylabel(\"y_true (Actual Label)\")\nplt.show()\n\n#1=\"Pozitif\"\n#0=\"Negatif\"\n\nprint('True negative = ', cmatrix_logreg[0][0])\nprint('False positive = ', cmatrix_logreg[0][1])\nprint('False negative = ', cmatrix_logreg[1][0])\nprint('True positive = ', cmatrix_logreg[1][1])\n\nprint(\"\\n\\n\")\n\n\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_true,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=4></a>\n# 4. HYPERPARAMETER TUNING and CROSS VALIDATION\n<a id=4.1></a>\n## 4.1. Hyper Parameter\n\nIn machine learning algorithms, some parameters are referred to as hyper parameters. These are the parameters that developers should choose the best result oriented. <br>\nIn other words, unlike other parameters, it is not known which of the values ​​to be given to these parameters is the most optimal.\n\nTo give an example of hyper parameters:\n     * \"K\" parameter in Knn algorithm.\n     * Alpha parameter in Ridge and Lasso regression.\n     * Coefficients in linear regression (bias, weight)\n     * Learning_rate and number_of_iteration (forward and backward repeats) for Logistic Regression\n     * Max_depth, max_features parameters for Random Forest.\n     * C and Gamma parameters for rbf kernel svm.\n     * Degree parameter for poly kernel svm.\n     * N_estimator, max_depth ... for Random forest\netc...\n\nWhat will be the most optimal, best parameters mentioned for the above algorithms? This problem is a model\nthey are very effective for their performance. <br>\nWith GridSearchCv, all combinations of hyper parameters are implemented and give us the best parameters.\n\n\n\n                                                       TR\n\nMakine öğrenmesi algoritmalarında bazı parametreler hiper parametre (hyper parameter) olarak geçer. Bu parametrelerin geliştiriciler tarafından en iyi sonuç odaklı seçmesi gereken parametrelerdir.<br>\nYani diğer parametrelerden farklı olarak bu parametrelere verilecek değerlerin en optimali hangisidir bilinmemektedir, geliştiricilerin deneyerek bulması gerekir.\n\nHiper parametrelere örnek verecek olursak:\n     * Knn algoritmasında \"k\" parametresi.\n     * Ridge ve Lasso regression'da alpha parametresi.\n     * Doğrusal regresyonda katsayılar (bias,weight)\n     * Logistic Regression için learning_rate ve number_of_iteration (forward ve backward tekrar sayısı)\n     * Random Forest için max_depth, max_features parametreleri.\n     * Rbf kernel svm için C ve Gama parametreleri.\n     * Poly kernel svm için degree parametresi.\n     * Random forest için n_estimator, max_depth...\nvb...\n\nYukarıdaki algoritmalar için bahsedilen en optimal, en iyi parametreler neler olacak?. Bu problem bir modelin\nperformansı için çok etkilidirler.<br>\nGridSearchCv ile tüm hiper parametreleri kombinasyonları gerçeklenir ve en iyi parametreleri bize verir.\n\n\n\n\n     "},{"metadata":{},"cell_type":"markdown","source":"<a id=4.2></a>\n## 4.2. Cross Validation\n\n\n- It includes methods used to test the reliability of models. Performs test of model performance.\nThe difference from other methods takes the data set as a whole and the model is tested by separating the data according to certain parameters as training and test, crossing the separated parts.\n                                                                          -  TR -\n\n- Modellerin güvenilirliğinin sınanmasından kullanılan yöntemler barındırır. Model başarımının sınanmasını gerçekleştirir.\nDiğer yöntemlerden farkı veri setini bütün olarak alır ve belirli parametrelere göre veriyi eğitim ve test olarak ayırarak, ayrılan parçaları çaprazlanarak modelin sınanması gerçekleştirilir.\n\n\n## 4.3. K-Fold Cv\n\n- It treats the data set as a whole to test the success and divides the data into k equal parts according to the given k parameter. Data corresponding to the divided pieces are randomly assigned from the data set.\n- The first part of K pieces is separated for testing, the rest is reserved for training, the model is tested.\n- In the second stage, the second part is separated for testing and the remaining legs are used for training and the model is tested.\n- This process continues until the last piece and k points are obtained for the model. The average of these scores gives the performance score of the model.\n\n                                                                           - TR -\n                                                                           \n- Başarının sınanması için veri kümesini bir bütü olarak ele alır ve verilen k parametresine göre veriyi k adet eşit parçaya böler. Bölünen eşit parçalara denk gelecek veriler, veri seti içerisinden rastgele olarak atanır.\n- K adet parçanın sırası ile 1. parçası test için ayrılır, gerisi eğitim için ayrılır, model test edilir.\n- İkinci aşamada 2. parçası test için ayrılır kalan paçalar eğitim için kullanılır ve model test edilir.\n- En sondaki parçaya kadar bu işlem devam eder ve modele ait k adet puan elde edilir. Bu puanların ortalaması modelin başarım puanını verir.\n\n![Cross Validation](https://i.hizliresim.com/xspPaZ.png)\n\n<a id=4.3></a>\n## 4.4.  LOOCV (Leave One Out Cv)\n\n- Each of the data in the data set is separated as an individual test, the remainder is used for training.\nAt each stage, test data is asked to the trained model and scoring occurs.\n- The number of records in the data set is formed and the average of these points gives the performance score of the model.\n\n                                                                            - TR -\n\n- Veri setinde bulunan verilerin her biri tek tek test olarak ayrılır, kalanlar eğitim için kullanlır.\nHer aşamada test verisi, eğitilmiş modele sorulur ve puanlamalar oluşur. \n- Veri setindeki kayıt sayısı kadar puan oluşur ve bu puanların ortalaması modelin başarım puanını verir.\n\n![LOOCV](https://i.hizliresim.com/3v7ohc.png)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#import library for parameter tuning\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.shape #ALL DATASET drop('Outcome')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape #Outcome","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=4.4></a>\n# 4.4. Parameter Tuning for Knn"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(68)\nknn.fit(x_train,y_train)\n\n\nresults = cross_validate(knn, x, y, cv=5,scoring='accuracy',return_train_score=True)\nprint(\"Cv:5, Test Score: {}\".format(results['test_score']))\nprint(\"Cv:5, Train Score: {}\".format(results['train_score']))\n\naccuracy = cross_val_score(knn,x,y,cv=5)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (accuracy.mean(),accuracy.std() * 2))\n\nf1 = cross_val_score(knn,x,y,cv=5,scoring='f1')\nprint('F1 Score : ',f1)\n\nmse = cross_val_score(knn,x,y,cv=5,scoring='neg_mean_squared_error')\nprint('Negative Mean Squared Error: ', mse)\n\n\nprint(\"Cv = 5, recall = \",cross_val_score(knn, x, y, scoring='recall'))\nprint(\"Cv = 5, precision = \",cross_val_score(knn, x, y, scoring='precision'))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid = {'n_neighbors': np.arange(1,100)}\nknn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn, grid, cv=5) # cross validation (cv) default = 5\nknn_cv.fit(x_train,y_train)# Fit\n\n# Print hyperparameter\n\nprint(\"Tuned hyperparameter k: {}\".format(knn_cv.best_params_)) \nprint(\"Best score: {}\".format(knn_cv.best_score_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=4.5></a>\n# 4.5. Parameter Tuning for Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier(criterion='gini',max_depth=3)\ndtree.fit(x,y)\ndtree_result = cross_validate(dtree,x, y, cv=5,scoring='accuracy',return_train_score=True)\nprint(\"Cv:5, Test Score: {}\".format(dtree_result['test_score']))\nprint(\"Cv:5, Train Score: {}\".format(dtree_result['train_score']))\n\n\naccuracy = cross_val_score(dtree,x,y,cv=5)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (accuracy.mean(),accuracy.std() * 2))\n\nf1 = cross_val_score(dtree,x,y,cv=5,scoring='f1')\nprint('F1 Score : ',f1)\n\nmse = cross_val_score(dtree,x,y,cv=5,scoring='neg_mean_squared_error')\nprint('Negative Mean Squared Error: ', mse)\nprint(\"Cv = 5, recall = \",cross_val_score(knn, x, y, scoring='recall'))\nprint(\"Cv = 5, precision = \",cross_val_score(knn, x, y, scoring='precision'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\n#create a dictionary of all values we want to test\nparam_grid = {'criterion':['gini', 'entropy'],'max_depth': list(range(3,15))}\n# decision tree model\ndtree_model=DecisionTreeClassifier(random_state=29)\n#use gridsearch to test all values\ndtree_gscv = GridSearchCV(dtree_model, param_grid)# cv default = 5 = 5-fold cv\n #fit model to data\ndtree_gscv.fit(x_train, y_train)\nprint(\"Tuned hyperparameters Criterion: {}, Max_depth: {}\".format\n      (dtree_gscv.best_params_['criterion'],dtree_gscv.best_params_['max_depth'])) \nprint(\"Best score: {}\".format(dtree_gscv.best_score_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=4.6></a>\n# 4.6. Parameter Tuning for Random Forest\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"rftree = RandomForestClassifier(criterion='entropy',max_depth=4,n_estimators=100,max_features='auto',random_state=1)\nrftree.fit(x,y)\nrftree_result = cross_validate(rftree,x, y, cv=5,scoring='accuracy',return_train_score=True)\nprint(\"Cv:5, Test Score: {}\".format(rftree_result['test_score']))\nprint(\"Cv:5, Train Score: {}\".format(rftree_result['train_score']))\n\n\naccuracy = cross_val_score(rftree,x,y,cv=5)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (accuracy.mean(),accuracy.std() * 2))\n\nf1 = cross_val_score(rftree,x,y,cv=5,scoring='f1')\nprint('F1 Score : ',f1)\n\nmse = cross_val_score(rftree,x,y,cv=5,scoring='neg_mean_squared_error')\nprint('Negative Mean Squared Error: ', mse)\nprint(\"Cv = 5, recall = \",cross_val_score(rftree, x, y, scoring='recall'))\nprint(\"Cv = 5, precision = \",cross_val_score(rftree, x, y, scoring='precision'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nparam_grid = { \n    'n_estimators': [10,50,100],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7],\n    'criterion' :['gini', 'entropy']\n}\nrfc =RandomForestClassifier(random_state=29)\nrfc_gscv=GridSearchCV(estimator=rfc, param_grid=param_grid) # cv default = 5\nrfc_gscv.fit(x_train,y_train)\n\nprint(\"Tuned best parameters for random forest: \",rfc_gscv.best_params_ ) \nprint(\"Best score: {}\".format(rfc_gscv.best_score_))\n\n                       \n# OUTPUT\n#Tuned best parameters for random forest:  {'criterion': 'entropy', 'max_depth': 4, 'max_features': 'auto', 'n_estimators': 100}\n#Best score: 0.780356524749048                     \n                     ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n# Parameter Tuning for SVM"},{"metadata":{},"cell_type":"markdown","source":"<a id=4.7></a>\n# 4.7. Parameter Tuning for SVM (kernel=\"linear\")"},{"metadata":{"trusted":true},"cell_type":"code","source":"lin_svc = SVC(kernel='linear',C=10)\nlin_svc.fit(x,y)\nlin_svc_result = cross_validate(lin_svc,x, y, cv=5,scoring='accuracy',return_train_score=True)\nprint(\"Cv:5, Test Score: {}\".format(lin_svc_result['test_score']))\nprint(\"Cv:5, Train Score: {}\".format(lin_svc_result['train_score']))\n\n\naccuracy = cross_val_score(lin_svc,x,y,cv=5)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (accuracy.mean(),accuracy.std() * 2))\n\nf1 = cross_val_score(lin_svc,x,y,cv=5,scoring='f1')\nprint('F1 Score : ',f1)\n\nmse = cross_val_score(lin_svc,x,y,cv=5,scoring='neg_mean_squared_error')\nprint('Negative Mean Squared Error: ', mse)\nprint(\"Cv = 5, recall = \",cross_val_score(lin_svc, x, y, scoring='recall'))\nprint(\"Cv = 5, precision = \",cross_val_score(lin_svc, x, y, scoring='precision'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nparam_grid = { 'C' : [0.001, 0.01, 0.1, 1, 10, 20]}\nsvm_linear = SVC(kernel=\"linear\")\nsvm_gscv0 = GridSearchCV(svm_linear,param_grid=param_grid)\nsvm_gscv0.fit(x_train,y_train)\n\nprint(\"Tuned best parameters for kernel linear svm: \",svm_gscv0.best_params_ ) \nprint(\"Best score: {}\".format(svm_gscv0.best_score_))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=4.8></a>\n# 4.8. Parameter Tuning for SVM (kernel=\"rbf\")"},{"metadata":{"trusted":true},"cell_type":"code","source":"rbf_svc = SVC(kernel='rbf',C=100,gamma=0.1)\nrbf_svc.fit(x,y)\nrbf_svc_result = cross_validate(rbf_svc,x, y, cv=5,scoring='accuracy',return_train_score=True)\nprint(\"Cv:5, Test Score: {}\".format(rbf_svc_result['test_score']))\nprint(\"Cv:5, Train Score: {}\".format(rbf_svc_result['train_score']))\n\n\naccuracy = cross_val_score(rbf_svc,x,y,cv=5)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (accuracy.mean(),accuracy.std() * 2))\n\nf1 = cross_val_score(rbf_svc,x,y,cv=5,scoring='f1')\nprint('F1 Score : ',f1)\n\nmse = cross_val_score(rbf_svc,x,y,cv=5,scoring='neg_mean_squared_error')\nprint('Negative Mean Squared Error: ', mse)\nprint(\"Cv = 5, recall = \",cross_val_score(rbf_svc, x, y, scoring='recall'))\nprint(\"Cv = 5, precision = \",cross_val_score(rbf_svc, x, y, scoring='precision'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nparam_grid = { \n   'C': np.logspace(-3, 2, 6), 'gamma': np.logspace(-3, 2, 6)\n}\nsvm_rbf = SVC(kernel=\"rbf\")\nsvm_gscv = GridSearchCV(svm_rbf,param_grid) #cv default = 5\nsvm_gscv.fit(x_train,y_train)\n\nprint(\"Tuned best parameters for kernel rbf svm: \",svm_gscv.best_params_ ) \nprint(\"Best score: {}\".format(svm_gscv.best_score_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=4.9></a>\n# 4.9. Parameter Tuning for SVM (kernel=\"poly\")"},{"metadata":{"trusted":true},"cell_type":"code","source":"poly_svc = SVC(kernel='poly',C=1,degree=3)\npoly_svc.fit(x,y)\npoly_svc_result = cross_validate(poly_svc,x, y, cv=5,scoring='accuracy',return_train_score=True)\nprint(\"Cv:5, Test Score: {}\".format(poly_svc_result['test_score']))\nprint(\"Cv:5, Train Score: {}\".format(poly_svc_result['train_score']))\n\n\naccuracy = cross_val_score(poly_svc,x,y,cv=5)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (accuracy.mean(),accuracy.std() * 2))\n\nf1 = cross_val_score(poly_svc,x,y,cv=5,scoring='f1')\nprint('F1 Score : ',f1)\n\nmse = cross_val_score(poly_svc,x,y,cv=5,scoring='neg_mean_squared_error')\nprint('Negative Mean Squared Error: ', mse)\nprint(\"Cv = 5, recall = \",cross_val_score(poly_svc, x, y, scoring='recall'))\nprint(\"Cv = 5, precision = \",cross_val_score(poly_svc, x, y, scoring='precision'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### from sklearn.svm import SVC\nparam_grid = { \n    'C' : [0.001, 0.01, 0.1, 1, 10, 20],\n    'degree':list(range(1,5))\n}\nsvm_poly = SVC(kernel=\"poly\")\nsvm_gscv3 = GridSearchCV(svm_poly,param_grid) #cv default = 5\nsvm_gscv3.fit(x_train,y_train)\n\nprint(\"Tuned best parameters for kernel poly svm: \",svm_gscv3.best_params_ ) \nprint(\"Best score: {}\".format(svm_gscv3.best_score_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=4.10></a>\n# 4.10 Parameter Tuning for SVM (kernel=\"sigmoid\")"},{"metadata":{"trusted":true},"cell_type":"code","source":"sigmoid_svc = SVC(kernel='sigmoid',gamma=0.5)\nsigmoid_svc.fit(x,y)\nsigmoid_svc_result = cross_validate(sigmoid_svc,x, y, cv=5,scoring='accuracy',return_train_score=True)\nprint(\"Cv:5, Test Score: {}\".format(sigmoid_svc_result['test_score']))\nprint(\"Cv:5, Train Score: {}\".format(sigmoid_svc_result['train_score']))\n\n\naccuracy = cross_val_score(sigmoid_svc,x,y,cv=5)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (accuracy.mean(),accuracy.std() * 2))\n\nf1 = cross_val_score(sigmoid_svc,x,y,cv=5,scoring='f1')\nprint('F1 Score : ',f1)\n\nmse = cross_val_score(sigmoid_svc,x,y,cv=5,scoring='neg_mean_squared_error')\nprint('Negative Mean Squared Error: ', mse)\nprint(\"Cv = 5, recall = \",cross_val_score(sigmoid_svc, x, y, scoring='recall'))\nprint(\"Cv = 5, precision = \",cross_val_score(sigmoid_svc, x, y, scoring='precision'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nparam_grid = { \n   'gamma': np.arange(0,1,0.1),\n    'coef0': np.logspace(-3, 2, 10)\n}\nsvm_sigmoid = SVC(kernel=\"sigmoid\")\nsvm_gscv4 = GridSearchCV(svm_sigmoid,param_grid) #cv default = 5\nsvm_gscv4.fit(x_train,y_train)\n\nprint(\"Tuned best parameters for kernel poly svm: \",svm_gscv4.best_params_ ) \nprint(\"Best score: {}\".format(svm_gscv4.best_score_))\n\n#Output\n#Tuned best parameters for kernel poly svm:  {'coef0': 0.003593813663804626, 'gamma': 0.5}\n#Best score: 0.7747317410868813\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=5></a>\n# Conclusion\n\n** For Cross Validation = 5**\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = ['Knn','Decision Tree(Gini)','Random Forest','Svm(Linear)','Svm(Rbf)','Svm(Poly)','Svm(Sigmoid)']\naccuracy_score=[77.7,77.1,78,77.1,76.9,77.1,77.5]\nconclusionf =dict(Model=classifier,Accuracy_Score=accuracy_score)\nconclusionf = pd.DataFrame(conclusionf)\nconclusionf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arange=np.arange(2,31)\ntrace1=go.Scatter(\n    x=conclusionf.Model,\n    y=conclusionf.Accuracy_Score,\n    mode=\"lines + markers\",\n    name=\"Accuracy_Score-For CV 5\",\n    marker=dict(color = 'rgba(16, 0, 2, 0.8)'),\n    \n)\ndata=trace1\nlayout = dict(title = '-Model VS Accuracy',\n              xaxis= dict(title= 'Model',ticklen= 5,zeroline= False),\n              yaxis= dict(title= 'Accuracy_Score',ticklen= 5,zeroline= False)\n             )\nfig = dict(data = data, layout = layout)\niplot(fig)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}