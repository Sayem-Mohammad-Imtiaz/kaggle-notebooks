{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## Goal: Use the [board game geek review data](https://www.kaggle.com/jvanelteren/boardgamegeek-reviews) to design an algorithm model that can predict the rating for the given  review. And then using this model to develop a predictor of reviews website. [Website Link](https://flask-rating-prediction.herokuapp.com/predicted). [Website Github](https://github.com/VegetableCattle/predictor-of-reviews)\n\n## Blog content: Introduction the implementation of Naive Bayse Classifier without using algorithm library.[Blog Link](https://yongli.netlify.app/post/nbcproject/index/)"},{"metadata":{},"cell_type":"markdown","source":"### 1.Algorithm principle\n### 1.1 Introduction\nThe Bayesian method is a method with a long history and a lot of theoretical foundations. At the same time, it is direct and efficient when dealing with many problems. Many advanced natural language processing models can also evolve from it. Therefore, learning the Bayesian method is a very good entry point for studying natural language processing problems.\n### 1.2 Bayesian formula\n####  Joint probability formula: $P(Y,X) = P(Y|X)P(X)=P(X|Y)P(Y) $\nAmong them, $P(Y) $ is called the prior probability, $P(Y∣X)$ is called the posterior probability, and $P(Y, X)$ is called the joint probability. This way we can derive the Bayesian formula.\n#### Bayesian formula: $P(Y|X)=\\frac{P(X|Y)P(Y)}{P(X)} $\n### 1.3 Naive Bayes Algorithm\nThe probability model classifier is a conditional probability model:\n\n$p(C|F_1,…,F_n) $\n\nThe independent variable C has several categories and the conditions depend on several feature variables, but the problem is that if the dimension of the number of features n is large or each feature can take a large number of values, it is not realistic to list the probability table based on the probability model. So we modified this model to make it feasible. According to the Bayesian formula:\n\n$p(C│F_1,…,F_n )=\\frac{p(C)*p(F_1,…,F_n |C)}{p(F_1,…,F_n)} $\n\nThe denominator does not depend on C, and the value of the feature is also given, so the denominator can be considered a constant. The molecules are then equivalent to a joint distribution model:\n\n$p(C|F_1,…,F_n) $\n\n$∝p(C)*p(F_1,…,F_n│C) $\n\n$∝p(C)*p(F_1│C)*p(F_2,…,F_n│C,F_1 ) $\n\n$∝p(C)*p(F_1│C)*p(F_2│C,F_1 )p(F_3│C,F_1,F_2 )…p(F_n│C,F_1,F_2…F_(n-1) )$\n\nAssuming that each feature is independent of other features, that is, the features are independent of each other, there is:\n\n$p(F_i│C,F_j )=p(F_i│C) $\n\nThis means that the conditional distribution of the variable C can be expressed as:\n\n$p(C│F_1,…,F_n )=\\frac{1}{Z} p(C)*∏_i^np(F_i│C)$\n\nThe corresponding classifier is the formula defined as follows:\n\n$classify(f_1,…,f_n )=argmax p(C=c)∏_i^np(F_i=f_i |C=c) $\n\nIn this model, I use the ratings as the class, the comments as object, the word of comments as feature."},{"metadata":{},"cell_type":"markdown","source":"### 2.Data Pre-processing\n### 2.1 Data cleaning\nFirst of all, this experiment only used the comment and rating columns in the dataset, so we filtered out the other columns. \n\nSecond, we filter out the data with empty comment, because our purpose is to use the comment text to predict its rating.\n\nThen we filter out comments in languages other than English.\n### 2.2 Data Pre-processing\nReplace the punctuation marks in the text with spaces to prepare for word segmentation later.\n\nBecause ratings are between 0-10 and are continuous values, we use the rounding method to convert these values to discrete values.\n\nAfter many experiments, it is found that the accuracy rate obtained by using 0-10 integer scores is very low, probably based on between 20% and 30%, because for example, 9 points and 10 points are considered satisfactory scores, and the wording of comment is very similar. So I referred to the iPhone App Store and Amazon ’s 5-point rating standards, and by dividing this data by 2, the ratings data is mapped to 1-5 points."},{"metadata":{},"cell_type":"markdown","source":"### Some basic function libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport re\nimport random\nimport pandas as pd\nfrom csv import reader","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### load data and data pre-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"def SegmentLineToWordsList(string):\n    return list([x.lower() for x in re.split(r'[\\s]\\s*',string.strip()) if x])\ndef load_csv(filename):\n    dataset = list()\n    with open(filename, 'r') as file:\n        csv_reader = reader(file)\n        for row in csv_reader:\n            if not row:\n                continue\n            if row[3] == '':\n                continue\n            if row[2] == 'rating':\n                continue\n            sentense = re.sub(\"[%s]+\"%('\"|#|$|%|&|\\|(|)|\\[|\\]|*|+|\\-|/|<|=|>|@|^|`|{|}|~|_|,|.|?|!|:|;'), ' ', row[3])\n            sentense = re.sub(\"[%s]+\"%('\\''),'',sentense)\n            pattern = r'\\w+'\n            ascii_pattern = re.compile(pattern, re.ASCII)\n            if len(ascii_pattern.findall(sentense)) == len(SegmentLineToWordsList(sentense)):\n                index = round(float(row[2]) / 2)\n                index = int(index)\n                if index == 5:\n                    index = 4\n                dataset.append([sentense, index])\n    return dataset\n\ndataset_org = load_csv('../input/boardgamegeek-reviews/bgg-13m-reviews.csv')\nprint(len(dataset_org))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split dataset to train set and test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"def splitDataset(dataset, ratio_train):\n    random.shuffle(dataset)\n    cnt_train = round(len(dataset) * ratio_train ,0)\n    train = []\n    test = []\n    for i in range(int(cnt_train)):\n        train.append(dataset[i])\n    for i in range(int(cnt_train) ,len(dataset)):\n        test.append(dataset[i])\n    return train, test\n\ntrain = []\ntest = []\ntrain, test = splitDataset(dataset_org, 0.75)\nprint(len(train))\nprint(len(test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.Contributions & Optimization\n### 3.1 implementation of Naive Bayse Classification without using algorithm library\n### 3.2 Map ratings to 1-5 intervals\nThrough many experiments, it is found that the score is mapped to the 1-5 interval, and then the discretization is performed, and the final algorithm accuracy rate is nearly doubled.\n### 3.3 Remove stop words\nWe observe **(\"This\", \"is\", \"a\", \"good\",\"and\", \"nice\", \"game\")** this sentence. In fact, words like **\"This\" and \"is\" are actually very neutral, no matter whether they appear in spam or not, they are not useful information to help judge. So we can directly ignore these typical words.** These words that are not helpful for our classification are called \"Stop Words\". This can reduce the time for us to train the model and judge the classification.\n\nSo the previous sentence becomes **(\"good\", \"nice\", \"game\")**"},{"metadata":{"trusted":true},"cell_type":"code","source":"stopSet = set({'i', 'im', 'me', 'my', 'myself', 'we', 'our', 'ours', 'us', 'ourselves', 'you', 'your', 'yours', \n               'yourself', \"youve\", 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', \n               'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', \n               'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', \n               'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \n               'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', \n               'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', \n               'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', \n               'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', \n               'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', \n               'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', \n               'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', \n               'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', \n               'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn', \n               \"havent\", \"wont\", 'mustnt', \"neednt\", 'couldnt', 'doesnt', \"shouldnt\", \"wasnt\", 'wouldnt', \"shes\",\n               \"shouldve\", \"werent\", \"isnt\", \"dont\", \"arent\", \"thatll\", \"hasnt\", \"didnt\", \"mightnt\", \"hadnt\", 'youre', 'theyre', })","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.4 Using inverted index dictionary to build vocabulary list\nI use the data structure like: word:\\[The number of occurrences in rating = 1,The number of occurrences in rating = 2,The number of occurrences in rating = 3,The number of occurrences in rating = 4,The number of occurrences in rating = 5\\].This can increase the retrieval speed when using Bayesian algorithm.\n### 3.5 Set the minimum threshold for the number of occurrences of words\nSet the minimum threshold for the number of occurrences of words to eliminate the influence of rare words. After many experiments, the threshold value set as len (train) * 0.00002.\n### 3.6 Handling repeated words using a mixed model\n#### 3.6.1 Polynomial model\nIf we consider the situation of repeated words, that is to say, the repeated words are regarded as their occurrence multiple times, and are directly derived according to the conditional independent assumption, there are:\n\n$P(“good”,“good”,“nice”,“game”∣c)=P(“good”∣c)P(“good”∣c)P(“nice”∣S)P(“game”∣c) $\n\nIn the statistical calculation of $P(\"good\"|c)$, the repeated words in each counted spam sample are counted multiple times.\n\n$P(\"good\"|c)=\\frac{The total number of occurrences of \"good\" in each rating = c comment}{\nThe sum of the number of occurrences of all words (counting the number of repetitions) in each comment with ratings = c}$\n#### 3.6.2 Bernoulli model\nThis more simplified method is to treat repeated words as if they only occur once.\n\n$P(“good”,“good”,“nice”,“game”∣c)=P(“good”∣c)P(“nice”∣S)P(“game”∣c) $\n\nStatistical calculation $P(\"Word\"∣c)$:\n\n$P(\"good\"|c)=\\frac{The number of rating = c comment which occurrences \"good\"}{\nThe sum of the number of occurrences of all words (Only count once) in each comment with ratings = c}$\n\nSuch a model is called a Bernoulli model (also called a binomial independent model). This way is more simplified and convenient. Of course, it loses the word frequency information, so the effect may be worse.\n#### 3.6.3 Mixed model\nThis method does not consider the number of occurrences of repeated words when calculating the probability of a sentence, but considers the number of occurrences of repeated words when calculating the probability P (\"word\" | c) of a word statistically. model."},{"metadata":{"trusted":true},"cell_type":"code","source":"def SegmentLineToWordsSet(sentense):\n    sentense = re.sub(\"[%s]+\"%('\"|#|$|%|&|\\|(|)|\\[|\\]|*|+|\\-|/|<|=|>|@|^|`|{|}|~|,|.|?|!|:|;'), ' ', sentense)\n    sentense = re.sub(\"[%s]+\"%('\\''),'',sentense)\n    #return set([x.lower() for x in re.split(r'[\\s|,|;|.|/|\\[|\\]|;|\\!|?|\\'|\\\\|\\)|\\(|\\\"|@|&|#|-|=|*|%|>|<|^|-]\\s*',sentense.strip()) if x and x not in stopSet and len(x) > 1])\n    return set([x.lower() for x in re.split(r'[\\s]\\s*',sentense.strip()) if x])\n\ndef buildVocabularyList(dataset):\n    dict_list = {}\n    pattern = re.compile('[0-9]+')\n    for row in dataset:\n        words = list(SegmentLineToWordsList(str(row[0]))) #Words that appear multiple times in the same comment are counted only once\n        #words = set()\n        #words = words.union(SegmentLineToWordsSet(str(row[0])))\n        for word in words:\n            if word in stopSet or len(word) == 1:\n            #if len(word) == 1:\n                continue\n            if pattern.findall(word):\n                continue\n            if word not in dict_list:\n                dict_list[word] = [0,0,0,0,0,0] #0-10 is rating,11 is sum\n            dict_list[word][row[1]] += 1\n            dict_list[word][len(dict_list[word])-1] += 1\n    for word in list(dict_list.keys()):\n        if dict_list[word][len(dict_list[word])-1] < len(train) * 0.00002:\n            del dict_list[word]\n    return dict_list\ntrain_dict = buildVocabularyList(train)\ntrain_dict","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Count the number of comments in each category to prepare for calculating the prior probability."},{"metadata":{"trusted":true},"cell_type":"code","source":"def getRatingProbability(dataset):\n    rating_num = [0,0,0,0,0]\n    for row in dataset:\n        rating_num[row[1]] += 1\n    return rating_num\nrating_num = getRatingProbability(dataset_org)\nprint(rating_num)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Count the number of words in each rating to prepare for the mixed model to calculate the conditional probability."},{"metadata":{"trusted":true},"cell_type":"code","source":"def getClassWordNum(dataset):\n    word_num = [0,0,0,0,0]\n    for word in list(train_dict.keys()):\n        for i in range(0,len(word_num)):\n            word_num[i] += train_dict[word][i]\n    return word_num\nword_num = getClassWordNum(dataset_org)\nprint(word_num)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.7 Using smoothing\nSmoothing techniques all give words that do not appear in the training set an estimated probability, and accordingly reduce the probability of other words that have already appeared. The smoothing technology is a real demand that arises because the data set is too small. If the data set is large enough, the effect of the smoothing technique on the results will become smaller. But because 1rating has a small number of comments, it makes sense to use smoothing techniques here.\n\n\nFor the Bernoulli model, a smoothing algorithm for $P(\"good\"|c)$ is:\n\n$P(\"good\"|c)=\\frac{The number of rating = c comment which occurrences \"good\" + lambda}{\nThe sum of the number of occurrences of all words (Only count once) in each comment with ratings = c + lambda * the number of ratings}$\n\n\nFor the Polynomial model, a smoothing algorithm for $P(\"good\"|c)$ is:\n\n$P(\"good\"|c)=\\frac{The total number of occurrences of \"good\" in each rating = c comment + lambda}{\nThe sum of the number of occurrences of all words (counting the number of repetitions) in each comment with ratings = c + lambda * The number of words in the vocabulary counted}$\n\n\n$ 0<lambda<=1$"},{"metadata":{"trusted":true},"cell_type":"code","source":"lambda_value = 0.0005\nlambda_cag = len(rating_num) * lambda_value\ndef getConditionalProbabilityUsingSmoothing(word):\n    conditional_probability = list()\n    for i in range(0,len(rating_num)):\n        if word not in train_dict:\n            pro = lambda_value/(len(train_dict)*lambda_value+word_num[i])\n        else:\n            pro = (lambda_value + train_dict[word][i])/(len(train_dict)*lambda_value+word_num[i])\n        conditional_probability.append(pro)\n    return conditional_probability","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### predict rating function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(review):\n    words = set()\n    words = words.union(SegmentLineToWordsSet(review))\n    probability = np.array(rating_num) / len(train)\n    pattern = re.compile('[0-9]+')\n    for word in words:\n        if pattern.findall(word):\n                continue\n        if word not in stopSet and len(word) > 1:\n        #if len(word) > 1:\n            probability *= getConditionalProbabilityUsingSmoothing(word)\n    probability = list(probability)\n    return probability.index(max(probability))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.Evaluation score\n### 4.1 accuracy metric function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy_metric(test_dataset):\n    correct = 0\n    for row in test_dataset:\n        if row[1] == predict(str(row[0])):\n            correct += 1\n    return correct / float(len(test_dataset)) * 100.0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2 Using part of train set to get the evaluation score\nBecause the data set is very large and training takes a long time, so here we did not use the development set to verify, but use part of the training set to evaluate the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_part = list()\nfor i in range(1,1000):\n    train_part.append(train[i])\nprint('Accuracy: %.3f%%' % accuracy_metric(train_part))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3 Using part of test set to get the evaluation score"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_part = list()\nfor i in range(1,10000):\n    test_part.append(test[i])\nprint('Accuracy: %.3f%%' % accuracy_metric(test_part))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4 Using all test set to get the evaluation score"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Accuracy: %.3f%%' % accuracy_metric(test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.Save the trained model for future website development."},{"metadata":{"trusted":true},"cell_type":"code","source":"f = open('dict_file.txt','w')\nf.write(str(train_dict))\nf.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load the saved model"},{"metadata":{"trusted":true},"cell_type":"code","source":"f = open('dict_file.txt','r')\na = f.read()\nread_dictionary = eval(a)\nf.close()\nprint(read_dictionary['greatest'])\nprint(len(read_dictionary))\nprint(len(train_dict))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.Challenge\na.The entire data set is very large, and training the model once takes too long.\n\nSolution:First, randomly sampled one-tenth of the data for algorithm design and model tuning. When the model optimization is completed, all data is used for training to obtain the final model.\n\nb.There are special characters and other languages in the comments data, it is difficult to use a regular expression to match successfully.\n\nSolution:By consulting multiple data and observing the comment text data for a long time, we finally use two regular expressions to match and observe whether the two results are the same. The first regular expression is to use ASCII, and the second regular expression is to replace punctuation with spaces. The symbol then uses spaces to cut the string.\n\nc.The accuracy of the algorithm is very low, probably between 20% -30%.\n\nSolution:Through many experiments and referred to the iPhone App Store and Amazon ’s 5-point rating standards, it is found that the score is mapped to the 1-5 interval, and then the discretization is performed, and the final algorithm accuracy rate is nearly doubled. And then by using Mixed model, smoothing and stop words, the final accuracy is improved to about 60%. But it is still not enough, if the reader has a better idea and don’t mind sharing with me, please email me, I will be grateful."},{"metadata":{},"cell_type":"markdown","source":"### 7.Hyper parameter tuning\na.smoothing lambda_value = 0.0005\n\nb.Select Mixed model(Combined the Polynomial model and Bernoulli model) to posterior probability.\n\nc.Map ratings to 1-5\n### 8.References\nProfessor Mr. Park's data mining Naïve Bayes lecture.\n\nhttps://machinelearningmastery.com/naive-bayes-classifier-scratch-python/\n\nhttps://en.wikipedia.org/wiki/Naive_Bayes_classifier\n\nhttps://gist.github.com/sebleier/554280\n\nhttps://blog.csdn.net/longxinchen_ml/article/details/50597149"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}