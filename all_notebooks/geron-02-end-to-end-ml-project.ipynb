{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 2章 エンドツーエンドの機械学習プロジェクト\n\n[2019年度 西南学院大学経済学部 演習II 講義ノート（担当 市東亘）](http://courses.wshito.com/semi2/2019-bayes-AI/index.html)  \nテキスト『scikit-learnとTensorFlowによる実践機械学習』Aurelien Geron著（長尾高弘 訳）オライリー出版\n\n\n## 2.3 データの準備\n\nワークスペースに第２章で使うカリフォルニアの住宅価格データセットをコピーする．すでに[別のユーザがKaggleにデータセット](https://www.kaggle.com/harrywang/housing)を追加してくれているのでそれを利用する．\n\n1. 画面右上の「+ADD DATASET」をクリックし，データセットの検索画面を出す．\n1. 画面右上の入力欄に「Search Dataset」と薄く表示されている部分に「housing」と入力．\n1. 入力すると自動で検索が始まる．検索結果が表示されたら「California Housing Data(1990)」の「Add」ボタンを押す．\n\nこれでワークスペースへのデータセットのコピーが開始される．しばらく待つとコピーが完了し，画面右の「Workspace」欄の「input」フォルダにデータセットのフォルダが現れる．\n\n### 2.3.2 データの読み込み\n\nデータファイルへのパスは，画面右「Workspace」内の各ファイルをクリックすると，画面下にデータのプレビューが現れ，その一番上にパスが表示される．例えば，`housing.csv`のパスは，`../input/housing.csv`である．\n\n従って，`housing.csv`を読み込む`load_housing_data()`関数（テキストp.44）の`HOUSING_PATH`変数を`../input`に置き換えれば良い．"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import os             # パスの接続にosモジュールを使用\nimport pandas as pd   # データ操作ライブラリであるpandasモジュールをpdというエイリアスでロード\n\nHOUSING_PATH = \"../input\"  # データファイルのパスをHOUSING_PATH変数に設定\n\ndef load_housing_data(housing_path=HOUSING_PATH):        # load_housing_data関数を定義\n    csv_path = os.path.join(housing_path, \"housing.csv\") # osモジュールを使用してファイルのパスを構築\n    return pd.read_csv(csv_path)                         # csvファイルを読み込み","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3.3 データの構造をざっと見てみる"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing = load_housing_data() # 先ほど定義したload_housing_data関数を実行し結果をhousing変数に読み込む\n\nhousing.head()  # head()メソッドで最初の数行を表示","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`housing`変数に格納されたデータは，DataFrame型のオブジェクト．\n\n* 各行のデータは区域ごとの観測値．\n* 区域の住宅価格中央値 `median_house_value` を様々な属性値を使って予測するのがここの目的．\n* 各列の属性一覧\n  * `longitude`（経度），`latitude`（緯度），`housing_median_age`（築年数の中央値），`total_rooms`（部屋数），`total_bedrooms`\n（寝室数），`population`（人口），`households`（世帯数），`median_income`（収入の中央値），`median_house_value`（住宅価格の中央値），`ocean_proximity`（海との位置関係）の10個．\n\nDataFrameのデータ構造は`info()`メソッドで得られる．（p.45，図2-6）"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.info() # DataFrameの構造を表示","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* `total_bedrooms`属性だけ非NULLのデータが20,433個しかないことが分かる．\n* `ocean_proximity`属性は`float`型ではないので数値データではない．先ほどの`head()`メソッドの結果を見るとカテゴリデータの模様．"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing['ocean_proximity'].value_counts() # カテゴリ毎のカウント数を表示","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"DataFrameの数値属性のサマリを得るには`describe()`メソッドを使う．（p.46，図2-7）"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.describe()  # DataFrameの統計サマリを表示","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"各属性の分布をヒストグラムで見てみる．（p.47，図2-8）"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n# 上の1行はJupyterノートブックでのみ必要\nimport matplotlib.pyplot as plt\nhousing.hist(bins=50, figsize=(20,15))\nplt.show()  # Jupyterノートブックではhist()メソッドが直接描画を出力するので，show()メソッドはなくてもOK","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3.4 テスト・データセットの作成\n\nデータ・スヌーピング・バイアス（data snooping bias）を避けるため，データは最低でも訓練データとテストデータに分ける必要がある．テキストp.30には訓練データ80%，テストデータ20%に分けるとあるが，実際には学習器の訓練中（アルゴリズムやハイパーパラメータの選定）のモデル評価に使うデータと，最終的な予測性能の見積もりに使うデータも分けるのが望ましい．一般には，学習器の訓練中に使う「訓練データセット（training dataset）」と，訓練中のモデル改善評価に使う「検査データセット（validation dataset）」，最後に１回だけ行うモデル性能評価に使う「テストデータセット（test dataset）」の３つに分割する．その割合は順に，50%，25%，25%に分割するのが一般的である．\n\nデータセットを構築する際に，クラスに極端な偏りがあり無作為抽出では全データセットのクラス分布を正しく反映できない場合は，層化（無作為）抽出（stratified random sampling）を使う．\n\n以下ランダム抽出と層化抽出の順に解説する．\n\n### ランダム抽出\n\n以下で定義する`split_data()`関数は，第2引数の`test_ratios`に分割する比率を指定すると，分割されたデータを配列で返す．ランダム抽出を再現する場合は，第3引数の`seed`に数字を指定する．デフォルトでは，0.5, 0.25, 0.25の3つに分割する．"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\ndef split_data (data, test_ratios=[0.5, 0.25, 0.25], seed=None):\n    if sum(test_ratios) != 1.0:\n        print(\"test_ratios must sum up to 1.0\")\n        return\n    if seed:\n        np.random.seed(seed)\n    shuffled_indices = np.random.permutation(len(data))\n    accum = 0\n    beg = 0\n    result = []\n    size = len(data)\n    for i, v in enumerate(test_ratios):\n        accum += v\n        to = int(size*accum)\n        result.append(data.iloc[shuffled_indices[beg:to]])\n        beg = to\n    return result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"使用例を以下に示す．Pythonでは配列を左辺に代入する際，複数の変数にunpackingしてくれる．"},{"metadata":{"trusted":true},"cell_type":"code","source":"training, validation, test = split_data(housing, [0.5, 0.3, 0.2])\nprint(len(training), \", \", len(validation), \", \", len(test))\nprint(len(training) + len(validation)+  len(test), \"==\", len(housing))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training, validation, test = split_data(housing, [0.5, 0.25, 0.25], 1234) # seedを指定\ntraining.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training2, validation2, test2 = split_data(housing, [0.5, 0.25, 0.25], 1234) # seedを指定\ntraining2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 層化抽出\n\n収入の中央値，`median_income`列をベースに層化抽出する方法を示す．層化する前に`median_income`の分布をヒストグラムで見てみる．"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.plot(y=[\"median_income\"], kind='hist')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing[\"median_income\"].max()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"最大値が15.0001．これを5段階の層に分割するために，データの値を圧縮する．最高値を5の値に圧縮するには`1/3`倍すれば良いが，そのまま圧縮すると高額所得の頻度が少ないままになるので`2/3`倍し，5以上の値は全て5番目の層に分類することにする．"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing[\"income_cat\"] = np.ceil(housing[\"median_income\"] * 2.0 / 3.0) # 圧縮した値を新たにincome_cat列に追加\nhousing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace=True) # where(条件, 条件Falseの時の値か処理, 置き換えるか否か)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.plot(y=[\"income_cat\"], kind='hist')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"上のヒストグラムによりうまく5段階に分類できた．`income_cat`の値に応じて層化抽出する．層化抽出するには scikit-learn の`StratifiedShuffleSplit`クラスを使う．`StratifiedShuffleSplit`はデータを2つに分割することしかできないので，0.5, 0.25, 0.25に分割したければ，最初に0.5で分割して，片方をさらに0.5で分割すれば良い．`n_splits`引数は今は1を指定する．この引数はCross Validationという標本再抽出を行う際に使用する．`random_state`引数は再現性のある分割を行う場合の乱数シードの指定である．"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\n\n# 1回目の1/2分割で訓練データセットが確定\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\nfor train_index, half_index in split.split(housing, housing[\"income_cat\"]):\n    strat_train_set = housing.loc[train_index]\n    half = housing.loc[half_index]\n# 2回目の1/2分割で検査データセットとテストデータセットを作成\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=99)\nfor test_index, validation_index in split.split(half, half[\"income_cat\"]):\n    strat_validation_set = housing.loc[validation_index]\n    strat_test_set = housing.loc[test_index]\n\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"各データセットの分布を確認する．"},{"metadata":{"trusted":true},"cell_type":"code","source":"strat_train_set.plot(y=[\"income_cat\"], kind='hist')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"strat_validation_set.plot(y=[\"income_cat\"], kind='hist')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"strat_test_set.plot(y=[\"income_cat\"], kind='hist')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.5 データクリーニングとデータ整形\n\n区域の住宅価格中央値 `median_house_value` を様々な属性値を使って予測するのがここの目的．この予測器を生成する機械学習アルゴリズムを実装した関数が要求するデータの形式にデータを整形する．\n\n- ターゲット: 住宅価格中央値 `median_house_value`\n- 予測子（predictors）: 住宅価格に影響を及ぼしそうな様々な属性データ．\n\nまず，`housing`変数に予測子のみからなるデータフレームを，`housing_labels`変数にターゲットの値（予測の正解値）を格納しておく．"},{"metadata":{"trusted":true},"cell_type":"code","source":"# median_house_value列と層化抽出のために一時的に作成したincome_cat列を取り除いたデータフレームのコピーを返す\nhousing = strat_train_set.drop([\"median_house_value\", \"income_cat\"], axis=1)\n\n# median_house_value列データのコピーを返す\nhousing_labels = strat_train_set[\"median_house_value\"].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.5.1 欠損値の処理\n\n欠損値があると機械学習アルゴリズムで問題が生じるのでデータフレーム内の欠損値の有無をチェックする．\n\nデータフレームの`isnull()`メソッドはデータフレームの各セルの結果を`True`，`False`としてデータフレームと同じ形（次元）のデータ構造で返す．`any()`メソッドは列方向に真偽値の論理和を返す．行方向に見たいい場合は`any(axis=1)`を使う．"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`total_bedrooms`列に欠損値が含まれているのがわかる．欠損値のあるデータ数をカウントしてみる．"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"10,320個のデータ中，107個の欠損値があることがわかる．\n\n"},{"metadata":{},"cell_type":"markdown","source":"**続く...**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}