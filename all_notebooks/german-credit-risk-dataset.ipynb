{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom collections import defaultdict, Counter\nfrom matplotlib.pyplot import plot\nimport matplotlib.pyplot as plt\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import GridSearchCV\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#names of 21 columns\ncols = ['existingchecking', 'duration', 'credithistory', 'purpose', 'creditamount', \n         'savings', 'employmentsince', 'installmentrate', 'statussex', 'otherdebtors', \n         'residencesince', 'property', 'age', 'otherinstallmentplans', 'housing', \n         'existingcredits', 'job', 'peopleliable', 'telephone', 'foreignworker', 'classification']\n\n#load the dataset\ndata = pd.read_csv('/kaggle/input/germancreditdata/german.data', names = cols, delimiter=' ')\n\n# preprocess numerical features\nnum_features = ['creditamount', 'duration', 'installmentrate', 'residencesince', 'age', \n           'existingcredits', 'peopleliable']\n\n# standardization\ndata[num_features] = StandardScaler().fit_transform(data[num_features])\n\n#preprocess categorical features\ncat_features = ['existingchecking', 'credithistory', 'purpose', 'savings', 'employmentsince',\n           'statussex', 'otherdebtors', 'property', 'otherinstallmentplans', 'housing', 'job', \n           'telephone', 'foreignworker']\n\n# one-hot encoding each of every categorical features\ndata = pd.get_dummies(data, columns = cat_features)\n\n# features and target set\nx = data.drop('classification', axis = 1)\n# replace targets with 1=good, 0=bad\ndata.classification.replace([1,2], [1,0], inplace=True)\ny = data.classification","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split features and target\nx = data.drop('classification', axis = 1)\ny = data.classification\nprint('x.shape:', x.shape, '\\ny.shape:', y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create correlation matrix\nf = plt.figure(figsize=(20, 20))\nplt.matshow(x.corr(), fignum=f.number)\nplt.xticks(range(x.shape[1]), fontsize=14, rotation=45)\nplt.yticks(range(x.shape[1]), fontsize=14)\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=14)\nplt.title('Correlation Matrix', fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Principal component analysis\ncov_mat = np.cov(x.T)\neig_vals, eig_vecs = np.linalg.eig(cov_mat)\ntot = sum(eig_vals)\nvar_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\nprint(\"1. Variance Explained\\n\", var_exp)\ncum_var_exp = np.cumsum(var_exp)\nprint(\"\\n\\n2. Cumulative Variance Explained by the first 50 PC\\n\", cum_var_exp[0:51])\nprint(\"\\n\\n3. Percentage of Variance Explained by the first 46 PC together sums up to:\", cum_var_exp[46])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dimensional reduction from 61 to 46\npca = PCA(n_components=46)\nprincipalComponents = pca.fit_transform(x)\nx_pca = pd.DataFrame(data = principalComponents)\nx_pca.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split train, test set\nxtrain, xtest, ytrain, ytest = train_test_split(x_pca, y,test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Since its a classification problem, its important to know if data is balanced or not\nprint(ytrain.value_counts())\nytrain.value_counts().plot.bar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply resampling\nsm = SMOTE()\nxtrain_res, ytrain_res = sm.fit_sample(xtrain, ytrain)\n# Print number of 'good' credits and 'bad credits, should be fairly balanced now\nprint(\"Before SMOTE\")\nunique, counts = np.unique(ytrain, return_counts=True)\nprint(dict(zip(unique, counts)))\nprint(\"After SMOTE\")\nunique, counts = np.unique(ytrain_res, return_counts=True)\nprint(dict(zip(unique, counts)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Classifiers=[LogisticRegression(),\n             DecisionTreeClassifier(),\n             RandomForestClassifier(),\n             GradientBoostingClassifier(),\n             AdaBoostClassifier(),\n             ExtraTreesClassifier(),\n             KNeighborsClassifier(),\n             SVC(),\n             GaussianNB()]\n\npipelines = []\nfor classifier in Classifiers:\n    pipeline = make_pipeline(classifier)\n    pipelines.append(pipeline)\n\ncv_acc = []\nmodel_names = ['LogisticRegression','DecisionTreeClassifier','RandomForestClassifier','GradientBoostingClassifier','AdaBoostClassifier','ExtraTreesClassifier','KNeighborsClassifier','SVC','GaussianNB']\nfor name, pipeline in zip(model_names,pipelines):\n    pipeline.fit(xtrain_res, ytrain_res) \n    pred = pipeline.predict(xtest)\n    cv_accuracies = cross_val_score(estimator=pipeline, X=xtrain_res, y=ytrain_res, cv=5)    \n    cv_acc.append(cv_accuracies.mean())\n    print(name)\n    print('Train acc: ', pipeline.score(xtrain_res, ytrain_res))\n    print('Test acc: ', pipeline.score(xtest, ytest))\n    print(f'CV acc: {cv_accuracies.mean()}')\n    print(classification_report(ytest, pred))\n    print('Confusion_matrix:')\n    print(f'{confusion_matrix(ytest, pred)}')\n    print('*'*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameter values at which cross-validation performance is maximum\nbest_acc = np.argmax(cv_acc)\nbest_classifier = Classifiers[best_acc]\nprint(\"Best classifier: {},\\n Train accuracy: {}, Cv accuracy: {}, Test accuracy: {}\".format(best_classifier, best_classifier.score(xtrain_res, ytrain_res), cv_acc[best_acc], best_classifier.score(xtest, ytest)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params={\n    'bootstrap': [False, True],\n    'class_weight': ['balanced', 'balanced_subsample', None],\n    'max_depth': [None, 3, 5, 7],\n    'max_features': ['auto', 'sqrt', 'log2', None],\n    'n_estimators': [50, 100, 200, 300, 400, 500],\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [3, 5, 7]}\n\nclf = ExtraTreesClassifier()\n\nrs = GridSearchCV(clf, params, cv=5, scoring= 'accuracy')\nrs.fit(xtrain_res, ytrain_res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model = rs.best_estimator_\npreds = best_model.predict(xtest)\ncv_accuracies = cross_val_score(estimator=best_model, X=xtrain_res, y=ytrain_res, cv=5)    \ncv_acc = cv_accuracies.mean()\ntest_acc = accuracy_score(ytest, preds)\nprint(best_model)\nprint('CV accuracy:', cv_acc, 'Test accuracy:', test_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = pd.DataFrame(ytest)\na.insert(1,'predictions', preds)\na\npd.merge(data, a.predictions, left_index=True, right_index=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}