{"cells":[{"metadata":{},"cell_type":"markdown","source":"<i><p style=\"font-size:24px; background-color: #ff9933; border: 2px dotted black; margin: 20px; padding: 20px;\">This kernel is dedicated to doing all the data preparation, transformation and feature engineering which was previously done in [ChaiEDA: India's Air Quality 2015-20 ðŸ‡®ðŸ‡³](https://www.kaggle.com/neomatrix369/chaieda-india-s-air-quality-2015-20), which is now going to contain visualisations and narrations and make use of the datasets prepared via this kernel. The extended [Air Quality India dataset can be found here](https://www.kaggle.com/neomatrix369/air-quality-data-in-india-extended). Please feel free to use/share it with you own notebooks.\n\n_(background colour is Saffron - if this gives a hint to you why I chose it so)_.\n    "},{"metadata":{},"cell_type":"markdown","source":"![](https://nirvanabeing.com/wp-content/uploads/2018/04/iaq_blog_1.jpg)"},{"metadata":{"trusted":true},"cell_type":"code","source":"DATASET_UPLOAD_FOLDER='/kaggle/working/upload'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%bash\nUPLOAD_FOLDER=/kaggle/working/upload\nmkdir -p ${UPLOAD_FOLDER}\ncp /kaggle/input/air-quality-data-in-india/*.csv ${UPLOAD_FOLDER} || true\ncp /kaggle/input/air-quality-data-in-india-extended/*.csv ${UPLOAD_FOLDER} || true\ncp /kaggle/input/air-quality-data-in-india-extended/*.fth ${UPLOAD_FOLDER} || true","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import os\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom math import pi\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nfrom IPython.display import HTML,display\n\nsns.set(style=\"whitegrid\", font_scale=1.75)\n\n\n# prettify plots\nplt.rcParams['figure.figsize'] = [20.0, 5.0]\n    \n%matplotlib inline\n\nwarnings.filterwarnings(\"ignore\")\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"%%time\ndf_station_hour = pd.read_csv(\"/kaggle/input/air-quality-data-in-india/station_hour.csv\")\ndf_city_hour    = pd.read_csv(\"/kaggle/input/air-quality-data-in-india/city_hour.csv\")\ndf_station_day  = pd.read_csv(\"/kaggle/input/air-quality-data-in-india/station_day.csv\")\ndf_city_day     = pd.read_csv(\"/kaggle/input/air-quality-data-in-india/city_day.csv\")\ndf_stations     = pd.read_csv(\"/kaggle/input/air-quality-data-in-india/stations.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def force_regenerate_dataset(force_regenerate: bool, dataset_name: str, target_dataframe: pd.DataFrame): \n    csv_filename = f\"{DATASET_UPLOAD_FOLDER}/{dataset_name}.csv\"\n    fth_filename = f\"{DATASET_UPLOAD_FOLDER}/{dataset_name}.fth\"\n    if force_regenerate or (not os.path.exists(fth_filename)) or (not os.path.exists(csv_filename)):\n        print(f\"{fth_filename} NOT found, will regenerate the dataset\")\n        target_dataframe.to_feather(fth_filename)\n        target_dataframe.to_csv(csv_filename)\n    else:\n        print(f\"{fth_filename} found, will NOT regenerate the dataset\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('Below is a list of columns of tables just as they are loaded:')\nprint('~~~')\nprint(f'df_stations: {list(df_stations.columns)}')\nprint('~~~')\nprint(f'df_station_day: {list(df_station_day.columns)}')\nprint('~~~')\nprint(f'df_station_hour: {list(df_station_hour.columns)}')\nprint('~~~')\nprint(f'df_city_day: {list(df_city_day.columns)}')\nprint('~~~')\nprint(f'df_city_hour: {list(df_city_hour.columns)}')\nprint('~~~')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<i><p style=\"font-size:16px; background-color: #66cdde; border: 2px dotted black; margin: 20px; padding: 20px;\">For initial EDA steps to view datasets, please refer to the [original kernel](https://www.kaggle.com/frtgnn). Here we start with some Data transformation and Feature engineering to summarise the information provided."},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering\n\nWhy introduce feature engineering already, we are not going to build a model in EDA? But the process of Feature Engineering helps understand the data much better by the process of summarisation and creation of classes that are hidden in granular data.\n\nThen using these classes and features as stepping stones we can climb up the layers and get a **\"birds-eye-view\"** i.e. a **big picture** of the table of data or the datasets provided.\n\nSuch summarisation processes also magnify or isolate bugs or issues or anomalies in the data itself (quality of the data or correctness of the data can be revealed via such processes)."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fields_to_show = ['City','AQI_Bucket']","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fields_to_ignore = ['StationId', 'StationName', 'State', 'Status', 'Region', 'Month', 'Year', 'Season', 'City', 'Date', 'AQI', 'AQI_Bucket']\nnames_of_pollutants = list(set(df_city_day.columns) - set(fields_to_ignore))\nprint(f\"Names of Pollutants: {list(names_of_pollutants)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Filling in AQI_Bucket missing values across all tables"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%time\ndf_station_day['AQI_Bucket'].fillna('Unknown', inplace=True)\ndf_station_hour['AQI_Bucket'].fillna('Unknown', inplace=True)\ndf_city_day['AQI_Bucket'].fillna('Unknown', inplace=True)\ndf_city_hour['AQI_Bucket'].fillna('Unknown', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding Regions to the Stations table\n\nUsing the classifications mentioned in https://en.wikipedia.org/wiki/Administrative_divisions_of_India"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"regions = ['1. Northern', '2. North Eastern', '3. Central', '4. Eastern', '5. Western', '6. Southern']\nstate_to_region_mapping = {\n    'Andhra Pradesh': regions[4], 'Assam': regions[1] , 'Bihar': regions[3], 'Chandigarh': regions[0],  \n    'Delhi': regions[0], 'Gujarat': regions[4], 'Haryana': regions[0], 'Jharkhand': regions[3], \n    'Karnataka': regions[5], 'Kerala': regions[5], 'Madhya Pradesh': regions[2], 'Maharashtra': regions[5], \n    'Meghalaya': regions[1], 'Mizoram': regions[1], 'Odisha': regions[3], 'Punjab': regions[0], \n    'Rajasthan': regions[0], 'Tamil Nadu': regions[5], 'Telangana': regions[5], 'Uttar Pradesh': regions[0],\n    'West Bengal': regions[3]\n}\n\ndef state_to_region(state):\n    if state in state_to_region_mapping:\n        return state_to_region_mapping[state]\n    return 'None'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%time\ndf_stations['Region'] = df_stations['State'].apply(state_to_region)\ndf_stations","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Filling in missing values in the Stations table\nThe status field has a number of NaN values, about 97 of the stations and also see the proportion of inactive, active and unknown status stations across the various Regions"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_stations['Status'].fillna('Unknown', inplace=True)\ndf_stations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%time\nforce_regenerate_dataset(False, 'stations_transformed', df_stations)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding states and regions to the City_day, Station_day, City_hour, Station_hour tables"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%time\ndf_city_day = df_city_day.merge(df_stations)\ndf_city_day[fields_to_show + list(df_stations.columns)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%time\ndf_station_day = df_station_day.merge(df_stations)\ndf_station_day[fields_to_show + list(df_stations.columns)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%time\ndf_city_hour = df_city_hour.merge(df_stations)\ndf_city_hour[fields_to_show + list(df_stations.columns)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%time\ndf_station_hour = df_station_hour.merge(df_stations)\ndf_station_hour[fields_to_show + list(df_stations.columns)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding AQ_Acceptability, Holidays, Day kind, Month, Year and Seasons to the City_day and Station_day tables\n\nSee https://en.wikipedia.org/wiki/Climate_of_India for details on seasons.\n\nHolidays information is based on the PyPi package `holidays`.\n\n`AQ_Acceptability` field is computed by using the idea (from David During) to separate the `AQI_Bucket` values into two different categories i.e. _Acceptable_ (for AQI_Bucket values of Good and Satisfactory) and _Unacceptable_ (all others).\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"old_and_new_fields_to_show = list(set(['Region', 'Season', 'Year', 'Month', \n                                       'Weekday_or_weekend', 'Regular_day_or_holiday', 'AQ_Acceptability'] + fields_to_show) \n                                  - set(['StationId', 'Date']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# The country's meteorological department follows the international standard of four seasons with some local adjustments: \n# - winter (January and February)\n# - summer (March, April and May) \n# - monsoon (rainy) season (June to September)\n# - post-monsoon period (October to December)\n\ndate_to_season_mapping = {'1. Winter': [1, 2], '2. Summer': [3, 5], '3. Monsoon': [6, 9],  '4. Post-Monsoon': [10, 12]}\n\ndef date_to_season(dates):\n    results = []\n    date_values = dates.values\n    for date in date_values:\n        month = int(date.split('-')[1])\n        result = 'None'\n        for each_season in date_to_season_mapping:\n            start, end = date_to_season_mapping[each_season]\n            if ((start < end) and (start <= month <= end)) or \\\n               ((start > end) and ((month >= start) or (month <= end))):\n                result = each_season\n                break\n\n        results.append(result)\n    return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"month_no_to_name_mapping = [\n    '01. Jan', '02. Feb', '03. Mar', '04. Apr', '05. May', '06. Jun', '07. Jul', \n    '08. Aug', '09. Sep', '10. Oct', '11. Nov', '12. Dec'\n]\n\ndef date_to_month_name(dates):\n    month_values = pd.DatetimeIndex(dates).month.values\n    results = []\n    for month in month_values:\n        result = month_no_to_name_mapping[month - 1]\n        results.append(result)\n    return results\n\ndef weekday_or_weekend(dates):\n    results = []\n    for date_value in pd.DatetimeIndex(dates.values):\n        weekno = date_value.weekday()\n        result = \"Weekday\" if weekno < 5 else \"Weekend\"\n        results.append(result)\n    return results\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import holidays\nholidays_india = holidays.India()\n\ndef regular_day_or_holiday(dates):\n    results = []\n    for date_value in pd.DatetimeIndex(dates.values):\n        result = \"Holiday (or Festival)\" if date_value.date() in holidays_india else \"Regular day\"\n        results.append(result)\n    return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def aq_acceptability(aqi_bucket):\n    results = []\n    for each_aqi_bucket in aqi_bucket.values:\n        result = \"Acceptable\" if each_aqi_bucket \\\n                in [\"Good\", \"Satisfactory\"] else \"Unacceptable\"\n        results.append(result)\n    return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%time\ndf_city_day['Month'] = date_to_month_name(df_city_day['Date'])\ndf_city_day['Year'] = pd.DatetimeIndex(df_city_day['Date']).year\ndf_city_day['Season'] = date_to_season(df_city_day['Date'])\ndf_city_day['Weekday_or_weekend'] = weekday_or_weekend(df_city_day['Date'])\ndf_city_day['Regular_day_or_holiday'] = regular_day_or_holiday(df_city_day['Date'])\ndf_city_day['AQ_Acceptability'] = aq_acceptability(df_city_day['AQI_Bucket'])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_city_day[old_and_new_fields_to_show]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%time\nforce_regenerate_dataset(False, 'city_day_transformed', df_city_day)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%time\ndf_station_day['Month'] = date_to_month_name(df_station_day['Date'])\ndf_station_day['Year'] = pd.DatetimeIndex(df_station_day['Date']).year\ndf_station_day['Season'] = date_to_season(df_station_day['Date'])\ndf_station_day['Weekday_or_weekend'] = weekday_or_weekend(df_station_day['Date'])\ndf_station_day['Regular_day_or_holiday'] = regular_day_or_holiday(df_station_day['Date'])\ndf_station_day['AQ_Acceptability'] = aq_acceptability(df_station_day['AQI_Bucket'])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_station_day[old_and_new_fields_to_show]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%time\nforce_regenerate_dataset(False, 'station_day_transformed', df_station_day)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding Holidays, Day kind, Day period to the City_hour and State_hour tables\n\nDefinitions of morning, afternoon, evening, and night as per Wikipedia:\n- [Morning](https://en.wikipedia.org/wiki/Morning): Morning is the period of time from sunrise to noon (4am to 11:59am)\n- [Afternoon](https://en.wikipedia.org/wiki/Afternoon): Afternoon is the time of the day between noon and evening (12pm to 5pm)\n- [Evening](https://en.wikipedia.org/wiki/Evening): Evening is the period of time from the end of the afternoon to the beginning of night (5pm to 8pm).\n- [Night or nighttime](https://en.wikipedia.org/wiki/Night): Night or nighttime (also spelled night-time or night time) is the period of ambient darkness from sunset to sunrise . Start around 8 pm and to last to about 4 am.\n\nHolidays information is based on the PyPi package `holidays`.\n\n`AQ_Acceptability` field is computed by using the idea (from David During) to separate the `AQI_Bucket` values into two different categories i.e. _Acceptable_ (for AQI_Bucket values of Good and Satisfactory) and _Unacceptable_ (all others)."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"date_to_day_period_mapping = {'1. Morning': [4, 11], '2. Afternoon': [12, 17], \n                              '3. Evening': [18, 19], '4. Night': [20, 4]}\ndef date_to_day_period(datetimes):\n    results = []\n    datetime_values = datetimes.values\n    for datetime in datetime_values:\n        _, time_of_day = datetime.split(' ')\n        hour, _, _ = time_of_day.split(':')\n        hour = int(hour)\n        result = 'None'\n        for each_day_period in date_to_day_period_mapping:\n            start, end = date_to_day_period_mapping[each_day_period]\n            if ((start < end) and (start <= hour <= end)) or \\\n               ((start > end) and ((hour >= start) or (hour <= end))):\n                result = each_day_period\n                break\n\n        results.append(result)\n    return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%time\ndf_city_hour['Day_period'] = date_to_day_period(df_city_hour['Datetime'])\ndf_city_hour['Month'] = date_to_month_name(df_city_hour['Datetime'])\ndf_city_hour['Year'] = pd.DatetimeIndex(df_city_hour['Datetime']).year\ndf_city_hour['Season'] = date_to_season(df_city_hour['Datetime'])\ndf_city_hour['Weekday_or_weekend'] = weekday_or_weekend(df_city_hour['Datetime'])\ndf_city_hour['Regular_day_or_holiday'] = regular_day_or_holiday(df_city_hour['Datetime'])\ndf_city_hour['AQ_Acceptability'] = aq_acceptability(df_city_hour['AQI_Bucket'])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_city_hour[set(old_and_new_fields_to_show + [\"Day_period\", \"Weekday_or_weekend\", 'Regular_day_or_holiday', 'AQ_Acceptability'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%time\nforce_regenerate_dataset(False, 'city_hour_transformed', df_city_hour)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%time\ndf_station_hour['Day_period'] = date_to_day_period(df_station_hour['Datetime'])\ndf_station_hour['Month'] = date_to_month_name(df_station_hour['Datetime'])\ndf_station_hour['Year'] = pd.DatetimeIndex(df_station_hour['Datetime']).year\ndf_station_hour['Season'] = date_to_season(df_station_hour['Datetime'])\ndf_station_hour['Weekday_or_weekend'] = weekday_or_weekend(df_station_hour['Datetime'])\ndf_station_hour['Regular_day_or_holiday'] = regular_day_or_holiday(df_station_hour['Datetime'])\ndf_station_hour['AQ_Acceptability'] = aq_acceptability(df_station_hour['AQI_Bucket'])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_station_hour[set(old_and_new_fields_to_show + [\"Day_period\", \"Weekday_or_weekend\", \n                                                  'Regular_day_or_holiday', 'AQ_Acceptability'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%time\nforce_regenerate_dataset(False, 'station_hour_transformed', df_station_hour)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Uploading newly created/updated csv to your Kaggle Dataset\n\nSetup your local environment with your Kaggle login details (`KAGGLE_KEY` and `KAGGLE_USERNAME`)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nimport os\nos.environ['KAGGLE_KEY'] = user_secrets.get_secret(\"KAGGLE_KEY\")\nos.environ['KAGGLE_USERNAME'] = user_secrets.get_secret(\"KAGGLE_USERNAME\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the `kaggle` Python client login, into your account from within the kernel."},{"metadata":{"trusted":true},"cell_type":"code","source":"import kaggle\nkaggle.api.authenticate()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get the metadata for the dataset you have already created manually - it's best to manually create it and upload the initial csv file(s) into it, to avoid subsequent issues with updating the dataset (as seen during my own end-to-end cycle).\n\nSave the metadata file as a json file but before that, add/update two keys `id` and `id_no` with the respective details as shown below and then save it."},{"metadata":{"trusted":true},"cell_type":"code","source":"OWNER_SLUG='neomatrix369'\nDATASET_SLUG='air-quality-data-in-india-extended'\ndataset_metadata = kaggle.api.metadata_get(OWNER_SLUG, DATASET_SLUG)\ndataset_metadata['id'] = dataset_metadata[\"ownerUser\"] + \"/\" + dataset_metadata['datasetSlug']\ndataset_metadata['id_no'] = dataset_metadata['datasetId']\nimport json\nwith open(f'{DATASET_UPLOAD_FOLDER}/dataset-metadata.json', 'w') as file:\n    json.dump(dataset_metadata, file, indent=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally call the `dataset_create_version()` api and pass it the folder where the metadata file exists and also where your `.csv` and `.fth` file(s) - those file(s) that you would like to upload into your existing Dataset (as a new version)."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# !kaggle datasets version -m \"Updating datasets\" -p /kaggle/working/upload\nkaggle.api.dataset_create_version(DATASET_UPLOAD_FOLDER, 'Updating datasets')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Credits\n\n- Forked from [Firat Gonen](https://www.kaggle.com/frtgnn)'s [Clean Air? India's Air Quality ðŸ‡®ðŸ‡³](https://www.kaggle.com/frtgnn/clean-air-india-s-air-quality) kernel - thanks for the foundation work\n- David During for all the insights during the ChaiEDA sessions, and also building on his idea of the KPI based on the AQI Index"},{"metadata":{},"cell_type":"markdown","source":"<i><p style=\"font-size:24px; background-color: #ff9933; border: 2px dotted black; margin: 20px; padding: 20px;\">This kernel is dedicated to doing all the data preparation, transformation and feature engineering which was previously done in [ChaiEDA: India's Air Quality 2015-20 ðŸ‡®ðŸ‡³](https://www.kaggle.com/neomatrix369/chaieda-india-s-air-quality-2015-20), which is now going to contain visualisations and narrations and make use of the datasets prepared via this kernel. The extended [Air Quality India dataset can be found here](https://www.kaggle.com/neomatrix369/air-quality-data-in-india-extended).\nPlease feel free to use/share it with your own notebooks."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}