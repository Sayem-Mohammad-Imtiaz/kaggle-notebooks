{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install kulc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd \nimport cv2\nimport numpy as np\nimport os\nfrom glob import glob\nimport math\nimport matplotlib.pyplot as plt\n\nimport re\nimport html\nimport string\nimport unicodedata\nfrom nltk.tokenize import word_tokenize\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport tensorflow\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom keras.layers import Input, Dense, LSTM, TimeDistributed, Embedding, Lambda\nfrom kulc.attention import ExternalAttentionRNNWrapper\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import regularizers\nimport tensorflow.keras.backend as K\nfrom nltk.translate.bleu_score import corpus_bleu","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df  =pd.read_csv(\"/kaggle/input/chest-xrays-indiana-university/indiana_reports.csv\")\ndf2 = pd.read_csv(\"/kaggle/input/chest-xrays-indiana-university/indiana_projections.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_special_chars(text):\n    re1 = re.compile(r'  +')\n    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n    return re1.sub(' ', html.unescape(x1))\n\n\ndef remove_non_ascii(text):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n\n\ndef to_lowercase(text):\n    return text.lower()\n\ndef remove_punctuation(text):\n    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n    translator = str.maketrans('', '', string.punctuation)\n    return text.translate(translator)\n\n\ndef replace_numbers(text):\n    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n    return re.sub(r'\\d+', '', text)\n\n\ndef remove_whitespaces(text):\n    return text.strip()\n\n\ndef remove_stopwords(words, stop_words):\n    \"\"\"\n    :param words:\n    :type words:\n    :param stop_words: from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n    or\n    from spacy.lang.en.stop_words import STOP_WORDS\n    :type stop_words:\n    :return:\n    :rtype:\n    \"\"\"\n    return [word for word in words if word not in stop_words]\n\n\ndef stem_words(words):\n    \"\"\"Stem words in text\"\"\"\n    stemmer = PorterStemmer()\n    return [stemmer.stem(word) for word in words]\n\ndef lemmatize_words(words):\n    \"\"\"Lemmatize words in text\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return [lemmatizer.lemmatize(word) for word in words]\n\ndef lemmatize_verbs(words):\n    \"\"\"Lemmatize verbs in text\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return ' '.join([lemmatizer.lemmatize(word, pos='v') for word in words])\n\ndef text2words(text):\n    return word_tokenize(text)\n\ndef normalize_text( text):\n    text = remove_special_chars(text)\n    text = remove_non_ascii(text)\n    text = remove_punctuation(text)\n    text = to_lowercase(text)\n    text = replace_numbers(text)\n    #words = text2words(text)\n    #stop_words = stopwords.words('english')\n    #words = remove_stopwords(words, stop_words)\n    #words = stem_words(words)# Either stem ovocar lemmatize\n    #words = lemmatize_words(words)\n    #words = lemmatize_verbs(words)\n\n    return text\n  \ndef normalize_corpus(corpus):\n    return [normalize_text(t) for t in corpus]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['report'] = df[df.columns[1:]].apply(\n    lambda x: ','.join(x.astype(str)),\n    axis=1)\ndf['report'] = df['report'].apply(normalize_text)\ndf['report'] = 'startseq '+df['report']+' endseq'\nvocab_size = 10000\nmax_len = 260\n\ntok = Tokenizer(num_words=vocab_size,  oov_token='UNK', )\ntok.fit_on_texts(df['report'].tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = df2[df2['projection']=='Frontal']\ndf  =pd.merge(df,df2,  on=['uid'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class det_gen(tensorflow.keras.utils.Sequence):\n    'Generates data from a Dataframe'\n    def __init__(self,df, tok, max_len,images_path, dim=(256,256), batch_size=8):\n        self.df=df\n        self.dim = dim\n        self.images_path = images_path\n        self.tok= tok\n        self.max_len = max_len\n        self.batch_size = batch_size\n        self.nb_iteration = math.ceil((self.df.shape[0])/self.batch_size)\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return self.nb_iteration\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.df=self.df.sample(frac=1)\n    \n    def load_img(self, img_path):\n        \n        img = cv2.imread(img_path)\n        img =cv2.resize(img,(self.dim))\n        \n        \n        return img\n        \n    \n    def __getitem__(self, index):\n        'Generate one batch of data'\n        \n        indicies = list(range(index*self.batch_size, min((index*self.batch_size)+self.batch_size ,(self.df.shape[0]))))\n        \n        images = []\n        for img_path in self.df['filename'].iloc[indicies].tolist():\n            img = self.load_img(os.path.join(self.images_path,img_path))\n            images.append(img)\n            \n            \n        \n        \n        x_batch = self.df['report'].iloc[indicies].tolist()\n        \n        x_batch_input = [sample[:-len(\" <end>\")] for sample in x_batch]\n        \n        x_batch_gt = [sample[len(\" <start>\"): ] for sample in x_batch]\n        \n        \n        x_batch_input = np.array(pad_sequences( self.tok.texts_to_sequences (x_batch_input),\n                          maxlen=self.max_len-1 ,\n                          padding='post',\n                          truncating='post'))\n        \n        x_batch_gt = np.array(pad_sequences( self.tok.texts_to_sequences (x_batch_gt),\n                          maxlen=self.max_len-1 ,\n                          padding='post',\n                          truncating='post'))\n        \n        \n        \n        \n        \n        \n        return [np.array(images), np.array(x_batch_input)] , np.array(x_batch_gt)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_split= 0.2\nimages_path = \"/kaggle/input/chest-xrays-indiana-university/images/images_normalized/\"\ndf = df.sample(frac=1)\ndf_train = df.iloc[:-int(df.shape[0]*validation_split)]\ndf_val   = df.iloc[-int(df.shape[0]*validation_split):]\ntrain_dataloader =  det_gen(df_train, tok, max_len,images_path)\nval_dataloader =  det_gen(df_val, tok, max_len,images_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install keras-self-attention","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras_self_attention import SeqSelfAttention\n## Input layers\nimg_input = layers.Input(shape= (256,256,3)) \nreport_input= layers.Input(shape= (max_len-1,))\n\n## Encoder ######################\n\nDensenet_model = tf.keras.applications.DenseNet121(\n            include_top=False,\n            weights=None,#\"imagenet\",\n            input_shape=(256,256,3),\n        )\nnumber_of_encoder_layers=  len(Densenet_model.layers)\n\nencoder_output = Densenet_model(img_input)\nencoder_output = layers.Flatten()(encoder_output)\n\nX_img = layers.Dropout(0.5)(encoder_output)\nX_img = layers.Dense(300, use_bias = False, \n                        kernel_regularizer=regularizers.l2(1e-4),\n                        name = 'dense_img')(X_img)\nX_img = layers.BatchNormalization(name='batch_normalization_img')(X_img)\nX_img = layers.Lambda(lambda x : K.expand_dims(x, axis=1))(X_img)\n\n##decoder ########################\n\nX_text = layers.Embedding(vocab_size, 300, mask_zero = True, name = 'emb_text')(report_input)\nX_text = layers.Dropout(0.5)(X_text)\n\n# Initial States\n\n\nLSTMLayer = layers.LSTM(300, return_sequences = True, return_state = True, dropout=0.5, name = 'lstm')\n\n# Take image embedding as the first input to LSTM\n_, a, c = LSTMLayer(X_img)\n\nA, _, _ = LSTMLayer(X_text, initial_state=[a, c])\n# attention = Dense(1, activation='tanh')(A)\n# attention = Flatten()(attention)\n# attention = Activation('softmax')(attention)\n# attention = RepeatVector(300)(attention)\n# attention = Permute([2, 1])(attention)\n# sent_representation = merge([activations, attention], mode='mul')\n# sent_representation = Lambda(lambda xin: K.sum(xin, axis=-2), output_shape=(units,))(sent_representation)\nattention = SeqSelfAttention(attention_activation='sigmoid')(A)\n\noutput = layers.TimeDistributed(layers.Dense(vocab_size, activation='softmax',\n                                 kernel_regularizer = regularizers.l2(1e-4), bias_regularizer = regularizers.l2(1e-4))\n                                , name = 'time_distributed_softmax')(attention)\n\n\n\n\nmodel  = Model(inputs=[img_input, report_input], outputs=output, name='NIC_greedy_inference_v2')\n\n\n##Inference models ################\n\n#encoder_inference model\nencoder_model = Model(img_input,[a,c])\n\n# Decoder model ###################\n\na0 = layers.Input(shape=(300,))\nc0 = layers.Input(shape=(300,))\n\nA, alast, clast = LSTMLayer(X_text, initial_state=[a0, c0])\noutput = layers.TimeDistributed(layers.Dense(vocab_size, activation='softmax',\n                                 kernel_regularizer = regularizers.l2(1e-4), \n                                 bias_regularizer = regularizers.l2(1e-4)), name = 'time_distributed_softmax')(A)\n\n\n\ndecoder_model = Model([report_input,a0,c0],[output,alast,clast])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs =5\nlr=1e-4\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer=Adam(lr))\nhist = model.fit_generator( train_dataloader,validation_data = val_dataloader,epochs = epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokens_to_text(tokens,tok,end_token='endseq'):\n    sentence=\"\"\n    for token in tokens:\n        if token ==0:\n            break\n        \n        word = tok.index_word[token]\n        \n        if word==end_token:\n            break\n            \n        sentence+= word+\" \"\n        \n    sentence = sentence.strip()\n    \n    return sentence\n\n\ndef greedy_inference(input_img, tok,encoder_model, decoder_model,max_len,start_token=\"startseq\",end_token='endseq'\n                     ,decoder_type=\"LSTM\"):\n    if decoder_type=='LSTM':\n        a0,c0  =encoder_model(np.expand_dims(input_img,axis=0))\n    elif decoder_type=='GRU': \n        hidden_layer  =encoder_model(np.expand_dims(input_img,axis=0))\n        \n    word = tok.word_index[start_token]\n    \n    words = []\n    \n    for index in range(max_len):\n        if decoder_type=='LSTM':\n            word_probs , a0,c0 = decoder_model.predict([[np.array([word]),a0,c0]])\n        elif decoder_type=='GRU': \n            word_probs , hidden_layer = decoder_model.predict([[np.array([word]),hidden_layer]])\n            hidden_layer=hidden_layer[0]\n        \n        word = np.argmax(word_probs)\n        \n        try:\n            if tok.index_word[word]==end_token:\n                break\n        except:\n            pass\n        \n        words.append(word)\n        \n    words = tokens_to_text(words,tok,end_token)\n    return words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_predictions_from_data_loader(data_loader,tok,encoder_model, decoder_model,max_len,start_token=\"startseq\"\n                                     ,end_token='endseq', inference_type='greedy',decoder_type='LSTM'):\n    \n    data_loader_iterator = data_loader.__iter__()\n    \n    pred_sentences = []\n    Gt_sentences = []\n    for index, (X,Y) in enumerate(data_loader_iterator):\n        for img,_,sample_y in zip(X[0],X[1],Y):\n            \n            if inference_type=='greedy':\n                pred_sentence = greedy_inference(img, tok,encoder_model, decoder_model,max_len,\n                                                 start_token=start_token,end_token=end_token,decoder_type=decoder_type)\n            \n            GT_sentence   = tokens_to_text(sample_y,tok)\n            \n            pred_sentences.append(pred_sentence)\n            Gt_sentences.append(GT_sentence)\n        \n        if index == data_loader.nb_iteration -1:\n            break\n        print(\"Done with batch number: {} \", index)\n        \n    return Gt_sentences, pred_sentences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_bleu_evaluation(GT_sentences, predicted_sentences):\n    BLEU_1 = corpus_bleu(GT_sentences, predicted_sentences, weights=(1.0, 0, 0, 0))\n    BLEU_2 = corpus_bleu(GT_sentences, predicted_sentences, weights=(0.5, 0.5, 0, 0))\n    BLEU_3 = corpus_bleu(GT_sentences, predicted_sentences, weights=(0.3, 0.3, 0.3, 0))\n    BLEU_4 = corpus_bleu(GT_sentences, predicted_sentences, weights=(0.25, 0.25, 0.25, 0.25))\n    \n    return BLEU_1,BLEU_2,BLEU_3,BLEU_4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate_from_dataloader(data_loader,tok,encoder_model, decoder_model,max_len,start_token='startseq',end_token=' endseq', inference_type='greedy'\n                             ,decoder_type=\"LSTM\"):\n    Gt_sentences, pred_sentences = get_predictions_from_data_loader(data_loader,tok,encoder_model, decoder_model,max_len,start_token=start_token,end_token=end_token, inference_type=inference_type,decoder_type=decoder_type)\n    BLEU_1,BLEU_2,BLEU_3,BLEU_4 = calculate_bleu_evaluation(Gt_sentences, pred_sentences)\n    \n    return BLEU_1,BLEU_2,BLEU_3,BLEU_4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BLEU_1,BLEU_2,BLEU_3,BLEU_4 =  evaluate_from_dataloader(val_dataloader,tok,encoder_model, decoder_model,max_len)\nprint(\"bleu 1 :\"+BLEU_1)\nprint('-------')\nprint(\"bleu 2: \"+BLEU_2)\nprint('-------')\nprint(\"bleu 3: \"+BLEU_3)\nprint('-------')\nprint(\"bleu 4: \"+BLEU_4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_size=300\nvocabulary_size=len(tok.word_index) + 1\n## Input layers\nimg_input = layers.Input(shape= (256,256,3)) \nreport_input= layers.Input(shape= (max_len-1,))\n\n## Encoder ######################\n\nDensenet_model = tf.keras.applications.VGG16(\n            include_top=False,\n            weights=None,\n            input_shape=(256,256,3),\n        )\nnumber_of_encoder_layers=  len(Densenet_model.layers)\n\nencoder_output = Densenet_model(img_input)\n# encoder_output = layers.Flatten()(encoder_output)\n# encoder_output = Dense((7*7*512))\n# encoder_output=layers.Reshape((7*7,512))\n\ncaptions = Embedding(vocabulary_size, embedding_size, input_length=max_len-1)(report_input)\n\naveraged_image_features = Lambda(lambda x: K.mean(x, axis=1))\naveraged_image_features = averaged_image_features(encoder_output)\ninitial_state_h = Dense(embedding_size)(averaged_image_features)\ninitial_state_c = Dense(embedding_size)(averaged_image_features)\nimage_features = TimeDistributed(Dense(512, activation=\"relu\"))(encoder_output)\n\nencoder = LSTM(embedding_size, return_sequences=True, return_state=True, recurrent_dropout=0.1)\nattented_encoder = ExternalAttentionRNNWrapper(encoder, return_attention=True)\n\noutput = TimeDistributed(Dense(vocabulary_size, activation=\"softmax\"), name=\"output\")\n\n# for training purpose\nattented_encoder_training_data, _, _ , _= attented_encoder([captions, image_features], initial_state=[initial_state_h, initial_state_c])\ntraining_output_data = output(attented_encoder_training_data)\n\nmodel = Model(inputs=[captions_input, image_features_input], outputs=training_output_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import merge\n## Input layers\nimg_input = layers.Input(shape= (256,256,3)) \nreport_input= layers.Input(shape= (max_len-1,))\n\n## Encoder ######################\n\nDensenet_model = tf.keras.applications.DenseNet121(\n            include_top=False,\n            weights=None,#\"imagenet\",\n            input_shape=(256,256,3),\n        )\nnumber_of_encoder_layers=  len(Densenet_model.layers)\n\nencoder_output = Densenet_model(img_input)\nencoder_output = layers.Flatten()(encoder_output)\n\nX_img = layers.Dropout(0.5)(encoder_output)\nX_img = layers.Dense(300, use_bias = False, \n                        kernel_regularizer=regularizers.l2(1e-4),\n                        name = 'dense_img')(X_img)\nX_img = layers.BatchNormalization(name='batch_normalization_img')(X_img)\nX_img = layers.Lambda(lambda x : K.expand_dims(x, axis=1))(X_img)\n\n##decoder ########################\n\nX_text = layers.Embedding(vocab_size, 300, mask_zero = True, name = 'emb_text')(report_input)\nX_text = layers.Dropout(0.5)(X_text)\n\n# Initial States\n\n\nLSTMLayer = layers.LSTM(300, return_sequences = True, return_state = True, dropout=0.5, name = 'lstm')\n\n# Take image embedding as the first input to LSTM\n_, a, c = LSTMLayer(X_img)\n\nA, _, _ = LSTMLayer(X_text, initial_state=[a, c])\nattention = Dense(1, activation='tanh')(A)\nattention = layers.Flatten()(attention)\nattention = layers.Activation('softmax')(attention)\nattention = layers.RepeatVector(300)(attention)\nattention = layers.Permute([2, 1])(attention)\nsent_representation = merge([A, attention], mode='mul')\nsent_representation = Lambda(lambda xin: K.sum(xin, axis=-2), output_shape=(units,))(sent_representation)\n\noutput = layers.TimeDistributed(layers.Dense(vocab_size, activation='softmax',\n                                 kernel_regularizer = regularizers.l2(1e-4), bias_regularizer = regularizers.l2(1e-4))\n                                , name = 'time_distributed_softmax')(sent_representation)\n\n\n\n\nmodel  = Model(inputs=[img_input, report_input], outputs=output, name='NIC_greedy_inference_v2')\n\n\n##Inference models ################\n\n#encoder_inference model\nencoder_model = Model(img_input,[a,c])\n\n# Decoder model ###################\n\na0 = layers.Input(shape=(300,))\nc0 = layers.Input(shape=(300,))\n\nA, alast, clast = LSTMLayer(X_text, initial_state=[a0, c0])\noutput = layers.TimeDistributed(layers.Dense(vocab_size, activation='softmax',\n                                 kernel_regularizer = regularizers.l2(1e-4), \n                                 bias_regularizer = regularizers.l2(1e-4)), name = 'time_distributed_softmax')(A)\n\n\n\ndecoder_model = Model([report_input,a0,c0],[output,alast,clast])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}