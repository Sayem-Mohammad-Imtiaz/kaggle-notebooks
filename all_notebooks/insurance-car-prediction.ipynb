{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Machine Learning Project by Teodor Chakarov","metadata":{}},{"cell_type":"markdown","source":"Atomobiles are big thing in everyday life of an average human. They help us being fast, flexable and independent. Though, we have a lot of social problems like air polution, traffic jams, car accidentsand etc.\nIn that case every country set as regulation for required insurance, at least one, in order to drive a car. People with expensive vehicles should be obligated to do an insurance on order to drive those fast cars.\n\nHere I'm going to see how many people tend to trust an insurance company and make a prediction models for Classification and Regression problems.\n","metadata":{}},{"cell_type":"markdown","source":"# Part 1 - Vehicle Insurance ","metadata":{}},{"cell_type":"markdown","source":"In this machine learning part, I'm going to inspect and try to build a model for an insurance company. I have a dataset which has people who use this insurance company's products for Health insurance. The dataset attribues are:\n1) Gender\n\n2) Age\n\n3) Driving License (1 - Yes, 0 - No)\n\n4) Redion Code - Unique code for the region of the customer\n\n5) Previously Insured - (0 - Person hasn't got previous vehicle insurance, 1 - Person has got previous vehicle insurance)\n\n6) Vehicle Age\n\n7) Previous Vehicle Damage\n\n8) Annual Premium - The amount customer needs to pay as premium in the year\n\n9) Policy Sales Chanel - Anonymized Code for the channel of outreaching to the customer ie. Different Agents, Over Mail, Over Phone, In Person, etc\n\n10) Vintage - Number of Days, Customer has been associated with the company\n\n11) Response - Does the customer wants to get vehicle insurance (0 - No, 1 - Yes)","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline\nimport seaborn as sns\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n\nfrom sklearn.linear_model import LogisticRegressionCV\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.svm import SVC\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, plot_confusion_matrix, plot_roc_curve\n\nfrom lightgbm import LGBMClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"insurance = pd.read_csv(\"/kaggle/input/health-insurance-cross-sell-prediction/train.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"insurance.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"insurance = insurance.drop('id', axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"insurance.describe().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"insurance.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"insurance.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Exploration\n\n\nI'm going briefly to explore the dataset, try to see correlations and how the data is acting between the columns ","metadata":{}},{"cell_type":"code","source":"print(f\"Number of observations are {insurance.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colors = ['blue', 'red']\nplt.title('Insurance Clients based on Gender',fontsize=15)\ncircle = plt.Circle((0, 0), 0.6, color = 'white')\ninsurance['Gender'].value_counts().plot(kind='pie', figsize=(8, 8), rot=1, colors=colors, autopct = '%.2f%%')\nplt.axis('off')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see almost equal distibution between men and women","metadata":{}},{"cell_type":"code","source":"plt.title('Vehicle age distribution',fontsize=15)\ninsurance['Vehicle_Age'].value_counts().plot(kind='bar', figsize=(8, 8))\nplt.xlabel('Years of a car')\nplt.ylabel('Count')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Customers with previous insurance","metadata":{}},{"cell_type":"code","source":"health = insurance.groupby(['Gender', 'Previously_Insured'], as_index='Gender').count()\nhealth","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colors = ['#1849CA', 'crimson', 'green', 'pink']\ncircle = plt.Circle((0, 0), 0.6, color = 'white')\nhealth.plot(x= 'Gender', y='Age',kind='pie', figsize=(8, 8), rot=1, colors=colors, autopct = '%.2f%%')\nplt.title('Insured by Gender')\nplt.legend()\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clearly men who don't have previous incuranse are more than men who do have but women are nearly equaly distributed.\n\n","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\nplt.title('Vehicle damage by gender',fontsize=15)\ninsurance['Vehicle_Damage'].value_counts().plot(kind='bar', figsize=(8, 8))\nplt.xlabel('Damaged car?')\nplt.ylabel('Count')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here again we have equaly distribution between two categories","metadata":{}},{"cell_type":"code","source":"men = insurance[insurance['Gender'] =='Male']\nfemale = insurance[insurance['Gender'] == 'Female']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(men.shape)\nprint(female.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### What did people choose for insuranse? ","metadata":{}},{"cell_type":"code","source":"people_without_insuranse_accept= insurance[(insurance['Previously_Insured'] == 0) & (insurance['Response'] == 1)]\npeople_with_insuranse =  insurance[(insurance['Previously_Insured'] == 1) & (insurance['Response'] == 1)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(people_without_insuranse_accept.shape)\nprint(people_with_insuranse.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"As we see people who don't have previous insurance and who will pay for one are {people_without_insuranse_accept.shape[0]} and people who will continue paying are {people_with_insuranse.shape[0]}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"people_without_insuranse_reject = insurance[( insurance['Previously_Insured'] == 0) & (insurance['Response'] == 0)]\npeople_with_insuranse_reject =  insurance[(insurance['Previously_Insured'] == 1) & (insurance['Response'] == 0)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(people_with_insuranse_reject.shape)\nprint(people_without_insuranse_reject.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"We can see that people who don't have previous insurance and won't pay for one are {people_without_insuranse_reject.shape[0]} and people who will stop paying are {people_with_insuranse_reject.shape[0]}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\n\nplt.title('Are people into vehicle insurance or not')\n\nsns.countplot(x = 'Previously_Insured', hue='Response', data = insurance)\nplt.ylabel(\"Count\")\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Our dataset until this point is very well balanced but here we can see that people's response about future insurance is really low. \nWe can see that bigger % of people who will pay for one are people who don't have previous insurance.\n\nPeople's response isn't balanced. That is either people are not satisfied with the insurance company's products or they don't need one.\n","metadata":{}},{"cell_type":"markdown","source":"### Distribution","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\nplt.title('Age Distribution')\nm = sns.kdeplot(x = men['Age'], shade = True, legend = True, label = 'Male')\nw = sns.kdeplot(x = female['Age'], shade = True, legend = True, label = 'Female')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We tend to have more people in ther young adult years (20-28) and (40-45) years","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\n\nplt.title('Vehicle Damage and customers response')\nsns.countplot(x = 'Vehicle_Damage', hue='Response', data = insurance)\n\nplt.xlabel(\"Vehicle Damage\")\nplt.ylabel(\"Count\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" We can see that people who got in a car accident will have an insurance.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\n\nplt.title('Vehicle Insured Clients')\nvehicle_damage = insurance[['Gender', 'Response', 'Age']]\nvehicle_damage = vehicle_damage[vehicle_damage['Response'] == 1]\nmen = vehicle_damage[vehicle_damage.Gender == 'Male']\nfemale = vehicle_damage[vehicle_damage.Gender == 'Female']\nm = sns.kdeplot(x = men['Age'], shade = True, legend = 'True', label = 'Male')\nw = sns.kdeplot(x = female['Age'], shade = True, legend = 'True', label = 'Female')\n\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Age definetly afects people response about the insurance. Pople between age of 30-50 tend to look forward an insurance.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\n\nplt.title('Vehicle Insured Clients')\nvehicle_damage = insurance[['Gender', 'Previously_Insured', 'Age']]\nvehicle_damage = vehicle_damage[vehicle_damage['Previously_Insured'] == 1]\nmen = vehicle_damage[vehicle_damage.Gender == 'Male']\nfemale = vehicle_damage[vehicle_damage.Gender == 'Female']\nm = sns.kdeplot(x = men['Age'], shade = True, legend = 'True', label = 'Male')\nw = sns.kdeplot(x = female['Age'], shade = True, legend = 'True', label = 'Female')\n\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"More of the perviously insuranced clients are at young age.","metadata":{}},{"cell_type":"markdown","source":"### In conclusion:\n1) In general our dataset is well balanced except people's response about new insurnace.\n\n2) Age, Car Accidents, Previously damaged cars, previous insuranced client are in relations with their response\n\n3) We have to make sure in the machine learning part to stratify the Response equaly in the training and testing set!","metadata":{}},{"cell_type":"markdown","source":"## Preparing for Machine Learning","metadata":{}},{"cell_type":"markdown","source":"We need to transform our string columns to categorical numbers in order to use them for algorithms","metadata":{}},{"cell_type":"code","source":"insurance.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"insurance.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"insurance_categorical = pd.get_dummies(insurance)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"insurance_categorical.columns = ['Age', 'Driving_License' , 'Region_Code', 'Previously_Insured',\n                                 'Annual_Premium', 'Policy_Sales_Channel', 'Vintage', 'Response',\n                                 'Gender_Female', 'Gender_Male','Vehicle_Age_1-2', 'Vehicle_Age_<1',\n                                 'Vehicle_Age_>2', 'Vehicle_Damage_No', 'Vehicle_Damage_Yes']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"insurance_categorical.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"insurance_categorical.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Selection\n\nWe are going to see the correlations and try to exclude some of the unimportant features in this dataset ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.heatmap(insurance_categorical.corr(), annot = True, fmt = '.1g')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So here we can clearly see the correlations in the datast.\n\nVintage, Annual Premium are in not strong correlation with any of the other features so we can exclude them in this feature selection.\n\nRegion code don't have strong relations as well but for now i think it is part of the bigger picture and i'm going to use it.\n\nI don't see features (except dummies features) over 0.9 as well to exclude them because of high variance.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"insurance_categorical = insurance_categorical[['Age', 'Driving_License', 'Region_Code', 'Previously_Insured',\n                   'Policy_Sales_Channel', 'Gender_Female', \"Gender_Male\",\n                   'Vehicle_Age_1-2', 'Vehicle_Age_<1', 'Vehicle_Age_>2', 'Vehicle_Damage_No',\n                          'Vehicle_Damage_Yes', 'Response']]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"insurance_categorical.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"insurance_categorical.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Normalization\n\nI am going to scale the data using standart MinMax scaler","metadata":{}},{"cell_type":"code","source":"scaler = MinMaxScaler()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"insurance_categorical_scaled = pd.DataFrame(scaler.fit_transform(insurance_categorical), index=insurance_categorical.index,\n                                            columns=insurance_categorical.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"insurance_categorical_scaled.describe().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = PCA()\npca_data = pca.fit_transform(insurance_categorical_scaled)\nnp.cumsum(pca.explained_variance_ratio_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see a how 'heavy' each column is fot the future machine learning. I will try without droping some of the columns first and then I will drop some of them.\nI want to see how we can interact and what results are we going to have for each hypothesis.","metadata":{}},{"cell_type":"markdown","source":"### Split data\n\nSince we have 381109 observations i'm going to split with training/testing sets like 70/30 and will use a Cross-validation with 8 splits of the training set.\n\nI'm going to stratify 'Response' because as we saw, people who respond with 'No' are more than 'Yes' and we want to split them equaly.","metadata":{}},{"cell_type":"code","source":"insurance_categorical_scaled_attribues = insurance_categorical_scaled.drop('Response', axis = 1)\ninsurance_categorical_scaled_target = insurance_categorical_scaled['Response']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(insurance_categorical_scaled_attribues.shape)\nprint(insurance_categorical_scaled_target.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"insurance_attrributes_train, insurance_attrributes_test,insurance_target_train, insurance_target_test = train_test_split(insurance_categorical_scaled_attribues,\n                                                                                                                         insurance_categorical_scaled_target,\n                                                                                                                         train_size = 0.7, stratify = insurance_categorical_scaled_target,\n                                                                                                                        random_state = 42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(insurance_attrributes_train.shape)\nprint(insurance_attrributes_test.shape)\nprint(insurance_target_train.shape)\nprint(insurance_target_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k_fold = StratifiedKFold(n_splits = 5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Machine Larning algorithms","metadata":{}},{"cell_type":"markdown","source":"The problem can be classified as Binary Classification\n\nIm going to try to see the best algorithm using gridSearch (for hyerparameter tuning), base linear classification algorithms and Ensemble methods as well.\n","metadata":{}},{"cell_type":"markdown","source":"The functions that i will use are:\n\n    1) GetModelScores - here i fit and train the model and give scores of both training and testing sets\n    \n    2) GetOnlyScores - here i will get only scores of the given model ","metadata":{}},{"cell_type":"code","source":"def GetModelScores (estimator, X_train, X_test, y_train, y_test):\n    scores_train = pd.DataFrame(columns= ['Accuracy','F1 Score','Precision','Recall','ROC_AUC'])\n    scores_test = pd.DataFrame(columns= ['Accuracy','F1 Score','Precision','Recall','ROC_AUC'])\n    \n    model = estimator\n    model.fit(X_train, y_train)\n    \n    prediction_train = model.predict(X_train)\n    prediction_test = model.predict(X_test)\n    \n    try:\n        score_train = model.predict_proba(X_train)[:,1]\n        roc_train= roc_auc_score(y_train, score_train, average = \"weighted\")\n    except:\n        roc_train = 0\n        \n    try:\n        score_test = model.predict_proba(X_test)[:,1]\n        roc_test= roc_auc_score(y_test, score_test, average = \"weighted\")\n    except:\n        roc_test = 0\n    \n    \n    \n    scores_train['Accuracy'] = accuracy_score(y_train, prediction_train)*100,\n    scores_train['F1 Score'] = f1_score(y_train, prediction_train, average = \"weighted\")*100,\n    scores_train['Precision'] = precision_score(y_train, prediction_train, average = \"weighted\")*100,\n    scores_train['Recall'] = recall_score(y_train, prediction_train, average = \"weighted\")*100,\n    scores_train['ROC_AUC'] = roc_train*100\n    \n       \n    scores_test['Accuracy'] = accuracy_score(y_test, prediction_test)*100,\n    scores_test['F1 Score'] = f1_score(y_test, prediction_test, average = \"weighted\")*100,\n    scores_test['Precision'] = precision_score(y_test, prediction_test, average = \"weighted\")*100,\n    scores_test['Recall'] = recall_score(y_test, prediction_test, average = \"weighted\")*100,\n    scores_test['ROC_AUC'] = roc_test*100\n    \n    print(scores_train)\n    print(scores_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def GetOnlyScores (estimator, y_test, X_test, y_train, X_train):\n    scores_train = pd.DataFrame(columns= ['Accuracy','F1 Score','Precision','Recall','ROC_AUC'])\n    scores_test = pd.DataFrame(columns= ['Accuracy','F1 Score','Precision','Recall','ROC_AUC'])\n    \n    prediction_train = estimator.predict(X_train)\n    prediction_test = estimator.predict(X_test)\n    \n    try:\n        score_train = estimator.predict_proba(X_train)[:,1]\n        roc_train= roc_auc_score(y_train, score_train, average = \"weighted\")\n    except:\n        roc_train = 0\n        \n    try:\n        score_test = estimator.predict_proba(X_test)[:,1]\n        roc_test= roc_auc_score(y_test, score_test, average = \"weighted\")\n    except:\n        roc_test = 0\n    \n   \n    scores_train['Accuracy'] = accuracy_score(y_train, prediction_train)*100,\n    scores_train['F1 Score'] = f1_score(y_train, prediction_train, average = \"weighted\")*100,\n    scores_train['Precision'] = precision_score(y_train, prediction_train, average = \"weighted\")*100,\n    scores_train['Recall'] = recall_score(y_train, prediction_train, average = \"weighted\")*100,\n    scores_train['ROC_AUC'] = roc_train*100\n       \n    scores_test['Accuracy'] = accuracy_score(y_test, prediction_test)*100,\n    scores_test['F1 Score'] = f1_score(y_test, prediction_test, average = \"weighted\")*100,\n    scores_test['Precision'] = precision_score(y_test, prediction_test, average = \"weighted\")*100,\n    scores_test['Recall'] = recall_score(y_test, prediction_test, average = \"weighted\")*100,\n    scores_test['ROC_AUC'] = roc_test*100\n\n    \n    print(scores_train)\n    print(scores_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### H0: Using all the attributes ","metadata":{}},{"cell_type":"markdown","source":"I'm going to see only basic algorithms withount hyperparameters and see the scores of them.\n\n#### 1) Logistic Regression","metadata":{}},{"cell_type":"code","source":"GetModelScores(LogisticRegressionCV(), insurance_attrributes_train, insurance_attrributes_test, insurance_target_train, insurance_target_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2)Decision Tree ","metadata":{}},{"cell_type":"code","source":"GetModelScores(DecisionTreeClassifier(),insurance_attrributes_train, insurance_attrributes_test, insurance_target_train, insurance_target_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3) Random Forest","metadata":{}},{"cell_type":"code","source":"GetModelScores(RandomForestClassifier(), insurance_attrributes_train, insurance_attrributes_test, insurance_target_train, insurance_target_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4) K-Neighbors","metadata":{}},{"cell_type":"code","source":"#GetModelScores(KNeighborsClassifier(), insurance_attrributes_train, insurance_attrributes_test, insurance_target_train, insurance_target_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### I made a SVC but it is really slow because of the big data: Accuracy: 0.877, F1 Score: 0.820, Precision: 0.771, Recall: 0.877","metadata":{}},{"cell_type":"markdown","source":"Based of the ROC Score i will perform Hyperparameter Tuning on Logistic Regression and Random Forest with Grid Search","metadata":{}},{"cell_type":"markdown","source":"#### 1) Logistic Regression","metadata":{}},{"cell_type":"code","source":"parameters = {\n    'Cs': [0.001, 0.01, 1, 10, 100],\n    \"max_iter\": [30, 50, 70]\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_logistic = GridSearchCV(LogisticRegressionCV(), param_grid = parameters, scoring = 'roc_auc', cv = k_fold, n_jobs =-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_logistic.fit(insurance_attrributes_train, insurance_target_train)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_logistic.best_estimator_","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_logistic.cv_results_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GetOnlyScores(grid_logistic, insurance_target_test, insurance_attrributes_test,\n              insurance_target_train, insurance_attrributes_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see mean_test_score is the same like ROC_AUC form testing set which is good mark its not overfitting for sure","metadata":{}},{"cell_type":"markdown","source":"#### 2) Random Forest","metadata":{}},{"cell_type":"code","source":"parameters = {\n    \"n_estimators\": [50, 200, 400],\n    \"max_depth\": [10, 50, 70]\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_forest = GridSearchCV(RandomForestClassifier(), parameters, scoring = 'roc_auc', cv = k_fold, n_jobs =-1)\ngrid_forest.fit(insurance_attrributes_train, insurance_target_train)\n\ngrid_forest.best_estimator_\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_forest.cv_results_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GetOnlyScores(grid_forest, insurance_target_test, insurance_attrributes_test, \n              insurance_target_train, insurance_attrributes_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'We see that the ROC score is better than logistic regression also the F1_score so for now we can stop with Random Forest with {grid_forest.best_estimator_}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(grid_forest, insurance_attrributes_test, insurance_target_test, normalize='pred')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Because of the high unbalanced target variable we have less than 50% True Positives","metadata":{}},{"cell_type":"markdown","source":"### H1: Machine Learning without high correlation attributes\n\nHere i will see the results if i drop some of the columns with high corr. That way I can perform manuall demensionality reduction to get better results","metadata":{}},{"cell_type":"code","source":"insurance_lower_dim = insurance_categorical.drop(['Gender_Female', 'Vehicle_Age_<1', 'Vehicle_Damage_No'], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"insurance_lower_dim.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"insurance_dim_target = insurance_lower_dim.Response\ninsurance_dim_attributes = insurance_lower_dim.drop('Response', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = MinMaxScaler()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"insurance_lower_dim_scaled = scaler.fit_transform(insurance_dim_attributes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"insurance_lower_dim_scaled.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dim_scaled_attributes_train, dim_scaled_attributes_test,dim_target_train, dim_target_test = train_test_split(insurance_lower_dim_scaled, insurance_dim_target,\n                                                                                                            test_size = 0.3,stratify = insurance_dim_target,\n                                                                                                            random_state = 42)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(dim_scaled_attributes_train.shape)\nprint(dim_scaled_attributes_test.shape)\nprint(dim_target_train.shape)\nprint(dim_target_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Random Forest Classifier for dimension reduction dataset","metadata":{}},{"cell_type":"code","source":"GetModelScores(RandomForestClassifier(), dim_scaled_attributes_train, dim_scaled_attributes_test,dim_target_train, dim_target_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Logistic Regression for dimension reduction dataset","metadata":{}},{"cell_type":"code","source":"GetModelScores(LogisticRegressionCV(cv = k_fold), dim_scaled_attributes_train, dim_scaled_attributes_test,dim_target_train, dim_target_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion between H0 and H1\n\nI am going to use the smaller dataset since the performance is the same as the 13 attributes one. \nThat way i can prevent future overfitting and will learn fastter.","metadata":{}},{"cell_type":"markdown","source":"### BOOSTING \n\nI want to see if i use boosting ML algorithms, can i get better results for the classes ","metadata":{}},{"cell_type":"markdown","source":"#### AdaBoostClassifier","metadata":{}},{"cell_type":"code","source":"GetModelScores(AdaBoostClassifier(), dim_scaled_attributes_train, dim_scaled_attributes_test,dim_target_train, dim_target_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### GradientBoostingClassifier","metadata":{}},{"cell_type":"code","source":"GetModelScores(GradientBoostingClassifier(), dim_scaled_attributes_train, dim_scaled_attributes_test,dim_target_train, dim_target_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### LGBMClassifier","metadata":{}},{"cell_type":"code","source":"GetModelScores(LGBMClassifier(), dim_scaled_attributes_train, dim_scaled_attributes_test,dim_target_train, dim_target_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We can see that LGBMClassifier is the best boosting algorithm and it is really fast with big data. I'm going to perform hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"parameters_grid = {\n             'num_leaves': [5, 10, 50], \n             'n_estimators': [200, 400, 600],\n             'reg_lambda': [5, 50, 100]\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_booster = GridSearchCV(LGBMClassifier(), parameters_grid, scoring = 'roc_auc',\n                            n_jobs = -1, cv = k_fold)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_booster.fit(dim_scaled_attributes_train, dim_target_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Here we can see with hyperparameters: {grid_booster.best_estimator_} we have best scores for:')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GetOnlyScores(grid_booster, dim_target_test, dim_scaled_attributes_test,dim_target_train, dim_scaled_attributes_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(grid_booster, dim_scaled_attributes_test, dim_target_test, normalize='pred')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that Negative response we have 88 % but Positives are 47%. That's because we have unbalanced Response.\nTo get better scores in this matrix I'm going to perform Oversampling","metadata":{}},{"cell_type":"markdown","source":"### H2: UNDER- AND OVER-SAMPLING","metadata":{}},{"cell_type":"markdown","source":"I'm going to use combined method SMOTETomek to see how good the model is going to perform.","metadata":{}},{"cell_type":"code","source":"from imblearn.combine import SMOTETomek","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"insurance_lower_dim.Response.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see original distribution of the target labels ","metadata":{}},{"cell_type":"code","source":"oversample_attributes = insurance_lower_dim.drop('Response', axis = 1)\noversample_target = insurance_lower_dim.Response","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"balance_data = SMOTETomek()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oversample_attributes_res, oversample_target_res = balance_data.fit_resample(oversample_attributes, oversample_target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oversample_target_res.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the positive response are equal to the negative one because i performed oversampling and got liitle bit undersampling of attributes","metadata":{}},{"cell_type":"code","source":"sc = MinMaxScaler()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oversample_target = sc.fit_transform(oversample_attributes_res)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oversample_attributes_train, oversample_attributes_test, oversample_target_train, oversample_target_test = train_test_split(oversample_attributes_res,\n                                                                                             oversample_target_res, test_size = 0.3, \n                                                                                            random_state = 42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### GridSearch with LGBMClassifier","metadata":{}},{"cell_type":"code","source":"mod_params = {\n              'n_estimators':[400, 600, 800],\n              'num_leaves': [10, 50, 80],\n              'reg_lambda': [0.001, 1, 5, 10]\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mod = GridSearchCV(LGBMClassifier(), mod_params, scoring = 'roc_auc', cv = k_fold)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mod.fit(oversample_attributes_train, oversample_target_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mod.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GetOnlyScores(mod, oversample_target_test, oversample_attributes_test, oversample_target_train, oversample_attributes_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(mod, oversample_attributes_test, oversample_target_test, normalize='pred')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc_curve(mod, oversample_attributes_test, oversample_target_test, name = \"Certainty of the algorithm\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see how certain the model is in predicting the output classes. By looking at the graph we can see our cureve is going up more than going to False Positive (right) side","metadata":{}},{"cell_type":"markdown","source":"### CONCLUSION\n\nWe can see we have less scores in comparison to parameter-tuned LGBMClassifier but the roc_auc score is better, also the confusion matrix gives us better results. For now i'm satisfied with our last model in which we performed:\n\n1) MinMax regularization\n\n2) Feature Selection in which we exclude from the original 15 to 10 columns \n\n3) Perform an over- and under-sampling in which we deal with unbalanced dataset\n\n4) We chose the best ML algorithm and it's boosting algorithm LGBMClassifier. It is fast with big data and we got the best scores with it.\n\n5) We perform Grid Search with Cross Validation in which we got best estimators.\n\n6) And at last we combined all of these steps to get 91% True Negatives and 76 % True Positives","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}