{"cells":[{"metadata":{"_uuid":"e8aa0f91961d335b6f1d5d62c705f08fc1ce42fc"},"cell_type":"markdown","source":"# Linear Regression to predict the average score \n\nMy first kernel, just trying to learn by practicing some some easy models, in this case Linear Regression\n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Import stuff\n\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import accuracy_score, mean_squared_error\n%matplotlib inline\n# Input data files are available in the \"../input/\" directory.\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f2fa1932b01dc45c91bca1e411d5b5cac5e632b"},"cell_type":"markdown","source":"Let's first check the shape of our data and how it is distributed","execution_count":null},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/StudentsPerformance.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"23d64185a32817c8af4ea2f66747318c7646bcb6"},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d719076cd28523eaf1339f4cf3ce45ade7c9057"},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f9a04f9c21fbde3c502672273d0c9052bfb1c74"},"cell_type":"markdown","source":"We can see that only the scores are actually numerical values and the rest are categorical. Since our model only accepts numerical values we will have to deal with this problem.\nFor now let's create a new feature that is the average of all the scores and a grouping of those scores  and see how they are distributed.","execution_count":null},{"metadata":{"trusted":true,"_uuid":"e4f2b0294ec163f9f32d642280ac6f9c7aef69e3"},"cell_type":"code","source":"df[\"average score\"] = (df[\"math score\"] + df[\"reading score\"] + df[\"writing score\"]) /3\ndf['average score group'] = pd.cut(df[\"average score\"], bins=[g for g in range(0, 101, 10)], include_lowest=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5dd7ff862989faeeec97fb7a60230c92a99e4a24"},"cell_type":"code","source":"df.hist(bins=20, figsize=(12,8))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5f6b856250c7766b05d0a5df29f7061712f4606"},"cell_type":"markdown","source":"Let's check our other attributes and their distribution","execution_count":null},{"metadata":{"trusted":true,"_uuid":"07b62cb7230598227dc85421aedd3b32651c3cf7"},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\np = sns.countplot(x='parental level of education', data = df, palette='deep')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea71a9d2d819ce314c65a27fc307270b027886f0"},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\n\n\np = sns.countplot(x='parental level of education', data = df, hue='average score group', palette=\"deep\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6d2471e8886f3cdc8626e317b7a48c630f8aa59"},"cell_type":"markdown","source":"We can see that we don't have that many data about children whose parents have a master's or bachelor's degree compared to the other groups.\nWe can also identify that there doesn't seem to exist a clear correlation between the different grades of the students and their parent's education.","execution_count":null},{"metadata":{"trusted":true,"_uuid":"b86223d3e7ae9451da4c3e606baa971dc5abde1c"},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\np = sns.countplot(x='race/ethnicity', data = df, palette='deep')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb912e9889ba6fede4d1248a496abf2383a3eeec"},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\n\n\np = sns.countplot(x='race/ethnicity', data = df, hue='average score group', palette=\"deep\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9ccebcda3e66fcd6ffd88eb145ccc2da3707c4b"},"cell_type":"markdown","source":"Again we cannot see a clear distinction of the grades based on the race group that the student belongs to.","execution_count":null},{"metadata":{"trusted":true,"_uuid":"8295c0f6022cd30074d7bdfde85e1e460c28c5a3"},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\np = sns.countplot(x='lunch', data = df, palette='deep')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bdbf24d603b3d98e7257b4b8f4be8547703d716b"},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\n\n\np = sns.countplot(x='lunch', data = df, hue='average score group', palette=\"deep\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c60d073ac86376c0125f480fb9bfcffd3c30601c"},"cell_type":"markdown","source":"With the lunch feature we can start to see a small distinction where the students that have a free/reduced lunch tend to have sligthly smaller grades. Let's analyse this assumption further","execution_count":null},{"metadata":{"trusted":true,"_uuid":"e76489bd2774658b73384949e0d22d4c5e22fb91"},"cell_type":"code","source":"fr_lunch = df[df['lunch']=='free/reduced']\nstd_lunch = df[df['lunch']=='standard']\n\nprint(\"Free/Reduced lunch mean\",fr_lunch['average score'].mean())\nprint(\"Standard lunch mean\",std_lunch['average score'].mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69514d1011ea3899fd1549c460d897a8a1778456"},"cell_type":"markdown","source":"We can indeed see a small difference between the means of both groups, which our algorithm will use to improve it's predictions.\n\nLike we said before,* scikit learn*'s ** Linear Regression** will not accept non numeric features so we will now change these into a **One Hot Encoding** version that will be better suited.","execution_count":null},{"metadata":{"trusted":true,"_uuid":"1746532465e88c926fbab40860dd8f4d5191e555"},"cell_type":"code","source":"new_df = df.copy()\n\none_hot = pd.get_dummies(df['gender'], prefix='gender')\nnew_df = new_df.join(one_hot)\none_hot = pd.get_dummies(df['race/ethnicity'], prefix='race/ethnicity')\nnew_df = new_df.join(one_hot)\none_hot = pd.get_dummies(df['parental level of education'], prefix='parental level of education')\nnew_df = new_df.join(one_hot)\none_hot = pd.get_dummies(df['lunch'], prefix='lunch')\nnew_df = new_df.join(one_hot)\none_hot = pd.get_dummies(df['test preparation course'], prefix='test preparation course')\nnew_df = new_df.join(one_hot)\n\nnew_df.drop([\"reading score\", \"writing score\", \"math score\", \"gender\", \"race/ethnicity\", \"parental level of education\", \"test preparation course\",\"lunch\", \"average score group\"], axis=1, inplace=True)\n\nnew_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dccabcd0295231fa5efd09545d5e13a73ab14591"},"cell_type":"markdown","source":"Now we separate the data into two groups, the train data which we will use to train our model and the test data where we will test the performance of our model.\nWe will also split both the groups into the actual features and the labels we are trying to predict, in this case the average score.","execution_count":null},{"metadata":{"trusted":true,"_uuid":"2bbadbe6997e87c62125f98e7c0dcd4097057c40"},"cell_type":"code","source":"train_set, test_set = train_test_split(new_df, test_size=0.20, random_state=21)\n\ntrain_X = train_set.drop('average score', axis=1)\ntrain_Y = train_set['average score'].copy()\n\ntest_X = test_set.drop('average score', axis=1)\ntest_Y = test_set['average score'].copy()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"923bcecadf9f8b6bf96cc82d8552d0bfdc4b46ac"},"cell_type":"markdown","source":"Finnaly we can start training the model! \nWe will be using the Linear Regression algorithm inside a Cross Validation function with 10 folds.","execution_count":null},{"metadata":{"trusted":true,"_uuid":"ebeda7880a8b8fbb1321f5b6bbe61f3f9845197b"},"cell_type":"code","source":"\nlin_reg = LinearRegression()\n\nresults = cross_validate(lin_reg, train_X, train_Y, cv=10, return_estimator=True)\n\nscores = results['test_score']\nprint(\"Scores:\",scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard deviation:\", scores.std())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"677aec7785283889bd50e7ffe14a94790c6d6c7a"},"cell_type":"markdown","source":"By analising the results we can see that our model has a mean error of about 0.2 which isn't very good. \nWe can also see that there is a large variance between the models that we attained with the Cross Validation.\n\nOf course all of this is based on the test group data. Let's see how our best model works when paired aggainst our test set.","execution_count":null},{"metadata":{"trusted":true,"_uuid":"066038fe751d5875a7cddeff71aace16ffaf7ddc"},"cell_type":"code","source":"# Find the best model\n\nbest = np.where(scores == min(scores))[0][0]\nbest_estimator = results['estimator'][best]\nfinal_predictions = best_estimator.predict(test_X)\nfinal_mse = mean_squared_error(test_Y, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\nprint(final_rmse)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4fe5d0694918694ef15a0ab1d64324002a486443"},"cell_type":"markdown","source":"So as we can see our estimator gives us a final error of around 12.6","execution_count":null},{"metadata":{"_uuid":"6486a7645583e8fa0fb35e4e878097ae50b96079"},"cell_type":"markdown","source":"","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}