{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n# **Loading and Understanding the Data**"},{"metadata":{},"cell_type":"markdown","source":"specifying path for the file"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"PATH = '/kaggle/input/sms-spam-collection-dataset/spam.csv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spam_data = pd.read_csv(f'{PATH}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We got an **UnicodeDecodeError**"},{"metadata":{},"cell_type":"markdown","source":"### For resolving this error we have to know value encoding "},{"metadata":{"trusted":true},"cell_type":"code","source":"import chardet\nwith open(f'{PATH}', 'rb') as rawdata:\n    result = chardet.detect(rawdata.read(100000))\nprint(result)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now wite the value of encoding inside the `read_csv()` "},{"metadata":{"trusted":true},"cell_type":"code","source":"spam_data = pd.read_csv(f'{PATH}', encoding='Windows-1252')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spam_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spam_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now we have to check the null values count in each columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"spam_data.isnull()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spam_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Remove the columns**"},{"metadata":{},"cell_type":"markdown","source":"Since there are maximum number of values are null in three column. so, we drop all the three columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"spam_data.drop(spam_data[[\"Unnamed: 2\"\t, \"Unnamed: 3\"\t,\"Unnamed: 4\"]], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Renaming the Column names"},{"metadata":{"trusted":true},"cell_type":"code","source":"spam_data.rename(columns={\"v1\" : 'label', \"v2\" : 'message'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now again we check the null values after deleting the column"},{"metadata":{"trusted":true},"cell_type":"code","source":"spam_data.isnull().sum().sort_index()/len(spam_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spam_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Till now we have fininshed our initial processing"},{"metadata":{},"cell_type":"markdown","source":"## **One Hot Encode the target variable**"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels  = pd.get_dummies(spam_data['label'], drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have two DataFrame one labels with ecoded value and our initial DataFrame.So we have to combine both to make a single DataFrame"},{"metadata":{"trusted":true},"cell_type":"code","source":"spam_data = pd.concat([spam_data, labels], axis=1)\nspam_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As, we can see that there are two columns of target value one in encoded value and one in string format, So we drop the string column of target value."},{"metadata":{"trusted":true},"cell_type":"code","source":"spam_data.drop(\"label\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spam_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we convert the message column into list "},{"metadata":{"trusted":true},"cell_type":"code","source":"messages = spam_data['message'].to_numpy().tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"messages[0:3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Text Preprocessing**"},{"metadata":{},"cell_type":"markdown","source":"Text preprocessing is an approach for cleaning and preparing text data for use in a specific context. Developers use it in almost all natural language processing (NLP) pipelines, including voice recognition software, search engine lookup, and machine learning model training. It is an essential step because text data can vary. From its format (website, text message, voice recognition) to the people who create the text (language, dialect), there are plenty of things that can introduce noise into your data."},{"metadata":{},"cell_type":"markdown","source":" We will use few common approaches for cleaning and processing text data. They include:\n\n   * Using Regex & NLTK libraries\n   * Removing unnecessary characters and formatting\n   * Tokenization – break multi-word strings into smaller components\n   * Normalization – a catch-all term for processing data; this includes stemming and lemmatization\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Noise Removal"},{"metadata":{},"cell_type":"markdown","source":"Text cleaning is a technique that developers use in a variety of domains. Depending on the goal of your project and where you get your data from, you may want to remove unwanted information, such as:\n\n   * Punctuation and accents\n   * Special characters\n   * Numeric digits\n   * Leading, ending, and vertical whitespace\n   * HTML formatting\n"},{"metadata":{},"cell_type":"markdown","source":"### Tokenization"},{"metadata":{},"cell_type":"markdown","source":"A few common operations that require tokenization include:\n\n   * Finding how many words or sentences appear in text\n   * Determining how many times a specific word or phrase exists\n   * Accounting for which terms are likely to co-occur\n"},{"metadata":{},"cell_type":"markdown","source":"### Normalization\n\nTokenization and noise removal are staples of almost all text pre-processing pipelines. However, some data may require further processing through text normalization. Text normalization is a catch-all term for various text pre-processing tasks.A few of them:\n\n  *  Upper or lowercasing\n  *  Stopword removal\n  *  Stemming – bluntly removing prefixes and suffixes from a word"},{"metadata":{},"cell_type":"markdown","source":"### Stopword Removal\n\nStopwords are words that we remove during preprocessing when we don’t care about sentence structure. They are usually the most common words in a language and don’t provide any information about the tone of a statement. They include words such as “a”, “an”, and “the”."},{"metadata":{},"cell_type":"markdown","source":"### Stemming\n\nIn natural language processing, stemming is the text preprocessing normalization task concerned with bluntly removing word affixes (prefixes and suffixes). For example, stemming would cast the word “going” to “go”. This is a common method used by search engines to improve matching between user input and website hits."},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = set(stopwords.words('english')) # for Stopword Removal\nstemmer = PorterStemmer()  # for stemming","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing Emial id's with the single string using regular expression\n\nmessages = [ re.sub(r\"[a-zA-Z0-9.!#$%&'*+/=?^_`{|}~-]+@[a-zA-Z]+[.]+[a-zA-Z]+[.]?[a-zA-Z]*\", 'EMAILID', word) for word in messages ]\n\n# Replacing web address with the single string using regular expression\n\nmessages = [ re.sub(r\"https?:\\/\\/w{0,3}\\w*?\\.(\\w*?\\.)?\\w{2,3}\\S*|www\\.(\\w*?\\.)?\\w*?\\.\\w{2,3}\\S*|(\\w*?\\.)?\\w*?\\.\\w{2,3}[\\/\\?]\\S*\", 'WEBADDRESS', word) for word in messages ]\n\n# Replacing Phone number with the single string using regular expression\n\nmessages = [ re.sub(r\"\\d{10}\\d{0,9}\", 'PHONENUMBER', word) for word in messages ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tokenizing the each sentence\n\ntokenized_by_word = [ word_tokenize(message) for message in messages] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# punctuations which are needed to be removed \n\npunctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def removePunctuations(word):\n    text = \"\"\n    for i in word:\n        if i not in punctuations: # Noise Removal\n            text += i.lower()     # Lowercasing the words\n    \n    return stemmer.stem(text)     # stemming the word","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def wordFilter(sentence):\n    message = []\n    for word in sentence:\n        cleaned_word =  removePunctuations(word)\n        \n        # checking the string wheather it is Stopword or not and also checking the spaces\n        \n        if cleaned_word.isspace() or cleaned_word == \"\" or cleaned_word in stop_words: \n            continue\n        else:\n            message.append(removePunctuations(word))\n    return message","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_message =  [ wordFilter(message) for message in tokenized_by_word]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_message[0:1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def flatten(my_list):\n  result = []\n  for el in my_list:\n    if isinstance(el, list):\n      flat_list = flatten(el)\n      result += flat_list\n    else:\n      result.append(el)\n  return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words_token = flatten(processed_message)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words_token[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# When building BoW vectors, we generally create a features dictionary\n\ndef create_features_dictionary(document_tokens):\n  features_dictionary = {}\n  index = 0\n  for token in document_tokens:\n    if token not in features_dictionary:\n      features_dictionary[token] = index\n      index += 1\n  return features_dictionary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Turning text into a BoW vector is known as feature extraction or vectorization. \n\ndef tokens_to_bow_vector(document_tokens, features_dictionary):\n  bow_vector = [0] * len(features_dictionary)\n  for token in document_tokens:\n    if token in features_dictionary:\n      feature_index = features_dictionary[token]\n      bow_vector[feature_index] += 1\n  return bow_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"message_dictionary = create_features_dictionary(words_token)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"message_vector = [tokens_to_bow_vector(message, message_dictionary) for message in processed_message]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"messages_label = spam_data['spam'].to_numpy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spam_classifier = MultinomialNB()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"dividing data into training and test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(message_vector, messages_label, test_size=0.20, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spam_classifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = spam_classifier.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy of test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}