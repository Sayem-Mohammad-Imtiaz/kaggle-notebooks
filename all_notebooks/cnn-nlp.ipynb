{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\nfilepath_dict = {'yelp':   '/kaggle/input/sentiment-labelled-sentences-data-set/sentiment labelled sentences/yelp_labelled.txt',\n                 'amazon': '/kaggle/input/sentiment-labelled-sentences-data-set/sentiment labelled sentences/amazon_cells_labelled.txt',\n                 'imdb':   '/kaggle/input/sentiment-labelled-sentences-data-set/sentiment labelled sentences/imdb_labelled.txt'}\n\ndf_list = []\nfor source, filepath in filepath_dict.items():\n    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n    df['source'] = source  # Add another column filled with the source name\n    df_list.append(df)\n\ndf = pd.concat(df_list)\nprint(df.iloc[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndf_yelp = df[df['source'] == 'yelp']\nsentences = df_yelp['sentence'].values\ny = df_yelp['label'].values\nsentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, random_state=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer()\nvectorizer.fit(sentences_train)\n\nX_train = vectorizer.transform(sentences_train)\nX_test  = vectorizer.transform(sentences_test)\nX_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nclassifier = LogisticRegression()\nclassifier.fit(X_train, y_train)\nscore = classifier.score(X_test, y_test)\n\nprint(\"Accuracy:\", score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for source in df['source'].unique():\n    df_source = df[df['source'] == source]\n    sentences = df_source['sentence'].values\n    y = df_source['label'].values\n\n    sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, random_state=1000)\n\n    vectorizer = CountVectorizer()\n    vectorizer.fit(sentences_train)\n    X_train = vectorizer.transform(sentences_train)\n    X_test  = vectorizer.transform(sentences_test)\n\n    classifier = LogisticRegression()\n    classifier.fit(X_train, y_train)\n    score = classifier.score(X_test, y_test)\n    print('Accuracy for {} data: {:.4f}'.format(source, score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras import layers\n\ninput_dim = X_train.shape[1]  # Number of features\n\nmodel = Sequential()\nmodel.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train,epochs=25,verbose=False,validation_data=(X_test, y_test),batch_size=40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\ndef plot_history(history):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training accuracy')\n    plt.plot(x, val_acc, 'r', label='Validation accuracy')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_history(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now try word embedding"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(sentences_train)\n\nX_train = tokenizer.texts_to_sequences(sentences_train)\nX_test = tokenizer.texts_to_sequences(sentences_test)\n\nvocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n\nprint(sentences_train[2])\nprint(X_train[2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"a : 2, his : 4, this : 6 More common the word, lower the number. The : 1. 0 is reserved for padding. Not assigned to any text.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\n\nmaxlen = 100\n\nX_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\nX_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n\nprint(X_train[0, :])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Made all vectors of equal length by appending zeros denoting no word matching with vocab, which was made using all the words. So no words at all!"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import layers\n\nembedding_dim = 50\n\nmodel = Sequential()\nmodel.add(layers.Embedding(input_dim=vocab_size, \n                           output_dim=embedding_dim, \n                           input_length=maxlen))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(10, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train,\n                    epochs=15,\n                    verbose=False,\n                    validation_data=(X_test, y_test),\n                    batch_size=14)\nloss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))\nplot_history(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras import layers\n\nembedding_dim = 50\n\nmodel = Sequential()\nmodel.add(layers.Embedding(input_dim=vocab_size, \n                           output_dim=embedding_dim, \n                           input_length=maxlen))\nmodel.add(layers.GlobalMaxPool1D())\nmodel.add(layers.Dense(10, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train,\n                    epochs=20,\n                    verbose=False,\n                    validation_data=(X_test, y_test),\n                    batch_size=25)\nloss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))\nplot_history(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Using Pretained embedded dataset. Skip making vocab and embedding."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\ndef create_embedding_matrix(filepath, word_index, embedding_dim):\n    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n\n    with open(filepath) as f:\n        for line in f:\n            word, *vector = line.split()\n            if word in word_index:\n                idx = word_index[word] \n                embedding_matrix[idx] = np.array(\n                    vector, dtype=np.float32)[:embedding_dim]\n\n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dim = 50\nembedding_matrix = create_embedding_matrix('/kaggle/input/nlpword2vecembeddingspretrained/glove.6B.50d.txt',tokenizer.word_index, embedding_dim)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\nnonzero_elements / vocab_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras import layers\n\nembedding_dim = 50\n\nmodel = Sequential()\nmodel.add(layers.Embedding(vocab_size, embedding_dim, \n                           weights=[embedding_matrix], \n                           input_length=maxlen, \n                           trainable=False))\nmodel.add(layers.GlobalMaxPool1D())\nmodel.add(layers.Dense(10, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train,\n                    epochs=50,\n                    verbose=False,\n                    validation_data=(X_test, y_test),\n                    batch_size=10)\nloss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))\nplot_history(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(layers.Embedding(vocab_size, embedding_dim, \n                           weights=[embedding_matrix], \n                           input_length=maxlen, \n                           trainable=True))\nmodel.add(layers.GlobalMaxPool1D())\nmodel.add(layers.Dense(10, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train,\n                    epochs=25,\n                    verbose=False,\n                    validation_data=(X_test, y_test),\n                    batch_size=20)\nloss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))\nplot_history(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}