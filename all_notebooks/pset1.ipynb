{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nimport tensorflow as tf\n#import preprocessing as pre\nfrom functools import partial\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"# This section need not be edited (It is intended to setup the working environment for tensorflow)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def config_gpu():\n    #Configure Gpus\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    if gpus:\n        try:\n        # Currently, memory growth needs to be the same across GPUs\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, True)\n            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n        except RuntimeError as e:\n            # Memory growth must be set before GPUs have been initialized\n            print(e)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"config_gpu()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checkpoint 1: Define Training/Test data size and dimension and explain the rationale for your selection------------------------","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training/Testing size:\n\nMinimum height is 496, minimum length is 384\n\nImage size needs to be smaller than 384"},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shapes = []\nfor folder in os.listdir('../input/mydata/Data'):\n    path = '../input/mydata/Data/' + folder\n    for file in os.listdir(path):\n        l,b,_=cv2.imread(path +'/'+ file).shape\n        shapes.append([l,b])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.max(shapes,axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nimg_size = 128\nval_size = 0.2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Preprocessing Images Functions:\ndef get_label(img_path):\n    ls = tf.strings.split(img_path, '/')\n    #perform one hot encoding\n    label = ls[-2] == ['NORMAL', 'CNV', 'DME', 'DRUNSEN']\n\n    return label\n\ndef preprocess_image(img_path, target_size):\n    label = get_label(img_path)\n    img = tf.io.read_file(img_path)\n    img = tf.image.decode_jpeg(img)\n    img = tf.image.resize(img, target_size)\n\n    return img, label\n\ndef preprocess_image_2(img_path, target_size, augment = False):\n    img, label = preprocess_image(img_path, target_size)\n    d3_img = tf.image.grayscale_to_rgb(img)\n\n    if augment == True:\n        extra_data = augment_data(d3_img)\n        d3_img = extra_data\n        label = [label]*len(extra_data)\n        \n    else:\n        pass\n    \n    return d3_img, label\n\nplt.figure()\n#print(preprocess_image_2('../input/mydata/Data/CNV/CNV-103044-1.jpeg', (128,128))[0][:,:,:])\nplt.imshow(preprocess_image_2('../input/mydata/Data/CNV/CNV-103044-1.jpeg', (128,128))[0][:,:,:])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_path = '../input/mydata/Data'\nls_files = tf.data.Dataset.list_files(data_path + '/*/*')\n#augment\n\n\nval = ls_files.take(int(val_size*len(list(ls_files))))\ntrain = ls_files.skip(int(val_size*len(list(ls_files))))\npreprocess_function = partial(preprocess_image, target_size = (img_size, img_size))\n\nval_pipeline = val.map(preprocess_function).shuffle(100).batch(batch_size)\ntrain_pipeline = train.map(preprocess_function).shuffle(100).batch(batch_size)\nval_pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checkpoint 2: Print a sample of the data and explain the selected range and the purpose of normalisation----------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image = tf.io.read_file('../input/mydata/Data/DRUNSEN/DRUSEN-2211381-2.jpeg')\nimage_array = tf.image.decode_jpeg(image).numpy()\n#range:\nmaximum = np.max(image_array)\nminimum = np.min(image_array)\n\nprint(image_array.shape)\nprint(image_array.reshape((496,768))[:15,:15])\nprint('The range of data is {} to {}'.format(minimum, maximum))\nprint('We can normalize the range to 0 to 1 to reduce the variance')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Build Base Model (As defined in the exercise)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import keras stuff\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = tf.keras.Sequential([\n    layers.Conv2D(16, 3, padding = 'same', activation = 'relu'),\n    layers.MaxPooling2D(),\n    layers.Conv2D(32, 3, padding = 'same', activation = 'relu'),\n    layers.MaxPooling2D(),\n    layers.Conv2D(64, 3, padding = 'same', activation = 'relu'),\n    layers.MaxPooling2D(),\n    layers.Flatten(),\n    layers.Dense(128, activation = 'relu'),\n    layers.Dense(4, activation = 'sigmoid')\n])\n\nmodel2 = tf.keras.Sequential([\n    layers.Conv2D(16, 3, padding = 'same', activation = 'relu'),\n    layers.MaxPooling2D(),\n    layers.Conv2D(32, 3, padding = 'same', activation = 'relu'),\n    layers.MaxPooling2D(),\n    layers.Conv2D(64, 3, padding = 'same', activation = 'relu'),\n    layers.MaxPooling2D(),\n    layers.Flatten(),\n    layers.Dense(128, activation = 'relu'),\n    layers.Dense(4, activation = 'sigmoid')\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define Loss function and Optimiser\nloss_func = tf.keras.losses.CategoricalCrossentropy()\noptimizer = tf.keras.optimizers.Adam()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"#Checkpoint 4: Explain selection of loss function and optimiser Optimizer\n\nCategorical Crossentropy was chosen because it is a multiclass classification problem\n\nAdam Optimizer was used because it is a state-of-the-art optimizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loss = tf.keras.metrics.Mean('train_loss')\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')\n\ntest_loss = tf.keras.metrics.Mean('test_loss')\ntest_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('test_accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define Training/Testing Function here\ndef train_step(model, optimizer, images, labels):\n    with tf.GradientTape() as tape:\n        predictions = model(images)\n        loss = loss_func(labels, predictions)\n    \n    gradients = tape.gradient(loss, model.trainable_variables)\n    \n    #backprop\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    \n    train_loss(loss)\n    train_accuracy(tf.argmax(labels, axis = 1), predictions)\n    \ndef test_step(model, images, labels):\n    pred = model(images)\n    loss = loss_func(labels, pred)\n    \n    test_loss(pred)\n    test_accuracy(tf.argmax(labels,axis = 1), pred)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_graph(ls):\n    train_loss, test_loss, train_acc, test_acc = ls\n    plt.figure(figsize=(8,8))\n    plt.plot(train_loss, label = 'training loss')\n    plt.plot(test_loss, label = 'test loss')\n    plt.plot(train_acc, label = 'training accuracy')\n    plt.plot(test_acc, label = 'test accuracy')\n    plt.xlabel('epochs')\n    plt.ylabel('metric values')\n    plt.ylim((0,1.5))\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, epochs, optimizer, training_pipe, testing_pipe):\n    plt.figure()\n\n    train_loss_ls = []\n    test_loss_ls = []\n    train_acc_ls = []\n    test_acc_ls = []\n    for e in range(epochs):\n        for i, (im, lb) in enumerate(training_pipe):\n            train_step(model, optimizer, im, lb)\n            print(i/len(train_pipeline))\n            \n        for j, (im, lb) in enumerate(testing_pipe):\n            test_step(model, im, lb)\n        train_loss_ls.append(train_loss.result())\n        test_loss_ls.append(test_loss.result())\n        train_acc_ls.append(train_accuracy.result())\n        test_acc_ls.append(test_accuracy.result())\n        print('Epoch {} - Training Loss: {}, Training Accuracy: {}, Validation Loss: {}, Validation Accuracy: {}'.format(1+e,train_loss.result(), train_accuracy.result(), test_loss.result(), test_accuracy.result()))\n        train_accuracy.reset_states()\n        train_loss.reset_states()\n        test_accuracy.reset_states()\n        test_loss.reset_states()\n        \n    ls_of_metrics = [train_loss_ls, test_loss_ls, train_acc_ls, test_acc_ls]\n    plot_graph(ls_of_metrics)\n    \n    return ls_of_metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checkpoint 5: Display training loss-epoch graph","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checkpoint 6: Display Accuracy-epoch graph\nvalues = train_model(model1, 20, optimizer, train_pipeline, val_pipeline)\nplot_graph(values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#Checkpoint 7: Define and explain the choice of transfer base model for transfer learning\n\nI used a ResNet-50 model as the base model, as it was trained on images and I think it should be able to extract the features properly. ResNet-50 has shortcut connections within the model and it is good for extending the model's ability to keep improving."},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model = tf.keras.applications.ResNet50(input_shape = (img_size,img_size,3), include_top = False, weights = 'imagenet')\nbase_model.trainable = False\nbase_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = keras.Input(shape = (img_size,img_size,3))\nx = base_model(inputs)\nx = keras.layers.Flatten()(x)\nx = keras.layers.Dense(128, activation = 'relu')(x)\noutputs = keras.layers.Dense(4, activation = 'softmax')(x)\n\nnew_model = keras.Model(inputs, outputs)\nprint(new_model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocess_function_2 = partial(preprocess_image_2, target_size = (128, 128))\nval_pipeline_2 = val.map(preprocess_function_2).shuffle(100).batch(batch_size)\ntrain_pipeline_2 = train.map(preprocess_function_2).shuffle(100).batch(batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trf_val = train_model(new_model, 20, optimizer, train_pipeline_2, val_pipeline_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_graph(trf_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checkpoint 8: Display graph printout of the base model with the OCT image classication extension\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checkpoint 9: The training data currently being used is simple and as such transfer learning isn't as advantageous,\n#however some observations can be made when comparing the first model with the second, discuss these observation.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The second model was able to produce a much higher accuracy at the start, for example, within 10 epochs, its validation accuracy has reached about 0.74, whereas for the first model, the validation accuracy has only reached about 0.5. However, the accuracy of the second model \n"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#Checkpoint 10: Discuss why this may not be favourable and the problems it presents. \n\nBiased datasets are not favourable as the model may take in the frequency of occurrence of a certain class into account and may skew the model predictions to predict less of that class, causing model performance to drop. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#3000 images for all except CNV which has 1000\n\nlst = os.listdir('../input/mydata/Data/CNV') # dir is your directory path\nlen(lst)\n\n#so we can triple the dataset in CNV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checkpoint 11: Show some methods that can be utilised to negate or minimise these effects. \n#Compare the accuracy and explain the pros and cons of these techniques (If any) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Image Augmentation can be used to generate more data in the under-represented class, through transformations such as zoom or reflection. Through this, the model can generalize better and give better performance. The pros of this technique is that it can be done rather easily and one does not need to collect more data. The con is that not all images can be augmented in any way and the training accuaracy may not improve as well.\n\n- Add class weights to make the model focus more on the under-represented class by modifying the loss function used.\n\nBelow, a combination of image augmentation on the whole dataset and adding class weights are used for the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"datagen = tf.keras.preprocessing.image.ImageDataGenerator(validation_split = 0.2,\n                                                            width_shift_range=0.2,\n                                                          height_shift_range = 0.2,\n                                                                shear_range = 0.1, \n                                                                horizontal_flip = True,\n                                                         )\n\ntrain_generator = datagen.flow_from_directory('../input/mydata/Data', target_size = (128,128), batch_size=32, subset = 'training', class_mode = 'categorical')\ntest_generator = datagen.flow_from_directory('../input/mydata/Data', target_size = (128,128), batch_size=32, subset = 'validation', class_mode = 'categorical')\n\n\nmodel2.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\nhist = model2.fit(train_generator, epochs = 20, validation_data = test_generator, class_weight = {0: 3.0, 1: 1.0, 2: 1.0, 3: 1.0})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\n\nfor key in hist.history.keys():\n    plt.plot(hist.history[key], label = key)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}