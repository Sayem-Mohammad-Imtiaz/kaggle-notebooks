{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 参考 https://github.com/AnthonyK97/Text-Classification-on-IMDB\n# https://github.com/AnthonyK97/Text-Classification-on-IMDB/blob/main/2%20CNN%2BGlove.ipynb\n\nimport os\nimport sys\nimport random\nimport time\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom collections import OrderedDict\nimport re, string\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\ndef seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    # some cudnn methods can be random even after fixing the seed\n    # unless you tell it to be deterministic\n    torch.backends.cudnn.deterministic = True\n    \n!mkdir ./model_bakup/\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nclass CFG:\n    batch_size = 20\n    lr = 0.01\n    eval_step_num = 300\n    best_eval_acc = 0.0\n    model_output_dir = './model_bakup/'\n    seed = 2032\n    \nglobal_start_t = time.time()\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-05-29T05:59:13.318089Z","iopub.execute_input":"2021-05-29T05:59:13.318441Z","iopub.status.idle":"2021-05-29T05:59:15.21215Z","shell.execute_reply.started":"2021-05-29T05:59:13.318362Z","shell.execute_reply":"2021-05-29T05:59:15.210776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_everything(seed=42)\n\nimdb_data = pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\nimdb_data['sentiment'] = imdb_data['sentiment'].map({'positive': 1, 'negative': 0})\nprint('before drop_duplicates, imdb_data.shape: ', imdb_data.shape)\nimdb_data = imdb_data.drop_duplicates()\nprint('after drop_duplicates, imdb_data.shape: ', imdb_data.shape)\nimdb_data = imdb_data.sample(25000)\nprint('after sample, imdb_data.shape: ', imdb_data.shape)\nimdb_data = imdb_data.sample(len(imdb_data)).reset_index(drop=True)  # shuffle\n\nimdb_data.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T05:59:15.21381Z","iopub.execute_input":"2021-05-29T05:59:15.214125Z","iopub.status.idle":"2021-05-29T05:59:16.610472Z","shell.execute_reply.started":"2021-05-29T05:59:15.214091Z","shell.execute_reply":"2021-05-29T05:59:16.60957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_WORDS = 10000   # 仅考虑最高频的10000个词\nMAX_LEN = 200\nword_count_dict = {}\n\ndef clean_text(text):\n    lowercase = text.lower().replace('\\n', ' ')\n    stripped_html = re.sub('<br />', ' ', lowercase)\n    cleaned_punctuation = re.sub('[%s]'%re.escape(string.punctuation), '', stripped_html)\n    return cleaned_punctuation\n\nfor review in imdb_data['review'].values:\n    cleaned_text = clean_text(review)\n    for word in cleaned_text.split(' '):\n        word_count_dict[word] = word_count_dict.get(word, 0) + 1\n            \ndf_word_dict = pd.DataFrame(pd.Series(word_count_dict, name='count'))\ndf_word_dict = df_word_dict.sort_values(by='count', ascending=False)\n\ndf_word_dict = df_word_dict[:MAX_WORDS-2]     # 总共取前max_words-2个词\ndf_word_dict['word_id'] = range(2, MAX_WORDS)\n\nword_id_dict = df_word_dict['word_id'].to_dict()\nword_id_dict['<unknown>'] = 0\nword_id_dict['<padding>'] = 1\n\ndf_word_dict.head(15)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T05:59:16.612043Z","iopub.execute_input":"2021-05-29T05:59:16.612294Z","iopub.status.idle":"2021-05-29T05:59:19.908779Z","shell.execute_reply.started":"2021-05-29T05:59:16.612268Z","shell.execute_reply":"2021-05-29T05:59:19.908002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_count_dict = {}\n\ndef clean_text(text):\n    lowercase = text.lower().replace('\\n', ' ')\n    stripped_html = re.sub('<br />', ' ', lowercase)\n    cleaned_punctuation = re.sub('[%s]'%re.escape(string.punctuation), '', stripped_html)\n    return cleaned_punctuation\n\nfor review in imdb_data['review'].values:\n    cleaned_text = clean_text(review)\n    for word in cleaned_text.split(' '):\n        word_count_dict[word] = word_count_dict.get(word, 0) + 1\n            \ndf_word_dict = pd.DataFrame(pd.Series(word_count_dict, name='count'))\ndf_word_dict = df_word_dict.sort_values(by='count', ascending=False)\n\ndf_word_dict = df_word_dict[:MAX_WORDS-2] # 总共取前max_words-2个词\ndf_word_dict['word_id'] = range(2, MAX_WORDS)\n\nword_id_dict = df_word_dict['word_id'].to_dict()\nword_id_dict['<unknown>'] = 0\nword_id_dict['<padding>'] = 1\n\ndf_word_dict.head(15)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-05-29T05:59:19.910419Z","iopub.execute_input":"2021-05-29T05:59:19.910774Z","iopub.status.idle":"2021-05-29T05:59:23.586518Z","shell.execute_reply.started":"2021-05-29T05:59:19.910738Z","shell.execute_reply":"2021-05-29T05:59:23.585767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pad(data_list, pad_length):\n    padded_list = data_list.copy()\n    \n    if len(data_list) > pad_length:\n        padded_list = data_list[-pad_length:]\n        \n    if len(data_list) < pad_length:\n        padded_list = [1] * (pad_length-len(data_list)) + data_list\n        \n    return padded_list\n\ndef text_to_token(text):\n    cleaned_text = clean_text(text)\n    word_token_list = [word_id_dict.get(word, 0) for word in cleaned_text.split(' ')]\n    pad_list = pad(word_token_list, MAX_LEN)\n    token = ' '.join([str(x) for x in pad_list])\n    return token\n            \nprocess_start_t = time.time()\nprint('start processing...')\nimdb_data['review_tokens'] = imdb_data['review'].map(text_to_token)\nprint('ok, cost time: ', time.time()-process_start_t)\nimdb_data.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T05:59:23.587763Z","iopub.execute_input":"2021-05-29T05:59:23.588091Z","iopub.status.idle":"2021-05-29T05:59:26.8853Z","shell.execute_reply.started":"2021-05-29T05:59:23.588056Z","shell.execute_reply":"2021-05-29T05:59:26.884565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data_train = imdb_data.iloc[:15000]\nimdb_data_valid = imdb_data.iloc[15000:20000]\nimdb_data_test = imdb_data.iloc[20000:]\nprint(f'imdb_data_train.shape: {imdb_data_train.shape}, imdb_data_valid.shape: {imdb_data_valid.shape}, '\n      f'imdb_data_test.shape: {imdb_data_test.shape}')","metadata":{"execution":{"iopub.status.busy":"2021-05-29T05:59:26.886519Z","iopub.execute_input":"2021-05-29T05:59:26.886856Z","iopub.status.idle":"2021-05-29T05:59:26.895004Z","shell.execute_reply.started":"2021-05-29T05:59:26.886819Z","shell.execute_reply":"2021-05-29T05:59:26.891679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"glove_path = '/kaggle/input/glove6b/glove.6B.100d.txt'\n\ncnt = 0\nword_2_vector_map = {}\nwith open(glove_path) as fin:\n    for line in fin:\n        line = line.strip()\n        word = line.split()[0]\n        vector = np.array([float(val) for val in line.split()[1:]])\n        word_2_vector_map[word] = vector\n        cnt += 1\nprint('cnt is', cnt, 'len of word_2_vector_map: ', len(word_2_vector_map))\n\nembed_size = 100\nglove6b_100d_weight = torch.zeros(len(word_id_dict), embed_size)\n\nfor word, idx in word_id_dict.items():\n    try:\n        vector = word_2_vector_map[word]\n    except:\n        print('not found in : ', word)\n        continue\n    glove6b_100d_weight[idx, :] = torch.from_numpy(vector)\n    \nprint('glove6b_100d_weight.shape: ', glove6b_100d_weight.shape)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T05:59:26.896269Z","iopub.execute_input":"2021-05-29T05:59:26.89678Z","iopub.status.idle":"2021-05-29T05:59:44.628847Z","shell.execute_reply.started":"2021-05-29T05:59:26.896743Z","shell.execute_reply":"2021-05-29T05:59:44.628056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg = CFG()\nseed_everything(seed=cfg.seed)\n\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-05-29T05:59:44.631093Z","iopub.execute_input":"2021-05-29T05:59:44.631428Z","iopub.status.idle":"2021-05-29T05:59:44.63746Z","shell.execute_reply.started":"2021-05-29T05:59:44.631392Z","shell.execute_reply":"2021-05-29T05:59:44.636679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class imdbDataset(Dataset):\n    def __init__(self, data_df):\n        self.data_df = data_df\n        \n    def __len__(self):\n        return len(self.data_df)\n    \n    def __getitem__(self, index):\n        label = self.data_df.iloc[index]['sentiment']\n        label = torch.tensor([float(label)], dtype=torch.float, device=device)\n        \n        tokens = self.data_df.iloc[index]['review_tokens']\n        feature = torch.tensor([int(x) for x in tokens.split(' ')], dtype=torch.long, device=device)\n            \n        return feature, label\n    \ndef generate_data_iter(cfg):\n    global imdb_data_train, imdb_data_valid, imdb_data_test\n    ds_train = imdbDataset(imdb_data_train)\n    ds_valid = imdbDataset(imdb_data_valid)\n    ds_test = imdbDataset(imdb_data_test)\n    print('len of ds_train: ', len(ds_train), 'len of ds_valid: ', len(ds_valid),\n          'len of ds_test: ', len(ds_test))\n\n    dl_train = DataLoader(ds_train, batch_size=cfg.batch_size, shuffle=True, num_workers=0)\n    dl_valid = DataLoader(ds_valid, batch_size=cfg.batch_size, shuffle=False, num_workers=0)\n    dl_test = DataLoader(ds_test, batch_size=cfg.batch_size, shuffle=False, num_workers=0)\n    return dl_train, dl_valid, dl_test\n\ndl_train, dl_valid, dl_test = generate_data_iter(cfg)\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-05-29T05:59:44.638704Z","iopub.execute_input":"2021-05-29T05:59:44.639037Z","iopub.status.idle":"2021-05-29T05:59:44.650383Z","shell.execute_reply.started":"2021-05-29T05:59:44.639001Z","shell.execute_reply":"2021-05-29T05:59:44.649549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CNN_Net(nn.Module):\n    def __init__(self):\n        global glove6b_100d_weight\n        super().__init__()\n        \n        #self.embedding = nn.Embedding(num_embeddings=MAX_WORDS, embedding_dim=3, padding_idx=1)\n        self.embedding = nn.Embedding.from_pretrained(glove6b_100d_weight, freeze=False)\n        assert self.embedding.weight.requires_grad==True, 'should be True, because freeze=False'\n        \n        self.conv = nn.Sequential()\n        self.conv.add_module('conv_1', nn.Conv1d(in_channels=100, out_channels=16, kernel_size=5))\n        self.conv.add_module('pool_1', nn.MaxPool1d(kernel_size=2))\n        self.conv.add_module('relu_1', nn.ReLU())\n        self.conv.add_module('conv_2', nn.Conv1d(in_channels=16, out_channels=128, kernel_size=2))\n        self.conv.add_module('pool_2', nn.MaxPool1d(kernel_size=2))\n        self.conv.add_module('relu_2', nn.ReLU())\n        \n        self.dense = nn.Sequential()\n        self.dense.add_module('flatten', nn.Flatten())\n        self.dense.add_module('linear', nn.Linear(6144, 1))\n        self.dense.add_module('sigmoid', nn.Sigmoid())\n        \n    def forward(self, x):\n        x = self.embedding(x).transpose(1, 2)\n        x = self.conv(x)\n        y = self.dense(x)\n        return y\n    \nmodel = CNN_Net()\nprint(model)\nmodel.to(device)     \n\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-05-29T05:59:44.651739Z","iopub.execute_input":"2021-05-29T05:59:44.652436Z","iopub.status.idle":"2021-05-29T05:59:48.580158Z","shell.execute_reply.started":"2021-05-29T05:59:44.652399Z","shell.execute_reply":"2021-05-29T05:59:48.579234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def accuracy(y_pred, y_true):\n    if type(y_pred)==list:\n        y_pred = np.array(y_pred)\n    y_pred = (y_pred > 0.5)\n    if type(y_true)==list:\n        y_true = np.array(y_true)\n    acc = (y_pred==y_true).mean()\n    return acc\n\ndef evaluate(model, dl_test, device):\n    global cfg\n    model.eval()\n    \n    y_true_lst, y_pred_lst = [], []\n    with torch.no_grad():\n        for step, batch in tqdm(enumerate(dl_test)):\n            feature, label = batch\n            feature, label = feature.to(device), label.to(device)\n            y_pred = model(feature)\n            y_pred_lst += list(y_pred.detach().cpu().numpy())\n            y_true_lst += list(label.detach().cpu().numpy())\n            \n    model.train() # 恢复模型为训练状态\n    acc = accuracy(y_pred_lst, y_true_lst)\n\n    return acc\n    \ndef train(model, dl_train, optimizer, loss_func, device):\n    global cfg, global_step_num\n    model.train()  # 将模型置为训练状态\n    \n    y_true_lst, y_pred_lst = [], []\n    #pbar = ProgressBar(n_total=len(dl_train), desc='Training')\n    \n    for step, batch in tqdm(enumerate(dl_train)):\n        global_step_num += 1\n        feature, label = batch\n        feature, label = feature.to(device), label.to(device)\n        #print('in train(), feature.shape:', feature.shape, 'label.shape: ', label.shape)\n        y_pred = model(feature)\n        train_loss = loss_func(y_pred, label)\n        y_pred_lst += list(y_pred.detach().cpu().numpy())\n        y_true_lst += list(label.detach().cpu().numpy())\n        train_loss.backward()\n        optimizer.step()\n        model.zero_grad()\n        \n    print('in train(), len(dl_train): ', len(dl_train))\n        \n    acc = accuracy(y_pred_lst, y_true_lst)\n    return acc\n\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-05-29T05:59:48.581437Z","iopub.execute_input":"2021-05-29T05:59:48.581969Z","iopub.status.idle":"2021-05-29T05:59:48.594327Z","shell.execute_reply.started":"2021-05-29T05:59:48.58193Z","shell.execute_reply":"2021-05-29T05:59:48.59352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"global_best_train_acc, global_best_valid_acc = 0.0, 0.0\nglobal_train_acc = 0.0\nglobal_step_num = 0\n\nepochs = 25\n# optimizer=torch.optim.Adagrad(model.parameters(), lr=cfg.lr)\noptimizer=torch.optim.Adam(model.parameters(), lr=cfg.lr)\nloss_func = nn.BCELoss()\n\nfor epoch in range(epochs):\n    train_acc = train(model, dl_train, optimizer, loss_func, device)\n    valid_acc = evaluate(model, dl_valid, device)\n    test_acc = evaluate(model, dl_test, device)\n    print(f'in epoch: {epoch}, train_acc: {train_acc:.5f}, valid_acc: {valid_acc:.5f}, test_acc: {test_acc:.5f}')\n    if train_acc > global_best_train_acc:\n        global_best_train_acc = train_acc\n    if valid_acc > global_best_valid_acc:\n        global_best_valid_acc = valid_acc\n        global_train_acc = train_acc\n        print(f'get new best_valid_acc: {global_best_valid_acc:.5f}, save the model now!')\n        torch.save(model.state_dict(), os.path.join(cfg.model_output_dir, 'best_step_model.pth'))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-05-29T05:59:48.59552Z","iopub.execute_input":"2021-05-29T05:59:48.595866Z","iopub.status.idle":"2021-05-29T06:00:28.076194Z","shell.execute_reply.started":"2021-05-29T05:59:48.595829Z","shell.execute_reply":"2021-05-29T06:00:28.073599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CNN_Net()\nmodel.to(device)\n\nmodel.load_state_dict(torch.load(os.path.join(cfg.model_output_dir, 'best_step_model.pth')))\ntest_acc = evaluate(model, dl_test, device)\nprint(f'final test_acc: {test_acc:.5f}, best_val_acc: {global_best_valid_acc:.5f}, '\n      f'train_acc: {global_train_acc:.5f}, best_train_acc: {global_best_train_acc:.5f}')\n\nprint('total finished, cost time: ', time.time() - global_start_t)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T06:00:38.436088Z","iopub.execute_input":"2021-05-29T06:00:38.436401Z","iopub.status.idle":"2021-05-29T06:00:41.111159Z","shell.execute_reply.started":"2021-05-29T06:00:38.436369Z","shell.execute_reply":"2021-05-29T06:00:41.110191Z"},"trusted":true},"execution_count":null,"outputs":[]}]}