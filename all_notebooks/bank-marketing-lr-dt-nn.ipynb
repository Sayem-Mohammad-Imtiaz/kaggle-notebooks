{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction\nThis notebook was created for analysis and prediction making of the *Bank marketing data set* from UCI Machine Learning Library. The data set can be accessed separately from the UCI Machine Learning Repository page, [here](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing)."},{"metadata":{},"cell_type":"markdown","source":"## Relevant Papers\n\nIn their paper \"A Data-Driven Approach to Predict the Success of Bank Telemarketing. (S. Moro, P. Cortez and P. Rita, June 2014)\", which can be found [here](http://repositorium.sdum.uminho.pt/bitstream/1822/30994/1/dss-v3.pdf), S. Moro, P. Cortez and P. Rita, propose a data mining (DM) approach to predict the success of telemarketing calls for selling bank long-term deposits, comparing four DM models: Logistic regression, Decision trees (DT), neural network (NN) and support vector machine."},{"metadata":{},"cell_type":"markdown","source":"## Attribute Information\n\n#### Bank client data\n`age`: numeric\n`job`: type of job (categorical: 'admin.', 'blue-collar', 'entrepreneur', 'housemaid', 'management', 'retired', 'self-employed', 'services', 'student', 'technician', 'unemployed', 'unknown')\n`marital`: marital status (categorical: 'divorced', 'married', 'single', 'unknown'; (note: 'divorced' means divorced or widowed))\n`education`:(categorical: 'basic.4y', 'basic.6y', 'basic.9y', 'high.school', 'illiterate', 'professional.course', 'university.degree', 'unknown')\n`default`: has credit in default? (categorical: 'no','yes','unknown')\n`housing`: has housing loan? (categorical: 'no', 'yes', 'unknown')\n`loan`: has personal loan? (categorical: 'no', 'yes', 'unknown')\n#### Related with the last contact of the current campaign:\n`contact`: contact communication type (categorical: 'cellular', 'telephone')\n`month`: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n`day_of_week`: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n`duration`: last contact duration, in seconds (numeric). \n**Important note:** `duration` highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n#### Other attributes:\n`campaign`: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n`pdays`: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n`previous`: number of contacts performed before this campaign and for this client (numeric)\n`poutcome`: outcome of the previous marketing campaign (categorical: 'failure', 'nonexistent', 'success')\n#### Social and economic context attributes\n`emp.var.rate`: employment variation rate - quarterly indicator (numeric)\n`cons.price.idx`: consumer price index - monthly indicator (numeric)\n`cons.conf.idx`: consumer confidence index - monthly indicator (numeric)\n`euribor3m`: euribor 3 month rate - daily indicator (numeric)\n`nr.employed`: number of employees - quarterly indicator (numeric)\n\n#### Output variable (desired target):\n`y` - has the client subscribed a term deposit? (binary: 'yes', 'no')"},{"metadata":{},"cell_type":"markdown","source":"## Models\n\nWe will create 3 models in order to make predictions and compare them with the original paper. These models are:\n- Logistic Regression\n- Decision tree\n- Neural Network\n\nAfter the initial predictions, each model will be \"optimized\" by `GridSearchCV` estimator, which will search for the best set of hyperparameters for every model.  "},{"metadata":{},"cell_type":"markdown","source":"## Metrics\n\nIn order to be consistent with the original paper and have the same base for our results, we will use the same metric: receiver operating characteristic (ROC) curve. The receiver operating characteristic (ROC) curve shows the performance of a two class classifier across the range of possible threshold values, plotting one minus the specificity (x-axis) versus the sensitivity (y-axis). The overall accuracy is given by the area under the curve, measuring the degree of discrimination that can be obtained from a given model. AUC is a popular classification metric that presents advantages of being independent of the class frequency or specific false positive/negative costs. The ideal method shouldpresent an AUC of 1.0, while an AUC of 0.5 denotes a random classifier.\n\nAdditional to AUC, metrics such as accuracy, cross-validation accuracy, recall, precision and f1-score will be used. Accuracy is $\\frac{Number of correct predictions}{Number of samples}$. When the dataset is imbalanced, accuracy may not be sufficient, because simply predicting all samples to be the major class can still get high accuracy. In such situation, a good metrics to use is f1 score. F1-score is calculated by $\\frac{2*precision*recall}{precision+recall}$, where precision is $\\frac{True Positives}{True Positives+False Positives}$ and recall is $\\frac{True Positives}{True Positives+False Negatives}$.\nPrecision measures a modelâ€™s ability to correctly identify positive samples and recall measures the proportion of positive samples that are identified. F1-score ranges from 0 (cannot make true positive predictio) to 1 (being correct in all predictions). "},{"metadata":{},"cell_type":"markdown","source":"## Goal \n\nUsing the models we created, we will try to predict the class value of `y` column with better scores of AUC, than the scores presented in the original paper. The dataset we will use for that is the *bank-additional-full.csv*, which is very close to the data analyzed in the original paper."},{"metadata":{},"cell_type":"markdown","source":"## Import libraries/packages "},{"metadata":{"trusted":true},"cell_type":"code","source":"### General libraries ###\nimport pandas as pd\nfrom pandas.api.types import CategoricalDtype\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport seaborn as sns\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\n##################################\n\n### ML Models ###\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.tree.export import export_text\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\n##################################\n\n### Metrics ###\nfrom sklearn import metrics\nfrom sklearn.metrics import f1_score,confusion_matrix, mean_squared_error, mean_absolute_error, classification_report, roc_auc_score, roc_curve, precision_score, recall_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part 1: Load and clean the data\n\nIn this section we will load the data from the csv file and check for any \"impurities\", such as null values or duplicate rows. If any of these will appear, we will remove them from the data set. We will also plot the correlations of the class column with all the other columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the data.\ndata=pd.read_csv('../input/bank-additional-full.csv')\n\n# Information\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove \"duration\" column as proposed above in the description.\ndata=data.drop(['duration'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Value counts for categorical columns \nfor i in data.columns[data.dtypes=='object']:\n    print(data[i].value_counts(),'\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we check for duplicate rows. If any, we remove them from the data set, since they provide only reduntant information."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for duplicate rows.\nprint(f\"There are {data.duplicated().sum()} duplicate rows in the data set.\")\n\n# Remove duplicate rows.\ndata=data.drop_duplicates()\nprint(\"The duplicate rows were removed.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also check for null values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for null values.\nprint(f\"There are {data.isna().any().sum()} cells with null values in the data set.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Information about the data set after the cleaning.\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label encoding on category columns.\nle=LabelEncoder()\nfor i in data.columns[data.dtypes=='object']:\n    data[i]=le.fit_transform(data[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below is the plot of the correlation matrix for the data set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation matrix.\nplt.figure(figsize=(20,20))\nsns.set(font_scale=1.1)\nsns.heatmap(data.corr(),annot=True, cmap='rainbow',linewidth=0.5, fmt='.2f')\nplt.title('Correlation matrix');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part 2: Pre-processing\n\nIn this part we prepare our data for our models. This means that we choose the columns that will be our independed variables and which column the class that we want to predict. Once we are done with that, we split our data into train and test sets and perfom a standardization upon them."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distinguish attribute columns and class column.\nX=data[data.columns[:-1]]\ny=data['y']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split to train and test sets. \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Standardization\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part 3: Modeling\n\nIn this section we build and try 3 models:\n - Logistic Regression\n - Decision tree\n - Neural network\n\nEach model will be trained and make a prediction for the test set. Accuracy, f1 score, confusion matrix and ROC will be calculated for each model. Then we will use the `GridSearchCV` module to tune our models and search for the best hyperparameters in order to increase the accuracy of each model."},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize a Logistic Regression estimator.\nlogreg=LogisticRegression(multi_class='auto', random_state=25, n_jobs=-1)\n\n# Train the estimator.\nlogreg.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions.\nlog_pred=logreg.predict(X_test)\n\n# CV score for MLP\ncv_logreg=cross_val_score(logreg, X_train, y_train, cv=10).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Metrics for Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy: 1 is perfect prediction.\nprint('Accuracy: %.3f' % logreg.score(X_test, y_test))\n\n# Cross-Validation accuracy\nprint('Cross-validation accuracy: %0.3f' % cv_logreg)\n\n# Precision\nprint('Precision: %.3f' % precision_score(y_test, log_pred))\n\n# Recall\nprint('Recall: %.3f' % recall_score(y_test, log_pred))\n\n# f1 score: best value at 1 (perfect precision and recall) and worst at 0.\nprint('F1 score: %.3f' % f1_score(y_test, log_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict probabilities for the test data.\nlogreg_probs = logreg.predict_proba(X_test)\n\n# Keep Probabilities of the positive class only.\nlogreg_probs = logreg_probs[:, 1]\n\n# Compute the AUC Score.\nauc_logreg = roc_auc_score(y_test, logreg_probs)\nprint('AUC: %.2f' % auc_logreg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confusion matrix for Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot confusion matrix for Logistic Regression.\nlogreg_matrix = confusion_matrix(y_test,log_pred)\nsns.set(font_scale=1.3)\nplt.subplots(figsize=(8, 8))\nsns.heatmap(logreg_matrix, annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for Logistic Regression');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Grid search for Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameters to be checked.\nparameters = {'C':[0.0001, 0.001, 0.01, 1, 0.1, 10, 100, 1000],\n              'penalty':['none','l2'] ,\n              'solver':['lbfgs','sag','saga','newton-cg']\n             }\n\n# Logistic Regression estimator.\ndefault_logreg=LogisticRegression(multi_class='auto', random_state=25, n_jobs=-1)\n\n# GridSearchCV estimator.\ngs_logreg = GridSearchCV(default_logreg, parameters, cv=10, n_jobs=-1, verbose=1)\n\n# Train the GridSearchCV estimator and search for the best parameters.\ngs_logreg.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions with the best parameters.\ngs_log_pred=gs_logreg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Grid search metrics for Logistic Regression "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best parameters.\nprint(\"Best Logistic Regression Parameters: {}\".format(gs_logreg.best_params_))\n\n# Cross validation accuracy for the best parameters.\nprint('Cross-validation accuracy: %0.3f' % gs_logreg.best_score_)\n\n# Accuracy: 1 is perfect prediction.\nprint('Accuracy: %0.3f' % (gs_logreg.score(X_test,y_test)))\n\n# Precision\nprint('Precision: %.3f' % precision_score(y_test, gs_log_pred))\n\n# Recall\nprint('Recall: %.3f' % recall_score(y_test, gs_log_pred))\n\n# f1 score: best value at 1 (perfect precision and recall) and worst at 0.\nprint('F1 score: %.3f' % f1_score(y_test, gs_log_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict probabilities for the test data.\ngs_logreg_probs = gs_logreg.predict_proba(X_test)\n\n# Keep Probabilities of the positive class only.\ngs_logreg_probs = gs_logreg_probs[:, 1]\n\n# Compute the AUC Score.\ngs_logreg_auc = roc_auc_score(y_test, gs_logreg_probs)\nprint('AUC: %.2f' % gs_logreg_auc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confusion Matrix for Grid search Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print confusion matrix for Logistic regression.\ngs_logreg_matrix = confusion_matrix(y_test,gs_log_pred)\nsns.set(font_scale=1.3)\nplt.subplots(figsize=(8, 8))\nsns.heatmap(gs_logreg_matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for GridSearchCV Logistic Regression');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the ROC curves.\nlogreg_fpr, logreg_tpr, logreg_thresholds = roc_curve(y_test, logreg_probs)\ngs_logreg_fpr, gs_logreg_tpr, gs_logreg_thresholds = roc_curve(y_test, gs_logreg_probs)\n\n# Plot the ROC curves.\nplt.figure(figsize=(8,8))\nplt.plot(logreg_fpr, logreg_tpr, color='black', label='LogReg ROC (AUC= %0.2f)'% auc_logreg)\nplt.plot(gs_logreg_fpr, gs_logreg_tpr, color='red', linestyle='--',label='GridSearch+LogReg ROC (AUC= %0.2f)'% gs_logreg_auc)\nplt.plot([0, 1], [0, 1], color='darkblue', linestyle='--',label='random')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curves')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize a decision tree estimator.\ntr = tree.DecisionTreeClassifier(max_depth=3,random_state=25)\n\n# Train the estimator.\ntr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the tree.\nfig=plt.figure(figsize=(23,15))\ntree.plot_tree(tr.fit(X_train, y_train),feature_names=X.columns,filled=True,rounded=True,fontsize=16);\nplt.title('Decision Tree');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the tree in a simplified version.\nr = export_text(tr, feature_names=X.columns.tolist())\nprint(r)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions.\ntr_pred=tr.predict(X_test)\n\n# CV score for Decision tree\ncv_tr=cross_val_score(tr, X_train, y_train, cv=10).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Metrics for Decision tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy: 1 is perfect prediction.\nprint('Accuracy: %.3f' % tr.score(X_test, y_test))\n\n# Cross-Validation accuracy\nprint('Cross-validation accuracy: %0.3f' % cv_tr)\n\n# Precision\nprint('Precision: %.3f' % precision_score(y_test, tr_pred))\n\n# Recall\nprint('Precision: %.3f' % recall_score(y_test, tr_pred))\n\n# f1 score: best value at 1 (perfect precision and recall) and worst at 0.\nprint('F1 score: %.3f' % f1_score(y_test, tr_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict propabilities for the test data.\ntr_probs = tr.predict_proba(X_test)\n\n# Keep Probabilities of the positive class only.\ntr_probs = tr_probs[:, 1]\n\n# Compute the AUC Score.\nauc_tr = roc_auc_score(y_test, tr_probs)\nprint('AUC: %.2f' % auc_tr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confusion Matrix for Decision tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print confusion matrix for Decision tree.\ntr_matrix = confusion_matrix(y_test,tr_pred)\nsns.set(font_scale=1.3)\nplt.subplots(figsize=(8,8))\nsns.heatmap(tr_matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for Decision tree');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Grid search for Decision tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameters to be checked.\nparameters = {'criterion':['gini','entropy'],\n              'max_depth':[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n             }\n\n# MLP estimator.\ndefault_tr = tree.DecisionTreeClassifier(random_state=25)\n\n# GridSearchCV estimator.\ngs_tree = GridSearchCV(default_tr, parameters, cv=10, n_jobs=-1,verbose=1)\n\n# Train the GridSearchCV estimator and search for the best parameters.\ngs_tree.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions with the best parameters.\ngs_tree_pred=gs_tree.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Grid search metrics for Decision tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best parameters.\nprint(\"Best Decision tree Parameters: {}\".format(gs_tree.best_params_))\n\n# Cross validation accuracy for the best parameters.\nprint('Cross-validation accuracy: %0.3f' % gs_tree.best_score_)\n\n# Accuracy: 1 is perfect prediction.\nprint('Accuracy: %0.3f' % (gs_tree.score(X_test,y_test)))\n\n# Precision\nprint('Precision: %.3f' % precision_score(y_test, gs_tree_pred))\n\n# Recall\nprint('Recall: %.3f' % recall_score(y_test, gs_tree_pred))\n\n# f1 score: best value at 1 (perfect precision and recall) and worst at 0.\nprint('F1 score: %.3f' % f1_score(y_test, gs_tree_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict probabilities for the test data.\ngs_tree_probs = gs_tree.predict_proba(X_test)\n\n# Keep Probabilities of the positive class only.\ngs_tree_probs = gs_tree_probs[:, 1]\n\n# Compute the AUC Score.\ngs_tree_auc = roc_auc_score(y_test, gs_tree_probs)\nprint('AUC: %.2f' % gs_tree_auc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confusion Matrix for Grid search Decision tree"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Print confusion matrix for Decision tree.\ngs_tr_matrix = confusion_matrix(y_test,gs_tree_pred)\nsns.set(font_scale=1.3)\nplt.subplots(figsize=(8, 8))\nsns.heatmap(gs_tr_matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for GridSearchCV Decision tree');","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Get the ROC Curves.\ngs_tr_fpr, gs_tr_tpr, gs_tr_thresholds = roc_curve(y_test, gs_tree_probs)\ntr_fpr, tr_tpr, tr_thresholds = roc_curve(y_test, tr_probs)\n\n# Plot the ROC curves.\nplt.figure(figsize=(8,8))\nplt.plot(tr_fpr, tr_tpr, color='red', label='Decision tree ROC (AUC= %0.2f)'% auc_tr)\nplt.plot(gs_tr_fpr, gs_tr_tpr, color='green', label='GridSearch+Decision tree ROC (AUC= %0.2f)'% gs_tree_auc)\nplt.plot([0, 1], [0, 1], color='darkblue', linestyle='--',label='random')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curves')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Neural network (MLP)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize a Multi-layer Perceptron classifier.\nmlp = MLPClassifier(hidden_layer_sizes=(100),max_iter=1000, random_state=25,shuffle=True,)\n\n# Train the classifier.\nmlp.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions.\nmlp_pred = mlp.predict(X_test)\n\n# CV score for MLP\ncv_mlp=cross_val_score(mlp, X_train, y_train, cv=10).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Metrics for Neural network (MLP)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy: 1 is perfect prediction.\nprint('Accuracy: %.3f' % mlp.score(X_test, y_test))\n\n# Cross-Validation accuracy\nprint('Cross-validation accuracy: %0.3f' % cv_mlp)\n\n# Precision\nprint('Precision: %.3f' % precision_score(y_test, mlp_pred))\n\n# Recall\nprint('Recall: %.3f' % recall_score(y_test, mlp_pred))\n\n# f1 score: best value at 1 (perfect precision and recall) and worst at 0.\nprint('F1 score: %.3f' % f1_score(y_test, mlp_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict probabilities for the test data.\nmlp_probs = mlp.predict_proba(X_test)\n\n# Keep probabilities of the positive class only.\nmlp_probs = mlp_probs[:, 1]\n\n# Compute the AUC Score.\nauc_mlp = roc_auc_score(y_test, mlp_probs)\nprint('AUC: %.2f' % auc_mlp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confusion Matrix for Neural network (MLP)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print confusion matrix for Multi-layer Perceptron.\nmatrix = confusion_matrix(y_test,mlp_pred)\nsns.set(font_scale=1.3)\nplt.subplots(figsize=(8, 8))\nsns.heatmap(matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for Multi-layer Perceptron');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Grid search for Neural network (MLP)"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Hyperparameters to be checked.\nparameters = {'activation':['logistic','relu'],'solver': ['lbfgs','adam','sgd'],\n              'alpha':10.0 ** -np.arange(1,4),\n              'hidden_layer_sizes':[(20),(4),(10),(14,2),(4,1),(10,5),(11,3)]}\n\n# Decision tree estimator.\ndefault_mlp = MLPClassifier(random_state=25)\n\n# GridSearchCV estimator.\ngs_mlp = GridSearchCV(default_mlp, parameters, cv=10, n_jobs=-1,verbose=1)\n\n# Train the GridSearchCV estimator and search for the best parameters.\ngs_mlp.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions with the best parameters.\ngs_mlp_pred=gs_mlp.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Grid search metrics for Neural network (MLP)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best parameters.\nprint(\"Best MLP Parameters: {}\".format(gs_mlp.best_params_))\n\n# Cross validation accuracy for the best parameters.\nprint('Cross-validation accuracy: %0.3f' % gs_mlp.best_score_)\n\n# Accuracy: 1 is perfect prediction.\nprint('Accuracy: %0.3f' % (gs_mlp.score(X_test,y_test)))\n\n# Precision\nprint('Precision: %.3f' % precision_score(y_test, gs_mlp_pred))\n\n# Recall\nprint('Recall: %.3f' % recall_score(y_test, gs_mlp_pred))\n\n# f1 score: best value at 1 (perfect precision and recall) and worst at 0.\nprint('F1 score: %.3f' % f1_score(y_test, gs_mlp_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict probabilities for the test data.\ngs_mlp_probs = gs_mlp.predict_proba(X_test)\n\n# Keep Probabilities of the positive class only.\ngs_mlp_probs = gs_mlp_probs[:, 1]\n\n# Compute the AUC Score.\ngs_mlp_auc = roc_auc_score(y_test, gs_mlp_probs)\nprint('AUC: %.2f' % gs_mlp_auc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confusion Matrix for Neural network (MLP)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot confusion matrix for GridSearchCV Multi-layer Perceptron.\nmatrix = confusion_matrix(y_test,gs_mlp_pred)\nplt.figure(figsize=(8,8))\nsns.heatmap(matrix,annot=True, cbar=False, cmap='twilight',linewidth=0.5,fmt=\"d\")\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix for GridSearchCV Multi-layer Perceptron');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the ROC curves.\ngs_mlp_fpr, gs_mlp_tpr,gs_mlp_thresholds = roc_curve(y_test, gs_mlp_probs)\nmlp_fpr, mlp_tpr, mlp_thresholds = roc_curve(y_test, mlp_probs)\n\n# Plot the ROC curve.\nplt.figure(figsize=(8,8))\nplt.plot(mlp_fpr, mlp_tpr, color='red', label='MLP ROC (AUC= %0.2f)'% auc_mlp)\nplt.plot(gs_mlp_fpr, gs_mlp_tpr, color='green', label='GridSearch+MLP ROC (AUC= %0.2f)'% gs_mlp_auc)\nplt.plot([0, 1], [0, 1], color='darkblue', linestyle='--',label='random')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curves')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics=['Accuracy','CV accuracy','Precision','Recall','F1','ROC AUC']\n\n# Plot metrics.\nfig = go.Figure(data=[\n    go.Bar(name='Logistic Regression', x=metrics,\n           y=[logreg.score(X_test, y_test),cv_logreg,precision_score(y_test, log_pred),recall_score(y_test, log_pred),f1_score(y_test, log_pred),auc_logreg]),\n    go.Bar(name='Decision tree', x=metrics,\n           y=[tr.score(X_test, y_test),cv_tr,precision_score(y_test, tr_pred),recall_score(y_test, tr_pred),f1_score(y_test, tr_pred),auc_tr]),\n    go.Bar(name='Neural Network', x=metrics,\n           y=[mlp.score(X_test, y_test),cv_mlp,precision_score(y_test, mlp_pred),recall_score(y_test, mlp_pred),f1_score(y_test, mlp_pred),auc_mlp]),\n    go.Bar(name='GridSearchCV+Logistic Regression',x=metrics,\n           y=[gs_logreg.score(X_test,y_test),gs_logreg.best_score_,precision_score(y_test, gs_log_pred),recall_score(y_test, gs_log_pred),f1_score(y_test, gs_log_pred),gs_logreg_auc]),\n    go.Bar(name='GridSearchCV+Decision tree',x=metrics,\n           y=[gs_tree.score(X_test,y_test),gs_tree.best_score_,precision_score(y_test, gs_tree_pred),recall_score(y_test, gs_tree_pred), f1_score(y_test, gs_tree_pred),gs_tree_auc]),\n    go.Bar(name='GridSearchCV+Neural Network', x=metrics, \n           y=[gs_mlp.score(X_test,y_test),gs_mlp.best_score_,precision_score(y_test, gs_mlp_pred),recall_score(y_test, gs_mlp_pred), f1_score(y_test, gs_mlp_pred),gs_mlp_auc])\n    ])\n\nfig.update_layout(title_text='Metrics for all models',\n                  barmode='group',xaxis_tickangle=-45,bargroupgap=0.05)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the ROC curve.\nplt.figure(figsize=(8,8))\nplt.plot(gs_mlp_fpr, gs_mlp_tpr, color='green', label='GridSearch+MLP ROC (AUC= %0.2f)'% gs_mlp_auc)\nplt.plot(gs_tr_fpr, gs_tr_tpr, color='orange', label='GridSearch+Decision tree ROC (AUC= %0.2f)'% gs_tree_auc)\nplt.plot(gs_logreg_fpr, gs_logreg_tpr, color='red',label='GridSearch+LogReg ROC (AUC= %0.2f)'% gs_logreg_auc)\nplt.plot([0, 1], [0, 1], color='darkblue', linestyle='--',label='random')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curves for GridSearch')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d={\n'': ['Logistic Regression','GridSearchCV + Logistic Regression','Decision Tree','GridSearchCV + Decision Tree','Neural Network (MLP)','GridSearchCV + Neural Network (MLP)'],\n'Accuracy': [logreg.score(X_test, y_test), gs_logreg.score(X_test,y_test),tr.score(X_test, y_test),gs_tree.score(X_test,y_test),mlp.score(X_test, y_test),gs_mlp.score(X_test, y_test)],\n'CV Accuracy': [cv_logreg, gs_logreg.best_score_, cv_tr,gs_tree.best_score_,cv_mlp,gs_mlp.best_score_],\n'Precision': [precision_score(y_test, log_pred), precision_score(y_test, gs_log_pred),precision_score(y_test, tr_pred),precision_score(y_test, gs_tree_pred),precision_score(y_test, mlp_pred),precision_score(y_test, gs_mlp_pred)],\n'Recall': [recall_score(y_test, log_pred), recall_score(y_test, gs_log_pred),recall_score(y_test, tr_pred),recall_score(y_test, gs_tree_pred),recall_score(y_test, mlp_pred),recall_score(y_test, gs_mlp_pred)],\n'F1': [f1_score(y_test, log_pred), f1_score(y_test, gs_log_pred),f1_score(y_test, tr_pred),f1_score(y_test, gs_tree_pred),f1_score(y_test, mlp_pred),f1_score(y_test, gs_mlp_pred)],\n'ROC AUC': [auc_logreg, gs_logreg_auc, auc_tr, gs_tree_auc, auc_mlp, gs_mlp_auc]\n}\n\nresults=pd.DataFrame(data=d).round(3).set_index('')\nresults","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}