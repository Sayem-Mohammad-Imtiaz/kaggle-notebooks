{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1: UNDERSTAND THE PROBLEM STATMENT AND BUSINESS CASE"},{"metadata":{},"cell_type":"markdown","source":"The goal is to detect fake news based on Recurrent Neural Networks.\nNatural Language processors (NLP) works by converting words into numbers \nThese numbers are then used to train an AI/ML model to make predictions \nWe will analyze thousand of news text to detect if it's fake or not"},{"metadata":{},"cell_type":"markdown","source":"# 2: IMPORT LIBRARIES AND DATASETS"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --upgrade tensorflow-gpu==2.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install jupyterthemes\n!pip install plotly\n!pip install --upgrade nbformat\n!pip install nltk\n!pip install spacy # spaCy is an open-source software library for advanced natural language processing\n!pip install WordCloud\n!pip install gensim # Gensim is an open-source library for unsupervised topic modeling and natural language processing\nimport nltk\nnltk.download('punkt')\n\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\nimport nltk\nimport re\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\n# import keras\nfrom tensorflow.keras.preprocessing.text import one_hot, Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Embedding, Input, LSTM, Conv1D, MaxPool1D, Bidirectional\nfrom tensorflow.keras.models import Model\nfrom jupyterthemes import jtplot\njtplot.style(theme='monokai', context='notebook', ticks=True, grid=False) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_true = pd.read_csv(\"/kaggle/input/fake-and-real-news-dataset/True.csv\")\ndf_fake = pd.read_csv(\"/kaggle/input/fake-and-real-news-dataset/Fake.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_true.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The number of rows of True news are : {}\".format(len(df_true)))\nprint(\"The number of rows of Fake news are : {}\".format(len(df_fake)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_true.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fake.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_true.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fake.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3: PERFORM FEATURE ENGINEERING"},{"metadata":{},"cell_type":"markdown","source":"* create a new columns called isfake - 1=True(Fake), 0=False\n* Combine 2 dataframes together"},{"metadata":{"trusted":true},"cell_type":"code","source":"# add a target class column to indicate whether the news is real or fake\ndf_true['isfake'] = 0\ndf_true.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fake['isfake'] = 1\ndf_fake.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Concatenate Real and Fake News\ndf = pd.concat([df_true, df_fake]).reset_index(drop = True)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#delete column in memory, not only in this notebook\ndf.drop(columns = ['date'], inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine title and text together\ndf['original'] = df['title'] + ' ' + df['text']\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['original'][0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  4: PERFORM DATA CLEANING"},{"metadata":{"trusted":true},"cell_type":"code","source":"# download stopwords\nnltk.download(\"stopwords\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Obtain additional stopwords from nltk\nfrom nltk.corpus import stopwords\n#I want the stop word in English language\nstop_words = stopwords.words('english')\n#we add stop words\nstop_words.extend(['from', 'subject', 're', 'edu', 'use'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove stopwords and remove words with 2 or less characters\ndef preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in stop_words:\n            result.append(token)\n            \n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply the function to the dataframe\ndf['clean'] = df['original'].apply(preprocess)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show original news\ndf['original'][0][:100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show cleaned up news after removing stopwords\nprint(df['clean'][0][:20])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Obtain the total words present in the dataset\nlist_of_words = []\nfor i in df.clean:\n    for j in i:\n        list_of_words.append(j)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(list_of_words)\nprint(len(list_of_words))\n# Obtain the total number of unique words\ntotal_words = len(list(set(list_of_words)))\ntotal_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# join the words into a string\ndf['clean_joined'] = df['clean'].apply(lambda x: \" \".join(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['clean_joined'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['clean'][2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['original'][2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5: VISUALIZE CLEANED UP DATASET"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the number of samples in 'subject'\nplt.figure(figsize = (8, 8))\nsns.countplot(y = \"subject\", data = df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the number of samples in 'subject'\nplt.figure(figsize = (8, 8))\nsns.countplot(y = \"isfake\", data = df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the word cloud for text that is Real\nplt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = stop_words).generate(\" \".join(df[df.isfake == 1].clean_joined))\nplt.imshow(wc, interpolation = 'bilinear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the word cloud for text that is Fake\nplt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = stop_words).generate(\" \".join(df[df.isfake == 0].clean_joined))\nplt.imshow(wc, interpolation = 'bilinear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# length of maximum document will be needed to create word embeddings \nmaxlen = -1\nfor doc in df.clean_joined:\n    tokens = nltk.word_tokenize(doc)\n    if(maxlen<len(tokens)):\n        maxlen = len(tokens)\nprint(\"The maximum number of words in any document is =\", maxlen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize the distribution of number of words in a text\nimport plotly.express as px\nfig = px.histogram(x = [len(nltk.word_tokenize(x)) for x in df.clean_joined], nbins = 100)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6: PREPARE THE DATA BY PERFORMING TOKENIZATION AND PADDING"},{"metadata":{},"cell_type":"markdown","source":"## TOKENIZER\n\nTokenizer allows us to vectorize text corpus by turning each text into a sequence of integers\n\n* **SENTENCE**: \n\n* \"budget fight looms republicans ...\"\n\n* **TOKENS**:\n\n* [3138, 3581, 2895, ...]"},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data into test and train \nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(df.clean_joined, df.isfake, test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import word_tokenize\n\n# Create a tokenizer to tokenize the words and create sequences of tokenized words\ntokenizer = Tokenizer(num_words = total_words)\ntokenizer.fit_on_texts(x_train)\ntrain_sequences = tokenizer.texts_to_sequences(x_train)\ntest_sequences = tokenizer.texts_to_sequences(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The encoding for document\\n\",df.clean_joined[0],\"\\n is : \",train_sequences[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add padding can either be maxlen = 4406 or smaller number maxlen = 40 seems to work well based on results\npadded_train = pad_sequences(train_sequences,maxlen = 40, padding = 'post', truncating = 'post')\npadded_test = pad_sequences(test_sequences,maxlen = 40, truncating = 'post') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,doc in enumerate(padded_train[:2]):\n     print(\"The padded encoding for document\",i+1,\" is : \",doc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7: UNDERSTAND THE THEORY AND INTUITION BEHIND RECURRENT NEURAL NETWORKS AND LSTM"},{"metadata":{},"cell_type":"markdown","source":"## RECURRENT NEURAL NETWORKS\n* Feedforward Neural Networks map a fixed size input (such as image) to a fixed size output (classes or probabilities)\n* A drawback in Feedforward Neural Networks is that they don't have any time dependency or memory effect\n* A RNN is a type of ANN that is designed to take temporal dimension into consideration having memory (internal state)"},{"metadata":{},"cell_type":"markdown","source":"* A RNN contains a temporal loop in which the hidden layer not only gives an output but feeds itself as well\n* AN extra dimension is added which is the time\n* RNN can recall what happened in the previous time stamp so it works great with sequence of text"},{"metadata":{},"cell_type":"markdown","source":"##  VANISHING GRADIENT PROBLEM\n\n* LSTM networks work much better compared to vanilla RNN since they overcome the vanishing gradient problem\n* The error has to propagate through all the previous layers resulting in a vanishing gradient\n* As the gradient goes smaller, the network weights are no longer updated\n* As more layers are added, the gradients of the loss function appraches zero, making the network hard to train"},{"metadata":{},"cell_type":"markdown","source":"## VANISHING GRADIENT PROBLEM\n\n* ANN gradients are calculated during **backpropagation**\n* In backpropagation, we calculate the derivatives of the network by moving from the outermost layer (close to output) back to the initial layers (close to inputs)\n* The chain rule is used during this calculation in which the derivatives from the final layer are multiplied by the derivatives from early layers\n* The gradients keep diminishing exponentially and therefore the weights and biases are longer being updated "},{"metadata":{},"cell_type":"markdown","source":"## GRADIENT DESCENT \n\n* Gradient Descent is an optimization algorithm used to obtain the optimized network **weight** and **bias** values\n* It works by iteratively trying to **minimize the cost function**\n* It works by calculating the gradient of the cost function and moving in the negative direction until the local/global minimum is achieved\n* If the positive of the gradient is taken, local/global maximum is achieved\n* The size of steps taken is called **LEARNING RATE**\n* If the learning rate increases, the area covered in the search space will increase so we might reach global minimum faster. However, we can overshoot the target"},{"metadata":{},"cell_type":"markdown","source":"## GRADIENT DESCENT WORKS AS FOLLOWS: \n\n1. Calculate the gradient (derivative) of the loss fuction\n2. Pick random values for weights (m,b) and substitute\n3. Calculate the step size (how much are we going to update the parameters?): \n     **step  size = learning rate * gradient**\n4. Update the parameters and repeat:\n     **new weight = old weight - step size**"},{"metadata":{},"cell_type":"markdown","source":"# 8: UNDERSTAND THE INTUITION BEHIND LONG SHORT TERM MEMORY (LSTM) NETWORKS"},{"metadata":{},"cell_type":"markdown","source":"## LSTM INTUITION\n\n* LSTM networks work better compared to vanilla RNN since they overcome vanishing gradient problem\n* In pratice, RNN fail to enstablish long term dependencies\n* LSTM networks are type of RNN that are designed to remember long term dependencies by default\n* LSTM can remember and recall information for a prolonged period of time  "},{"metadata":{},"cell_type":"markdown","source":"# 9: BUILD AND TRAIN THE MODEL"},{"metadata":{},"cell_type":"markdown","source":"## EMBEDDING LAYER\n\n* Embedding layers learn the low-dimensional continuous representation of input discrete variables\n* For example, let assume that we have 100,000 unique values in our data and want to train the model with this data. Even though we can train the model to generate accurate results, it would require more data to train\n* Alternatively, by introducing embedding layer, you can specify the number of low-dimensional features that you would need to represent the input data, in this take let take the value of 200\n* Now, what happens is the embedding layer learns the way to represent 100,000 variables with 200 variables only (think it as PCA or Autoencoder)\n* This helps subsequent layers to learn more effectively with less computer resource"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sequential Model\nmodel = Sequential()\n\n# embeddidng layer\nmodel.add(Embedding(total_words, output_dim = 128))\n# model.add(Embedding(total_words, output_dim = 240))\n\n\n# Bi-Directional RNN and LSTM\nmodel.add(Bidirectional(LSTM(128)))\n\n# Dense layers\nmodel.add(Dense(128, activation = 'relu'))\n#1 output because it's binary classification. The output will be 0 or 1\nmodel.add(Dense(1,activation= 'sigmoid'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert y_train into an array\ny_train = np.asarray(y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train the model\n\n* We started with entire training set\n* We divide training into 2 sets: 90% to train the model and 10% to perform cross-validation\n* We apply cross validation to make sure that the model is not overfitting the training data\n* If the error in the training data is going down and the error in the validation data is going down as well,  it's a good sign: the model is able to generalize\n* If the error in the training data is going down and the error in the validation data is going up, it means the model started to overfit the training data and we need to stop the training"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(padded_train, y_train, batch_size = 64, validation_split = 0.1, epochs = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make prediction\npred = model.predict(padded_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# if the predicted value is >0.5 it is real else it is fake\nprediction = []\nfor i in range(len(pred)):\n    if pred[i].item() > 0.5:\n        prediction.append(1)\n    else:\n        prediction.append(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting the accuracy\nfrom sklearn.metrics import accuracy_score\n\naccuracy = accuracy_score(list(y_test), prediction)\n\nprint(\"Model Accuracy : \", accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CONFUSION MATRIX\n\n* I want to visually represent what was the actual ground truth\n* We misclassified 14 samples as fake instead of true, 12 samples as true instead of fake"},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(list(y_test), prediction)\nplt.figure(figsize = (25, 25))\nsns.heatmap(cm, annot = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# category dict\ncategory = { 0: 'Fake News', 1 : \"Real News\"}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exercise"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sequential Model\nmodel = Sequential()\n\n# embeddidng layer\n#model.add(Embedding(total_words, output_dim =128))\nmodel.add(Embedding(total_words, output_dim = 240))\n\n\n# Bi-Directional RNN and LSTM\nmodel.add(Bidirectional(LSTM(128)))\n\n# Dense layers\nmodel.add(Dense(128, activation = 'relu'))\n#1 output because it's binary classification. The output will be 0 or 1\nmodel.add(Dense(1,activation= 'sigmoid'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\nmodel.summary()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}