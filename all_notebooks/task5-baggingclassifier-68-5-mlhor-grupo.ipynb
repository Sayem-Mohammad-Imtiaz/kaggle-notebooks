{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importação das bibliotecas","metadata":{}},{"cell_type":"markdown","source":"https://colab.research.google.com/drive/1tc6LiZJSn_YSzBMYuZePl8ataHzObtz-?usp=sharing","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.options.display.max_columns = None\npd.options.display.max_rows = None\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import RandomizedSearchCV, cross_val_score, train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.decomposition import PCA\n\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier, BaggingClassifier, RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importação dos dados","metadata":{}},{"cell_type":"code","source":"# tweet\n\ntweets = pd.read_csv('Tweets/train_tweets.csv')\ntweets_media = pd.read_csv('Tweets/train_tweets_vectorized_media.csv')\ntweets_text = pd.read_csv('Tweets/train_tweets_vectorized_text.csv')\n\n# user\nuser = pd.read_csv('Users/users.csv')\n\n# teste\nteste_tweets = pd.read_csv('Tweets/test_tweets.csv')\nteste_tweets_media = pd.read_csv('Tweets/test_tweets_vectorized_media.csv')\nteste_tweets_text = pd.read_csv('Tweets/test_tweets_vectorized_text.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Análise exploratória","metadata":{}},{"cell_type":"markdown","source":"Vemos que algumas das variáveis que consideramos interessantes de analisar, todas mantém boas proporções com relação à viralização do tweet.","metadata":{}},{"cell_type":"code","source":"print(tweets[\"virality\"].value_counts(),\"\\n\")\n\nplt.figure(figsize=(10,5))\nsns.barplot(sorted(tweets[\"virality\"].unique()),tweets[\"virality\"].value_counts(), palette=\"deep\")\nplt.title(\"Número de Tweets por nível de viralização\")\nplt.xlabel(\"Viralização\")\nplt.ylabel(\"Frequencia\");","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tweets[\"tweet_attachment_class\"].value_counts(),\"\\n\")\n\nplt.figure(figsize=(10,5))\nsns.barplot(sorted(tweets[\"tweet_attachment_class\"].unique()),tweets[\"tweet_attachment_class\"].value_counts(), palette = \"deep\")\nplt.title(\"Número de Tweets por Attachment Class\")\nplt.xlabel(\"tweet_attachment_class\")\nplt.ylabel(\"Frequencia\");","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.FacetGrid(tweets, col=\"virality\", height=4, aspect=1, col_wrap=3, palette = \"deep\")\ng.map(sns.histplot, \"tweet_attachment_class\");","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tweets[\"tweet_mention_count\"].value_counts(),\"\\n\")\n\nplt.figure(figsize=(10,5))\nsns.barplot(sorted(tweets[\"tweet_mention_count\"].unique()),tweets[\"tweet_mention_count\"].value_counts(), palette = \"deep\")\nplt.title(\"Número de Tweets por Menções realizadas\")\nplt.xlabel(\"tweet_mention_count\")\nplt.ylabel(\"Frequencia\");","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.FacetGrid(tweets, col=\"virality\", height=4, aspect=1, col_wrap=3, palette = \"deep\")\ng.map(sns.histplot, \"tweet_mention_count\");","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tweets[\"tweet_url_count\"].value_counts(),\"\\n\")\n\nplt.figure(figsize=(10,5))\nsns.barplot(sorted(tweets[\"tweet_url_count\"].unique()), tweets.groupby(by=[\"tweet_url_count\"])[\"virality\"].describe()[\"count\"], palette = \"deep\")\nplt.title(\"Número de Tweets por URL's no texto\")\nplt.xlabel(\"tweet_url_count\")\nplt.ylabel(\"Frequencia\");","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.FacetGrid(tweets, col=\"virality\", height=4, aspect=1, col_wrap=3, palette = \"deep\")\ng.map(sns.histplot, \"tweet_url_count\");","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tweets[\"tweet_created_at_year\"].value_counts(),\"\\n\")\n\nplt.figure(figsize=(10,5))\nsns.barplot(sorted(tweets[\"tweet_created_at_year\"].unique()), tweets.groupby(by=[\"tweet_created_at_year\"])[\"virality\"].describe()[\"count\"], palette = \"deep\")\nplt.title(\"Número de Tweets por Ano\")\nplt.xlabel(\"Ano\")\nplt.ylabel(\"Frequencia\");","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.FacetGrid(tweets, col=\"virality\", height=4, aspect=1, col_wrap=3, palette = \"deep\")\ng.map(sns.histplot, \"tweet_created_at_year\");","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tweets[\"tweet_hashtag_count\"].value_counts(),\"\\n\")\n\nplt.figure(figsize=(10,5))\nsns.barplot(sorted(tweets[\"tweet_hashtag_count\"].unique()), tweets.groupby(by=[\"tweet_hashtag_count\"])[\"virality\"].describe()[\"count\"], palette = \"deep\")\nplt.title(\"Número de Tweets por Quantidade de hashtag's\")\nplt.xlabel(\"Ano\")\nplt.ylabel(\"Frequencia\");","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.FacetGrid(tweets, col=\"virality\", height=4, aspect=1, col_wrap=3, palette = \"deep\")\ng.map(sns.histplot, \"tweet_hashtag_count\");","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = tweets.corr(\"pearson\")\n\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(15,10))\n\n# Generate a custom diverging colormap\ncmap = sns.color_palette(\"viridis\")\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5});","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Junção dos Dados","metadata":{}},{"cell_type":"markdown","source":"Tratamento das imagens. Iremos usar a informação da quantidade de imagens por tweet ao invés de tratar seu conteúdo.","metadata":{}},{"cell_type":"code","source":"tweets_media[tweets_media['tweet_id']==205]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No caso acima percebe-se que o tweet 205 possui 3 imagens, logo, é só agrupa-las e realizar este procedimento para todos os tweet_id's.","metadata":{}},{"cell_type":"code","source":"# Treino\n\nn_images = tweets_media.groupby(\"tweet_id\").size().reset_index()\nn_images.columns = [\"tweet_id\", \"num_images\"]\n\n# Adicionando zeros aos tweet_ids que não possuem imagem\nfor tweet in tweets.tweet_id.unique(): \n    if tweet not in n_images.tweet_id.values:\n        df = pd.DataFrame({\"tweet_id\":tweet, \"num_images\":0}, index = [\"0\"])\n        n_images = pd.concat([n_images, df], axis = 0, ignore_index = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Teste\n\nn_images_test = teste_tweets_media.groupby(\"tweet_id\").size().reset_index()\nn_images_test.columns = [\"tweet_id\", \"num_images\"]\n\n# Adicionando zeros aos tweet_ids que não possuem imagem\nfor tweet in teste_tweets.tweet_id.unique(): \n    if tweet not in n_images_test.tweet_id.values:\n        df = pd.DataFrame({\"tweet_id\":tweet, \"num_images\":0}, index = [\"0\"])\n        n_images_test = pd.concat([n_images_test, df], axis = 0, ignore_index = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = n_images.groupby(by=[\"num_images\"])[\"tweet_id\"].describe()\ndf","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\n\nsns.barplot(df.index, df[\"count\"], palette = \"deep\");\nplt.title(\"Número de Tweets por Quantidade de imagens\")\nplt.xlabel(\"Número de Imagens\")\nplt.ylabel(\"Frequencia\");","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Treino\n## tweet\ntrain = tweets.merge(n_images, on=\"tweet_id\", how = 'left')\ntrain = train.merge(tweets_text, on=\"tweet_id\", how = 'left')\n## user\ntrain = train.merge(user,left_on='tweet_user_id',right_on='user_id', how='left')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.FacetGrid(train, col=\"virality\", height=4, aspect=1, col_wrap=3, palette = \"deep\")\ng.map(sns.histplot, \"num_images\");","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Teste\n## tweet\ntest = teste_tweets.merge(n_images_test, on=\"tweet_id\", how = 'left')\ntest = test.merge(teste_tweets_text, on=\"tweet_id\", how = 'left')\n## user\ntest = test.merge(user,left_on='tweet_user_id',right_on='user_id', how='left')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.FacetGrid(test, col=\"virality\", height=4, aspect=1, col_wrap=3, palette = \"deep\")\ng.map(sns.histplot, \"num_images\");","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dando uma olhada geral nos nossos dados:","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Já que usaremos modelos e métodos que necessitam que os dados sejam númericos, iremos olhar mais profundamente as colunas não numéricas e tratá-las","metadata":{}},{"cell_type":"code","source":"train.select_dtypes(include='bool').head(5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As colunas que tem dados booleanos, basta transformar para 0 ou 1:","metadata":{}},{"cell_type":"code","source":"train[\"tweet_has_attachment\"] = train[\"tweet_has_attachment\"].astype(int)\ntrain[\"user_has_location\"] = train[\"user_has_location\"].astype(int)\ntrain[\"user_has_url\"] = train[\"user_has_url\"].astype(int)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[\"tweet_has_attachment\"] = test[\"tweet_has_attachment\"].astype(int)\ntest[\"user_has_location\"] = test[\"user_has_location\"].astype(int)\ntest[\"user_has_url\"] = test[\"user_has_url\"].astype(int)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Colunas Objeto:","metadata":{}},{"cell_type":"code","source":"train.select_dtypes(include='object').head(7)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Para a variável tweet_attachment_class, testamos o label encoder e o one hot encoding. Este ultimo melhorou o desempenho dos modelos no geral.","metadata":{}},{"cell_type":"code","source":"df = pd.get_dummies(train['tweet_attachment_class'])\ntrain = pd.concat([train, df], axis = 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.get_dummies(test['tweet_attachment_class'])\ntest = pd.concat([test, df], axis = 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No caso do \"tweet_topic_ids\", após alguns testes percebemos que o modelo fica melhor se retirarmos essa variável","metadata":{}},{"cell_type":"code","source":"# Após os tratamentos temos\n\nprint(f\"Número de colunas no conjunto de treino: {len(train.columns)}\")\nprint(f\"Número de colunas no conjunto de teste: {len(test.columns)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelagem","metadata":{}},{"cell_type":"code","source":"X = train.drop([\"virality\",\"tweet_id\", \"tweet_user_id\", \"tweet_topic_ids\", 'tweet_attachment_class'], axis=1)\ny = train[\"virality\"]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test.drop([\"tweet_id\", \"tweet_user_id\", \"tweet_topic_ids\", 'tweet_attachment_class'], axis = 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PCA 1\npca1 = PCA(n_components=50)\npca1.fit(X)\nX1 = pca1.transform(X)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PCA 2\npca2 = PCA(n_components=75)\npca2.fit(X)\nX2 = pca2.transform(X)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PCA 3\npca3 = PCA(n_components=100)\npca3.fit(X)\nX3 = pca3.transform(X)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PCA 4\npca4 = PCA(n_components=125)\npca4.fit(X)\nX4 = pca4.transform(X)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Testamos com as 4 quantidades de componentes e a que se saiu melhor foi o PCA com 75 componentes","metadata":{}},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X2, y, test_size = 0.25, random_state=37)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc = RandomForestClassifier(random_state = 37, max_depth=10, bootstrap=False)\nrfc.fit(X_train,y_train)\ny_pred = rfc.predict(X_val)\n\nprint(accuracy_score(y_val, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bag = BaggingClassifier(random_state=37, n_estimators= 100, max_samples= 14812, max_features= 37, bootstrap_features= True, bootstrap= True, n_jobs = -1,\n                       base_estimator = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='manhattan', \n                                                             metric_params=None, n_jobs=-1, n_neighbors=5, p=2, weights='uniform'))\nbag.fit(X_train,y_train)\ny_pred = bag.predict(X_val)\n\nprint(accuracy_score(y_val, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reg = xgb.XGBClassifier()\nreg.fit(X_train,y_train)\ny_pred = reg.predict(X_val)\n\nprint(accuracy_score(y_val, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"etr = ExtraTreesClassifier(random_state= 37, max_features=None, max_depth=13, criterion= 'gini', n_estimators= 200)\netr.fit(X_train, y_train)\ny_pred = etr.predict(X_val)\n\nprint(accuracy_score(y_val, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat = CatBoostClassifier()\ncat.fit(X_train, y_train)\ny_pred = cat.predict(X_val)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(accuracy_score(y_val, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"O melhor modelo foi o BaggingClassifier","metadata":{}},{"cell_type":"markdown","source":"Vamos tunar o modelo para obter o melhor desempenho","metadata":{}},{"cell_type":"code","source":"n_samples = X2.shape[0]\nn_features = X2.shape[1]\n\nbag = BaggingClassifier(random_state=37, n_jobs = -1)\n\nparams = {'base_estimator': [None, LogisticRegression(random_state=37, n_jobs = -1), KNeighborsClassifier(n_jobs = -1)],\n          'n_estimators': [20,50,100],\n          'max_samples': [0.5, 1.0, n_samples//2, ],\n          'max_features': [0.5, 1.0, n_features//2, ],\n          'bootstrap': [True, False],\n          'bootstrap_features': [True, False]}\n\nbagging_classifier_grid = RandomizedSearchCV(bag, param_distributions = params, cv = 3, n_jobs = -1, verbose = 1, random_state=37, n_iter=25)\nbagging_classifier_grid.fit(X_train, y_train)\n\nprint('Train Accuracy : %.3f'%bagging_classifier_grid.best_estimator_.score(X_train, y_train))\nprint('Test Accuracy : %.3f'%bagging_classifier_grid.best_estimator_.score(X_val, y_val))\nprint('Best Accuracy Through Grid Search : %.3f'%bagging_classifier_grid.best_score_)\nprint('Best Parameters : ',bagging_classifier_grid.best_params_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = BaggingClassifier(random_state=37, n_estimators= 50, max_samples= 14812, max_features = 0.5, bootstrap_features= True, bootstrap= True, n_jobs=-1,\n                       base_estimator = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='manhattan', \n                                                             metric_params=None, n_jobs=-1, n_neighbors=5, p=2, weights='uniform'))\n\nmedia = round(np.mean(cross_val_score(model, X2, y, scoring='accuracy', cv=StratifiedKFold(n_splits=5, random_state=37))),3)\nerro = round(np.std(cross_val_score(model, X2, y, scoring='accuracy', cv=StratifiedKFold(n_splits=5, random_state=37))),3)\nprint('Bagging :  '+str(media)+' +- '+str(erro))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Criação do arquivo de submissão","metadata":{}},{"cell_type":"code","source":"X_test = pca2.transform(test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X2, y)\ny_pred = model.predict(X_test)\nsubmission = pd.DataFrame(data = {\"tweet_id\": test[\"tweet_id\"], \n                                  \"virality\": y_pred})\nsubmission.to_csv(\"solution_format.csv\",index=False)","metadata":{},"execution_count":null,"outputs":[]}]}