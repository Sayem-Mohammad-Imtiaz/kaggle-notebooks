{"cells":[{"metadata":{"_uuid":"66e75b3af6a25a64a109c6e2d7f75fe027d9e8b0"},"cell_type":"markdown","source":"This notebook implements the **Principal Component Ananlyis(PCA)** as explained in the book \"**Python Machine Learning**\" by **Sebastian Raschka** and **Vahid Mirjalili**.\n\n**Prerequisites**:\n\n* Python\n* pandas\n* numpy\n\n**Dataset**: Wine\n\n**Note:** Descriptive comments explain the code in a better way"},{"metadata":{"_uuid":"bc89f53639d231d452a696114b922115741a4ec5"},"cell_type":"markdown","source":"Import necessary packages:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nfrom matplotlib.colors import ListedColormap","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"Load the Wine dataset in a panda dataframe:"},{"metadata":{"trusted":true,"_uuid":"00d039ba132431c1ac56b05192b3ef9ce53acdcc"},"cell_type":"code","source":"df_wine = pd.read_csv('../input/Wine.csv');\ndf_wine.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89ba6587e724f37a007a40dd205597d2065a0504"},"cell_type":"markdown","source":"Add headers in the data:"},{"metadata":{"trusted":true,"_uuid":"d0192010d5abe0a320f4fb93c286f235224b3dcb"},"cell_type":"code","source":"df_wine.columns = [  'name'\n                 ,'alcohol'\n             \t,'malicAcid'\n             \t,'ash'\n            \t,'ashalcalinity'\n             \t,'magnesium'\n            \t,'totalPhenols'\n             \t,'flavanoids'\n             \t,'nonFlavanoidPhenols'\n             \t,'proanthocyanins'\n            \t,'colorIntensity'\n             \t,'hue'\n             \t,'od280_od315'\n             \t,'proline'\n                ]\ndf_wine.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1f4c3f1845bff799fce7fe3053ffed921eeb809"},"cell_type":"markdown","source":"Step 1 : Preprocess the data into train and test sets with 70%:30% ratio respectively and standardize the data as is a requirement for PCA to assign equal importance to each feature beforehand"},{"metadata":{"trusted":true,"_uuid":"2615f64d044e89863caa3590bae7bf19e8aefd3f"},"cell_type":"code","source":"#make train-test sets\nfrom sklearn.model_selection import train_test_split;\nX, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values;\n#print(np.unique(y))\n#split with stratify on y for equal proportion of classes in train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, stratify = y,random_state = 0);\n\n#standardize the features with same model on train and test sets\nfrom sklearn.preprocessing import StandardScaler;\nsc = StandardScaler();\nX_train_std = sc.fit_transform(X_train);\nX_test_sd = sc.transform(X_test);\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2e61995eaa4fa9c05176e64765016628376a812"},"cell_type":"markdown","source":"Step 2: Make a covariance matrix of the training data and extract eigenvectors and eigenvalues"},{"metadata":{"trusted":true,"_uuid":"3a669958375c99ae8eb10431d0fa48286ee19876"},"cell_type":"code","source":"cov_mat = np.cov(X_train_std.T);\neigen_vals, eigen_vecs = np.linalg.eig(cov_mat);\nprint('\\nEigenvalues \\n%s' % eigen_vals)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a5a0303e11a6caea0a0774600127a97c6190f6c9"},"cell_type":"markdown","source":"Each principal component i.e. eigenvector indicate the variance and we have to select the top *k* eigenvectors based upon their magnitude i.e . eigenvalues. Lets plot the variance explained ratios of the eigenvalues for the above dataset:"},{"metadata":{"trusted":true,"_uuid":"bdc36671d3f53cd943c74891bf4d62a285bcaf6e"},"cell_type":"code","source":"tot = sum(eigen_vals);\nvar_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)];\ncum_var_exp = np.cumsum(var_exp);\nplt.bar(range(1,14), var_exp, alpha=0.5, align='center',label='individual explained variance')\nplt.step(range(1,14), cum_var_exp, where='mid',label='cumulative explained variance')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal component index')\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7db98965d99f673f9cc855e16b953ea540636cd"},"cell_type":"markdown","source":"The above plot clearly shows that the first principal component explains almost 40% of the variance in the data and the first and second combined explain 60%."},{"metadata":{"_uuid":"917fe458a4f99e2d6d0187d05b8bbbb19d86e970"},"cell_type":"markdown","source":"Step 3: Select the top *k* eigenvectors and compose the projection matrix *W* using these vectors"},{"metadata":{"trusted":true,"_uuid":"7190f2be1b0bc6029654553e06afbd0cbfe5e220"},"cell_type":"code","source":"eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i]) for i in range(len(eigen_vals))]\n#Sort the (eigenvalue, eigenvector) tuples from high to low\neigen_pairs.sort(key=lambda k: k[0], reverse=True)\n#chossing k = 2 for better representation via 2-dimensional scatter plot.\nw = np.hstack((eigen_pairs[0][1][:, np.newaxis],eigen_pairs[1][1][:, np.newaxis]))\nprint('Matrix W:\\n', w)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3a18a71fcb8499dd2e136ac4af2882bc7e36684"},"cell_type":"markdown","source":"Step 4: Transform original data(13-dimensional) to new dimensioanl sub-space(2-dimensional) using the projection matrix consisiting of two principal components"},{"metadata":{"trusted":true,"_uuid":"bb065a6cf3a821025e6062dc05de7a67417f50cf"},"cell_type":"code","source":"X_train_pca = X_train_std.dot(w)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ffeb2f369453b1fa3f71a76b174cae9a417fd35"},"cell_type":"markdown","source":"Lets visualize the new 2-dimensional dataset via a scatter plot:"},{"metadata":{"trusted":true,"_uuid":"cae9ab613fcff742cee044f3b6334d760265b1e1"},"cell_type":"code","source":"colors = ['r', 'b', 'g']\nmarkers = ['s', 'x', 'o']\nfor l, c, m in zip(np.unique(y_train), colors, markers):plt.scatter(X_train_pca[y_train==l, 0],X_train_pca[y_train==l, 1],c=c, label=l, marker=m)\nplt.xlabel('PC 1')\nplt.ylabel('PC 2')\nplt.legend(loc='lower left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c8d221d05ac4dedf35426b11cf989706aa5ccbb"},"cell_type":"markdown","source":"The above plot clearly show that the data is more spread along PC1 as had been seen in the variance ratio plot. A linear classifier could very well seperate the classes in the new feature space."},{"metadata":{"trusted":true,"_uuid":"748aaf08cbad2318e1647a0f9a76179526c8a43b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}