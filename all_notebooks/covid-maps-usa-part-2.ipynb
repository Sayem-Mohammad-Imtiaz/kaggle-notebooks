{"cells":[{"metadata":{},"cell_type":"markdown","source":"[Link to Covid Maps - USA Part 1](https://www.kaggle.com/blakkmagic/covid-maps-usa-part-1)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n\nimport geopandas as gpd\nfrom shapely.geometry import LineString\nfrom geopandas.tools import geocode\nimport folium\nfrom folium import Choropleth, Circle, Marker\nfrom folium.plugins import HeatMap, FastMarkerCluster\nfrom folium import plugins\nimport math\nimport webbrowser\nfrom IPython.display import HTML\nimport matplotlib.pyplot as plt\nfrom pandasql import sqldf\nimport plotly.express as px\n\n#turn off settingwithcopywarning off\npd.options.mode.chained_assignment = None","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Step 4 - Geocoding with an external file continued...**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"So far the maps we have created have been based on the total number of cases or deaths. It could also be interesting to map the cases or deaths over time to visualise the spread of Covid-19 in the US. So this time instead of taking the max date, we will use numerous dates. Ideally, I would like to use all dates but notebook will crash when committed so we will restrict the dataset to just the first 80 days. Day 80 is roughly the 10th of April.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#Have a look at initial dataset\nUS_covid_data = pd.read_csv(\"../input/us-counties-covid-19-dataset/us-counties.csv\")\n#Restrict dates to be before 11th April\nUS_covid_data = US_covid_data.loc[US_covid_data['date']<'2020-04-11']\n#Create a concatenated column - not actually important since we'll map with fips column\nUS_covid_data['concat'] = US_covid_data['county']+str(', ')+US_covid_data['state']\n\nUS_covid_data = US_covid_data.sort_values(['date'], ascending = True)\nUS_covid_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"US_covid_data['concat'] = US_covid_data['county']+str(', ')+US_covid_data['state']\nUS_covid_data = US_covid_data.sort_values(['date'], ascending = True)\nUS_covid_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this point we need to account for New York City by inserting fips = 36061. We will do it in a slightly different way to last time by using the fillna() function to account for the multiple rows of NY City data. If you were to cut the data to only show New York City you will see that the only null values exist in the 'fips' column. Therefore, simple use of the fillna function to replace all nulls with 36061 will be required. From the code below we can see that originally the 'fips' column contains only nulls and after manipulation it only contains the value 36061","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(\"Values in fips column prior to manipulation: \\n\" +str(US_covid_data.loc[US_covid_data['county'] == 'New York City'].fips.value_counts())+\"\\n\")\nUS_covid_data.loc[US_covid_data['county'] == 'New York City'] = US_covid_data.loc[US_covid_data['county'] == 'New York City'].fillna(36061.0)\nprint(\"Values in fips column after manipulation: \\n\" +str(US_covid_data.loc[US_covid_data['county'] == 'New York City'].fips.value_counts()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following steps are used create the 'time' aspect this heat map. Will be looking at time in terms of days e.g. day 1,2,3,... as opposed to looking at it from an actual date perspective.\n\nWill be using SQL type functions to return and order distinct dates from the US_covid_data dataframe I created so that the represent day 1,2,3 etc. Final step is to join the 'day' column back to the US_covid_data dataframe as this will soon be joined with the GeoDataFrame and then be mapped.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"q1='select DISTINCT date FROM US_covid_data'\ndf_new=sqldf(q1)\ndf_new['day'] = np.arange(len(df_new))\ndf_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"concat_result = US_covid_data.merge(df_new,on='date', how = 'left')\nconcat_result.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it's time to prepare the GeoDataFrame. Could work with what we used earlier to create the static heat maps however I will re-prepare it here so it is easy to follow along","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"us_counties_shapefile = gpd.read_file(\"../input/us-counties-geocoded/tl_2017_us_county.shp\")\nus_counties_shapefile.head()\nus_counties_dataframe = pd.DataFrame(us_counties_shapefile[['GEOID', 'INTPTLAT', 'INTPTLON']])\nus_counties_dataframe['GEOID'] = us_counties_dataframe['GEOID'].astype('float64')\n\nconcat_result2 = concat_result.merge(us_counties_dataframe,left_on = 'fips', right_on = 'GEOID', how = 'left')\n\nconcat_result2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it is time to account for Kansas City. This will mean inserting the latitude and longitude for Kansas City of 39.0997 and -94.5786 into the 'INTPTLAT' and 'INTPTLON' columns.\n\nShould also note that we will fill in the 'fips' and 'GEOID' columns with arbitrary numbers so that these rows aren't dropped when we use the dropna() function later on.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Fill in 'fips' column with arbitrary number = 1\nconcat_result2.loc[concat_result2['county'] == 'Kansas City','fips'] = concat_result2.loc[concat_result2['county'] == 'Kansas City','fips'].fillna(1)\n\n#Fill in 'GEOID' column with arbitrary number = 1\nconcat_result2.loc[concat_result2['county'] == 'Kansas City','GEOID'] = concat_result2.loc[concat_result2['county'] == 'Kansas City','GEOID'].fillna(1)\n\n#Fill in 'INTPTLAT' column with +39.09970\nconcat_result2.loc[concat_result2['county'] == 'Kansas City','INTPTLAT'] = concat_result2.loc[concat_result2['county'] == 'Kansas City','INTPTLAT'].fillna('+39.0997000')\n\n#Fill in 'INTPTLON' column with -94.57860\nconcat_result2.loc[concat_result2['county'] == 'Kansas City','INTPTLON'] = concat_result2.loc[concat_result2['county'] == 'Kansas City','INTPTLON'].fillna('-94.5786000')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After dealing with NY City, Kansas City and the time aspect it is now time do clean the data up a bit more before mapping. This includes dropping rows where the county is 'unknown' which is represented by null values in the 'GEOID' and 'fips' column.\n\nWe will also duplicate rows based on the 'cases' column. Given there is a time aspect involved here, the resulting dataset would be over 10 million rows if I did not restrict the dataset to end at the 10th of April. The map will take a long time to produce even after restricting and it is a bit clunky. ","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"concat_result2['INTPTLAT'] = concat_result2['INTPTLAT'].astype('str')\nconcat_result2['INTPTLAT']\nconcat_result2['INTPTLAT'] = concat_result2['INTPTLAT'].str[1:]\nconcat_result2['INTPTLAT']\n\nconcat_result2.dropna(how = 'any', inplace = True)\n\nconcat_result_cases = concat_result2.loc[concat_result2.index.repeat(concat_result2['cases'])]\nprint(\"Shape of dataset to be mapped: \" +str(concat_result_cases.shape))\n\n\nconcat_result_cases['INTPTLAT'] = concat_result_cases['INTPTLAT'].astype('float64')\nconcat_result_cases['INTPTLON'] = concat_result_cases['INTPTLON'].astype('float64')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Heat Map with Time - Cases**\n**For reasons I don't understand the play, forward, backward and loop buttons are missing their logos but they do work on the bottom left. Fps also seems to be 1 whereas I have set min fps to 4. There is no slider to change the fps either**","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"max_date = concat_result_cases['day'].max()\n\nheat_data = [[[row['INTPTLAT'],row['INTPTLON']] \n              for index, row in concat_result_cases[concat_result_cases['day'] == i].iterrows()] for i in range(0,max_date)]\n\nmap4 = folium.Map(location=[40, -95], zoom_start=4)\n\nhm = plugins.HeatMapWithTime(heat_data,auto_play=True,max_opacity=0.8, min_speed = 4, overlay=False,radius = 16.5, display_index=True)\nhm.add_to(map4)\n\nmap4.save('plot_data4.html')   \nHTML('<iframe src=plot_data4.html width=800 height=600></iframe>')\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Working with heat maps certaintly carries visual appeal. We will now move on. In [part 3](https://www.kaggle.com/blakkmagic/covid-maps-usa-part-3) of this analysis we will use Choropleth maps to visualise the Covid-19 dataset","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}