{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 1: Method-ML Algorithams","metadata":{}},{"cell_type":"code","source":"#ML Librarires \nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score, confusion_matrix, precision_recall_curve, roc_curve\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb\n\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.over_sampling import SMOTE\n\nimport seaborn as sns\nimport missingno as msno\nimport plotly.express as px\nimport  matplotlib.pyplot as plt\n\n\nplt.style.use('seaborn')\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Read, Data Visualization,EDA Analysis,Data Pre-Processing,Data Splitting","metadata":{}},{"cell_type":"code","source":"#Data Read\ndf=pd.read_csv('../input/parkinsons-disease-classification/pd_speech_features.csv',index_col=0, delimiter=',', skiprows=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.loc[:,~df.columns.duplicated()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=df.sample(frac=1).reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.apply(lambda x: sum(x.isnull()),axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plotting data \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport plotly.express as px","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking the target variable countplot\nsns.countplot(data=df,x = 'class',palette='plasma')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Imbalanced data distribution for target class.","metadata":{}},{"cell_type":"code","source":"sns.set(rc={'figure.figsize':(10,8)})\nfig = sns.countplot(x = \"class\" , data = df)\nplt.xlabel(\"class\")\nplt.ylabel(\"Count\")\nplt.title(\"Class Count\")\nplt.grid(True)\nplt.show(fig)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Box Plotting All features distribution corresponding Target column\ni=1\nplt.figure(figsize=(40,40))\nfor c in df.columns[:49]:\n    plt.subplot(10,5,i)\n    plt.title(f\"Boxplot of {c}\",fontsize=16)\n    plt.yticks(fontsize=12)\n    plt.xticks(fontsize=12)\n    sns.boxplot(y=df[c],x=df['class'])\n    i+=1\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking the target variable countplot\n\nplt.figure(figsize=(25,15))\nsns.set_style('white')\nsns.countplot(x='class', data = df, palette='GnBu')\nsns.despine(left=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"N.B. = I prefer to use models without outlier & imbalanced treatment, in many cases it can improve the model performance. But it also leads to change of information which might alter real/practical situations","metadata":{}},{"cell_type":"markdown","source":"Data Splitting","metadata":{}},{"cell_type":"code","source":"dataX=df.drop('class',axis=1)\ndataY=df['class']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(dataX,dataY,test_size=0.15,random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('X_train',X_train.shape)\nprint('X_test',X_test.shape)\nprint('y_train',y_train.shape)\nprint('y_test',y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dims = X_train.shape[1]\nprint(dims, 'dims')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dims = X_test.shape[1]\nprint(dims, 'dims')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_roc_(false_positive_rate,true_positive_rate,roc_auc):\n    plt.figure(figsize=(5,5))\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(false_positive_rate,true_positive_rate, color='red',label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],linestyle='--')\n    plt.axis('tight')\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors  import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV,cross_val_score\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.metrics import roc_curve, auc\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 1 for ML Algorithms","metadata":{}},{"cell_type":"markdown","source":"With PCA Analysis","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\npca=PCA(n_components=50)\nX_train=pca.fit_transform(X_train)\nX_test=pca.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('X_train',X_train.shape)\nprint('X_test',X_test.shape)\nprint('y_train',y_train.shape)\nprint('y_test',y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr=LogisticRegression(C=0.1,penalty='l2',random_state=42)\nlr.fit(X_train,y_train)\n\ny_pred=lr.predict(X_test)\n\n\ny_proba=lr.predict_proba(X_test)\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\nplot_roc_(false_positive_rate,true_positive_rate,roc_auc)\n\n\nfrom sklearn.metrics import r2_score,accuracy_score\n\n#print('Hata Oranı :',r2_score(y_test,y_pred))\nprint('Accurancy Oranı :',accuracy_score(y_test, y_pred))\nprint(\"Logistic TRAIN score with \",format(lr.score(X_train, y_train)))\nprint(\"Logistic TEST score with \",format(lr.score(X_test, y_test)))\nprint()\n\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)\nsns.heatmap(cm,annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn=KNeighborsClassifier(n_jobs=2, n_neighbors=22)\nknn.fit(X_train,y_train)\n\ny_pred=knn.predict(X_test)\n\ny_proba=knn.predict_proba(X_test)\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\nplot_roc_(false_positive_rate,true_positive_rate,roc_auc)\n\nfrom sklearn.metrics import r2_score,accuracy_score\n\nprint('Accurancy Oranı :',accuracy_score(y_test, y_pred))\nprint(\"KNN TRAIN score with \",format(knn.score(X_train, y_train)))\nprint(\"KNN TEST score with \",format(knn.score(X_test, y_test)))\nprint()\n\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)\nsns.heatmap(cm,annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Withou PCA Analysis & Using Machine Learning Algorithms; Part 2 for ML Algorithms","metadata":{}},{"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(dataX,dataY,test_size=0.15,random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitting Logistic Regression To the training set \nfrom sklearn.linear_model import LogisticRegression   \n  \nclassifier = LogisticRegression(penalty='l2',solver='lbfgs',class_weight='balanced', max_iter=1000,random_state = 42) \nclassifier.fit(X_train, y_train)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = classifier.predict(X_test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# making confusion matrix between \n#  test set of Y and predicted value. \nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred) \nprint (cm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report,accuracy_score\nprint(classification_report(y_test,y_pred))\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred)*100)\n\nprint(y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"names = [ \"MLP-Neural Net\", \"Naive Bayes\", \"QDA\"]\n\nclassifiers = [\n    MLPClassifier(),\n    GaussianNB(),\n    QuadraticDiscriminantAnalysis()\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\n# iterate over classifiers\nresults = {}\nfor name, clf in zip(names, classifiers):\n    scores = cross_val_score(clf, X_train, y_train, cv=5)\n    results[name] = scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name, scores in results.items():\n    print(\"%20s | Accuracy: %0.2f%% (+/- %0.2f%%)\" % (name, 100*scores.mean(), 100*scores.std() * 2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nclf = SVC(kernel=\"linear\")\n\n# prepare a range of values to test\nparam_grid = [\n  {'C': [.01, .1, 1, 10], 'kernel': ['linear']},\n ]\n\ngrid = GridSearchCV(estimator=clf, param_grid=param_grid)\ngrid.fit(X_train, y_train)\nprint(grid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summarize the results of the grid search\nprint(\"Best score: %0.2f%%\" % (100*grid.best_score_))\nprint(\"Best estimator for parameter C: %f\" % (grid.best_estimator_.C))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 3 for Algorithms","metadata":{}},{"cell_type":"code","source":"seed = 42\n\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.model_selection import RandomizedSearchCV, cross_val_score, StratifiedKFold\nfrom sklearn.metrics import classification_report, roc_auc_score, roc_curve\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier,\\\n                            BaggingClassifier,VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline, Pipeline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split the data into train and test\ndef split_data(X, Y, seed=42, train_size=0.8):\n    xtrain, xtest, ytrain, ytest = train_test_split(X, Y, train_size=train_size, random_state = seed, stratify=Y)\n    xtrain, xtest = preprocess(xtrain, xtest)\n    return (xtrain, xtest, ytrain, ytest)\n\n# preprocess the data for training\ndef preprocess(x1, x2=None):\n    sc = StandardScaler()\n    x1 = pd.DataFrame(sc.fit_transform(x1), columns=x1.columns)\n    if x2 is not None:\n        x2 = pd.DataFrame(sc.transform(x2), columns=x2.columns)\n        return (x1,x2)\n    return x1\n\n# for model evaluation and training\ndef eval_model(model, X, Y, seed=1):\n    xtrain, xtest, ytrain, ytest = split_data(X, Y)\n    model.fit(xtrain, ytrain)\n    \n    trainpred = model.predict(xtrain)\n    trainpred_prob = model.predict_proba(xtrain)\n    testpred = model.predict(xtest)\n    testpred_prob = model.predict_proba(xtest)\n    \n    print(\"Train ROC AUC : %.4f\"%roc_auc_score(ytrain, trainpred_prob, multi_class='ovr'))\n    print(\"\\nTrain classification report\\n\",classification_report(ytrain, trainpred))\n    \n    ### make a bar chart for displaying the wrong classification of one class coming in which other class\n    \n    print(\"\\nTest ROC AUC : %.4f\"%roc_auc_score(ytest, testpred_prob, multi_class='ovr'))\n    print(\"\\nTest classification report\\n\",classification_report(ytest, testpred))\n    \ndef plot_importance(columns, importance):\n    plt.bar(columns, importance)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Feature Extraction, Importance & Splitting\n\nY= df['class']\n\nX = df.drop(['class'],axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_sc = preprocess(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating array of models","metadata":{}},{"cell_type":"code","source":"model_logr = LogisticRegression(random_state=seed,n_jobs=-1)\nmodel_nb = GaussianNB()\nmodel_dt = DecisionTreeClassifier(random_state=seed)\nmodel_dt_bag = BaggingClassifier(model_dt, random_state=seed, n_jobs=-1)\nmodel_ada = AdaBoostClassifier(random_state=seed)\nmodel_gbc = GradientBoostingClassifier(random_state=seed)\nmodel_rf = RandomForestClassifier(random_state=seed, n_jobs=-1)\nmodel_xgb = XGBClassifier(random_state=seed)\nmodel_lgbm = LGBMClassifier(random_state=seed, n_jobs=-1)\nmodel_knn = KNeighborsClassifier(n_jobs=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = []\nmodels.append(('LR',model_logr))\nmodels.append(('NB',model_nb))\nmodels.append(('DT',model_dt))\nmodels.append(('Bag',model_dt_bag))\nmodels.append(('Ada',model_ada))\nmodels.append(('GBC',model_gbc))\nmodels.append(('RF',model_rf))\nmodels.append(('XGB',model_xgb))\nmodels.append(('LGBM',model_lgbm))\nmodels.append(('KNN',model_knn))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Running the algorithms","metadata":{}},{"cell_type":"code","source":"cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n\nresults = []\nnames = []\n\nfor name, model in models:\n    scores = cross_val_score(model, X_sc, Y, scoring='f1_weighted', cv=cv, n_jobs=-1)\n    accuracy = scores.mean()\n    std = scores.std()\n    print(f\"{name} : Mean ROC {accuracy} STD:({std})\")\n    results.append(scores)\n    names.append(name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,6))\nax.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" # Using Deep Neural Networks ; Part4","metadata":{}},{"cell_type":"code","source":"print(X_train.shape , y_train.shape)\nprint(X_test.shape , y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Keras\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nfrom keras.layers import Dense, Dropout , BatchNormalization\nfrom keras.utils import np_utils\nfrom keras.optimizers import RMSprop, Adam\n\n#tf \nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(64,input_dim=X_train.shape[1],activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\nmodel.add(Dense(128,activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\nmodel.add(Dense(256,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(512,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(1,activation='sigmoid'))\n\nmodel.compile(loss = 'binary_crossentropy',\n              optimizer = 'adam',\n              metrics = ['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ModelCheckpoint","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fBestModel = 'best_model.h5' \nearly_stop = EarlyStopping(monitor='val_loss', patience=2, verbose=1) \nbest_model = ModelCheckpoint(fBestModel, verbose=0, save_best_only=True)\n\nmodel.fit(X_train, y_train, validation_data = (X_test, y_test), epochs=100, \n          batch_size=62, verbose=True, callbacks=[best_model, early_stop])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = model.evaluate(X_test, y_test, verbose=1)\nprint('Accuracy: ', score[1]*100)\nprint( 'loss:', score[0]*100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction = model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction = (prediction > 0.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(metrics.classification_report(y_test, prediction))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Future Work & Suggestions : \n* If u  want to improve this work.You can focus on Class distribution ..\n* definetly it'll improve..Bcz class distribuion is imbalanced .\n* And do the hyperameter tuning for Deep Neural Networks","metadata":{}},{"cell_type":"markdown","source":"Research Projects Details; https://github.com/sohel-ccse?tab=projects","metadata":{}}]}