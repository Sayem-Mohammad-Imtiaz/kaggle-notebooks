{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Kaggle Heart Attack Prediction dataset","metadata":{}},{"cell_type":"markdown","source":"This data set has data from patients and looks to classify the patient as having low probability for having a heart attack (0) or having a high probability for having a heart attack (1).\n\nThe link to the data set is [Kaggle](https://www.kaggle.com/rashikrahmanpritom/heart-attack-analysis-prediction-dataset)","metadata":{}},{"cell_type":"markdown","source":"## Loading packages for analyzing and modeling data","metadata":{}},{"cell_type":"code","source":"# Packages to hold and pre-process data\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# Visualization packages\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Modeling packages\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader, TensorDataset\n\nimport sklearn\nfrom sklearn import svm\n\nimport itertools\n\n# Due to having and exploratory component when visualizing plenty of warnings come up, so ignore them for this notebook\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Analysis and Preprocessing","metadata":{}},{"cell_type":"code","source":"#Load data into DataFrames\nheart_data = pd.read_csv('../input/heart-attack-analysis-prediction-dataset/heart.csv')\no2_data = pd.read_csv('../input/heart-attack-analysis-prediction-dataset/o2Saturation.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heart_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"o2_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the first overview of the two files in the dataset, it can be seen that the hear.csv file containes 303 observations, while the o2Saturation.csv file containes 3585 observations. Given the difference in observations, and that there is no additional information the analysis for predicting values will be performed only with the heart.csv features. Since there are no columns with missing data there will be no need to perform any data imputation.","metadata":{}},{"cell_type":"code","source":"heart_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the column names and the information on the first 5 observations we can confirm the data set contains both continuos variables and categorical variables. For continuos variables the statistical properties could be of interest, for categorial data the count of sub-types can give some insights to the data. To validate which names of columns are categorial print out unique values, if the set is small, it is an indication of some ordinal or categorical data. ","metadata":{}},{"cell_type":"code","source":"heart_data.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create dictionary using column names and unique count, and from that pass it to a DataFrame\ndic_unique = dict(zip([i for i in heart_data.columns],[len(heart_data[i].unique()) for i in heart_data.columns]))\nprint(dic_unique)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the low count in unique entries the continuos, categorial, and target/predicted columns will be","metadata":{}},{"cell_type":"code","source":"cont_colms=[\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\ncateg_colms=[\"sex\",\"cp\",\"fbs\",\"restecg\",\"exng\",\"slp\",\"caa\",\"thall\"]\noutput_colms=[\"output\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of continuous variables is {}\".format(len(cont_colms)))\nprint(\"Number of categorial variables is {}\".format(len(categ_colms)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Continuos Variables","metadata":{}},{"cell_type":"code","source":"# Statistics for continuous variables\nheart_data[cont_colms].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Show distribution of continuous variables\nheart_data[cont_colms].hist(figsize=(15,15),bins=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the histogram plots it can be seen that the continuous variables have some outliers, as well as some underlying distribution of values. This suggests using box plots to better understand the data. Seaborn has boxenplots and violin plots, since we want to visualize outliers, we will use the boxenplot. To understand any effect with respect to the output the histogram or distribution plots will be separated according to the output. ","metadata":{}},{"cell_type":"code","source":"# Plot categorial count plots on 2 x 3 subplot grid\nfig, axs = plt.subplots(nrows=2,ncols=3,figsize=(20,20))\nfig.suptitle(\"Distribution of Continuous Data\")\nfor i in range(2):\n    for j in range(3):\n        if i==1 and j ==2:\n            continue\n        else:\n            sns.boxenplot(data=heart_data,x=cont_colms[3*i+j],ax=axs[i,j])\n            axs[i,j].xaxis.grid(True)\nfig.delaxes(axs[1,2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot categorial count plots on 2 x 3 subplot grid\n# Here displot and kdeplot give similar output, however the first is a figure level function and the second is an axis level function, \n# so the latter will be used to loop with the same construt as before\n# Bandiwth adjustment parameter was chosen to smooth out all underlying distributions\nfig, axs = plt.subplots(nrows=2,ncols=3,figsize=(20,20))\nfig.suptitle(\"KDE for continuos variables segmented by output\")\nfor i in range(2):\n    for j in range(3):\n        if i==1 and j ==2:\n            continue\n        else:\n            sns.kdeplot(data=heart_data,x=cont_colms[2*i+j],ax=axs[i,j],hue=\"output\",bw_adjust=0.75,fill=True)            \nfig.delaxes(axs[1,2])        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the plots it seems to indicate that for **age** and **thalachh** variables there is a difference in the mean and shape of the distribution. This could be a sampling issue, further investigations are needed to test differnce between the means. ","metadata":{}},{"cell_type":"markdown","source":"To avoid overfitting use Pearson correlation coefficient to understand relation between continuous variables","metadata":{}},{"cell_type":"code","source":"corr = heart_data[cont_colms].corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)]=True\nplt.figure(figsize=(10,10))\nsns.heatmap(corr,mask=mask,square=True,linewidths=.5,annot=True,fmt=\".3f\",vmin=-.5,vmax=.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Witht the Pearson correlation coefficient it seems there is a very strong negative correlation between **thalachh** and **age**, as well as between **thalachh** and **oldpeak**. However, since the number of observations is small we will keep the features and adjust the models accordingly. ","metadata":{}},{"cell_type":"markdown","source":"## Categorical Variables","metadata":{}},{"cell_type":"markdown","source":"From the count of unique entries it can be seen that the categorical data has between 2 and 5 unique entries depending on the column, so it would be interesting to do the count per each sub-type and visualize it","metadata":{}},{"cell_type":"code","source":"# Create dictionary with count of subtypes for each categorical columns\nunique_vals=[]\ncateg_counts=[]\nfor i, col_name in enumerate(heart_data[categ_colms]):\n    val_cat = list(heart_data[col_name].unique())\n    sub_count = [len(heart_data[heart_data[col_name]==cat_count]) for j, cat_count in enumerate(heart_data[col_name].unique())]\n    unique_vals.append(val_cat)\n    categ_counts.append(sub_count)\nprint(unique_vals)\nprint(categ_counts)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot categorial count plots on 3 x 3 subplot grid\nfig, axs = plt.subplots(nrows=3,ncols=3,figsize=(20,20))\nfig.suptitle(\"Count of Categorical Data\")\nfor i in range(3):\n    for j in range(3):\n        if i==2 and j ==2:\n            continue\n        else:\n            sns.countplot(data=heart_data,x=categ_colms[3*i+j],ax=axs[i,j])\nfig.delaxes(axs[2,2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating models to understand data","metadata":{}},{"cell_type":"markdown","source":"Since the goal is to perform binary classification based on the available features, this suggests using the algorithms:\n - SVM\n - Neural Networks\n\nThe data that will be used will be scaled to train the models. \n\nTo compare the models, the data will be split into training and testing data, and the metric to compare them will be the accuracy of the model on the validation set. \n","metadata":{}},{"cell_type":"code","source":"#Separate features and objective\nX = heart_data.drop([\"output\"],axis=1).copy()\nY = heart_data[\"output\"].copy()\n\n# Scaling of continuous variables using Robust Scales as the samples have outliers\nscaler = sklearn.preprocessing.RobustScaler()\nX[cont_colms]=scaler.fit_transform(X[cont_colms])\nX.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training and testing split\nx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.2,shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SVM","metadata":{}},{"cell_type":"markdown","source":"The SVM packge of Scikitlearn implements vector classification with the module SVC, which allos for regularization and use of different kernels. The hyper parameters of the models will be adjusted to maximize the accuracy on the validation set. ","metadata":{}},{"cell_type":"code","source":"# Implement vector classification\nclf = svm.SVC(C=10,kernel=\"poly\")\nclf.fit(x_train,y_train)\naccuracy_train = clf.score(x_train,y_train)\ny_pred = clf.predict(x_test)\naccuracy_test = sklearn.metrics.accuracy_score(y_test,y_pred)\nresults = dict({\"Training Accuracy\":[accuracy_train],\"Testing Accuracy\":[accuracy_test]})\nresult=pd.DataFrame.from_dict(results)\nresult.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform hyperparameter search, the search range for exponential range started -2 to 3, and iteratively shortened the range -1 to 1\nc_param = np.logspace(-1,1,30)\ngamma_param = np.logspace(-1,1,30)\ntraining_accuracy=[]\ntesting_accuracy=[]\nresults = pd.DataFrame({\"Regularization\":[],\"Gamma\":[],\"Training Accuracy\":[],\"Testing Accuracy\":[]})\nfor i, c in enumerate(c_param):\n    for j,gamma in enumerate(gamma_param):\n        clf = svm.SVC(C=c,gamma=gamma)\n        clf.fit(x_train,y_train)\n        y_pred = clf.predict(x_test)\n        training_accuracy.append(clf.score(x_train,y_train))\n        testing_accuracy.append(sklearn.metrics.accuracy_score(y_test,y_pred))\n        \n# Create cartesian product of parameters for DataFrame\nc_list = []\ng_list = []\nfor elem in itertools.product(c_param,gamma_param):\n    c_list.append(elem[0])\n    g_list.append(elem[1])\n\nresults = pd.DataFrame({\"Regularization\":c_list,\"Gamma\":g_list,\"Training Accuracy\":training_accuracy,\"Testing Accuracy\":testing_accuracy})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mloc = results[\"Testing Accuracy\"].argmax()\nresults.iloc[mloc]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With SVM the max accuracy on the validation set is **0.868852**","metadata":{}},{"cell_type":"markdown","source":"## Neural Network","metadata":{}},{"cell_type":"markdown","source":"For testing purposes we will define a neural netowkr with 2 hidden layers whose dimensions are to be defined, and the output is two components of a vector which will be transformed using a softmax function and from that define the classification","metadata":{}},{"cell_type":"code","source":"class NetClassifier(nn.Module):\n    def __init__(self,hidden1,hidden2):\n        super(NetClassifier,self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(13,hidden1),\n            nn.ReLU(),\n            nn.Linear(hidden1,hidden2),\n            nn.ReLU(),\n            nn.Linear(hidden2,2),\n        )\n    def forward(self,x):\n        x = self.model(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create an instance of the class\nnet = NetClassifier(20,20)\nprint(net)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define training parameters\nEPOCHS = 50\nBATCH_SIZE=32\nLEARNING_RATE = 0.01\n\n# Define optimizer and loss function\noptimizer = optim.Adam(net.parameters(),lr=LEARNING_RATE)\nloss_function = nn.CrossEntropyLoss()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define variables as tensors to train the model\nx_train_t = torch.tensor(x_train.values, dtype=torch.float)\nx_test_t = torch.tensor(x_test.values, dtype=torch.float)\ny_train_t = torch.tensor(y_train.values, dtype=torch.long)\ny_test_t = torch.tensor(y_test.values, dtype=torch.long)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create DataSet and DataLoaders to train the neural network\ntrain_data_set = TensorDataset(x_train_t,y_train_t) \ntest_data_set = TensorDataset(x_test_t,y_test_t)\n\ntrain_dataloader = DataLoader(train_data_set,batch_size = BATCH_SIZE)\ntest_dataloader = DataLoader(test_data_set,batch_size = BATCH_SIZE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save losses for each bath for all epochs for plotting learning curve\nlosses=[]\nfor epoch in range(EPOCHS):\n    epoch_loss=0\n    epoch_acc=0\n    for xb,yb in train_dataloader:\n        # Zero gradients for training\n        optimizer.zero_grad()\n        # Use current model parameters to predict output\n        y_pred = net(xb)\n        #y_pred = torch.flatten(y_pred)\n        # Turn probabilities into prediction \n        #pred = torch.round(torch.sigmoid(torch.flatten(y_pred)))\n        # Calculate loss, use float type to calculate loss\n        loss = loss_function(y_pred,yb)\n        losses.append(loss.item())\n        # Backpropagate\n        loss.backward()\n        # Step in the optimizer\n        optimizer.step()\n        epoch_loss+=loss.item()\n        epoch_acc+=(yb == torch.argmax(y_pred,dim=1)).float().mean()\n# Print epoch loss\n    #print(\"Epoch {:>02d} | Loss {:.5f} \".format(epoch,epoch_loss/len(train_dataloader)))\n    print(\"Epoch {:>02d} | Loss {:.5f} | Acc {:.3f}\".format(epoch,epoch_loss/len(train_dataloader),epoch_acc/len(train_dataloader)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# With trained nn predict the values for validation set and test accuracy\ny_val = net(x_test_t)\ny_val = torch.argmax(y_val,dim=1)\naccuracy = (y_val == y_test_t).float().mean()\nprint(\"Classification accuracy for nn is {}\".format(accuracy))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot learning curve\nplt.figure(figsize=(15,5))\nsns.lineplot(np.arange(1,len(losses)+1),losses)\nplt.title(\"Learning curve\")\nplt.ylabel(\"Batch Loss\")\nplt.xlabel(\"Batch iteration\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wrap the implemented class to perform search for hyper parameters that leads to best accuracy on validation set","metadata":{}},{"cell_type":"code","source":"# Define range of parameters to test\nhid1=np.arange(15,51,1)\nhid2=np.arange(15,51,1)\n# Create empty tuple to store results for training and testing values\ntraining_acc=[]\ntraining_loss=[]\ntesting_acc=[]\n\n# Define training parameters\nEPOCHS = 80\nBATCH_SIZE=32\nLEARNING_RATE = 0.01\n\n# Search parameter space\nfor i, hidden1 in enumerate(hid1):\n    for j, hidden2 in enumerate(hid2):\n        # Define instance of class to test\n        netb = NetClassifier(hidden1,hidden2)\n        # Define optimizer and loss function\n        optimizer = optim.Adam(netb.parameters(),lr=LEARNING_RATE)\n        loss_function = nn.CrossEntropyLoss()\n        # Perform training\n        for epoch in range(EPOCHS):\n            epoch_loss=0\n            epoch_acc=0\n            for xb,yb in train_dataloader:\n            # Zero gradients for training\n                optimizer.zero_grad()\n            # Use current model parameters to predict output\n                y_pred = net(xb)\n            # Calculate loss, use float type to calculate loss\n                loss = loss_function(y_pred,yb)\n                losses.append(loss.item())\n                # Backpropagate\n                loss.backward()\n                # Step in the optimizer\n                optimizer.step()\n                epoch_loss+=loss.item()\n                epoch_acc+=(yb == torch.argmax(y_pred,dim=1)).float().mean()\n        # Store accuracy on training set at end of training\n        training_acc.append((torch.argmax(netb(x_train_t),dim=1)==y_train_t).float().mean().numpy())\n        # Store loss on training set at end of training\n        training_loss.append(loss_function(netb(x_train_t),y_train_t).item())\n        # Store accuracy on testing set at end of training\n        testing_acc.append((torch.argmax(netb(x_test_t),dim=1)==y_test_t).float().mean().numpy())\n\n# Create cartesian product of parameters for DataFrame\nh1_list = []\nh2_list = []\nfor elem in itertools.product(hid1,hid2):\n    h1_list.append(elem[0])\n    h2_list.append(elem[1])\n        \n# Put results in a Data Frame\nresults = pd.DataFrame({\"Neurons H1\":h1_list,\"Neurons H2\":h2_list,\"Training Loss\":training_loss,\n                        \"Training Accuracy\":training_acc,\"Testing Accuracy\":testing_acc})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = results.astype(\"float\")\nmloc = results[\"Testing Accuracy\"].argmax()\nresults.iloc[mloc]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the parameter search for the neural network architecture that predicts best on the validation set, it can be seen that the parameters **H1=17**, **H2=36** give an accuracy for the training and testing set of about **79%**. That is it performs about as well on seen data as on unseen data. ","metadata":{}},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"code","source":"#Define LogisticRegression class by implementing a linear model with a sigmoid activation layer, use BCELoss for loss function\nclass LogisticRegression(nn.Module):\n    def __init__(self,n_input_features):\n        super(LogisticRegression,self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(n_input_features,1),\n            nn.Sigmoid()\n        )\n    \n    def forward(self,x):\n        x=self.model(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_features = X.shape[1]\nlr = LogisticRegression(n_features)\nprint(lr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define learning parameters\nlearning_rate=0.001\nEPOCHS_LR=300\nBATCH_SIZE=32\n#Define optimizer\nlr_optimizer = optim.Adam(lr.parameters(),lr=learning_rate)\n#Define loss function\nlr_loss = nn.BCELoss()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define variables as tensors to train the model, BCELoss requires target of type float\nx_train_t = torch.tensor(x_train.values, dtype=torch.float)\nx_test_t = torch.tensor(x_test.values, dtype=torch.float)\ny_train_t = torch.tensor(y_train.values, dtype=torch.float)\ny_test_t = torch.tensor(y_test.values, dtype=torch.float)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create DataSet and DataLoaders to train the neural network\ntrain_data_set = TensorDataset(x_train_t,y_train_t) \ntest_data_set = TensorDataset(x_test_t,y_test_t)\n\ntrain_dataloader = DataLoader(train_data_set,batch_size = BATCH_SIZE)\ntest_dataloader = DataLoader(test_data_set,batch_size = BATCH_SIZE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tuple to store batch loss\nlosses_lr=[]\n# Model training\nfor epoch in range(EPOCHS_LR):\n    for xb,yb in train_dataloader:\n        # Zero gradients in optimizer\n        lr_optimizer.zero_grad()\n        # Forward pass on batch\n        y_pred = lr(xb)\n        y_pred = torch.flatten(y_pred)\n        loss = lr_loss(y_pred,yb)\n        losses_lr.append(loss.item)\n        # Backward pass on batch\n        loss.backward()\n        #Optimizer step\n        lr_optimizer.step()\n    if epoch%10==0:\n        print(\"Epoch {:>2d} | Loss {:.4f}\".format(epoch,loss.item()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.lineplot(np.arange(1,len(losses)+1,1),losses)\nplt.title(\"Learning curve training logistic regression\")\nplt.xlabel(\"Batch iteration\")\nplt.ylabel(\"BCELoss\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Forward pass of trained network\ny_pred = lr(x_test_t)\ny_pred_class = y_pred.round().flatten()\nacc = (y_pred_class == y_test_t).float().mean()\nprint(\"Accuracy on classification of validation set is {:.4f}\".format(acc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusions","metadata":{}},{"cell_type":"markdown","source":"From the data it was understood that there is are indicators that lead to higher chances of having a heart attack. \n\n - From the implemented models to understand the relation between the features and the probability of a person having a heart attack it is easy to implement models with at least 80% accuracy. Higher accuracy can be achieved by hyper-parameter tuning, but it is also important to consider that a type 2 error for the classifier can be very dangerous for a patient. ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}