{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 🐍 Next DS Job: XGBoost, LightGBM, LogReg, R.Forest","metadata":{}},{"cell_type":"markdown","source":"# SUMMARY\n\n1.  This notebook is created to implement different ML models on **HR Analytics: Job Change of Data Scientists** dataset.\n> **(XGBoost, LightGBM, LogisticRegression, RandomForest)**\n1.  The dataset is preprocessed in several ways. \n> **(KNNImputer, LabelEncoding, OneHotEncoding)**\n1.  Finally, the results are compared in order to find the best model and preprocessing combination.\n1.  The best model is used to predict test data.","metadata":{}},{"cell_type":"markdown","source":"# Table Of Contents\n\n* [1. EDA & Preparing the Mappings](#chapter1)\n* [2. Data Preparation](#chapter2)\n    * [2.1. Handle Missing Values](#chapter2.1)\n    * [2.2. Dataset for LightGBM](#chapter2.2)\n    * [2.3. Correlation](#chapter2.3)\n    * [2.4. Dataset for LogisticRegression, RandomForest, XGBoost](#chapter2.4)\n        * [2.4.1 Encoding with manual mapping](#chapter2.4.1)\n        * [2.4.2 KNN Imputer](#chapter2.4.1)\n    * [2.3. Dataset Summary](#chapter2.5) \n* [3. Models](#chapter3)\n    * [3.1. Model 1: Logistic Regression](#chapter3.1)\n    * [3.2. Model 2: Random Forest](#chapter3.2)\n    * [3.3. Model 3: XGBoost](#chapter3.3)\n    * [3.4. Model 4: LightGBM](#chapter3.4)\n* [4. Model Comparisons](#chapter4)\n* [5. Feature Importances](#chapter5)\n* [6. Predict aug_test.csv](#chapter6)","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import KNNImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nfrom datetime import datetime\n\nimport xgboost as xgb\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('../input/hr-analytics-job-change-of-data-scientists/aug_train.csv')\ntest_data = pd.read_csv('../input/hr-analytics-job-change-of-data-scientists/aug_test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. EDA & Preparing the mappings <a class=\"anchor\" id=\"chapter1\"></a>","metadata":{}},{"cell_type":"code","source":"train_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Train shape:', train_data.shape)\nprint('Test shape:', test_data.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def perc_missing(df):\n    print('***  Count Missing Values ***')\n    print (train_data.isnull().sum().sort_values(ascending=False))\n    print('\\n---------------\\n')\n    print('*** Percentage Missing Values ***')\n    print ((df.isnull().sum() / len(df)*100).sort_values(ascending=False))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"perc_missing(train_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# columns that has at least one null value\ntrain_data.columns[train_data.isnull().any()].tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Save the columns as series.","metadata":{}},{"cell_type":"code","source":"enrollee_id = test_data['enrollee_id']\ntarget = train_data['target']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **--> City**\n- **Nominal Categorical Attribute**: There is no ordering between the cities.","metadata":{}},{"cell_type":"code","source":"train_data.city.value_counts().sort_values()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Label Encoder can be used for encoding cities to an integer.","metadata":{}},{"cell_type":"markdown","source":"### **--> City Development Index**\n- **Continues Numerical Attribute**:","metadata":{}},{"cell_type":"code","source":"train_data.city_development_index.hist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.city_development_index.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- There is no null value in city_development_index column.","metadata":{}},{"cell_type":"markdown","source":"### **--> Gender**\n- **Nominal Categorical Attribute**: There is no ordering between gender categories.","metadata":{}},{"cell_type":"code","source":"train_data.gender.value_counts().sort_values()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,5))\npatches, texts, autotexts = plt.pie(x=train_data.gender.value_counts().tolist(), labels=train_data.gender.value_counts().index, autopct='%1.2f%%')\n\n#make percent texts bigger\nplt.setp(autotexts, fontsize=14)\n\n#make label texts bigger\nplt.setp(texts, fontsize=14)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.gender.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- There are null values in Gender column. They will be handled later.\n- Categorical values are converted to numerical values by using mapping with dictionary.\n- For LGBM model, missing values left as NaN.","metadata":{}},{"cell_type":"code","source":"map_gender_lgbm = {'Other': 0, 'Female':1, 'Male':2}\nmap_gender = {'null': 0, 'Other': 1, 'Female':2, 'Male':3}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **--> Relevant Experience**\n- **Ordinal Categorical Attribute**","metadata":{}},{"cell_type":"code","source":"train_data.relevent_experience.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = train_data.relevent_experience.value_counts().tolist()\nlabels = train_data.relevent_experience.value_counts().index\n\npatches, texts, autotexts = plt.pie(x=data, labels=labels, autopct='%1.2f%%')\n\n#make percent texts bigger\nplt.setp(autotexts, fontsize=14)\n\n#make label texts bigger\nplt.setp(texts, fontsize=14)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.relevent_experience.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- There is no missing value for Relevant Experience column.\n- This is ordinal categorical variable, so there is ordering between categories. \n- Having relavant experience > Having no experience.\n- So, while convering them to numerical values, the ordering can be considered.","metadata":{}},{"cell_type":"code","source":"map_relevent_experience_lgbm = {'No relevent experience': 0, 'Has relevent experience': 1}\nmap_relevent_experience = {'null':0, 'No relevent experience': 1, 'Has relevent experience': 2}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **--> Enrolled University**\n- **Ordinal Categorical Attribute**","metadata":{}},{"cell_type":"code","source":"train_data.enrolled_university.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = train_data.enrolled_university.value_counts().tolist()\nlabels = train_data.enrolled_university.value_counts().index\n\npatches, texts, autotexts = plt.pie(x=data, labels=labels, autopct='%1.2f%%')\n\n#make percent texts bigger\nplt.setp(autotexts, fontsize=14)\n\n#make label texts bigger\nplt.setp(texts, fontsize=14)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.enrolled_university.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- There are missing values.\n- While converting ordinal categorical values to numerical values, the ordering can be considered.","metadata":{}},{"cell_type":"code","source":"map_enrolled_university_lgbm = {'no_enrollment': 0, 'Part time course': 1, 'Full time course' : 2}\nmap_enrolled_university = {'null': 0, 'no_enrollment': 1, 'Part time course': 2, 'Full time course' : 3}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **--> Education Level**\n- **Ordinal Categorical Attribute**","metadata":{}},{"cell_type":"code","source":"train_data.education_level.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = train_data.education_level.value_counts().tolist()\nlabels = train_data.education_level.value_counts().index\n\npatches, texts, autotexts = plt.pie(x=data, labels=labels, autopct='%1.2f%%')\n\n#make percent texts bigger\nplt.setp(autotexts, fontsize=14)\n\n#make label texts bigger\nplt.setp(texts, fontsize=14)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.education_level.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- There are missing values.\n- After conversion missing values will remain as NaN.","metadata":{}},{"cell_type":"code","source":"map_education_level_lgbm = {'Primary School':0, 'High School':1, 'Graduate': 2, 'Masters':3, 'Phd':4}\nmap_education_level = {'null': 0, 'Primary School':1, 'High School':2, 'Graduate': 3, 'Masters':4, 'Phd':5}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **--> Major Discipline**\n- **Nominal Categorical Attribute**","metadata":{}},{"cell_type":"code","source":"train_data.major_discipline.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.major_discipline.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- There are a lot of missing values.\n- After conversion missing values will remain as NaN.","metadata":{}},{"cell_type":"code","source":"map_major_discipline_lgbm = {'No Major':0, 'Arts':1, 'Business Degree':2, 'Other': 3, 'Humanities':4, 'STEM':5}\nmap_major_discipline = {'null': 0, 'No Major':1, 'Arts':2, 'Business Degree':3, 'Other': 4, 'Humanities':5, 'STEM':6}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **--> Experience**\n- **Numerical Categorical Variable**","metadata":{}},{"cell_type":"markdown","source":"- This is numerical variable that includes non-numeric values like <1 and >20.\n- These values can be handled with mapping.","metadata":{}},{"cell_type":"code","source":"train_data.experience.value_counts().sort_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.countplot(train_data.experience, order=['<1','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','>20'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- These are ordinal categorical variables which are in string format.\n- They can be converted to numerical variables, by handling >20 and <1 values.\n- If the model is LGBM then missing values left as NaN, otherwise will be encoded.","metadata":{}},{"cell_type":"code","source":"def experience_replace(exp, model):\n    if exp == '>20':\n        return 21\n    elif exp == '<1':\n        return 0\n    elif exp is not np.NaN:\n        return int(exp)\n    elif exp is np.NaN and model != 'LGBM':\n        return 22\n    else:\n        return exp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **--> Company Size**\n> - **Ordinal Categorical Variable**","metadata":{}},{"cell_type":"code","source":"train_data.company_size.value_counts().sort_values()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# number of instances based on company_size ordered by company_size\nplt.figure(figsize=(10,5))\nsns.countplot(train_data.company_size, order=['<10','10/49','50-99','100-500','500-999','1000-4999','5000-9999','10000+'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"map_company_size_lgbm = {'<10': 0,'10/49': 1,'50-99': 2,'100-500': 3,'500-999': 4,'1000-4999':5, '5000-9999':6, '10000+':7}\nmap_company_size = {'null': 0, '<10': 1,'10/49': 2,'50-99': 3,'100-500': 4,'500-999': 5,'1000-4999':6, '5000-9999':7, '10000+':8}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **--> Company Type**\n- **Nominal Categorical Variable**","metadata":{}},{"cell_type":"code","source":"train_data.company_type.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.countplot(train_data.company_type)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"map_company_type_lgbm = {\n    'Pvt Ltd': 5,\n    'Funded Startup':4,\n    'Early Stage Startup':3,\n    'Other':2,\n    'Public Sector':1,\n    'NGO':0\n}\n\nmap_company_type = {\n    'Pvt Ltd': 6,\n    'Funded Startup':5,\n    'Early Stage Startup':4,\n    'Other':3,\n    'Public Sector':2,\n    'NGO':1,\n    'null':0\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **--> Last New Job**\n- **Numerical Categorical Variable**","metadata":{}},{"cell_type":"markdown","source":"- This is numerical variable that includes non-numeric values like never and >4.\n- These values can be handled with mapping.","metadata":{}},{"cell_type":"code","source":"train_data.last_new_job.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Convert >4 and 'never' to numerical values.\n- If model is LGBM, missing values left as NaN, otherwise they are encoded.","metadata":{}},{"cell_type":"code","source":"# function for replacing the values, and converting them to integer\ndef lastnewjob_replace(lnj, model):\n    if lnj == '>4':\n        return 5\n    elif lnj == 'never':\n        return 0\n    elif lnj is not np.NaN:\n        return int(lnj)\n    elif lnj is np.NaN and model != 'LGBM':\n        return 6\n    else:\n        return lnj","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **--> Training Hours**","metadata":{}},{"cell_type":"code","source":"train_data.training_hours.hist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.training_hours.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- There is no missing value.","metadata":{}},{"cell_type":"markdown","source":"### **--> Target**\n- Binary classification problem","metadata":{}},{"cell_type":"code","source":"train_data.target.hist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Data Preparation <a class=\"anchor\" id=\"chapter2\"></a>","metadata":{}},{"cell_type":"markdown","source":"- Merge train and test dataset before implementing encoding","metadata":{}},{"cell_type":"code","source":"all_data = pd.concat([train_data.drop(['target','enrollee_id'], axis=1), test_data.drop(['enrollee_id'], axis=1)], axis=0)\nall_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.1 Handle Missing Values <a class=\"anchor\" id=\"chapter2.1\"></a>","metadata":{}},{"cell_type":"markdown","source":"- Missing values can be handled with various ways.\n    - **Deleting** rows or columns that includes at least 1 missing value.\n    - **Encoding** all missing values to same number for a given column.\n    - **Imputation**: Filling the missing values with a relevant value. The relevant value can be\n        - median, mod, mean value of the column.\n        - a value that is found by implementing another machine learning algorithm to predict missing value.\n            - Ex: K-NearestNeigbors algorithm. The algorithm finds a value by evaluating the similar instances","metadata":{}},{"cell_type":"markdown","source":"The models will be executed with below dataset configurations.\n\n- LightGBM:\n    - missing values left\n- Logistic Regression, Random Forest, XGBoost (missing values should be handled for these models)\n    - missing values encoded with mapping\n    - missing values imputed with KNN","metadata":{}},{"cell_type":"code","source":"def convert_dataset(df_data, model):\n    \n    # do not change df_data\n    # converting will be done on returned dataset\n    temp_data = df_data.copy()\n    \n    le = LabelEncoder()\n    temp_data.city = le.fit_transform(temp_data.city)\n    temp_data.last_new_job = temp_data.last_new_job.apply(lastnewjob_replace, args=(model,))\n    temp_data.experience = temp_data.experience.apply(experience_replace, args=(model,))\n    \n    # convert categorical values, left missing values as null\n    if model == 'LGBM':\n        temp_data.gender = temp_data.gender.map(map_gender_lgbm)\n        temp_data.relevent_experience = temp_data.relevent_experience.map(map_relevent_experience_lgbm)\n        temp_data.enrolled_university = temp_data.enrolled_university.map(map_enrolled_university_lgbm)\n        temp_data.education_level = temp_data.education_level.map(map_education_level_lgbm)\n        temp_data.major_discipline = temp_data.major_discipline.map(map_major_discipline_lgbm)\n        temp_data.company_size = temp_data.company_size.map(map_company_size_lgbm)\n        temp_data.company_type = temp_data.company_type.map(map_company_type_lgbm)\n        \n        \n    # convert categorical values, encode missing values    \n    else:\n        # first fill NaN values with a string 'null', mapper are handling with 'null' string\n        temp_data.fillna('null', inplace=True)\n        temp_data.gender = temp_data.gender.map(map_gender)\n        temp_data.relevent_experience = temp_data.relevent_experience.map(map_relevent_experience)\n        temp_data.enrolled_university = temp_data.enrolled_university.map(map_enrolled_university)\n        temp_data.education_level = temp_data.education_level.map(map_education_level)\n        temp_data.major_discipline = temp_data.major_discipline.map(map_major_discipline)\n        temp_data.company_size = temp_data.company_size.map(map_company_size)\n        temp_data.company_type = temp_data.company_type.map(map_company_type)\n\n\n        \n    return temp_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 Dataset for LightGBM <a class=\"anchor\" id=\"chapter2.2\"></a>","metadata":{}},{"cell_type":"markdown","source":"- Categorical values are encoded.\n- Missing values are left as NaN. LightGBM can automatically handle missing values.\n- Check: https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html#\n- **convert_dataset(df, 'LGBM')** function will encode the values while considering the order, and leaving missing values as NaN","metadata":{}},{"cell_type":"code","source":"convert_dataset(all_data, 'LGBM')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data = convert_dataset(all_data, 'LGBM')\nlgbm_train_data = all_data.iloc[0:train_data.shape[0], :]\nlgbm_test_data = all_data.iloc[train_data.shape[0]:, :]\n\n# add the id column to test data\nlgbm_test_data.loc[:,'enrollee_id'] = enrollee_id.values\n# add the target column to train data\nlgbm_train_data.loc[:,'target'] = target.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.3 Correlation among Columns <a class=\"anchor\" id=\"chapter2.3\"></a>","metadata":{}},{"cell_type":"markdown","source":"- Outstanding correlations\n    - Experience vs Relevant Experience: This is self-explanatory relationship.\n    - Experience vs Enrolled University: People with higher experience usually dont enroll university.\n    - Experience vs Last New Job: While experience is increasing, the difference in years between the previus job and current job is increasing\n    - City development index vs Target (Looking for a job change): Negative correlation.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,10))\nsns.heatmap(lgbm_train_data.corr(), annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.4 Dataset for LogisticRegression, RandomForest, XGBoost <a class=\"anchor\" id=\"chapter2.4\"></a>","metadata":{}},{"cell_type":"markdown","source":"### **2.4.1. Encoding with manuel mapping**","metadata":{}},{"cell_type":"markdown","source":"- Call convert_dataset function by giving model name as Others\n- All missing values are encoded","metadata":{}},{"cell_type":"code","source":"all_data = pd.concat([train_data.drop(['target','enrollee_id'], axis=1), test_data.drop(['enrollee_id'], axis=1)], axis=0)\nall_data = convert_dataset(all_data, 'Others')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"others_train_data = all_data.iloc[0:train_data.shape[0], :]\nothers_test_data = all_data.iloc[train_data.shape[0]:, :]\n\n# add the id column to test data\nothers_test_data.loc[:,'enrollee_id'] = enrollee_id.values\n# add the target column to train data\nothers_train_data.loc[:,'target'] = target.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"others_train_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"others_test_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **2.4.2. KNN Imputer**","metadata":{}},{"cell_type":"markdown","source":"- First convert the original dataset without filling missing values.\n- Converting parameter is LGBM, because it will left missing values as NaN\n- Then, missing values will be imputed by using KNN algorithm.","metadata":{}},{"cell_type":"code","source":"# Get the original data, merge it.\nall_data = pd.concat([train_data.drop(['target','enrollee_id'], axis=1), test_data.drop(['enrollee_id'], axis=1)], axis=0)\n# convert it\nall_data = convert_dataset(all_data, 'LGBM')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_columns = all_data.columns[all_data.isnull().any()].tolist()\nmissing_columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Since the categorical columns are labeled with numerical values, the values found by KNN should be rounded to integer.","metadata":{}},{"cell_type":"code","source":"knn_imputer = KNNImputer(n_neighbors=3)\n\narr = knn_imputer.fit_transform(all_data.loc[:,all_data.columns != 'target'])\nall_data_knn_imputed = pd.DataFrame(arr, columns = all_data.loc[:,all_data.columns != 'target'].columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data_knn_imputed[missing_columns] = np.round(all_data_knn_imputed[missing_columns])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data_knn_imputed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Split the dataset as train and test","metadata":{}},{"cell_type":"code","source":"train_data_knn_imputed = all_data_knn_imputed.iloc[0:train_data.shape[0], :].copy()\ntest_data_knn_imputed = all_data_knn_imputed.iloc[train_data.shape[0]:, :].copy()\n\n# add the id column to test data\ntest_data_knn_imputed.loc[:,'enrollee_id'] = enrollee_id.values\n# add the target column to train data\ntrain_data_knn_imputed.loc[:,'target'] = target.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_knn_imputed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_knn_imputed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.5 Datasets Summary <a class=\"anchor\" id=\"chapter2.5\"></a>\n\n- **lgbm_train_data**           : Dataset prepared for the LGBM model. All missing values are left as **NaN**.\n- **others_train_data**         : Dataset prepared for the models except LGBM. All missing values are handled with **manuel mapping**.\n- **train_data_knn_imputed**    : Dataset prepared for the models except LGBM. All missing values are handled with **KNN** imputer.\n\nAnd, corresponding test datasets are prepared.","metadata":{}},{"cell_type":"markdown","source":"# 3. Models <a class=\"anchor\" id=\"chapter3\"></a>\n\n- Function for saving and printing model results for comparison","metadata":{}},{"cell_type":"code","source":"auc_scores = []\naccuracy_scores = []\nprecision_scores = []\nrecall_scores = []\nf1_scores = []\nmodel_list = []\nimputer_list = []\ntimestamp = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_report(estimator,X,y, model, imputer):\n    \n    print('\\n\\n','*'*15,'REPORT','*'*15,'\\n')\n    print('Model: ', model)\n    print('Imputer:', imputer)\n    \n    if model == 'LGBM':\n        auc = roc_auc_score(y, estimator.predict(X))\n        y_predict = [1 if x > 0.5 else 0 for x in estimator.predict(X) ]\n        cmatrix = confusion_matrix(y, y_predict)\n    else:\n        auc = roc_auc_score(y, estimator.predict_proba(X)[:,1])\n        y_predict= estimator.predict(X)\n        \n        \n    precision, recall, fscore, support = precision_recall_fscore_support(y, y_predict)\n    accuracy = accuracy_score(y, y_predict)\n    \n    \n    #print\n    print('AUC: ', auc)\n    print('*'*40)\n    print(classification_report(y, y_predict))\n    print('*'*40)\n    \n    if model == 'LGBM':\n        sns.heatmap(cmatrix, annot=True, fmt='d', cmap='Blues')\n    else:\n        plot_confusion_matrix(estimator,X, y, values_format='d')\n    \n    \n    #save\n    auc_scores.append(auc)\n    precision_scores.append(precision[1])\n    f1_scores.append(fscore[1])\n    recall_scores.append(recall[1])\n    accuracy_scores.append(accuracy)\n    model_list.append(model)\n    imputer_list.append(imputer)\n    timestamp.append(datetime.now())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.1 Model 1: Logistic Regression <a class=\"anchor\" id=\"chapter3.1\"></a>\n\n- For the nominal categorical variables, one hot encoding will be used in order to prevent introducing ordinal relation between values.\n- Tree-based models, such as Decision Trees, Random Forests, and Boosted Trees, typically don't perform well with one-hot encodings with lots of levels.\n- For Logistic regression, one hot encoding will be used for nominal categorical variable, label encoding will be used for ordinal categorical variable.\n- For RandomForest, XGBoost, and LightGBM, LabelEncoding will be used.","metadata":{}},{"cell_type":"markdown","source":"- Since the data is skewed, it is better to implement stratified splitting based on target value. ","metadata":{}},{"cell_type":"code","source":"def Logistic_Regression(df, imputer):\n    \n    # one hot encoding for Nominal categorical variables\n    train_data_ohe = pd.get_dummies(df, columns=['gender'], prefix='G', prefix_sep='_')\n    train_data_ohe = pd.get_dummies(df, columns=['enrolled_university'], prefix='EU', prefix_sep='_')\n    train_data_ohe = pd.get_dummies(df, columns=['major_discipline'], prefix='MD', prefix_sep='_')\n    \n    # Grid Search\n    X = train_data_ohe.drop(['target'],axis=1)\n    y = train_data_ohe['target']\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size=0.2, stratify= y)\n\n    lr = LogisticRegression(max_iter=2000)\n\n    params={'C':np.logspace( -10, 1, 15)}\n\n    gs_lr = GridSearchCV(lr, param_grid = params, scoring=('roc_auc'), cv=5, n_jobs=-1)\n    gs_lr.fit(X_train,y_train)\n    \n    print('Best parameters: ', gs_lr.best_params_)\n    print('Best score: ', gs_lr.best_score_)\n    print_report(gs_lr,X_valid,y_valid, 'LogisticRegression', imputer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Logistic_Regression(train_data_knn_imputed, 'KNN')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Logistic_Regression(others_train_data, 'Manuel_Mapping')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2. Model 2: Random Forest <a class=\"anchor\" id=\"chapter3.2\"></a>","metadata":{}},{"cell_type":"code","source":"def Random_Forest(df, imputer):\n \n    X = df.drop(['target'], axis=1)\n    y = df['target']\n\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size=0.2, stratify= y)\n\n    rf_clf = RandomForestClassifier(n_estimators=100)\n\n    param_grid = {\n        'max_depth': range(2,20,2),\n        'criterion': ['gini','entropy'],\n        'min_samples_split' : [2,5,10,20,50,100,150]\n    }\n\n    ss = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n    gs_rf = GridSearchCV(rf_clf,param_grid, cv=ss.split(X_train,y_train), scoring='roc_auc', n_jobs=-1)\n    gs_rf.fit(X_train,y_train)\n    \n    \n    print('Best parameters: ', gs_rf.best_params_)\n    print('Best score: ', gs_rf.best_score_)\n    print_report(gs_rf,X_valid,y_valid, 'RandomForest', imputer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Random_Forest(train_data_knn_imputed, 'KNN')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Random_Forest(others_train_data, 'Manuel_Mapping')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3 Model 3: XGBoost <a class=\"anchor\" id=\"chapter3.3\"></a>","metadata":{}},{"cell_type":"code","source":"# gamma means min_split_loss in XGBoost: minimum loss reduction required to make a further partition on a leaf node of the tree.\n# the larger the gamma is, the more conservative the algorithm will be.\n\n# collsample_bytree: is the subsample ratio of columns when constructing each tree. Subsampling occurs once for every tree constructed.\n\n\ndef XGBoost_(df, imputer):\n\n    X = df.drop(['target'], axis=1)\n    y = df['target']\n\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size=0.2, stratify= y)\n    \n    xgb_clf = xgb.XGBClassifier(use_label_encoder=False)\n\n\n    parameters = {\n         \"eta\"    : [0.01, 0.05, 0.10] ,\n         \"max_depth\"        : [ 5, 6, 8],\n         \"gamma\"            : [ 0.3, 0.4, 0.5 ],\n         \"colsample_bytree\" : [ 0.4, 0.5 , 0.7 ]\n         }\n\n\n    gs_xgboost = GridSearchCV(xgb_clf, parameters, n_jobs=-1, scoring='roc_auc', cv=3)\n    gs_xgboost.fit(X_train,y_train)\n    \n    print('Best parameters: ', gs_xgboost.best_params_)\n    print('Best score: ', gs_xgboost.best_score_)\n    print_report(gs_xgboost,X_valid,y_valid, 'XGBoost', imputer)\n    \n    return gs_xgboost","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgboost_knn_model = XGBoost_(train_data_knn_imputed, 'KNN')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgboost = XGBoost_(others_train_data, 'Manuel_Mapping')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.4. Model 4: Light GBM <a class=\"anchor\" id=\"chapter3.4\"></a>\n\n- For LightGBM, the dataset: manually encoded but missing values left.","metadata":{}},{"cell_type":"code","source":"def LightGBM_(df, imputer):\n    X = df.drop(['target'], axis=1)\n    y= df['target']\n\n    cat_features = ['city', 'gender', 'enrolled_university', 'education_level', 'major_discipline', 'company_size', 'company_type']\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size=0.2, shuffle=True, stratify=y, random_state=1301)\n    \n    train_lgbm_dataset_format = lgb.Dataset(X_train, y_train, categorical_feature=cat_features)\n    valid_lgbm_dataset_format = lgb.Dataset(X_valid, y_valid, categorical_feature=cat_features)\n    \n    params = {'objective':'binary',\n          'metric' : 'auc',\n          'boosting_type' : 'gbdt',\n          'colsample_bytree' : 0.93,\n          'num_leaves' : 50,\n          'max_depth' : -1,\n          'n_estimators' : 1000,\n          'min_child_samples': 200, \n          'min_child_weight': 0.08,\n          'reg_alpha': 2,\n          'reg_lambda': 5,\n          'subsample': 0.9,\n          'verbose' : -1,\n          'num_threads' : 4,\n          'learning_rate': 0.015,\n          'random_seed' : 100\n        }\n    \n    lgbm = lgb.train(params,\n                 train_lgbm_dataset_format,\n                 3000,\n                 valid_sets=valid_lgbm_dataset_format,\n                 early_stopping_rounds= 40,\n                 verbose_eval= 10\n                 )\n\n    print_report(lgbm, X_valid, y_valid, model='LGBM', imputer=imputer)\n    \n    return lgbm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm = LightGBM_(lgbm_train_data, 'Manuel_Mapping')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Model Comparisons <a class=\"anchor\" id=\"chapter4\"></a>","metadata":{}},{"cell_type":"code","source":"results = {'Timestamp': timestamp, 'Model':model_list, 'Imputer': imputer_list, 'AUC':auc_scores, 'Accuracy':accuracy_scores, 'Precision': precision_scores, 'Recall': recall_scores, 'F1_Score': f1_scores}\nresults = pd.DataFrame(results)\nresults","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Based on the AUC score the best model is **XGBoost** which is trained with manually mapped dataset.","metadata":{}},{"cell_type":"markdown","source":"# 5. Feature Importances <a class=\"anchor\" id=\"chapter5\"></a>\n\n- Feature importances based on the **XGBoost** model.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importances = pd.concat(\n    [pd.DataFrame(xgboost.best_estimator_.feature_importances_, columns=['Importances']),\n     pd.Series(others_train_data.drop(['target'], axis=1).columns, name='Features')], axis=1).sort_values(by='Importances', ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nax = sns.barplot(x='Features', y='Importances', data=feature_importances)\n\nplt.xticks(rotation=30)\nax.set_title('Feature Importances', fontsize='18')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Predict aug_test.csv <a class=\"anchor\" id=\"chapter6\"></a>","metadata":{}},{"cell_type":"markdown","source":"- **others_test_data** is a dataframe that holds preprocessed aug_test.csv records ","metadata":{}},{"cell_type":"code","source":"others_test_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = xgboost.predict_proba(others_test_data.drop(['enrollee_id'], axis=1))[:,1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.concat([others_test_data['enrollee_id'], pd.Series(y_pred, name='target')], axis=1)\nsubmission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}