{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"e87aba30-a38c-9982-17d4-a1c7e164a903"},"source":"# Components of Crime 1: Principal Components Analysis of Law Enforcement Agency Data\n\n<br />\n<br />\n<br />\n\n### Table of Contents\n\n* Introduction\n\n* Analysis of Law Enforcement Agency Data\n * Load Data\n * Covariance and Eigenvalue Analysis\n * Principal Components Analysis\n * Subspace Projection\n * K-Means Clustering Analysis\n \n\n<br />\n<br />\n<br />\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"c4414a38-97b6-5277-022d-eaf75247c9d4"},"source":"# Introduction \n\nThis notebook analyzes the California crime data set from the FBI.\n\nIn this notebook, we'll be loading eight data files, which form four data sets:\n* Law enforcement agencies\n* Cities\n* Counties\n* Campuses\n\nEach set has two data files, one with data about law enforcement and the other with data about crimes."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5a76609c-11c4-bd04-37a3-c124812a1908"},"outputs":[],"source":"# must for data analysis\n% matplotlib inline\nimport numpy as np\nimport pandas as pd\n\n# plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.pyplot import *\n\n# useful for data wrangling\nimport io, os, re, subprocess\n\n# for sanity\nfrom pprint import pprint"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"950f8404-5b06-1642-0f55-374348c6516c"},"outputs":[],"source":"# learn you some machines\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"77952957-4ad6-bfaf-d5d9-7cbadce0481f"},"outputs":[],"source":"data_files = os.listdir('../input/')\npprint(data_files)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d077aa2b-4b51-4745-6281-ded0cbb53b11"},"source":"## Analysis of Law Enforcement Agency Data\n\nStart by analyzing crime statistics broken out by law enforcement agency. \n\nWe can utilize PCA to reduce the number of dimensions of our data set, but it would be nice if that information were in a more useful form - or, even better, if it gave us a new way to look at law enforcement agencies. \n\nWhat we'll do is to apply principal components analysis to our data set to reduce the dimensionality, then create a K-Means clustering algorithm to group points that are neighbors in the lower-dimensional PCA space. This can help us to identify the important characteristics of a law enforcement agency and group similar law enforcement agencies based on the values of these important characteristics."},{"cell_type":"markdown","metadata":{"_cell_guid":"1343dd93-de2e-ebaa-f638-9527aa7c92e4"},"source":"## Load Data\n\nThe data requires a bit of wrangling to get into a DataFrame. We worked out a few functions in a prior notebook (link):"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"87afcd16-003a-852f-2020-957a04bedcf3"},"outputs":[],"source":"def ca_law_enforcement_by_agency(data_directory):\n    filename = 'ca_law_enforcement_by_agency.csv'\n\n    # Load file into list of strings\n    with open(data_directory + '/' + filename) as f:\n        content = f.read()\n\n    content = re.sub('\\r',' ',content)\n    [header,data] = content.split(\"civilians\\\"\")\n    header += \"civilians\\\"\"\n    \n    data = data.strip()\n    agencies = re.findall('\\w+ Agencies', data)\n    all_but_agencies = re.split('\\w+ Agencies',data)\n    del all_but_agencies[0]\n    \n    newlines = []\n    for (a,aba) in zip(agencies,all_but_agencies):\n        newlines.append(''.join([a,aba]))\n    \n    # Combine into one long string, and do more processing\n    one_string = '\\n'.join(newlines)\n    sio = io.StringIO(one_string)\n    \n    # Process column names\n    columnstr = header.strip()\n    columnstr = re.sub('\\s+',' ',columnstr)\n    columnstr = re.sub('\"','',columnstr)\n    columns = columnstr.split(\",\")\n    columns = [s.strip() for s in columns]\n\n    # Load the whole thing into Pandas\n    df = pd.read_csv(sio,quotechar='\"',names=columns,thousands=',')\n\n    return df\n\ndef ca_offenses_by_agency(data_directory):\n    filename = 'ca_offenses_by_agency.csv'\n\n    # Load file into list of strings\n    with open(data_directory + '/' + filename) as f:\n        lines = f.readlines()\n    \n    one_line = '\\n'.join(lines[1:])\n    sio = io.StringIO(one_line)\n    \n    # Process column names\n    columnstr = lines[0].strip()\n    columnstr = re.sub('\\s+',' ',columnstr)\n    columnstr = re.sub('\"','',columnstr)\n    columns = columnstr.split(\",\")\n    columns = [s.strip() for s in columns]\n    \n    # Load the whole thing into Pandas\n    df = pd.read_csv(sio,quotechar='\"',names=columns,thousands=',')\n\n    return df\n\ndf1 = ca_law_enforcement_by_agency('../input/')\ndf1.head()\n\ndf2 = ca_offenses_by_agency('../input/')\ndf2.head()\n\ndf = pd.merge(df1,df2)"},{"cell_type":"markdown","metadata":{"_cell_guid":"0781dc6f-33f2-90af-7319-b71cc0b435bf"},"source":"The DataFrame that results contains combined data about law enforcement and crimes:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2574bc25-caa3-ae7e-b459-a4542ac17828"},"outputs":[],"source":"print(df.shape)\nprint(df.head(2))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8927db74-b303-3896-a20e-101b05134659"},"outputs":[],"source":"# We should note that the columns\n# \"violent crime\" and \"property crime\" \n# are sums of other columns.\n\ncol1 = df['Violent crime']\ncol2 = (df['Murder and nonnegligent manslaughter']+df['Rape (revised definition)']+df['Robbery']+df['Aggravated assault'])\n\nprint(\"Columns col1 (violent crime) and col2 (sum of violent types of crime) are identical.\")\nprint((col2-col1)[:10])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"53ae83ba-dd26-0806-4d63-0faf0aaa4cb1"},"outputs":[],"source":"# This column does not have data\ntry:\n    del df['Rape (legacy definition)']\nexcept KeyError:\n    pass\n\ndf = df.replace(np.nan,0.0)\n\nfor col in df.columns.tolist():\n    print(\"Number of NaNs in column %s is %d\"%(col, df[col].isnull().sum() ))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d4a8f6ca-7b5d-5497-c5f5-9165d5eccbcf"},"outputs":[],"source":"pca_cols = df.columns.tolist()[3:]\n\nX_orig = df[pca_cols].values"},{"cell_type":"markdown","metadata":{"_cell_guid":"8da7bb80-f5c1-9c29-6374-d40f3f4f7394"},"source":"## Covariance and Eigenvalue Analysis\n\nNext we use a function that normalizes our data, so that each input variable has a mean of 0 and a variance of 1. Once we've done that, we can proceed with a covariance and eigenvalue analysis:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f2113b84-a6fb-61ca-db6e-ebc0a1f66238"},"outputs":[],"source":"def get_normed_mean_cov(X):\n    X_std = StandardScaler().fit_transform(X)\n    X_mean = np.mean(X_std, axis=0)\n    \n    ## Automatic:\n    #X_cov = np.cov(X_std.T)\n    \n    # Manual:\n    X_cov = (X_std - X_mean).T.dot((X_std - X_mean)) / (X_std.shape[0]-1)\n    \n    return X_std, X_mean, X_cov\n\nX_std, X_mean, X_cov = get_normed_mean_cov(X_orig)"},{"cell_type":"markdown","metadata":{"_cell_guid":"13f5cdad-1d85-ff24-ddc6-b26da65f4780"},"source":"The covariance matrix can be visualized with a heatmap, which will reveal any structure (variables that co-vary, and can therefore be reduced using PCA):"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7bf7254b-35a4-eed7-8e02-1868a7d61c62"},"outputs":[],"source":"xlabels = pca_cols\nxlabels = [re.sub(\"Murder and nonnegligent manslaughter\",\"Murder, Manslaughter\",j) for j in xlabels]\nxlabels = [re.sub(\"Total law enforcement employees\",\"Tot law enf empl\",j) for j in xlabels]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b884fc87-ecf9-e848-f352-475b0acd67d8"},"outputs":[],"source":"fig = plt.figure(figsize=(6,6))\nsns.heatmap(pd.DataFrame(X_cov), \n            xticklabels=xlabels, yticklabels=xlabels,\n            vmin=-1,vmax=1,\n            annot=False, square=True, cmap='BrBG')\nplt.title('Heatmap of Covariance Matrix Magnitude: Law Enforcement Agency Data', size=14)\n\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"10d8ad04-aa78-f102-cf37-b85548779543"},"source":"This visualization shows which variables vary together. The darker squares show a correspondence between changes in variables. The covariance matrix shows that rape and robbery, both violent crimes, co-vary positively with property crimes. We can also see that burglary is an outlier among property crimes in that it does not co-vary strongly with other property crimes. We also see that every crime, with the exception of aggrevated assault, co-varies positively with the total number of civilians in the law enforcement agency."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"56a7ab0e-82cc-3de0-4247-115d11c1f8c6"},"outputs":[],"source":"eigenvals, eigenvecs = np.linalg.eig(X_cov)\n\neigenvals = np.abs(eigenvals)\neigenvecs = np.abs(eigenvecs)\n\n# Eigenvalues are not necessarily sorted, but eigenval[i] *does* correspond to eigenvec[i]\n#print \"Eigenvals shape: \"+str(eigenvals.shape)\n#print \"Eigenvecs shape: \"+str(eigenvecs.shape)\n\n# Create a tuple of (eigenvalues, eigenvectors)\nunsrt_eigenvalvec = [(eigenvals[i], eigenvecs[:,i]) for i in range(len(eigenvals))]\n\n# Sort tuple by eigenvalues\neigenvalvec = sorted(unsrt_eigenvalvec, reverse=True, key=lambda x:x[0])\n\n## This is noisy, but interesting:\n#pprint([pair for pair in eigenvalvec])\n## We will visualize this below."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ed2a3abc-6d10-de04-bd5a-9c30646d99d8"},"outputs":[],"source":"\nfig = plt.figure(figsize=(6,3))\nsns.heatmap(pd.DataFrame([pair[1] for pair in eigenvalvec]), \n            annot=False, cmap='coolwarm',\n            xticklabels=xlabels, yticklabels=range(len(eigenvalvec)),\n            vmin=-1,vmax=1)\n\nplt.ylabel(\"Ranked Eigenvalue\")\nplt.xlabel(\"Eigenvector Components\")\nplt.title('Eigenvalue Analysis: Law Enforcement Agency Data', size=14)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"bcff08b1-593c-33c2-c553-e43305f09b62"},"source":"The eigenvalue visualization above shows each eigenvector on a row. The eigenvectors are sorted in order from highest to lowest eigenvalue. Higher values closer to the top of the plot indicate a variable that does a good job of representing the rest of the data set. While there is no single dominant quantity, there are a few quantities that we can drop or ignore.\n\nFor example, the murder and manslaughter column is nearly entirely gray - meaning that the number of cases of murder or manslaughter is the least statistically representative quantity we could look at when assessing a law enforcement agency. (Take note, all you crime journalists salivating over the latest murder statistics - murder stats are no judge of a law enforcement agency.)\n\nIt turns out that the eigenvectors of the covariance matrix, which we're visualizing above, are precisely the same as the principal components. The visualization above is showing the values of the principal components.\n\nWe can also look at the explained variance, which is a measure of how much of the variance in the data can be reprsented by a given principal component. This plot will tell us how useful PCA will be."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"287e01d1-4d02-88c1-ed91-5aa62ab2b995"},"outputs":[],"source":"lam_sum = sum([j[0] for j in eigenvalvec])\nexplained_variance = [(lam_k/lam_sum) for lam_k in sorted(eigenvals, reverse=True)]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3d5be9d8-856c-134c-0560-02befe57d939"},"outputs":[],"source":"plt.figure(figsize=(6, 4))\n\nplt.bar(range(len(explained_variance)), explained_variance, \n        alpha=0.5, align='center',\n        label='Individual Explained Variance $\\lambda_{k}$')\n\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Ranked Eigenvalues')\nplt.title(\"Scree Graph: Law Enforcement Agency Data\", size=14)\n\nplt.legend(loc='best')\nplt.ylim([0,np.max(explained_variance)+0.1])\nplt.tight_layout()"},{"cell_type":"markdown","metadata":{"_cell_guid":"e2b1cd04-dcc6-4e66-d093-1895fa5b1bc8"},"source":"Not bad - the first principal component accounts for quite a bit of variance. Typically we specify a minimum amount of variance in the original data set that must be retained, and select a number of principal components based on this. By plotting the cumulative sum of the explained variance, we can use a number of components that gives us an explained variance of 90% or more:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"03d81180-1636-0ac3-20ba-2eed01ea3d61"},"outputs":[],"source":"fig = plt.figure(figsize=(6,4))\nax1 = fig.add_subplot(111)\n\nax1.plot(np.cumsum(explained_variance),'o')\n\nax1.set_ylim([0,1.01])\n\nax1.set_xlabel('Number of Principal Components')\nax1.set_ylabel('Cumulative explained variance')\nax1.set_title('Explained Variance: Law Enforcement Agency Data', size=14)\n\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c51cf2c6-a56c-cc65-bd34-18c1e958cffd"},"outputs":[],"source":"print(np.cumsum(explained_variance)[:4])"},{"cell_type":"markdown","metadata":{"_cell_guid":"667b7d88-aabf-9895-e0bf-5f14018a73fa"},"source":"Four components it is - those four dimensions will account for 91.4% of the variance in the original data. Now we'll begin our PCA analysis.\n\n## Principal Components Analysis\n\nTo perform PCA, we can do the linear algebra by hand (good learning experience), or we can let scikit-learn do all the heavy lifting for us (better idea). Perform PCA by creating a PCA object and setting parameters (mainly, number of components). Then fit it to the normalized data (vectors of input variables in `X_std`):"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"17a4a2f4-2885-cf4b-a8fb-e4c4669146b9"},"outputs":[],"source":"N_PCA = 4\n\n# 4 components should explain about 90% of the variance.\nsklearn_pca = PCA(n_components = N_PCA).fit(X_std)\nprint(sklearn_pca.components_.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"42312f66-cf39-4519-58ce-8afaa5a9d1fd"},"outputs":[],"source":"print(\"Principal Components:\")\nprint(sklearn_pca.components_)"},{"cell_type":"markdown","metadata":{"_cell_guid":"8ac0f1fa-ae07-93b8-5670-54e80b309171"},"source":"We already visualized these principal components in a heatmap, but it is useful to break out the first four principal components and examine them with bar charts to more easily see which components contribute to the principal components."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6f8b4e49-28b0-36ce-b7af-3a28c65c0372"},"outputs":[],"source":"# This requires a weird bar chart label offset.\n# \n# xticks() controls where the tick marks are located,\n# and parameters like rotation angle of text.\n# this is available through pyplot (plt).\n#\n# xticklabels controls the x tick labels.\n# of course, this is NOT available through pyplot.\n# you have to have a handle to the axis itself.\n# that's why I use gca().set_xticklabels()\n# \n# the more sane way would be plt.xticklabels()\n\ncolors = [sns.xkcd_rgb[z] for z in ['dusty purple','dusty green','dusty blue','orange']]\nfor i in range(4):\n    fig = plt.figure(figsize=(6,4))\n    xstuff = list(range(len(sklearn_pca.components_[i])))\n    sns.barplot(xstuff,\n                sklearn_pca.components_[i], color=colors[i])\n    \n    gca().set_xticklabels(xlabels)\n    \n    plt.xticks(np.arange(len(sklearn_pca.components_[i]))-0.1,rotation=90,size=14)\n    plt.ylabel('Principal Component '+str(i+1)+' Value',size=12)\n    plt.title('Principal Component '+str(i+1),size=12)\n    plt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"8f9160fd-af78-7d66-f96a-81dda8921a51"},"source":"Interestingly, the first principal component acts to greatly dampen the effect of certain variables - some we would expect, namely, the number of aggrevated assaults, burglaries, and occurrences of murder or manslaughter, but also some that we would not expect, such as total number of law enforcement employees, total number of officers, and total occurrences of violent crime. Because these data consist of crime statistics, we would anticipate that a quantity such as total number of officers would be a good prediction of other crime statistics, but it is not. In fact, the number of civilians in the law enforcement agency is the quantity that represents crime data better.\n\nWhen we perform PCA, we are performing a linear transform that reduces the dimensions; this transform preserves the original data (does not skew it) with respect to certain directions - namely, the eigenvectors. This transform can be expressed as:\n\n$$\n\\mathbf{Z} = \\mathbf{W} \\mathbf{X}\n$$\n\nwhere $\\mathbf{W}$ is the projection matrix (that's what we're fitting when we call the `fit()` method of the PCA object), $\\mathbf{X}$ is the high-dimensional list of input vectors, and $\\mathbf{Z}$ is a low-dimensional representation of said input vectors. In our case, this will be a four-element vector instead of a sixteen-element vector. "},{"cell_type":"markdown","metadata":{"_cell_guid":"4dedef21-39ab-1a74-1d14-c8239b838e35"},"source":"## Subspace Projection\n\nBecause we've picked a small number of principal components, we can perform a subspace projection from the high-dimensional data to the lower-dimensional subspace, and use that to visualize the structure of the data. While it may be difficult to think about what clusters in principal component space mean, it's easy to build intuition by visualization. Here's a scatterplot of the projection of data into principal component subspace:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bad007de-372e-4322-0e65-7ea6dbd16a4e"},"outputs":[],"source":"Z = sklearn_pca.fit_transform(X_std)\nprint(Z.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9b4d5670-ab65-8f91-08f5-b9bab1d2efa7"},"outputs":[],"source":"fig = plt.figure(figsize=(14,6))\nax1, ax2 = [fig.add_subplot(120 + i + 1) for i in range(2)]\n\n\n\nax1.scatter( Z[:,0], Z[:,1], s=80 )\n\nax1.set_title('Principal Components 0 and 1\\nSubspace Projection', size=14)\nax1.set_xlabel('Principal Component 0')\nax1.set_ylabel('Principal Component 1')\n\n\n\nax2.scatter( Z[:,2], Z[:,3], s=80 )\n\nax2.set_title('Principal Components 2 and 3\\nSubspace Projection', size=14)\nax1.set_xlabel('Principal Component 0')\nax1.set_ylabel('Principal Component 1')\n\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3353b83a-7443-9ae0-f4c0-bdc5b0cc93be"},"outputs":[],"source":"for i in range(4):\n    print(\"Explained Variance, Principal Component %d: %0.4f\"%(i,sklearn_pca.explained_variance_[i]/np.sum(sklearn_pca.explained_variance_.sum())))"},{"cell_type":"markdown","metadata":{"_cell_guid":"6cebe035-e960-ada9-0baa-5f8145655f90"},"source":"Note that the explained variance quantity above is reporting the within-model explained variance, not the total explained variance accounting for all input variables."},{"cell_type":"markdown","metadata":{"_cell_guid":"f564a0b2-5984-8043-e3f9-fa06032af44e"},"source":"## K-Means Cluster Analysis\n\nFrom the plots of the data in the four principal component dimensions above, we can visually see some clusters. We can use a k-means cluster analysis to group neighboring points together into clusters. Once we've done that, we can figure out which law enforcement agencies correspond to which group, and derive a useful way of classifying law enforcement agencies."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9e15fd1e-ad4d-ed4c-45e8-6fcdf2c1b913"},"outputs":[],"source":"km = KMeans(n_clusters=6, n_init=4, random_state=False)\nkm.fit(Z)\nprint(km.n_clusters)\nprint(km.predict(Z))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"81335f94-c7c7-c5d4-edcd-414d9f9b2a8e"},"outputs":[],"source":"# To color each point by the digit it represents,\n# create a color map with N elements (N rgb values).\n# One for each cluster.\n#\n# Then, use the system response (y_training), which conveniently\n# is a digit from 0 to 9.\ndef get_cmap(n):\n    colorz = plt.get_cmap('Set1')\n    return[ colorz(float(i)/n) for i in range(n)]\n\ncolorz = get_cmap( km.n_clusters )\ncolors = [colorz[j] for j in km.predict(Z)]\n\nfig = plt.figure(figsize=(12,4))\nax1, ax2 = [fig.add_subplot(120 + i + 1) for i in range(2)]\n\ns1 = ax1.scatter( Z[:,0], Z[:,1] , c=colors, s=80 )\nax1.set_title('Principal Components 0 and 1\\nSubspace Projection')\n\ns2 = ax2.scatter( Z[:,2], Z[:,3] , c=colors, s=80 )\nax2.set_title('Principal Components 2 and 3\\nSubspace Projection')\n\n\n# ------------\n# thanks to matplotlib for legend stupid-ness.\n# guess i'll just draw the legend myself.\nlabels = [\"Cluster \"+str(j) for j in range(km.n_clusters)]\nrs = []\nfor i in range(len(colorz)):\n    p = Rectangle((0,0), 1, 1, fc = colorz[i])\n    rs.append(p)\nax1.legend(rs, labels, loc='best')\nax2.legend(rs, labels, loc='best')\n# ------------\n\nax1.set_ylim([-3,7])\nax2.set_ylim([-3,7])\n\nax1.set_xlabel(\"Principal Component 0\")\nax1.set_ylabel(\"Principal Component 1\")\n\nax2.set_xlabel(\"Principal Component 2\")\nax2.set_ylabel(\"Principal Component 3\")\n\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6fe75444-7715-777c-f0e6-6aa96afae2eb"},"outputs":[],"source":"# Store the cluster number in a new DataFrame column \ncluster_col = km.predict(Z)\ndf['Cluster'] = cluster_col"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"78e952e7-fc41-6bb9-0d5e-366de3b3d5f5"},"outputs":[],"source":"for k in range(km.n_clusters):\n    if k!=0:\n        print(\"-\"*20)\n    print(\"Cluster %d:\"%(k))\n    pprint(df['Agency'][df['Cluster']==k].tolist())"},{"cell_type":"markdown","metadata":{"_cell_guid":"ea8bb567-b6af-3876-9184-26f4cdd12e09"},"source":"It looks like the cluster analysis has recovered groupings of law enforcement agencies into school districts (two clusters), tribal law enforcement agencies, and three clusters corresponding to the three law enforcement agencies that are statistical outliers from each other and from other law enforcement agencies (parks and recreation, a state hospital, and BART police).\n\nThe analysis provides two novel observations:\n\n * The San Bernadino school district law enforcement agency is statistically much different from the three other school district law enforcement agencies;\n\n * The tribal law enforcement agency cluster also includes an airport, a rancheria, and a developmental center, meaning these agencies operate similarly to tribal law enforcement agencies."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a2c9ce37-992e-2c28-cb66-991c33bf4f93"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}