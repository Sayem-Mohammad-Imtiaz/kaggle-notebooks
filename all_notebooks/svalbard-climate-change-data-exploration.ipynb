{"nbformat":4,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","version":"3.6.1","pygments_lexer":"ipython3"}},"nbformat_minor":1,"cells":[{"cell_type":"markdown","metadata":{"_uuid":"0b0797202a532ea8a1e919b98ff59d927e270b62","_cell_guid":"5aea0be2-5d1d-41df-a9c6-791d76f38ac4"},"source":"# Svalbard Climate Change Exploration"},{"cell_type":"markdown","metadata":{"_uuid":"b968ef5e74384514a928cebcc4ac6296c3bd18ee","_cell_guid":"6421f9e5-395a-4324-a4f4-7fc09b5546af"},"source":"## Svalbard Climate Change Project Motivation ##\nThis is my first data science project, first time using python and first kaggle project. I am looking to improve all my skills and appreciate any comments and suggestions. \n\n## Context ##\nI read an article in the Guardian, __\"Here's What Happens When you Try to Replicate Climate Change Contrarian Paper\" (https://www.theguardian.com/environment/climate-consensus-97-per-cent/2015/aug/25/heres-what-happens-when-you-try-to-replicate-climate-contrarian-papers)__.\nI became interested in the problem of predicting climate and specifically seeing if I could practice forecasting climate with the data that was used. I decided to investigate one of the studies mentioned in the article by Humlum, et. al, __Identifying natural contributions to late Holocene climate change (https://www.researchgate.net/publication/232402119_Identifying_natural_contributions_to_late_Holocene_climate_change)__, to see if I could use the dataset for my own exploration. One the interesting things in Humlum's paper was a prediction that the temperature would drop from 2015-2017 and then rise again. Since the paper was written, we now some have data on some of the period he forecast.  I am actually not making a case for or against climate change, but wanted to use this project to learn about data science and predictive modeling.  I found the paper interesting and, unlike it critics, did not 'deny' climate change but proposed that there are potentially long term cyclic patterns in climate caused by solar and lunar affects on the Earth. __Critics of their model (http://static-content.springer.com/esm/art%3A10.1007%2Fs00704-015-1597-5/MediaObjects/704_2015_1597_MOESM1_ESM.pdf)__ have suggested that the method was based on biased selection of data and then curvefitting the data.   However, I personally found the creation of the model using wavelet analysis interesting and reserve judgement on the process to others who have better qualificatins than me to weigh in.\n\nI am not a climatologist nor am I using more sophisticated climate forecasting models.  I was surprised that there the data set isn't being used by others.\n\n## The Data and Background ##\nI extracted a text file from the NASA database on climate for the Svarbard and Isfjord weather stations.  It has monthly, seasonal, and annual recorded temperatures.\n\nThe data represents a combination of 2+ weather stations on an island governed by Norway close to each other but with very little overlapping time periods, so we have data for over 100 years. I obtained the data from NASA and combined data from 2 weather stations:\nIsfjord Radio: (78.1 N,13.6 E), 1912 - 1976 \nSvalbard Luft: (78.2 N,15.5 E), 1977 - 2017 (partial) \n\nMy understanding is that these radio stations have moved several times, though close to each other.  The Isfjord Radio weather station was moved to Svalbard and is relatively close (47km). It is also an area with one of the longest human-recorded weather information in the Artic region and could be representative of any climate changes.  It is fairly isolated from the local affects of human generated air quality changes.  \n== Missing Data ==\nOne problem is missing data.  These are represented in the file with a temperature of 999.90.  I had to figure out what to do - which for me was challenging.\n\n##  The Plan ##\nThis is a data exploration.  The plan is to start simple with annual temperatures and see if I can solve for annual temperature change using linear regression.  I plan to look at monthly cycles as well as seasonal cycles as well.  I am also interested attempting to use wavelet analysis to replicate some of the results.  Another interest of mine is geomagnetic pole reversals...a future project.\n"},{"cell_type":"code","metadata":{"_uuid":"09c468af447389e8ddbe7547b00e3eea15852c6d","collapsed":true,"_cell_guid":"8bac2d1d-6570-44c6-9b99-1bab278569c1"},"execution_count":null,"source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"7cdbb6077580c55f9840912bd491ec47c0d743b6","_cell_guid":"91ae43c1-e8e1-4201-b6dd-4c8174a3ca1d"},"source":"## Outline of the planned data exploration\n1. Load data\n2. Extract annualized data\n3. Clean it - deal with missing readings\n4. Solve for a linear equations\n5. Plot and solve for r squared\n6. Try to optimize a fitted polynomial curve to the data\n7. Try using Single Vector Regression to fit a curve (though this may be overfitting)\n8. Do a few predictions"},{"cell_type":"code","metadata":{"_uuid":"fa726276bfa8839965aafbe976ba38bce2db5676","_cell_guid":"111daf57-88b9-47e7-a376-c193b3dd18ef"},"execution_count":null,"source":"#First read in the data and visually check it.\nclimate_df = pd.read_csv('../input/svalbard-climate-1912-2017.csv', header=0)\nprint(\"climate data frame\", climate_df)\n","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"545c33938ec16fa63e5358bada278421499f5e73","_cell_guid":"33ecba9d-9a56-4df7-b7b4-68bdc4b6c240"},"source":"Notice that 2004, 2005 and 2017 have a lot of missing data.  I think I will toss out 2017 as it is incomplete and fill in the missing data for rest. I read this article https://www.niwa.co.nz/our-science/climate/information-and-resources/nz-temp-record/temperature-trends-from-raw-data/technical-note-on-the-treatment-of-missing-data and used the technique of removing the missing data and averaging over the rest it to fill in the missing data.\n\nFirst I will explore the data by MetAnn, which is the average annual temperature."},{"cell_type":"code","metadata":{"_uuid":"24ea144e27957ad43243749e6cd63ee02ef0abd8","collapsed":true,"_cell_guid":"7591549b-77e0-4abf-b446-43eee692c14e"},"execution_count":null,"source":"#Extract X and Y arrays.  X is year, Y is average annual temperature in the MetANN column\nX=[]\nY=[]\n\nfor i in climate_df.YEAR:\n    X.append([1, i])\n    X.append\n#I am not sure if this is the best way to replace missing data with 999.90 values\n#calculate mean of the known data\nmean_metANN = np.mean(climate_df.metANN[climate_df.metANN<999])\n#fill in missing data with mean of known data\nfor i in climate_df.metANN:\n    if i < 999:\n        Y.append(i)\n    else:\n        Y.append(mean_metANN)","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"6c9f2eacc25916dc806571449da2bb073f98a747","collapsed":true,"_cell_guid":"b1a8dc13-687f-4538-884a-1df31ef32d10"},"execution_count":null,"source":"#convert to an np array to plot\nX= np.asarray(X)\nY= np.asarray(Y)\n","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"2ae2802130f555a83bd085a088a954e358e5fa8a","collapsed":true,"_cell_guid":"815ddada-6b07-4f39-8249-3efe68c0bacf"},"execution_count":null,"source":"#create solve function\ndef solve_w (X_s, Y_s):\n    w_solve = np.linalg.solve(np.dot(X_s.T, X_s), np.dot(X_s.T, Y_s))\n    Yhat_solve = np.dot(X_s, w_solve)\n    return w_solve, Yhat_solve","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"75bee689681446b2aa76e5bcb2d288d1cd43f6ad","collapsed":true,"_cell_guid":"c31212e1-ea58-4b1e-b2fd-41d60f897210"},"execution_count":null,"source":"#create plot function (probably don't really need this)\n#define a function to plot data with a label\ndef plot_it (X, Y, lab, mark=\"-\", col='blue'):\n    plt.plot(X, Y, label = lab, linestyle=mark, color=col)","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"46dd557d76f2680e57b976dd66637d5652bac2a2","collapsed":true,"_cell_guid":"8d059c2f-2c4e-41f6-8bdd-80df3a703d89"},"execution_count":null,"source":"#create residual mean squared function\n# determine how good the model is by computing the r-squared\ndef calc_r2(X, Y, Yhat):\n    d1 = Y - Yhat\n    d2 = Y - Y.mean()\n    r2 = 1 - d1.dot(d1) / d2.dot(d2)\n    return r2","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"d40d3da579910ab43022ffeecf323fdb18788237","collapsed":true,"_cell_guid":"4e041c39-bdea-4950-bfba-a4e076a88298"},"execution_count":null,"source":"#Ok, solve for Yhat (predicted value) and also calculate least square\n\nw, Yhat = solve_w(X, Y)\n\n#for fun, also try the lin alg least square method, I think this should be similar or the same as Yhat\nA = np.vstack([X[:,1], np.ones(len(X))]).T\nm, c = np.linalg.lstsq(A, Y)[0]\n","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"228da1ff76bf8bc0cc11802a951e7e9252f7e4c7","_cell_guid":"f665d05e-3caa-4606-a199-be9e65343016"},"execution_count":null,"source":"#now plot everything\n\nfig = plt.figure(figsize=(8,8))\nplot_it(X[:,1], Y, \"Y\", \"-\", 'blue')\nplot_it(X[:,1], Yhat, \"Yhat\", \"dashdot\", 'red')\nplot_it(X[:,1], X[:,1] * m + c, \"Least Square\", \"--\", 'green')\n#plt.plot(X[:,1], Y, label=\"Y\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Temperature (C)\")\nplt.legend()\nplt.show()","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"87a2e05df5ca4e6e7dcb22f9157738cbf26a25dc","_cell_guid":"e34b8479-580e-48bc-8df8-67522f51b2aa"},"execution_count":null,"source":"#Calcuate r-squared\nr2 = calc_r2(X, Y, Yhat)\nprint (\"the r-squared is:\", r2)","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"49331370fb0b98ab8fa433159bd18f570b2b14b2","collapsed":true,"_cell_guid":"93b18203-1816-4068-89b2-0fdd9f0dfcac"},"execution_count":null,"source":"#doesn't look like a very good fit\n#Lets look at moving averages to see how well they can fit","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"3370a37b6117ee98a5e7e8eadb3c55e966e88151","collapsed":true,"_cell_guid":"f692a35e-b13c-4c4e-8512-3edbcf914629"},"execution_count":null,"source":"#define a function for returning an array of moving averages over a period, n*2 is moving average period) \n# not sure if I got this quite correctly\n\ndef moving_average(a, n=3):\n    ret = np.cumsum(a, dtype=float)\n    ret[n:] = ret[n:] - ret[:-n]\n    return ret[n:-n] / n\n\n","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"7ea349f3819699a5bf74ae61c12eddb9670e261a","_cell_guid":"cf324a51-6976-4dea-b1f9-2333d33018e5"},"execution_count":null,"source":"#now try with moving averages\nperiods = 5\nY_ma = moving_average(Y, periods)\n\n#solve and plot\n#print(\"shape X\", X.shape, \"X[periods:-periods]\", X[periods:-periods].shape, \"Y_ma shape\", Y_ma.shape)\nw, Yhat_ma = solve_w(X[periods:-periods], Y_ma)\n\n\n#calculate least square\nfig = plt.figure(figsize=(8,8))\nplot_it(X[:,1], Y, \"Y\")\nplot_it(X[periods:-periods,1], Y_ma, \"Y moving average\", \"solid\", \"green\")\nplot_it(X[periods:-periods,1], Yhat_ma, \"Yhat moving average prediction\", \"dashdot\", \"red\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Temperature (C)\")\nplt.legend()\nplt.show()\nr2 = calc_r2(X[periods:-periods,1], Y[periods:-periods], Y_ma)\nprint (\"the r-squared of moving average is:\", r2)\nr2 = calc_r2(X[periods:-periods,1], Y[periods:-periods], Yhat_ma)\nprint (\"the r-squared of prediction from moving average is:\", r2)","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"a0a32256ff338580ff892f3700ca3620ee8b25dc","_cell_guid":"c6932438-e0a7-458b-a235-62d6a94cc87f"},"execution_count":null,"source":"#Now try fitting a polynomial\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#now with different equations\nplt.figure(figsize=(8,8))\nplot_it(X[:,1], Y, \"Y\", \"solid\", \"blue\")\n\nr_array = []\ndim_array=[]\nfor dim in range(1, 50, 1):\n    z = np.polyfit(X[:,1], Y, dim)\n    p = np.poly1d(z)\n    #plot Y and predicted Y\n    plot_it(X[:,1], p(X[:, 1]), dim, \"solid\", \"green\")\n    r = calc_r2(X, Y, p(X[:, 1]))\n    dim_array.append(dim)\n    r_array.append(r)\nplt.xlabel(\"Year\")\nplt.ylabel(\"Temperature (C)\")\n#plt.legend()\nplt.show()","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"88dba558f5f143c7aa0e7c85d858c675a940a543","_cell_guid":"42d047ec-78b3-4e9b-98a7-427646b4c513"},"execution_count":null,"source":"#plot r squared of polynomials and calculate minimum number of degrees\nplt.xlabel(\"dim\")\nplt.ylabel(\"r sqared\")\nplot_it(dim_array, r_array, \"r2\")\nplt.legend()\nplt.show()\nmax_dim = r_array.index(max(r_array))\nr_max = max(r_array)\nprint(\"maximum is dim: \", max_dim, \"with r: \", r_max)","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"776c278a48981ed4877f63ccc41996093fda3365","_cell_guid":"78daac92-1e44-4a8d-8848-048776962bf9"},"execution_count":null,"source":"#It looks like a polynomial with about 36 dimensions is our best fit using linear algrebra\nz = np.polyfit(X[:,1], Y, max_dim)\np = np.poly1d(z)\n\n#plot Y and predicted Y with polynomial\nplot_it(X[:,1], Y, \"original annual data\", 'solid', 'blue')\nplot_it(X[:,1], p(X[:, 1]), \"poly max dim\", \"dashed\", \"green\")\nplt.legend()\nplt.show()\nprint(\"r squared for dimension max dim is: \", calc_r2(X, Y, p(X[:, 1])))\nprint(\"The equation coefficients are: \", p)","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"d65202a39088efcd4e1278aa8272c6c36b345268","_cell_guid":"b48b1183-c8e4-474d-82bc-81bccbdafd55"},"execution_count":null,"source":"#test with 2017, just for fun\np(2017)","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"21f8e133a15e5dacbcc311c7d6f0b45c385b76c9","collapsed":true,"_cell_guid":"cc981e75-e057-4463-8bd5-fc16243d1d74"},"execution_count":null,"source":"#Now try with support vector regression\nfrom sklearn.svm import SVR\n#convert to matrix\nx = np.matrix(X[:,1]).T\ny = Y\n","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"bae5351768a9f9dea89f47425857ec3b18979413","collapsed":true,"_cell_guid":"7fca548b-b694-4589-ae19-b6b682611e4b"},"execution_count":null,"source":"#SVR kernels\n# #############################################################################\n# Fit regression model\nsvr_rbf = SVR(kernel='rbf', C=100.0, gamma=0.1)\nsvr_lin = SVR(kernel='linear', C=100.0)\n# this is very slow when I try several degrees svr_poly = SVR(kernel='poly', C=10, degree=1)\n#I would like to understand why polyfit seems so much better?? So I use that instead.\ny_lin = svr_lin.fit(x, y).predict(x)\ny_rbf = svr_rbf.fit(x,y).predict(x)\n#y_poly = svr_poly.fit(x, y).predict(x)\n","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"a19e0d0d7a05be8b5f29d36b3ed48a4d433aaee0","_cell_guid":"17f2dc44-95fd-423a-8f49-920f5dd39ffe"},"execution_count":null,"source":"#plot the different models\n\n# #############################################################################\n# Look at the results\nlw = 2\nplt.figure(figsize=(10,5))\nplt.scatter(X[:,1], y, color='darkorange', label='data')\nplt.plot(X[:,1], y_rbf, color='navy', lw=lw, label='RBF model')\nplt.plot(X[:,1], y_lin, color='c', lw=lw, label='Linear model')\n#plt.plot(X[:,1], y_poly, color='cornflowerblue', lw=lw, label='Polynomial model')\nplt.plot(X[:,1], p(X[:, 1]), color='cornflowerblue', lw=lw, label='Polynomial model')\nplt.xlabel('data')\nplt.ylabel('target')\nplt.title('Support Vector Regression')\nplt.legend()\nplt.show()\nprint(\"rsqared:  y_rbf\",calc_r2(X[:,1], Y, y_rbf ))\nprint(\"rsqared:  y_lin\",calc_r2(X[:,1], Y, y_lin ))\n#print(\"rsqared:  y_poly\",calc_r2(X[:,1], Y, y_poly ))\nprint(\"rsqared:  Polyfit\",calc_r2(X[:,1], Y, p(X[:,1])))","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"d426df18d379c89c70b5d3e6425ee8089f37a46b","_cell_guid":"38607a9d-2294-4ad3-a1b5-f2dd44c5afba"},"source":"## Conclusion and Future Work\nIt looks like Support Vector regression gives the best fit for the current data, though I am not sure if it is overfitting the data.  I am interested in developing my skills for future work."},{"cell_type":"code","metadata":{"_uuid":"ffa04be051145268c6e7b5108dcd2086ba2fa419","collapsed":true,"_cell_guid":"ee8c20ac-0737-4c26-a4fc-266c5d059e2a"},"execution_count":null,"source":"#Now some fun.  Make some predictions.  (I now this is not really a valid method)\nx_predict = range(2016, 2025)\ny_predict = svr_rbf.fit(x,y).predict(np.matrix(x_predict).T)\n","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"50bc52e6e4be5dd4469b88b586d41fb3601416a0","_cell_guid":"24a16577-f1c4-4e32-bbce-7a736f87608e"},"execution_count":null,"source":"#now put it together and plot - past and future\nfig = plt.figure(figsize=(10,8))\nplt.plot(x,y)\nplt.plot(x_predict, y_predict, color=\"blue\", linestyle='dashed')\nplt.show()","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"bd1f01aba367e7bc15cfc9005a974451cb3dd87d","collapsed":true,"_cell_guid":"e1c0f070-dd0a-40c8-ac53-a2db25207815"},"execution_count":null,"source":"","outputs":[]}]}