{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt  \nimport warnings\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nimport random\nimport time\n%matplotlib inline\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-16T18:31:39.713659Z","iopub.execute_input":"2021-07-16T18:31:39.714082Z","iopub.status.idle":"2021-07-16T18:31:39.725493Z","shell.execute_reply.started":"2021-07-16T18:31:39.714045Z","shell.execute_reply":"2021-07-16T18:31:39.724695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"warnings.filterwarnings('ignore') \ndata = '/kaggle/input/adult-dataset/adult.csv' \ndf = pd.read_csv(data, header=None, sep=',',names=['Age','WorkClass','fnlwgt', 'Education','YearsOfEd', 'Marital', 'Occupation', 'Relation', 'Race', 'Gender','Gain','Loss','HPR','Country','income']) \ndf","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:31:59.869793Z","iopub.execute_input":"2021-07-16T18:31:59.870443Z","iopub.status.idle":"2021-07-16T18:31:59.960063Z","shell.execute_reply.started":"2021-07-16T18:31:59.870403Z","shell.execute_reply":"2021-07-16T18:31:59.959082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.get_dummies(df, columns=['WorkClass', 'Education', 'Marital', 'Occupation', 'Relation', 'Race', 'Gender','Country'])\ndf","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:32:13.221916Z","iopub.execute_input":"2021-07-16T18:32:13.222283Z","iopub.status.idle":"2021-07-16T18:32:13.285503Z","shell.execute_reply.started":"2021-07-16T18:32:13.222251Z","shell.execute_reply":"2021-07-16T18:32:13.284366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def missing_value_funder(data):\n    for i in data.columns:\n        NAN = np.sum(pd.isna(data[i]))\n        if NAN>0:\n          print('column {} has missing values'.format(i))\n        else:\n          print('column {} has not any missing values'.format(i))\n\n\nmissing_value_funder(df)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:32:24.718688Z","iopub.execute_input":"2021-07-16T18:32:24.719059Z","iopub.status.idle":"2021-07-16T18:32:24.783046Z","shell.execute_reply.started":"2021-07-16T18:32:24.718988Z","shell.execute_reply":"2021-07-16T18:32:24.781969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col = list(df.columns)\ncolumn = []\nfor i in col:\n    if i!='income':\n        column.append(i)\ncolumn.append('income')\ndf = df[column]\ndf","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:32:37.745578Z","iopub.execute_input":"2021-07-16T18:32:37.745942Z","iopub.status.idle":"2021-07-16T18:32:37.781927Z","shell.execute_reply.started":"2021-07-16T18:32:37.745912Z","shell.execute_reply":"2021-07-16T18:32:37.780736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def trainTestSplit(dataFrame, testSize):\n    if isinstance(testSize, float):\n        testSize = round(testSize * len(dataFrame))\n    indices = dataFrame.index.tolist()\n    testIndices = random.sample(population = indices, k = testSize)\n    dataFrameTest = dataFrame.loc[testIndices]\n    dataFrameTrain = dataFrame.drop(testIndices)\n    return dataFrameTrain, dataFrameTest\n\ndef checkPurity(data):\n    if len(np.unique(data[:, -1])) == 1:\n        return True\n    else:\n        return False\n\ndef classifyData(data):\n    uniqueClasses, uniqueClassesCounts = np.unique(data[:, -1], return_counts = True)\n    return uniqueClasses[uniqueClassesCounts.argmax()]\n\ndef getPotentialSplits(data, randomAttributes):\n    potentialSplits = {}\n    _, columns = data.shape\n    columnsIndices = list(range(columns - 1))\n    if randomAttributes != None  and len(randomAttributes) <= len(columnsIndices):\n        columnsIndices = randomAttributes\n    for column in columnsIndices:\n        values = data[:, column]\n        uniqueValues = np.unique(values)\n        if len(uniqueValues) == 1:\n            potentialSplits[column] = uniqueValues\n        else:\n            potentialSplits[column] = []\n            for i in range(len(uniqueValues)):\n                if i != 0:\n                    currentValue = uniqueValues[i]\n                    previousValue = uniqueValues[i - 1]\n                    potentialSplits[column].append((currentValue + previousValue) / 2)\n    return potentialSplits\n\ndef splitData(data, splitColumn, splitValue):\n    splitColumnValues = data[:, splitColumn]\n    return data[splitColumnValues <= splitValue], data[splitColumnValues > splitValue]\n\ndef calculateEntropy(data):\n    _, uniqueClassesCounts = np.unique(data[:, -1], return_counts = True)\n    probabilities = uniqueClassesCounts / uniqueClassesCounts.sum()\n    return sum(probabilities * -np.log2(probabilities))\n\ndef calculateOverallEntropy(dataBelow, dataAbove):\n    pDataBelow = len(dataBelow) / (len(dataBelow) + len(dataAbove))\n    pDataAbove = len(dataAbove) / (len(dataBelow) + len(dataAbove))\n    return pDataBelow * calculateEntropy(dataBelow) + pDataAbove * calculateEntropy(dataAbove)\n\ndef determineBestSplit(data, potentialSplits, randomSplits = None):\n    overallEntropy = 9999\n    bestSplitColumn = 0\n    bestSplitValue = 0\n    if randomSplits == None:\n        for splitColumn in potentialSplits:\n            for splitValue in potentialSplits[splitColumn]:\n                dataBelow, dataAbove = splitData(data, splitColumn, splitValue)\n                currentOverallEntropy = calculateOverallEntropy(dataBelow, dataAbove)\n                if currentOverallEntropy <= overallEntropy:\n                    overallEntropy = currentOverallEntropy\n                    bestSplitColumn = splitColumn\n                    bestSplitValue = splitValue\n    else:\n        for i in range(randomSplits):\n            randomSplitColumn = random.choice(list(potentialSplits))\n            randomSplitValue = random.choice(potentialSplits[randomSplitColumn])\n            dataBelow, dataAbove = splitData(data, randomSplitColumn, randomSplitValue)\n            currentOverallEntropy = calculateOverallEntropy(dataBelow, dataAbove)\n            if currentOverallEntropy <= overallEntropy:\n                overallEntropy = currentOverallEntropy\n                bestSplitColumn = randomSplitColumn\n                bestSplitValue = randomSplitValue\n    return bestSplitColumn, bestSplitValue\n\ndef buildDecisionTree(dataFrame, currentDepth = 0, minSampleSize = 2, maxDepth = 1000, randomAttributes = None, randomSplits = None):\n    if currentDepth == 0:\n        global COLUMN_HEADERS\n        COLUMN_HEADERS = dataFrame.columns\n        data = dataFrame.values\n        if randomAttributes != None and randomAttributes <= len(COLUMN_HEADERS) - 1:\n            randomAttributes = random.sample(population = list(range(len(COLUMN_HEADERS) - 1)), k = randomAttributes)\n        else:\n            randomAttributes = None\n    else:\n        data = dataFrame\n    if checkPurity(data) or len(data) < minSampleSize or currentDepth == maxDepth:\n        return classifyData(data)\n    else:\n        currentDepth += 1\n        potentialSplits = getPotentialSplits(data, randomAttributes)\n        splitColumn, splitValue = determineBestSplit(data, potentialSplits, randomSplits)\n        dataBelow, dataAbove = splitData(data, splitColumn, splitValue)\n        if len(dataBelow) == 0 or len(dataAbove) == 0:\n            return classifyData(data)\n        else:\n            question = str(COLUMN_HEADERS[splitColumn]) + \" <= \" + str(splitValue)\n            decisionSubTree = {question: []}\n            yesAnswer = buildDecisionTree(dataBelow, currentDepth, minSampleSize, maxDepth, randomAttributes, randomSplits)\n            noAnswer = buildDecisionTree(dataAbove, currentDepth, minSampleSize, maxDepth, randomAttributes, randomSplits)\n            if yesAnswer == noAnswer:\n                decisionSubTree = yesAnswer\n            else:\n                decisionSubTree[question].append(yesAnswer)\n                decisionSubTree[question].append(noAnswer)\n            return decisionSubTree\n\ndef classifySample(sample, decisionTree):\n    if not isinstance(decisionTree, dict):\n        return decisionTree\n    question = list(decisionTree.keys())[0]\n    attribute, value = question.split(\" <= \")\n    if sample[attribute] <= float(value):\n        answer = decisionTree[question][0]\n    else:\n        answer = decisionTree[question][1]\n    return classifySample(sample, answer)\n\ndef decisionTreePredictions(dataFrame, decisionTree):\n    predictions = dataFrame.apply(classifySample, axis = 1, args = (decisionTree,))\n    return predictions\n\ndef calculateAccuracy(predictedResults, category):\n    resultCorrect = predictedResults == category\n    return resultCorrect.mean()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:32:55.009947Z","iopub.execute_input":"2021-07-16T18:32:55.010316Z","iopub.status.idle":"2021-07-16T18:32:55.033638Z","shell.execute_reply.started":"2021-07-16T18:32:55.010286Z","shell.execute_reply":"2021-07-16T18:32:55.032713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataFrame = df\ndataFrameTrain, dataFrameTest = trainTestSplit(dataFrame, testSize = 0.3)\n\nprint(\"Decision Tree - Adult Dataset\")\n\ni = 1\naccuracyTrain = 0\nwhile i < 5:\n    decisionTree = buildDecisionTree(dataFrameTrain, maxDepth = i)\n    decisionTreeTestResults = decisionTreePredictions(dataFrameTest, decisionTree)\n    accuracyTest = calculateAccuracy(decisionTreeTestResults, dataFrameTest.iloc[:, -1]) * 100\n    decisionTreeTrainResults = decisionTreePredictions(dataFrameTrain, decisionTree)\n    accuracyTrain = calculateAccuracy(decisionTreeTrainResults, dataFrameTrain.iloc[:, -1]) * 100\n    print(\"maxDepth = {}: \".format(i), end = \"\")\n    print(\"accTest = {0:.2f}%, \".format(accuracyTest), end = \"\")\n    print(\"accTrain = {0:.2f}%, \".format(accuracyTrain), end = \"\")\n    i += 1","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:34:06.09679Z","iopub.execute_input":"2021-07-16T18:34:06.097146Z","iopub.status.idle":"2021-07-16T19:47:26.944034Z","shell.execute_reply.started":"2021-07-16T18:34:06.097114Z","shell.execute_reply":"2021-07-16T19:47:26.943019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataFrameTrain_main = dataFrameTrain['income'].to_numpy()\ndataFrameTrain_main_new = []\nfor i in dataFrameTrain_main:\n    if i == '<=50K':\n        dataFrameTrain_main_new.append(0)\n    else :\n        dataFrameTrain_main_new.append(1)\n\n\ndataFrameTest_main = dataFrameTest['income'].to_numpy()\ndataFrameTest_main_new = []\nfor i in dataFrameTest_main:\n    if i == '<=50K':\n        dataFrameTest_main_new.append(0)\n    else :\n        dataFrameTest_main_new.append(1)\n\n\ndecisionTreeTrainResults_np = decisionTreeTrainResults.to_numpy()\ndecisionTreeTrainResults_new = []\nfor i in decisionTreeTrainResults_np:\n    if i == '<=50K':\n        decisionTreeTrainResults_new.append(0)\n    else :\n        decisionTreeTrainResults_new.append(1)\n\n\ndecisionTreeTestResults_np = decisionTreeTestResults.to_numpy()\ndecisionTreeTestResults_new = []\nfor i in decisionTreeTestResults_np:\n    if i == '<=50K':\n        decisionTreeTestResults_new.append(0)\n    else :\n        decisionTreeTestResults_new.append(1)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T19:57:58.321551Z","iopub.execute_input":"2021-07-16T19:57:58.321953Z","iopub.status.idle":"2021-07-16T19:57:58.339597Z","shell.execute_reply.started":"2021-07-16T19:57:58.321918Z","shell.execute_reply":"2021-07-16T19:57:58.338504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precision_train = precision_score(dataFrameTrain_main_new,decisionTreeTrainResults_new)\nrecall_train = recall_score(dataFrameTrain_main_new,decisionTreeTrainResults_new)\nprecision_test = precision_score(dataFrameTest_main_new,decisionTreeTestResults_new)\nrecall_test = recall_score(dataFrameTest_main_new,decisionTreeTestResults_new)\n\nprint('for train : precision = {} \\n recall = {}'.format(precision_train,recall_train))\nprint('for test : precision = {} \\n recall = {}'.format(precision_test,recall_test))","metadata":{"execution":{"iopub.status.busy":"2021-07-16T19:58:02.457582Z","iopub.execute_input":"2021-07-16T19:58:02.457925Z","iopub.status.idle":"2021-07-16T19:58:02.548882Z","shell.execute_reply.started":"2021-07-16T19:58:02.457896Z","shell.execute_reply":"2021-07-16T19:58:02.54817Z"},"trusted":true},"execution_count":null,"outputs":[]}]}