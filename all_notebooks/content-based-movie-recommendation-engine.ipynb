{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Content-Based Movie Recommendation Engine"},{"metadata":{},"cell_type":"markdown","source":"What movie should I watch next? Will I really have to scour through Reddit's Top 250 and IMDB and various other lists just to stumble across the name of the next fated film? \n\nNo, I won't! Because today we will build a simple movie recommendation engine that uses content-based filtering to provide a list of movies similar to the one we select."},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install rake_nltk\n\nimport pandas as pd\nimport numpy as np\nfrom rake_nltk import Rake\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom ast import literal_eval","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing\n### Loading Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"movies = pd.read_csv('../input/tmdb-movie-metadata/tmdb_5000_movies.csv')\ncredits = pd.read_csv('../input/tmdb-movie-metadata/tmdb_5000_credits.csv')\n\n# Join datasets\ncredits.columns = ['id', 'title', 'cast', 'crew']\n\nalldata = movies.merge(credits, on = 'id')\nalldata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cleaning Features"},{"metadata":{},"cell_type":"markdown","source":"Our content-based filtering system will not be using all of these columns, so I will cut the dataset down to only include the relevant features. Then we can clean up the feature contents."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\n# Trim dataset to include relevant features\ndf = alldata[['id', 'original_title', 'genres', 'keywords', 'overview', 'original_language', 'cast', 'crew']]\n\n# Parse stringed list features into python objects\nfeatures = ['keywords', 'genres', 'cast', 'crew']\nfor i in features:\n    df[i] = alldata[i].apply(literal_eval)\n    \n# Extract list of genres\ndef list_genres(x):\n    l = [d['name'] for d in x]\n    return(l)\ndf['genres'] = df['genres'].apply(list_genres)\n\n# Extract top 3 cast members\ndef list_cast(x):\n    l = [d['name'] for d in x]\n    if len(l) > 3:\n        l = l[:3]\n    return(l)\ndf['cast'] = df['cast'].apply(list_cast)\n\n# Extract top 5 keywords\ndef list_keywords(x):\n    l = [d['name'] for d in x]\n    if len(l) > 5:\n        l = l[:5]\n    return(l)\ndf['keywords'] = df['keywords'].apply(list_keywords)\n\n# Extract director\ndef get_director(x):\n    for i in x:\n        if i['job'] == 'Director':\n            return i['name']\n    return np.nan\ndf['director'] = df['crew'].apply(get_director)\n\n# Drop the now unnecessary crew feature\ndf = df.drop('crew', axis = 1)\n\n# Clean features of spaces and lowercase all to ensure uniques\ndef clean_feat(x):\n    if isinstance(x, list):\n        return [i.lower().replace(\" \",\"\") for i in x]\n    else:\n        if isinstance(x, str):\n            return x.lower().replace(\" \", \"\")\n        else:\n            return ''\n\nfeatures = ['keywords', 'genres', 'cast', 'director']\nfor i in features:\n    df[i] = df[i].apply(clean_feat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have several features with lists of keywords that are all lowercase and stripped of spaces, therefore making them unique keywords. "},{"metadata":{},"cell_type":"markdown","source":"### Missing Values\nLet's check for missing values, since they could be problematic when it comes to creating more keywords for overview."},{"metadata":{"trusted":true},"cell_type":"code","source":"missing = df.columns[df.isnull().any()]\ndf[missing].isnull().sum().to_frame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replace NaN from overview with an empty string\ndf['overview'] = df['overview'].fillna('')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating bag of keywords\n\nWe will use genres, keywords, overview, cast, and director to create a bag of words column.\n\nLet's use Rake from the nltk package to extract keywords from the overview feature, which is a summary of the plot. We'll put those keywords into a new column: plotwords."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Initialize empty column\ndf['plotwords'] = ''\n\n# function to get keywords from a text\ndef get_keywords(x):\n    plot = x\n    \n    # initialize Rake using english stopwords from NLTK, and all punctuation characters\n    rake = Rake()\n    \n    # extract keywords from text\n    rake.extract_keywords_from_text(plot)\n    \n    # get dictionary with keywords and scores\n    scores = rake.get_word_degrees()\n    \n    # return new keywords as list, ignoring scores\n    return(list(scores.keys()))\n\n# Apply function to generate keywords\ndf['plotwords'] = df['overview'].apply(get_keywords)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have our plot keywords, let's combine our our cleaned features with them to create a bag of words. We'll make a new dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_keys = pd.DataFrame() \n\ndf_keys['title'] = df['original_title']\ndf_keys['keywords'] = ''\n\ndef bag_words(x):\n    return(' '.join(x['genres']) + ' ' + ' '.join(x['keywords']) + ' ' +  ' '.join(x['cast']) + \n           ' ' + ' '.join(x['director']) + ' ' + ' '.join(x['plotwords']))\ndf_keys['keywords'] = df.apply(bag_words, axis = 1)\n\ndf_keys.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating Model\n\nWe will use CountVectorizer from scikit-learn to convet the keywords into a matrix of token counts, producing the frequency of each word."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create count matrix\ncv = CountVectorizer()\ncv_mx = cv.fit_transform(df_keys['keywords'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we apply the cosine_similarity function to find similarity between two movies. A brief overview drawn from [Machine Learning Plus](https://www.machinelearningplus.com/nlp/cosine-similarity/):\n\n> Cosine similarity is a metric used to determine how similar the documents are irrespective of their size. \nMathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. In this context, the two vectors I am talking about are arrays containing the word counts of two documents. \n\n>When plotted on a multi-dimensional space, where each dimension corresponds to a word in the document, the cosine similarity captures the orientation (the angle) of the documents and not the magnitude. If you want the magnitude, compute the Euclidean distance instead. The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance because of the size (like, the word ‘cricket’ appeared 50 times in one document and 10 times in another) they could still have a smaller angle between them. Smaller the angle, higher the similarity."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create cosine similarity matrix\ncosine_sim = cosine_similarity(cv_mx, cv_mx)\ncosine_sim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create list of indices for later matching\nindices = pd.Series(df_keys.index, index = df_keys['title'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Recommendation \nNow we will write the actual recommendation function."},{"metadata":{"trusted":true},"cell_type":"code","source":"def recommend_movie(title, n = 10, cosine_sim = cosine_sim):\n    movies = []\n    \n    # retrieve matching movie title index\n    if title not in indices.index:\n        print(\"Movie not in database.\")\n        return\n    else:\n        idx = indices[title]\n    \n    # cosine similarity scores of movies in descending order\n    scores = pd.Series(cosine_sim[idx]).sort_values(ascending = False)\n    \n    # top n most similar movies indexes\n    # use 1:n because 0 is the same movie entered\n    top_n_idx = list(scores.iloc[1:n].index)\n        \n    return df_keys['title'].iloc[top_n_idx]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Testing our Recommendation Engine!"},{"metadata":{"trusted":true},"cell_type":"code","source":"recommend_movie('Toy Story', n = 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recommend_movie('The Avengers')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recommend_movie('The Hobbit: An Unexpected Journey')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recommend_movie('Ocean\\'s Eleven', n = 7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Remarks\n\nIn the future, I might join this project with the larger movies dataset on Kaggle. That way I can use release date as a keyword as well. Along that vein, I could add a function that limits release date to x number of years forward and backward when recommending the movie.\n\nIt'd also be good to explore if I can weight the director keyword more than other keywords.\n\nThank you for reading! I hope you enjoyed."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}