{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/bod-results/","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 0 : Description of the used approaches and corresponding scores."},{"metadata":{"trusted":true},"cell_type":"code","source":"results = pd.read_csv('../input/bod-results/BOD Kaggle_results.csv',usecols=['Methodology','RMSE','Change log'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_colwidth', -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results['Methodology'] = results['Methodology'].str.replace('\\n','')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 1 : EDA"},{"metadata":{},"cell_type":"markdown","source":"# Reading of data and basic statistics"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/prediction-bod-in-river-water/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv('../input/prediction-bod-in-river-water/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data['Id'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Id'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr =df[df.columns.to_list()].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the plot below it's obvious that the biggest correlation is between first two features and our target value."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.heatmap(corr, \n        xticklabels=corr.columns,\n        yticklabels=corr.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(columns='Id', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We won't need an Id column for our predictions."},{"metadata":{},"cell_type":"markdown","source":"We will also analyse the distribution of our dataset w.r.t target value."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\nplt.figure(figsize=(14,10))\nsns.distplot(df['target'])\nplt.title('Distribution of data points in the dataset');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lower,upper = np.percentile(df['target'],[1,99])\ndf['target'] = np.clip(df['target'],lower,upper)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\nplt.figure(figsize=(14,10))\nsns.distplot(df['target'])\nplt.title('Distribution of data points in the dataset');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Imputation of missing values"},{"metadata":{},"cell_type":"markdown","source":"Let's check our dataset for missing values. It's obvious that the feature columns from 3 to 7 \nconsist of mostly NaN values. Thus it's better to drop them."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns.to_list()[:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[df.columns.to_list()[:3]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We still have some NaN values in first two features, thus we will impute them using KNN."},{"metadata":{"trusted":true},"cell_type":"code","source":"nan_cols = df.isna().sum()[df.isna().sum()>0].index.to_list()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import KNNImputer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imputer = KNNImputer(n_neighbors=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nan_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[nan_cols] = imputer.fit_transform(df[nan_cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df[df.columns.to_list()].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering"},{"metadata":{},"cell_type":"markdown","source":"Let's also create a new feature which is just an average of the availbale ones."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns.to_list()[1:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['combined'] = df[df.columns.to_list()[1:]].mean(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[['target','1','2','combined']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the correlation between new feature and target value is also big enough, thus we will use it for models training."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df[df.columns.to_list()].corr()\ndisplay(corr)\nplt.figure(figsize=(10,8))\nsns.heatmap(corr, \n        xticklabels=corr.columns,\n        yticklabels=corr.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 2 : first level models training"},{"metadata":{},"cell_type":"markdown","source":"# Making gbd model and training "},{"metadata":{},"cell_type":"markdown","source":"First of all we will train a gbd model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error,make_scorer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score, train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from skopt import forest_minimize","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the score is validated using rmse, we will implement it and use as a scorer for cross validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmse(y,y_pred):\n    return np.sqrt(mean_squared_error(y_true=y,y_pred=y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scorer = make_scorer(rmse,greater_is_better=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_columns = df.columns.to_list()\ntarget_column = feature_columns.pop(0)\n(feature_columns,target_column)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df[target_column].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df[feature_columns].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Function <b>optimize_gbd</b> - finds the best hyperparameters for GradientBoostingRegressor in a defined space with respect to averaged by cross validation with kfold of 5 rmse score. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def optimize_gbd(space):\n    alpha,learning_rate, max_depth,max_features,max_leaf_nodes,n_estimators= space\n    gbd = GradientBoostingRegressor(alpha=alpha,learning_rate=learning_rate, max_depth=max_depth,\n                                    max_features=max_features,max_leaf_nodes=max_leaf_nodes,\n                                    n_estimators=n_estimators,random_state=5, criterion='mse')\n    score =  -1*cross_val_score(gbd,X,y,cv=5,scoring=scorer).mean()\n    print('Error : {}'.format(score))\n    return score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Given best parameters, <b>function train_best_gbd</b> trains the best model."},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_best_gbd(space):\n    alpha,learning_rate, max_depth,max_features,max_leaf_nodes,n_estimators= space\n    max_depth,max_features,max_leaf_nodes,n_estimators = int(max_depth), int(max_features), int(max_leaf_nodes), int(n_estimators)\n    gbd = GradientBoostingRegressor(alpha=alpha,learning_rate=learning_rate, max_depth=max_depth,\n                                    max_features=max_features,max_leaf_nodes=max_leaf_nodes,\n                                    n_estimators=n_estimators,random_state=5,\n                                   criterion='mse')\n    gbd.fit(X=X,y=y)\n    error = np.sqrt(mean_squared_error(y_pred=gbd.predict(X),y_true=y))\n    print('Overall error : {}'.format(error))\n    return gbd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"space - the space of parameters we will seek for best ones in"},{"metadata":{"trusted":true},"cell_type":"code","source":"space = [(0.1,0.9),#alpha\n         (1e-3,0.8),#learning_rate\n         (2,20),#max_depth\n         (1,3),#max_features\n         (2,100),#max_leaf_nodes\n         (100,1000)#n_estimators\n    \n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As there are lot's of parameters that we tune, we will make 200 calls to forest minimize optimizer to find the best ones."},{"metadata":{"trusted":true},"cell_type":"code","source":"best_params = forest_minimize(optimize_gbd,dimensions=space,n_calls=40,n_jobs=6,random_state=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Function <b>to_df</b> casts our experiments to data frame instance."},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_df(best_params,cols=['alpha','learning_rate','max_depth',\n                            'max_features','max_leaf_nodes','n_estimators']):\n    params =  np.array(best_params['x_iters'])\n    df = pd.DataFrame(columns=cols,data=params)\n    df['scores'] = best_params['func_vals']\n    return df\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_scores = to_df(best_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_scores.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_scores.loc[df_scores['scores'].argmin()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_scores['scores'].median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_scores['scores'].quantile(0.25)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best model is not the right choice as it's too complex. The better choice is to use the middle accurate model.\nFor this purprose we will choose a the subarray of experiments, which consists of middle accurate models."},{"metadata":{"trusted":true},"cell_type":"code","source":"by_percentile = df_scores[(df_scores['scores']>df_scores['scores'].quantile(0.45)) & (df_scores['scores']<df_scores['scores'].quantile(0.55))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"by_percentile","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We then can choose the random experiment. We will set a constant seed for reproducability."},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(14)\nchoice = np.random.choice(len(by_percentile))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now will choose the best hyperparameters with respect to our current choice."},{"metadata":{"trusted":true},"cell_type":"code","source":"params = by_percentile.iloc[choice][['alpha','learning_rate','max_depth',\n                            'max_features','max_leaf_nodes','n_estimators']].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are ready to train our first model in an ensemble."},{"metadata":{"trusted":true},"cell_type":"code","source":"params = df_scores.loc[df_scores['scores'].argmin()].drop('scores').values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbd_trained = train_best_gbd(params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will store the predictions of the gbd in ensemble_df as the first feature for the second lvl model."},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble_df = pd.DataFrame()\nensemble_df['gbd'] = gbd_trained.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's also visualize the predictions of our model and real y values with respect to timestamp."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\nplt.figure(figsize=(14,10))\nplt.scatter(range(len(df)),df['target'],color='black')\nplt.plot(range(len(df)),ensemble_df['gbd'],color='blue')\nplt.legend(['Values predicted with GBT','Real values'])\nplt.xlabel('Timestamps')\nplt.ylabel('Target values')\nplt.title('Results of GBT model');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ExtraTree train"},{"metadata":{},"cell_type":"markdown","source":"The second and last model we will use in ensemble is ExtraTreesRegressor. We will also tune this model with respect to average cross val score with kfold of 5 using rmse as the scoring function."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def optimize_extratree(space):\n    n_estimators,max_depth,min_samples_split,min_samples_leaf,max_features = space\n    extra_tree = ExtraTreesRegressor(min_samples_split=min_samples_split,max_features=max_features,random_state=5,\n                            min_samples_leaf=min_samples_leaf,max_depth=max_depth,n_estimators=n_estimators)\n    score =  -1*cross_val_score(extra_tree,X,y,cv=5,scoring=scorer).mean()\n    print('Error : {}'.format(score))\n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_best_extratree(space):\n    n_estimators,max_depth,min_samples_split,min_samples_leaf,max_features = list(map(int,space))\n    extra_tree = ExtraTreesRegressor(min_samples_split=min_samples_split,max_features=max_features,random_state=5,\n                            min_samples_leaf=min_samples_leaf,max_depth=max_depth,n_estimators=n_estimators)\n    extra_tree.fit(X,y)\n    error = np.sqrt(mean_squared_error(y_true=extra_tree.predict(X),y_pred=y))\n    print('Overall error : {}'.format(error))\n    return extra_tree","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"space = [(100,1000),#n_estimators\n        (8,20),#max_depth\n        (2,4),#min_samples_split\n         (2,5),#min_samples_leaf\n        (1,3)#max_features\n        ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_params= forest_minimize(optimize_extratree,random_state=5,dimensions=space,n_calls=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_scores = to_df(best_params,cols=[\"n_estimators\",\"max_depth\",\"min_samples_split\",\n                                    \"min_samples_leaf\",\n                                    \"max_features\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_scores.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_scores['scores'].mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can already choose the middle accurate model, that has 1.458386 cross val score."},{"metadata":{"trusted":true},"cell_type":"code","source":"params = df_scores.iloc[2][[\"n_estimators\",\"max_depth\",\"min_samples_split\",\n                                    \"min_samples_leaf\",\n                                    \"max_features\"]].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ex_tree_reg = train_best_extratree(params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble_df['extra_tree'] = ex_tree_reg.predict(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will also visualize results of Extra Trees."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\nplt.figure(figsize=(14,10))\nplt.scatter(range(len(df)),df['target'],color='black')\nplt.plot(range(len(df)),ensemble_df['extra_tree'],color='green')\nplt.legend(['Values predicted with ETR','Real values'])\nplt.xlabel('Timestamps')\nplt.ylabel('Target values')\nplt.title('Results of ETR model');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 3 : Second level model training"},{"metadata":{},"cell_type":"markdown","source":"# Gathering everything togather"},{"metadata":{},"cell_type":"markdown","source":"As we now have two features from both gbd and extra tree, we can train a linear regression using y as target. We use a linear regession here because the relationship is simple enough, thus we don't need to use a more complex model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble_df['y'] = y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We tend to have a different correlation between gbd and extra_tree predictions with target value, which is  good for overall generalization."},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble_df.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"first_lvl_features = ensemble_df[['gbd','extra_tree']].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = ensemble_df['y'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_second_lvl = LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"-1*cross_val_score(lr_second_lvl,first_lvl_features,labels,scoring=scorer).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_second_lvl.fit(first_lvl_features,labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sqrt(mean_squared_error(y_pred=lr_second_lvl.predict(first_lvl_features),y_true=labels))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we can show the linear regression predictions over \"stacked\" features.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\nplt.figure(figsize=(14,10))\nplt.scatter(range(len(df)),df['target'],color='black')\nplt.plot(range(len(df)),lr_second_lvl.predict(first_lvl_features),color='red')\nplt.legend(['Final predictions with second level model','Real values'])\nplt.xlabel('Timestamps')\nplt.ylabel('Target values')\nplt.title('Results of predictions using second level model');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\nplt.figure(figsize=(14,10))\nplt.scatter(range(len(df)),df['target'],color='black')\nplt.plot(range(len(df)),ensemble_df['extra_tree'],color='green')\nplt.plot(range(len(df)),ensemble_df['gbd'],color='blue')\nplt.plot(range(len(df)),lr_second_lvl.predict(first_lvl_features),color='red')\nplt.legend(['Values predicted with ETR','Values predicted with GBT','Final predictions with second level model','Real values'])\nplt.xlabel('Timestamps')\nplt.ylabel('Target values')\nplt.title('Results of predictions using second level model');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's show the outliers which weren't covered by my model."},{"metadata":{"trusted":true},"cell_type":"code","source":"def detect_outliers(df,constant=2):\n    outliers = df[(df['target']<=df['target'].mean()-constant*df['target'].std()) | (df['target']>=df['target'].mean()+constant*df['target'].std())]\n    return outliers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outliers = detect_outliers(df,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\nfrom matplotlib.lines import Line2D\nlegend_elements = [Line2D([0], [0], color='red', lw=4, label='Final predictions with second level model'),\n                   Line2D([0], [0], marker='o', label='Outliers in real values',\n                          markerfacecolor='orange',color='#999999', markersize=15),\n                   Line2D([0], [0], marker='o',color='#999999', label='Real values',\n                          markerfacecolor='black', markersize=15),\n                   ]\nplt.figure(figsize=(14,10))\nfor c,i in enumerate(df['target'].values):\n    if i in outliers['target'].values:\n        plt.scatter(c,i,color='orange',label='Outliers in real values')\n    else:\n        plt.scatter(c,i,color='black',label='Real values')\nplt.plot(range(len(df)),lr_second_lvl.predict(first_lvl_features),color='red')\nplt.legend(handles=legend_elements)\nplt.xlabel('Timestamps')\nplt.ylabel('Target values')\nplt.title('Results of predictions using second level model');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\nplt.figure(figsize=(14,10))\nplt.scatter(range(len(df))[-24:],df['target'][-24:],color='black')\nplt.plot(range(len(df))[-24:],lr_second_lvl.predict(first_lvl_features)[-24:],color='red')\nplt.legend(['Final predictions with second level model','Real values'])\nplt.xlabel('Monthes')\nplt.ylabel('Target values')\nplt.title('Results of predictions using second level model (showing data only for last two years)')\nplt.yticks(np.arange(min(df['target'][-24:]),max(df['target'][-24:])+0.5,0.5))\nplt.xticks(range(len(df))[-24:],range(1,25));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making submission"},{"metadata":{},"cell_type":"markdown","source":"We are now ready to make the final submission using our model stacking technique."},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = pd.read_csv(\"../input/prediction-bod-in-river-water/test.csv\",usecols=['Id','1','2'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We still need to create the additional feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df['combined'] = sub_df[sub_df.columns.to_list()[1:]].mean(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_columns = sub_df.columns.to_list()[1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble_df_test = pd.DataFrame()\nensemble_df_test['gbd'] = gbd_trained.predict(sub_df[feature_columns])\nensemble_df_test['extra_tree'] = ex_tree_reg.predict(sub_df[feature_columns])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble_df_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finnaly we can predict the final value."},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df['Predicted'] = lr_second_lvl.predict(ensemble_df_test.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = sub_df[['Id','Predicted']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df['Predicted'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We hope, that this notebook was useful for you."},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}