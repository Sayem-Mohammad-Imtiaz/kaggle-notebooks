{"cells":[{"metadata":{"_uuid":"6b9ec4dbd47dfe440e827ec73ed6f7f66af54197"},"cell_type":"markdown","source":"# Bolsonaro, before running for president and after beginning the campaign\nSome say that Bolsonaro changed a lot and became moderate to light compared to his previous behaviour, let's investigate that!\n\nThe official announcement that Jair Bolsonaro was going to be running for president in 2018 was in July 22, 2018 (https://g1.globo.com/politica/eleicoes/2018/noticia/2018/07/22/psl-confirma-candidatura-de-jair-bolsonaro-a-presidencia-da-republica.ghtml), but he was clearly doing campaign way earlier as confirmed by some journalists (https://especiais.gazetadopovo.com.br/eleicoes/2018/campanha-presidente-jair-bolsonaro-presidencial/).\n\nI will compare Bolsonaro pre-2018 and after-2018 (2018 included in this set)."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"208971545956afa3a0eb5d1d9ea45dffc0fb51c5"},"cell_type":"code","source":"import pandas as pd\nimport scattertext as st\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c01464f9833cf6ab809cc9b9cce73d04ef9574d3"},"cell_type":"code","source":"df = pd.read_csv('../input/jair-bolsonaro-twitter-data/bolsonaro_tweets.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f92e22b07d20df33ad8d92287ca58da75d9c820","scrolled":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"031e94fe84effb889057baca6ccd8864009a1ad4"},"cell_type":"code","source":"df_before = df[df['date'] < '2018-01-01'].copy()\ndf_before.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07fa63ee8666c932689ad6e17243e66a5baec410"},"cell_type":"code","source":"df_after = df[df['date'] >= '2018-01-01'].copy()\ndf_after.tail()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10fb6b8fd3e17202e5aacb25d0d711f9c8eb45de"},"cell_type":"markdown","source":"# Cleaning the text\n\nLet's do some cleaning on the text before doing word clouds and using the scatter text library for visualization"},{"metadata":{"trusted":true,"_uuid":"8e2a8cc85aa57133f24a5437276f87ea7e6622dc"},"cell_type":"code","source":"def clean_df(df_clean):\n    remove_names = False # if True, assumes you have a nomes.txt file with common brazilian names in your current dir\n    remove_usernames = False\n    \n    # Copy the original text for later metadata\n    df_clean['original_text'] = df_clean['text']\n\n    # Lower case\n    df_clean['text'] = df_clean['text'].apply(\n        lambda x: \" \".join(x.lower() for x in x.split()))\n\n    # Remove usernames\n    if remove_usernames:\n        df_clean['text'] = df_clean['text'].str.replace(\n            '@[^\\s]+', \"\")\n\n    # Remove links\n    df_clean['text'] = df_clean['text'].str.replace(\n        'https?:\\/\\/.*[\\r\\n]*', '')\n\n    # Remove punctuation\n    df_clean['text'] = df_clean['text'].str.replace(\n        '[^\\w\\s]', '')\n\n    # Remove stopwords\n    from nltk.corpus import stopwords\n    stop = stopwords.words('portuguese')\n    df_clean['text'] = df_clean['text'].apply(\n        lambda x: \" \".join(x for x in x.split() if x not in stop))\n\n    # Remove common brazilian names\n    if remove_names:\n        nomes = pd.read_csv('nomes.txt', encoding='latin', header=None)\n        lista_nomes = (nomes[0].str.lower()).tolist()\n        df_clean['text'] = df_clean['text'].apply(lambda x: \" \".join(\n            x for x in x.split() if x not in lista_nomes))\n\n    # Remove numbers\n    df_clean['text'] = df_clean['text'].str.replace(\n        '\\d+', '')\n\n    # Remove words with 1-3 chars\n    df_clean['text'] = df_clean['text'].str.replace(\n        r'\\b(\\w{1,3})\\b', '')\n\n    # Replace accents and ç\n    df_clean.text = df_clean.text.str.normalize('NFKD')\\\n        .str.encode('ascii', errors='ignore')\\\n        .str.decode('utf-8')\n    \n    return df_clean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c8043c6bfe36038ffae3376a043383ef14af0821"},"cell_type":"code","source":"df_before = clean_df(df_before)\ndf_before.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97edda0c8113e5fe22fe6acd121a5630148e7e85"},"cell_type":"code","source":"df_after = clean_df(df_after)\ndf_after.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07620940e3acfbe1844ccb9305ed82397b983862"},"cell_type":"markdown","source":"We see that some tweets disappeared as they were just emoji. We won't bother cleaning these rows as our libraries won't take them in consideration anyways. A future idea that we could implement is to substitute each emoji by a word that describes it.\n\n# Word clouds\n\nWe're going to use [this](https://github.com/amueller/word_cloud) word cloud library to provide a beautiful visualization. I will keep the background _white_ in the _before_ dataframe and **dark** in the **after** dataframe just to help us visualize."},{"metadata":{"trusted":true,"_uuid":"6bcb01da69f9ae3dfe28301fd93710befd441750"},"cell_type":"code","source":"text = \" \".join(review for review in df_before.text)\nwordcloud = WordCloud(\n    width=3000,\n    height=2000,\n    background_color='white').generate(text)\nfig = plt.figure(\n    figsize=(40, 30))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ead15ba54799b3dd921f89098ef490f208b1ecf"},"cell_type":"code","source":"text = \" \".join(review for review in df_after.text)\nwordcloud = WordCloud(\n    width=3000,\n    height=2000,\n    background_color='black').generate(text)\nfig = plt.figure(\n    figsize=(40, 30))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee3faa54023fa381158d94a4ee1463311d20d86c"},"cell_type":"markdown","source":"# ScatterText\n\nWe will investigate the differences in the corpus using [this](https://github.com/JasonKessler/scattertext) scattertext library.\n\nLet's prepare the data by combining the dataframes in one and creating a \"metadata\" column that will help us discern the tweets for further investigation"},{"metadata":{"trusted":true,"_uuid":"bed31f1ff3122bda028dca1ff9baff310d0110c8"},"cell_type":"code","source":"df_before['metadata'] = df_before.date.map(str) + \" | \" + df_before.original_text\ndf_after['metadata'] = df_after.date.map(str) + \" | \" + df_after.original_text\n\ndf_before['category'] = 'Before'\ndf1 = df_before[['metadata', 'category', 'text']]\n\ndf_after['category'] = 'After'\n\ndf2 = df_after[['metadata', 'category', 'text']]\n\ndf_combined = df1.append(df2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d98131b455d58d76909fce83289d6512bfc64100"},"cell_type":"markdown","source":"Ok, now let's use the library to compare the corpus. The code will generate an .html file on your current folder. I will post some screenshots, but I suggest that you access a live version [here](https://s3.amazonaws.com/scatter-bolsonaro-before-after-2018/bolsonaro_before_vs_after2018.html) and play around with it. The scattertext library is awesome!"},{"metadata":{"trusted":true,"_uuid":"eb1e2fc6a946091ca93b49a5ba33b5c579c42868"},"cell_type":"code","source":"corpus = (st.CorpusFromPandas(df_combined,\n                                  category_col='category',\n                                  text_col='text',\n                                  nlp=st.whitespace_nlp_with_sentences)\n              .build()\n              .get_unigram_corpus()\n              .compact(st.ClassPercentageCompactor(term_count=1,\n                                                   term_ranker=st.OncePerDocFrequencyRanker)))\nhtml = st.produce_characteristic_explorer(\n    corpus,\n    category='Before',\n    category_name='Before',\n    not_category_name='After',\n    metadata=corpus.get_df()['metadata']\n)\nopen('bolsonaro_before_vs_after2018.html', 'wb').write(html.encode('utf-8'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b45d57a5f55913d77bceeb6c766960ea5d5d2d6"},"cell_type":"markdown","source":"Below you can see the scatter plot generated by our code. The 'y' axis is the Rank Difference, on top you can see the words that were used more by Bolsonaro before 2018 and less after 2018. On the bottom you see the words used more by the current president after 2018 and less before 2018. The middle line consists of words that were used evenly on both periods. The 'x' axis is the Characteristic to Corpus, that shows how frequent the words are present in this data."},{"metadata":{"trusted":true,"_uuid":"9678b0f7ce31442fbc0cdddea1892f78dc1e9cda"},"cell_type":"code","source":"Image(filename='../input/screenshots/scatter.png')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30e377aa0e3136796bc341c4840918af8313aa0d"},"cell_type":"markdown","source":"The term \"câmara\" (Chamber of Deputies in portuguese) was the top term before 2018 from Bolsonaro on twitter, which is reasonable since he was Federal Deputy for Rio de Janeiro from 1991 to 2018."},{"metadata":{"trusted":true,"_uuid":"2797fa07f424879d9110da8b7e9700bfee3d6045"},"cell_type":"code","source":"Image(filename='../input/screenshots/camara.png')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6795d11c4658dda21b87649e02363a8c065735e0"},"cell_type":"markdown","source":"The term \"forte\" (strong in portuguese) was the top term after 2018. One of the cool features of scattertext is that you can click on the word, or type it on the search box and see the frequency it appears in each category and even see where it appears with the metadata we created.\n\nInvestigating further this term we can see that it's almost always followed by \"abraço\" (hug in portuguese), a characteristic expression used by Bolsonaro: \"forte abraço!\" (big hug! or something like that)"},{"metadata":{"trusted":true,"_uuid":"b6d4c72827776d14bc336f99a31e7f20c5579ad9"},"cell_type":"code","source":"Image(filename='../input/screenshots/forte.png')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1be2277584e63ae9c8dd2059424edacf8a056b42"},"cell_type":"markdown","source":"We can see that 'verdade' (truth in portuguese) is the most characteristic word of this corpus."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"5381315aee1a7fc5609868b84e6850b786cc467c"},"cell_type":"code","source":"Image(filename='../input/screenshots/verdade.png')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a003001b29a4e3df2be56503e7db351232444ca"},"cell_type":"markdown","source":"# What if we tryed to create a model?\nIn kaggle, no kernel is complete without some type of classification. Let's try to create a model to classify a tweet between the two classes of our hypothesis. \nLet's prepare the data by assigning a target column with **0s** to the before-2018 class and **1s** to the after-2018 class."},{"metadata":{"trusted":true,"_uuid":"026964ec38a7540f9da3b2b6d804ec3a53bb0b26"},"cell_type":"code","source":"# Removing the empty rows from our datasets\ndf1 = df1[df1['text'] != '']\ndf2 = df2[df2['text'] != '']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"70501b750d899d0a75f1f2c573a1bfa5dacd44c6"},"cell_type":"code","source":"df1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17a0d6769bc399c7bbf1e5409a557d1747d50031"},"cell_type":"code","source":"df2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6abdebabbb6d16a3c940874de6da4378787a7290"},"cell_type":"code","source":"y = np.append(np.zeros(3104), np.ones(2133))\ny","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ab0ca31b31d04273f06dc082f6e87b1e9e6ec19"},"cell_type":"code","source":"text_array = np.append(df1['text'].values, df2['text'].values)\nlen(text_array)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92e749d322714d1eb65b2c6cfb6f13dfe1d703e0","scrolled":false},"cell_type":"code","source":"# Creating the Bag of Words model\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features = 1000) # here we only use 1500 most frequent words to reduce sparcity, we could also use dimensionality reduction for this\nX = cv.fit_transform(text_array).toarray()\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n\n# Fitting a Random Forest Classifier to the training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 100, random_state = 0)\nclassifier.fit(X_train, y_train)\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Evaluating our results and robustness of our model\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(accuracies)\nprint(accuracies.mean())\nprint(accuracies.std())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"151264bf30d8f8a78b2f9e28a4165db337b0cbe9"},"cell_type":"markdown","source":"Accuracies in the high 60s and the variance is ok..."},{"metadata":{"_uuid":"28cbc75f8618d1112d03529cae03ad799e8f4f5a"},"cell_type":"markdown","source":"# Testing with new tweets\nLet's take new tweets from Bolsonaro and run our model on it. Will our model be able to predict these were tweeted after 2018? \n\nI scraped all tweets from @jairbolsonaro from 2019-01-26 until 2019-03-07 and uploaded here as .csv"},{"metadata":{"trusted":true,"_uuid":"5ab07a2bf9781a6de8eaaf272780f3eb9e67ca70"},"cell_type":"code","source":"new_df = pd.read_csv('../input/jb-20190126-20190308csv/jb_20190126_20190308.csv')\nnew_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6da1631c3a3a2446ae795b5ea15fcf99e815c33"},"cell_type":"code","source":"new_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9978428ab6c0227f213e196b925863fdfa3352b1"},"cell_type":"markdown","source":"We need to preprocess our text as we did with our training data."},{"metadata":{"trusted":true,"_uuid":"08a6ee1a5467dcb6c3563aee64964333d11423bc"},"cell_type":"code","source":"new_df = clean_df(new_df)\nnew_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"563668d813de255ce23ad5969456bc8cb0734e0a"},"cell_type":"code","source":"new_df = new_df[new_df['text'] != '']\nnew_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3d211ed4cb0741c36ecd14622a8b0973eb30f5b"},"cell_type":"code","source":"text_array = new_df['text'].values\ny = np.ones(207)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1f78f1b45929e364aea7688955bfcec50c7d99a"},"cell_type":"markdown","source":"# Let's test our model!"},{"metadata":{"trusted":true,"_uuid":"99b4aae5dbd5fef470d850e7f8ebd6510220183d"},"cell_type":"code","source":"# Creating the Bag of Words\nX = cv.fit_transform(text_array).toarray()\n\ny_pred = classifier.predict(X)\n\ncm = confusion_matrix(y, y_pred)\nprint(cm)\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(y, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"168cc2bc282c077816e295fc7c5dfa65dcd1d32b"},"cell_type":"markdown","source":"The accuracy is close to our training set! Haha, at least we can say we beat chance!\n\nBut... We need to check if our model predicted one thing correctly..."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"87e98d12e70b12d3cb08601d8f5319b7a9a30702"},"cell_type":"code","source":"text_array[19]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be61be60c3820b0f356c5ba8fb819a79822104c0"},"cell_type":"code","source":"y_pred[19]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8b3ca5ac218615ce3a04be2577e1dff3d0fdeee"},"cell_type":"markdown","source":"=(\n\n# Conclusion\n\nI suggest you guys to play around with the .html and investigate your hypothesis supported by the data, as we should always do!\n\n## Model\nOur model is undoubtedly production ready! /s\n\nWe could have also used grid search to fine tune the hyperparameters of our classifier and test other classification algorithms. \n\nJokes apart, NLP is fascinating and research on it is advancing quickly, especially with deep learning. I encourage you to try new techniques on this datasets.\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}