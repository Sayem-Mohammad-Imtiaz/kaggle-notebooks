{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Classifying Mushrooms with a Decision Tree\n\n\nThis is a beginner friendly introduction to making a decision tree classifier to predict whether a mushroom is edible or not. \n\nIt accompanies my blog article on Decision Trees, which you can find [here](https://madelinecaples.hashnode.dev/if-mushrooms-grew-on-trees). "},{"metadata":{},"cell_type":"markdown","source":"### Importing libraries "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading Data "},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_PATH = '/kaggle/input/mushroom-classification/'\n\nmushrooms = pd.read_csv(DATA_PATH + 'mushrooms.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check our pandas dataframe to make sure the data was properly loaded\nmushrooms.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We want to see what kind of features our data has. From our dataframe (above) we can see that all of the data appears to be categorical (as opposed to numerical).\n\n\nLet's take a look at the columns in our dataframe. This will tell us what kind of features we are dealing with."},{"metadata":{"trusted":true},"cell_type":"code","source":"mushrooms.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## What are we predicting? \n\nWe are predicting the class of the mushroom (found in `mushrooms['class']`). Specifically, we want to know whether a mushroom is edible or poisonous (e or p), based on it's features. Things like it's odor, habitat, capt shape, color, etc.  "},{"metadata":{},"cell_type":"markdown","source":"### Overview of what we will be doing: \n\n1. Check distribution of data\n2. Split the data into X and y \n3. Encode data \n4. Get a baseline model: we'll use a decision tree \n5. Evaluate the model: check the loss, score, and feature importances\n6. Remove the features that have a low importance \n7. Create a new model without as many features"},{"metadata":{},"cell_type":"markdown","source":"### Check distribution of data\n\nWe want to check to make sure our data is distributed pretty evenly across the two classes we have. This will tell us if our dataset is **balanced**."},{"metadata":{"trusted":true},"cell_type":"code","source":"x = mushrooms['class']\nax = sns.countplot(x=x, data=mushrooms)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have slightly more instances of edible mushrooms than poisonous mushrooms, but the difference isn't so great that we will worry about it. "},{"metadata":{},"cell_type":"markdown","source":"## Split the data\n\nWe need to split our data into X - **features** and y - the **target**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split into features and labels \nX = mushrooms.drop(\"class\", axis=1)\ny = mushrooms[\"class\"]\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encode the data \n\nOur machine learning model won't understand the categorical data that we have, unless we turn it into numbers. This process is called **encoding**. \n\nWe are going to use Pandas categorical method to do this. This will turn the letters in our categorical data into a different number for each unique letter. "},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = X.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in columns: \n    X[col] = X[col].astype('category').cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we'll do the same for the label y."},{"metadata":{"trusted":true},"cell_type":"code","source":"y = y.astype('category').cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Baseline Decision Tree\n\nNow that our data is encoded into numbers we are ready to make our decision tree. \n\nWe'll have to import another library, sklearn to do this. We'll also import the DecisionTreeClassifier, and train_test_split from sklearn, so that we can break our data up into training and test sets. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn \nfrom sklearn.model_selection import train_test_split \nfrom sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"baseline = DecisionTreeClassifier(random_state=42) # setting the random state will ensure that we get the same results each time\nbaseline.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate baseline "},{"metadata":{},"cell_type":"markdown","source":"Note that in the following section I will be comparing some different examples of the Decision Tree trained on our mushrooms dataset. While I will include scoring for the sake of comparison, I am not really trying to optimize the model. Instead I am playing around with different parameters to give you an idea of the capabilities of the Decision Tree Classifier, and what different trees will look like. "},{"metadata":{},"cell_type":"markdown","source":"To evaluate this baseline model we will look at the following: \n\n* Accuracy \n* Precision \n* Recall\n* F1 Score\n\nWe can visualize those all at once by using the `metrics.classification_report` functionality that is built into sklearn. \n\nWe'll also visualize: \n\n* Tree \n* Feature importance"},{"metadata":{},"cell_type":"markdown","source":"### A few functions to make life a little easier: "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Okay I give in, let's just turn this piece of code into a function!\nfrom sklearn import tree\ndef show_tree(model): \n    fig = plt.figure(figsize=(30,25))\n    ann = tree.plot_tree(model,\n                       feature_names=mushrooms.columns, \n                       class_names=mushrooms[\"class\"],\n                   filled=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\ndef print_classification_report(model, X_test, y_test): \n    y_preds = model.predict(X_test)\n    print(metrics.classification_report(y_test, y_preds, target_names=['edible', 'poisonous']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tip: don't forget to add the \"print\" or it will look weird and the columns won't line up!\nprint_classification_report(baseline, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So if our model scored perfectly on precision, recall, accuracy... doesn't that mean our work is done? Well, no. Not really. A perfect score is a sign of overfitting. The decision tree has classified everything a little too specifically by memorizing the training data, and is probably over optimizing at each split in the tree. "},{"metadata":{},"cell_type":"markdown","source":"### Visualize the tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"show_tree(baseline)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get feature importances \n\n# Define a function so we can use it again later \ndef random_forest_feature_importance(model, df): \n  return pd.DataFrame({'cols': df.columns, 'imp': model.feature_importances_}).sort_values('imp', ascending=False)\n\nfeature_importance = random_forest_feature_importance(baseline, X)\nfeature_importance[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_feat_imp(feat_importance):\n    return feat_importance.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n\nplot_feat_imp(feature_importance[:30]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see with this plot, many of our features are barely doing anything at all for the model. The most important features appear to be gill-color, spore-print-color, population, gill-size, and odor. Bruises and habit are of almost equal importance, and it goes down from there. \n\nWe are going to try to create a more robust tree by trimming away the less importance features. \n\nNote how the decision tree has been very helpful in discovering the features that we can pretty safely cut out of our model, without the accuracy suffering. We wouldn't have known this before making the decision tree, except perhaps with some expert knowledge of mushrooms. "},{"metadata":{},"cell_type":"markdown","source":"## Tinkering with the Decision Tree \n\n* Get rid of features that aren't serving the model well \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"to_keep = feature_importance[:4].cols\nlen(to_keep)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train[to_keep]\nX_test = X_test[to_keep]\n\nlen(X_train.columns), len(X_test.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_4features = DecisionTreeClassifier(random_state=42)\nmodel_4features.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_classification_report(model_4features, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our model is still doing quite well, but without as many features, it isn't achieving a perfect score. This is actually a sign that we might not be overfitting anymore. "},{"metadata":{"trusted":true},"cell_type":"code","source":"show_tree(model_4features)\n# Tip: double click to see a larger version of the plot","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although this time we only have 4 features, because we still have a lot of samples, the tree is still pretty deep. Now Let's try to limit the depth of the tree. "},{"metadata":{"trusted":true},"cell_type":"code","source":"model_shallow = DecisionTreeClassifier(max_depth=4, random_state=42)\nmodel_shallow.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Setting max depth to 4 means that our tree will only be 4 nodes deep. Let's see if that significantly harmed the model's accuracy, precision, and recall. "},{"metadata":{"trusted":true},"cell_type":"code","source":"print_classification_report(model_shallow, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will try setting the min_samples_split. This controls the minimum number of samples that are required to split an *internal node* (aka *branch*). "},{"metadata":{"trusted":true},"cell_type":"code","source":"show_tree(model_shallow)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_min_split = DecisionTreeClassifier(min_samples_split=35, random_state=42)\nmodel_min_split.fit(X_train, y_train)\nshow_tree(model_min_split)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_classification_report(model_min_split, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### That's it for now! \n\nPlease feel free to make a copy of this notebook and play around on your own with the DecisionTreeClassifier. There are a lot of other parameters that you can tinker with to see how it effects the tree. I hope this gives you a little bit of an idea about decision trees and how they can be useful to classifying data. \n\nThank you for reading. Please leave me a comment with suggestions for future blog posts, if you'd like. \n\n### Further Reading\n\nDon't forget you can check out the blog article that accompanies this notebook [here](https://madelinecaples.hashnode.dev/if-mushrooms-grew-on-trees). \n\nAlso check out the [Sklearn Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}