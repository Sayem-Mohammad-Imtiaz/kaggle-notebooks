{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":"# Import the necessary libraries"},{"metadata":{"trusted":true,"_uuid":"d6fb32fd69316596e236eab5fb8cf77c848508c3"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom gensim.models import Word2Vec\nimport multiprocessing \nimport keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.models import Model\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.utils import to_categorical\nfrom keras.callbacks import EarlyStopping\nimport nltk\n!pip install tweet-preprocessor\nfrom nltk.corpus import stopwords\nimport preprocessor as p\nfrom nltk.tokenize import sent_tokenize as sent\nfrom nltk.tokenize import word_tokenize as word\n#nltk.download('stopwords')\nfrom nltk.stem import PorterStemmer \nimport re\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_excel('/kaggle/input/personality-data/mbti9k_comments250.xlsx',nrows=50)\nprint(len(df))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f674695f1742479cefdeec0e81ab469f7b6ec90f"},"cell_type":"markdown","source":"### Load the data into Pandas dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\nBAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\nSTOPWORDS = set(stopwords.words('english'))\nFLOAT=re.compile('[-+]?\\d*\\.\\d+|\\d+')\nps = PorterStemmer() \ndef clean_text(text):\n    \"\"\"\n        text: a string\n        \n        return: modified initial string\n    \"\"\"\n    text = FLOAT.sub('', text)\n    text = text.lower() # lowercase text\n    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n    \n    text = text.replace('x', '')\n#    text = re.sub(r'\\W+', '', text)\n    text = ' '.join(ps.stem(word) for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n    return text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prepared data for word embedding"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_text=df['comment']\n#raw_text=data['body']\ncleaned_text=[]\nword_text=[]\nf_text=[]\n#print(datetime.now(pytz.timezone('Asia/Calcutta')).strftime('%Y-%m-%d %H:%M:%S'))\nfor i in range(0,len(raw_text)):\n    text1=p.clean(raw_text[i])\n    sent_text=sent(text1)\n    for sen in sent_text:\n        text=clean_text(sen) \n        text=word(text) #Tokenize into words\n        f_text.append(text)  #Preparing input for word to vector model\n    #if i==2:\n    #print(raw_text[i])\n        #print('\\n\\n\\n')\n        #print(f_text)\n        #break\nprint(len(f_text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_label1=df['type']\nraw_label=[y.lower() for y in raw_label1]\nlabel=[]\nfor x in raw_label:\n    if x=='intj':\n     label.append(1)\n    else:\n     label.append(0)\n        \n    \nprint(len(label) )   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMB_DIM=300\nw2v=Word2Vec(f_text,size=EMB_DIM,window=5,min_count=5,negative=15,iter=10,sg=0,workers=multiprocessing.cpu_count())\nmodels=w2v.wv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models.save_word2vec_format('model_sg.bin')\nmodels.save_word2vec_format('model_sg.txt', binary=False)\nresult=models.similar_by_word('friend')\nprint(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_val=[]\nfor i in range(0,len(raw_text)):\n    text1=p.clean(raw_text[i])\n    text=clean_text(text1) \n    data_val.append(text)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The maximum number of words to be used. (most frequent)\nMAX_NB_WORDS = 50000\n# Max number of words in each complaint.\nMAX_SEQUENCE_LENGTH = 400\n# This is fixed.\nEMBEDDING_DIM = 100\ntokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(data_val)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = tokenizer.texts_to_sequences(data_val)\nX = sequence.pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', X.shape)\nprint(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y=label","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"150e244a39b814d8a41bbe0e419bc5f28e457dd6"},"cell_type":"markdown","source":"Split into training and test data."},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"aa3386af09469682c66cc53a1830a4e42f0e70b6"},"cell_type":"code","source":"X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.15)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c5378d55c271e01480c1ac07f94ff99a80f900d6"},"cell_type":"markdown","source":"### Process the data\n* Tokenize the data and convert the text to sequences.\n* Add padding to ensure that all the sequences have the same shape.\n* There are many ways of taking the *max_len* and here an arbitrary length of 150 is chosen."},{"metadata":{"trusted":true,"_uuid":"bdca14f2b8cd7bd7cb5ee66fd40ea522217c03c6"},"cell_type":"code","source":"max_words = 1000\nmax_len = 150\ntok = Tokenizer(num_words=max_words)\ntok.fit_on_texts(X_train)\nsequences = tok.texts_to_sequences(X_train)\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad8706caa7a447fb49b44919fd109129e4082a93"},"cell_type":"markdown","source":"### RNN\nDefine the RNN structure."},{"metadata":{"trusted":true,"_uuid":"78fff25b8be1de575bff071a2027f3dd2b11b911"},"cell_type":"code","source":"def RNN():\n    inputs = Input(name='inputs',shape=[MAX_SEQUENCE_LENGTH])\n    layer = Embedding(MAX_NB_WORDS,EMBEDDING_DIM,input_length=MAX_SEQUENCE_LENGTH)(inputs)\n    layer = LSTM(64)(layer)\n    layer = Dense(256,name='FC1')(layer)\n    layer = Activation('relu')(layer)\n    layer = Dropout(0.5)(layer)\n    layer = Dense(1,name='out_layer')(layer)\n    layer = Activation('sigmoid')(layer)\n    model = Model(inputs=inputs,outputs=layer)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d7c489e32bff6d12b8c08c07a91e9ba5d302e0e"},"cell_type":"markdown","source":"Call the function and compile the model."},{"metadata":{"trusted":true,"_uuid":"a0ede32d4127e8b4990fd74fe97fadef9e565d17"},"cell_type":"code","source":"model = RNN()\nmodel.summary()\nmodel.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc2e0a3ec50d14c790b82d66f9255456ec6a69da"},"cell_type":"markdown","source":"Fit on the training data."},{"metadata":{"trusted":true,"_uuid":"98f6d6318352420ea49c532cda158f715f940f4b"},"cell_type":"code","source":"model.fit(X_train,Y_train,batch_size=128,epochs=10,\n          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"448ab38c2f804e47df48eb45385393aaec168032"},"cell_type":"markdown","source":"The model performs well on the validation set and this configuration is chosen as the final model."},{"metadata":{"_uuid":"ccca7839445a7d663ee7bc425a16e247df3e0e5b"},"cell_type":"markdown","source":"Process the test set data."},{"metadata":{"_uuid":"0b60d7d2bcc0aabf77c8c8766c59f8d73cd34547"},"cell_type":"markdown","source":"Evaluate the model on the test set."},{"metadata":{"trusted":true,"_uuid":"0db183049b59d96388812a98efedfc865b7cc141"},"cell_type":"code","source":"accr = model.evaluate(X_test,Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e121ab83f4a0b9f7376ab24aa25d67051171f89"},"cell_type":"code","source":"print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}