{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Parsing from 'Data File -> 0_subject'\nsubject_0_path = '../input/ppg-data/PPG-BP Database/Data File/0_subject'\nsubject_0_df = pd.DataFrame()\n\nfor root, dirs, files in os.walk(subject_0_path):\n    for file in tqdm(files):\n        df = pd.read_csv(os.path.join(root, file), sep='\\t', header=None).transpose()\n        \n        df.set_axis([file.split('.')[0]], axis=1, inplace=True)\n        subject_0_df = pd.concat([subject_0_df, df], axis=1)\n        \nsubject_0_df.dropna(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subject_0_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=subject_0_df.skew()\ny=y.where(y>0.7)\ny=y.where(y<1.25)\nsk=pd.DataFrame(y)\nsk.transpose()\nsk.dropna(inplace=True)\nsk=sk.transpose()\nsk","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sk.columns.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sk.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf_min_max_scaledkk = subject_0_df.copy() \n  \n \ncolumnn= df_min_max_scaledkk.columns.values\nfor column in columnn:\n    skeww=df_min_max_scaledkk[column].skew()\n    if skeww<0.7 or skeww>1.25:\n        df_min_max_scaledkk.drop([column], axis=1, inplace=True)\n\ndf_min_max_scaledkk        \n        \n   \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subject_0_df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscalar = StandardScaler()\n\nsubject_0_df1=subject_0_df.copy()\nz=subject_0_df.values\n\n\nscalar.fit(z)\nx_normalized = scalar.transform(z)\nsubject_0_df1= pd.DataFrame(x_normalized)\n\nsubject_0_df1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf_min_max_scaled = subject_0_df.copy() \n  \n \ncolumns= df_min_max_scaled.columns.values\nfor column in columns:\n    df_min_max_scaled[column] = (df_min_max_scaled[column] - df_min_max_scaled[column].mean()) / df_min_max_scaled[column].std()     \n   \ndisplay(df_min_max_scaled)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.signal import butter, lfilter\n\n\nfiltered_sig =df_min_max_scaled.copy() \n\ncutoff=15\nnyquist = 1000*0.5\nwn = cutoff/nyquist\nb, a = butter(7, wn, btype='lowpass')\nfor column in columns:\n    filtered_sig[column] = lfilter(b, a, filtered_sig[column].values)\nfiltered_sig","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nsns.lineplot(filtered_sig.index, filtered_sig['259_1'])\nsns.lineplot(filtered_sig.index, filtered_sig['120_1'])\nsns.lineplot(filtered_sig.index, filtered_sig['106_1'])\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom scipy import signal\nfiltered_sig1 = df_min_max_scaled.copy() \n\nL=40\nb = (np.ones(L))/L \na = np.ones(1)  \nfor column in columns:\n     filtered_sig1[column] = signal.lfilter(b,a,filtered_sig1[column]) \n\nfiltered_sig1\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\n\nsns.lineplot(filtered_sig1.index, filtered_sig1['259_1'])\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,8))\n\nsns.lineplot(df_min_max_scaled.index, df_min_max_scaled['41_2'])\n\nsns.lineplot(filtered_sig.index, filtered_sig['41_2'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,8))\n\nsns.lineplot(filtered_sig.index, filtered_sig['239_2'])\n\nsns.lineplot(filtered_sig.index, filtered_sig1['239_2'])\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,8))\n\nsns.lineplot(filtered_sig.index, filtered_sig['239_1'])\nsns.lineplot(filtered_sig.index, filtered_sig1['239_1'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,8))\n\nsns.lineplot(filtered_sig.index, filtered_sig['201_2'])\nsns.lineplot(filtered_sig.index, filtered_sig1['201_2'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,8))\n\nsns.lineplot(filtered_sig.index, filtered_sig['228_3'])\n\nsns.lineplot(filtered_sig.index, filtered_sig1['228_3'])\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,8))\n\nsns.lineplot(filtered_sig.index, filtered_sig['415_3'])\nsns.lineplot(filtered_sig.index, filtered_sig1['415_3'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,8))\n\nsns.lineplot(filtered_sig.index, filtered_sig['41_2'])\n\nsns.lineplot(filtered_sig.index, filtered_sig1['41_2'])\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,8))\n\nsns.lineplot(filtered_sig.index, filtered_sig['197_2'])\n\nsns.lineplot(filtered_sig.index, filtered_sig1['197_2'])\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,8))\n\nsns.lineplot(filtered_sig.index, filtered_sig['142_3'])\n\nsns.lineplot(filtered_sig.index, filtered_sig1['142_3'])\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,8))\n\nsns.lineplot(filtered_sig.index, filtered_sig['203_2'])\n\nsns.lineplot(filtered_sig.index, filtered_sig1['203_2'])\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,8))\n\nsns.lineplot(filtered_sig.index, filtered_sig['246_2'])\n\nsns.lineplot(filtered_sig.index, filtered_sig1['246_2'])\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,8))\n\nsns.lineplot(filtered_sig.index, filtered_sig['415_2'])\n\nsns.lineplot(filtered_sig.index, filtered_sig1['415_2'])\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,8))\n\nsns.lineplot(filtered_sig.index, filtered_sig['54_1'])\n\nsns.lineplot(filtered_sig.index, filtered_sig1['54_1'])\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,8))\n\nsns.lineplot(filtered_sig.index, filtered_sig['141_2'])\n\nsns.lineplot(filtered_sig.index, filtered_sig1['141_2'])\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,8))\n\nsns.lineplot(filtered_sig.index, filtered_sig['418_3'])\n\nsns.lineplot(filtered_sig.index, filtered_sig1['418_3'])\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,8))\n\nsns.lineplot(filtered_sig.index, filtered_sig['189_1'])\n\nsns.lineplot(filtered_sig.index, filtered_sig1['189_1'])\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,8))\n\nsns.lineplot(filtered_sig.index, filtered_sig['198_3'])\nsns.lineplot(filtered_sig.index, filtered_sig1['198_3'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\nsns.lineplot(df_min_max_scaled.index, df_min_max_scaled['259_1'])\n\nsns.lineplot(filtered_sig1.index, filtered_sig1['259_1'])\n\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=filtered_sig.index.values\ny=filtered_sig['111_2'].values\n\npoly_co=np.polyfit(x,y,4)\nprint(poly_co)\nynew=np.poly1d(poly_co)\ne=ynew(x)\nplt.plot(x,ynew(x),x,y)\ny1=filtered_sig['111_2'].values-ynew(x)\nprint(y1)\nprint(y1.shape)\nprint(e)\nprint(e.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\n\nsns.lineplot(filtered_sig.index, filtered_sig['111_2'])\nsns.lineplot(x,y1)\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nX = filtered_sig.index\nX = np.reshape(X, (len(X), 1))\ny = filtered_sig['24_1'].values\nmodel = LinearRegression()\nmodel.fit(X, y)\ntrend = model.predict(X)\nplt.plot(y)\nplt.plot(trend)\nplt.legend(['data', 'trend'])\nplt.show()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"detr = [y[i] - trend[i] for i in range(0, len(y))]\nplt.plot(detr)\nplt.title('data detrended in a linear fashion')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pf = PolynomialFeatures(degree=4)\nXp = pf.fit_transform(X)\nXp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"md2 = LinearRegression()\nmd2.fit(Xp, y)\ntrendp = md2.predict(Xp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(X, y)\nplt.plot(X, trendp)\nplt.legend(['data', 'polynomial trend'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"detrpoly = [y[i] - trendp[i] for i in range(0, len(y))]\nplt.plot(X, detrpoly)\nplt.title('polynomially detrended data')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pywt\nimport sys\n\ndatarec=df_min_max_scaled.copy()\nindex=df_min_max_scaled.index.values\nfor column in columns:\n    data1=datarec[column].values\n    w = pywt.Wavelet('db8')\n    maxlev = pywt.dwt_max_level(len(data1), w.dec_len)\n    threshold =1.11# Threshold for filtering\n    # Decompose into wavelet components, to the level selected:\n    coeffs = pywt.wavedec(data1, 'db8', level=maxlev)\n    #cA = pywt.threshold(cA, threshold*max(cA)\n    for i in range(1, len(coeffs)):\n        coeffs[i] = pywt.threshold(coeffs[i], threshold*max(coeffs[i]))\n        \n        \n    datarec[column] = pywt.waverec(coeffs, 'db8')\n    \n    \ndatarec\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\n\n\nsns.lineplot(filtered_sig.index, datarec['259_2'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\n\n\nsns.lineplot(filtered_sig.index, datarec['201_2'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\n\n\nsns.lineplot(filtered_sig.index, datarec['239_2'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\n\n\nsns.lineplot(filtered_sig.index, datarec['239_1'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\n\n\nsns.lineplot(filtered_sig.index, datarec['228_3'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\n\n\nsns.lineplot(filtered_sig.index, datarec['415_3'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\n\n\nsns.lineplot(filtered_sig.index, datarec['41_2'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\n\n\nsns.lineplot(filtered_sig.index, datarec['197_2'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\n\n\nsns.lineplot(filtered_sig.index, datarec['142_3'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\n\n\nsns.lineplot(filtered_sig.index, datarec['203_2'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\n\n\nsns.lineplot(filtered_sig.index, datarec['246_2'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\n\n\nsns.lineplot(filtered_sig.index, datarec['415_2'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\n\n\nsns.lineplot(filtered_sig.index, datarec['54_1'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\n\n\nsns.lineplot(filtered_sig.index, datarec['141_2'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\n\n\nsns.lineplot(filtered_sig.index, datarec['418_3'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\n\n\nsns.lineplot(filtered_sig.index, datarec['189_1'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\n\n\nsns.lineplot(filtered_sig.index, datarec['198_3'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numpy import cos, sin, pi, absolute, arange\nfrom scipy.signal import kaiserord, lfilter, firwin, freqz\nfrom pylab import figure, clf, plot, xlabel, ylabel, xlim, ylim, title, grid, axes, show\n\n\n#------------------------------------------------\n# Create a signal for demonstration.\n#------------------------------------------------\n\nsample_rate = 1000.0\n\nt = df_min_max_scaled.index\nx = df_min_max_scaled['259_1'].values\n\n\n#------------------------------------------------\n# Create a FIR filter and apply it to x.\n#------------------------------------------------\n\n# The Nyquist rate of the signal.\nnyq_rate = sample_rate / 2.0\n\n# The desired width of the transition from pass to stop,\n# relative to the Nyquist rate.  We'll design the filter\n# with a 5 Hz transition width.\nwidth = 5.0/nyq_rate\n\n# The desired attenuation in the stop band, in dB.\nripple_db = 60.0\n\n# Compute the order and Kaiser parameter for the FIR filter.\nN, beta = kaiserord(ripple_db, width)\n\n# The cutoff frequency of the filter.\ncutoff_hz = 10.0\n\n# Use firwin with a Kaiser window to create a lowpass FIR filter.\ntaps = firwin(N, cutoff_hz/nyq_rate, window=('kaiser', beta))\n\n# Use lfilter to filter x with the FIR filter.\nfiltered_x = lfilter(taps, 1.0, x)\n\n# The phase delay of the filtered signal.\ndelay = 0.5 * (N-1) / sample_rate\n\nfigure(3)\n# Plot the original signal.\nplot(t, x)\n# Plot the filtered signal, shifted to compensate for the phase delay.\nplot(t, filtered_x, 'r-')\n# Plot just the \"good\" part of the filtered signal.  The first N-1\n# samples are \"corrupted\" by the initial conditions.\nplot(t[N-1:]-delay, filtered_x[N-1:], 'g')\n\nxlabel('t')\ngrid(True)\n\nshow()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.signal import cheby1, lfilter\n\nimport matplotlib.pyplot as plt \n\nfiltered_sigc =df_min_max_scaled.copy() \n\ncutoff=10\nnyquist = 1000*0.5\nwn = cutoff/nyquist\nb, a = cheby1(9,0.1, wn, btype='lowpass')\nfor column in columns:\n    filtered_sigc[column] = lfilter(b, a, filtered_sigc[column].values)\nfiltered_sigc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\n\nsns.lineplot(filtered_sigc.index, filtered_sigc['201_2'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\n\nsns.lineplot(filtered_sigc.index, filtered_sigc['239_1'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\n\nsns.lineplot(filtered_sigc.index, filtered_sigc['239_2'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\n\nsns.lineplot(filtered_sigc.index, filtered_sigc['228_3'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\n\nsns.lineplot(filtered_sigc.index, filtered_sigc['415_3'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\n\nsns.lineplot(filtered_sigc.index, filtered_sigc['41_2'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\n\nsns.lineplot(filtered_sigc.index, filtered_sigc['197_2'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\n\nsns.lineplot(filtered_sigc.index, filtered_sigc['142_3'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\n\nsns.lineplot(filtered_sigc.index, filtered_sigc['203_2'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\n\nsns.lineplot(filtered_sigc.index, filtered_sigc['246_2'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\n\nsns.lineplot(filtered_sigc.index, filtered_sigc['415_2'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\n\nsns.lineplot(filtered_sigc.index, filtered_sigc['54_1'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\n\nsns.lineplot(filtered_sigc.index, filtered_sigc['141_2'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\n\nsns.lineplot(filtered_sigc.index, filtered_sigc['418_2'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\n\nsns.lineplot(filtered_sigc.index, filtered_sigc['189_1'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\n\nsns.lineplot(filtered_sigc.index, filtered_sigc['198_3'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numpy.fft import rfft, irfft, rfftfreq\nfrom scipy import fftpack\n\nfilter5= subject_0_df.copy() \nfor column in columns:\n    fourier = rfft(filter5[column].values)\n    frequencies = rfftfreq(filter5[column].values.size, d=2e-2 / filter5[column].values.size)\n    fourier[frequencies >350] = 0\n    filter5[column]=irfft(fourier)\nfilter5\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\n\n\n\nsns.lineplot(filtered_sigc.index, filter5['201_2'])\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf_min_max_scaledff = filter5.copy() \n  \n \ncolumns= df_min_max_scaledff.columns.values\nfor column in columns:\n    df_min_max_scaledff[column] = (df_min_max_scaledff[column] - df_min_max_scaledff[column].min()) / (df_min_max_scaledff[column].max() - df_min_max_scaledff[column].min())     \n\n   \ndisplay(df_min_max_scaledff)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\n\nfig, ax = plt.subplots(figsize=(15,12))\n\nsns.lineplot(filtered_sig.index, df_min_max_scaledff['201_2'])\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\n\nfig, ax = plt.subplots(figsize=(15,12))\n\nsns.lineplot(filtered_sig.index, df_min_max_scaledff['239_2'])\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\n\nfig, ax = plt.subplots(figsize=(15,12))\n\nsns.lineplot(filtered_sig.index, df_min_max_scaledff['239_1'])\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\n\nfig, ax = plt.subplots(figsize=(15,12))\n\nsns.lineplot(filtered_sig.index, df_min_max_scaledff['228_3'])\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\n\nfig, ax = plt.subplots(figsize=(15,12))\n\nsns.lineplot(filtered_sig.index, df_min_max_scaledff['415_3'])\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\n\nfig, ax = plt.subplots(figsize=(15,12))\n\nsns.lineplot(filtered_sig.index, df_min_max_scaledff['411_2'])\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\n\nfig, ax = plt.subplots(figsize=(15,12))\n\nsns.lineplot(filtered_sig.index, df_min_max_scaledff['41_2'])\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\n\nfig, ax = plt.subplots(figsize=(15,12))\n\nsns.lineplot(filtered_sig.index, df_min_max_scaledff['197_2'])\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\n\nfig, ax = plt.subplots(figsize=(15,12))\n\nsns.lineplot(filtered_sig.index, df_min_max_scaledff['142_3'])\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\n\nfig, ax = plt.subplots(figsize=(15,12))\n\nsns.lineplot(filtered_sig.index, df_min_max_scaledff['203_2'])\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\n\nfig, ax = plt.subplots(figsize=(15,12))\n\nsns.lineplot(filtered_sig.index, df_min_max_scaledff['246_2'])\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\n\nfig, ax = plt.subplots(figsize=(15,12))\n\nsns.lineplot(filtered_sig.index, df_min_max_scaledff['415_2'])\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\n\nfig, ax = plt.subplots(figsize=(15,12))\n\nsns.lineplot(filtered_sig.index, df_min_max_scaledff['54_1'])\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\n\nfig, ax = plt.subplots(figsize=(15,12))\n\nsns.lineplot(filtered_sig.index, df_min_max_scaledff['141_2'])\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\n\nfig, ax = plt.subplots(figsize=(15,12))\n\nsns.lineplot(filtered_sig.index, df_min_max_scaledff['418_3'])\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\n\nfig, ax = plt.subplots(figsize=(15,12))\n\nsns.lineplot(filtered_sig.index, df_min_max_scaledff['189_1'])\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\n\nfig, ax = plt.subplots(figsize=(15,12))\n\nsns.lineplot(filtered_sig.index, df_min_max_scaledff['198_3'])\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=filtered_sig.index.values\ntren= filtered_sig.copy()\ndetren= filtered_sig.copy()\n\nfor colu in columns:\n    y=filtered_sig[colu].values\n    poly_co=np.polyfit(x,y,4)\n    ynew=np.poly1d(poly_co)\n    t=ynew(x)\n    dt=filtered_sig[colu].values-t\n    t=t.reshape(-1,1)\n    tren[colu]=t\n    dt=dt.reshape(-1,1)\n    detren[colu]=dt\n\ndetren\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\nsns.lineplot(filtered_sig.index, filtered_sig['201_2'])\nsns.lineplot(filtered_sig.index, tren['201_2'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\nsns.lineplot(filtered_sig.index, filtered_sig['201_2'])\nsns.lineplot(filtered_sig.index, detren['201_2'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\nsns.lineplot(filtered_sig.index, filtered_sig['415_3'])\nsns.lineplot(filtered_sig.index, detren['415_3'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15,12))\nsns.lineplot(filtered_sig.index, filtered_sig['198_3'])\nsns.lineplot(filtered_sig.index, detren['198_3'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=filtered_sig.index.values\ntren1= filtered_sig.copy()\ndetren1= filtered_sig.copy()\n\nfor colu in columns:\n    y=datarec[colu].values\n    poly_co=np.polyfit(x,y,4)\n    ynew=np.poly1d(poly_co)\n    t=ynew(x)\n    dt=datarec[colu].values-t\n    t=t.reshape(-1,1)\n    tren1[colu]=t\n    dt=dt.reshape(-1,1)\n    detren1[colu]=dt\n\ndetren1\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\nfig = plt.subplots(figsize=(30, 8))\n\nx = filtered_sig['245_2']\n\n\n\nplt.plot(x)\n\nplt.xticks(np.arange(0, len(x)+1, 20))\n\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nfrom numpy import NaN, Inf, arange, isscalar, asarray, array\n\ndef peakdet(v, delta, x = None):\n    \"\"\"\n    Converted from MATLAB script at http://billauer.co.il/peakdet.html\n    \n    Returns two arrays\n    \n    function [maxtab, mintab]=peakdet(v, delta, x)\n    %PEAKDET Detect peaks in a vector\n    %        [MAXTAB, MINTAB] = PEAKDET(V, DELTA) finds the local\n    %        maxima and minima (\"peaks\") in the vector V.\n    %        MAXTAB and MINTAB consists of two columns. Column 1\n    %        contains indices in V, and column 2 the found values.\n    %      \n    %        With [MAXTAB, MINTAB] = PEAKDET(V, DELTA, X) the indices\n    %        in MAXTAB and MINTAB are replaced with the corresponding\n    %        X-values.\n    %\n    %        A point is considered a maximum peak if it has the maximal\n    %        value, and was preceded (to the left) by a value lower by\n    %        DELTA.\n    \n    % Eli Billauer, 3.4.05 (Explicitly not copyrighted).\n    % This function is released to the public domain; Any use is allowed.\n    \n    \"\"\"\n    maxtab = []\n    mintab = []\n       \n    if x is None:\n        x = arange(len(v))\n    \n    v = asarray(v)\n    \n    if len(v) != len(x):\n        sys.exit('Input vectors v and x must have same length')\n    \n    if not isscalar(delta):\n        sys.exit('Input argument delta must be a scalar')\n    \n    if delta <= 0:\n        sys.exit('Input argument delta must be positive')\n    \n    mn, mx = Inf, -Inf\n    mnpos, mxpos = NaN, NaN\n    \n    lookformax = True\n    \n    for i in arange(len(v)):\n        this = v[i]\n        if this > mx:\n            mx = this\n            mxpos = x[i]\n        if this < mn:\n            mn = this\n            mnpos = x[i]\n        \n        if lookformax:\n            if this < mx-delta:\n                maxtab.append((mxpos, mx))\n                mn = this\n                mnpos = x[i]\n                lookformax = False\n        else:\n            if this > mn+delta:\n                mintab.append((mnpos, mn))\n                mx = this\n                mxpos = x[i]\n                lookformax = True\n\n    return array(maxtab), array(mintab)\n\nif __name__==\"__main__\":\n    import matplotlib.pyplot as plt\n    \n    \n    \n    series = filtered_sig['218_1']\n    maxtab, mintab = peakdet(series,.3)\n    fig = plt.subplots(figsize=(30, 8))\n    \n    plt.plot(series)\n    plt.xticks(arange(0, len(series)+1, 30))\n\n    print(array(mintab[:,0]))\n    plt.scatter(array(maxtab)[:,0], array(maxtab)[:,1], color='blue')\n    plt.scatter(array(mintab)[:,0], array(mintab)[:,1], color='red')\n\n    plt.show()\n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom scipy.signal import argrelmax, argrelmin, firwin, convolve\n\n\nsignal = filtered_sig['160_1'].values\nextrema_index = np.sort(np.unique(np.concatenate((argrelmax(signal)[0], argrelmin(signal)[0]))))\nextrema = signal[extrema_index]\nprint(extrema.tolist())    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom scipy.signal import argrelmax, argrelmin\n\n\n\ndef extract_ppg45(single_waveform, sample_rate=1000):\n    def __next_pow2(x):\n        return 1<<(x-1).bit_length()\n    features = []\n    maxima_index = argrelmax(np.array(single_waveform))[0]\n    minima_index = argrelmin(np.array(single_waveform))[0]\n    derivative_1 = np.diff(single_waveform, n=1) * float(sample_rate)\n    derivative_1_maxima_index = argrelmax(np.array(derivative_1))[0]\n    derivative_1_minima_index = argrelmin(np.array(derivative_1))[0]\n    derivative_2 = np.diff(single_waveform, n=2) * float(sample_rate)\n    derivative_2_maxima_index = argrelmax(np.array(derivative_2))[0]\n    derivative_2_minima_index = argrelmin(np.array(derivative_2))[0]\n    sp_mag = np.abs(np.fft.fft(single_waveform, n=__next_pow2(len(single_waveform))*16))\n    freqs = np.fft.fftfreq(len(sp_mag))\n    sp_mag_maxima_index = argrelmax(sp_mag)[0]\n    # x\n    x = single_waveform[maxima_index[0]]\n    features.append(x)\n    # y\n    y = single_waveform[maxima_index[1]]\n    features.append(y)\n    # z\n    z = single_waveform[minima_index[0]]\n    features.append(z)\n    # t_pi\n    t_pi = float(len(single_waveform)) / float(sample_rate)\n    features.append(t_pi)\n    # y/x\n    features.append(y / x)\n    # (x-y)/x\n    features.append((x - y) / x)\n    # z/x\n    features.append(z / x)\n    # (y-z)/x\n    features.append((y - z) / x)\n    # t_1\n    t_1 = float(maxima_index[0] + 1) / float(sample_rate)\n    features.append(t_1)\n    # t_2\n    t_2 = float(minima_index[0] + 1) / float(sample_rate)\n    features.append(t_2)\n    # t_3\n    t_3 = float(maxima_index[1] + 1) / float(sample_rate)\n    features.append(t_3)\n    # delta_t\n    delta_t = t_3 - t_2\n    features.append(delta_t)\n    # width\n    single_waveform_halfmax = max(single_waveform) / 2\n    width = 0\n    for value in single_waveform[maxima_index[0]::-1]:\n        if value >= single_waveform_halfmax:\n            width += 1\n        else:\n            break\n    for value in single_waveform[maxima_index[0]+1:]:\n        if value >= single_waveform_halfmax:\n            width += 1\n        else:\n            break\n    features.append(float(width) / float(sample_rate))\n    # A_2/A_1\n    features.append(sum(single_waveform[:maxima_index[0]]) / sum(single_waveform[maxima_index[0]:]))\n    # t_1/x\n    features.append(t_1 / x)\n    # y/(t_pi-t_3)\n    features.append(y / (t_pi - t_3))\n    # t_1/t_pi\n    features.append(t_1 / t_pi)\n    # t_2/t_pi\n    features.append(t_2 / t_pi)\n    # t_3/t_pi\n    features.append(t_3 / t_pi)\n    # delta_t/t_pi\n    features.append(delta_t / t_pi)\n    # t_a1\n    t_a1 = float(derivative_1_maxima_index[0]) / float(sample_rate)\n    features.append(t_a1)\n    # t_b1\n    t_b1 = float(derivative_1_minima_index[0]) / float(sample_rate)\n    features.append(t_b1)\n    # t_e1\n    t_e1 = float(derivative_1_maxima_index[1]) / float(sample_rate)\n    features.append(t_e1)\n    # t_f1\n    t_f1 = float(derivative_1_minima_index[1]) / float(sample_rate)\n    features.append(t_f1)\n    # b_2/a_2\n    a_2 = derivative_2[derivative_2_maxima_index[0]]\n    b_2 = derivative_2[derivative_2_minima_index[0]]\n    features.append(b_2 / a_2)\n    # e_2/a_2\n    e_2 = derivative_2[derivative_2_maxima_index[1]]\n    features.append(e_2 / a_2)\n    # (b_2+e_2)/a_2\n    features.append((b_2 + e_2) / a_2)\n    # t_a2\n    t_a2 = float(derivative_2_maxima_index[0]) / float(sample_rate)\n    features.append(t_a2)\n    # t_b2\n    t_b2 = float(derivative_2_minima_index[0]) / float(sample_rate)\n    features.append(t_b2)\n    # t_a1/t_pi\n    features.append(t_a1 / t_pi)\n    # t_b1/t_pi\n    features.append(t_b1 / t_pi)\n    # t_e1/t_pi\n    features.append(t_e1 / t_pi)\n    # t_f1/t_pi\n    features.append(t_f1 / t_pi)\n    # t_a2/t_pi\n    features.append(t_a2 / t_pi)\n    # t_b2/t_pi\n    features.append(t_b2 / t_pi)\n    # (t_a1-t_a2)/t_pi\n    features.append((t_a1 - t_a2) / t_pi)\n    # (t_b1-t_b2)/t_pi\n    features.append((t_b1 - t_b2) / t_pi)\n    # (t_e1-t_2)/t_pi\n    features.append((t_e1 - t_2) / t_pi)\n    # (t_f1-t_3)/t_pi\n    features.append((t_f1 - t_3) / t_pi)\n    # f_base\n    f_base = freqs[sp_mag_maxima_index[0]] * sample_rate\n    features.append(f_base)\n    # sp_mag_base\n    sp_mag_base = sp_mag[sp_mag_maxima_index[0]] / len(single_waveform)\n    features.append(sp_mag_base)\n    # f_2\n    f_2 = freqs[sp_mag_maxima_index[1]] * sample_rate\n    features.append(f_2)\n    # sp_mag_2\n    sp_mag_2 = sp_mag[sp_mag_maxima_index[1]] / len(single_waveform)\n    features.append(sp_mag_2)\n    # f_3\n    f_3 = freqs[sp_mag_maxima_index[2]] * sample_rate\n    features.append(f_3)\n    # sp_mag_3\n    sp_mag_3 = sp_mag[sp_mag_maxima_index[2]] / len(single_waveform)\n    features.append(sp_mag_3)\n    return features\n\n\ndef extract_svri(single_waveform):\n    def __scale(data):\n        data_max = max(data)\n        data_min = min(data)\n        return [(x - data_min) / (data_max - data_min) for x in data]\n    max_index = np.argmax(single_waveform)\n    single_waveform_scaled = __scale(single_waveform)\n    return np.mean(single_waveform_scaled[max_index:]) / np.mean(single_waveform_scaled[:max_index])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndff=pd.read_csv('../input/ppgsignal-purify-update/PPG-signal-purify-update.csv')\n\n\ndff","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndfff=pd.read_csv('../input/ppgsignal-purify-update/PPG-signal-purify-update.csv')\ndfff","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfff.set_index(\"D\", inplace = True)\ndfff\ndfff.index.name=None\ndfff","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfff=dfff.transpose()\ndfff\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(dfff['210_2'].values)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pp=dfff.copy()\n\npp=pd.concat([pp]*23, ignore_index=True)\n\npp= pp.drop(45) \npp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns= pp.columns.values\ncolumns=columns[0:100]\nprint(columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\n  \n \ncolumns= pp.columns.values\ncolumns=columns[0:218]\n\nfor column in columns:\n    signal = filtered_sig[column].values\n    arr=dfff[column].values\n    f=arr[0]\n    l=arr[1]\n    signal=signal[f:l]\n    f=extract_ppg45(signal,sample_rate=1000)\n    f=np.array(f)\n    f=f.reshape(-1,1)\n    pp[column]=f\npp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\npp=pp.transpose()\npp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pp.to_csv('feature.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ppp=pd.read_csv('feature.csv')\nppp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Data=pd.read_csv('../input/datasetbp-feature/Dataset_BP_45feature.csv')\nData","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Jan  2 17:14:34 2019\n\n@author: rezwan\n\"\"\"\n###=========== First of all, we will import the needed dependencies \nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error \nfrom matplotlib import pyplot as plt\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport warnings \nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib import cm as cm\n# plot the heatmap\n#import seaborn as sns\n\n#### Seed\nimport random\nseed = 42\nrandom.seed(seed)\n\n\ndef correlation_matrix(df):\n\n    plt.title('Feature Correlation')\n    # calculate the correlation matrix\n    corr = df.corr()\n    # plot the heatmap\n    sns.heatmap(corr, \n            xticklabels=corr.columns,\n            yticklabels=corr.columns)\n\ndef hist_feats(df):\n    df.hist(figsize = (8,6))\n    plt.show()\n\ndef DNN_model(X):\n    NN_model = Sequential()\n\n    # The Input Layer :\n    NN_model.add(Dense(18, kernel_initializer='normal',input_dim = X.shape[1], activation='relu'))\n    \n    # The Hidden Layers :\n    NN_model.add(Dense(25, kernel_initializer='normal',activation='relu'))\n    NN_model.add(Dense(20, kernel_initializer='normal',activation='relu'))\n    NN_model.add(Dense(15, kernel_initializer='normal',activation='relu'))\n    \n    # The Output Layer :\n    NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n    \n    # Compile the network :\n    NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n    \n    return NN_model\n\n\nimport math\n\ndef average(x):\n    assert len(x) > 0\n    return float(sum(x)) / len(x)\n\ndef pearson_corr(x, y):\n    assert len(x) == len(y)\n    n = len(x)\n    assert n > 0\n    avg_x = average(x)\n    avg_y = average(y)\n    diffprod = 0\n    xdiff2 = 0\n    ydiff2 = 0\n    for idx in range(n):\n        xdiff = x[idx] - avg_x\n        ydiff = y[idx] - avg_y\n        diffprod += xdiff * ydiff\n        xdiff2 += xdiff * xdiff\n        ydiff2 += ydiff * ydiff\n\n    return diffprod / math.sqrt(xdiff2 * ydiff2)\n\n##=========================Bland-Altman plot \nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom numpy.random import random\n\ndef bland_altman_plot(data1, data2, *args, **kwargs):\n    data1     = np.asarray(data1)\n    data2     = np.asarray(data2)\n    mean      = np.mean([data1, data2], axis=0)\n    diff      = data1 - data2                   # Difference between data1 and data2\n    md        = np.mean(diff)                   # Mean of the difference\n    sd        = np.std(diff, axis=0)            # Standard deviation of the difference\n\n    plt.title('Bland-Altman Plot')\n    plt.legend()\n    plt.scatter(mean, diff, *args, **kwargs)\n    plt.axhline(md,           color='r', linestyle='--', label=\"md\")\n    plt.axhline(md + 1.96*sd, color='g', linestyle='--', label=\"md + 1.96*sd\")\n    plt.axhline(md - 1.96*sd, color='b', linestyle='--', label=\"md - 1.96*sd\")\n    \n    plt.xlabel(\"Average Hemoglobin(gm/dL)\")\n    plt.ylabel(\"Difference Hemoglobin(gm/dL)\")\n    plt.show()\n\ndef act_pred_plot(y, predicted, r=None):\n    fig, ax = plt.subplots()\n    ax.text(y.min(), y.max(), \"Pearson's R = \" + str(r))\n    ax.scatter(y, predicted)\n    ax.plot([y.min(), y.max()], [y.min(), y.max()], 'r-', lw=1)\n    ax.set_xlabel('Reference Hemoglobin(gm/dL)')\n    ax.set_ylabel('Estimated Hemoglobin(gm/dL)')\n    plt.show()\n\ndef rmse(targets,predictions):\n    return np.sqrt(((predictions - targets) ** 2).mean())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Jan  2 17:14:34 2019\n\n@author: rezwan\n\"\"\"\n###=========== First of all, we will import the needed dependencies \nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error \nfrom matplotlib import pyplot as plt\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport warnings \nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib import cm as cm\n# plot the heatmap\n#import seaborn as sns\n\n#### Seed\nimport random\nseed = 42\nrandom.seed(seed)\n\n\ndef correlation_matrix(df):\n\n    plt.title('Feature Correlation')\n    # calculate the correlation matrix\n    corr = df.corr()\n    # plot the heatmap\n    sns.heatmap(corr, \n            xticklabels=corr.columns,\n            yticklabels=corr.columns)\n\ndef hist_feats(df):\n    df.hist(figsize = (8,6))\n    plt.show()\n\ndef DNN_model(X):\n    NN_model = Sequential()\n\n    # The Input Layer :\n    NN_model.add(Dense(18, kernel_initializer='normal',input_dim = X.shape[1], activation='relu'))\n    \n    # The Hidden Layers :\n    NN_model.add(Dense(25, kernel_initializer='normal',activation='relu'))\n    NN_model.add(Dense(20, kernel_initializer='normal',activation='relu'))\n    NN_model.add(Dense(15, kernel_initializer='normal',activation='relu'))\n    \n    # The Output Layer :\n    NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n    \n    # Compile the network :\n    NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n    \n    return NN_model\n\n\nimport math\n\ndef average(x):\n    assert len(x) > 0\n    return float(sum(x)) / len(x)\n\ndef pearson_corr(x, y):\n    assert len(x) == len(y)\n    n = len(x)\n    assert n > 0\n    avg_x = average(x)\n    avg_y = average(y)\n    diffprod = 0\n    xdiff2 = 0\n    ydiff2 = 0\n    for idx in range(n):\n        xdiff = x[idx] - avg_x\n        ydiff = y[idx] - avg_y\n        diffprod += xdiff * ydiff\n        xdiff2 += xdiff * xdiff\n        ydiff2 += ydiff * ydiff\n\n    return diffprod / math.sqrt(xdiff2 * ydiff2)\n\n##=========================Bland-Altman plot \nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom numpy.random import random\n\ndef bland_altman_plot(data1, data2, *args, **kwargs):\n    data1     = np.asarray(data1)\n    data2     = np.asarray(data2)\n    mean      = np.mean([data1, data2], axis=0)\n    diff      = data1 - data2                   # Difference between data1 and data2\n    md        = np.mean(diff)                   # Mean of the difference\n    sd        = np.std(diff, axis=0)            # Standard deviation of the difference\n\n    plt.title('Bland-Altman Plot')\n    plt.legend()\n    plt.scatter(mean, diff, *args, **kwargs)\n    plt.axhline(md,           color='r', linestyle='--', label=\"md\")\n    plt.axhline(md + 1.96*sd, color='g', linestyle='--', label=\"md + 1.96*sd\")\n    plt.axhline(md - 1.96*sd, color='b', linestyle='--', label=\"md - 1.96*sd\")\n    \n    plt.xlabel(\"Average Hemoglobin(gm/dL)\")\n    plt.ylabel(\"Difference Hemoglobin(gm/dL)\")\n    plt.show()\n\ndef act_pred_plot(y, predicted, r=None):\n    fig, ax = plt.subplots()\n    ax.text(y.min(), y.max(), \"Pearson's R = \" + str(r))\n    ax.scatter(y, predicted)\n    ax.plot([y.min(), y.max()], [y.min(), y.max()], 'r-', lw=1)\n    ax.set_xlabel('Reference Hemoglobin(gm/dL)')\n    ax.set_ylabel('Estimated Hemoglobin(gm/dL)')\n    plt.show()\n\ndef rmse(targets,predictions):\n    return np.sqrt(((predictions - targets) ** 2).mean())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Thu Sep 17 15:58:17 2020\n\n@author: Rezwan\n\"\"\"\n\nimport torch\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport numpy as np\n\nfrom torch.autograd import Variable\n\nimport numpy\nfrom sklearn import metrics\n\n\nimport torch\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\n\ntorch.manual_seed(42) \n\n\ndef model_drop(inp):\n    n_hidden = 700\n    model_drop= torch.nn.Sequential(\n        torch.nn.Linear(inp, n_hidden), ## input feature 48\n        torch.nn.Dropout(0.5),  \n        torch.nn.ReLU(),\n        torch.nn.Linear(n_hidden, n_hidden),\n        torch.nn.Dropout(0.5), \n        torch.nn.ReLU(),\n        torch.nn.Linear(n_hidden, 1),\n    )\n    \n    return model_drop\n\n# model = model_drop(X.shape[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sun Jan 27 23:39:10 2019\n\n@author: rezwan\n\"\"\"\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot\n#import numpy as np\nfrom numpy.random import random\n#### Seed\nimport random\nimport numpy as np\nseed = 42\nnp.random.seed(seed)\n\n\nimport math\n\ndef average(x):\n    assert len(x) > 0\n    return float(sum(x)) / len(x)\n\ndef pearsonr(x, y):\n    assert len(x) == len(y)\n    n = len(x)\n    assert n > 0\n    avg_x = average(x)\n    avg_y = average(y)\n    diffprod = 0\n    xdiff2 = 0\n    ydiff2 = 0\n    for idx in range(n):\n        xdiff = x[idx] - avg_x\n        ydiff = y[idx] - avg_y\n        diffprod += xdiff * ydiff\n        xdiff2 += xdiff * xdiff\n        ydiff2 += ydiff * ydiff\n\n    return diffprod / math.sqrt(xdiff2 * ydiff2)\n\ndef rmse(targets,predictions):\n    return np.sqrt(((predictions - targets) ** 2).mean())\n\n\ndef mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\ndef filter_nan(s,o):\n    \"\"\"\n    this functions removed the data  from simulated and observed data\n    whereever the observed data contains nan\n    \n    this is used by all other functions, otherwise they will produce nan as \n    output\n    \"\"\"\n    if np.sum(~np.isnan(s*o))>=1:\n        data = np.array([s.flatten(),o.flatten()])\n        data = np.transpose(data)\n        data = data[~np.isnan(data).any(1)]\n        s = data[:,0]\n        o = data[:,1]\n    return s, o\n\ndef index_agreement(s, o):\n    \"\"\"\n\tindex of agreement\n\t\n\tWillmott (1981, 1982) \n\tinput:\n        s: simulated\n        o: observed\n    output:\n        ia: index of agreement\n    \"\"\"\n    s,o = filter_nan(s,o)\n    ia = 1 -(np.sum((o-s)**2))/(np.sum(\n    \t\t\t(np.abs(s-np.mean(o))+np.abs(o-np.mean(o)))**2))\n    return ia\n\n##=========================Bland-Altman plot \n\n\ndef bland_altman_plot(data1, data2, *args, **kwargs):\n    \n        data1     = np.asarray(data1)\n        data2     = np.asarray(data2)\n        mean      = np.mean([data1, data2], axis=0)\n        diff      = data1 - data2                   # Difference between data1 and data2\n        md        = np.mean(diff)                   # Mean of the difference\n        sd        = np.std(diff, axis=0)            # Standard deviation of the difference\n\n        fig, ax = plt.subplots()\n        plt.title('Bland-Altman Plot')\n    #    plt.legend()\n        plt.scatter(mean, diff, *args, **kwargs)\n\n        plt.axhline(md + 1.96*sd, color='g', label=\"md + 1.96*sd\", linestyle='--')\n        plt.axhline(md,           color='r', label=\"md\",           linestyle='--')\n        plt.axhline(md - 1.96*sd, color='b', label=\"md - 1.96*sd\", linestyle='--')\n\n        labels = [\"md + 1.96*sd\", \"md\", \"md - 1.96*sd\"]\n        handles, _ = ax.get_legend_handles_labels()\n\n        # Slice list to remove first handle\n        plt.legend(handles = handles[:], labels = labels)\n\n        plt.xlabel(\"Average Glucose(mmol/L)\")\n        plt.ylabel(\"Difference Glucose(mmol/L)\")\n       # plt.savefig(\"/media/rezwan/Study/#Undergrade/Glucose/Fig_GA/Bland-Altman_Gl.pdf\", dpi = 100)\n        plt.show()\n        \n    \n        data1     = np.asarray(data1)\n        data2     = np.asarray(data2)\n        mean      = np.mean([data1, data2], axis=0)\n        diff      = data1 - data2                   # Difference between data1 and data2\n        md        = np.mean(diff)                   # Mean of the difference\n        sd        = np.std(diff, axis=0)            # Standard deviation of the difference\n\n        fig, ax = plt.subplots()\n        plt.title('Bland-Altman Plot')\n    #    plt.legend()\n        plt.scatter(mean, diff, *args, **kwargs)\n\n        plt.axhline(md + 1.96*sd, color='g', label=\"md + 1.96*sd\", linestyle='--')\n        plt.axhline(md,           color='r', label=\"md\",           linestyle='--')\n        plt.axhline(md - 1.96*sd, color='b', label=\"md - 1.96*sd\", linestyle='--')\n\n        labels = [\"md + 1.96*sd\", \"md\", \"md - 1.96*sd\"]\n        handles, _ = ax.get_legend_handles_labels()\n\n        # Slice list to remove first handle\n        plt.legend(handles = handles[:], labels = labels)\n\n        plt.xlabel(\"Average Glucose(mmol/L)\")\n        plt.ylabel(\"Difference Glucose(mmol/L)\")\n      #  plt.savefig(\"/media/rezwan/Study/#Undergrade/Glucose/Fig_without_GA/Bland-Altman_Gl.pdf\", dpi = 100)\n        plt.show()\n\ndef act_pred_plot(y, predicted, r=None, st=None, GA = None):\n    if GA == \"GA\":\n        fig, ax = plt.subplots()\n\n        ax.text(y.min(), y.max(), str('$MAE$ = %0.3f$\\pm%0.3f$' %(r[0],np.std(st[0])/10.0)))\n        ax.text(y.min(), y.max()-1, str('$R^2$ = %.3f$\\pm%0.3f$' %(r[1], np.std(st[1])/10.0)))\n\n        ax.scatter(y, predicted)\n        ax.plot([y.min(), y.max()], [y.min(), y.max()], 'r-', lw=1)\n        ax.set_xlabel('Reference Glucose(mmol/L)')\n        ax.set_ylabel('Estimated Glucose(mmol/L)')\n        #plt.savefig(\"/media/rezwan/Study/#Undergrade/Glucose/Fig_GA/Act_vs_Pred_Gl.pdf\", dpi = 100)\n        plt.show()\n        \n    else:\n        fig, ax = plt.subplots()\n\n        ax.text(y.min(), y.max(), str('$MAE$ = %0.3f$\\pm%0.3f$' %(r[0],np.std(st[0])/10.0)))\n        ax.text(y.min(), y.max()-1, str('$R^2$ = %.3f$\\pm%0.3f$' %(r[1], np.std(st[1])/10.0)))\n\n        ax.scatter(y, predicted)\n        ax.plot([y.min(), y.max()], [y.min(), y.max()], 'r-', lw=1)\n        ax.set_xlabel('Reference Glucose(mmol/L)')\n        ax.set_ylabel('Estimated Glucose(mmol/L)')\n        #plt.savefig(\"/media/rezwan/Study/#Undergrade/Glucose/Fig_without_GA/Act_vs_Pred_Gl.pdf\", dpi = 100)\n        plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Tue Sep 15 15:31:56 2020\n\n@author: Rezwan\n\"\"\"\n\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nimport numpy as np\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\n\n\n#from measrmnt_indices import pearsonr, rmse, bland_altman_plot\n\n#### Seed\nimport random\nseed = 42\nnp.random.seed(seed)\n\n\n############ Read CSV file\ndataFr = pd.read_csv(\"../input/dataset-ga/Dataset_Gl.csv\")\ndataFr.head()\ndataFr.shape\n#dataFr.drop(dataFr.columns[[0,1]], axis=1, inplace=True)\ndataFr.head()\ndict = {'Sex':{'M':1, 'F':0}}      # label = column name\ndataFr.replace(dict,inplace = True) \ndf = dataFr\nprint(df.head())\nprint(df.shape)\n\nprint(df.isnull().any().any())\nprint(df.isnull().sum().sum())\nprint(df.isnull().any().any())\nprint(df.isnull().sum())\n\n\n# #####=============Standard scaler\n# # Xorg = df.as_matrix()  # Take one dataset: hm\n# Xorg = df.values  # Take one dataset: hm\n\n# scaler = StandardScaler()\n# Xscaled = scaler.fit_transform(Xorg)\n# ## store these off for predictions with unseen data\n# Xmeans = scaler.mean_\n# Xstds = scaler.scale_\n\n# y = Xscaled[:, 48]\n# X = np.delete(Xscaled, 48, axis=1)\n\n\ny = df.iloc[:, 48]\nX = df.iloc[:, 0:48]\n\nscaler = MinMaxScaler(feature_range=(0, 1))\n# scaler = StandardScaler()\n\nX = scaler.fit_transform(X)\n\n\n\n# model = SVR(kernel='rbf') \n# ############### Apply 10-fold Cross validation\n# n_splits = 10\n# cv_set = np.repeat(-1.,X.shape[0])\n# skf = KFold(n_splits = n_splits ,shuffle=True, random_state=42)\n# for train_index,test_index in skf.split(X, y):\n#     x_train,x_test = X[train_index],X[test_index]\n#     y_train,y_test = y[train_index],y[test_index]\n    \n#     if x_train.shape[0] != y_train.shape[0]:\n#         raise Exception()\n        \n#     model.fit(x_train,y_train)\n#     predicted_y = model.predict(x_test)\n#     print(\"Individual R: \" +str(pearsonr(y_test, predicted_y)))\n#     cv_set[test_index] = predicted_y\n \n# print(\"Overall R: \" + str(pearsonr(y,cv_set)))\n\n\nscores = []\ncv_set = np.repeat(-1.,X.shape[0])\n\nbest_svr = SVR(kernel='rbf') ### Model: SVR\n# best_svr = LinearRegression()\n\n\ncv = KFold(n_splits=10, random_state=42, shuffle=True)\n\nfor train_index, test_index in cv.split(X):\n    print(\"Train Index: \", train_index, \"\\n\")\n    print(\"Test Index: \", test_index)\n\n    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n    \n    if X_train.shape[0] != y_train.shape[0]:\n        raise Exception()\n    \n    best_svr.fit(X_train,y_train)\n    predicted_y = best_svr.predict(X_test)\n    \n    print(\"Individual R^2: \" +str(metrics.r2_score(y_test, predicted_y)))\n    cv_set[test_index] = predicted_y\n \nprint(\"Overall R^2: \" + str(metrics.r2_score(y, cv_set)))\n\nprint(\"R^2 Score: \" + str(metrics.r2_score(y, cv_set)))\nprint(\"MAE: \" + str(metrics.mean_absolute_error(y, cv_set)))\nprint(\"MSE: \" + str(metrics.mean_squared_error(y, cv_set)))\nprint(\"RMSE: \" + str(rmse(y, cv_set)))\nprint(\"MSLE: \" + str(metrics.mean_squared_log_error(y, cv_set)))\nprint(\"EVS: \" + str(metrics.explained_variance_score(y, cv_set)))\n    \n################\n\n##### Plot Actual and Prdiction values\nplt.plot(y, color='b', label = 'Actual')\nplt.plot(cv_set, color='r', label = 'Predicted')\nplt.xlabel(\"Time\")\nplt.ylabel(\"Hemoglobin Concentration(gm/dL)\")\nplt.legend(loc='best')\nplt.show()\n\n#### bland_altman_plot\nbland_altman_plot(y, cv_set)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Mon Jan 28 00:28:57 2019\n\n@author: rezwan\n\"\"\"\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Thu Nov 29 21:18:30 2018\n\n@author: rezwan\n\"\"\"\n\"\"\"\nSource: \n    (1) https://github.com/rezwanh001/PPG/blob/master/ppg/learn.py\n\n\n\"\"\"\n#### Seed\n\nimport random\nimport numpy as np\nseed = 42\n# random.seed(seed)\nnp.random.seed(seed)\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization, Activation, Dropout\nfrom keras import optimizers\nfrom sklearn.metrics import r2_score\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import metrics\nfrom sklearn.neural_network import MLPClassifier, MLPRegressor\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n#from measrmnt_indices import *\n\ndef LinReg():\n    ## https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n    reg = LinearRegression()\n    return reg\n\n##https://scikit-learn.org/stable/auto_examples/svm/plot_svm_regression.html\ndef LinSVR():\n    svr_lin = SVR(kernel='linear', C=1e3)\n    return svr_lin\n\ndef RbfSVR():\n    svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n    return svr_rbf\n\ndef PLS():\n    #https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSRegression.html\n    pls2 = PLSRegression(n_components=2)\n    return pls2\n\ndef DTR():\n    #https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html\n    regr_2 = DecisionTreeRegressor(max_depth=17)\n    return regr_2\n\ndef MLPR():\n    #https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html\n    MLR = MLPRegressor(hidden_layer_sizes=(50, ), \n                 activation='relu', solver=\"adam\")\n    return MLR\n\ndef RFR():\n    # Fitting the Random Forest Regression Model to the dataset\n#    from sklearn.ensemble import RandomForestRegressor\n    regressor = RandomForestRegressor(n_estimators = 150, random_state = 42)\n    return regressor\n\ndef DNN(X):\n    model = Sequential()\n\n    # The Input Layer :\n    model.add(Dense(100, kernel_initializer='normal',input_dim = X.shape[1], activation='relu'))\n    \n    # The Hidden Layers :\n    model.add(Dense(150, kernel_initializer='normal',activation='relu'))\n    model.add(Dense(200, kernel_initializer='normal',activation='relu'))\n    model.add(Dropout(0.25))\n    model.add(Dense(250, kernel_initializer='normal',activation='relu'))\n    model.add(Dense(300, kernel_initializer='normal',activation='relu'))\n    model.add(Dropout(0.5))\n    \n    # The Output Layer :\n    model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n    \n    # Compile the network :\n#    model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['MAE'])\n    model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mse'])\n    \n    return model\n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold, KFold\nimport numpy as np\nfrom sklearn.metrics import f1_score, r2_score\n#from measrmnt_indices import *\nimport torch\nfrom torch.autograd import Variable\nfrom sklearn import metrics\n\n#from torch_dnn_model import model_drop\n\n\n\n#### Seed\nimport random\nseed = 42\nnp.random.seed(seed)\n\n\nclass FitenessFunction:\n    \n    def __init__(self,n_splits = 5,*args,**kwargs):\n        \"\"\"\n            Parameters\n            -----------\n            n_splits :int, \n                Number of splits for cv\n            \n            verbose: 0 or 1\n        \"\"\"\n        self.n_splits = n_splits\n    \n\n    def calculate_fitness(self,x,y, model,flag):\n        \n        if flag == \"torch\":\n            cv_set = np.repeat(-1.,x.shape[0])\n            \n            cv = KFold(n_splits=10, random_state=42, shuffle=False)\n            \n            # model_drop = model\n            model = model_drop(x.shape[1])\n            \n            for train_index, test_index in cv.split(x):\n                # print(\"Train Index: \", train_index, \"\\n\")\n                # print(\"Test Index: \", test_index)\n            \n                X_train, X_test, y_train, y_test = x[train_index], x[test_index], y[train_index], y[test_index]\n                \n                if X_train.shape[0] != y_train.shape[0]:\n                    raise Exception()\n                    \n                y_train = y_train.to_numpy()\n                y_test = y_test.to_numpy()\n                \n                X_train = Variable(torch.from_numpy(X_train))\n                X_test = Variable(torch.from_numpy(X_test))\n                \n                y_train = Variable(torch.from_numpy(y_train))\n                y_test = Variable(torch.from_numpy(y_test))\n            \n                    \n                ###### Epoch: 500\n                    \n                epochs = 500\n            \n                for epoch in range(epochs):\n                    #make a prediction\n                    yhat_drop = model_drop(X_train.float())\n                    \n                    #calculate the loss\n                    loss_drop = criterion(yhat_drop, y_train.float())\n                    \n                    #store the loss for  both the training and validation \n                    \n                    # LOSS['training_data_dropout'].append(loss_drop.item())\n                    # model_drop.eval()\n                    \n                    # LOSS['validation_data_dropout'].append(criterion(model_drop(X_test.float()), y_test.float()).item())\n                    # model_drop.train()\n                    \n                    #clear gradient \n                    optimizer_drop.zero_grad()\n                    \n                    #Backward pass: compute gradient of the loss with respect to all the learnable parameters\n                    loss_drop.backward()\n                    \n                    #the step function on an Optimizer makes an update to its parameters\n                    optimizer_drop.step()\n                ######################################\n                \n                \n                ## Cal. pred for test set\n                    \n                model_drop.eval()\n                yhat_drop_pred = model_drop(X_test.float())\n                \n                # print(\"Individual R^2 Score: \" + str(metrics.r2_score(y_test.numpy(), yhat_drop_pred.detach().numpy())))\n                # cv_set[test_index] = yhat_drop_pred.detach().numpy().reshape(len(yhat_drop_pred),)\n                \n                return metrics.r2_score(y_test.numpy(), yhat_drop_pred.detach().numpy())\n                \n                        \n        else:\n            cv_set = np.repeat(-1.,x.shape[0])\n            skf = KFold(n_splits = self.n_splits ,shuffle=True, random_state=42)\n            model = model########## Import model\n            for train_index,test_index in skf.split(x,y):\n                x_train,x_test = x[train_index],x[test_index]\n                y_train,y_test = y[train_index],y[test_index]\n                if x_train.shape[0] != y_train.shape[0]:\n                    raise Exception()\n                model.fit(x_train,y_train)\n                predicted_y = model.predict(x_test)\n                cv_set[test_index] = predicted_y\n            return pearsonr(y,cv_set)\n            \n\n    \n    def calculate_fitness_2(self,X,y):\n        \n        row = X.shape[0]\n        col = X.shape[1]\n        feat = col\n        \n        DX = []\n        DY = []\n        \n        import math\n        import numpy as np\n        for i in range(row):\n          for j in range(col):\n            Dy = (y[i] - y[j])\n            sm = 0\n            if Dy >= 0:\n              sm = sm\n              for k in range(feat):\n                # print(i, j, k)\n                sm += (X[i][k] - X[j][k])**2\n        \n            elif Dy < 0:\n              sm = -sm\n              for k in range(feat):\n                # print(i, j, k)\n                sm += (X[i][k] - X[j][k])**2\n            \n            ## Sum\n            DY.append(Dy)\n            DX.append(math.sqrt(sm / feat))\n              \n#        print(len(DY))\n#        print(len(DX))\n#        print(\"=============\")\n#        print(DX)\n#        print(DY)\n#        \n        ###================ Calculate SDxDy\n        DX_mean = np.mean(DX)\n        #print(DX_mean)\n        DY_mean = np.mean(DY)\n        #print(DY_mean)\n        \n        sm_DX_DY = 0\n        for i in range(len(DX)):\n            sm_DX_DY += (DX[i] - DX_mean) * (DY[i] - DY_mean)\n        \n        SDxDy = sm_DX_DY / (feat-1)\n#        print(\"SDxDy : \" + str(SDxDy))\n            \n        ###===============Calculte SDx\n        sm_DX = 0\n        for i in range(len(DX)):\n            sm_DX += (DX[i] - DX_mean)**2\n            \n        SDx = sm_DX / (feat-1)\n#        print(\"SDx : \" + str(SDx))\n        \n        ###========== Calculte SDy\n        sm_Dy = 0\n        for i in range(len(y)):\n            sm_Dy += (DY[i] - DY_mean)**2\n        \n        SDy = sm_Dy / (feat - 1)\n#        print(\"SDy : \" + str(SDy))    \n        \n        #### Now Calculate corelation-Coefficient: R\n        R = (SDxDy) / math.sqrt(SDx * SDy)\n#        print(\"R : \" +str(R)) \n        return R\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from deap import base, creator\nimport random\nimport numpy as np\nfrom deap import tools\n#import fitness_function as ff\nimport matplotlib.pyplot as plt\n#### Seed\nimport random\nseed = 42\nnp.random.seed(seed)\n\n\nclass Feature_Selection_GA_Filter:\n    \"\"\"\n        FeaturesSelectionGA\n        This class uses Genetic Algorithm to find out the best features for an input model\n        using Distributed Evolutionary Algorithms in Python(DEAP) package. Default toolbox is\n        used for GA but it can be changed accordingly.\n\n    \n    \"\"\"\n    def __init__(self,x,y,cv_split=10,verbose=1):\n        \"\"\"\n            Parameters\n            -----------\n            model : scikit-learn supported model, \n                x :  {array-like}, shape = [n_samples, n_features]\n                     Training vectors, where n_samples is the number of samples \n                     and n_features is the number of features.\n \n                y  : {array-like}, shape = [n_samples]\n                     Target Values\n            cv_split: int\n                     Number of splits for cross_validation to calculate fitness.\n            \n            verbose: 0 or 1\n        \"\"\"\n#        self.model =  model\n        self.n_features = x.shape[1]\n        self.toolbox = None\n        self.creator = self._create()\n        self.cv_split = cv_split\n        self.x = x\n        self.y = y\n        self.verbose = verbose\n        if self.verbose==1:\n            print(\"Model will select best features among {} features using cv_split :{}.\".format(x.shape[1],cv_split))\n            print(\"Shape od train_x: {} and target: {}\".format(x.shape,y.shape))\n        self.final_fitness = []\n        self.fitness_in_generation = {}\n        self.best_ind = None\n        self.mean_fitness = []\n        self.best_fitness = []\n    \n    def evaluate(self,individual):\n        fit_obj = FitenessFunction(self.cv_split)\n        np_ind = np.asarray(individual)\n        if np.sum(np_ind) == 0:\n            fitness = 0.0\n        else:\n            feature_idx = np.where(np_ind==1)[0]\n            fitness = fit_obj.calculate_fitness_2(self.x[:,feature_idx],self.y)\n        \n        if self.verbose == 1:\n            print(\"Individual: {}  Fitness_score: {} \".format(individual,fitness))\n#        self.mean_fitness.append(fitness)    \n        return fitness,\n    \n    \n    def _create(self):\n        creator.create(\"FeatureSelect\", base.Fitness, weights=(1.0,))\n        creator.create(\"Individual\", list, fitness=creator.FeatureSelect)\n        return creator\n    \n    def create_toolbox(self):\n        \"\"\" \n            Custom creation of toolbox.\n            Parameters\n            -----------\n                self\n            Returns\n            --------\n                Initialized toolbox\n        \"\"\"\n        \n        self._init_toolbox()\n        return toolbox\n        \n    def register_toolbox(self,toolbox):\n        \"\"\" \n            Register custom created toolbox. Evalute function will be registerd\n            in this method.\n            Parameters\n            -----------\n                Registered toolbox with crossover,mutate,select tools except evaluate\n            Returns\n            --------\n                self\n        \"\"\"\n        toolbox.register(\"evaluate\", self.evaluate)\n        self.toolbox = toolbox\n     \n    \n    def _init_toolbox(self):\n        toolbox = base.Toolbox()\n        toolbox.register(\"attr_bool\", random.randint, 0, 1)\n        # Structure initializers\n        toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, self.n_features)\n        toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n        return toolbox\n        \n        \n    def _default_toolbox(self):\n        toolbox = self._init_toolbox()\n        toolbox.register(\"mate\", tools.cxTwoPoint)\n        toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.1)\n#        toolbox.register(\"select\", tools.selTournament, tournsize=3)\n        toolbox.register(\"select\", tools.selRoulette)\n        toolbox.register(\"evaluate\", self.evaluate)\n        return toolbox\n    \n    def get_final_scores(self,pop,fits):\n        self.final_fitness = list(zip(pop,fits))\n        \n    \n        \n    def generate(self,n_pop,ngen,cxpb = 0.10,mutxpb = 0.2,set_toolbox = False):\n        \n        \"\"\" \n            Generate evolved population\n            Parameters\n            -----------\n                n_pop : {int}\n                        population size\n                cxpb  : {float}\n                        crossover probablity\n                mutxpb: {float}\n                        mutation probablity\n                n_gen : {int}\n                        number of generations\n                set_toolbox : {boolean}\n                              If True then you have to create custom toolbox before calling \n                              method. If False use default toolbox.\n            Returns\n            --------\n                Fittest population\n        \"\"\"\n        \n        \n        \n        if self.verbose==1:\n            print(\"Population: {}, crossover_probablity: {}, mutation_probablity: {}, total generations: {}\".format(n_pop,cxpb,mutxpb,ngen))\n        \n        if not set_toolbox:\n            self.toolbox = self._default_toolbox()\n        else:\n            raise Exception(\"Please create a toolbox.Use create_toolbox to create and register_toolbox to register. Else set set_toolbox = False to use defualt toolbox\")\n        pop = self.toolbox.population(n_pop)\n        CXPB, MUTPB, NGEN = cxpb,mutxpb,ngen\n\n        # Evaluate the entire population\n        print(\"EVOLVING.......\")\n        fitnesses = list(map(self.toolbox.evaluate, pop))\n        \n        for ind, fit in zip(pop, fitnesses):\n            ind.fitness.values = fit\n\n        for g in range(NGEN):\n            print(\"-- GENERATION {} --\".format(g+1))\n            offspring = self.toolbox.select(pop, len(pop))\n            self.fitness_in_generation[str(g+1)] = max([ind.fitness.values[0] for ind in pop])\n            # Clone the selected individuals\n            offspring = list(map(self.toolbox.clone, offspring))\n\n            # Apply crossover and mutation on the offspring\n            for child1, child2 in zip(offspring[::2], offspring[1::2]):\n                if random.random() < CXPB:\n                    self.toolbox.mate(child1, child2)\n                    del child1.fitness.values\n                    del child2.fitness.values\n\n            for mutant in offspring:\n                if random.random() < MUTPB:\n                    self.toolbox.mutate(mutant)\n                    del mutant.fitness.values\n\n            # Evaluate the individuals with an invalid fitness\n            weak_ind = [ind for ind in offspring if not ind.fitness.valid]\n            fitnesses = list(map(self.toolbox.evaluate, weak_ind))\n            ##=================== \n            print(\"Best fitness from a generation: \" +str(np.max(fitnesses)))\n            print(\"Mean fitness from a generation: \" +str(np.mean(fitnesses)))\n            self.best_fitness.append(np.max(fitnesses))\n            self.mean_fitness.append(np.mean(fitnesses))\n            ##=================\n            for ind, fit in zip(weak_ind, fitnesses):\n                ind.fitness.values = fit\n            print(\"Evaluated %i individuals\" % len(weak_ind))\n\n            # The population is entirely replaced by the offspring\n            pop[:] = offspring\n            \n                    # Gather all the fitnesses in one list and print the stats\n        fits = [ind.fitness.values[0] for ind in pop]\n        \n        length = len(pop)\n        mean = sum(fits) / length\n        sum2 = sum(x*x for x in fits)\n        std = abs(sum2 / length - mean**2)**0.10\n        if self.verbose==1:\n            print(\"  Min %s\" % min(fits))\n            print(\"  Max %s\" % max(fits))\n            print(\"  Avg %s\" % mean)\n            print(\"  Std %s\" % std)\n    \n        print(\"-- Only the fittest survives --\")\n\n        best_ind = tools.selBest(pop, 1)[0]\n        print(\"Best individual is %s, %s\" % (best_ind, best_ind.fitness.values))\n#        self.get_final_scores(pop,fits)\n        \n        return best_ind\n#        return pop\n    \n    def plot_feature_set_score(self,ngen):\n#        print(len(self.mean_fitness))\n#        print(len(self.best_fitness))\n        \n        plt.figure(figsize=(8,6))\n        plt.title(\"Feature Set Fitness\")\n        plt.plot(self.best_fitness, 'ro-', linewidth=1, label=\"Best Fitness\")\n        plt.plot(self.mean_fitness, 'ko-', linewidth=1, label=\"Mean Fitness\")\n        plt.xlabel(\"Generation\")\n        plt.ylabel(\"Fitness value\")\n        plt.xlim(0, len(self.best_fitness))\n        plt.legend()\n        plt.tight_layout()\n        #plt.savefig(\"/media/rezwan/Study/#Undergrade/Glucose/Fig_GA/GA_plot_fit_Gl.pdf\", dpi = 100)\n        plt.show()\n    \n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport random\nimport numpy as np\nfrom deap import base,creator\nfrom deap import tools\n#import fitness_function as ff\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch.autograd import Variable\n#from torch_dnn_model import model_drop\n#### Seed\nimport random\nseed = 42\n# random.seed(seed)\nnp.random.seed(seed)\n\nclass Feature_Selection_GA_Wrap:\n    \"\"\"\n        FeaturesSelectionGA\n        This class uses Genetic Algorithm to find out the best features for an input model\n        using Distributed Evolutionary Algorithms in Python(DEAP) package. Default toolbox is\n        used for GA but it can be changed accordingly.\n\n    \n    \"\"\"\n    def __init__(self,x,y,model=None,cv_split=10,verbose=1, flag=\"torch\"):\n        \"\"\"\n            Parameters\n            -----------\n            model : scikit-learn supported model, \n                x :  {array-like}, shape = [n_samples, n_features]\n                     Training vectors, where n_samples is the number of samples \n                     and n_features is the number of features.\n \n                y  : {array-like}, shape = [n_samples]\n                     Target Values\n            cv_split: int\n                     Number of splits for cross_validation to calculate fitness.\n            \n            verbose: 0 or 1\n        \"\"\"\n        \n        self.flag = flag\n        self.x = x\n        self.y = y\n        self.n_features = x.shape[1]\n        \n        if self.flag == \"torch\":\n            self.model =  model_drop(self.n_features)\n        else:\n            self.model =  model\n            \n        \n        self.toolbox = None\n        self.creator = self._create()\n        self.cv_split = cv_split\n        \n        \n        self.verbose = verbose\n        if self.verbose==1:\n            print(\"Model will select best features among {} features using cv_split :{}.\".format(x.shape[1],cv_split))\n            print(\"Shape od train_x: {} and target: {}\".format(x.shape,y.shape))\n        self.final_fitness = []\n        self.fitness_in_generation = {}\n        self.best_ind = None\n        self.mean_fitness = []\n        self.best_fitness = []\n    \n    def evaluate(self,individual):\n        fit_obj =FitenessFunction(self.cv_split)\n        np_ind = np.asarray(individual)\n        if np.sum(np_ind) == 0:\n            fitness = 0.0\n        else:\n            feature_idx = np.where(np_ind==1)[0]\n            fitness = fit_obj.calculate_fitness(self.x[:,feature_idx], self.y, self.model, self.flag)\n        \n        if self.verbose == 1:\n            print(\"Individual: {}  Fitness_score: {} \".format(individual,fitness))\n        self.mean_fitness.append(fitness)    \n        return fitness,\n    \n    \n    def _create(self):\n        creator.create(\"FeatureSelect\", base.Fitness, weights=(1.0,))\n        creator.create(\"Individual\", list, fitness=creator.FeatureSelect)\n        return creator\n    \n    def create_toolbox(self):\n        \"\"\" \n            Custom creation of toolbox.\n            Parameters\n            -----------\n                self\n            Returns\n            --------\n                Initialized toolbox\n        \"\"\"\n        \n        self._init_toolbox()\n#        return toolbox\n        \n    def register_toolbox(self,toolbox):\n        \"\"\" \n            Register custom created toolbox. Evalute function will be registerd\n            in this method.\n            Parameters\n            -----------\n                Registered toolbox with crossover,mutate,select tools except evaluate\n            Returns\n            --------\n                self\n        \"\"\"\n        toolbox.register(\"evaluate\", self.evaluate)\n        self.toolbox = toolbox\n     \n    \n    def _init_toolbox(self):\n        toolbox = base.Toolbox()\n        toolbox.register(\"attr_bool\", random.randint, 0, 1)\n        # Structure initializers\n        toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, self.n_features)\n        toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n        return toolbox\n        \n        \n    def _default_toolbox(self):\n        toolbox = self._init_toolbox()\n        toolbox.register(\"mate\", tools.cxTwoPoint)\n        toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.1)\n#        toolbox.register(\"select\", tools.selTournament, tournsize=3)\n        toolbox.register(\"select\", tools.selRoulette)\n        toolbox.register(\"evaluate\", self.evaluate)\n        return toolbox\n    \n    def get_final_scores(self,pop,fits):\n        self.final_fitness = list(zip(pop,fits))\n        \n    \n        \n    def generate(self,n_pop,ngen,cxpb = 0.10,mutxpb = 0.2,set_toolbox = False):\n        \n        \"\"\" \n            Generate evolved population\n            Parameters\n            -----------\n                n_pop : {int}\n                        population size\n                cxpb  : {float}\n                        crossover probablity\n                mutxpb: {float}\n                        mutation probablity\n                n_gen : {int}\n                        number of generations\n                set_toolbox : {boolean}\n                              If True then you have to create custom toolbox before calling \n                              method. If False use default toolbox.\n            Returns\n            --------\n                Fittest population\n        \"\"\"\n        \n        \n        \n        if self.verbose==1:\n            print(\"Population: {}, crossover_probablity: {}, mutation_probablity: {}, total generations: {}\".format(n_pop,cxpb,mutxpb,ngen))\n        \n        if not set_toolbox:\n            self.toolbox = self._default_toolbox()\n        else:\n            raise Exception(\"Please create a toolbox.Use create_toolbox to create and register_toolbox to register. Else set set_toolbox = False to use defualt toolbox\")\n        pop = self.toolbox.population(n_pop)\n        CXPB, MUTPB, NGEN = cxpb,mutxpb,ngen\n\n        # Evaluate the entire population\n        print(\"EVOLVING.......\")\n        fitnesses = list(map(self.toolbox.evaluate, pop))\n        \n        for ind, fit in zip(pop, fitnesses):\n            ind.fitness.values = fit\n\n        for g in range(NGEN):\n            print(\"-- GENERATION {} --\".format(g+1))\n            offspring = self.toolbox.select(pop, len(pop))\n            self.fitness_in_generation[str(g+1)] = max([ind.fitness.values[0] for ind in pop])\n            # Clone the selected individuals\n            offspring = list(map(self.toolbox.clone, offspring))\n\n            # Apply crossover and mutation on the offspring\n            for child1, child2 in zip(offspring[::2], offspring[1::2]):\n                if random.random() < CXPB:\n                    self.toolbox.mate(child1, child2)\n                    del child1.fitness.values\n                    del child2.fitness.values\n\n            for mutant in offspring:\n                if random.random() < MUTPB:\n                    self.toolbox.mutate(mutant)\n                    del mutant.fitness.values\n\n            # Evaluate the individuals with an invalid fitness\n            weak_ind = [ind for ind in offspring if not ind.fitness.valid]\n            fitnesses = list(map(self.toolbox.evaluate, weak_ind))\n            ##=================== \n            print(\"Best fitness from a generation: \" +str(np.max(fitnesses)))\n            print(\"Mean fitness from a generation: \" +str(np.mean(fitnesses)))\n            self.best_fitness.append(np.max(fitnesses))\n            self.mean_fitness.append(np.mean(fitnesses))\n            ##=================\n            for ind, fit in zip(weak_ind, fitnesses):\n                ind.fitness.values = fit\n            print(\"Evaluated %i individuals\" % len(weak_ind))\n\n            # The population is entirely replaced by the offspring\n            pop[:] = offspring\n            \n                    # Gather all the fitnesses in one list and print the stats\n        fits = [ind.fitness.values[0] for ind in pop]\n        \n        length = len(pop)\n        mean = sum(fits) / length\n        sum2 = sum(x*x for x in fits)\n        std = abs(sum2 / length - mean**2)**0.10\n        if self.verbose==1:\n            print(\"  Min %s\" % min(fits))\n            print(\"  Max %s\" % max(fits))\n            print(\"  Avg %s\" % mean)\n            print(\"  Std %s\" % std)\n    \n        print(\"-- Only the fittest survives --\")\n\n        best_ind = tools.selBest(pop, 1)[0]\n        print(\"Best individual is %s, %s\" % (best_ind, best_ind.fitness.values))\n#        self.get_final_scores(pop,fits)\n        \n        return best_ind\n#        return pop\n    \n    def plot_feature_set_score(self,ngen):\n#        print(len(self.mean_fitness))\n#        print(len(self.best_fitness))\n        \n        plt.figure(figsize=(6,4))\n        plt.title(\"Feature Set Fitness\")\n        plt.plot(self.best_fitness, 'r', linewidth=1, label=\"Best Fitness\")\n        plt.plot(self.mean_fitness, 'k', linewidth=1, label=\"Mean Fitness\")\n        plt.xlabel(\"Generation\")\n        plt.ylabel(\"Fitness value\")\n        plt.xlim(0, len(self.best_fitness))\n        plt.legend()\n        plt.tight_layout()\n        plt.show()\n    \n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Tue Sep 15 18:59:21 2020\n\n@author: Rezwan\n\"\"\"\n\n\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Tue Sep 15 15:31:56 2020\n\n@author: Rezwan\n\"\"\"\n\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nimport numpy as np\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\n\n\n# measrmnt_indices import pearsonr, rmse, bland_altman_plot, act_pred_plot\n\n#### Seed\nimport random\nseed = 42\nnp.random.seed(seed)\n\n\n############ Read CSV file\ndataFr = pd.read_csv(\"../input/dataset-ga/Dataset_Gl.csv\")\ndataFr.head()\ndataFr.shape\n#dataFr.drop(dataFr.columns[[0,1]], axis=1, inplace=True)\ndataFr.head()\ndict = {'Sex':{'M':1, 'F':0}}      # label = column name\ndataFr.replace(dict,inplace = True) \ndf = dataFr\nprint(df.head())\nprint(df.shape)\n\nprint(df.isnull().any().any())\nprint(df.isnull().sum().sum())\nprint(df.isnull().any().any())\nprint(df.isnull().sum())\n\n\n\n## Min max Normalization:\n\ny = df.iloc[:, 48]\nX = df.iloc[:, 0:48]\n\nscaler = MinMaxScaler(feature_range=(0, 1))\n# scaler = StandardScaler()\n\nX = scaler.fit_transform(X)\n\n\n#####======================== Feature selection Method Start ===================\n\n\n### import GA's files for Filter Analysis\n#from feature_selection_ga_wrap import *\n#from feature_selection_ga_filter import *\n#from fitness_function import *\n\nmodel = SVR(kernel='rbf')\n# fsga = Feature_Selection_GA_Wrap(X,y, model)\nfsga = Feature_Selection_GA_Filter(X,y)\npop = fsga.generate(100,50 ) ## population size and Generation = 20, 50 || 10,50 (200, 100)\npp = fsga.plot_feature_set_score(50) ## Generation\n\nprint(\"Best Indices: \" +str(pop))\n\n\n#######################plot slescted features\ncolumns_names = df.columns.values\nselected_columns_names = []\nget_best_ind = []\nfor i in range(len(pop)):\n    if pop[i] == 1:\n        get_best_ind.append(i)\nprint(len(get_best_ind))        \nselected_columns_names =columns_names[get_best_ind]\n\nvalues_of_selected_col = []\nfor val in selected_columns_names:\n    values_of_selected_col.append(max(df[val]))\n    \n###############\n#plt.rcdefaults()\nplt.figure(figsize=(10,8))\nfig, ax = plt.subplots()\n# Example data\npeople = selected_columns_names\ny_pos = np.arange(len(people))\nperformance = values_of_selected_col\n\n\nax.barh(y_pos, np.array(performance), align='center',\n        color='green', ecolor='black')\nax.set_yticks(y_pos)\nax.set_yticklabels(people)\nax.invert_yaxis()  # labels read top-to-bottom\nax.set_xlabel('Value')\nax.set_title('Selected Features')\n\nplt.show()\n\n\n######======================= End Feature Selction =========================\n\n#################################=== Set it on Machine learning Model\nget_best_ind = []\nfor i in range(len(pop)):\n    if pop[i] == 1:\n        get_best_ind.append(i)\n        \nprint(len(get_best_ind))\n\nX_selct = X[:, get_best_ind]\nprint(X_selct.shape)\n\nmodel = SVR(kernel='rbf') ####### Model name\n\n############### Apply 10-fold Cross validation\nn_splits = 10\ncv_set = np.repeat(-1.,X_selct.shape[0])\nskf = KFold(n_splits = n_splits ,shuffle=True, random_state=42)\nfor train_index,test_index in skf.split(X_selct, y):\n    x_train,x_test = X_selct[train_index],X_selct[test_index]\n    y_train,y_test = y[train_index],y[test_index]\n    if x_train.shape[0] != y_train.shape[0]:\n        raise Exception()\n    model.fit(x_train,y_train)\n    predicted_y = model.predict(x_test)\n    # print(\"Individual R: \" +str(pearsonr(y_test, predicted_y)))\n    # cv_set[test_index] = predicted_y\n    print(\"Individual R^2: \" +str(metrics.r2_score(y_test, predicted_y)))\n    cv_set[test_index] = predicted_y\n \nprint(\"Overall R^2: \" + str(metrics.r2_score(y,cv_set)))\n\n### ===== For Get real values\n# y = (y * Xstds[48]) + Xmeans[48]\n# cv_set = (cv_set * Xstds[48]) + Xmeans[48]\n### ===============\n# print(\"Overall R: \" + str(pearsonr(y,cv_set)))\nprint(\"R^2 Score: \" + str(metrics.r2_score(y, cv_set)))\nprint(\"MAE: \" + str(metrics.mean_absolute_error(y, cv_set)))\nprint(\"MSE: \" + str(metrics.mean_squared_error(y, cv_set)))\nprint(\"RMSE: \" + str(rmse(y, cv_set)))\nprint(\"MSLE: \" + str(metrics.mean_squared_log_error(y, cv_set)))\nprint(\"EVS: \" + str(metrics.explained_variance_score(y, cv_set)))\n\n\n################# ====== plot bland_altman_plot\nbland_altman_plot(y, cv_set)\n\n##### === Plot estimated and predicted\n# r = pearsonr(y, cv_set)\nr_2 = metrics.r2_score(y,cv_set)\nact_pred_plot(y, cv_set, r_2)\n\n\n\n######### ========================================================================\n# scores = []\n# cv_set = np.repeat(-1.,X.shape[0])\n\n# best_svr = SVR(kernel='rbf') ### Model: SVR\n# # best_svr = LinearRegression()\n\n\n# cv = KFold(n_splits=10, random_state=42, shuffle=False)\n\n# for train_index, test_index in cv.split(X):\n#     print(\"Train Index: \", train_index, \"\\n\")\n#     print(\"Test Index: \", test_index)\n\n#     X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n    \n#     if X_train.shape[0] != y_train.shape[0]:\n#         raise Exception()\n    \n#     best_svr.fit(X_train,y_train)\n#     predicted_y = best_svr.predict(X_test)\n    \n#     print(\"Individual R^2: \" +str(metrics.r2_score(y_test, predicted_y)))\n#     cv_set[test_index] = predicted_y\n \n# print(\"Overall R^2: \" + str(metrics.r2_score(y, cv_set)))\n\n# print(\"R^2 Score: \" + str(metrics.r2_score(y, cv_set)))\n# print(\"MAE: \" + str(metrics.mean_absolute_error(y, cv_set)))\n# print(\"MSE: \" + str(metrics.mean_squared_error(y, cv_set)))\n# print(\"RMSE: \" + str(rmse(y, cv_set)))\n# print(\"MSLE: \" + str(metrics.mean_squared_log_error(y, cv_set)))\n# print(\"EVS: \" + str(metrics.explained_variance_score(y, cv_set)))\n    \n# ################\n\n# ##### Plot Actual and Prdiction values\n# plt.plot(y, color='b', label = 'Actual')\n# plt.plot(cv_set, color='r', label = 'Predicted')\n# plt.xlabel(\"Time\")\n# plt.ylabel(\"Hemoglobin Concentration(gm/dL)\")\n# plt.legend(loc='best')\n# plt.show()\n\n# #### bland_altman_plot\n# bland_altman_plot(y, cv_set)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}