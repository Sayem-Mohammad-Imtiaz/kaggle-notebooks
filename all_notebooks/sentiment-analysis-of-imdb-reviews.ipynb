{"cells":[{"metadata":{"_uuid":"50c306956ce01eba983c41e092501f0802b91cd5"},"cell_type":"markdown","source":"**Library Section**"},{"metadata":{"collapsed":true,"_uuid":"07645e269c8ce473f5b321084f9677d53a9cf348","trusted":true},"cell_type":"code","source":"################################################# import libraries ###########################################\n\nimport pandas as pd\nimport os\nfrom nltk.corpus import stopwords\nimport string\nimport re\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.metrics import adjusted_rand_score\nfrom collections import Counter\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.plotly as py\nimport operator\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom wordcloud import WordCloud\nimport time\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.cluster import AgglomerativeClustering","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"edcb3746ca2526e01bdea9e62e4fb4416ca4d160","trusted":true},"cell_type":"code","source":"def rem_sw(df):\n    # Downloading stop words\n    stop_words = set(stopwords.words('english'))\n\n    # Removing Stop words from training data\n    count = 0\n    for sentence in df:\n        sentence = [word for word in sentence.lower().split() if word not in stop_words]\n        sentence = ' '.join(sentence)\n        df.loc[count] = sentence\n        count+=1\n    return(df)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"0edc8ba17411c83b9388c84a059c55aea2e6ad1a","trusted":true},"cell_type":"code","source":"def rem_punc(df):\n    count = 0\n    for s in df:\n        cleanr = re.compile('<.*?>')\n        s = re.sub(r'\\d+', '', s)\n        s = re.sub(cleanr, '', s)\n        s = re.sub(\"'\", '', s)\n        s = re.sub(r'\\W+', ' ', s)\n        s = s.replace('_', '')\n        df.loc[count] = s\n        count+=1\n    return(df)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"d3faef1baa19c3de988c5b51c5b9138eaaa8f322","trusted":true},"cell_type":"code","source":"def lemma(df):\n\n    lmtzr = WordNetLemmatizer()\n\n    count = 0\n    stemmed = []\n    for sentence in df:    \n        word_tokens = word_tokenize(sentence)\n        for word in word_tokens:\n            stemmed.append(lmtzr.lemmatize(word))\n        sentence = ' '.join(stemmed)\n        df.iloc[count] = sentence\n        count+=1\n        stemmed = []\n    return(df)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"68fdcb1d70cd1b257f5c186ac8c860fd5e1ec19f","trusted":true},"cell_type":"code","source":"def stemma(df):\n\n    stemmer = SnowballStemmer(\"english\") #SnowballStemmer(\"english\", ignore_stopwords=True)\n\n    count = 0\n    stemmed = []\n    for sentence in df:\n        word_tokens = word_tokenize(sentence)\n        for word in word_tokens:\n            stemmed.append(stemmer.stem(word))\n        sentence = ' '.join(stemmed)\n        df.iloc[count] = sentence\n        count+=1\n        stemmed = []\n    return(df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc9b97f9b11d780bb3adf8fbabd26444e7d3fad7"},"cell_type":"markdown","source":"Phase 1: Preprocessing\n-------------------"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a785f46c93fcd9a033ae71e028f81fb7317ad27b"},"cell_type":"code","source":"df_master = pd.read_csv(\"../input/imdb_master.csv\", encoding='latin-1', index_col = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8270ea2c7d740bd2cf66970271ccd1d2aec7066","collapsed":true},"cell_type":"code","source":"df_master = df_master[df_master.label != 'unsup']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"afd95a479e5a8da186cf8698fedba343416c4957"},"cell_type":"code","source":"imdb_train = df_master[df_master['type'] == 'train'].copy()\nimdb_test =  df_master[df_master['type'] == 'test'].copy()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a0807640b3f0ed526a874882f26257020706d1ee","trusted":true,"collapsed":true},"cell_type":"code","source":"imdb_train['review'] = rem_sw(imdb_train['review'])\nimdb_test['review'] = rem_sw(imdb_test['review'])\n\nimdb_train['review'] = rem_punc(imdb_train['review'])\nimdb_test['review'] = rem_punc(imdb_test['review'])\n\nimdb_train['review'] = lemma(imdb_train['review'])\nimdb_train['review'] = stemma(imdb_train['review'])\n\nimdb_test['review'] = lemma(imdb_test['review'])\nimdb_test['review'] = stemma(imdb_test['review'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"975e2e5cbbb27a290586556c0665147fb3494ab3"},"cell_type":"markdown","source":"Phase 2: Model Training\n--------------------"},{"metadata":{"trusted":true,"_uuid":"7a4294eb9ff36f7d6a436acd3e1d0b9f5855a131","collapsed":true},"cell_type":"code","source":"from gensim.models import Word2Vec\n\nmodel = Word2Vec(imdb_train['review'].apply(lambda s: s.split()))\nmodel.save(\"word2vec.model\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"178f8e4b8a06fa63bce631fb34e052eace061c10","scrolled":true},"cell_type":"code","source":"model.wv.most_similar('movi')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7cd31f3a60438279a2b3532ffe35a763c8a8c06"},"cell_type":"code","source":"model.wv.words_closer_than('actor', 'star')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71b35333247531a0f3387e5da6c09f0169967b90"},"cell_type":"code","source":"model.wv.similarity('actor', 'star')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2ba4d3e57c51256fe722bc0d6b0544ceb1a54a3","collapsed":true},"cell_type":"code","source":"kmeans_args = {\n    'n_clusters': 1000,\n}\n\nclustering = KMeans(**kmeans_args).fit_predict(model.wv.vectors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"cf0c7fe26fb2e4eab6eb0ca50986a873c050e8bf"},"cell_type":"code","source":"word2centroid = {k: v for k, v in zip(model.wv.index2word, clustering)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d571b0c0bb1aa71000c68ace41f5212ac6f1d72b","collapsed":true},"cell_type":"code","source":"from numpy import zeros\n\ndef make_bag_of_centroids(sentence, word_centroid_map, cluster_size):\n    centroids = zeros(cluster_size, dtype=\"float32\")\n\n    for word in sentence:\n        if word in word_centroid_map:\n            centroids[word_centroid_map[word]] += 1\n\n    return centroids\n\nas_centroid = lambda s: make_bag_of_centroids(s.split(), word2centroid, kmeans_args['n_clusters'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea3e160ba1462edf13431d0b60fea0648d2b0508"},"cell_type":"code","source":"imdb_train[:1].review.apply(as_centroid).tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0b08ac671caac9c68384f70cc277dce670c72b5","collapsed":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.preprocessing import scale\n\nfit = XGBClassifier().fit(scale(imdb_train.review.apply(as_centroid).tolist()), imdb_train.label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"3004c2b9f9d1e07d5aba4c3dbe092307e75a9a39","collapsed":true},"cell_type":"code","source":"predictions = fit.predict(scale(imdb_test.review.apply(as_centroid).tolist()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f51e2c0685ec963aff244c01b5807641c3c9d5fd"},"cell_type":"code","source":"sum(True for a,b in zip(predictions, imdb_test.label) if a == b) / len(imdb_test), sum(True for a,b in zip(predictions, imdb_test.label) if a != b) / len(imdb_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"645217c8db8c877a5c8711c40c6072ec293a6c63"},"cell_type":"code","source":"imdb_test['prediction'] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77f0b83e575e1fe6c6b6b14aaca4a6958d4a45f2"},"cell_type":"code","source":"# from http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n\nimport itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import confusion_matrix\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(imdb_test.label, predictions)\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['pos', 'neg'],\n                      title='Confusion matrix, without normalization')\n\n# Plot normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['pos', 'neg'], normalize=True,\n                      title='Normalized confusion matrix')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e6b6ccdb2865019b93fe1fd67c15e227d4794de"},"cell_type":"code","source":"imdb_test[imdb_test.label != imdb_test.prediction]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}