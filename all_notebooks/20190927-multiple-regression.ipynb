{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Multiple Regression\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the dataset\ndataset = pd.read_csv('../input/50-startups/50_Startups.csv')\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, 4].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Try running this code, you'll get a warning message. \n\n# Encoding categorical data\n# Encoding the Independent Variable\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder_X = LabelEncoder()\nX[:, 3] = labelencoder_X.fit_transform(X[:, 3]) #Change text into number\nonehotencoder = OneHotEncoder(categorical_features = [3])\nX = onehotencoder.fit_transform(X).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding categorical data\n# Encoding the Independent Variable\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nct = ColumnTransformer(\n    [('one_hot_encoder', OneHotEncoder(), [3])],    # The column numbers to be transformed (here is [3] but can be [0, 1, 3])\n    remainder='passthrough'                         # Leave the rest of the columns untouched\n)\n\nX = np.array(ct.fit_transform(X), dtype=np.float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Avoiding the dummy variable trap\n#Remove the first column (Column 0th) -> Take values from column 1 onward\nX = X[:, 1:] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting Multiple Linear Regression to the Training Set\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predicting the Test Set Results\ny_pred = regressor.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Building the optimal model using Backward Elimination\nimport statsmodels.api as sm\nX = np.append(arr=np.ones((50,1)).astype(int), values = X, axis=1) #Add 1 to the 1st column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_opt is the X that contains only the optimal variables\nX_opt = X[:,[0,1,2,3,4,5]] # Initialize the metrix\nregressor_OLS = sm.OLS(endog=y, exog = X_opt).fit()\nregressor_OLS.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The number shows that x2 has the highest P-value (0.99), so we remove it.\nX_opt = X[:,[0,1,3,4,5]] # Initialize the metrix\nregressor_OLS = sm.OLS(endog=y, exog = X_opt).fit()\nregressor_OLS.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The number shows that x1 has the highest P-value (0.94), so we remove it.\nX_opt = X[:,[0,3,4,5]] # Initialize the metrix\nregressor_OLS = sm.OLS(endog=y, exog = X_opt).fit()\nregressor_OLS.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The number shows that x4 has the highest P-value (0.6), so we remove it.\nX_opt = X[:,[0,3,5]] # Initialize the metrix\nregressor_OLS = sm.OLS(endog=y, exog = X_opt).fit()\nregressor_OLS.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The number shows that x5 has the highest P-value (0.06, so we remove it.\nX_opt = X[:,[0,3]] # Initialize the metrix\nregressor_OLS = sm.OLS(endog=y, exog = X_opt).fit()\nregressor_OLS.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}