{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Fake News Detection Model - NLP Transfer Learning\n\n\n\n|Version| Changes| Score|\n|--------|-------|------|\n|Version 1| Pretrained model NNML-128DIM| 2.26|\n|Version 2| Embedding using Pad Sequence| 1.8022|\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import re\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings('ignore')\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfrom nltk.stem import WordNetLemmatizer\n\n# Tensorflow libraries\n# Tensorflow libraries\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing import text, sequence\n# from tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import regularizers\n\nimport tensorflow_hub as hub\n\n\n# sklearn libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import roc_auc_score\n\nfrom gensim.models import Word2Vec # Word2Vec module\nfrom gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_punctuation, remove_stopwords, strip_numeric, stem_text\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## Load Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/fake-news-content-detection/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/fake-news-content-detection/test.csv\")\nsubmission_data = pd.read_csv(\"/kaggle/input/fake-news-content-detection/sample submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sample data from training data\ntrain_data.sample(3)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataset information\ntrain_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[train_data.duplicated(['Text'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_data.drop_duplicates(['Text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.sample(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split data into Train and Holdout"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['NewsText'] = train_data['Text_Tag'].astype(str) +\" \"+ train_data['Text']\ntest_data['NewsText'] = test_data['Text_Tag'].astype(str) +\" \"+ test_data['Text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stemmer object\nwnl = WordNetLemmatizer()\n\nclass DataPreprocess:\n    \n    def __init__(self):\n        self.filters = [strip_tags,\n                       strip_numeric,\n                       strip_punctuation,\n                       lambda x: x.lower(),\n                       lambda x: re.sub(r'\\s+\\w{1}\\s+', '', x),\n                       remove_stopwords]\n    def __call__(self, doc):\n        clean_words = self.__apply_filter(doc)\n        return clean_words\n    \n    def __apply_filter(self, doc):\n        try:\n            cleanse_words = set(preprocess_string(doc, self.filters))\n            filtered_words = set(wnl.lemmatize(w, 'v') for w in cleanse_words)\n            return ' '.join(cleanse_words)\n        except TypeError as te:\n            raise(TypeError(\"Not a valid data {}\".format(te)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['Processed'] = train_data['NewsText'].apply(DataPreprocess())\ntest_data['Processed'] = test_data['NewsText'].apply(DataPreprocess())\n\n# train_data['Processed'] = train_data['Text'].apply(DataPreprocess())\n# test_data['Processed'] = test_data['Text'].apply(DataPreprocess())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data['Processed']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_data['Processed']\ny = train_data['Labels']\n\ny_category = keras.utils.to_categorical(y, 6)\n\n# Split data into Train and Holdout as 80:20 ratio\nX_train, X_valid, y_train, y_valid = train_test_split(X, y_category, shuffle=True, test_size=0.33, random_state=111)\n\nprint(\"Train shape : {}, Holdout shape: {}\".format(X_train.shape, X_valid.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Compute class weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"def word_embedding(train, test, max_features, max_len=200):\n    try:\n        # Keras Tokenizer class object\n        tokenizer = text.Tokenizer(num_words=max_features)\n        tokenizer.fit_on_texts(train)\n        \n        train_data = tokenizer.texts_to_sequences(train)\n        test_data = tokenizer.texts_to_sequences(test)\n        \n        # Get the max_len\n        vocab_size = len(tokenizer.word_index) + 1\n        \n        # Padd the sequence based on the max-length\n        x_train = sequence.pad_sequences(train_data, maxlen=max_len, padding='post')\n        x_test = sequence.pad_sequences(test_data, maxlen=max_len, padding='post')\n        # Return train, test and vocab size\n        return tokenizer, x_train, x_test, vocab_size\n    except ValueError as ve:\n        raise(ValueError(\"Error in word embedding {}\".format(ve)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features = 5000\nmax_len = 128\noutput_dim = len(np.unique(y))\n\n# Test data\nX_test = test_data['Processed']\n\ntokenizer, x_pad_train, x_pad_valid, vocab_size = word_embedding(X_train, X_valid, max_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test data\nX_test = test_data['Processed']\n\ntokenizer.fit_on_sequences(X_test)\n\nX_test_seq = tokenizer.texts_to_sequences(X_test)\nx_pad_test = sequence.pad_sequences(X_test_seq, maxlen=max_len, padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_classweights(target):\n    \"\"\"\n    Computes the weights of the target values based on the samples\n    :param target: Y-target variable\n    :return: dictionary object\n    \"\"\"\n    # compute class weights\n    class_weights = class_weight.compute_class_weight('balanced',\n                                                     np.unique(target),\n                                                     target)\n    \n    # make the class weight list into dictionary\n    weights = {}\n    \n    # enumerate the list\n    for index, weight in enumerate(class_weights):\n        weights[index] = weight\n        \n    return weights\n\n# Get the class weights for the target variable\nweights = compute_classweights(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_rnn(vocab_size, output_dim, max_len):\n    # Building RNN model\n    model = Sequential([\n        keras.layers.Embedding(vocab_size,128,\n                              input_length=max_len),\n        keras.layers.BatchNormalization(),\n#         keras.layers.Bidirectional(keras.layers.LSTM(128,return_sequences=True)),\n        keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.002)),\n        keras.layers.GlobalMaxPool1D(), # Remove flatten layer\n        keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.002)),\n        keras.layers.Dropout(0.3),\n        keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.002)),\n        keras.layers.Dropout(0.3),\n        keras.layers.Dense(output_dim, activation='softmax')\n    ])\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rnn_model = build_rnn(vocab_size, output_dim, max_len)\n\n# Summary of the model\nrnn_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compile the model\nrnn_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), \n                  loss=keras.losses.CategoricalCrossentropy(from_logits=True), \n                  metrics=[tf.metrics.AUC()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = rnn_model.fit(x_pad_train, \n                        y_train,\n                        batch_size=512,\n                        epochs=20,\n                        verbose=1,\n                        validation_data=(x_pad_valid, y_valid),\n                       class_weight=weights)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = rnn_model.evaluate(x_pad_valid, y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_preds = rnn_model.predict_proba(x_pad_test, batch_size=256)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_preds[:,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df = pd.DataFrame({'0': y_preds[:,0],\n                        '1': y_preds[:,1],\n                        '2': y_preds[:,2],\n                        '3': y_preds[:,3],\n                        '4': y_preds[:,4],\n                        '5': y_preds[:,5]}, index=test_data.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df.to_csv(\"fake_news_ann_08.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}