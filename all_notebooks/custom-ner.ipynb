{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\ndf=pd.read_json('../input/nlp-course/restaurant.json')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"menu = [\"Cheese Steak\", \"Cheesesteak\", \"Steak and Cheese\", \"Italian Combo\", \"Tiramisu\", \"Cannoli\",\n        \"Chicken Salad\", \"Chicken Spinach Salad\", \"Meatball\", \"Pizza\", \"Pizzas\", \"Spaghetti\",\n        \"Bruchetta\", \"Eggplant\", \"Italian Beef\", \"Purista\", \"Pasta\", \"Calzones\",  \"Calzone\",\n        \"Italian Sausage\", \"Chicken Cutlet\", \"Chicken Parm\", \"Chicken Parmesan\", \"Gnocchi\",         \n        \"Chicken Pesto\", \"Turkey Sandwich\", \"Turkey Breast\", \"Ziti\", \"Portobello\", \"Reuben\",\n        \"Mozzarella Caprese\",  \"Corned Beef\", \"Garlic Bread\", \"Pastrami\", \"Roast Beef\",              #menu items already given\n        \"Tuna Salad\", \"Lasagna\", \"Artichoke Salad\", \"Fettuccini Alfredo\", \"Chicken Parmigiana\",\n        \"Grilled Veggie\", \"Grilled Veggies\", \"Grilled Vegetable\", \"Mac and Cheese\", \"Macaroni\",  \n         \"Prosciutto\", \"Salami\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Getting the train data"},{"metadata":{"trusted":true},"cell_type":"code","source":"menu_lower=[x.lower() for x in menu]     #converting all items to lowercase\n\nimport spacy\nnlp=spacy.load('en_core_web_lg')\n\nmenu_doc=[nlp(item) for item in menu_lower] #converting the list of menu items into doc objects","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\ntext_list=[_.rstrip() for _ in df['text']]\ntext_list=text_list[:100]                  #getting the first 100 reviews from the dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(text_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"text_doc=[nlp(text) for text in text_list]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sent_list=[]           #converting the text into sentences so that we can only extract sentences with the food item for training data\nfor doc in text_doc:\n    for sent in doc.sents:\n        sent_list.append(sent.text.rstrip())\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(sent_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sent_doc=[nlp(doc) for doc in sent_list] #sent_list is a list of strings so converting into list of docs to apply matcher ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom spacy.matcher import PhraseMatcher \n\nmatcher=PhraseMatcher(nlp.vocab,attr='LOWER')\nmatcher.add('Menu',menu_doc)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for doc in sent_doc:\n    matches=matcher(doc)\n    for match_id,start,end in matches:\n        print(doc[start:end],doc[start:end].start_char,doc[start:end].end_char)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_DATA=[]                                        #making the training data in the format needed\nfor doc in sent_doc:\n    ent=[]\n    matches=matcher(doc)\n    if matches:\n        for match_id,start,end in matches:                                         \n            ent.append((doc[start:end].start_char,doc[start:end].end_char,\"FOOD\"))\n        TRAIN_DATA.append((f\"{doc}\", {\"entities\": ent}))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(TRAIN_DATA))\nTRAIN_DATA\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the ner component\nner=nlp.get_pipe('ner')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LABEL=\"FOOD\"             # Add the new label to ner\nner.add_label(LABEL)\n\n# Resume training\noptimizer = nlp.resume_training()\nmove_names = list(ner.move_names)\n\n# List of pipes you want to train\npipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n\n# List of pipes which should remain unaffected in training\nother_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing requirements\nfrom spacy.util import minibatch, compounding\nimport random\n\n# Begin training by disabling other pipeline components\nwith nlp.disable_pipes(*other_pipes) :\n    sizes = compounding(1.0, 4.0, 1.001)\n    # Training for 30 iterations     \n    for itn in range(30):\n        # shuffle examples before training\n        random.shuffle(TRAIN_DATA)\n        # batch up the examples using spaCy's minibatch\n        batches = minibatch(TRAIN_DATA, size=sizes)\n        # ictionary to store losses\n        losses = {}\n        for batch in batches:\n            texts, annotations = zip(*batch)\n            # Calling update() over the iteration\n            nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n            print(\"Losses\", losses)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predicting on NEW data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from spacy import displacy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp.pipe_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing the NER\n\ntest_text = \"I just love the cheese maggi with toppings & the Biryani too.\"\ndoc = nlp(test_text)\n\ndisplacy.render(doc,style='ent')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_text = \"Whenever we visit we order Fish curry and the kids love it!!!.The Ghee Dosa is delicious.Yesterday was fun\"\ndoc = nlp(test_text)\n\ndisplacy.render(doc,style='ent')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_text = \"The food is horrible.Vada Pav tastes like hard cardboard.Never gonna eat Idli here,it tastes horrible! \"\ndoc = nlp(test_text)\n\ndisplacy.render(doc,style='ent')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_text = \"The Chapathi is green and sticky,such a waste of $15 .I would rather stay hungry than to put Roti,Beef in my mouth \"\ndoc = nlp(test_text)\n\ndisplacy.render(doc,style='ent')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Another set of training data\n\n\n# # New label to add\n# LABEL = \"FOOD\"\n\n# # Training examples in the required format\n# TRAIN_DATA =[ (\"Pizza is a common fast food.\", {\"entities\": [(0, 5, \"FOOD\")]}),\n#               (\"Cheese Pasta is an italian recipe.I love Cheese Pasta\", {\"entities\": [(0, 12, \"FOOD\"),(41,53,\"FOOD\")]}),\n#               (\"China's noodles are very famous\", {\"entities\": [(8,15, \"FOOD\")]}),\n#               (\"Shrimps are famous in China too\", {\"entities\": [(0,7, \"FOOD\")]}),\n#               (\"Lasagna is another classic of Italy\", {\"entities\": [(0,7, \"FOOD\")]}),\n#               (\"Sushi is extemely famous and expensive Japanese dish\", {\"entities\": [(0,5, \"FOOD\")]}),\n#               (\"Unagi is a famous seafood of Japan\", {\"entities\": [(0,5, \"FOOD\")]}),\n#               (\"Tempura , Soba are other famous dishes of Japan\", {\"entities\": [(0,7, \"FOOD\")]}),\n#               (\"Udon is a healthy type of noodles\", {\"entities\": [(0,4, \"ORG\")]}),\n#               (\"Chocolate souffl√© is extremely famous french cuisine\", {\"entities\": [(0,17, \"FOOD\")]}),\n#               (\"Flamiche is french pastry\", {\"entities\": [(0,8, \"FOOD\")]}),\n#               (\"Burgers are the most commonly consumed fastfood\", {\"entities\": [(0,7, \"FOOD\")]}),\n#               (\"Burgers are the most commonly consumed fastfood\", {\"entities\": [(0,7, \"FOOD\")]}),\n#               (\"Frenchfries are considered too oily\", {\"entities\": [(0,11, \"FOOD\")]})\n#            ]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}