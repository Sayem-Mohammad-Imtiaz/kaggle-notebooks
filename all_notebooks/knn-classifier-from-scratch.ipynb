{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this notebook, I attempted to develop a KNN classifier from scratch. It is an open-to-develop model, so I would be glad to hear your suggestions.","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n#To develop the KNN classifier from scratch\nfrom collections import Counter\n\n#To evaluate the performance of the model\nimport numpy as np\nfrom numpy import genfromtxt\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import confusion_matrix,accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier \nimport time\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Developing the KNN classifier from scratch","metadata":{}},{"cell_type":"code","source":"# Minkowski distance formula for classifier\ndef minkowski(x,y,p):\n    inner=[]\n    for i in range(len(x)):\n        inner.append(pow((abs(x[i]-y[i])),p))\n    dist=pow(sum(inner),1/p)\n    return dist","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class KNN_classifier: \n    def __init__(self,k=5,p=2):\n        self.k = k\n        self.p = p\n        \n    #List to hold x_train and y_train together   \n    def labeled_data(self, x_train, y_train):\n        self.labeled_data=list(zip(x_train,y_train))        \n        \n    #Prediction method    \n    def predictor(self, x_test):        \n        predictions=[]\n        for point in x_test:\n            distances=[]\n            for line in self.labeled_data:                \n                distances.append([(minkowski(point,line[0],self.p)),line[-1]])\n                \n            distances.sort(key=lambda x: x[0])\n            classes=list(map(lambda x: x[1], distances[:self.k]))\n            predictions.append(Counter(classes).most_common()[0][0])\n               \n        return predictions ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing the performance of the model","metadata":{}},{"cell_type":"code","source":"data_array = genfromtxt('/kaggle/input/seed-from-uci/Seed_Data.csv', delimiter=',',skip_header=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the seed data set, the first 7 columns hold the features and the last column holds the labels. We have a total of 210 data belonging to 3 classes, without any null-values.","metadata":{}},{"cell_type":"code","source":"features=data_array[:,:7]\nlabel=data_array[:,7:8].reshape(-1)\nX_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.3, random_state=0, stratify=label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's evaluate the performance of KNN classifiers, both developed by me and available on Sklearn for different k values.","metadata":{}},{"cell_type":"code","source":"#My KNN classifier\nstart = time.time()\nmy_accuracies=[]\nmy_confusion_matrices=[]\nmy_times=[]\n#k parameter tuning\nfor i in range(1,20):\n    inner_start = time.time()\n    \n    #model\n    knn = KNN_classifier(k=i)\n    knn.labeled_data(X_train,y_train)\n    predicted=knn.predictor(X_test)\n    \n    #indicators\n    my_accuracies.append([(accuracy_score(y_test, predicted)),i])\n    my_confusion_matrices.append([(confusion_matrix(y_test, predicted)),i])\n    inner_end = time.time()\n    my_times.append([(inner_end-inner_start),i])\n    \nend = time.time()\nprint(\"Total Time:\",end - start)\n\nsorted_my_accuracies=sorted(my_accuracies,key=lambda x: x[0], reverse=True)  \nprint(\"Top 5 Accuracies:\",my_accuracies[:5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sklearn's KNN classifier\nstart = time.time()\nsklearn_accuracies=[]\nsklearn_confusion_matrices=[]\nsklearn_times=[]\n#k parameter tuning\nfor i in range(1,20):\n    inner_start = time.time()\n    \n    #model\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    y_pred = knn.predict(X_test)\n    \n    #indicators\n    sklearn_accuracies.append([(accuracy_score(y_test, y_pred)),i])\n    sklearn_confusion_matrices.append([(confusion_matrix(y_test, y_pred)),i])\n    inner_end = time.time()\n    sklearn_times.append([(inner_end-inner_start),i])\nend = time.time()\nprint(\"Total Time:\",end - start)    \n\nsorted_sklearn_accuracies=sorted(sklearn_accuracies,key=lambda x: x[0], reverse=True)  \nprint(\"Top 5 Accuracies:\",sorted_sklearn_accuracies[:5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=range(1,20)\nplt.plot(x, list(map(lambda x: x[0], my_accuracies)),marker='o', label = \"My KNN\")\nplt.plot(x, list(map(lambda x: x[0], sklearn_accuracies)), marker='o',label = \"Sklearn KNN\")\nplt.xlabel(\"Number of Neighbors\")\nplt.ylabel(\"Accuracies\")\nplt.title('Accuracy Comparison of KNN Classifiers for Different K Values')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"My model appears to give lower accuracy for k = 10 and k = 15 compared to the Sklearn model. However, the same accuracy result was obtained for k values other than that.","metadata":{}},{"cell_type":"code","source":"print(\"My results for k=2\")\nprint(\"------------------\")\nprint(\"Accuracy:\", my_accuracies[1][0] )\nprint(\"Time:\",my_times[1][0])\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test, predicted))\nprint(\"****************************************\")\nprint(\"Sklearn results for k=2\")\nprint(\"------------------\")\nprint(\"Accuracy:\", sklearn_accuracies[1][0] )\nprint(\"Time:\",sklearn_times[1][0])\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When we compare the models for k = 2, which is one of the small k values that gives the highest accuracy in both classifiers, it is seen that the confusion matrices are the same. The most significant difference appears to be time for the k = 2 value. Sklearn's KNN arranges the data for finding the closest neighbors efficiently during the prediction phase. Since my model does not do such a process, the time difference is an expected result and this part can be improved.","metadata":{}}]}