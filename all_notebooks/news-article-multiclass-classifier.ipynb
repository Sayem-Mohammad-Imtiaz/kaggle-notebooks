{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Classification of News Articles \n\nIt is a notebook for multiclass classification of News articles which are having classes numbered 1 to 4, where 1 is \"World News\", 2 is \"Sports News\", 3 is \"Business News\" and 4 is \"Science-Technology News\".","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Importing libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport nltk\nimport string as s\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport re\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data=pd.read_csv(\"/kaggle/input/ag-news-classification-dataset/train.csv\",header=0,names=['classid','title','desc'])\ntest_data=pd.read_csv(\"/kaggle/input/ag-news-classification-dataset/test.csv\",header=0,names=['classid','title','desc'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analyzing Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Countplot of Train data**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train_data.classid);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Countplot of Testdata**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(test_data.classid);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seeing the countplot of the training data and testing data we can say that the datasets are balanced","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Splitting Data into Input and Label ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x=train_data.desc\ntest_x=test_data.desc\ntrain_y=train_data.classid\ntest_y=test_data.classid","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing of Data\n\nThe data is preprocessed, in NLP it is also known as text normalization. Some of the most common methods of text normalization are \n* Tokenization\n* Lemmatization\n* Stemming\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Removal of HTML tags","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_html(text):\n    html_pattern = re.compile('<.*?>')\n    return html_pattern.sub(r'', text)\n\ntrain_x=train_x.apply(remove_html)\ntest_x=test_x.apply(remove_html)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Removal of URLs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_urls(text):\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)\n\ntrain_x=train_x.apply(remove_urls)\ntest_x=test_x.apply(remove_urls)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tokenization of Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def word_tokenize(txt):\n    tokens = re.findall(\"[\\w']+\", txt)\n    return tokens\ntrain_x=train_x.apply(word_tokenize)\ntest_x=test_x.apply(word_tokenize)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Removal of Stopwords","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stopwords(lst):\n    stop=stopwords.words('english')\n    new_lst=[]\n    for i in lst:\n        if i.lower() not in stop:\n            new_lst.append(i)\n    return new_lst\n\ntrain_x=train_x.apply(remove_stopwords)\ntest_x=test_x.apply(remove_stopwords) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Removal of Punctuation Symbols","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_punctuations(lst):\n    new_lst=[]\n    for i in lst:\n        for  j in  s.punctuation:\n            i=i.replace(j,'')\n        new_lst.append(i)\n    return new_lst\ntrain_x=train_x.apply(remove_punctuations) \ntest_x=test_x.apply(remove_punctuations)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Removal of Numbers(digits)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_numbers(lst):\n    nodig_lst=[]\n    new_lst=[]\n\n    for i in  lst:\n        for j in  s.digits:\n            i=i.replace(j,'')\n        nodig_lst.append(i)\n    for i in  nodig_lst:\n        if  i!='':\n            new_lst.append(i)\n    return new_lst\ntrain_x=train_x.apply(remove_numbers)\ntest_x=test_x.apply(remove_numbers)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stemming of Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\n\ndef stemming(text):\n    porter_stemmer = nltk.PorterStemmer()\n    roots = [porter_stemmer.stem(each) for each in text]\n    return (roots)\n\ntrain_x=train_x.apply(stemming)\ntest_x=test_x.apply(stemming)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lemmatization of Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmatizer=nltk.stem.WordNetLemmatizer()\ndef lemmatzation(lst):\n    new_lst=[]\n    for i in lst:\n        i=lemmatizer.lemmatize(i)\n        new_lst.append(i)\n    return new_lst\ntrain_x=train_x.apply(lemmatzation)\ntest_x=test_x.apply(lemmatzation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_extrawords(lst):\n    stop=['href','lt','gt','ii','iii','ie','quot','com']\n    new_lst=[]\n    for i in lst:\n        if i not in stop:\n            new_lst.append(i)\n    return new_lst\n\ntrain_x=train_x.apply(remove_extrawords)\ntest_x=test_x.apply(remove_extrawords) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x=train_x.apply(lambda x: ''.join(i+' ' for i in x))\ntest_x=test_x.apply(lambda x: ''.join(i+' '  for i in x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Extraction\n \n Features are extracted from the dataset and TF-IDF(Term Frequency - Inverse Document Frequency) is used for this purpose.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text  import TfidfVectorizer\ntfidf=TfidfVectorizer(min_df=8,ngram_range=(1,3))\ntrain_1=tfidf.fit_transform(train_x)\ntest_1=tfidf.transform(test_x)\nprint(\"No. of features extracted\")\nprint(len(tfidf.get_feature_names()))\nprint(tfidf.get_feature_names()[:100])\n\ntrain_arr=train_1.toarray()\ntest_arr=test_1.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(train_arr[:100], columns=tfidf.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training of Model\n\n### Model 1- Multinomial Naive Bayes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.naive_bayes  import MultinomialNB \nNB_MN=MultinomialNB(alpha=0.52)\nNB_MN.fit(train_arr,train_y)\npred=NB_MN.predict(test_arr)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"first 20 actual labels\")\nprint(test_y.tolist()[:20])\nprint(\"first 20 predicted labels\")\nprint(pred.tolist()[:20])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation of Results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics  import f1_score,accuracy_score\nprint(\"F1 score of the model\")\nprint(f1_score(test_y,pred,average='micro'))\nprint(\"Accuracy of the model\")\nprint(accuracy_score(test_y,pred))\nprint(\"Accuracy of the model in percentage\")\nprint(round(accuracy_score(test_y,pred)*100,3),\"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import  confusion_matrix\nsns.set(font_scale=1.5)\ncof=confusion_matrix(test_y, pred)\ncof=pd.DataFrame(cof, index=[i for i in range(1,5)], columns=[i for i in range(1,5)])\nplt.figure(figsize=(8,8))\n\nsns.heatmap(cof, cmap=\"PuRd\",linewidths=1, annot=True,square=True,cbar=False,fmt='d',xticklabels=['World','Sports','Business','Science'],yticklabels=['World','Sports','Business','Science'])\nplt.xlabel(\"Predicted Class\");\nplt.ylabel(\"Actual Class\");\n\nplt.title(\"Confusion Matrix for News Article Classification\");","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}