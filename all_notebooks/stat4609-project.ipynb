{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"Possible Features:\nNLP\nSentiment(Distribution)\nPCA\nT-SNE\n\nClassification Model:\n"},{"metadata":{},"cell_type":"markdown","source":"# # fake accounts"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fake = pd.read_csv(\"/kaggle/input/social-network-fake-account-dataset/fake_account.csv\",sep='\\t', names=['id', 'post'])\ndf_fake","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fake.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fake = df_fake.dropna()\ndf_fake","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import jieba\nimport re\nimport string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from emoji import UNICODE_EMOJI\nUNICODE_EMOJI","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seg_list = [list(jieba.cut(s, cut_all=False)) for s in df_fake['post']]\nseg_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eng_list = [[w for w in seg_list[i] if re.match('^[a-z|A-Z|]*$',w)] for i in range(len(seg_list))]\n# posts that contain English characters\neng_list1 = [l for l in eng_list if l != []]\neng_list1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(eng_list1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list of posts that contain only Chinese\npunc = \"！？｡。＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃《》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏. \\t\\n\"\nall_chin = [[w for w in seg_list[i] \n             if w not in punc \n             and w not in string.punctuation \n             and w not in UNICODE_EMOJI\n             and not re.match('^[a-z|A-Z|0-9|.]*$',w) ] for i in range(len(seg_list))]\nall_chin","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of posts that contain only English\nall_eng = [l for l in all_chin if l == []]\nlen(all_eng)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# # convert to simplified Chinese"},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install mafan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import mafan\nfrom mafan import simplify, tradify\nall_chin_simp = [[simplify(w) for w in all_chin[i]] for i in range(len(all_chin))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"all_chin_simp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# # try tf-idf vectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"# without removing stop words\ndocument = [\" \".join(s) for s in all_chin_simp]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"document","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_model = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\").fit(document)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_model.vocabulary_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# # try stop words"},{"metadata":{"trusted":true},"cell_type":"code","source":"#还没想好怎么弄。。。","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# # random checking"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"np.random.seed(10)\nfor i in np.random.randint(109945, size=20):\n    print(str(i)+':',seg_list[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check how many posts contain emoji\nemoji_list = [[w for w in seg_list[i] if w in UNICODE_EMOJI] for i in range(len(seg_list))]\nlen([l for l in emoji_list if l != []])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# posts in all_chin list that contain emoji\nchin_emoji_list = [[w for w in all_chin[i] if w in UNICODE_EMOJI] for i in range(len(all_chin))]\nlen([l for l in chin_emoji_list if l != []])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# legitimate accounts"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_real = pd.read_csv(\"/kaggle/input/social-network-fake-account-dataset/legitimate_account.csv\",\n                      sep='\\t',\n                      names=['id', 'time', 'forward', 'comment', 'like', 'post'])\ndf_real","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_real = df_real.dropna().reset_index(drop=True)\ndf_real","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of unique users\nlen(df_real['id'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seg_list_real = [list(jieba.cut(s, cut_all=False)) for s in df_real['post']]\nseg_list_real","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eng_list_real = [[w for w in seg_list_real[i] if re.match('^[a-z|A-Z|]*$',w)] for i in range(len(seg_list_real))]\neng_list_real1 = [l for l in eng_list_real if l != []]\neng_list_real1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of posts that contain English characters\nlen(eng_list_real1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nall_chin_real = [[w for w in seg_list_real[i] \n                  if w not in punc \n                  and w not in string.punctuation \n                  and w not in UNICODE_EMOJI\n                  and not re.match('^[a-z|A-Z|0-9|.]*$',w) ] for i in range(len(seg_list_real))]\nall_chin_real","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_chin_simp_real = [[simplify(w) for w in all_chin_real[i]] for i in range(len(all_chin_real))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_chin_simp_real","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of posts that contain only English\nall_eng_real = [l for l in all_chin_real if l == []]\nlen(all_eng_real)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# # random checking"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(3)\nfor i in np.random.randint(1225881, size=20):\n    print(str(i)+':',seg_list_real[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# # emoji checking"},{"metadata":{"trusted":true},"cell_type":"code","source":"# check how many posts contain emoji\nemoji_list_real = [[w for w in seg_list_real[i] if w in UNICODE_EMOJI] for i in range(len(seg_list_real))]\nlen([l for l in emoji_list_real if l != []])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[l for l in emoji_list_real if l != []]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# posts in all_chin list that contain emoji\nchin_emoji_list_real = [[w for w in all_chin_real[i] if w in UNICODE_EMOJI] for i in range(len(all_chin_real))]\nlen([l for l in chin_emoji_list_real if l != []])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}