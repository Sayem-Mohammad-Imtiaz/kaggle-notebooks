{"cells":[{"metadata":{},"cell_type":"markdown","source":"Roteiro do Trabalho.\n\n1. Leitura do Dataset \n1. Pré-Processamento\n    - Remoção de Tags\n    - Caracteres Especiais\n    - Emojis\n    - StopWords\n1. Embbeding USE\n1. clusterização\n1. Resultados\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport re\nimport seaborn as sns\nimport io","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Leitura do Dataset"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#abrir Tweets.csv diretamente de uma pasta no seu computador.\n#Executar a celula e usar o \"choose files\" para selecionar o arquivo \"Tweets.csv\"\npd.options.display.max_colwidth = 255\ndf = pd.read_csv('../input/airline-sentiment/Tweets.csv')\n#visualizar tweets\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exemplo de comentários. \ncomments = df['text']\ncomments.head(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importar modelo do USE diretamente do tensorflow hub\nmodule_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\nmodel = hub.load(module_url)\nprint (\"module %s loaded\" % module_url)\n\n#função que recebe os tweets pré-processados e retorna os embeddings\ndef embed(input):\n    return model(input)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analise dos dados\nprint('Head: ',df.columns)\n# Dimensões do dataset\nprint(\"\\nShape: \", df.shape)\n# Descrição do dataset\nprint('\\nDescrição:')\nprint(df.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribuição do dataset\nimport seaborn as sns\nsns.countplot(x='airline_sentiment', data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Balançeamento para que os plot dos clusters fique melhor. \ndf['airline_sentiment'].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_neg = df.query(\"airline_sentiment == 'negative'\").head(1000).copy()\ndf_neu = df.query(\"airline_sentiment == 'neutral'\").head(1000).copy()\ndf_pos = df.query(\"airline_sentiment == 'positive'\").head(1000).copy()\n\ndf_balanced = pd.concat([df_neg,df_neu,df_pos], ignore_index=True)\ndf_balanced = df_balanced.sample(frac=1).reset_index(drop=True)\n\ncomments = df_balanced['text'].copy()\n\nsns.countplot(x='airline_sentiment', data=df_balanced)\n\n#Para usar o dataset inteiro, descomentar:\n#df_balanced = df.copy()\ndf = df_balanced.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Pré Processamento"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Para usar a função de pré processamento, observar os parâmetros. \n\n# Parâmetros:\n# data = dataframe\n# html = remove caracteres hmtl (s/n)\n# emoji = remove emojis\n# punct = remove pontuação (s/n)\n# lower =  tranforma para caixa baixa (s/n)\n# stopw = remove stopwords (s/n)\n# lng = linguagem para stopwords (padrão english)\n# token = realiza tokenize (s/n)\n# stm = realiza Stemming (s/n)\n# lmz = realiza Lemmatizing (s/n)\n\n#retorno dataframe limpo\n\ndef limpa_dataframe(data, column, hmtl='s', emoji = 's', punct = 's', lower = 's', stopw = 's', lng = 'english', token = 's', stm = 's', lmz = 's'):\n    \n    import nltk\n    from nltk import word_tokenize\n    from nltk.stem.porter import PorterStemmer        \n    from nltk.stem import WordNetLemmatizer\n    import bs4\n    import string\n    \n    wn = nltk.WordNetLemmatizer()    \n    ps = nltk.PorterStemmer()\n    stopword = nltk.corpus.stopwords.words(lng)\n\n    #Removendo Tag HTML\n    if (hmtl =='s'):\n        data[column] = data[column].apply(lambda x: bs4.BeautifulSoup(x, 'lxml').get_text())\n       \n    # Remove Emojis\n    def deEmojify(inputString):\n        import unicodedata\n        from unidecode import unidecode\n        \n        returnString = \"\"\n\n        for character in inputString:\n            try:\n                character.encode(\"ascii\")\n                returnString += character\n            except UnicodeEncodeError:\n                replaced = unidecode(str(character))\n                if replaced != '':\n                    returnString += replaced\n                else:\n                    try:\n                         returnString += \"[\" + unicodedata.name(character) + \"]\"\n                    except ValueError:\n                         returnString += \"[x]\"\n\n        return returnString\n    if(emoji=='s'):\n        data[column] = data[column].apply(lambda x: deEmojify(x))\n\n    \n    # Removendo a pontuação\n    def remove_punct(text):\n        text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n        return text_nopunct\n    \n    if (punct == 's'):\n        data[column] = data[column].apply(lambda x: remove_punct(x))\n    \n    #Caixa baixa\n    if (lower == 's'):\n        data[column] = [token.lower() for token in data[column]]\n    \n    #Tokenização\n    if (token == 's'):\n        data[column] = [word_tokenize(word) for word in data[column]]\n    \n    #StopWords\n    def remove_stopwords(tokenized_list):\n        text = [word for word in tokenized_list if word not in stopword]\n        return text\n    \n    if(stopw == 's'):\n        data[column] = data[column].apply(lambda x: remove_stopwords(x))\n    \n    #Steeming   \n    def stemming(tokenized_text):\n        text = [ps.stem(word) for word in tokenized_text]\n        return text\n    \n    if (stm == 's'):\n        data[column] = data[column].apply(lambda x: stemming(x))\n\n    \n    #Lemmatizing\n    def lemmatizing(tokenized_text):\n        text = [wn.lemmatize(word) for word in tokenized_text]\n        return text\n    \n    if (lmz == 's'):\n        data[column] = data[column].apply(lambda x: lemmatizing(x))\n    \n    data[column] = [' '.join(word) for word in data[column]]\n\n\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cria um dataset so com os dados limpos. \ndf_clean = df_balanced.copy()\ndf_clean  = limpa_dataframe(df_clean, column = 'text', hmtl='s', emoji = 's', punct = 's', stopw = 's', lng = 'english', token = 's', stm = 's', lmz = 's')\ndf_clean['text'].head(30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.Embbeding"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cria o embbeding\nmessages = df_clean['text']\n\nmessage_embeddings = embed(messages)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converte em duas dimensões para plotar. TNSE\nfrom sklearn.manifold import TSNE\n\ntsne = TSNE(n_components = 2, perplexity = 5)\ntsne_result = tsne.fit_transform(message_embeddings)\ntsne_df = pd.DataFrame ({'X':tsne_result[:,0],\n                         'Y':tsne_result[:,1]})\n\nsns.scatterplot(x = 'X', y = 'Y',\n                data = tsne_df\n               )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Clusterização"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.spatial.distance import pdist\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom scipy.cluster.hierarchy import cophenet\nfrom scipy.cluster.hierarchy import fcluster\n\n#Determina o X\n#X = message_embeddings\n# Só consegui seguir usando um dataset com duas dimensões. Acima disso não foi possível. \nX = tsne_result\n\n#Conforme dito em aula\nZ = linkage(X, 'average', 'cosine')\n\n# Clusteriza\n# Não sei se é isso que foi falado em aula em usar o -1. \nthreshold = 1-0.9\n\nc = fcluster(Z, threshold, criterion=\"distance\")    \n\n# Clusters encontrados. \nmyset = set(c.tolist())\nmy_list = list(set(myset))\nprint('Clusters Encontrados')\nprint(len(my_list))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Resultados"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot do resultado da clusterização. \n\nplt.figure(figsize=(10, 8))\nplt.scatter(X[:,0], X[:,1], c=c, cmap='prism')  \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dendrogram da clusterização\nplt.title('Hierarchical Clustering Dendrogram (truncated)')\nplt.xlabel('sample index')\nplt.ylabel('distance')\ndendrogram(\n    Z,\n    truncate_mode='lastp',  # show only the last p merged clusters\n    p=100,  # show only the last p merged clusters\n    show_leaf_counts=False,  # otherwise numbers in brackets are counts\n    leaf_rotation=90.,\n    leaf_font_size=12.,\n    show_contracted=True,  # to get a distribution impression in truncated branches\n)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# K-Means para referência. Me parece que serve pra essa necessidade. \nfrom sklearn.cluster import KMeans\nclf_k  = KMeans(n_clusters= 3, init='k-means++', max_iter=300,n_init=5, random_state=0)\npred_y_k = clf_k.fit_predict(X)\n\n\nsns.scatterplot(x = 'X', y = 'Y',\n                hue = pred_y_k,\n                palette = ['red','orange','blue'],\n                data = tsne_df\n               )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#EM - GaussianMixture para referência. Não consegui fazer direito. DBSCAN também não consegui fazer funcionar. \n\nfrom sklearn.mixture import GaussianMixture\nclf_em = GaussianMixture(n_components=5, init_params='random', covariance_type='full')\npred_y_em = clf_em.fit_predict(X)\n\n# Clusters encontrados. \nmyset = set(pred_y_em.tolist())\nmy_list = list(set(myset))\nprint(len(my_list))\n\nsns.scatterplot(x = 'X', y = 'Y',\n                hue = pred_y_em,\n                data = tsne_df\n               )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cria um df com os comentarios e seu cluster para comparação. \nexport = pd.DataFrame()\nexport['Comentario'] = df['text']\nexport['cluster'] = c\nprint(len(df))\nsns.countplot(x='cluster', data=export)\n\n# Estou pegando os comentarios originais, com emojos, tags, etc. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Aqui vejo positivos e neutros em maioria. Me parece serem na maioria dúvidas. \npd.options.display.max_colwidth = 255\nprint(export.query(\"cluster == 1\")['Comentario'].head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Aqui não entendi as relaçãoes. Ao meu ver parecem ser em maioria neutras para ruins. Tem a ver com experiencias de voo. \nprint(export.query(\"cluster == 2\")['Comentario'].head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# São geralmente positivos. Parece ter relação com tarifas, preços e e atendimento. \nprint(export.query(\"cluster == 3\")['Comentario'].head(10))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}