{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Face Generation\n\nIn this project, you'll define and train a DCGAN on a dataset of faces. Your goal is to get a generator network to generate *new* images of faces that look as realistic as possible!\n\n","metadata":{}},{"cell_type":"markdown","source":"# **Loading and Preprocessing data**","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\nimport os\nfrom keras import preprocessing\nfrom keras import Sequential\nfrom keras.layers import Conv2D,Dropout,Dense,Flatten,Conv2DTranspose,BatchNormalization,LeakyReLU,Reshape\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2021-07-10T14:23:34.852671Z","iopub.execute_input":"2021-07-10T14:23:34.853042Z","iopub.status.idle":"2021-07-10T14:23:34.858666Z","shell.execute_reply.started":"2021-07-10T14:23:34.853005Z","shell.execute_reply":"2021-07-10T14:23:34.857439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cwd=os.getcwd()\nos.chdir(cwd)\nprint(os.listdir(\"../input\"))","metadata":{"execution":{"iopub.status.busy":"2021-07-10T14:23:34.860529Z","iopub.execute_input":"2021-07-10T14:23:34.861124Z","iopub.status.idle":"2021-07-10T14:23:34.876567Z","shell.execute_reply.started":"2021-07-10T14:23:34.861076Z","shell.execute_reply":"2021-07-10T14:23:34.875415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_celeb=[]\ntrain_path_celeb=\"../input/celeba-dataset/img_align_celeba/img_align_celeba/\"\nfor path in os.listdir(train_path_celeb):\n    if \".jpg\" in path:\n        path_celeb.append(os.path.join(train_path_celeb,path))","metadata":{"execution":{"iopub.status.busy":"2021-07-10T14:23:34.878846Z","iopub.execute_input":"2021-07-10T14:23:34.879346Z","iopub.status.idle":"2021-07-10T14:23:35.272012Z","shell.execute_reply.started":"2021-07-10T14:23:34.879299Z","shell.execute_reply":"2021-07-10T14:23:35.270906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_path=path_celeb[0:50000]\n","metadata":{"execution":{"iopub.status.busy":"2021-07-10T14:23:35.273634Z","iopub.execute_input":"2021-07-10T14:23:35.273916Z","iopub.status.idle":"2021-07-10T14:23:35.281542Z","shell.execute_reply.started":"2021-07-10T14:23:35.273889Z","shell.execute_reply":"2021-07-10T14:23:35.280594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"the dataset have more than 202k images of which only 50k are being selected for the training purpose","metadata":{}},{"cell_type":"code","source":"len(new_path)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T14:23:35.282772Z","iopub.execute_input":"2021-07-10T14:23:35.283034Z","iopub.status.idle":"2021-07-10T14:23:35.295871Z","shell.execute_reply.started":"2021-07-10T14:23:35.283008Z","shell.execute_reply":"2021-07-10T14:23:35.294709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\ncrop=(30,55,150,175)   #croping size for the image so that only the face at centre is obtained\nimages=[np.array((Image.open(path).crop(crop)).resize((64,64))) for path in new_path]\n\nfor i in range(len(images)):\n    images[i]=((images[i]-images[i].min())/(255-images[i].min()))\n    #images[i] = images[i]*2-1  #uncomment this if activation is tanh for generator last layer\n    \nimages=np.array(images)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T14:23:35.297255Z","iopub.execute_input":"2021-07-10T14:23:35.297557Z","iopub.status.idle":"2021-07-10T14:25:25.26584Z","shell.execute_reply.started":"2021-07-10T14:23:35.297527Z","shell.execute_reply":"2021-07-10T14:25:25.264619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data=images\nprint(train_data.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T14:25:25.267993Z","iopub.execute_input":"2021-07-10T14:25:25.268301Z","iopub.status.idle":"2021-07-10T14:25:25.272352Z","shell.execute_reply.started":"2021-07-10T14:25:25.26827Z","shell.execute_reply":"2021-07-10T14:25:25.271691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Visualization**","metadata":{"execution":{"iopub.status.busy":"2021-07-10T14:06:40.180328Z","iopub.execute_input":"2021-07-10T14:06:40.18086Z","iopub.status.idle":"2021-07-10T14:06:40.187491Z","shell.execute_reply.started":"2021-07-10T14:06:40.180814Z","shell.execute_reply":"2021-07-10T14:06:40.186553Z"}}},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nfig,ax=plt.subplots(2,5)\nfig.suptitle(\"Real Images\")\nidx=800\nfor i in range(2):\n    for j in range(5):\n        ax[i,j].imshow(train_data[idx].reshape(64,64,3))\n        idx+=600\n        \nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T14:25:25.273484Z","iopub.execute_input":"2021-07-10T14:25:25.273931Z","iopub.status.idle":"2021-07-10T14:25:26.11683Z","shell.execute_reply.started":"2021-07-10T14:25:25.273891Z","shell.execute_reply":"2021-07-10T14:25:26.115699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Note**  -- the image pixels are normalized dividing each pixel by 255 and then modified to bring within (-1,1) range by multiplying with 2 and substracting 1 since the last layer activation of generator is tanh whose range limits (-1,1)","metadata":{}},{"cell_type":"code","source":"X_train=train_data","metadata":{"execution":{"iopub.status.busy":"2021-07-10T14:25:26.118499Z","iopub.execute_input":"2021-07-10T14:25:26.118877Z","iopub.status.idle":"2021-07-10T14:25:26.248072Z","shell.execute_reply.started":"2021-07-10T14:25:26.118836Z","shell.execute_reply":"2021-07-10T14:25:26.246774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Architecture of the Model**\n\nThe core to the DCGAN architecture uses a standard CNN architecture on the discriminative model. For the generator, convolutions are replaced with upconvolutions, so the representation at each layer of the generator is actually successively larger, as it mapes from a low-dimensional latent vector onto a high-dimensional image.\n\n\n\nUse batch normalization in both the generator and the discriminator.\n\nRemove fully connected hidden layers for deeper architectures.\n\nUse ReLU activation in generator for all layers except for the output, which uses Tanh.\n\nUse LeakyReLU activation in the discriminator for all layers.","metadata":{}},{"cell_type":"markdown","source":"# **Generator**","metadata":{}},{"cell_type":"code","source":"noise_shape=100\n","metadata":{},"execution_count":null,"outputs":[]}]}