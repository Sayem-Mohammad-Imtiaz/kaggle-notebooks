{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"!pip install -U dabl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport dabl\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom mlens.ensemble import SuperLearner\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix , classification_report\nimport optuna\nfrom optuna.samplers import TPESampler\nimport warnings\n\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.dropna()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"markdown","source":"### Heat Map Correlation","metadata":{}},{"cell_type":"code","source":"sns.heatmap(df.corr(), cmap='viridis_r')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distribution Plots","metadata":{}},{"cell_type":"code","source":"sns.set_theme(style=\"darkgrid\")\nsns.displot(df['age'], kde=True)\nplt.xlabel(\"Age (in years)\")\nplt.title(f\"Distribution of Ages\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(df[df['stroke'] == 0][\"age\"], label='No Stroke')\nsns.distplot(df[df['stroke'] == 1][\"age\"], label='Stroke')\nplt.title('No Stroke/Stroke by Age')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(df['avg_glucose_level'], kde=True)\nplt.xlabel(\"Average Glucose Level\")\nplt.title(f\"Distribution of Average Glucose Level\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(df[df['stroke'] == 0][\"avg_glucose_level\"], label='No Stroke')\nsns.distplot(df[df['stroke'] == 1][\"avg_glucose_level\"], label='Stroke')\nplt.title('No Stroke/Stroke by Avg Glucose Level')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(df['bmi'], kde=True)\nplt.xlabel(\"Body Mass Index\")\nplt.title(f\"Distribution of Body Mass Index\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(df[df['stroke'] == 0][\"bmi\"], label='No Stroke')\nsns.distplot(df[df['stroke'] == 1][\"bmi\"], label='Stroke')\nplt.title('No Stroke/Stroke by BMI')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### General Plots","metadata":{}},{"cell_type":"code","source":"dabl.plot(df, target_col='stroke')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"code","source":"x = df.iloc[:, 1:-1].values\ny = df.iloc[:, -1].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Encoding","metadata":{}},{"cell_type":"code","source":"ct = ColumnTransformer(transformers= [('encoder', OneHotEncoder(), [0,5,9])], remainder= 'passthrough')\nx = np.array(ct.fit_transform(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le = LabelEncoder()\nx[:, 15] = le.fit_transform(x[:, 15])\nx[:, 16] = le.fit_transform(x[:, 16])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SMOTE","metadata":{}},{"cell_type":"code","source":"x, y = SMOTE().fit_resample(x,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Split dataset","metadata":{}},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.33, random_state= 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Size x_train: \", x_train.shape)\nprint(\"Size y_train: \", y_train.shape)\nprint(\"Size x_test: \", x_test.shape)\nprint(\"Size y_test: \", y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Scaling","metadata":{}},{"cell_type":"code","source":"sc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Selection","metadata":{}},{"cell_type":"code","source":"class Optimizer:\n    def __init__(self, metric, trials=30):\n        self.metric = metric\n        self.trials = trials\n        self.sampler = TPESampler(seed=42)\n        \n    def objective(self, trial):\n        model = create_model(trial)\n        model.fit(x_train, y_train)\n        preds = model.predict(x_test)\n        if self.metric == 'acc':\n            return accuracy_score(y_test, preds)\n        else:\n            return f1_score(y_test, preds)\n            \n    def optimize(self):\n        study = optuna.create_study(direction=\"maximize\", sampler=self.sampler)\n        study.optimize(self.objective, n_trials=self.trials)\n        return study.best_params","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Random Forest","metadata":{}},{"cell_type":"code","source":"rf = RandomForestClassifier(random_state=42)\nrf.fit(x_train, y_train)\npreds = rf.predict(x_test)\n\nprint(\"Random Forest accuracy: \", accuracy_score(y_test, preds))\nprint(\"Random Forest f1-score: \", f1_score(y_test, preds))\n\ndef create_model(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 2, 6)\n    n_estimators = trial.suggest_int(\"n_estimators\", 2, 150)\n    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n    model = RandomForestClassifier(\n        min_samples_leaf=min_samples_leaf, \n        n_estimators=n_estimators, \n        max_depth=max_depth, \n        random_state=666\n    )\n    return model\n\noptimizer = Optimizer('f1')\nrf_f1_params = optimizer.optimize()\nrf_f1_params['random_state'] = 42\nrf_f1 = RandomForestClassifier(\n    **rf_f1_params\n)\nrf_f1.fit(x_train, y_train)\npreds = rf_f1.predict(x_test)\n\nprint('Optimized on F1 score')\nprint('Optimized Random Forest: ', accuracy_score(y_test, preds))\nprint('Optimized Random Forest f1-score: ', f1_score(y_test, preds))\n\noptimizer = Optimizer('acc')\nrf_acc_params = optimizer.optimize()\nrf_acc_params['random_state'] = 42\nrf_acc = RandomForestClassifier(\n    **rf_acc_params\n)\nrf_acc.fit(x_train, y_train)\npreds = rf_acc.predict(x_test)\n\nprint('Optimized on accuracy')\nprint('Optimized Random Forest: ', accuracy_score(y_test, preds))\nprint('Optimized Random Forest f1-score: ', f1_score(y_test, preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XGBoost","metadata":{}},{"cell_type":"code","source":"xgb = XGBClassifier(\n    random_state=42\n)\nxgb.fit(x_train, y_train)\npreds = xgb.predict(x_test)\n\nprint('XGBoost accuracy: ', accuracy_score(y_test, preds))\nprint('XGBoost f1-score: ', f1_score(y_test, preds))\n\ndef create_model(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 2, 6)\n    n_estimators = trial.suggest_int(\"n_estimators\", 1, 150)\n    learning_rate = trial.suggest_uniform('learning_rate', 0.0000001, 1)\n    gamma = trial.suggest_uniform('gamma', 0.0000001, 1)\n    subsample = trial.suggest_uniform('subsample', 0.0001, 1.0)\n    model = XGBClassifier(\n        learning_rate=learning_rate, \n        n_estimators=n_estimators, \n        max_depth=max_depth, \n        gamma=gamma, \n        subsample=subsample,\n        random_state=666\n    )\n    return model\n\noptimizer = Optimizer('f1')\nxgb_f1_params = optimizer.optimize()\nxgb_f1_params['random_state'] = 42\nxgb_f1 = XGBClassifier(\n    **xgb_f1_params\n)\nxgb_f1.fit(x_train, y_train)\npreds = xgb_f1.predict(x_test)\n\nprint('Optimized on F1 score')\nprint('Optimized XGBoost accuracy: ', accuracy_score(y_test, preds))\nprint('Optimized XGBoost f1-score: ', f1_score(y_test, preds))\n\noptimizer = Optimizer('acc')\nxgb_acc_params = optimizer.optimize()\nxgb_acc_params['random_state'] = 42\nxgb_acc = XGBClassifier(\n    **xgb_acc_params\n)\nxgb_acc.fit(x_train, y_train)\npreds = xgb_acc.predict(x_test)\n\nprint('Optimized on accuracy')\nprint('Optimized XGBoost accuracy: ', accuracy_score(y_test, preds))\nprint('Optimized XGBoost f1-score: ', f1_score(y_test, preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logistic Regression","metadata":{}},{"cell_type":"code","source":"lr = LogisticRegression(\n    random_state=666\n)\nlr.fit(x_train, y_train)\npreds = lr.predict(x_test)\n\nprint('Logistic Regression: ', accuracy_score(y_test, preds))\nprint('Logistic Regression f1-score: ', f1_score(y_test, preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Decision Tree","metadata":{}},{"cell_type":"code","source":"dt = DecisionTreeClassifier(\n    random_state=666\n)\ndt.fit(x_train, y_train)\npreds = dt.predict(x_test)\n\nprint('Decision Tree accuracy: ', accuracy_score(y_test, preds))\nprint('Decision Tree f1-score: ', f1_score(y_test, preds))\n\ndef create_model(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 2, 6)\n    min_samples_split = trial.suggest_int('min_samples_split', 2, 16)\n    min_weight_fraction_leaf = trial.suggest_uniform('min_weight_fraction_leaf', 0.0, 0.5)\n    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n    model = DecisionTreeClassifier(\n        min_samples_split=min_samples_split, \n        min_weight_fraction_leaf=min_weight_fraction_leaf, \n        max_depth=max_depth, \n        min_samples_leaf=min_samples_leaf, \n        random_state=666\n    )\n    return model\n\noptimizer = Optimizer('f1')\ndt_f1_params = optimizer.optimize()\ndt_f1_params['random_state'] = 666\ndt_f1 = DecisionTreeClassifier(\n    **dt_f1_params\n)\ndt_f1.fit(x_train, y_train)\npreds = dt_f1.predict(x_test)\n\nprint('Optimized on F1-score')\nprint('Optimized Decision Tree accuracy: ', accuracy_score(y_test, preds))\nprint('Optimized Decision Tree f1-score: ', f1_score(y_test, preds))\n\noptimizer = Optimizer('acc')\ndt_acc_params = optimizer.optimize()\ndt_acc_params['random_state'] = 666\ndt_acc = DecisionTreeClassifier(\n    **dt_acc_params\n)\ndt_acc.fit(x_train, y_train)\npreds = dt_acc.predict(x_test)\n\nprint('Optimized on accuracy')\nprint('Optimized Decision Tree accuracy: ', accuracy_score(y_test, preds))\nprint('Optimized Decision Tree f1-score: ', f1_score(y_test, preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### K-Nearest Neighbors","metadata":{}},{"cell_type":"code","source":"knn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\npreds = knn.predict(x_test)\n\nprint('KNN accuracy: ', accuracy_score(y_test, preds))\nprint('KNN f1-score: ', f1_score(y_test, preds))\n\nsampler = TPESampler(seed=0)\ndef create_model(trial):\n    n_neighbors = trial.suggest_int(\"n_neighbors\", 2, 25)\n    model = KNeighborsClassifier(n_neighbors=n_neighbors)\n    return model\n\noptimizer = Optimizer('f1')\nknn_f1_params = optimizer.optimize()\nknn_f1 = KNeighborsClassifier(\n    **knn_f1_params\n)\nknn_f1.fit(x_train, y_train)\npreds = knn_f1.predict(x_test)\n\nprint('Optimized on F1-score')\nprint('Optimized KNN accuracy: ', accuracy_score(y_test, preds))\nprint('Optimized KNN f1-score: ', f1_score(y_test, preds))\n\noptimizer = Optimizer('acc')\nknn_acc_params = optimizer.optimize()\nknn_acc = KNeighborsClassifier(\n    **knn_acc_params\n)\nknn_acc.fit(x_train, y_train)\npreds = knn_acc.predict(x_test)\n\nprint('Optimized on accuracy')\nprint('Optimized KNN accuracy: ', accuracy_score(y_test, preds))\nprint('Optimized KNN f1-score: ', f1_score(y_test, preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### AdaBoost","metadata":{}},{"cell_type":"code","source":"abc = AdaBoostClassifier(\n    random_state=666\n)\nabc.fit(x_train, y_train)\npreds = abc.predict(x_test)\n\nprint('AdaBoost accuracy: ', accuracy_score(y_test, preds))\nprint('AdaBoost f1-score: ', f1_score(y_test, preds))\n\ndef create_model(trial):\n    n_estimators = trial.suggest_int(\"n_estimators\", 2, 150)\n    learning_rate = trial.suggest_uniform('learning_rate', 0.0005, 1.0)\n    model = AdaBoostClassifier(\n        n_estimators=n_estimators, \n        learning_rate=learning_rate, \n        random_state=666\n    )\n    return model\n\noptimizer = Optimizer('f1')\nabc_f1_params = optimizer.optimize()\nabc_f1_params['random_state'] = 666\nabc_f1 = AdaBoostClassifier(\n    **abc_f1_params\n)\nabc_f1.fit(x_train, y_train)\npreds = abc_f1.predict(x_test)\n\nprint('Optimized on F1-score')\nprint('Optimized AdaBoost accuracy: ', accuracy_score(y_test, preds))\nprint('Optimized AdaBoost f1-score: ', f1_score(y_test, preds))\n\noptimizer = Optimizer('acc')\nabc_acc_params = optimizer.optimize()\nabc_acc_params['random_state'] = 666\nabc_acc = AdaBoostClassifier(**abc_acc_params)\nabc_acc.fit(x_train, y_train)\npreds = abc_acc.predict(x_test)\n\nprint('Optimized on accuracy')\nprint('Optimized AdaBoost accuracy: ', accuracy_score(y_test, preds))\nprint('Optimized AdaBoost f1-score: ', f1_score(y_test, preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Support Vector Machine","metadata":{}},{"cell_type":"code","source":"svc = SVC(random_state=666)\nsvc.fit(x_train, y_train)\npreds = svc.predict(x_test)\n\nprint(\"SupportVectorClassifier accuracy: \", accuracy_score(y_test, preds))\nprint(\"SupportVectorClassifier f1-score: \", f1_score(y_test, preds))\n\ndef create_model(trial):\n    kernel = trial.suggest_categorical('kernel', ['rbf', 'sigmoid'])\n    degree = trial.suggest_int('degree', 2, 5)\n    gamma = trial.suggest_categorical('gamma', ['auto', 'scale'])\n    model = SVC(\n        kernel=kernel,\n        degree=degree,\n        gamma=gamma,\n        random_state=0\n    )\n    return model\n\noptimizer = Optimizer('f1')\nsvc_f1_params = optimizer.optimize()\nsvc_f1_params['random_state'] = 666\nsvc_f1 = SVC(**svc_f1_params)\nsvc_f1.fit(x_train, y_train)\npreds = svc_f1.predict(x_test)\n\nprint('Optimized on F1-score')\nprint(\"Optimized SupportVectorClassifier accuracy: \", accuracy_score(y_test, preds))\nprint(\"Optimized SupportVectorClassifier f1-score: \", f1_score(y_test, preds))\n\noptimizer = Optimizer('accuracy')\nsvc_acc_params = optimizer.optimize()\nsvc_acc_params['random_state'] = 666\nsvc_acc = SVC(**svc_acc_params)\nsvc_acc.fit(x_train, y_train)\npreds = svc_acc.predict(x_test)\n\nprint('Optimized on accuracy')\nprint(\"Optimized SupportVectorClassifier accuracy: \", accuracy_score(y_test, preds))\nprint(\"Optimized SupportVectorClassifier f1-score: \", f1_score(y_test, preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Super Learner","metadata":{}},{"cell_type":"code","source":"model = SuperLearner(folds=5, random_state=42)\nmodel.add([svc, abc, xgb, rf, dt, knn])\nmodel.add_meta(LogisticRegression())\nmodel.fit(x_train, y_train)\npreds = model.predict(x_test)\nprint('SuperLearner accuracy: ', accuracy_score(y_test, preds))\nprint('SuperLearner f1-score: ', f1_score(y_test, preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ensemble","metadata":{}},{"cell_type":"code","source":"mdict = {\n    'RF': RandomForestClassifier(random_state=666),\n    'XGB': XGBClassifier(random_state=666),\n    'LR': LogisticRegression(random_state=666),\n    'DT': DecisionTreeClassifier(random_state=666),\n    'KNN': KNeighborsClassifier(),\n    'ABC': AdaBoostClassifier(random_state=666),\n    'SVC': SVC(random_state=666),\n    'OARF': RandomForestClassifier(**rf_acc_params),\n    'OFRF': RandomForestClassifier(**rf_f1_params),\n    'OAXGB': XGBClassifier(**xgb_acc_params),\n    'OFXGB': XGBClassifier(**xgb_f1_params),\n    'OADT': DecisionTreeClassifier(**dt_acc_params),\n    'OFDT': DecisionTreeClassifier(**dt_f1_params),\n    'OAKNN': KNeighborsClassifier(**knn_acc_params),\n    'OFKNN': KNeighborsClassifier(**knn_f1_params),\n    'OAABC': AdaBoostClassifier(**abc_acc_params),\n    'OFABC': AdaBoostClassifier(**abc_f1_params),\n    'OASVC': SVC(**svc_acc_params),\n    'OFSVC': SVC(**svc_f1_params)\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model(trial):\n    model_names = list()\n    models_list = [\n        'RF', 'XGB', 'DT', 'LR', 'KNN', 'ABC', 'SVC', 'OARF', 'OFRF', 'OAXGB', 'OFXGB',\n        'OADT', 'OFDT', 'OAKNN', 'OFKNN', 'OAABC', 'OFABC', 'OASVC', 'OFSVC'\n    ]\n    \n    head_list = [\n        'RF', 'XGB', 'DT', \n        'KNN', 'LR', 'ABC', \n        'SVC'\n    ]\n    n_models = trial.suggest_int(\"n_models\", 2, 6)\n    for i in range(n_models):\n        model_item = trial.suggest_categorical('model_{}'.format(i), models_list)\n        if model_item not in model_names:\n            model_names.append(model_item)\n    \n    folds = trial.suggest_int(\"folds\", 2, 6)\n    \n    model = SuperLearner(\n        folds=folds, \n        random_state=666\n    )\n    \n    models = [\n        mdict[item] for item in model_names\n    ]\n    model.add(models)\n    head = trial.suggest_categorical('head', head_list)\n    model.add_meta(\n        mdict[head]\n    )\n        \n    return model\n\ndef objective(trial):\n    model = create_model(trial)\n    model.fit(x_train, y_train)\n    preds = model.predict(x_test)\n    score = accuracy_score(y_test, preds)\n    return score\n\nstudy = optuna.create_study(\n    direction=\"maximize\", \n    sampler=sampler\n)\nstudy.optimize(\n    objective, \n    n_trials=50\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = study.best_params\n\nhead = params['head']\nfolds = params['folds']\ndel params['head'], params['n_models'], params['folds']\nresult = list()\nfor key, value in params.items():\n    if value not in result:\n        result.append(value)\n        \nprint(result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Final Model","metadata":{}},{"cell_type":"code","source":"model = SuperLearner(\n    folds=folds, \n    random_state=666\n)\n\nmodels = [\n    mdict[item] for item in result\n]\nmodel.add(models)\nmodel.add_meta(mdict[head])\n\nmodel.fit(x_train, y_train)\n\npreds = model.predict(x_test)\n\nprint('Optimized SuperLearner accuracy: ', accuracy_score(y_test, preds))\nprint('Optimized SuperLearner f1-score: ', f1_score(y_test, preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(confusion_matrix(y_test, preds))\nprint(classification_report(y_test, preds))","metadata":{},"execution_count":null,"outputs":[]}]}