{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Analyze Eminem Lyrics\n\nHello everyone! In this notebook we will analyze natural language in the dataset which includes columns about Eminem music, especially about his lyrics. In my opinion it is great dataset, because NLP is very interesting Data Science sphere. So, let`s start.\n\n> WARNING! There are Explicit Lyrics!","metadata":{"execution":{"iopub.status.busy":"2021-07-09T15:37:15.963815Z","iopub.execute_input":"2021-07-09T15:37:15.96415Z","iopub.status.idle":"2021-07-09T15:37:15.975583Z","shell.execute_reply.started":"2021-07-09T15:37:15.964076Z","shell.execute_reply":"2021-07-09T15:37:15.974516Z"}}},{"cell_type":"markdown","source":"# 1) Import Libraries and Load Data\n\nFirstly, lets import all useful libraries. Secondly, load data.","metadata":{}},{"cell_type":"code","source":"import os\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport seaborn as sns\n# Library for creating WordCloud\nfrom wordcloud import WordCloud\n\n# Library for working with Text Data\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom string import punctuation\n\npunctuation = set(punctuation)\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.manifold import TSNE","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:12.994622Z","iopub.execute_input":"2021-07-09T16:11:12.995187Z","iopub.status.idle":"2021-07-09T16:11:13.006117Z","shell.execute_reply.started":"2021-07-09T16:11:12.995146Z","shell.execute_reply":"2021-07-09T16:11:13.004894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load Data\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:13.033635Z","iopub.execute_input":"2021-07-09T16:11:13.034014Z","iopub.status.idle":"2021-07-09T16:11:13.040426Z","shell.execute_reply.started":"2021-07-09T16:11:13.033981Z","shell.execute_reply":"2021-07-09T16:11:13.039255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = \"/kaggle/input/eminem-lyrics/Eminem_Lyrics.csv\"\ndata = pd.read_csv(PATH, sep='\\t', comment='#', encoding = \"ISO-8859-1\")","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:13.081734Z","iopub.execute_input":"2021-07-09T16:11:13.082133Z","iopub.status.idle":"2021-07-09T16:11:13.104762Z","shell.execute_reply.started":"2021-07-09T16:11:13.08209Z","shell.execute_reply":"2021-07-09T16:11:13.10379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2) Fast looking on data\n\nLet`s see head of our data frame, list of columns, size and nan/null values in this dataset.","metadata":{}},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:13.142328Z","iopub.execute_input":"2021-07-09T16:11:13.142705Z","iopub.status.idle":"2021-07-09T16:11:13.158913Z","shell.execute_reply.started":"2021-07-09T16:11:13.142664Z","shell.execute_reply":"2021-07-09T16:11:13.157679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:13.213753Z","iopub.execute_input":"2021-07-09T16:11:13.214233Z","iopub.status.idle":"2021-07-09T16:11:13.227988Z","shell.execute_reply.started":"2021-07-09T16:11:13.214204Z","shell.execute_reply":"2021-07-09T16:11:13.22664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"There are {data.shape[0]} rows in dataframe.\")\nprint(f\"And {data.shape[1]} columns.\")\n\nprint(\"\\n\")\n\nprint(f\"Columns: {data.columns}\")\n\nprint(\"\\n\")\n\nprint(f\"Percentage of Null values: \\n {data.isnull().sum() / data.shape[0]}\")\nprint(\"\\n\")\nprint(f\"Percentage of NA values: \\n {data.isna().sum() / data.shape[0]}\")","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:13.270108Z","iopub.execute_input":"2021-07-09T16:11:13.270463Z","iopub.status.idle":"2021-07-09T16:11:13.283629Z","shell.execute_reply.started":"2021-07-09T16:11:13.270427Z","shell.execute_reply":"2021-07-09T16:11:13.282606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"How we can see dataset includes 348 rows and 6 columns. CSV File consists of 6 columns: Album Name, Song Name, Song Lyrics, Album URL, Song Views and Release Date. Unfortunately, there are some missing values in Views and Release Date, but we can see that there are only 5% and even less of them in columns.\n\n# 3) Data preprocessing.\n\nWell, we have examined our dataset, found some problems and are ready to prepare it for our future analysis.\n\nData cleaning plan:\n\n1) Droping useless columns;\n\n2) Working with NA and Null values;\n\n3) Preparing columns like Song Lyrics, Song Views and Release Date.","metadata":{}},{"cell_type":"markdown","source":"# 3.1) Useless columns.\n\nOn second step we saw that there is column about URL. We should drop it, because in our analysis we don`t need in it.","metadata":{}},{"cell_type":"code","source":"main_data = data.drop([\"Album_URL\"], axis = 1)\nmain_data = main_data.drop([\"Unnamed: 6\"], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:13.305582Z","iopub.execute_input":"2021-07-09T16:11:13.306147Z","iopub.status.idle":"2021-07-09T16:11:13.312848Z","shell.execute_reply.started":"2021-07-09T16:11:13.306101Z","shell.execute_reply":"2021-07-09T16:11:13.311845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main_data.head(1)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:13.369711Z","iopub.execute_input":"2021-07-09T16:11:13.370121Z","iopub.status.idle":"2021-07-09T16:11:13.382566Z","shell.execute_reply.started":"2021-07-09T16:11:13.370089Z","shell.execute_reply":"2021-07-09T16:11:13.3816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.2) NA and Null values.\n\nWe have wrote conclusion about dataset and what it includes on step 2. In this conclusion we wrote that there are:\n\n1) Few missing values (only 5% or even less);\n\n2) There aren`t any values in column Views which we can use for filling missing values, because this column about song views and if we fill it with mean or other way result will not be accurate;\n\n3) But we have got better situation with Release_date column. There we can use method where we fill na values with values in next or in last row, because Date is linear value.\n\nIt means that we can fill values in column about Release Date, but because of situation with Views column we wont do it.","metadata":{}},{"cell_type":"code","source":"main_data = main_data.dropna(axis = \"rows\")\n\nmain_data","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:13.417627Z","iopub.execute_input":"2021-07-09T16:11:13.418053Z","iopub.status.idle":"2021-07-09T16:11:13.437741Z","shell.execute_reply.started":"2021-07-09T16:11:13.418017Z","shell.execute_reply":"2021-07-09T16:11:13.436861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.3) Preparing Columns.\n\nHere we should prepare our columns for our future analysis.\n\nFirstly, lets work with Views and Release_date columns.","metadata":{}},{"cell_type":"code","source":"# unique values in Views column\nviews_1 = main_data[\"Views\"].unique()\nviews_1","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:13.469742Z","iopub.execute_input":"2021-07-09T16:11:13.47022Z","iopub.status.idle":"2021-07-09T16:11:13.477309Z","shell.execute_reply.started":"2021-07-09T16:11:13.470192Z","shell.execute_reply":"2021-07-09T16:11:13.47651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see a lot of values with letters K and M (thousands and millions), but we can, also, see strange value 'November 12, 2004' and values with \\n things. We have to work with them.","metadata":{}},{"cell_type":"code","source":"main_data[main_data[\"Views\"] == \"November 12, 2004\"]","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:13.533902Z","iopub.execute_input":"2021-07-09T16:11:13.534416Z","iopub.status.idle":"2021-07-09T16:11:13.547732Z","shell.execute_reply.started":"2021-07-09T16:11:13.534374Z","shell.execute_reply":"2021-07-09T16:11:13.546722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wrong_index = [222]\n\nmain_data = main_data.drop(wrong_index, axis = 0)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:13.585547Z","iopub.execute_input":"2021-07-09T16:11:13.585928Z","iopub.status.idle":"2021-07-09T16:11:13.591429Z","shell.execute_reply.started":"2021-07-09T16:11:13.585895Z","shell.execute_reply":"2021-07-09T16:11:13.590431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function which preprocess Views column\ndef to_number(string):\n  string = list(string)\n  letter_ban = [\"K\", \"M\", \"\\n\"]\n  letter_K = [True for element in string if element == \"K\"]\n  letter_M = [True for element in string if element == \"M\"]\n  string = [element for element in string if element not in letter_ban]\n\n  number = float(\"\".join(string))\n\n  if True in letter_K:\n    number *= 1000\n  elif True in letter_M:\n    number *= 1000000\n\n  return round(number)\n\n# example\nprint(to_number('1.9M\\n'))","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:13.649592Z","iopub.execute_input":"2021-07-09T16:11:13.650973Z","iopub.status.idle":"2021-07-09T16:11:13.659274Z","shell.execute_reply.started":"2021-07-09T16:11:13.650929Z","shell.execute_reply":"2021-07-09T16:11:13.658354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main_data[\"Views\"] = main_data[\"Views\"].apply(lambda num: to_number(num))\n\nmain_data","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:13.697612Z","iopub.execute_input":"2021-07-09T16:11:13.697975Z","iopub.status.idle":"2021-07-09T16:11:13.719505Z","shell.execute_reply.started":"2021-07-09T16:11:13.697946Z","shell.execute_reply":"2021-07-09T16:11:13.718371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Excellent! Lets work with Release_date column.","metadata":{}},{"cell_type":"code","source":"date_1 = main_data[\"Release_date\"].unique()\ndate_1","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:13.753634Z","iopub.execute_input":"2021-07-09T16:11:13.754181Z","iopub.status.idle":"2021-07-09T16:11:13.761153Z","shell.execute_reply.started":"2021-07-09T16:11:13.754148Z","shell.execute_reply":"2021-07-09T16:11:13.760155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is only one incorrect value. Is \" \". We must delete it.","metadata":{}},{"cell_type":"code","source":"main_data[main_data[\"Release_date\"] == \" \"]","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:13.837546Z","iopub.execute_input":"2021-07-09T16:11:13.838052Z","iopub.status.idle":"2021-07-09T16:11:13.851322Z","shell.execute_reply.started":"2021-07-09T16:11:13.838006Z","shell.execute_reply":"2021-07-09T16:11:13.850292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wrong_index = [121]\n\nmain_data = main_data.drop(wrong_index, axis = 0)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:13.901624Z","iopub.execute_input":"2021-07-09T16:11:13.902016Z","iopub.status.idle":"2021-07-09T16:11:13.907936Z","shell.execute_reply.started":"2021-07-09T16:11:13.901982Z","shell.execute_reply":"2021-07-09T16:11:13.906871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function which preprocess Release_date column in Year variant\ndef to_year(date):\n  string = list(date)\n\n  if len(string) == 4:\n    return int(date)\n  elif len(string) == 6:\n    date = 1999\n  else:\n    string = \"\".join(string)\n    date = int(string[-4:])\n  \n  return date\n\n# example\nprint(to_year('July 13, 2006'))","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:13.929617Z","iopub.execute_input":"2021-07-09T16:11:13.929991Z","iopub.status.idle":"2021-07-09T16:11:13.936871Z","shell.execute_reply.started":"2021-07-09T16:11:13.929955Z","shell.execute_reply":"2021-07-09T16:11:13.935874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main_data[\"Release_date\"] = main_data[\"Release_date\"].apply(lambda date: to_year(date))\nmain_data","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:13.969531Z","iopub.execute_input":"2021-07-09T16:11:13.969911Z","iopub.status.idle":"2021-07-09T16:11:13.991132Z","shell.execute_reply.started":"2021-07-09T16:11:13.969877Z","shell.execute_reply":"2021-07-09T16:11:13.989923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great. Now we can go to next stage. Lets prepare lyrics to future analysis.","metadata":{}},{"cell_type":"code","source":"main_data[\"Lyrics\"][2]","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:14.009491Z","iopub.execute_input":"2021-07-09T16:11:14.00992Z","iopub.status.idle":"2021-07-09T16:11:14.016145Z","shell.execute_reply.started":"2021-07-09T16:11:14.00989Z","shell.execute_reply":"2021-07-09T16:11:14.015464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see that there are a many things like \"\\n\" or [Outro], [Verse 1] and etc. We must work with that.\n\nFirstly, filter introduction words. For example, [intro], [Verse 2].","metadata":{}},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\nword_tokenizer = word_tokenize","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:14.049499Z","iopub.execute_input":"2021-07-09T16:11:14.050141Z","iopub.status.idle":"2021-07-09T16:11:14.054132Z","shell.execute_reply.started":"2021-07-09T16:11:14.050108Z","shell.execute_reply":"2021-07-09T16:11:14.053138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function which filter things like [Outro], [Verse 1], [Chorus], etc in texts\ndef intro_words_filter(text):\n  \n  prepared_text = re.sub(r'\\[([^]]*)]', '', text)\n  prepared_text = prepared_text.replace(\"  \", \" \")\n\n  return prepared_text\n\n# example\nintro_words_filter(\"[Verse 1] Before  I check  the mic (Check, check, one, two) [Verse 2]\")","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:14.089852Z","iopub.execute_input":"2021-07-09T16:11:14.090231Z","iopub.status.idle":"2021-07-09T16:11:14.096933Z","shell.execute_reply.started":"2021-07-09T16:11:14.090197Z","shell.execute_reply":"2021-07-09T16:11:14.096231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main_data[\"Prepared_Lyrics\"] = main_data[\"Lyrics\"].apply(lambda text: intro_words_filter(text))\n\nmain_data","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:14.15357Z","iopub.execute_input":"2021-07-09T16:11:14.153933Z","iopub.status.idle":"2021-07-09T16:11:14.1811Z","shell.execute_reply.started":"2021-07-09T16:11:14.153904Z","shell.execute_reply":"2021-07-09T16:11:14.179861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next step is filtering punctuation and stop words.","metadata":{}},{"cell_type":"code","source":"def filtring_punct(text):\n  elements = [element if element not in punctuation else '' for element in text]\n  return ''.join(elements)\n\nstop_words_to_filter = stopwords.words('english')\ndef filter_stop_words(text, stop_words_to_filter):\n  filtered_text = [elem for elem in text if elem not in stop_words_to_filter]\n  return filtered_text","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:14.221844Z","iopub.execute_input":"2021-07-09T16:11:14.222221Z","iopub.status.idle":"2021-07-09T16:11:14.230512Z","shell.execute_reply.started":"2021-07-09T16:11:14.222186Z","shell.execute_reply":"2021-07-09T16:11:14.22958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main_data[\"Prepared_Lyrics\"] = main_data[\"Prepared_Lyrics\"].apply(lambda text: filtring_punct(text))\nmain_data[\"Prepared_Lyrics\"] = main_data[\"Prepared_Lyrics\"].apply(word_tokenizer)\nmain_data[\"Prepared_Lyrics\"] = main_data[\"Prepared_Lyrics\"].apply(lambda text: filter_stop_words(text, stop_words_to_filter))\n\nmain_data","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:14.277587Z","iopub.execute_input":"2021-07-09T16:11:14.277969Z","iopub.status.idle":"2021-07-09T16:11:16.761941Z","shell.execute_reply.started":"2021-07-09T16:11:14.277938Z","shell.execute_reply":"2021-07-09T16:11:16.761248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Third step is texts lemmitizing.","metadata":{}},{"cell_type":"code","source":"def lemmatize_text(text, lemmatizer):\n  return [lemmatizer.lemmatize(element) for element in text]","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:16.763044Z","iopub.execute_input":"2021-07-09T16:11:16.763403Z","iopub.status.idle":"2021-07-09T16:11:16.767293Z","shell.execute_reply.started":"2021-07-09T16:11:16.763377Z","shell.execute_reply":"2021-07-09T16:11:16.766688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main_data[\"Prepared_Lyrics\"] = main_data[\"Prepared_Lyrics\"].apply(lambda text: lemmatize_text(text, lemmatizer))\n\nmain_data","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:16.76888Z","iopub.execute_input":"2021-07-09T16:11:16.769266Z","iopub.status.idle":"2021-07-09T16:11:17.586114Z","shell.execute_reply.started":"2021-07-09T16:11:16.76924Z","shell.execute_reply":"2021-07-09T16:11:17.585465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# also, after lemmitizing we have to filter stop words again for better result\nmain_data[\"Prepared_Lyrics\"] = main_data[\"Prepared_Lyrics\"].apply(lambda text: filter_stop_words(text, stop_words_to_filter))","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:17.587317Z","iopub.execute_input":"2021-07-09T16:11:17.587724Z","iopub.status.idle":"2021-07-09T16:11:18.014812Z","shell.execute_reply.started":"2021-07-09T16:11:17.587695Z","shell.execute_reply":"2021-07-09T16:11:18.013935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Result:","metadata":{}},{"cell_type":"code","source":"main_data","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:18.015818Z","iopub.execute_input":"2021-07-09T16:11:18.016207Z","iopub.status.idle":"2021-07-09T16:11:18.05093Z","shell.execute_reply.started":"2021-07-09T16:11:18.01618Z","shell.execute_reply":"2021-07-09T16:11:18.050233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4) Analyzing.\nHere we will analyze features of this dataset.","metadata":{}},{"cell_type":"markdown","source":"# 4.1) Views, Albums Names and Release Dates.\nReview vis of all numeric values in data:","metadata":{}},{"cell_type":"code","source":"sns.pairplot(main_data)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:18.051888Z","iopub.execute_input":"2021-07-09T16:11:18.052272Z","iopub.status.idle":"2021-07-09T16:11:19.216435Z","shell.execute_reply.started":"2021-07-09T16:11:18.052244Z","shell.execute_reply":"2021-07-09T16:11:19.215752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"songs_views_all = main_data[\"Views\"]\n\n# matplotlib settings\nplt.figure(figsize=(12, 10))\nplt.grid(True)\n\nplt.plot(songs_views_all)\nplt.xlabel(\"Count of Views\")\nplt.ylabel(\"Views\")\nplt.title(\"Views linear plot\")","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:19.217387Z","iopub.execute_input":"2021-07-09T16:11:19.217775Z","iopub.status.idle":"2021-07-09T16:11:19.440631Z","shell.execute_reply.started":"2021-07-09T16:11:19.217747Z","shell.execute_reply":"2021-07-09T16:11:19.439958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nplt.grid(True)\n\nsns.boxplot(data = songs_views_all, linewidth = 2.5, width = 0.5, orient = \"vertical\")\nplt.xlabel(\"Boxplot\")\nplt.ylabel(\"Views\")\nplt.title(\"Views Boxplot\")","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:19.442783Z","iopub.execute_input":"2021-07-09T16:11:19.443073Z","iopub.status.idle":"2021-07-09T16:11:19.603611Z","shell.execute_reply.started":"2021-07-09T16:11:19.443046Z","shell.execute_reply":"2021-07-09T16:11:19.602672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see graphs about all songs and songs views in dataset.","metadata":{}},{"cell_type":"code","source":"# the most popular songs\npopular_songs = main_data.sort_values(by = \"Views\", ascending = False)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:19.605173Z","iopub.execute_input":"2021-07-09T16:11:19.60544Z","iopub.status.idle":"2021-07-09T16:11:19.610242Z","shell.execute_reply.started":"2021-07-09T16:11:19.605414Z","shell.execute_reply":"2021-07-09T16:11:19.609425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"songs_titles = popular_songs[\"Song_Name\"].unique()[:5]\nsongs_views = popular_songs[\"Views\"].unique()[:5]\n\n# matplotlib settings\nfig = plt.figure(figsize = (8, 5))\nax = fig.add_subplot(111)\n\nax.grid(True)\n\nax.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.2e'))\n\nax.bar(songs_titles, songs_views)\nplt.xlabel(\"Songs Names\")\nplt.ylabel(\"Views\")\nplt.title(\"The most popular Eminem Songs\")","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:19.611529Z","iopub.execute_input":"2021-07-09T16:11:19.611934Z","iopub.status.idle":"2021-07-09T16:11:19.807189Z","shell.execute_reply.started":"2021-07-09T16:11:19.611894Z","shell.execute_reply":"2021-07-09T16:11:19.806172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see that the most popular Eminem songs in this dataframe are Rap God, Killshot, Godzilla, Lose Yourself, The Monster.","metadata":{}},{"cell_type":"code","source":"songs_titles = popular_songs[\"Song_Name\"].unique()[:5]\nsongs_date = np.sort(main_data[\"Release_date\"].unique())[::-1]\n\ndate_views = []\nfor year in songs_date:\n  date_views.append(main_data[\"Views\"][(main_data[\"Release_date\"] == year)].sum())\n\n# matplotlib settings\nfig = plt.figure(figsize = (10, 6))\nax = fig.add_subplot(111)\n\nax.grid(True)\n\n#ax.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.2e'))\nax.xaxis.set_major_locator(mtick.MultipleLocator(2))\n\nax.plot(songs_date, date_views, marker = \"o\", linewidth = 3)\nplt.xlabel(\"Songs Release Dates\")\nplt.ylabel(\"Views by year\")\nplt.title(\"The most Listenable songs by Years\")\nstyle = dict(facecolor = \"black\", arrowstyle = \"-\")\nax.annotate(xy = (2013, date_views[4]), xytext = (2008, date_views[4]), s = \"The Marshall Mathers LP2 album\", \n            ha = \"right\", va = \"center\", arrowprops = style)\nax.text(x = 2018.5, y = date_views[2], s = \"Kamikaze\")","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:19.808438Z","iopub.execute_input":"2021-07-09T16:11:19.808781Z","iopub.status.idle":"2021-07-09T16:11:20.147954Z","shell.execute_reply.started":"2021-07-09T16:11:19.808749Z","shell.execute_reply":"2021-07-09T16:11:20.147229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see that the most listenable songs in years description. Rating maximums are 2013 - The Marshall Mathers LP2 album, 2018 - Kamikaze.","metadata":{}},{"cell_type":"code","source":"def split_list(alist, wanted_parts=1):\n    length = len(alist)\n    return [alist[i*length // wanted_parts: (i+1)*length // wanted_parts] for i in range(wanted_parts)]\n\nalbum_titles = main_data[\"Album_Name\"].unique()\nalbum_views = []\n\nfor album in album_titles:\n  album_views.append(main_data[\"Views\"][main_data[\"Album_Name\"] == album].sum())\n\nalbum_titles = split_list(album_titles, 6)\nalbum_titles[0][0] = \"Music To Be Murdered By\"\nalbum_views = split_list(album_views, 6)\n\n# matplotlib settings\nplt.figure(figsize=(32,32))\n\nfor i in range(6):\n  plt.subplot(2, 3,(i%12)+1)\n  plt.title(\"Albums Views\")\n  plt.ylabel(f\"Views {i}\")\n  plt.xlabel(f\"Album Names {i}\")\n  plt.grid(True)\n  plt.bar(album_titles[i], album_views[i])","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:20.149058Z","iopub.execute_input":"2021-07-09T16:11:20.149488Z","iopub.status.idle":"2021-07-09T16:11:21.279463Z","shell.execute_reply.started":"2021-07-09T16:11:20.149439Z","shell.execute_reply":"2021-07-09T16:11:21.278782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In these graphs we can see the most widespread albums. Lets analyze top 5 of them. P.S. We dont take a look on Killshot, Curtain: The Hits, because killshot album includes one song and Curtain: The Hist hit Eminem songs, but not unique songs.","metadata":{}},{"cell_type":"code","source":"TMMLP_album = main_data[main_data[\"Album_Name\"] == \"The Marshall Mathers LP\"][\"Views\"]\nTMMLP2_album = main_data[main_data[\"Album_Name\"] == \"The Marshall Mathers LP2\"][\"Views\"]\nKamikaze_album = main_data[main_data[\"Album_Name\"] == \"Kamikaze\"][\"Views\"]\nsingles_album = main_data[main_data[\"Album_Name\"] == \"The Singles\"][\"Views\"]\nMTMB_album = main_data[main_data[\"Album_Name\"] == \"Music To Be Murdered By: Side B\"][\"Views\"]\n\ntop_albums_df = pd.DataFrame({\"The Marshall Mathers LP\" : TMMLP_album,\n                              \"The Singles\" : singles_album,\n                              \"The Marshall Mathers LP2\" : TMMLP2_album,\n                              \"Kamikaze\" : Kamikaze_album,\n                              \"Music To Be Murdered By: Side B\" : MTMB_album})\n\n# matplotlib settings\nfig = plt.figure(figsize=(10, 6))\nax = fig.add_subplot(111)\nax.grid(True)\n\nax.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.2e'))\n\nsns.boxplot(data = top_albums_df, linewidth = 2.5, width = 0.5, orient = \"horizontal\")\nplt.title(\"Song Views in different Albums\")\nplt.xlabel(\"Views\")\nplt.ylabel(\"Album Names\")","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:21.280566Z","iopub.execute_input":"2021-07-09T16:11:21.281028Z","iopub.status.idle":"2021-07-09T16:11:21.55653Z","shell.execute_reply.started":"2021-07-09T16:11:21.280986Z","shell.execute_reply":"2021-07-09T16:11:21.555602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see that there are views mean in Kamikaze album is bigger than in The Marshall Mathers LP2 and The Marshall Mathers LP albums, but there is outlier as \"Rap God\" song has more than 15000000 views.\n\n# 4.2) Lyrics\nHere we will work with Count Vectorizer. After that we will visualize results.","metadata":{}},{"cell_type":"markdown","source":"# 4.2.1) Preparing Count Vectorizer and data for it.","metadata":{}},{"cell_type":"code","source":"# function for converting lists of texts to strings\ndef to_string(text):\n  words = [element for element in text]\n\n  return ' '.join(words)\n\n# example\nto_string([\"A\", \"B\", \"C\"])","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:21.557994Z","iopub.execute_input":"2021-07-09T16:11:21.558314Z","iopub.status.idle":"2021-07-09T16:11:21.566143Z","shell.execute_reply.started":"2021-07-09T16:11:21.558283Z","shell.execute_reply":"2021-07-09T16:11:21.565057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_modelling = main_data\ndata_modelling[\"Lyrics_Modelling\"] = data_modelling[\"Prepared_Lyrics\"].apply(lambda text: to_string(text))\n\ndata_modelling","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:21.567848Z","iopub.execute_input":"2021-07-09T16:11:21.568295Z","iopub.status.idle":"2021-07-09T16:11:21.622559Z","shell.execute_reply.started":"2021-07-09T16:11:21.568255Z","shell.execute_reply":"2021-07-09T16:11:21.62166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function for creating dictionary with counts of words in texts\ndef word_count(text):\n  main_dict = {}\n\n  for sentence in text:  \n        for word in word_tokenizer(sentence):\n            if word not in main_dict:\n                main_dict[word] = 0 \n            main_dict[word] += 1\n        \n  return {k:v for k,v in sorted(main_dict.items(), key=lambda kv: kv[1], reverse=True)}\n\ntexts = data_modelling[\"Lyrics_Modelling\"]\nword_counts = word_count(texts)\nword_counts","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-09T16:11:21.624Z","iopub.execute_input":"2021-07-09T16:11:21.624341Z","iopub.status.idle":"2021-07-09T16:11:22.951012Z","shell.execute_reply.started":"2021-07-09T16:11:21.62431Z","shell.execute_reply":"2021-07-09T16:11:22.950083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ban_words = [\"Im\", \"I\", \"u\", \"Its\", \"wan\", \"The\", \"got\", \"But\", \"get\", \"And\", \"dont\",\n             \"so\", \"If\", \"My\", \"Me\", \"So\", \"Me\", \"me\", \"em\", \"youre\", \"aint\", \"na\",\n             \"You\", \"you\", \"gon\", \"cant\", \"We\", \"thats\", \"To\", \"This\", \"Ima\", \"Id\",\n             \"Ive\", \"ya\", \"Youre\", \"That\", \"Ta\", \"It\", \"A\", \"\\x91Cause\", \"In\", \"Then\",\n             \"I\\x92m\", \"yall\", \"Or\", \"Why\", \"it\\x92s\", \"Ill\"]\n\nfiltered_count = {k:v for k, v in word_counts.items() if k not in ban_words}\nfiltered_count = {k:v for k, v in filtered_count.items() if k.isalpha()}\n\nmin_frequency = 5\n\nfiltered_count = {k:v for k, v in filtered_count.items() if v > min_frequency}\n\nfiltered_count","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-09T16:11:22.952642Z","iopub.execute_input":"2021-07-09T16:11:22.953064Z","iopub.status.idle":"2021-07-09T16:11:23.020012Z","shell.execute_reply.started":"2021-07-09T16:11:22.953023Z","shell.execute_reply":"2021-07-09T16:11:23.018876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating Count Vectorizer.","metadata":{}},{"cell_type":"code","source":"main_dict = filtered_count.keys()\n\ncount_vectorizer = CountVectorizer(vocabulary = main_dict)\n\ncount_vectorizer.fit(texts)\n\nterm_matrix = count_vectorizer.transform(texts)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:23.021634Z","iopub.execute_input":"2021-07-09T16:11:23.022078Z","iopub.status.idle":"2021-07-09T16:11:23.337842Z","shell.execute_reply.started":"2021-07-09T16:11:23.022034Z","shell.execute_reply":"2021-07-09T16:11:23.336879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating dictionary for visualization","metadata":{}},{"cell_type":"code","source":"terms = count_vectorizer.get_feature_names()\ncount_terms = term_matrix.toarray().sum(axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:23.339336Z","iopub.execute_input":"2021-07-09T16:11:23.339766Z","iopub.status.idle":"2021-07-09T16:11:23.348132Z","shell.execute_reply.started":"2021-07-09T16:11:23.339725Z","shell.execute_reply":"2021-07-09T16:11:23.347077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dictionary = dict(zip(terms, count_terms))\ndictionary\n\ndictionary = pd.Series(dictionary) \ndictionary = dictionary.sort_values(ascending=False) ","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:23.349608Z","iopub.execute_input":"2021-07-09T16:11:23.35011Z","iopub.status.idle":"2021-07-09T16:11:23.363023Z","shell.execute_reply.started":"2021-07-09T16:11:23.350066Z","shell.execute_reply":"2021-07-09T16:11:23.361967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Word Cloud visualization","metadata":{}},{"cell_type":"code","source":"names = dictionary.index\nvalues = dictionary.values\n\n# creating word cloud graph\ndef plot_word_cloud(word_list):\n    \n    wordcloud = WordCloud(background_color=\"white\", max_words=1000, width=900, height=900, collocations=False)\n    wordcloud = wordcloud.generate_from_frequencies(word_list)\n    plt.figure(figsize=(12, 8))\n    plt.title(\"The most widespread words in Eminem Lyrics\")\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.show() \n\n    \nplot_word_cloud(dictionary)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:11:23.364326Z","iopub.execute_input":"2021-07-09T16:11:23.364713Z","iopub.status.idle":"2021-07-09T16:11:29.846412Z","shell.execute_reply.started":"2021-07-09T16:11:23.364675Z","shell.execute_reply":"2021-07-09T16:11:29.845371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5) Conclusion.\nWe`ve analyzed data about Eminem songs lyrics and rating, found out some interesting things and practiced NLP analyzis.\n\nThank you everyone who check this notebook. If you like my notebook upvote it and if you dislike, please, write your comments it will help me to improve my skills. Good luck!","metadata":{}}]}