{"cells":[{"cell_type":"markdown","metadata":{},"source":"**[Feature Engineering Home Page](https://www.kaggle.com/learn/feature-engineering)**\n\n---\n"},{"cell_type":"markdown","metadata":{},"source":"# Introduction\n\nYou can provide more information for your model by creating new features from the data itself. For example, you can calculate the number of total projects in the last week and the duration of the fundraising period. The features you can create are different for every dataset so it takes a bit of creativity and experimentation. We're actually a bit limited here since I'm working with only one table. Typically you'll have access to multiple tables with relevant data that you can use to create new features.\n\nFirst I'll show you how to make new features using categorical features, then a few examples of generated numerical features."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()\nfrom sklearn.preprocessing import LabelEncoder\n\nks = pd.read_csv('../input/kickstarter-projects/ks-projects-201801.csv',\n                 parse_dates=['deadline', 'launched'])\n\n# Drop live projects\nks = ks.query('state != \"live\"')\n\n# Add outcome column, \"successful\" == 1, others are 0\nks = ks.assign(outcome=(ks['state'] == 'successful').astype(int))\n\n# Timestamp features\nks = ks.assign(hour=ks.launched.dt.hour,\n               day=ks.launched.dt.day,\n               month=ks.launched.dt.month,\n               year=ks.launched.dt.year)\n\n# Label encoding\ncat_features = ['category', 'currency', 'country']\nencoder = LabelEncoder()\nencoded = ks[cat_features].apply(encoder.fit_transform)\n\ndata_cols = ['goal', 'hour', 'day', 'month', 'year', 'outcome']\nbaseline_data = ks[data_cols].join(encoded)"},{"cell_type":"markdown","metadata":{},"source":"# Interactions\n\nOne of the easiest ways to create new features is by combining categorical variables. For example, if one record has the country `\"CA\"` and category `\"Music\"`, you can create a new value `\"CA_Music\"`. This is a new categorical feature that can provide information about correlations between categorical variables. This type of feature is typically called an **interaction**. In general, you would build interaction features from all pairs of categorical features. You can make interactions from three or more features as well, but you'll tend to get diminishing returns.\n\nPandas lets us simply add string columns together like normal Python strings."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"interactions = ks['category'] + \"_\" + ks['country']\nprint(interactions.head(10))"},{"cell_type":"markdown","metadata":{},"source":"Then, label encode the interaction feature and add it to our data."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"label_enc = LabelEncoder()\ndata_interaction = baseline_data.assign(category_country=label_enc.fit_transform(interactions))\ndata_interaction.head()"},{"cell_type":"markdown","metadata":{},"source":"In the next exercise, you'll build interaction terms for all pairs of categorical features."},{"cell_type":"markdown","metadata":{},"source":"# Number of projects in the last week\n\nFirst up I'll show you how to count the number of projects launched in the preceeding week for each record. To do this I'll use the `.rolling` method on a series with the `\"launched\"` column as the index. I'll create the series, using `ks.launched` as the index and `ks.index` as the values, then sort the times. Using a time series as the index allows us to define the rolling window size in terms of hours, days, weeks, etc."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# First, create a Series with a timestamp index\nlaunched = pd.Series(ks.index, index=ks.launched, name=\"count_7_days\").sort_index()\nlaunched.head(20)"},{"cell_type":"markdown","metadata":{},"source":"There are seven projects that have obviously wrong launch dates, but we'll just ignore them. Again, this is something you'd handle when cleaning the data, but it's not the focus of this mini-course.\n\nWith a timeseries index, you can use `.rolling` to select time periods as the window. For example `launched.rolling('7d')` creates a rolling window that contains all the data in the previous 7 days. The window contains the current record, so if we want to count all the *previous* projects but not the current one, we'll need to subtract 1. I'll also plot the results so we can make sure it looks right."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"count_7_days = launched.rolling('7d').count() - 1\nprint(count_7_days.head(20))\n\n# Ignore records with broken launch dates\nplt.plot(count_7_days[7:]);\nplt.title(\"Competitions in the last 7 days\");"},{"cell_type":"markdown","metadata":{},"source":"Now that we have the counts, we need to adjust the index so we can join it with the other training data. "},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"count_7_days.index = launched.values\ncount_7_days = count_7_days.reindex(ks.index)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"count_7_days.head(10)"},{"cell_type":"markdown","metadata":{},"source":"Now join the new feature with the other data again using `.join` since we've matched the index."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"baseline_data.join(count_7_days).head(10)"},{"cell_type":"markdown","metadata":{},"source":"# Time since the last project in the same category\n\nIt's possible that projects in the same category compete for donors. If you're trying to fund a video game and another game project was just launched, you might not get as much money. What I'd like to do then is calculate the time since the last project in the same category.\n\nA handy method for performing operations within groups is to use `.groupby` then `.transform`. The `.transform` method takes a function then passes a series or dataframe to that function for each group. This will a return a dataframe with the same indices as the original dataframe. What we can do is perform a groupby on `\"category\"` and use transform to calculate the time differences for each category."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"def time_since_last_project(series):\n    # Return the time in hours\n    return series.diff().dt.total_seconds() / 3600.\n\ndf = ks[['category', 'launched']].sort_values('launched')\ntimedeltas = df.groupby('category').transform(time_since_last_project)\ntimedeltas.head(20)"},{"cell_type":"markdown","metadata":{},"source":"We get `NaN`s here for projects that are the first in their category. We'll need to fill those in with something like the mean or median. We'll also need to reset the index so we can join it with the other data."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Final time since last project\ntimedeltas = timedeltas.fillna(timedeltas.median()).reindex(baseline_data.index)\ntimedeltas.head(20)"},{"cell_type":"markdown","metadata":{},"source":"# Transforming numerical features"},{"cell_type":"markdown","metadata":{},"source":"If we look at the distribution of the values in `\"goal\"` we see most projects have goals less than \n5000 USD. However, there is a long tail of goals going up to $100,000. Some models work better when the features are normally distributed, so it might help to transform the goal values. Common choices for this are the square root and natural logarithm. These transformations can also help constrain outliers.\n\nHere I'll transform the goal feature using the square root and log functions, then fit a model to see if it helps"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"plt.hist(ks.goal, range=(0, 100000), bins=50);\nplt.title('Goal');"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"plt.hist(np.sqrt(ks.goal), range=(0, 400), bins=50);\nplt.title('Sqrt(Goal)');"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"plt.hist(np.log(ks.goal), range=(0, 25), bins=50);\nplt.title('Log(Goal)');"},{"cell_type":"markdown","metadata":{},"source":"The log transformation won't help our model since tree-based models are scale invariant. However, this should help if we had a linear model or neural network.\n\nOther transformations include squares and other powers, exponentials, etc. These might help the model discriminate, like the kernel trick for SVMs. Again, it takes a bit of experimentation to see what works. One method is to create a bunch of new features and later choose the best ones with feature selection algorithms.\n\nNext up, you'll get practice generating features with the TalkingData ad data."},{"cell_type":"markdown","metadata":{},"source":"---\n**[Feature Engineering Home Page](https://www.kaggle.com/learn/feature-engineering)**\n\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum) to chat with other Learners.*"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":4}