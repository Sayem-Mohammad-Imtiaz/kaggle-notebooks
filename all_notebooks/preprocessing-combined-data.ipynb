{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Covid-19 Analytics"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import Libraries \nimport numpy as np\nimport json \nimport pandas as pd\nimport glob \nimport os","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>\nThe aim of this notebook is to bring together the provided dataset into a single dataframe. \nThe text are recovered from the relevant folders. The section title are included in the dataframe text column. \nThe conclusion and title were  assigned  separate columns in the new dataframe. The preprocessed data were augmented with the information contained in the metadata. \n</p>\n<p>\n\n    The final data output has been uploaded: so you don't need to repeat this process. Check the bottom of this notebook to have a feel of the merged data.\nFeel free to use.\n</p>"},{"metadata":{},"cell_type":"markdown","source":"### Directory set up"},{"metadata":{},"cell_type":"markdown","source":"<p>\nI had two folders called 'computations' and 'dataset' in a directory named 'covid'. The dataset\nwas downloaded and extracted in the folder 'dataset'. This notebook was placed in\n'computations' folder. Below I try to set up the folder path to organize my work.\n    \n</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"# set the base directory.\nBASE_DIR = os.path.join( os.path.dirname(path), '' )\n# data set folder \nDATASET_DIR  =  BASE_DIR + 'dataset/CORD-19-research-challenge'\n# data directories list\nDATA_PATH_LIST = [DATASET_DIR +str(\"/\")+ i for i in os.listdir(DATASET_DIR) ]\n# get the name of the important folder from the folder names\nDATA_FOCUS = [i.split('/')[-1] for i in DATA_PATH_LIST ] \n# deletee the unimportant subset\ndel DATA_FOCUS[2:5], DATA_FOCUS[3]\nDATA_FOCUS ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>\nIn the cell below, I try to extract the full path to the json files in the folders named above\n</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"# collect data set\nfolder_contents = {}\nfor i in DATA_PATH_LIST: #  DATA_PATH_LIST contains all the folder  paths containing the dataset\n    temp = i.split('/')[-1] # get the last elemnt of the path name\n    if temp in  DATA_FOCUS: # DATA FOCUS has the names of the folders one is interested in\n        filename = i+'/'+temp+'/pdf_json'  # json files reside in the subfolder pdf_json\n        jsonfiles = glob.glob(os.path.join(filename, \"*.json\")) # collect all json file path names in the folder\n        if temp != 'biorxiv_medrxiv': # exclude biorxiv_medrxiv ; it has no pmc_json subfolder\n            filename2 = i+'/'+temp+'/pmc_json' # some dataset are in the folder pmc_json\n            jsonfiles2 = glob.glob(os.path.join(filename2, \"*.json\"))# collect json file path in the folder\n            folder_contents[temp+'_pmc']  = jsonfiles # save in dictionary\n        folder_contents[temp]  = jsonfiles   # save the file path names in dictionary\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>\nIn the cell below, the keys to the dictionary are listed. The values of the keys correspond to the full path names of all\njson files contained in that folder</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"# keys in the dictionary\nfolder_contents.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fulldata=[] # list for storing dataset\n\nfor i in  folder_contents.keys(): # for each key(=folder name) in the dictionary folder_contents\n    print(i) # print the source folder name /key\n   \n    #if i == 'biorxiv_medrxiv':\n    for j in folder_contents[i]: # for each json file path in the value (folder name)\n        paper_id = ''; # to store paper id\n        temp = ''; # string to store text\n        with open(j) as json_data: # open json file. j==json file path\n            data = json.load(json_data)  # load the data\n            paper_id = data['paper_id']  # get paper id\n            temp+= ' ' + data['metadata']['title']  # get the title\n            temp += ' :'\n            conclusions = ''\n            for text in data['body_text']:  # the 'element' body_text contains the text information of interest\n                if text['section'] !=  'CONCLUSIONS': # check that  'element' section in the text is not conclusion\n                    temp += 'section: ' + text['section'] + ':' # get the section name\n                    temp += text['text'] # get the correspnsing section text\n                elif text['section'] ==  'CONCLUSIONS': # if the text is a conclusion\n                    conclusions +=  text['text'] # add to the connclusion string\n                else:\n                    temp += text['text'] # else add text\n                \n            datadict = {'paper_id': paper_id  , 'text':temp , 'conclusions':conclusions,  'source':i} # collect data\n            #title_doc[paper_id] = data['metadata']['title']  \n        \n            fulldata.append(datadict) # add to list\n            \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>\nThe 'fulldata' wich is a list of dictionaries would be made into a dataframe in the cell below.\n</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"# make data into data frame\ncovid = pd.DataFrame(fulldata)\ncovid.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# describe the columns with non empty conclusions\n# 1189 articles with conclusions\ncovid.loc[covid['conclusions'] != '' ]['conclusions'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# save the dataset\ncovid.to_parquet('cleaned_covid.parquet')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Meta data load"},{"metadata":{},"cell_type":"markdown","source":"<p>\nIn this section the metadata is loaded. Metadata would later be combined  with the covid dataframe above.\n</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"# load metadata\nmeta_data = pd.read_csv(DATASET_DIR+'/metadata.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# check metadata\nmeta_data.head()[1:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# check number of free Nans in sha\nlen(meta_data) - meta_data['sha'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# change the sha id to paper id\n# useful for joining the dataset\nmeta_data.rename(columns={'sha':'paper_id'}, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Merge Data"},{"metadata":{},"cell_type":"markdown","source":"<p>\nHere the metadata os merged with thw covid data. This becomes the full data frame that will be stored.\n</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"# merge covid data with meta data\nmerged_data = covid.merge(meta_data, on='paper_id', how='inner', suffixes=('_1', '_2'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# save as feather file\nmerged_data.to_parquet('merged_data.parquet.gzip',compression='gzip')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Final Merged Data **"},{"metadata":{"trusted":true},"cell_type":"code","source":"ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load data\nmerged_data = pd.read_parquet(\"merged_data.parquet.gzip\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shape of the final data \nmerged_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# columns in the data \nmerged_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# describe the datatset\nmerged_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# articles with Non emoty  conclusions\nmerged_data[merged_data['conclusions']!='']['conclusions']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# articles with null title\nmerged_data[merged_data['title'].isnull()]['title']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_data['abstract'][7]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion\n\n<p>\nEnjoy and stay safe .\n\n</p>"},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}