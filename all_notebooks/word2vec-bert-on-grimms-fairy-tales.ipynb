{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom nltk.tokenize import sent_tokenize, word_tokenize \nimport gensim \nfrom gensim.models import Word2Vec \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Word2Vec\nsource: https://www.geeksforgeeks.org/python-word-embedding-using-word2vec/"},{"metadata":{},"cell_type":"markdown","source":"Word2vec can utilize either of two model architectures to produce a distributed representation of words: continuous bag-of-words (CBOW) or continuous skip-gram. In the continuous bag-of-words architecture, the model predicts the current word from a window of surrounding context words. The order of context words does not influence prediction (bag-of-words assumption). In the continuous skip-gram architecture, the model uses the current word to predict the surrounding window of context words. The skip-gram architecture weighs nearby context words more heavily than more distant context words.[1][5] According to the authors' note,[6] CBOW is faster while skip-gram is slower but does a better job for infrequent words. <br>\nsource: https://en.wikipedia.org/wiki/Word2vec#CBOW_and_skip_grams"},{"metadata":{},"cell_type":"markdown","source":"## 1.1 Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Reads ‘alice.txt’ file \nimport pandas as pd\ndf = pd.read_csv(\"/kaggle/input/grimms-fairy-tales/grimms_fairytales.csv\") \nf = df['Text'][1]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.1.1 Tokenization"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenized_text = []\n# iterate through each sentence in the file \nfor sentence in sent_tokenize(f): \n    sentence_tokenized = [] \n      \n    # tokenize the sentence into words \n    for word in word_tokenize(sentence): \n        sentence_tokenized.append(word.lower()) \n  \n    tokenized_text.append(sentence_tokenized) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.1.2 Parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"strings = ['hans','luck']\nmodels_quantity = 4\nsizes = []\nwindows = []\nfor i in range(1,models_quantity+1):\n    sizes.append(i*100)\n    windows.append(i*5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.2 CBOW"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ncbow_fixed_size_results = []\nfor i in range(0,models_quantity):\n    cbow_model = gensim.models.Word2Vec(tokenized_text, min_count = 1,  size = sizes[0], window = windows[i])\n    cbow_fixed_size_results.append(cbow_model.wv.similarity(strings[0], strings[1]))\n\nplt.plot(windows, cbow_fixed_size_results)\nplt.ylabel('cosine similarity')\nplt.xlabel('windows')\nplt.show()\n\ncbow_fixed_window_results = []\nfor i in range(0,models_quantity):\n    cbow_model = gensim.models.Word2Vec(tokenized_text, min_count = 1,  size = sizes[i], window = windows[0])\n    cbow_fixed_window_results.append(cbow_model.wv.similarity(strings[0], strings[1]))\n\nplt.plot(sizes, cbow_fixed_window_results)\nplt.ylabel('cosine similarity')\nplt.xlabel('sizes')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.3 Skip Gram"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Skip Gram model \nskip_gram_fixed_size_results = []\nfor i in range(0,models_quantity):\n    skip_gram_model = gensim.models.Word2Vec(tokenized_text, min_count = 1,  size = sizes[0], window = windows[i], sg = 1)\n    skip_gram_fixed_size_results.append(skip_gram_model.wv.similarity(strings[0], strings[1]))\n\nplt.plot(windows, skip_gram_fixed_size_results)\nplt.ylabel('cosine similarity')\nplt.xlabel('windows')\nplt.show()\n\nskip_gram_fixed_window_results = []\nfor i in range(0,models_quantity):\n    skip_gram_model = gensim.models.Word2Vec(tokenized_text, min_count = 1,  size = sizes[i], window = windows[0], sg = 1)\n    skip_gram_fixed_window_results.append(skip_gram_model.wv.similarity(strings[0], strings[1]))\n\nplt.plot(sizes, skip_gram_fixed_window_results)\nplt.ylabel('cosine similarity')\nplt.xlabel('sizes')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. BERT"},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Named Entity Recognition\nsource: https://huggingface.co/transformers/usage.html#named-entity-recognition "},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AutoModelForTokenClassification, AutoTokenizer\nimport torch\n\nmodel = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\nlabel_list = [\n    \"O\",       # Outside of a named entity\n    \"B-MISC\",  # Beginning of a miscellaneous entity right after another miscellaneous entity\n    \"I-MISC\",  # Miscellaneous entity\n    \"B-PER\",   # Beginning of a person's name right after another person's name\n    \"I-PER\",   # Person's name\n    \"B-ORG\",   # Beginning of an organisation right after another organisation\n    \"I-ORG\",   # Organisation\n    \"B-LOC\",   # Beginning of a location right after another location\n    \"I-LOC\"    # Location\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_index = 29\nsequence = df['Text'][text_index].replace('\\n',' ').replace('‘','').replace(\"’\",\"\").translate(str.maketrans('', '', string.punctuation))\n#sequence = \"On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\"\nsequence = (sequence[:2000] + ' ..') if len(sequence) > 2000 else sequence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bit of a hack to get the tokens with the special tokens\ntokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))\ninputs = tokenizer.encode(sequence, return_tensors=\"pt\")\n\noutputs = model(inputs)[0]\npredictions = torch.argmax(outputs, dim=2)\nprint(df['Title'][text_index])\nfor token, prediction in zip(tokens, predictions[0].numpy()):\n    if 'O' not in label_list[prediction]:\n        print(token, label_list[prediction])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}