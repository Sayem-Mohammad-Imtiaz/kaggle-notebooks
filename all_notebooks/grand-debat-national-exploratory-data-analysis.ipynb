{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Grand Débat National - Exploratory Data Analysis\n*** \n**Jérémy Lesuffleur - 06/04/2019**\n\n## Context\nFollowing the [Yellow vests movement](https://en.wikipedia.org/wiki/Yellow_vests_movement), the President of the French Republic launched the **Grand Débat National**, a natio-wide public debate. Every French citizens were invited to express their views and propositions on four main themes: *ecological transition*, *taxation*, *organisation of the State*, *democracy and citizenship*.\n\nThis national debate produced a huge amount of data, mainly textual data, provided [in Open Data here](https://www.data.gouv.fr/fr/datasets/donnees-ouvertes-du-grand-debat-national). In this kernel, we will load and **discover the dataset**, see what it contains, do an **exploratory analysis** and run some **natural language processing** algorithms. This notebook is not pretending to undertake a deep analysis of the dataset but rather **an overview of its content**."},{"metadata":{},"cell_type":"markdown","source":"## Table of contents\n\n* [Importing the data](#importing)\n\n* [Discovering the dataset](#discovering)\n\n* [Who are the contributors?](#who)\n   \n* [Shaping the questions](#shaping)\n\n* [Closed questions analysis](#closed)\n\n* [Open questions analysis](#open)\n\n* [Conclusion](#conclusion)"},{"metadata":{},"cell_type":"markdown","source":"## Environment\n\nLoading environment libraries and importing modules."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# basics\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport os\n\n# string\nimport string\nimport unidecode\nimport re\nfrom textwrap import wrap # wrapping long text into lines\n\n# plot\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport geopandas as gpd\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n%matplotlib inline\n\n# text mining\nimport nltk\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom stop_words import get_stop_words\n\n# Because we have some long strings to deal with:\npd.options.display.max_colwidth = 300","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing the data\n<a id=\"importing\"></a>\n***"},{"metadata":{"_kg_hide-input":false},"cell_type":"markdown","source":"Let's see the files available in the dataset:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"os.listdir('../input/granddebat')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is one file for each of the **4 main themes** of the debate. The file *EVENTS.csv* is different, it contains information about public events held regarding the debate, it will not be used here.  \n\nWe match each **file** with its corresponding **theme**."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"themes = {\n    'LA_FISCALITE_ET_LES_DEPENSES_PUBLIQUES.csv':'La fiscalité et les dépenses publiques',\n    'ORGANISATION_DE_LETAT_ET_DES_SERVICES_PUBLICS.csv':\"Organisation de l'état et des services publics\",\n    'DEMOCRATIE_ET_CITOYENNETE.csv':'Démocratie et citoyenneté',\n    'LA_TRANSITION_ECOLOGIQUE.csv':'La transition écologique'\n}\n\nfilenames = list(themes.keys())\nthemes = list(themes.values())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now import each file, all in one **list of dataframes** for easier use.  \n\nWe pay special attention to data types: *ZipCode* must be read as *strings* and date columns as *timestamps*."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"filepaths = [os.path.join(\"..\", \"input\", \"granddebat\", filename) for filename in filenames]\ncol_date = ['createdAt', 'publishedAt', 'updatedAt']\ndf_list = [pd.read_csv(filepath, low_memory=False,\n                       dtype={'authorZipCode':'str'},\n                       parse_dates=col_date) for filepath in filepaths]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Discovering the dataset\n<a id=\"discovering\"></a>\n***"},{"metadata":{},"cell_type":"markdown","source":"## Available variables\nThe 4 dataframes share some **common variables**, other columns are **questions** that are specific to the theme. The common variables are the following:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"col_common = set.intersection(*[set(df.columns) for df in df_list])\ncol_common","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that we have some information about the **author**: a unique *Id*, a *Type* (we will dig into it later) and a *ZipCode*. Not that much: we don't have any information about the age or the gender of the author for instance.\n\nWe can wonder if those variables contain some **missing values**:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"pd.concat([df[df.columns.intersection(col_common)] for df in df_list]).isnull().mean() * 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset is rather clean. The variables `updatedAt` and `trashedStatus` are poorly filled because most contributions are neither updated nor trashed."},{"metadata":{},"cell_type":"markdown","source":"## Dataset size\n\nEach line of the dataframes corresponds to one **contribution**: the answers of an author to the questions of the corresponding theme. Let's see how many contributions we have for each dataset, and how many questions:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_infos = pd.DataFrame({\n     'theme': themes,\n     'nb_contributions': [df.shape[0] for df in df_list],\n     'nb_questions': [sum(~df.columns.isin(col_common)) for df in df_list]\n    })\ndf_infos","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset is huge: **over 500 thousand contributions**. There are a lot of analysis to be done!\nThe survey about *taxation* was the most answered, but this may be due to it being the shortest: 8 questions only."},{"metadata":{},"cell_type":"markdown","source":"## When were the contributions submitted?"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We will have a look at the `createdAt` variable to spot **when** the contributions were submitted, and at what time of the day."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Daily contributions\nday_contrib = pd.concat([df.createdAt for df in df_list]).dt.date.value_counts().sort_index()\n\nfig, ax = plt.subplots(figsize = (18,6))\nday_contrib.plot()\nax.set_title('Daily contributions')\nax.set_xlabel('Date')\nfig.autofmt_xdate()\nax.set_ylim(bottom=0)\nplt.show(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see a first peak at the very beginning of the *Grand Débat* (the website was opened for contributions on tuesday 2019-01-22), and then another peak on sunday 2019-03-10. On those particular days, the submissions **exceeded 25,000 contributions by day**.\n\nLet's look also at the time the contributions were made:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Hourly contributions\nhour_contrib = pd.concat([df.createdAt for df in df_list]).dt.hour.value_counts().sort_index()\n\nfig, ax = plt.subplots(figsize = (18,6))\nhour_contrib.plot()\nax.set_title('Hourly contributions')\nax.set_xlabel('Hour')\nax.set_ylim(bottom=0)\nplt.show(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The number of contribution per hour reaches a peak in the late afternoon, between 18h and 19h (we don't know whether it is **UTC** or **CET** since the raw data are characters without TZ indication). "},{"metadata":{},"cell_type":"markdown","source":"# Who are the contributors?\n<a id=\"who\"></a>\n***"},{"metadata":{},"cell_type":"markdown","source":"In this section we will have a closer look at the **authors** of the *Grand Débat*. For each contribution we have an `authorID` that is shared among datasets. \n\nEveryone could submit any number of contribution for each theme. An author wrote **252 contributions** about the *organisation of the State*!"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Maximal number of contributions per author per theme:\npd.DataFrame({'theme':themes,\n              'max_contrib_per_author':[df.groupby('authorId').size().max() for df in df_list]})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we focus on contributors, we aggregate the table by `authorId` in order to have one line per author. If an author has several `authorType` or `authorZipCode` (that should be rare), we keep the most frequent one: the *mode*. \n\nWe also add a count statistics: how many contributions that author made over the whole dataset."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def mode_na(x): \n    m = pd.Series.mode(x)\n    return m.values[0] if not m.empty else np.nan\n\nauthors = pd.concat([df[df.columns.intersection(col_common)] for df in df_list])\n# With pandas>=0.24, we would use: pandas.Series.mode\nauthors = authors.groupby('authorId').agg({'id':'count', # number of contributions\n                                           'authorType':mode_na,\n                                           'authorZipCode':mode_na})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first statistics we can get out of this new dataframe is the **number of distinct contributors**:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"authors.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are more than **250,000 distinct contributors**. As a city population, it would be in the [top 10 of France most populated cities](https://fr.wikipedia.org/wiki/Liste_des_communes_de_France_les_plus_peupl%C3%A9es)!"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"n_contrib = authors.id.value_counts().reset_index(name='counts')\nn_contrib.loc[n_contrib['index'] > 4, 'index'] = '>4'\nn_contrib = n_contrib.groupby('index').agg(sum)\nfig, ax = plt.subplots(figsize=(18,6))\nax = sns.barplot(x='index',\n            y='counts',\n            data=n_contrib.reset_index(),\n            palette=sns.color_palette('Blues'))\nax.set_xlabel('Number of contributions')\nax.set_title('Authors per number of contributions')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As can be seen, around 50% of the authors submitted a single contribution. \n\nOnly **50,000** authors have at least 4 contributions and hence may have answered the 4 themes."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(18,6))\nax = sns.countplot(x='authorType',\n                   data=authors,\n                   palette=sns.color_palette('Blues'))\nax.set_yscale('log')\nax.set_title('Author types')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We notice that the great majority of respondents, 84% actually, are **citizens** *(mind the log scale!)*, i.e. they are neither politicals, officials nor part of an organisation."},{"metadata":{},"cell_type":"markdown","source":"## Contributors map\n<a id=\"map\"></a>\n\n### Retrieving zipcodes"},{"metadata":{},"cell_type":"markdown","source":"In this section we will plot a map of contributors in France. For this purpose we will use the `authorZipCode` variable. First thing to check is the quality of this column: does it contains missing values?"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"authors.authorZipCode.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Good news, only 3 missing values! But are the other correct however?  \n\nIn France, the zipcode must contains **exactly 5 digits**. Let's check:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"authors.authorZipCode.str.len().value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can notice, the zipcode was not forced to be 5 digit. Some are more, some are less.\n\nSince we cannot use those incorrect zipcodes, we set them to **nan**. There are around 3000, it is not much regarding the number of contributors."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"authors.loc[authors.authorZipCode.str.len() != 5, 'authorZipCode'] = np.nan","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"But keep in mind it doesn't mean that the remaining zipcodes are necessarily correct! The remaining incorrect zipcodes will be lost as well. Here is an example:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"authors.authorZipCode[49789]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Link it with open data"},{"metadata":{},"cell_type":"markdown","source":"To have an idea of how much each part of France contribute to the *Grand Débat*, we must compute the **author rate**: number of contributors per inhabitant.  \n\nWe can compute the number of contributors per zipcode. The local open data in France (population, geo boundaries) is usually at the **commune** scale, but the relation between the commune and the zipcode is **many to many**! We have to aggregate to the **department** scale to have a **one to many** link with both the commune and the zipcode.\n\nWe will use two datasets:  \n\n1. **communes-francaises/code-postal-code-insee-2015.csv**: a dataset containing the communes population, and link between commune, zipcode and department [(source)](https://data.opendatasoft.com/explore/dataset/code-postal-code-insee-2015%40public/table/).\n2. **contours-des-departements-francais/departements-20180101.shp**: the boundaries of French departments [(source)](https://www.data.gouv.fr/fr/datasets/contours-des-departements-francais-issus-d-openstreetmap).\n\nLet's use the first dataset to compute the author rate per department:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# French variant of csv:\ncommunes_fr = pd.read_table('../input/communes-francaises/code-postal-code-insee-2015.csv',\n                            encoding = 'utf-8', delimiter=\";\", dtype ={'Code_postal':'str'})\n\n# Population per department (population is at INSEE_COM scale, wich is the commune)\npopulation_dep = communes_fr[['INSEE_COM', 'CODE_DEPT', 'POPULATION']].drop_duplicates().\\\ngroupby('CODE_DEPT').sum()\n\n# Link zipcode/department\nzc_dep = communes_fr[['Code_postal', 'CODE_DEPT']].drop_duplicates()\n\n# Authors per zipcode\nauthors_zc = authors.assign(code_postal=authors.authorZipCode.str.slice(0, 5)).\\\ngroupby('code_postal').size().reset_index(name='counts')\n\n# Authors per department\nauthors_dep = zc_dep.set_index('Code_postal').join(authors_zc.set_index('code_postal')).\\\ngroupby('CODE_DEPT').sum()\n# Adding population\nrate_dep = authors_dep.join(population_dep)\n# Computing rate\nrate_dep['author_rate'] = rate_dep.counts/rate_dep.POPULATION * 1000 # per 1000","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To ensure the consistency of our data, we can check if we have, as expected, a French population of 65 millions, and 250 thousand contributors:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(rate_dep.POPULATION.sum())\nprint(rate_dep.counts.sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we use the second dataset to combine those data with the department boundaries."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"map_dep = gpd.read_file('../input/contours-des-departements-francais/departements-20180101.shp',\n                        encoding = 'utf-8')\nmap_dep = map_dep[['code_insee', 'geometry']]\n\n# Mainland only\nmap_dep = map_dep[~map_dep.code_insee.isin(['971', '972','973','974','976'])]\n\n# In this dataset, department 69 (Lyon) is split in two: 69D and 69M. We merge them.\nmap_dep.loc[map_dep.code_insee.str.contains('69'), 'code_insee'] = '69'\nmap_dep = map_dep.dissolve(by='code_insee')\n\n# Add variable\nmap_dep = map_dep.join(rate_dep['author_rate'])\n\n# Set CRS from latitude/longitude to Lambert93 for a better grid projection\nmap_dep.crs = {'init': 'epsg:4326'}\nmap_dep = map_dep.to_crs({'init': 'epsg:2154'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can use the `plot` function to plot a `GeoDataFrame` object:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, figsize=(18, 12))\nmap_dep.plot(column='author_rate', cmap='Blues', ax=ax, linewidth=0.1, edgecolor='black')\nax.axis('off')\nax.set_title('Contributors of the Grand Débat per inhabitant (‰)',\n             fontdict={'fontsize':'25', 'fontweight':'3'})\n\n# Add colorbar\nsm = plt.cm.ScalarMappable(cmap='Blues',\n                           norm=plt.Normalize(vmin=0, vmax=map_dep.author_rate.max()))\nsm._A = []\n# Place the cbar next to the plot\ncbar = fig.colorbar(sm,make_axes_locatable(ax).append_axes(\"right\", size=\"5%\"))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The departments with the highest contributor rate are: **Paris** and **Haute-Garonne** (*7.1* and *6.3* contributors per 1000 inhabitant, respectively).  \nOn the contrary the departments with lowest rate are: **Seine-Saint-Denis** and **Haute-Corse** (<*1.8* contributors per 1000 inhabitant).\n"},{"metadata":{},"cell_type":"markdown","source":"# Shaping questions\n<a id=\"shaping\"></a>\n***"},{"metadata":{},"cell_type":"markdown","source":"After this very brief dataset analysis, it is time to focus on the variables of interest: **the questions**. Each dataframe contains several questions, but we will try to treat them all at once.  \n\nThe column names for the questions are a bit messy, we will rename them for more clarity. We build a dataframe containing information about each question: old and new name, title, and the theme and dataframe they are linked to."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"questions = pd.concat([pd.DataFrame({'old_name':df_list[i].columns,\n                                     'df_id':i,\n                                     'theme':themes[i]}) for i in range(len(df_list))])\nquestions = questions[-questions[\"old_name\"].isin(col_common)].reset_index(drop=True)\nquestions = questions.assign(new_name=(pd.Series(\n    ['Q{}'.format(i) for i in range(1, questions.shape[0] + 1)])))\nquestions = questions.assign(question=pd.Series(\n    [name.split(' - ')[1] for name in questions.old_name]))\n\n# Questions rename\ndict_rename = {old:new for old, new in zip(questions.old_name,questions.new_name)}\nfor df in df_list:\n    df.rename(columns=dict_rename,inplace=True)\n    \nquestions.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For each question, we compute the following statistics:\n* **nbrow**: number of rows (i.e. number of contributions for the corresponding theme)\n* **nbnnull**: number of answers that are not *null* (answer is *null* if the contributor skipped that question)\n* **nbunique**: number of distinct answers\n* **nnull_rate**: nbnnull/nbrow * 100\n* **unique_rate**: nbunique/nbnnull * 100\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"questions['nbrow'] = questions.apply(lambda g: df_list[g.df_id].shape[0], axis=1)\nquestions['nbnnull'] = questions.apply(lambda g: df_list[g.df_id].loc[:,g.new_name]\\\n                                       .notnull().sum(), axis=1)\nquestions['nbunique'] = questions.apply(lambda g: df_list[g.df_id].loc[:,g.new_name]\\\n                                        .nunique(), axis=1)\n\nquestions['nnull_rate'] = questions.nbnnull/questions.nbrow * 100\nquestions['unique_rate'] = questions.nbunique/questions.nbnnull * 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can notice that some questions have **very few distinct answers**:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"questions['closed'] = questions['nbunique'] <= 3\nsum(questions.closed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Those 19 questions are **closed-ended question**: the answer is forced into a few choices, mainly **Yes** or **No**."},{"metadata":{},"cell_type":"markdown","source":"We can now aggregate at the **theme** scale:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"questions.groupby(['theme']).agg({'question':'count', 'closed':'sum',\n                                  'nbrow':'mean', 'nnull_rate':'mean'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can note that the two themes **ecological transition** and **taxation** gave more interest to respondants: they have both more contributions and less null values.\n\nIn particular, we see that there are lot of null values, we want to understand that. Let's see which questions have the most null values:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"questions.sort_values('nnull_rate').head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that all of those questions start with **\"Si...\"** *(\"If...\")*. They are conditional: an answer is not necessarily expected.  \n\nIf we pay attention we can notice that the `unique_rate` is also very low, this is because a lot of contributors answered **\"non concerné\"** *(\"not applicable\")*, for instance with question **Q40**:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_list[1].Q40.value_counts().head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some other questions have low `unique_rate` because they are **guided question**: choices were given but the respondant could decide to answer something else. This is the case for instance for questions **Q91**, **Q79** and **Q4**:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_list[3].Q91.value_counts().head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Closed questions analysis\n<a id=\"closed\"></a>\n***"},{"metadata":{},"cell_type":"markdown","source":"For each of the 19 **close questions**, we plot the count of each answer in order to identify most popular opinions.\n\nWe use the **seaborn** library for plotting."},{"metadata":{"trusted":true,"_kg_hide-output":false,"_kg_hide-input":true},"cell_type":"code","source":"# Add frequencies to a countplot\n# Source: https://stackoverflow.com/questions/33179122/seaborn-countplot-with-frequencies\ndef add_frequencies(ax, ncount):\n    for p in ax.patches:\n        x=p.get_bbox().get_points()[:,0]\n        y=p.get_bbox().get_points()[1,1]\n        ax.annotate('{:.1f} %'.format(100.*y/ncount), (x.mean(), y), \n                ha='center', va='bottom', size='small', color='black', weight='bold')\n        \n# Countplot of questions_df\ndef countplot_qdf(questions_df, suptitle):\n    n = questions_df.shape[0]\n    \n    # If there is nothing to plot, we stop here\n    if n==0:\n        return\n    \n    # Numbers of rows and cols in the subplots\n    ncols = 3\n    nrows = (n+3)//ncols\n    fig,ax = plt.subplots(nrows, ncols, figsize=(25,6*nrows))\n    fig.tight_layout(pad=9, w_pad=10, h_pad=7)\n    fig.suptitle(suptitle, size=30, fontweight='bold')\n    \n    # Hide exceeding subplots\n    for i in range(n, ncols*nrows):\n        ax.flatten()[i].axis('off')\n        \n    # Countplot for each question\n    for index, row in questions_df.iterrows():\n        plt.sca(ax.flatten()[index])\n        # We add the sort_values argument to always have the same order: Oui, Non...\n        xlabels = df_list[row.df_id].loc[:,row.new_name]\n        xlabels = xlabels.value_counts().index.sort_values(ascending=False)\n        axi = sns.countplot(x=row.new_name,\n                           data=df_list[row.df_id],\n                           order = xlabels)\n        # Wrap long questions into lines\n        axi.set_title(\"\\n\".join(wrap(row.new_name + '. ' + row.question, 60)))\n        axi.set_xlabel('')\n        # We also set a wrap here (for one very long answer...)\n        axi.set_xticklabels([\"\\n\".join(wrap(s, 17)) for s in xlabels])\n        axi.set_ylabel('Nombre de réponses')\n        add_frequencies(axi, row.nbnnull)\n        \n# Plotting questions, grouped by theme\nfor i in range(len(themes)):\n    countplot_qdf(questions[(questions.closed) & (questions.df_id == i)].reset_index(), themes[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"On the themes of **State organisation, democracy and citizenship**: when asked their opinion, contributors always take side for **change**. In particular, most popular demands include:\n* revising the **functioning of the administration** (**Q26** - **91%**) \n* taking into account the **blank vote** (**Q52** - **82%**) \n* **transforming the Assemblies** (**Q59** - **86%**)"},{"metadata":{},"cell_type":"markdown","source":"On the theme of **ecological transition**, we can note that **69%** of respondents consider their daily life being impacted by climate change (**Q81**). However **95%** of them think they can personally contribute to the environmental protection (**Q83**). Solutions could arise from **heating method** (**61%** - **Q87**) or **mobility** (**42%** - **Q89**)."},{"metadata":{},"cell_type":"markdown","source":"# Open questions analysis\n<a id=\"open\"></a>\n***"},{"metadata":{},"cell_type":"markdown","source":"Most of the information of the dataset lies in the **open questions**, but they are the most **difficult** to analysis!\n\nWe can start with with a basic statistic, the **number of words** contained in the whole dataset."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Count words in a string, a word being here any sequence of characters between white spaces\ndef count_words(s):\n    if s is np.nan:\n        return(0)\n    return(len(s.split()))\n\n# For each dataframe:\n# filter on questions and title\n# count words for each contribution of each question\n# sum it all\nn_words = [df.filter(regex=r'title|^Q', axis=1).apply(np.vectorize(count_words)).sum().sum()\\\n           for df in df_list]\nsum(n_words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The contributions contain **167 million words**! That is equivalent to **325 times** ***Les Misérables***, or **154 times the whole** ***Harry Potter*** **series**.  \n\nIt is impossible to make an exhaustive analysis of the dataset via human reading... **Artificial Intelligence seems necessary** to interpret this dataset. Here you come kagglers!"},{"metadata":{},"cell_type":"markdown","source":"### Basic text mining\n\nTo finish, let's try some simple text mining. We will investigate the information hidden in the contributions. \n\nWe have **75 open questions**. If we do a question-wise analysis, this notebook will get very sprawling. We aggregate all questions at the **theme** scale.\n\nTo begin, one must define **stop words**: those are the most common words that don't give any insight, and must be filtered out when doing **natural language processing**."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Get French stop words (from both nltk and stop_words libraries)\nstop_words = list(set(get_stop_words('fr')).union(stopwords.words('french')))\n# Put them in lowercase ASCII\nstop_words = [unidecode.unidecode(w.lower()) for w in stop_words]\n# Add punctuation and some missing words\nstop_words = set(stop_words +\n                 list(string.punctuation) +\n                 [\"’\", \"...\", \"'\", \"\", \">>\", \"<<\"] +\n                 [\"oui\", \"non\", \"plus\", \"toute\", \"toutes\", \"faut\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next important step is to run a **tokenization**, i.e. splitting text into words. This might be tricky because of punctuation, wich is slightly different according to the language. There are some important features we have to take into considreation: **punctuation**, **case**, **encoding** and **stop words**."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Get tokens from list of strings (can probably be optimised)\ndef get_tokens(s):\n    # MosesTokenizer has been moved out of NLTK due to licensing issues\n    # So we define a simple tokenizer based on regex, designed for French language\n    pattern = r\"[cdjlmnstCDJLMNST]['´`]|\\w+|\\$[\\d\\.]+|\\S+\"\n    tokenizer = RegexpTokenizer(pattern)\n    tokens = tokenizer.tokenize(\" \".join(s.dropna()))\n    # remove punctuation (for words like \"j'\")\n    tokens = [w.translate(str.maketrans('', '', string.punctuation)) for w in tokens]\n    # lowercase ASCII\n    tokens = [unidecode.unidecode(w.lower()) for w in tokens]\n    # remove stop words from tokens\n    tokens = [w for w in tokens if w not in stop_words]\n    return(tokens)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use the tokens to draw a **word cloud**. This is a visual representation of **n-gram** counts. The more frequent a term is, the bigger it will appear on the plot."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def plot_wordcloud(s, title, mw = 500):\n    wordcloud = WordCloud(width=1200, height=600, max_words=mw,\n                          background_color=\"white\").generate(\" \".join(s))\n    plt.figure(figsize=(20, 10))\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.title(title, fontsize=50, pad=50)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's plot a wordcloud for each of the 4 **themes**. We will see what are the **most raised topics** among each of them. \n\n*It may take a few minutes and some GBs of RAM.*"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"col_q = questions.new_name[~questions.closed].append(pd.Series('title'))\nfor i in range(len(themes)):\n    col_q_i = df_list[i].columns.intersection(col_q)\n    tokens = pd.concat([df_list[i][col].dropna() for col in col_q_i])\n    tokens = get_tokens(tokens)\n    plot_wordcloud(tokens, title = themes[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is room for improvment: we count **singular** and **plural** separately, for instance \"*services public*\" and \"*service public*\" on the second graph. We need to add **stemming** here.\n\nWe conclude on these clouds, any interpretations on these results are up to you!"},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n<a id=\"conclusion\"></a>\n***"},{"metadata":{},"cell_type":"markdown","source":"This is the end of this notebook for now, there are a lot more to say about this dataset, but the notebook is getting long. **Thank you for reading it**! \n\nIt was aimed to make you discover the dataset, and maybe inspire you for some more investigation on it.  \n\nWe have seen here only the surface of the data, I may do a second part with **deeper textual analysis**.  \n\nFeel free to **upvote** if you enjoyed this notebook, to **fork** it for further analysis, or to **comment** for any suggestion. All ideas are welcome!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}