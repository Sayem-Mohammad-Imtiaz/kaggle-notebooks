{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# !conda install numpy pytorch torchvision cpuonly -c pytorch -y\n# !pip install matplotlib --upgrade --quiet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install jovian --upgrade --quiet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport math\nimport glob\nimport random\nfrom time import perf_counter \n\nimport torchvision.transforms as transforms\nfrom torchvision.utils import save_image, make_grid\nfrom torchvision.transforms import ToTensor, ToPILImage\nfrom torchvision.models import vgg19\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset\n\nfrom PIL import Image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"project_name='Super_Resolution_Using_GAN'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import jovian\n# jovian.commit(project=project_name, environment=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FeatureExtractor(nn.Module):\n    def __init__(self):\n        super(FeatureExtractor, self).__init__()\n        \n        vgg19_model = vgg19(pretrained=True)\n        \n        self.feature_extractor = nn.Sequential(*list(vgg19_model.features.children())[:18]) #only using initial 18 layers\n        #print(self.feature_extractor)\n\n    def forward(self, img):\n        return self.feature_extractor(img)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ResidualBlock(nn.Module):\n    def __init__(self, in_features):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1)\n        self.bn1 = nn.BatchNorm2d(in_features, 0.8)\n        self.prelu = nn.PReLU()\n\n    def forward(self, x):\n        xin = x\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.prelu(x)\n        x = self.conv1(x)\n        x = self.bn1(x)\n        return xin + x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class GeneratorNet(nn.Module):\n    def __init__(self, in_channels=3, out_channels=3, n_residual_blocks=16):\n        super(GeneratorNet, self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=9, stride=1, padding=4)\n        self.prelu = nn.PReLU()\n        \n        res_blocks = []\n        for _ in range(n_residual_blocks):\n            res_blocks.append(ResidualBlock(64))\n            \n        self.res_blocks = nn.Sequential(*res_blocks)\n\n        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(64, 0.8)\n\n        upsampling = []\n        for out_features in range(2):\n            upsampling += [\n                nn.Conv2d(64, 256, 3, 1, 1),\n                nn.BatchNorm2d(256),\n                nn.PixelShuffle(upscale_factor=2),\n                nn.PReLU(),\n            ]\n        self.upsampling = nn.Sequential(*upsampling)\n\n        self.conv3 = nn.Conv2d(64, out_channels, kernel_size=9, stride=1, padding=4)\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        out1 = self.prelu(self.conv1(x))\n        \n        out = self.res_blocks(out1)\n        \n        out2 = self.bn2(self.conv2(out))\n        \n        out = torch.add(out1, out2)\n        out = self.upsampling(out)\n        out = self.tanh(self.conv3(out))\n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, input_shape):\n        super(Discriminator, self).__init__()\n\n        self.input_shape = input_shape\n        in_channels, in_height, in_width = self.input_shape\n        \n        patch_h, patch_w = int(in_height / 2 ** 4), int(in_width / 2 ** 4)\n        self.output_shape = (1, patch_h, patch_w)\n\n        def discriminator_block(in_filters, out_filters, first_block=False):\n            layers = []\n            layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n            \n            if not first_block:\n                layers.append(nn.BatchNorm2d(out_filters))\n            \n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n            layers.append(nn.BatchNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            \n            return layers\n\n        layers = []\n        in_filters = in_channels\n        for i, out_filters in enumerate([64, 128, 256, 512]):\n            layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n            in_filters = out_filters\n\n        layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1))\n\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, img):\n        return self.model(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class opt:\n    epoch = 0\n    n_epochs = 5\n    dataset_name = \"img_align_celeba\" #https://www.kaggle.com/jessicali9530/celeba-dataset/activity\n    batch_size = 4\n\n    lr = 0.0002\n    b1 = 0.5\n    b2 = 0.999\n    n_cpu = 8\n    \n    hr_height = 256\n    hr_width = 256\n    channels = 3\n    \n    sample_interval = 100\n    checkpoint_interval = 2\n    nrOfImages = 10000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean = np.array([0.485, 0.456, 0.406])\nstd = np.array([0.229, 0.224, 0.225])\n\nclass ImageDataset(Dataset):\n    def __init__(self, root, hr_shape):\n\n        hr_height, hr_width = hr_shape\n        \n        self.lr_transform = transforms.Compose(\n            [\n                transforms.Resize((hr_height // 4, hr_height // 4), Image.BICUBIC),\n                transforms.ToTensor(),\n                transforms.Normalize(mean, std),\n            ]\n        )\n        self.hr_transform = transforms.Compose(\n            [\n                transforms.Resize((hr_height, hr_height), Image.BICUBIC),\n                transforms.ToTensor(),\n                transforms.Normalize(mean, std),\n            ]\n        )\n        #print('root: ', root)\n        self.files = sorted(glob.glob(root + \"/*.*\"))\n        #print('self.files: ', self.files)\n        self.files = self.files[0:opt.nrOfImages]\n\n    def __getitem__(self, index):\n        img = Image.open(self.files[index % len(self.files)])\n        img_lr = self.lr_transform(img)\n        img_hr = self.hr_transform(img)\n\n        return {\"lr\": img_lr, \"hr\": img_hr}\n\n    def __len__(self):\n        return len(self.files)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.makedirs(\"train_images\", exist_ok=True)\nos.makedirs(\"saved_models\", exist_ok=True)\n\ncuda = torch.cuda.is_available()\nhr_shape = (opt.hr_height, opt.hr_width)\n\ngenerator = GeneratorNet()\ndiscriminator = Discriminator(input_shape = (opt.channels, *hr_shape))\nfeature_extractor = FeatureExtractor()\n\nfeature_extractor.eval()\n\ncriterion_GAN = torch.nn.MSELoss()\ncriterion_content = torch.nn.L1Loss()\n\nif cuda:\n    generator = generator.cuda()\n    discriminator = discriminator.cuda()\n    feature_extractor = feature_extractor.cuda()\n    criterion_GAN = criterion_GAN.cuda()\n    criterion_content = criterion_content.cuda()\n\nif opt.epoch != 0:\n    generator.load_state_dict(torch.load(\"saved_models/generator_%d.pth\" % opt.epoch-1))\n    discriminator.load_state_dict(torch.load(\"saved_models/discriminator_%d.pth\" % opt.epoch-1))\n    \noptimizer_G = torch.optim.Adam(generator.parameters(), lr = opt.lr, betas = (opt.b1, opt.b2))\noptimizer_D = torch.optim.Adam(discriminator.parameters(), lr = opt.lr, betas = (opt.b1, opt.b2))\n\nTensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n\ndataloader = DataLoader(\n    ImageDataset(\"../input/celeba-dataset/img_align_celeba/%s\" % opt.dataset_name, hr_shape = hr_shape),\n    batch_size = opt.batch_size,\n    shuffle = True,\n    num_workers = opt.n_cpu, )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch in range(opt.epoch, opt.n_epochs):\n    for i, imgs in enumerate(dataloader):\n\n        imgs_lr = Variable(imgs[\"lr\"].type(Tensor))\n        imgs_hr = Variable(imgs[\"hr\"].type(Tensor))\n\n        valid = Variable(Tensor(np.ones((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n        fake = Variable(Tensor(np.zeros((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n\n        optimizer_G.zero_grad()\n\n        gen_hr = generator(imgs_lr)\n\n        loss_GAN = criterion_GAN(discriminator(gen_hr), valid)\n\n        gen_features = feature_extractor(gen_hr)\n        real_features = feature_extractor(imgs_hr)\n        loss_content = criterion_content(gen_features, real_features.detach())\n\n        loss_G = loss_content + 1e-3 * loss_GAN\n\n        loss_G.backward()\n        optimizer_G.step()\n\n  \n        optimizer_D.zero_grad()\n\n        loss_real = criterion_GAN(discriminator(imgs_hr), valid)\n        loss_fake = criterion_GAN(discriminator(gen_hr.detach()), fake)\n\n        loss_D = (loss_real + loss_fake) / 2\n\n        loss_D.backward()\n        optimizer_D.step()\n\n        print(\"\\n[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\" % (epoch, opt.n_epochs, i, len(dataloader), loss_D.item(), loss_G.item()))\n\n        batches_done = epoch * len(dataloader) + i\n        if batches_done % opt.sample_interval == 0:\n            imgs_lr = nn.functional.interpolate(imgs_lr, scale_factor=4)\n            gen_hr = make_grid(gen_hr, nrow=1, normalize=True)\n            imgs_lr = make_grid(imgs_lr, nrow=1, normalize=True)\n            img_grid = torch.cat((imgs_lr, gen_hr), -1)\n            save_image(img_grid, \"train_images/%d.png\" % batches_done, normalize=False)\n\n    if opt.checkpoint_interval != -1 and epoch % opt.checkpoint_interval == 0:\n        torch.save(generator.state_dict(), \"saved_models/generator_%d.pth\" % epoch) \n        torch.save(discriminator.state_dict(), \"saved_models/discriminator_%d.pth\" % epoch)\n\ntorch.save(generator.state_dict(), \"saved_models/generator.pth\")\ntorch.save(discriminator.state_dict(), \"saved_models/discriminator.pth\")        \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# jovian.reset()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# jovian.log_hyperparams(start_epoch=opt.epoch,\n#                        number_of_epochs=opt.n_epochs,\n#                        lrs=opt.lr,\n#                        beta1=opt.b1,\n#                        beta2=opt.b2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# jovian.log_metrics(generator_loss=loss_G.item(), discriminator_loss=loss_D.item())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#inference\nos.makedirs(\"images_inference\", exist_ok=True)\n\nnetwork = GeneratorNet()\nnetwork = network.eval()\n\nif torch.cuda.is_available():\n    network.cuda()\n    network.load_state_dict(torch.load('saved_models/generator.pth'))\nelse:\n    network.load_state_dict(torch.load('saved_models/generator.pth', map_location=lambda storage, loc: storage))\n\nim_number = '200080'\nimgs_lr = Image.open('../input/celeba-dataset/img_align_celeba/img_align_celeba/' + im_number + '.jpg')\n\nimgs_lr = Variable(ToTensor()(imgs_lr)).unsqueeze(0)\n\nif torch.cuda.is_available():\n    imgs_lr = imgs_lr.cuda()\n    \nwith torch.no_grad():\n    start = perf_counter()\n    gen_hr = network(imgs_lr)\n    elapsed = (perf_counter() - start)\n\n    print('time cost: ' + str(elapsed) + 'sec')\n        \n    print('Shape imgs_lr:', imgs_lr.shape)\n    print('Shape gen_hr:', gen_hr.shape)\n    imgs_lr = nn.functional.interpolate(imgs_lr, scale_factor=4)\n    \n    #imgs_lr = ToPILImage()(imgs_lr[0].data.cpu())\n    #gen_hr = ToPILImage()(gen_hr[0].data.cpu())\n    print('Shape imgs_lr post interpolation:', imgs_lr.shape)\n\n    gen_hr = make_grid(gen_hr, nrow=1, normalize=True)\n    imgs_lr = make_grid(imgs_lr, nrow=1, normalize=True)\n    img_grid = torch.cat((imgs_lr, gen_hr), -1)\n    save_image(img_grid, \"images_inference/\"+ str(im_number) + \".png\", normalize=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# jovian.commit(project=project_name, outputs=[\"saved_models/generator.pth\", \"saved_models/discriminator.pth\"], environment=None)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}