{"cells":[{"metadata":{},"cell_type":"markdown","source":"# What is Transfer Learning?\nTransfer learning is a machine learning method where a model trained for a particular task can be used as a starting point for a model to perform a different task.  \n\nIn transfer learning, we first train a base network on a base dataset and task, and then we repurpose the learned features, or transfer them, to a second target network to be trained on a target dataset and task. This process will tend to work if the features are general, meaning suitable to both base and target tasks, instead of specific to the base task.  \n\nIt is a popular approach in deep learning where pre-trained models are used as the starting point on computer vision and natural language processing tasks given the vast compute and time resources required to develop neural network models on these problems and from the huge jumps in skill that they provide on related problems.  \n\nIt is essentially important in case of deep learning because Deep Neural Networks require quite a large amount of data to perform exceptionally well and that comes with a large overhead of data storage and time required to perform such training every single time. So, it is not always feasible because of various reasons like data size or time overhead or hardware resource limitations etc. The alternative is, the model is trained on a huge dataset but only ONCE. Then those weights are saved for that particular NN graph architecture and can be called back as an initialization to perform a different task.  \n\n![](https://www.topbots.com/wp-content/uploads/2019/12/cover_transfer_learning_1600px_web.jpg)\nImage Source:- https://www.topbots.com/wp-content/uploads/2019/12/cover_transfer_learning_1600px_web.jpg\n\nBecause of the pre-trained weights in case of CNN, the model has already \"learnt\" to extract basic features like edges, shapes, etc which can be used to further fine tune the NN to the exact purpose we need to use it for.  \n\nThese weights and popular architectures are often available Open-Source. These can be found out in the transfer-learning framework of all major software available.  \n\n# Pytorch\n![](https://biii.eu/sites/default/files/2019-03/PyTorch-logo.jpg)\nImage Source:- https://biii.eu/sites/default/files/2019-03/PyTorch-logo.jpg200/1*4br4WmxNo0jkcsY796jGDQ.jpeg\n\nPyTorch is an optimized tensor library for deep learning using GPUs and CPUs.  \n\nWe are going to use Pytorch here because of it's extensive transfer learning module and easy implementation. It is especially popular because it provides an easy and intuitive implementation of DNN in a more pythonic fashion as compared to any other architectures. The syntax resembles closely with Numpy, thus it is also well suited to be used by beginners and get up to speed with implementing the latest SOTA DNN architectures pretty easily and quickly.  \n\nIt also proritizes thinking abour models and algorithms rather than worrying about their syntax. Pytorch has full compatibility with CPU, GPU and TPU.\n\n# Problem Statement\nThis is a fun dataset similar to Cats vs Dogs. But instead we have to identify the characters in images as either Aliens or Predators. The charecters are based on the popular movie [Alien vs Predator](https://en.wikipedia.org/wiki/Alien_vs._Predator_(film)).  \nHere we are going to learn how to leverage transfer learning though the Pytorch library to perform this classification.\n\n## Dataset Description\nThe dataset contains only 247 images each of Aliens and Predators in training set and 100 images each in Test set.  \nSituations like this are more suited for Transfer learning because the train set is very small and without using transfer learning it is going to be very difficult for the NN not to overfit to this data.  \n\nSince this is a binary classification problem and the classes are well balanced, we will use **Accuracy** as the performance metric in this notebook.\n\n# About this Notebook\nThis notebook is intended for guiding beginners through the ropes of Transfer Learning and creating basic Pytorch DNN models, so it will be completely beginner friendly. I will try to explain all concepts I am using in this notebook, but it surely requires some basic understanding of Python, Neural Networks, CNN algorithms and Pytorch syntaxes.  \n\nWith this in mind, let's get started...  \n\n# Imports\nLet's first import all the libraries that we are going to use in this notebook."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/pytorch-image-models/pytorch-image-models-master')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Asthetics\nimport warnings\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\n# General\nimport pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport os\nimport time\nimport random\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\n# Visialisation\nimport matplotlib.pyplot as plt\nfrom matplotlib import offsetbox\n%matplotlib inline\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nfrom PIL import Image\nimport cv2\n\n# Deep Learning\nimport torch\nimport torchvision\nimport timm\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Augmentation\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This generic function below looks if GPU is available in the instance. If it is available it will store the same in the device variable to be used later. Else it will use the CPU. This small helper function makes the code quite robust to the devices it is training on."},{"metadata":{"trusted":true},"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n    \nprint(f'Using device: {device}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Many models/libraries have a random initialization state which might differ from one run to another. Which might lead to difference in performance purely due to randomness and not due to any changes in code or algorithm. To account for such difference, let's fix the randomness by seeding the values to a fixed integer so that we have a much more predictable performance measure."},{"metadata":{"trusted":true},"cell_type":"code","source":"RANDOM_SEED = 42","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=RANDOM_SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_everything()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will get predictable performance everytime we run this code. Moving on, let's see some of the images and do a quick EDA...\n\n# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images_path = '../input/alien-vs-predator-images/data/train'\nvalid_images_path = '../input/alien-vs-predator-images/data/validation'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's first crate a helper function that takes in the name of the class and plots same sample images from the class based on user input.  \nThis enables us to write cleaner code and not repeat the same syntax again and again."},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_image(im_class, examples=4, train_images_path=train_images_path):\n    path = os.path.join(train_images_path, im_class)\n    _, _, filenames = next(os.walk(path))\n    image_list = random.sample(filenames, examples)\n    plt.figure(figsize=(20,10))\n    for i, img in enumerate(image_list):\n        full_path = os.path.join(train_images_path, im_class, img)\n        img = Image.open(full_path)\n        plt.subplot(1 ,examples, i%examples +1)\n        plt.axis('off')\n        plt.imshow(img)\n        plt.title(im_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_image(im_class = 'alien')\nshow_image(im_class = 'predator')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we know what Aliens and Predators look like (for the people who have not already see the movie ðŸ˜œ).  \n\nWe already know from the dataset description that there are equal number of alien and predator images. So, there is no need to check specifically for class balance again.  \n\nSo, now let's move on to Model Building...\n\n# Model\n\nWe will use a simple **EfficientNet B3** model for demonstration purposes. However you can fork this notebook and play with various other pre-trained models and come up with your own observations.  \n\n## Augmentation\nThere another well known concept called **image augmentations** in CNN. What augmentation generally does is, it artificially increases the dataset size by subtly modifying the existing images to create new ones (while training). One added advantage of this is:- The model becomes more generalized and focuses to finding features and representations rather than completely overfitting to the training data. It also sometimes helps the model train on more noisy data as compared to conventional methods.  \n\nExample:-  \n![](https://www.researchgate.net/publication/319413978/figure/fig2/AS:533727585333249@1504261980375/Data-augmentation-using-semantic-preserving-transformation-for-SBIR.png)  \nSource:- https://www.researchgate.net/publication/319413978/figure/fig2/AS:533727585333249@1504261980375/Data-augmentation-using-semantic-preserving-transformation-for-SBIR.png\n\nOne of the most popular image augmentation libraries is **Albumentations**. It has an extensive list of image augmentations, the full list can be found in their [documentation](https://albumentations.ai/docs/).  \n\n*Tip:- Not all augmentations are applicable in all conditions. It really depends on the dataset and the problem. Example:- If your task is to identify if a person is standing or sleeping, applying a rotational augmentation can make the model worse.*  \n\nWith that in mind, let's define our augmentations:-"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_train_transforms():\n    return albumentations.Compose(\n        [\n            albumentations.Resize(256,256),\n            albumentations.HorizontalFlip(p=0.5),\n            albumentations.VerticalFlip(p=0.5),\n            albumentations.Rotate(limit=180, p=0.7),\n            albumentations.RandomBrightness(limit=0.6, p=0.5),\n            albumentations.Cutout(\n                num_holes=8, max_h_size=8, max_w_size=8,\n                fill_value=0, always_apply=False, p=0.5\n            ),\n            albumentations.ShiftScaleRotate(\n                shift_limit=0.25, scale_limit=0.1, rotate_limit=0\n            ),\n            albumentations.Normalize(\n                [0.485, 0.456, 0.406], [0.229, 0.224, 0.225],\n                max_pixel_value=255.0, always_apply=True\n            ),\n            ToTensorV2(p=1.0),\n        ]\n    )\n\ndef get_valid_transforms():\n    return albumentations.Compose(\n        [\n            albumentations.Resize(256,256),\n            albumentations.Normalize(\n                [0.485, 0.456, 0.406], [0.229, 0.224, 0.225],\n                max_pixel_value=255.0, always_apply=True\n            ),\n            ToTensorV2(p=1.0)\n        ]\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the augmentation pipeline is set, we just need to call it in our data generator.  \n\n## Dataset\nBefore creating models we need to create a data-generator that points to a directory and streams the images to the NN for training/validation/test. So, let's go ahead and create that..."},{"metadata":{"trusted":true},"cell_type":"code","source":"class AlienVsPredator(Dataset):\n    def __init__(self, images_filepaths, transform=None):\n        self.images_filepaths = images_filepaths\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images_filepaths)\n\n    def __getitem__(self, idx):\n        image_filepath = self.images_filepaths[idx]\n        image = cv2.imread(image_filepath)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        if os.path.normpath(image_filepath).split(os.sep)[-2] == \"alien\":\n            label = 1.0\n        else:\n            label = 0.0\n        if self.transform is not None:\n            image = self.transform(image=image)[\"image\"]\n        return image, label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This class inherits from `Dataset` class from Pytorch designed to be used for similar purposes.  \n\nThe code snippet below lists the full path to every image in training directory."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_im_paths = []\nfor path, subdirs, files in os.walk(train_images_path):\n    for name in files:\n        train_im_paths.append(os.path.join(path, name))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The code snippet below does the same for validation directory."},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_im_paths = []\nfor path, subdirs, files in os.walk(valid_images_path):\n    for name in files:\n        valid_im_paths.append(os.path.join(path, name))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now creating the train and validation Pytorch Datasets using the full image paths and the dataset class defined above..."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = AlienVsPredator(images_filepaths=train_im_paths,\n                                transform=get_train_transforms())\nvalid_dataset = AlienVsPredator(images_filepaths=valid_im_paths,\n                                transform=get_valid_transforms())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Metrics\nNow let's create some helper functions that will enable us to track certain performance metric (accuracy in this case) during training."},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_accuracy(output, target):\n    output = torch.sigmoid(output) >= 0.5\n    target = target == 1.0\n    \n    return torch.true_divide((target == output).sum(dim=0), output.size(0)).item()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MetricMonitor:\n    def __init__(self, float_precision=3):\n        self.float_precision = float_precision\n        self.reset()\n\n    def reset(self):\n        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n\n    def update(self, metric_name, val):\n        metric = self.metrics[metric_name]\n\n        metric[\"val\"] += val\n        metric[\"count\"] += 1\n        metric[\"avg\"] = metric[\"val\"] / metric[\"count\"]\n\n    def __str__(self):\n        return \" | \".join(\n            [\n                \"{metric_name}: {avg:.{float_precision}f}\".format(\n                    metric_name=metric_name, avg=metric[\"avg\"],\n                    float_precision=self.float_precision\n                )\n                for (metric_name, metric) in self.metrics.items()\n            ]\n        )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now moving on to model definition...  \n\nWe have created a simple dictionary with all the parameters for defining our model. This is easy to use since only changing the model parameters in this dictionary will change the respective parameters throughout the network. Thus we do not have to keep looking for all places one parameter was referred if we wanted to tweak the NN."},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'model': 'efficientnet_b3',\n    'device': device,\n    'lr': 0.001,\n    'batch_size': 32,\n    'num_workers' : 0,\n    'epochs': 5,\n    'out_features': 1\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loader = DataLoader(\n    train_dataset, batch_size=params['batch_size'], shuffle=True,\n    num_workers=params['num_workers'], pin_memory=True,\n)\n\nval_loader = DataLoader(\n    valid_dataset, batch_size=params['batch_size'], shuffle=False,\n    num_workers=params['num_workers'], pin_memory=True,\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CNN Model\nWe will inherit from the nn.Module class to define our model. This is a easy as well as effective way of defining the model as it allows very granular control over the complete NN. We are not using the full capability of it here since it is a tutorial model, but practicing similar definitions will help if/when you decide to play around a little more with the NN layers and functions.  \n\nAlso we are using timm for instancing a pre-trained model.  \nThe complete list of Pytorch pre-trained image models through timm can be found [here](https://rwightman.github.io/pytorch-image-models/)  "},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"class AlienNet(nn.Module):\n    def __init__(self, model_name=params['model'], out_features=params['out_features'],\n                 pretrained=True):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained)\n        '''\n        Efficient net has initally been trained to predict 1000 different\n        classes. But for our purpose we just need to predict a binary class.\n        Thus we will now modify the classification layer of Efficientnet to\n        give output for our particular type of problem.\n        '''\n        n_features = self.model.classifier.in_features\n        self.model.classifier = nn.Linear(n_features, out_features)\n    \n    def forward(self, x):\n        x = self.model(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"model = AlienNet()\nmodel = model.to(params['device'])\ncriterion = nn.BCEWithLogitsLoss().to(params['device'])\noptimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train and Validation\nLet's define our training and validation function now and call upon the dataset using the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(train_loader, model, criterion, optimizer, epoch, params):\n    metric_monitor = MetricMonitor()\n    model.train()\n    stream = tqdm(train_loader)\n    for i, (images, target) in enumerate(stream, start=1):\n        images = images.to(params['device'], non_blocking=True)\n        target = target.to(params['device'], non_blocking=True).float().view(-1, 1)\n        output = model(images)\n        loss = criterion(output, target)\n        accuracy = calculate_accuracy(output, target)\n        metric_monitor.update('Loss', loss.item())\n        metric_monitor.update('Accuracy', accuracy)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        stream.set_description(\n            \"Epoch: {epoch}. Train.      {metric_monitor}\".format(\n                epoch=epoch,\n                metric_monitor=metric_monitor)\n        )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def validate(val_loader, model, criterion, epoch, params):\n    metric_monitor = MetricMonitor()\n    model.eval()\n    stream = tqdm(val_loader)\n    with torch.no_grad():\n        for i, (images, target) in enumerate(stream, start=1):\n            images = images.to(params['device'], non_blocking=True)\n            target = target.to(params['device'], non_blocking=True).float().view(-1, 1)\n            output = model(images)\n            loss = criterion(output, target)\n            accuracy = calculate_accuracy(output, target)\n\n            metric_monitor.update('Loss', loss.item())\n            metric_monitor.update('Accuracy', accuracy)\n            stream.set_description(\n                \"Epoch: {epoch}. Validation. {metric_monitor}\".format(\n                    epoch=epoch,\n                    metric_monitor=metric_monitor)\n            )","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"for epoch in range(1, params['epochs'] + 1):\n    train(train_loader, model, criterion, optimizer, epoch, params)\n    validate(val_loader, model, criterion, epoch, params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Saving the Model\nNow that the model is trained and we have achieved a good accuracy **>97% on validation dataset**, it is always a good idea to save the weights of the NN so that we will not need to re-train the model in case we want to use it sometime."},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(model.state_dict(), f\"{params['model']}_{params['epochs']}epochs_weights.h5\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction\nNow let's use the trained model to do some predictions on the validation dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"examples = 4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_list = random.sample(valid_im_paths, examples)\nvalid_dataset = AlienVsPredator(images_filepaths=image_list,\n                                transform=get_valid_transforms())\nval_loader = DataLoader(\n    valid_dataset, batch_size=examples, shuffle=False,\n    num_workers=0, pin_memory=True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()\npredicted_labels = []\nwith torch.no_grad():\n    for (images, target) in val_loader:\n        images = images.to(params['device'], non_blocking=True)\n        output = model(images)\n        predictions = (torch.sigmoid(output) >= 0.5)[:, 0].cpu().numpy()\n        predicted_labels += [\"Alien\" if is_alien else \"Predator\" for is_alien in predictions]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nfor i, img in enumerate(image_list):\n    full_path = image_list[i]\n    img = Image.open(full_path)\n    plt.subplot(1 ,examples, i%examples +1)\n    plt.axis('off')\n    plt.imshow(img)\n    plt.title(f'{predicted_labels[i]}')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is a simple starter kernel on implementation of Transfer Learning using Pytorch.  \nPytorch has many SOTA Image models which you can try out using the guidelines in this notebook.  \n\nI hope you have learnt something from this notebook. Please feel free to ask below in case of any doubt. I will try my best to answer your questions and make you understand the concepts.  \n\n**If you liked this notebook and use parts of it in you code, please upvote this kernel. It keeps me inspired to come-up with such beginner friendly tutorial type notebooks like this one and share it with the community.**\n\nThanks and happy kaggling!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}