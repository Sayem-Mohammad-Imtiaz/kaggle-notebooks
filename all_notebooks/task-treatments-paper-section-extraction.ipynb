{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport glob\nimport json\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load metadata and filter to papers containing covid-19 terms in title or abstract\n\ntext_files = glob.glob('../input/coronawhy-plus/v6_text/*tsv')\n\nmetadata_df = pd.read_csv('../input/coronawhy/clean_metadata.csv', index_col=0)\n\nmetadata_df.loc[:, 'title_abstract'] = metadata_df.loc[:, 'title'].str.lower() + ' ' + metadata_df.loc[:, 'abstract'].str.lower()\nmetadata_df.loc[:, 'title_abstract'] = metadata_df.loc[:, 'title_abstract'].fillna('')\n\ncovid19_df = metadata_df.loc[metadata_df.title_abstract.str.contains('covid-19|sars-cov-2|2019-ncov|sars coronavirus 2|2019 novel coronavirus')]\n\nprint(covid19_df.shape)\n\ncovid19_sha_list = covid19_df.sha.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nLoad annotation dataframe snapshot.\n\"\"\"\n\nannot_df = pd.read_csv('../input/covid-pc-task-study-design-annotation-200412/Study_Design_Annotation_Snapshot_4_12_20.csv')\n\n\"\"\"\nEdit columns to make more usable.\n\"\"\"\n\n#Rename columns\nannot_df_old_columns = annot_df.columns.tolist()\nprint(annot_df_old_columns)\n\nannot_df_new_columns = [\n    'assignee',\n    'cord_uid',\n    'sha',\n    'title',\n    'url',\n    'in_silico',\n    'in_vitro',\n    'in_vivo',\n    'system_review_ma_rct',\n    'rct',\n    'non_rct',\n    'historical_comparator',\n    'descriptive_study',\n    'system_review_ma_non_rct',\n    'other'\n                       ]\nannot_df_edit = annot_df.copy()\nannot_df_edit.columns = annot_df_new_columns\n\n#Drop irrelevant columns\nannot_df_edit = annot_df_edit.drop(['assignee','title', 'url', 'other'], axis=1)\n\n#Generate a sum column to separate non-annotated papers from annotated\nannot_df_edit['sum'] = annot_df_edit.sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Breakdown of paper annotations:\")\nprint(annot_df_edit['sum'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"annot_df_annotated = annot_df_edit.loc[annot_df_edit['sum'] != 0]\nann_shas = annot_df_annotated.sha.tolist()\nann_shas = [i.split(';')[0] for i in ann_shas]\n\nprint(\"Annotated sha ids:\")\nprint(len(ann_shas))\n\nann_cord_uids = annot_df_annotated.cord_uid.tolist()\nprint(\"Annotated cord_uids:\")\nprint(len(ann_cord_uids))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\"\"\"\nFunctions to extract section headers and the text within sections from the v6_text.json in the coronawhy-plus dataset produced my Mike Honey\n\n\"\"\"\n\ndef generate_section_dict(text_df, paper_sha_list, sections):\n    \"\"\"\n    Returns a dict of {paper_id: section: sentences} for specified paper id and specified sections\n    \"\"\"\n    \n    text_df_with_ids = text_df.loc[text_df.paper_id.isin(paper_sha_list)]\n    text_df_of_sections = text_df_with_ids.loc[text_df_with_ids.section.isin(sections)]\n    section_dict = text_df_of_sections.groupby('paper_id')['sentence'].apply(list).to_dict()\n\n    #concatenate sentences\n    section_concat_sentences_dict = {k: \" \".join(v) for k,v in section_dict.items()}\n    \n    return section_concat_sentences_dict\n   \ndef process_text_tsv_files(text_files, paper_sha_list, sections_oi):\n\n    \"\"\"\n    Iterate through v6_text.json files, extracting papers by sha_id and specified sections.\n    \n    \"\"\"\n    master_dict = {}\n      \n    for text_file in text_files:\n        print(\"Processing %s...\" % text_file)\n        text_df = pd.read_csv(text_file, sep='\\t')\n        text_df.loc[:, 'sentence'] = text_df.loc[:, 'sentence'].astype(str)\n        text_df.loc[:, 'section'] = text_df.loc[:, 'section'].astype(str)\n\n        for section in sections_oi:\n            print(\"Extracting %s section...\" % section)\n            \n            if section not in master_dict.keys():\n                master_dict[section] = {}\n                \n            tmp_dict = generate_section_dict(text_df, paper_sha_list, [section])\n            for k, v in tmp_dict.items():\n                master_dict[section].setdefault(k, []).append(v)\n        \n    return master_dict\n\ndef extract_paper_section_headers(text_files, paper_sha_ids):\n    master_dict = {}\n    for text_file in text_files:\n        print(\"Processing %s...\" % text_file)\n        text_df = pd.read_csv(text_file, sep='\\t')\n        \n        text_df_for_ids = text_df.loc[text_df.paper_id.isin(paper_sha_ids)]\n        section_dict = text_df_for_ids.groupby('paper_id')['section'].apply(list).to_dict()\n        section_dict_unique = {k: list(set(v)) for k,v in section_dict.items()}\n        \n        for paper_id, sections in section_dict_unique.items():\n            for section in sections:\n                master_dict.setdefault(paper_id, set([])).add(section)\n    \n    return master_dict\n\ndef extract_paper_sections(text_files, paper_sha_ids):\n    \n    master_dict = {}\n    for text_file in text_files:\n        print(\"Processing %s...\" % text_file)\n        text_df = pd.read_csv(text_file, sep='\\t')\n        \n        text_df_for_ids = text_df.loc[text_df.paper_id.isin(paper_sha_ids)]\n        \n        sha_text_dict = dict(tuple(text_df_for_ids.groupby('paper_id')))\n        for paper_sha, paper_df in sha_text_dict.items():\n            if paper_sha not in master_dict:\n                master_dict[paper_sha] = {}\n                \n            section_sentences_dict = dict(tuple(paper_df[['section', 'sentence']].groupby('section')))\n            for section, sentence_df in section_sentences_dict.items():\n                sentence_df.loc[:, 'sentence'] = sentence_df.sentence.fillna('')\n                sentences = sentence_df.sentence.tolist()\n                \n                current_sentences = master_dict[paper_sha].get(section, '')\n                new_sentences = current_sentences + ' ' + ' '.join(sentences)\n                master_dict[paper_sha][section] = new_sentences\n                \n    return master_dict\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#covid19_paper_sections = extract_paper_section_headers(text_files, covid19_sha_list)\n\nannot_paper_sections = extract_paper_section_headers(text_files, ann_shas)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nSection counts\n\"\"\"\n\nfrom collections import defaultdict\nimport re\n\nsection_counts_dict = defaultdict(int)\n#for sha, sections in covid19_paper_sections.items():\nfor sha, sections in annot_paper_sections.items():\n    for section in sections:\n        section_counts_dict[section] += 1\n        \n\"\"\"\nInclude:\nmethods\nresults\nstatistics\n\nExclude:\nintroduction\ndiscussion\nfunding\nconclusions\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_sections = [str(i) for i in section_counts_dict.keys()]\nunique_sections","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nFunctions to generate regex match patterns from synonymous words/phrases for filtering subject headers\n\n\"\"\"\n\ndef extract_regex_pattern(section_list, pattern):\n    r = re.compile(pattern)\n    extracted_list = list(filter(r.match, section_list))\n    remaining_list = list(set(section_list) - set(extracted_list))\n    \n    return remaining_list, extracted_list\n\ndef construct_regex_match_pattern(terms):\n    terms = ['.*%s.*' % i for i in terms]\n    pattern = '|'.join(terms)\n    return pattern\n\n#Examples\n#figure and table references\nr = re.compile(\".*figref|.*tabref|.*figure\")\nremaining_list, extracted_list = extract_regex_pattern(unique_sections, r)\nprint(\"Figure and table references:\")\nprint(extracted_list)\n\n#author\nr = re.compile(\".*author\")\nremaining_list, extracted_list = extract_regex_pattern(unique_sections, r)\nprint(\"Sections containing author:\")\nprint(extracted_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nThese are the sections names currently being filtered out.\n\n\"\"\"\n\nexclusion_regex_pattern_terms = [\n    'discussion', \n    'conclusion', \n    'conflicts of interest',\n    'conflict of interest',\n    'fund', \n    'ideas and opinions', \n    'article in press', \n    'legal aspects', \n    'acknowledgement',\n    'acknowledgment',\n    'declaration of', \n    'implications of all the available evidence', \n    'research in context', \n    'author',\n    'interpretation',\n    'competing interests',\n    'references',\n    'article in press',\n    'disclaimer',\n    'contributions',\n    'disclosure',\n    'references',\n    'looking ahead',\n    'summarizing the findings',\n    'literature',\n    'history',\n    'future',\n    'historic',\n    'editor',\n    'contributors',\n    'license'\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"exc_terms = construct_regex_match_pattern(exclusion_regex_pattern_terms)\nincl_sections, excl_sections = extract_regex_pattern(unique_sections, exc_terms)\n\nwith open('excl_paper_sections.txt', 'w') as f:\n    for excl_section in excl_sections:\n        f.write('%s\\n' % excl_section)\n\n#Write sorted counts of all sections (covid19 papers) to text file\nwith open('annot_paper_sections_by_freq.txt', 'w') as f:\n    for i in sorted(section_counts_dict.items(), key=lambda k_v: k_v[1], reverse=True):\n        f.write('%s\\t%s\\n' % (i[0], i[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#covid19_shas_with_sections = covid19_paper_sections.keys()\nannot_shas_with_sections = annot_paper_sections.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sample_papers_without_sections_sha_list = list(set(covid19_sha_list) - set(covid19_shas_with_sections))[:20]\nsample_papers_without_sections_sha_list = list(set(ann_shas) - set(annot_shas_with_sections))[:20]\nsample_papers_without_sections_sha_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"papers_without_sections_dict = extract_paper_section_headers(text_files, sample_papers_without_sections_sha_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sha_section_texts = extract_paper_sections(text_files, covid19_shas_with_sections)\nsha_section_texts = extract_paper_sections(text_files, annot_shas_with_sections)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nGenerate a JSON containing corpus papers:\n{\n    paper_sha: concatenated text from all relevant sections\n}\n\"\"\"\n\nsha_filtered_and_concat_texts = {}\n\nfor sha, section_texts in sha_section_texts.items():\n    \n    all_section_texts = []\n    for section, text in section_texts.items():\n        if section not in excl_sections:\n            all_section_texts.append(text)\n        \n    sha_filtered_and_concat_texts[sha] = ' '.join(all_section_texts)\n\nprint(len(sha_filtered_and_concat_texts))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#with open('covid19_corpus_paper_sections_200410.json', 'w') as f:\nwith open('ann_corpus_paper_sections_200417.json', 'w') as f:\n    json.dump(sha_section_texts, f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_papers_without_sections_sha_list = [i.split(';')[0] for i in sample_papers_without_sections_sha_list]\n\npapers_without_sections_text_dict = extract_paper_sections(text_files, sample_papers_without_sections_sha_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"papers_without_sections_text_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nThe aggregate file is too big to load into the notebook directly, but it can be parsed with ijson\n\n\n\"\"\"\n\n\n!pip install ijson\n\nimport ijson","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n\nv6_text.json individual files\n\nijson items:\nKeys:\ndict_keys(['paper_id', 'language', 'section', 'sentence', 'lemma', 'UMLS', 'GGP', 'SO', 'TAXON', 'CHEBI', 'GO', 'CL', 'DNA', 'CELL_TYPE', 'CELL_LINE', 'RNA', \n'PROTEIN', 'DISEASE', 'CHEMICAL', 'CANCER', 'ORGAN', 'TISSUE', 'ORGANISM', 'CELL', 'AMINO_ACID', 'GENE_OR_GENE_PRODUCT', 'SIMPLE_CHEMICAL', 'ANATOMICAL_SYSTEM', \n'IMMATERIAL_ANATOMICAL_ENTITY', 'MULTI-TISSUE_STRUCTURE', 'DEVELOPING_ANATOMICAL_STRUCTURE', 'ORGANISM_SUBDIVISION', 'CELLULAR_COMPONENT', 'PATHOLOGICAL_FORMATION', \n'ORGANISM_SUBSTANCE', 'sentence_id'])\n\nExample item:\n{'paper_id': '566b5c62fc77292ebe09295d59e7fbf6fc914260', 'language': 'en', 'section': 'survey methodology', \n'sentence': 'Older children who obtained parental consent were given diaries with simplified language to fill in on their own (see Table S1 for more details).', \n'lemma': ['old', 'child', 'who', 'obtain', 'parental', 'consent', 'be', 'give', 'diary', 'with', 'simplify', 'language', 'to', 'fill', 'in', 'on', '-PRON-', 'own', '(', 'see', 'table', 's1', 'for', 'more', 'detail', ')', '.'], \n'UMLS': ['Child', 'parent', 'Consent', 'Diaries', 'Programming Languages'], \n'GGP': [], 'SO': [], 'TAXON': [], 'CHEBI': [], 'GO': [], 'CL': [], 'DNA': [], 'CELL_TYPE': [], 'CELL_LINE': [], 'RNA': [], \n'PROTEIN': [], 'DISEASE': [], 'CHEMICAL': [], 'CANCER': [], 'ORGAN': [], 'TISSUE': [], 'ORGANISM': ['children'], 'CELL': [], 'AMINO_ACID': [], 'GENE_OR_GENE_PRODUCT': [], \n'SIMPLE_CHEMICAL': [], 'ANATOMICAL_SYSTEM': [], 'IMMATERIAL_ANATOMICAL_ENTITY': [], 'MULTI-TISSUE_STRUCTURE': [], 'DEVELOPING_ANATOMICAL_STRUCTURE': [], \n'ORGANISM_SUBDIVISION': [], 'CELLULAR_COMPONENT': [], 'PATHOLOGICAL_FORMATION': [], 'ORGANISM_SUBSTANCE': [], 'sentence_id': '566b51354014260'}\n\n\"\"\"\n\naggregate_text_file = '../input/coronawhy/v6_text.json'\n\nwith open(aggregate_text_file) as f:\n    objects = ijson.items(f, \"item\")\n    for obj in objects:\n        print(obj['paper_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}