{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/league-of-legends-diamond-ranked-games-10-min/high_diamond_ranked_10min.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(columns=['gameId','redWardsPlaced', 'redWardsDestroyed', 'redFirstBlood', 'redKills',\n       'redDeaths', 'redAssists', 'redEliteMonsters', 'redDragons',\n       'redHeralds', 'redTowersDestroyed', 'redTotalGold',\n       'redTotalExperience', 'redTotalMinionsKilled',\n       'redTotalJungleMinionsKilled', 'redGoldDiff', 'redExperienceDiff','redAvgLevel',\n       'redCSPerMin', 'redGoldPerMin']) #remove game ID","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.rename(columns={'blueWins':'target'})\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2 = data.drop(columns=['target'])\npearsoncorr = data2.corr(method='pearson')\nsb.heatmap(pearsoncorr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#change 0 to blue win, 1 to red win\ndata['target'].replace([0,1],[1,0],inplace=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['target'].value_counts()\n#0 = blue wins\n#1 = red wins\n#data set is pretty balanced in terms of win and loses for each team","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(data.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataTypeSeries = data.columns.to_series().groupby(data.dtypes).groups #set df.types as variable dataTypeSeries\n \nprint('Data type of each column of Dataframe :')\nprint(dataTypeSeries)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.utils import resample\nfrom sklearn.feature_selection import RFE\nfrom sklearn.decomposition import PCA\nfrom sklearn import linear_model, decomposition, datasets\nfrom sklearn.preprocessing import scale","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pipleine for imputing, OHE, preprocessing\nnumeric_features = ['blueWardsPlaced', 'blueWardsDestroyed', 'blueFirstBlood',\n       'blueKills', 'blueDeaths', 'blueAssists', 'blueEliteMonsters',\n       'blueDragons', 'blueHeralds', 'blueTowersDestroyed', 'blueTotalGold',\n       'blueTotalExperience', 'blueTotalMinionsKilled',\n       'blueTotalJungleMinionsKilled', 'blueGoldDiff', 'blueExperienceDiff',\n       'blueAvgLevel', 'blueCSPerMin', 'blueGoldPerMin']\n\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())])\n\n#categorical_features = []\n\n#categorical_transformer = Pipeline(steps=[\n    #('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    #('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features)])\n        #('cat', categorical_transformer, categorical_features)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\n#from sklearn.metrics import plot_precision_recall_curve\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import roc_auc_score, roc_curve, auc\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import f1_score\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.datasets import make_classification\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifiers = [\n    #LogisticRegression(),\n    #DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    #AdaBoostClassifier(),\n    #XGBClassifier(),\n    #SVC(kernel='linear', \n            #class_weight='balanced', # penalize\n            #probability=True),\n    #GaussianNB()\n    #MLPClassifier(),\n    #KNeighborsClassifier()\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = Pipeline(steps=[('preprocessor', preprocessor),\n                        ('feature_selection', SelectFromModel(RandomForestClassifier(n_estimators = 100))),\n                        ('clf', None)])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# y are the values we want to predict\ny = np.array(data['target'])\n# Remove the labels from the features\n# axis 1 refers to the columns\nX = data.drop('target', axis = 1)\n# Saving feature names for later use\nX_list = list(X.columns)\nX_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training Features Shape:', X_train.shape)\nprint('Training Labels Shape:', y_train.shape)\nprint('Testing Features Shape:', X_test.shape)\nprint('Testing Labels Shape:', y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_transformer_feature_names(columnTransformer):\n  output_features = []\n\n  for name, pipe, features in columnTransformer.transformers_:\n    if name!='remainder':\n      for i in pipe:\n        trans_features = []\n        if hasattr(i, 'categories_'):\n          trans_features.extend(i.get_feature_names(features))\n        else:\n          trans_features = features\n      output_features.extend(trans_features)\n\n  return np.array(output_features)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_things = []\nprecision_recall_things = []","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"for classifier in classifiers:\n    clf.set_params(clf=classifier).fit(X_train, y_train)\n    classifier_name = classifier.__class__.__name__\n    print(str(classifier))\n    print('~Model Score: %.3f' % clf.score(X_test, y_test))\n\n    y_score = clf.predict_proba(X_test)[:,1]\n\n    y_pred = clf.predict(X_test)\n    \n    roc_auc = roc_auc_score(y_test, y_score)\n    fpr, tpr, _ = roc_curve(y_test, y_score)\n    roc_things.append((fpr, tpr, '{} AUC: {:.3f}'.format(classifier_name, roc_auc)))\n    \n    precision, recall, thresholds = precision_recall_curve(y_test, y_score)\n    pr_auc = auc(recall, precision)\n    precision_recall_things.append((recall, precision, thresholds, '{} AUC: {:.3f}'.format(classifier_name, pr_auc)))\n    #plot_precision_recall_curve(clf, X_test, y_test)\n     \n    feature_names = get_transformer_feature_names(clf.named_steps['preprocessor'])\n\n    try:\n      importances = classifier.feature_importances_\n      indices = np.argsort(importances)[::-1]\n      print('~Feature Ranking:')\n    \n      for f in range (X_test.shape[1]):\n        print ('{}. {} {} ({:.3f})'.format(f + 1, feature_names[indices[f]], indices[f], importances[indices[f]]))\n    except:\n      pass\n    \n    scores = cross_val_score(clf, X, y, cv=5)\n    print('~Accuracy: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std() * 2))\n    \n    titles_options = [(\"Confusion Matrix\", 'true')]\n    for title, normalize in titles_options:\n        disp = plot_confusion_matrix(clf, X_test, y_test,\n                                     cmap=plt.cm.Blues,\n                                     normalize=normalize)\n        disp.ax_.set_title(title)\n\n        print(title)\n        print(disp.confusion_matrix)\n\n    plt.show()\n    \n    print('~Confusion Matrix:''\\n',\n    confusion_matrix(y_test, y_pred))\n    print('~Classification Report:''\\n',\n    classification_report(y_test, y_pred))\n   \n    print('~Average Precision Score: {:.3f}'.format(average_precision_score(y_test, y_score)))\n    print('~roc_auc_score: {:.3f}'.format(roc_auc))\n    print('~precision-recall AUC: {:.3f}'.format(pr_auc))\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ROC Curves summarize the trade-off between the true positive rate and false positive rate for a predictive model using different probability thresholds.\nroc_plt = plt.figure()\nlw = 4\nfor roc_thing in roc_things:\n    fpr, tpr, label = roc_thing\n    plt.plot(fpr, tpr, lw=lw, label=label)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.title('ROC curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Precision-Recall curves summarize the trade-off between the true positive rate and the positive predictive value for a \n#predictive model using different probability thresholds.\npr_plt = plt.figure()\nfor pr_thing in precision_recall_things:\n    recall, precision, _, label = pr_thing\n    plt.plot(recall, precision, lw=lw, label=label)\nratio = y_test[y_test].shape[0] / y_test.shape[0]\nplt.hlines(y=ratio, xmin=0, xmax=1, color='navy', lw=lw, linestyle='--')\nplt.vlines(x=ratio, ymin=0, ymax=1, color='navy', lw=lw, linestyle='--')\nplt.title('Precision-recall plot')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import hmean\nimport numpy.ma as ma\n\nrecall, precision, thresholds, _ = precision_recall_things[0]\n\na = np.column_stack((recall,precision))\n\na = ma.masked_less_equal(a, 0)\na = ma.mask_rows(a)\nf1 = hmean(a,axis=1)\n\nthreshold_that_maximizes_f1 = thresholds[np.argmax(f1)]\nprint('threshold that optimizes f1: {}'.format(threshold_that_maximizes_f1))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}