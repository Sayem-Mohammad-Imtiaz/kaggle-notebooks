{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Esercitazione su k-Means e Hierarchical Clustering"},{"metadata":{},"cell_type":"markdown","source":"## Indice contenuti\n- [Obiettivo esercitazione](#Obiettivo-esercitazione)\n- [Descrizione ed analisi del dataset](#Descrizione-ed-analisi-del-dataset)\n- [Analisi esplorativa del dataset](#Analisi-esplorativa-del-dataset)\n    - [Caricamento in memoria del dataset](#Caricamento-in-memoria-del-dataset)\n    - [Pulizia del dataset](#Pulizia-del-dataset)\n    - [Data Preparation](#Data-Preparation)\n    - [Trattamento Outliers](#Trattamento-Outliers)\n- [Clustering via k-Means](#Clustering-via-k-Means)\n    - [k-Means++](#kMeans++)\n    - [Trovare il numero ottimale di clusters](#Trovare-il-numero-ottimale-di-clusters)\n        - [Metodo Elbow](#Metodo-Elbow)\n        - [Analisi di Silhouette](#Analisi-di-Silhouette)\n    - [BoxPlot ottenuti con k-Means](BoxPlot-ottenuti-con-k-Means)\n- [Clustering gerarchico](#Clustering-gerarchico)\n    - [Agglomerative clustering](#Agglomerative-Clustering)\n- [DBSCAN](#DBSCAN)\n- [Analisi dei risultati ottenuti](#Analisi-dei-risultati-ottenuti)\n<hr>"},{"metadata":{},"cell_type":"markdown","source":"## Obiettivo esercitazione\nL'esercitazione ha l'obiettivo di applicare su un dataset reale i differenti algoritmi di clustering, in particolare k-Means e Hierarchical Clustering.\n\nSi effettueranno, inoltre, differenti variazioni all'applicazione standard degli algoritmi per comprendere l'utilizzo dei differenti iper-parametri a seconda delle documentazioni ufficiali dei metodi utilizzati."},{"metadata":{},"cell_type":"markdown","source":"## Descrizione ed analisi del dataset\nIl dataset che verrà utilizzato è disponibile su <a href=\"https://archive.ics.uci.edu/ml/datasets/online+retail\">UCI</a> e tratta informazioni circa le transazioni di acquisti online effettuati tra il 01/12/2010 e il giorno 09/12/2011 al fine di identificare dei cluster tra la clientela in base agli acquisti effettuati.\n\nIn particolare, si desidera applicare la segmentazione degli utenti in base ad un fattore RFM che tiene conto dei seguenti aspetti:\n\n- <b>R (Recency)</b>: Numero di giorni trascorsi dall'ultimo acquisto\n- <b>F (Frequency)</b>: Numero di transazioni effettuate\n- <b>M (Monetary)</b>: Ammontare economico delle transazioni registrate\n\nLe features presenti nel dataset sono le seguenti:\n- <b>InvoiceNo</b>: Numero della fattura. Tipo di dato nominale, univoco e espresso su 6 cifre. _Se il codice comincia per la lettera 'c', indica la cancellazione dell'ordine.\n- <b>StockCode</b>: Codice univoco del prodotto acquistato, espresso su 5 cifre.\n- <b>Description</b>: Descrizione del prodotto. Tipo di dato nominale.\n- <b>Quantity</b>: Quantità di ciascun prodotto acquistato in una singola transazione. Tipo di dato numerico.\n- <b>InvoiceDate</b>: Timestamp dell'emissione della fattura. Tipo di dato numerico.\n- <b>UnitPrice</b>: Prezzo unitario del prodotto acquistato in sterline. Tipo di dato numerico.\n- <b>CustomerID</b>: Codice univoco dell'acquirente. Tipo di dato nominale.\n- <b>Country</b>: Nome della nazione di residenza dell'acquirente. Tipo di dato nominale."},{"metadata":{},"cell_type":"markdown","source":"## Analisi esplorativa del dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import delle l'analisi esplorativa dei dati\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport datetime as dt\nimport matplotlib.pyplot as plt\n\n# import delle librerie richieste per l'applicazione di algoritmi di clustering\nimport sklearn\nfrom sklearn.cluster import KMeans\nfrom scipy.cluster.hierarchy import linkage\nfrom sklearn.metrics import silhouette_score\nfrom scipy.cluster.hierarchy import cut_tree\nfrom scipy.cluster.hierarchy import dendrogram\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Caricamento in memoria del dataset"},{"metadata":{},"cell_type":"markdown","source":"Con il seguente comando si effettua il caricamento in memoria di quanto contenuto nel dataset _'OnlineRetail.csv'_.\n\nPer condurre una prima fase di analisi esplorativa e comprendere la natura dei dati a disposizione, si stampano di seguito i primi cinque esempi presenti nel dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#encoding: Encoding to use for UTF when reading/writing\n#header\nstore = pd.read_csv('../input/online-retail-ii-uci/online_retail_II.csv', sep=\",\", encoding=\"ISO-8859-1\", header=0)\nstore.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Per ottenere informazioni statistiche inerenti ciascuna feature a disposizione, mediante il metodo _describe()_ si è provveduto al calcolo delle seguenti informazioni:\n- <b>count</b>: conteggio del numero di esempi per la feature selezionata\n- <b>mean</b>: media aritmetica per la feature selezionata\n- <b>std</b>: deviazione standard per la feature selezionata\n- <b>min</b>: valore minimo presentato dagli esempi per la feature selezionata\n- <b>25%</b>: primo quartile calcolato sugli esempi per la feature selezionata\n- <b>50%</b>: secondo quartile calcolato sugli esempi per la feature selezionata\n- <b>75%</b>: terzo quartile calcolato sugli esempi per la feature selezionata\n- <b>max</b>: valore massimo presentato dagli esempi per la feature selezionata"},{"metadata":{"trusted":true},"cell_type":"code","source":"store.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Successivamente, al fine di comprendere le dimensioni (in termini di esempi e di features a disposizione), mediante apposito attributo si stampano il numero di righe e di colonne del DataFrame:"},{"metadata":{"trusted":true},"cell_type":"code","source":"store.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Al fine di ottenere una descrizione complessiva del Dataframe (e dunque del relativo dataset) caricato, mediante il metodo _info()_ si sono ottenute le seguenti informazioni:\n- <b>#</b>: numero di feature presente nel DataFrame\n- <b>Column</b>: intestazione delle features nel DataFrame\n- <b>Non-Null Count</b>: contatore di valori non nulli per ogni feature presente nel DataFrame\n- <b>Dtype</b>: tipo di dato memorizzato per ogni feature presente nel DataFrame"},{"metadata":{"trusted":true},"cell_type":"code","source":"store.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"store.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pulizia del dataset"},{"metadata":{},"cell_type":"markdown","source":"### Gestione dei valori mancanti\nAl fine di gestire propriamente i dati mancanti, di seguito è realizzata una funzione che indichi, per ogni feature presente nel dataset, la percentuale dei dati mancanti.\n\nCome è possibile osservare, le seguenti features presentano valori nulli:\n- Description\n- CustomerID"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_null = round(100*(store.isnull().sum())/len(store), 2)\ndf_null","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dato l'alto numero di esempi presenti nel dataset, si decide di rimuovere le istanze che presentino valori nulli. Tale operazione è svolta utilizzando il metodo _dropna()_. \n\nSuccessivamente, invece, si provvede a ristampare il nuovo numero di esempi e di features che presenta il dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"store = store.dropna()\nstore.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation\nSecondo quanto anticipato in apertura, si provvede ad introdurre nuove features per poter valutare il comportamento dei clienti. In particolare, si definiscono le seguenti tre features:\n- <b>R (Recency)</b>: Numero di giorni trascorsi dall'ultimo acquisto\n- <b>F (Frequency)</b>: Numero di transazioni effettuate\n- <b>M (Monetary)</b>: Ammontare economico delle transazioni registrate"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Per effettuare operazioni di Join, il tipo di dato CustomerID viene convertito in tipo String\nstore['Customer ID'] = store['Customer ID'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Introduzione del nuovo attributo Monetary\nstore['Amount'] = store['Quantity']*store['Price']\nrfm_m = store.groupby('Customer ID')['Amount'].sum()\nrfm_m = rfm_m.reset_index()\nrfm_m.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Introduzione del nuovo attributo Frequency\n\nrfm_f = store.groupby('Customer ID')['Invoice'].count()\nrfm_f = rfm_f.reset_index()\nrfm_f.columns = ['Customer ID', 'Frequency']\nrfm_f.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Unione dei due dataframe: corrisponde ad un Inner-JOIN SQL\n\nrfm = pd.merge(rfm_m, rfm_f, on='Customer ID', how='inner')\nrfm.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Conversione della data nel tipo supportato da Python DateTime per effettuare le dovute operazioni\nstore['InvoiceDate'] = pd.to_datetime(store['InvoiceDate'],format='%Y-%m-%d %H:%M')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calcolo della data massima registrata all'interno del dataset\nmax_date = max(store['InvoiceDate'])\nmax_date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calcolo della differenza tra la data massima registrata nel dataset e il valore espresso per _InvoiceDate_\nstore['Diff'] = max_date - store['InvoiceDate']\nstore.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Raggruppando gli esempi per CustomerID, si prende il valore minore della data\nrfm_p = store.groupby('Customer ID')['Diff'].min()\nrfm_p = rfm_p.reset_index()\nrfm_p.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Introduzione della feature Recency, estrapolando dalla data solo il numero di giorni\nrfm_p['Diff'] = rfm_p['Diff'].dt.days\nrfm_p.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Unione dei dataframe, al fine di ottenere l'ultimo DataFrame complessivo\nrfm = pd.merge(rfm, rfm_p, on='Customer ID', how='inner')\n#Intestazione delle colonne\nrfm.columns = ['Customer ID', 'Amount', 'Frequency', 'Recency']\n#Stampa dei primi 5 esempi\nrfm.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Trattamento Outliers\nPer rimuovere gli outliers presenti nel dataset, inizialmente si realizza un boxplot nel quale vengono plottati gli esempi rispetto agli attributi _Amount_, _Frequency_ e _Recency_.\n\nSuccessivamente, onde evitare problemi circa la presenza di dati espressi su un diverso range numerico, si effettua la standardizzazione, mediante apposito metodo _StandardScaler()_ applicata sul dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"attributes = ['Amount','Frequency','Recency']\nplt.rcParams['figure.figsize'] = [10,8]\nsns.boxplot(data = rfm[attributes], orient=\"v\", palette=\"Set2\" ,whis=1.5,saturation=1, width=0.7)\nplt.title(\"Outliers nel dataset\", fontsize = 14, fontweight = 'bold')\nplt.ylabel(\"Range\", fontweight = 'bold')\nplt.xlabel(\"Attributes\", fontweight = 'bold')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rimozione degli outliers per Amount utilizzando InterQuartileRange\nQ1 = rfm.Amount.quantile(0.05)\nQ3 = rfm.Amount.quantile(0.95)\nIQR = Q3 - Q1\nrfm = rfm[(rfm.Amount >= Q1 - 1.5*IQR) & (rfm.Amount <= Q3 + 1.5*IQR)]\n\n# Rimozione degli outliers per Recency utilizzando InterQuartileRange\nQ1 = rfm.Recency.quantile(0.05)\nQ3 = rfm.Recency.quantile(0.95)\nIQR = Q3 - Q1\nrfm = rfm[(rfm.Recency >= Q1 - 1.5*IQR) & (rfm.Recency <= Q3 + 1.5*IQR)]\n\n# Rimozione degli outliers per Frequency utilizzando InterQuartileRange\nQ1 = rfm.Frequency.quantile(0.05)\nQ3 = rfm.Frequency.quantile(0.95)\nIQR = Q3 - Q1\nrfm = rfm[(rfm.Frequency >= Q1 - 1.5*IQR) & (rfm.Frequency <= Q3 + 1.5*IQR)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Scaling degli attributi\nrfm_df = rfm[['Amount', 'Frequency', 'Recency']]\n\nsc = StandardScaler()\ndf_scaled = sc.fit_transform(rfm_df)\n#Stampa delle nuove dimensioni\ndf_scaled.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Conversione a dataframe\ndf_scaled = pd.DataFrame(df_scaled)\n#Intestazione delle colonne\ndf_scaled.columns = ['Amount', 'Frequency', 'Recency']\n#Stampa dei primi 5 esempi standardizzati\ndf_scaled.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Clustering via k-Means\n\nk-Means è un algoritmo di clustering non supervisionato, tra i più semplici e popolari messi a disposizione dalla libreria _Sci-Kit_.\n\nUn cluster è definito come un insieme di punti dati che il clustering è uno degli algoritmi di apprendimento automatico non supervisionati più semplici e popolari.\n\nDefinito il valore del parametro k, che esplica il numero di centroidi da identificare nel dataset. Un centroide è la posizione immaginaria o reale che rappresenta il centro di ciascun cluster.\n\nL'algoritmo prevede l'assegnazione di ogni punto dati viene a ciascuno dei cluster utilizzando la nozione di distanza. In altre parole, l'algoritmo k-Means identifica il numero k di centroidi e quindi assegna ogni punto dati al cluster più vicino, mantenendo i centroidi i più piccoli possibili.\n\nPer clusterizzare i dati presenti nel dataset, l'algoritmo k-Means identifica randomicamente un primo gruppo di centroidi e tali sono utilizzati come punti iniziali per ogni cluster. Successivamente, si effettua il ricalcolo dei centroidi ogni qualvolta un nuovo esempio è assegnato al cluster, al fine di ottimizzare le posizioni dei centroidi.\nIl processo di ottimizzazione termina quando si raggiunge il numero delle iterazioni massime (definite) oppure quando si è giunti alla convergenza del metodo."},{"metadata":{},"cell_type":"markdown","source":"### kMeans++\nNel k-means classico, si utilizza un seme casuale per posizionare i centroidi iniziali, che a volte può provocare cattivi raggruppamenti o una lenta convergenza se i centroidi iniziali sono scelti male. Un modo per risolvere questo problema è eseguire l'algoritmo k-mean più volte su un set di dati e scegliere il modello con le migliori prestazioni in termini di SSE.\n\nUn'altra strategia è quella di posizionare i centroidi iniziali molto distanti tra loro tramite l'algoritmo k-means ++, che porta a risultati migliori e più coerenti rispetto ai classici k-mean \n\nPer utilizzare il k-Means++ basterà porre l'attributo init = 'k-means++' (che è già posto di default). Per utilizzare il k-Means classico bisognerà porre l'attributo init = 'random'."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Metodo k-Means con un numero di clusters arbitrario.\n# - n_clusters: numero di cluster desiderati (3) - limitazione del k-Means;\n# - n_init: esegue l'algoritmo n volte in modo indipendente, con diversi centroidi casuali per scegliere il modello finale come quello con il SSE più basso.\n# - max_iter: indica il numero massimo di iterazioni per ogni singola esecuzione (qui, 300). \n\n\n#method = KMeans(n_clusters=4, random_state = 1, max_iter=300, tol=1e-04, init='random', n_init=10)\nmethod = KMeans(n_clusters=4, random_state = 1, max_iter=300, tol=1e-04, init='k-means++', n_init=10)\nmethod.fit(df_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Stampa delle etichette relative ai cluster\nmethod.labels_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Trovare il numero ottimale di clusters"},{"metadata":{},"cell_type":"markdown","source":"#### Metodo Elbow\nAl fine di identificare il giusto numero per il parametro _n_clusters_ è possibile definire un metodo grafico che consenta, variando il parametro mediante una lista di valori espressi, di poter valutare l'attributo _intertia_ (ovvero la somma della radice delle distanze dei campioni dal centro del cluster più vicino)."},{"metadata":{"trusted":true},"cell_type":"code","source":"elbow_values = []\nrange_n_clusters = [1, 2, 3, 4, 5, 6, 7, 8]\nfor num_clusters in range_n_clusters:\n    method = KMeans(n_clusters=num_clusters, random_state = 1, max_iter=300, tol=1e-04, init='k-means++', n_init=10)\n    method.fit(df_scaled)\n    \n    elbow_values.append(method.inertia_)\n    \n# Plot del valore della somma della radice delle distanze al crescere del numero dei cluster\nplt.plot(range(1, 9), elbow_values, marker='o')\nplt.ylabel(\"Sum of Squared Distance\")\nplt.xlabel(\"Numero dei cluster\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Analisi di Silhouette\n\nL'analisi di Silhouette si riferisce a un metodo di interpretazione e convalida della coerenza dei dati rispetto ai cluster identificati.\n\nIl valore dek coefficiente di Silhouette è una misura di quanto un oggetto sia simile al proprio cluster (coesione) rispetto ad altri cluster (separazione). Tale valore è espresso in un intervallo [-1, +1], dove un valore alto indica che l'esempio è ben adattato al proprio cluster e scarsamente abbinato ai cluster vicini. Se la maggior parte degli oggetti ha un valore elevato, la suddivisione degli esempi nei rispettivi cluster è appropriata. Se molti punti, invece, hanno un valore basso o negativo, la suddivisione degli esempi nei cluster definiti potrebbe risultare inappropriata.\n\nL'analisi di Silhouette può essere condotta utilizzando una qualsiasi metrica di distanza, come la distanza euclidea o la distanza di Manhattan.\nIn particolare, può essere espressa come segue:\n$$\\text{silhouette score}=\\frac{p-q}{max(p,q)}$$\n\n$p$ è la distanza media tra il punto e il centroide del cluster più vicino.\n\n$q$ è la distanza media intra-cluster definita su tutti i punti presenti nel proprio cluster.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Definizione della lista del numero di cluster da testare\nrange_n_clusters = list(x for x in range (2,10+1))\n\nfor num_clusters in range_n_clusters:\n    method = KMeans(n_clusters=num_clusters, max_iter=50)\n    method.fit(df_scaled)\n    cluster_labels = method.labels_\n    # Calcolo coefficiente di silhouette\n    silhouette_avg = silhouette_score(df_scaled, cluster_labels)\n    print(\"Per n_clusters={0}, il coefficiente di Silhouette è pari a {1}\".format(num_clusters, silhouette_avg))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ottenuti i risultati del coefficiente di Silhouette, si sceglie il valore 3 per il parametro _n_clusters_ e si riesegue nuovamente l'algoritmo k-Means registrando le relative etichette."},{"metadata":{"trusted":true},"cell_type":"code","source":"method = KMeans(n_clusters=3, random_state = 1, max_iter=300, tol=1e-04, init='k-means++', n_init=10)\nmethod.fit(df_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"method.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assegnazione delle etichette a ciascun esempio presente nel DataFrame\nrfm['Cluster_Id'] = method.labels_\n# Stampa dei primi 5 esempi\nrfm.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### BoxPlot ottenuti con k-Means"},{"metadata":{"trusted":true},"cell_type":"code","source":"features_list = ['Amount', 'Frequency', 'Recency']\n\nfor feature in features_list:\n    sns.boxplot(x='Cluster_Id', y=feature, data=rfm)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Clustering gerarchico\n\nGli algoritmi gerarchici in genere clusterizzano i dati usando le misure di distanza. Tuttavia, l'uso delle funzioni di distanza non è obbligatorio. Molti algoritmi gerarchici utilizzano altri metodi di clustering, ad esempio metodi density-based o graph-based, come subroutine per la costruzione della gerarchia.\n\nUno dei motivi principali di utiulizzo di tale modalità di clustering è che diversi livelli di granularità del clustering forniscono dei dettagli specifici per l'applicazione. Ciò fornisce una tassonomia di cluster, che possono essere esplorati sulla base di tali dettagli semantici.\n\nL'organizzazione gerarchica consente la navigazione manuale molto conveniente per un utente, specialmente quando il contenuto dei cluster può essere descritto in modo semanticamente comprensibile. In altri casi, tali organizzazioni gerarchiche possono essere utilizzate dagli algoritmi di indicizzazione, rispetto alle macroaree di riferimento.\nInoltre, tali metodi possono talvolta essere utilizzati anche per creare cluster \"piatti\" migliori (dove tutte le categorie sono posto allo stesso livello). Alcuni metodi gerarchici agglomerativi e metodi di divisione, possono fornire cluster di qualità migliore rispetto ai metodi di partizionamento come k-Means, sebbene con un costo computazionale più elevato.\n\nEsistono due tipi di algoritmi gerarchici, a seconda di come viene costruito l'albero gerarchico dei cluster:\n- Metodi bottom-up (agglomerativi): i singoli punti dati vengono successivamente agglomerati in cluster di livello superiore. La principale variazione tra i diversi metodi è nella scelta della funzione obiettivo utilizzata per fondere i cluster.\n- Metodi top-down (divisivi): un approccio top-down viene utilizzato per partizionare successivamente i punti in una struttura ad albero. Un algoritmo di clustering piatto può essere utilizzato per il partizionamento in un determinato passo. Tale approccio offre un'enorme flessibilità in termini di scelta del compromesso tra l'equilibrio nella struttura ad albero e l'equilibrio nel numero di punti in ciascun nodo della struttura ad albero."},{"metadata":{},"cell_type":"markdown","source":"**Single Linkage:<br>**\n\nNel clustering che sfrutta la modalità di collegamento _single linkage_, la distanza tra due cluster è definita come la più piccola distanza calcolabile tra due punti in ciascun cluster. Per esempio, la distanza tra il cluster “r” e “s” è uguale alla lunghezza dell'arco tra i due punti più vicini, così come visibile dalla figura riportata.\n\n![](https://www.saedsayad.com/images/Clustering_single.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applicazione del metodo Single Linkage\nsingle_linkage = linkage(df_scaled, method=\"single\", metric='euclidean')\ndendrogram(single_linkage)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Complete Linkage<br>**\n\nNel metodo _Complete Linkage_, la distanza tra due cluster è definita come la più grande distanza tra due punti in ciascun cluster.\n\nPer esempio, la distanza tra i cluster “r” e “s” è uguale alla lunghezza dell'arco tra i due punti più distanti dei due cluster.\n\n![](https://www.saedsayad.com/images/Clustering_complete.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applicazione del metodo Complete linkage\ncomplete_linkage = linkage(df_scaled, method=\"complete\", metric='euclidean')\ndendrogram(complete_linkage)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Average Linkage:<br>**\n\nCon il metodo _Average Linkage_, la distanza tra due cluster è definita come la distanza media presente tra ciascun punto di un cluster con tutti i punti dell'altro cluster.\n\nPer esempio, la distanza tra i cluster “r” e “s” è uguale alla lunghezza mediata dell'arco che connette i punti di un cluster all'altro.\n\n![](https://www.saedsayad.com/images/Clustering_average.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applicazione del metodo Average linkage\navg_linkage = linkage(df_scaled, method=\"average\", metric='euclidean')\ndendrogram(avg_linkage)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Taglio del Dendrogramma in base al valore di K"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Desiderando un numero di cluster pari a 3, si inizializza il parametro n_clusters=3\ncluster_labels = cut_tree(avg_linkage, n_clusters=3).reshape(-1, )\n#Stampa delle etichette dei cluster\ncluster_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assegnazione delle etichette a ciascun esempio presente nel DataFrame\nrfm['Cluster_Labels'] = cluster_labels\n# Stampa dei primi 5 elementi presenti nel DataFrame\nrfm.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot delle Features\n\nfeatures_list = ['Amount', 'Frequency', 'Recency']\n\nfor feature in features_list:\n    sns.boxplot(x='Cluster_Labels', y=feature, data=rfm)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Agglomerative Clustering"},{"metadata":{},"cell_type":"markdown","source":"Nel clustering agglomerativo, come già accennato, i singoli punti dati vengono agglomerati iterativamente in cluster di livello superiore.\nNel primo step, ogni singolo punto costituisce un cluster. Successivamente, si agglomerano insieme via via sempre più punti, andando a costruire cluster sempre più popolati.\nIl metodo si arresta quando si raggiunge un certo numero di cluster.\nNel metodo seguente vengono utilizzati i seguenti parametri:\n- <b>n_clusters=3</b>: si desiderano tre cluster come suggerito dal metodo Elbow\n- <b>affinity</b>: metrica utilizzata per computare il linkage. Si utilizza la distanza euclidea.\n    - <b>euclidean</b>\n    - <b>l1</b>\n    - <b>l2</b>\n    - <b>manhattan</b>\n    - <b>cosine</b>\n    - <b>precomputed</b>\n- <b>linkage</b>: criterio di collegamento da utilizzare. Ne esistono diversi:\n    - <b>ward</b>: minimizza la varianza dei cluster che devono essere fusi insieme\n    - <b>average</b>: usa la media delle distanze di ogni osservazione nei due insiemi\n    - <b>complete</b>: usa la distanza massima tra due punti negli insiemi\n    - <b>single</b>: usa la distanza minima tra due insiemi"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\n\nac = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='complete')\nagglomerative_cluster_labels = ac.fit_predict(df_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assegnazione delle etichette a ciascun esempio presente nel DataFrame\nrfm['Agglomerative_Clustering'] = agglomerative_cluster_labels\n# Stampa dei primi 5 elementi presenti nel DataFrame\nrfm.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot delle Features\n\nfeatures_list = ['Amount', 'Frequency', 'Recency']\n\nfor feature in features_list:\n    sns.boxplot(x='Agglomerative_Clustering', y=feature, data=rfm)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DBSCAN\n\nDBSCAN è un algoritmo di clustering Density-Based utilizzabile su dataset che presentano punti rumorosi. È un algoritmo non parametrico di clustering basato sulla densità: dato un insieme di punti in uno spazio, raggruppa i punti che sono altamente vicini, contrassegnando come punti anomali i punti che si trovano da soli in regioni a bassa densità. \n\nDBSCAN è uno degli algoritmi di clustering più comuni e anche i più citati nella letteratura scientifica e presenta i seguenti vantaggi:\n- Non richiede la specifica a priori di un numero di cluster, a differenza di k-Means\n- Gestione accurata dei punti rumorosi\n- Robusto in presenza degli outliers\n\nPer DBSCAN, invece, si identificano i seguenti svantaggi:\n- Non deterministico: i punti presenti sulle frontiere possono essere assegnati a cluster differenti, in base all'ordine in cui i dati sono processati\n- La qualità dei risultati restituiti da DBSCAN dipende dalla misura di distanza usata\n- Sensibile al fenomeno della \"Curse of dimensionality\" in presenza di dataset con un numero di features elevato"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import DBSCAN\ndbscan = DBSCAN(metric='euclidean')\ndbscan.fit(df_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dbscan_labels = dbscan.labels_\n#Stampa delle etichette dei cluster\ndbscan_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assegnazione delle etichette a ciascun esempio presente nel DataFrame\nrfm['DensityBased_Labels'] = dbscan_labels\n# Stampa dei primi 5 elementi presenti nel DataFrame\nrfm.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot delle Features\n\nfeatures_list = ['Amount', 'Frequency', 'Recency']\n\nfor feature in features_list:\n    sns.boxplot(x='DensityBased_Labels', y=feature, data=rfm)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analisi dei risultati ottenuti"},{"metadata":{},"cell_type":"markdown","source":"Di seguito vengono riportate le interpretazioni dei risultati ottenuti dopo l'applicazione e i relativi cluster restituiti da ciascun algoritmo mostrato.\n\n<hr>\n\n**k-Means**, dopo aver definito un numero di cluster pari a 3, ha portato all'identificazione dei seguenti risultati:\n- I clienti con ClusterID=1 sono quei clienti che hanno un alto numero di transizioni se posti a confronto con gli altri clienti\n- I clienti con ClusterID=1 sono clienti che acquistano più frequentemente rispetto agli altri clienti\n- I clienti con ClusterID=2 sono clienti che non hanno acquistato recentemente e, pertanto, destano poco interesse dal punto di vista di business\n\n<hr>\n\n**Clustering Gerarchico**, dopo aver definito un numero di cluster pari a 3, ha portato all'identificazione dei seguenti risultati:\n- I clienti con Cluster_Labels=2 sono quei clienti che hanno un alto numero di transizioni se posti a confronto con gli altri clienti\n- I clienti con Cluster_Labels=2 sono clienti che acquistano più frequentemente rispetto agli altri clienti\n- I clienti con Cluster_Labels=0 sono clienti che non hanno acquistato recentemente e, pertanto, destano poco interesse dal punto di vista di business\n\n<hr>\n\n**DBSCAN** ha restituito ben 7 cluster che identificano la naturale suddivisione dei dati rispetto ai dati a disposizione, potendo vedere dai precedenti boxplot, le differenti distribuzioni per le tre features oggetto di studio."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}