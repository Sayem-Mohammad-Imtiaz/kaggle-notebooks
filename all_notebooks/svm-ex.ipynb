{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -U cvxopt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import print_function\nimport numpy as np \nimport matplotlib.pyplot as plt\nfrom scipy.spatial.distance import cdist\nfrom sklearn.svm import SVC\n\nfrom cvxopt import matrix, solvers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 0. Sample data"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(22)\n\nmeans = [[2, 2], [4, 2]]\ncov = [[.3, .2], [.2, .3]]\nN = 20\nX0 = np.random.multivariate_normal(means[0], cov, N) # class 1\nX1 = np.random.multivariate_normal(means[1], cov, N) # class -1 \nX = np.concatenate((X0.T, X1.T), axis = 1) # all data \ny = np.concatenate((np.ones((1, N)), -1*np.ones((1, N))), axis = 1) # labels ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(X0[:, 0], X0[:, 1], 'b^', markersize = 4, alpha = .8)\nplt.plot(X1[:, 0], X1[:, 1], 'go', markersize = 4, alpha = .8)\nplt.axis('equal')\nplt.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Solve SVM using optimization of dual function"},{"metadata":{"trusted":true},"cell_type":"code","source":"# build K\nV = np.concatenate((X0.T, -X1.T), axis = 1)\nK = matrix(V.T.dot(V)) # see definition of V, K near eq (8)\n\np = matrix(-np.ones((2*N, 1))) # all-one vector \n# build A, b, G, h \nG = matrix(-np.eye(2*N)) # for all lambda_n >= 0\nh = matrix(np.zeros((2*N, 1)))\nA = matrix(y) # the equality constrain is actually y^T lambda = 0\nb = matrix(np.zeros((1, 1))) \nsolvers.options['show_progress'] = False\nsol = solvers.qp(K, p, G, h, A, b)\n\nl = np.array(sol['x'])\nprint('lambda = ')\nprint(l.T)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epsilon = 1e-6 # just a small number, greater than 1e-9\nS = np.where(l > epsilon)[0]\n\nVS = V[:, S]\nXS = X[:, S]\nyS = y[:, S]\nlS = l[S]\n# calculate w and b\nw = VS.dot(lS)\nb = np.mean(yS.T - w.T.dot(XS))\n\nprint('w = ', w.T)\nprint('b = ', b)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# w1*x1 + w2*x2 + b = 0\nsepX1 = np.linspace(2.3, 3.3, 100)\nsepX2 = -b/w[1] - w[0]*sepX1/w[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(X0[:, 0], X0[:, 1], 'b^', markersize = 4, alpha = .8)\nplt.plot(X1[:, 0], X1[:, 1], 'go', markersize = 4, alpha = .8)\nplt.plot(sepX1, sepX2, '-r', label=\"%f*x1 + %f*x2 + %f = 0\"%(w[0], w[1], b))\nplt.axis('equal')\nplt.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Solve SVM using sklearn-svm, cvxopt and gradient descent for SVM soft margin"},{"metadata":{},"cell_type":"markdown","source":"#### Prepare data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# To support both python 2 and python 3\nfrom __future__ import division, print_function, unicode_literals\n# list of points \nimport numpy as np \nimport matplotlib.pyplot as plt\nfrom scipy.spatial.distance import cdist\nfrom matplotlib.backends.backend_pdf import PdfPages\nnp.random.seed(22)\n\nmeans = [[2, 2], [4, 2]]\ncov = [[.7, 0], [0, .7]]\nN = 20\nX0 = np.random.multivariate_normal(means[0], cov, N) # each row is a data point \nX1 = np.random.multivariate_normal(means[1], cov, N)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#with PdfPages('data.pdf') as pdf:\nplt.plot(X0[:, 0], X0[:, 1], 'bs', markersize = 8, alpha = 1)\nplt.plot(X1[:, 0], X1[:, 1], 'ro', markersize = 8, alpha = 1)\nplt.axis('equal')\nplt.ylim(0, 4)\nplt.xlim(0, 5)\n\n# hide tikcs \ncur_axes = plt.gca()\ncur_axes.axes.get_xaxis().set_ticks([])\ncur_axes.axes.get_yaxis().set_ticks([])\n\nplt.xlabel('$x_1$', fontsize = 20)\nplt.ylabel('$x_2$', fontsize = 20)\n    #pdf.savefig()\n    # plt.savefig('logistic_2d.png', bbox_inches='tight', dpi = 300)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.vstack((X0, X1))\ny = np.vstack((np.ones((N,1 )), -np.ones((N,1 )))).reshape((2*N,))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Using sklearn"},{"metadata":{"trusted":true},"cell_type":"code","source":"C = 100\nclf = SVC(kernel = 'linear', C = C)\nclf.fit(X, y) \n\nw_sklearn = clf.coef_.reshape(-1, 1)\nb_sklearn = clf.intercept_[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(w_sklearn.T, b_sklearn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Using duality problem"},{"metadata":{"trusted":true},"cell_type":"code","source":"from cvxopt import matrix, solvers\n# build K\nV = np.concatenate((X0.T, -X1.T), axis = 1)\nK = matrix(V.T.dot(V))\n\np = matrix(-np.ones((2*N, 1)))\n# build A, b, G, h \nG = matrix(np.vstack((-np.eye(2*N), np.eye(2*N))))\n\nh = matrix(np.vstack((np.zeros((2*N, 1)), C*np.ones((2*N, 1)))))\nA = matrix(y.reshape((-1, 2*N))) \nb = matrix(np.zeros((1, 1))) \nsolvers.options['show_progress'] = False\nsol = solvers.qp(K, p, G, h, A, b)\n\nl = np.array(sol['x'])\nprint('lambda = \\n', l.T)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"S = np.where(l > 1e-5)[0]\nS2 = np.where(l < .99*C)[0]\n\nM = [val for val in S if val in S2] # intersection of two lists","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XT = X.T # we need each col is one data point in this alg\nVS = V[:, S]\n# XS = XT[:, S]\n# yS = y[ S]\nlS = l[S]\n# lM = l[M]\nyM = y[M]\nXM = XT[:, M]\nw_dual = VS.dot(lS).reshape(-1, 1)\nb_dual = np.mean(yM.T - w_dual.T.dot(XM))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(w_dual.T, b_dual) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Using gradient descent"},{"metadata":{"trusted":true},"cell_type":"code","source":"def cost(w, lam):\n    u = w.T.dot(Z) # as in (23)\n    return (np.sum(np.maximum(0, 1 - u)) + \\\n            .5*lam*np.sum(w*w)) - .5*lam*w[-1]*w[-1]\n\ndef grad(w, lam):\n    u = w.T.dot(Z) # as in (23)\n    H = np.where(u < 1)[1]\n    ZS = Z[:, H]\n    g = (-np.sum(ZS, axis = 1, keepdims = True) + lam*w)\n    g[-1] -= lam*w[-1]\n    return g\n\neps = 1e-6\ndef num_grad(w):\n    g = np.zeros_like(w)\n    for i in range(len(w)):\n        wp = w.copy()\n        wm = w.copy()\n        wp[i] += eps \n        wm[i] -= eps \n        g[i] = (cost(wp, lam) - cost(wm, lam))/(2*eps)\n    return g ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def grad_descent(w0, eta, lam):\n    w = w0\n    it = 0 \n    while it < 100000:\n        it = it + 1\n        g = grad(w, lam)\n        w -= eta*g\n        if (it % 10000) == 1:\n            print('iter %d' %it + ' cost: %f' %cost(w, lam))\n        if np.linalg.norm(g) < 1e-5:\n            break \n    return w ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X0_bar = np.vstack((X0.T, np.ones((1, N)))) # extended data\nX1_bar = np.vstack((X1.T, np.ones((1, N)))) # extended data \n\nZ = np.hstack((X0_bar, - X1_bar)) # as in (22)\nlam = 1./C","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w0 = np.random.randn(X0_bar.shape[0], 1) \ng1 = grad(w0, lam)\ng2 = num_grad(w0)\ndiff = np.linalg.norm(g1 - g2)\nprint('Gradient difference: %f' %diff)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w0 = np.random.randn(X0_bar.shape[0], 1) \nw = grad_descent(w0, 0.001, lam)\nw_hinge = w[:-1].reshape(-1, 1)\nb_hinge = w[-1]\nprint(w_hinge.T, b_hinge)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotResult(X0, X1, w, b, title):\n    fig, ax = plt.subplots()\n\n    w0 = w[0]\n    w1 = w[1]\n    x1 = np.arange(-10, 10, 0.1)\n    y1 = -w0/w1*x1 - b/w1\n    y2 = -w0/w1*x1 - (b-1)/w1\n    y3 = -w0/w1*x1 - (b+1)/w1\n    plt.plot(x1, y1, 'k', linewidth = 3)\n    plt.plot(x1, y2, 'k')\n    plt.plot(x1, y3, 'k')\n\n    # equal axis and lim\n    plt.axis('equal')\n    plt.ylim(0, 3)\n    plt.xlim(2, 4)\n\n    # hide tikcs \n    cur_axes = plt.gca()\n    cur_axes.axes.get_xaxis().set_ticks([])\n    cur_axes.axes.get_yaxis().set_ticks([])\n\n    # fill two regions\n    y4 = 10*x1\n    plt.plot(x1, y1, 'k')\n    plt.fill_between(x1, y1, color='blue', alpha='0.1')\n    plt.fill_between(x1, y1, y4, color = 'red', alpha = '.1')\n\n    plt.xlabel('$x_1$', fontsize=12)\n    plt.ylabel('$x_2$', fontsize=12)\n    plt.title('Solution found by ' + title, fontsize=12)\n\n    plt.plot(X0[:, 0], X0[:, 1], 'bs', markersize = 8, alpha = .8)\n    plt.plot(X1[:, 0], X1[:, 1], 'ro', markersize = 8, alpha = .8)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotResult(X0, X1, w_sklearn, b_sklearn, 'sklearn')\nplotResult(X0, X1, w_dual, b_dual, 'dual')\nplotResult(X0, X1, w_hinge, b_hinge, 'hinge')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Change C\nlsC = [1e-2, 1, 10, 1000]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"X0_bar = np.vstack((X0.T, np.ones((1, N)))) # extended data\nX1_bar = np.vstack((X1.T, np.ones((1, N)))) # extended data \n\nZ = np.hstack((X0_bar, - X1_bar)) # as in (22)\nfor C in lsC:\n    lam = 1./C\n    w0 = np.random.randn(X0_bar.shape[0], 1) \n    w = grad_descent(w0, 0.001, lam)\n    w_hinge = w[:-1].reshape(-1, 1)\n    b_hinge = w[-1]\n    print(w_hinge.T, b_hinge)\n    plotResult(X0, X1, w_hinge, b_hinge, 'hinge')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Kernel function demo"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\n\n# XOR dataset and targets\nX = np.c_[(0, 0),\n          (1, 1),\n          #---\n          (1, 0),\n          (0, 1)].T\nY = [0] * 2 + [1] * 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# figure number\nfignum = 1\n\n# fit the model\nfor kernel in ('sigmoid', 'poly', 'rbf'):\n    clf = svm.SVC(kernel=kernel, gamma=4, coef0 = 0)\n    clf.fit(X, Y)\n    # plot the line, the points, and the nearest vectors to the plane\n    fig, ax = plt.subplots()\n    plt.figure(fignum, figsize=(4, 3))\n    plt.clf()\n\n    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,\n                facecolors='None')\n    plt.plot(X[:2, 0], X[:2, 1], 'ro', markersize = 8)\n    plt.plot(X[2:, 0], X[2:, 1], 'bs', markersize = 8)\n\n    plt.axis('tight')\n    x_min, x_max = -2, 3\n    y_min, y_max = -2, 3\n\n    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n    Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(XX.shape)\n    plt.figure(fignum, figsize=(4, 3))\n    CS = plt.contourf(XX, YY, np.sign(Z), 200, cmap='jet', alpha = .2)\n    plt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'],\n                levels=[-.5, 0, .5])\n    plt.title(kernel, fontsize = 15)\n    plt.xlim(x_min, x_max)\n    plt.ylim(y_min, y_max)\n\n    plt.xticks(())\n    plt.yticks(())\n    fignum = fignum + 1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. SVM applications"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import confusion_matrix \nfrom sklearn import datasets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Iris flowers classification using svm\n!ls ../input/iris-flower-dataset\nfilename = \"../input/iris-flower-dataset/IRIS.csv\"\npdfData = pd.read_csv(filename)\npdfData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pdfData.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pdfData.hist(bins=50, figsize=(20,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainSet, testSet = train_test_split(pdfData, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lsLabel = set(pdfData[\"species\"])\nprint(lsLabel)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for l in lsLabel:\n    pdfData[\"label_%s\"%l[5:]] = (pdfData[\"species\"] == l)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Look for correlation\ncorrMatrix = pdfData.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corrMatrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for l in lsLabel:\n    print(corrMatrix[\"label_%s\"%l[5:]].sort_values(ascending=False))\n    print(\"-\"*30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lsFt = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\nprint(trainSet[lsFt][trainSet[lsFt].isnull()])\ntraining = trainSet[lsFt].dropna().values\ntrainingLabel = trainSet[\"species\"].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = testSet[lsFt].dropna().values\ntestLabel = testSet[\"species\"].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = {}\npredictions = {}\nlsKernel = ('linear', 'poly', 'rbf')\nfor kernel in lsKernel:\n    clf[kernel] = svm.SVC(kernel=kernel, gamma=4, coef0 = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lsLabel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for kernel in lsKernel:\n    print(kernel)\n    labels = list(lsLabel)\n    clf[kernel].fit(training, trainingLabel)\n    \n    predictions[kernel] = clf[kernel].predict(test) \n\n    # model accuracy for X_test   \n    accuracy = clf[kernel].score(test, testLabel) \n    print(accuracy)\n\n    # creating a confusion matrix \n    cm = confusion_matrix(testLabel, predictions[kernel], labels) \n    print(cm)\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(cm)\n    # plt.title('Confusion matrix')\n    fig.colorbar(cax)\n    ax.set_xticklabels([''] + labels)\n    ax.set_yticklabels([''] + labels)\n    \n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.show()\n    print(\"-\"*30)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}