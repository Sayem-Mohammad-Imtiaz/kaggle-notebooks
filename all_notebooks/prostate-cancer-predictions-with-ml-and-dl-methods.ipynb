{"cells":[{"metadata":{"_uuid":"a679995615a32bdd09f5213d70b25820aff0f2e5"},"cell_type":"markdown","source":"**INTRODUCTION**\n\nHello! I'll train Prostate Cancer datas with some machine learning and deep learning methods. I'm believe in this course will learn a lot of thing to us. You'll see this kernel:\n\n* EDA (Exploratory Data Analysis)\n* Data Preprocessing (Scaling, Reshaping)\n* Test-Train Datas Split\n* Logistic Regression Classification\n* KNN Classification\n* Support Vector Machine (SVM) Classification\n* Naive Bayes Classification\n* Desicion Tree Classification\n* Random Forest Classification\n* Artificial Neural Network\n* Recurrent Neural Network\n* Compare all of these Classification Models\n* Conclusion"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import confusion_matrix # We'll use a lot of times it!\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"Cancer = pd.read_csv('../input/Prostate_Cancer.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e5558aa7e42975c3cb3bb5be8fcdf63d2b8d75e"},"cell_type":"code","source":"Cancer.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73f8fc7a937835b320251278256f0d26d79706b2"},"cell_type":"code","source":"Cancer.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f605aa194d543d468d79491735e5f66581839fd"},"cell_type":"code","source":"Cancer.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0dea2bf6bf39c45d21f4528fb0981c579899a63d"},"cell_type":"code","source":"Cancer.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc63d2953223deb5d2a369185f98f1235f5a4489"},"cell_type":"code","source":"Cancer.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f24b49bec7a5c0498d0d1687951532eb2802928"},"cell_type":"code","source":"# We don't care id of the columns. So, we drop that!\nCancer.drop(['id'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fea789ad3a09ca6599247903c7e15a2f8e32a253"},"cell_type":"code","source":"Cancer.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f05f47ec1176fe84f7651fbd472dc81dc66143a1"},"cell_type":"code","source":"# diagnosis_result is the most important column for us. Because we'll classify datas depend on this column.\n# We have to integers for classification. Therefore, we must convert them from object to integer.\nCancer.diagnosis_result = [1 if each == 'M' else 0 for each in Cancer.diagnosis_result]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87b9aaf92ea781bb739f752dfc50a509ce1c013b"},"cell_type":"code","source":"# Let's check it.\nCancer.diagnosis_result.value_counts()\n# And then, we assigned 1 and 0 to M and B. Let's some classification!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21161b74913379540eba9a143ba7529200e296bb"},"cell_type":"code","source":"# We should assign x and y values for test-train datas split.\ny = Cancer.diagnosis_result.values\nx_data = Cancer.drop(['diagnosis_result'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d040dc3e591b2a30996e614f709980719506e9e"},"cell_type":"code","source":"# See our values\ny","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17f12e91c50682da8a14a5448517586cfd94f903"},"cell_type":"code","source":"x_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"732d8ab954f68af7b0d89eec4fa6789dcdd2d8e6"},"cell_type":"code","source":"# Normalization: Normalization means all of the values of data, scale between 0 and 1.\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0,1))\nx = scaler.fit_transform(x_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76764d713cb7aa2c9bd0310d471b539f455fe4d5"},"cell_type":"code","source":"x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63a63a87d3ce57ae5eba95acacca1b5be355b7f2"},"cell_type":"code","source":"# We are ready to split datas as train and test.\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n#%40 data will assign as 'Test Datas'\nmethod_names=[] # In Conclusion part, I'll try to show you which method gave the best result.\nmethod_scores=[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"753ff7cdc84f6d495b67a5539f04ea8937e5d780"},"cell_type":"code","source":"# Let's look at new values.\nx_train","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c4637886423e06e40c77bc00a5b88f027572c1f"},"cell_type":"markdown","source":"**And now time to classification!**\n"},{"metadata":{"trusted":true,"_uuid":"949e0fbc546a778ac9fb85ad065457009d38e27c"},"cell_type":"code","source":"# Firstly, we start with Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression()\nlog_reg.fit(x_train, y_train) #Fitting\nprint(\"Logistic Regression Classification Test Accuracy {}\".format(log_reg.score(x_test,y_test)))\nmethod_names.append(\"Logistic Reg.\")\nmethod_scores.append(log_reg.score(x_test,y_test))\n\n#Confusion Matrix\ny_pred = log_reg.predict(x_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6eb8470d8b5b7e5c44dcc63cd786c326f5be83fc"},"cell_type":"code","source":"# Continue with; KNN Classification!\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=5)  # 5 is optional.\nknn.fit(x_train,y_train)\nprint(\"Score for Number of Neighbors = 5: {}\".format(knn.score(x_test,y_test)))\nmethod_names.append(\"KNN\")\nmethod_scores.append(knn.score(x_test,y_test))\n\n#Confusion Matrix\ny_pred = knn.predict(x_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea5a44d0281f4f34a77c066f0ed12bac885d35cf"},"cell_type":"code","source":"# SVM!\nfrom sklearn.svm import SVC\nsvm = SVC(random_state=42)\nsvm.fit(x_train,y_train)\nprint(\"SVM Classification Score is: {}\".format(svm.score(x_test,y_test)))\nmethod_names.append(\"SVM\")\nmethod_scores.append(svm.score(x_test,y_test))\n\n#Confusion Matrix\ny_pred = svm.predict(x_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19b6321c1b5a18a045dc4e1636e2b368c082769d"},"cell_type":"code","source":"# Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nnaive_bayes = GaussianNB()\nnaive_bayes.fit(x_test,y_test)\nprint(\"Naive Bayes Classification Score: {}\".format(naive_bayes.score(x_test,y_test)))\nmethod_names.append(\"Naive Bayes\")\nmethod_scores.append(naive_bayes.score(x_test,y_test))\n\n#Confusion Matrix\ny_pred = naive_bayes.predict(x_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51faaac7300e99421c31e7fd635ee977392caa73"},"cell_type":"code","source":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\ndec_tree = DecisionTreeClassifier()\ndec_tree.fit(x_train,y_train)\nprint(\"Decision Tree Classification Score: \",dec_tree.score(x_test,y_test))\nmethod_names.append(\"Decision Tree\")\nmethod_scores.append(dec_tree.score(x_test,y_test))\n\n#Confusion Matrix\ny_pred = dec_tree.predict(x_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ad7376894ea8ffa78e4765b778993007a387e70"},"cell_type":"code","source":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrand_forest = RandomForestClassifier(n_estimators=100, random_state=42)\nrand_forest.fit(x_train,y_train)\nprint(\"Random Forest Classification Score: \",rand_forest.score(x_test,y_test))\nmethod_names.append(\"Random Forest\")\nmethod_scores.append(rand_forest.score(x_test,y_test))\n\n#Confusion Matrix\ny_pred = rand_forest.predict(x_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d93408822f4b3d162ec6ad75cbaaafbebf663692"},"cell_type":"code","source":"# ANN!\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential # initialize neural network library\nfrom keras.layers import Dense # build our layers library\ndef build_classifier():\n    classifier = Sequential() # initialize neural network\n    classifier.add(Dense(units = 50, kernel_initializer = 'uniform', activation = 'relu', input_dim = x_train.shape[1]))\n    classifier.add(Dense(units = 10, kernel_initializer = 'uniform', activation = 'relu'))\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return classifier\nclassifier = KerasClassifier(build_fn = build_classifier, epochs = 200)\naccuracies = cross_val_score(estimator = classifier, X = x_train, y = y_train, cv = 3)\nmean = accuracies.mean()\nvariance = accuracies.std()\nprint(\"Accuracy mean: \"+ str(mean))\nprint(\"Accuracy variance: \"+ str(variance))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b708053280ccbc39ca8dd9f8a52aebb4355c0d19"},"cell_type":"code","source":"method_names.append(\"ANN\")\nmethod_scores.append(0.851)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a68eec281f6f88ff2548e94935305d9d904b47a"},"cell_type":"code","source":"trainX = np.reshape(x_train, (x_train.shape[0], x_train.shape[1],1))\ntestX = np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1))\n# Print and check shapes\nprint(\"Shape of trainX is {}\".format(trainX.shape))\nprint(\"Shape of testX is {}\".format(testX.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd120115b813ce73a5667d7dbe3123965e799cf4"},"cell_type":"code","source":"from keras.layers import Dense, SimpleRNN, Dropout\nfrom keras.metrics import mean_squared_error\nfrom keras.models import Sequential\nmodel = Sequential()\n# Add the first layer and Dropout regularization\nmodel.add(SimpleRNN(units=100,activation='tanh',return_sequences=True, \n                    input_shape=(trainX.shape[1],1)))\nmodel.add(Dropout(0.20))\n# Second layer and Dropout regularization\nmodel.add(SimpleRNN(units = 100, activation='tanh',return_sequences=True))\nmodel.add(Dropout(0.20))\n# Third layer and Dropout regularization\nmodel.add(SimpleRNN(units = 70, activation='tanh', return_sequences= True))\nmodel.add(Dropout(0.20))\n# Fourth layer and Dropout regularization\nmodel.add(SimpleRNN(units = 50))\nmodel.add(Dropout(0.20))\n# Add final or output layer\nmodel.add(Dense(units=1))\n\n# Compile our RNN model\nmodel.compile(optimizer = 'adam', loss = 'mean_squared_error',metrics = ['accuracy'])\n# Fitting the RNN to the training set\nmodel.fit(trainX, y_train, epochs = 200, batch_size=32)\n# Remember; epochs, batch_size etc. are just some of hyper parameters. \n# You can change these parameters whatever you want\nmean = accuracies.mean()\nvariance = accuracies.std()\nprint(\"Accuracy mean: \"+ str(mean))\nprint(\"Accuracy variance: \"+ str(variance))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b90913a8a1a728d6f6d500ac683417af7b6a370"},"cell_type":"code","source":"method_names.append(\"RNN\")\nmethod_scores.append(0.887)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89fae4f677c7d6638e95f040fea61aae7e3493f8"},"cell_type":"markdown","source":"**CONCLUSION**\n\nWe've already completed to train our data with a lot of different method. Let's look which method is given the best result to us!\n"},{"metadata":{"trusted":true,"_uuid":"af35e768076f7018e68d2d191ffb12e64d7fef06"},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nplt.ylim([0.60,0.90])\nplt.bar(method_names,method_scores,width=0.5)\nplt.xlabel('Method Name')\nplt.ylabel('Method Score')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7174b5604b31c95b8d6fc16205719b3cce83e02"},"cell_type":"markdown","source":"As we can see easily; RNN gave us the best result! I hope you learned something like I did. Please comment me!"},{"metadata":{"trusted":true,"_uuid":"0ded5dd1b51582fdb1027bd24ca3ca6fb74c0b45"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}