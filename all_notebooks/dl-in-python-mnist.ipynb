{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# More Imports\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.datasets import mnist\nfrom keras.utils import to_categorical\n\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First let's start out be reading the data:","metadata":{}},{"cell_type":"code","source":"tr = pd.read_csv('../input/mnist-in-csv/mnist_train.csv')\ntt = pd.read_csv('../input/mnist-in-csv/mnist_test.csv')\n\nX = tr.copy()\ny = tr.pop('label')\n\n# Coerce to float and normalize\nX = X.drop(['label'], axis=1).astype('float64') / 256\n\ntest = tt.copy()\ntest_y = to_categorical(tt.pop('label'))\ntest_X = tt.astype('float64') / 256","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we import necessary things from Keras:","metadata":{}},{"cell_type":"markdown","source":"Now let's see that the data actually makes sense. Do we need to do anything funky to it?","metadata":{}},{"cell_type":"code","source":"#(X_train, y_train), (X_valid, y_valid) = mnist.load_data()\nX_train, X_valid, y_train, y_valid = train_test_split(X, y,\n                                                     test_size=0.15,\n                                                     random_state=0x1337)\n\n\ny_train = to_categorical(y_train)\ny_valid = to_categorical(y_valid)\n\nfig = plt.figure\n\ndef show_image(row: pd.Series) -> None:\n    image = row.to_numpy().reshape((28,28))\n    \n    plt.imshow(image, cmap='gray')\n\nshow_image(X_train.iloc[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's make a neural network","metadata":{}},{"cell_type":"code","source":"model = Sequential([\n    Dense(512, activation='relu', input_shape=[28*28,]),\n    Dense(10, activation='softmax'),\n])\n\nmodel.compile(\n    optimizer='rmsprop',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we do the training and see what we get:","metadata":{}},{"cell_type":"code","source":"history = model.fit(X_train, y_train,\n                    epochs=5,\n                    batch_size=128,\n                    validation_data=(X_valid, y_valid))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hist = pd.DataFrame(history.history)\nhist[['loss', 'accuracy', 'val_accuracy']].plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's see how our model performs:","metadata":{}},{"cell_type":"code","source":"loss, accuracy = model.evaluate(test_X, test_y)\nprint(f'\\nmodel achieved {loss*100:.2f}% loss with {accuracy*100:.2f}% accuracy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}