{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nfrom sklearn.utils import shuffle\nimport pandas as pd\nimport pickle\nfrom matplotlib.pyplot import MultipleLocator","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 来源：An Estimation Method of Continuous Non-Invasive Arterial Blood Pressure Waveform Using Photoplethysmography: A U-Net Architecture-Based Approach\n# 作者: Athaya\n# 在自己的数据集上复现其模型","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\ntf.config.experimental_connect_to_cluster(resolver)\n# This is the TPU initialization code that has to be at the beginning.\ntf.tpu.experimental.initialize_tpu_system(resolver)\nprint(\"All devices: \", tf.config.list_logical_devices('TPU'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"strategy = tf.distribute.TPUStrategy(resolver)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(r'../input/blood-pressure-datasets/Train_Merge_Data.csv')\nvalidation_data = pd.read_csv(r'../input/blood-pressure-datasets/Validation_Merge_Data.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = shuffle(train_data)\nvalidation_data = shuffle(validation_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_label_Sbp = train_data.iloc[:,610]\ntrain_label_Sbp = train_label_Sbp.values\n\ntrain_label_Dbp = train_data.iloc[:,611]\ntrain_label_Dbp = train_label_Dbp.values\n\ntrain_data = train_data.iloc[:,:600]\ntrain_data = train_data.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_label_Sbp = validation_data.iloc[:,610]\nvalidation_label_Sbp = validation_label_Sbp.values\n\nvalidation_label_Dbp = validation_data.iloc[:,611]\nvalidation_label_Dbp = validation_label_Dbp.values\n\nvalidation_data = validation_data.iloc[:,:600]\nvalidation_data = validation_data.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"train_data_information:\")\nprint(train_data.shape)\nprint(train_label_Sbp.shape)\nprint(train_label_Dbp.shape)\nprint(\"validation_data_information:\")\nprint(validation_data.shape)\nprint(validation_label_Sbp.shape)\nprint(validation_label_Dbp.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=32\ndef create_model():\n    inputs = keras.Input(shape=(600,1))\n\n    conv1 = layers.Conv1D(x*2,3,padding='same')(inputs)\n    conv1 = layers.BatchNormalization()(conv1)\n    conv1 = layers.Activation(tf.nn.relu)(conv1)\n\n    conv1 = layers.Conv1D(x*2,3,padding='same')(conv1)\n    conv1 = layers.BatchNormalization()(conv1)\n    conv1 = layers.Activation(tf.nn.relu)(conv1)\n\n    pool1 = layers.MaxPooling1D(pool_size=2)(conv1)\n    \n    conv2 = layers.Conv1D(x*4,3,padding='same')(pool1)\n    conv2 = layers.BatchNormalization()(conv2)\n    conv2 = layers.Activation(tf.nn.relu)(conv2)\n\n    conv2 = layers.Conv1D(x*4,3,padding='same')(conv2)\n    conv2 = layers.BatchNormalization()(conv2)\n    conv2 = layers.Activation(tf.nn.relu)(conv2)\n\n    pool2 = layers.MaxPooling1D(pool_size=2)(conv2)\n    \n    \n    conv3 = layers.Conv1D(x*8,3,padding='same')(pool2)\n    conv3 = layers.BatchNormalization()(conv3)\n    conv3 = layers.Activation(tf.nn.relu)(conv3)\n\n    conv3 = layers.Conv1D(x*8,3,padding='same')(conv3)\n    conv3 = layers.BatchNormalization()(conv3)\n    conv3 = layers.Activation(tf.nn.relu)(conv3)\n\n    pool3 = layers.MaxPooling1D(pool_size=2)(conv3)\n    \n    \n    conv4 = layers.Conv1D(x*16,3,padding='same')(pool3)\n    conv4 = layers.BatchNormalization()(conv4)\n    conv4 = layers.Activation(tf.nn.relu)(conv4)\n\n    conv4 = layers.Conv1D(x*16,3,padding='same')(conv4)\n    conv4 = layers.BatchNormalization()(conv4)\n    conv4 = layers.Activation(tf.nn.relu)(conv4)\n    \n    conv4 = layers.Dropout(0.5)(conv4)\n    ####\n    pool4 = layers.MaxPooling1D(pool_size=2)(conv4)\n    \n\n    \n    conv5 = layers.Conv1D(x*32,3,padding='same')(pool4)\n    conv5 = layers.BatchNormalization()(conv5)\n    conv5 = layers.Activation(tf.nn.relu)(conv5)\n\n    conv5 = layers.Conv1D(x*32,3,padding='same')(conv5)\n    conv5 = layers.BatchNormalization()(conv5)\n    conv5 = layers.Activation(tf.nn.relu)(conv5)\n    \n    conv5 = layers.Dropout(0.5)(conv5)\n    \n    \n    ####\n    conv5 = layers.UpSampling1D(size=2)(conv5)\n    conv5 = layers.ZeroPadding1D((0,1))(conv5)\n    \n    conv5 = layers.Conv1D(x*16,2,padding='same')(conv5)\n    \n    up6 = layers.Concatenate(axis=2)([conv5,conv4])\n    \n    conv6 = layers.Conv1D(x*16,3,padding='same')(up6)\n    conv6 = layers.BatchNormalization()(conv6)\n    conv6 = layers.Activation(tf.nn.relu)(conv6)\n\n    conv6 = layers.Conv1D(x*16,3,padding='same')(conv6)\n    conv6 = layers.BatchNormalization()(conv6)\n    conv6 = layers.Activation(tf.nn.relu)(conv6)\n    \n    ####\n    conv6 = layers.UpSampling1D(size=2)(conv6)\n    \n    conv6 = layers.Conv1D(x*8,2,padding='same')(conv6)\n    conv6 = layers.BatchNormalization()(conv6)\n    conv6 = layers.Activation(tf.nn.relu)(conv6)\n    \n    up7 = layers.Concatenate(axis=2)([conv6,conv3])\n\n    conv7 = layers.Conv1D(x*8,3,padding='same')(up7)\n    conv7 = layers.BatchNormalization()(conv7)\n    conv7 = layers.Activation(tf.nn.relu)(conv7)\n\n    conv7 = layers.Conv1D(x*8,3,padding='same')(conv7)\n    conv7 = layers.BatchNormalization()(conv7)\n    conv7 = layers.Activation(tf.nn.relu)(conv7)\n    \n    ####\n    \n    conv7 = layers.UpSampling1D(size=2)(conv7)\n    \n    conv7 = layers.Conv1D(x*4,2,padding='same')(conv7)\n    conv7 = layers.BatchNormalization()(conv7)\n    conv7 = layers.Activation(tf.nn.relu)(conv7)\n    \n    up8 = layers.Concatenate(axis=2)([conv7,conv2])\n\n    conv8 = layers.Conv1D(x*4,3,padding='same')(up8)\n    conv8 = layers.BatchNormalization()(conv8)\n    conv8 = layers.Activation(tf.nn.relu)(conv8)\n\n    conv8 = layers.Conv1D(x*4,3,padding='same')(conv8)\n    conv8 = layers.BatchNormalization()(conv8)\n    conv8 = layers.Activation(tf.nn.relu)(conv8)\n    \n    ####\n    \n    conv8 = layers.UpSampling1D(size=2)(conv8)\n    \n    conv8 = layers.Conv1D(x*2,2,padding='same')(conv8)\n    conv8 = layers.BatchNormalization()(conv8)\n    conv8 = layers.Activation(tf.nn.relu)(conv8)\n    \n\n    up9 = layers.Concatenate(axis=2)([conv8,conv1])\n\n    conv9 = layers.Conv1D(x*2,3,padding='same')(up9)\n    conv9 = layers.BatchNormalization()(conv9)\n    conv9 = layers.Activation(tf.nn.relu)(conv9)\n\n    conv9 = layers.Conv1D(x*2,3,padding='same')(conv9)\n    conv9 = layers.BatchNormalization()(conv9)\n    conv9 = layers.Activation(tf.nn.relu)(conv9)\n\n\n    com_layer = layers.GlobalAveragePooling1D()(conv9)\n    \n    com_layer = layers.Dense(32,activation='relu',kernel_regularizer=tf.keras.regularizers.L2(0.4))(com_layer)\n    \n    outputs_sbp = layers.Dense(1,name='Sbp')(com_layer)\n    outputs_dbp = layers.Dense(1,name='Dbp')(com_layer)\n\n    model = keras.Model(inputs=inputs,outputs=[outputs_sbp,outputs_dbp])\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"自定义评价指标模块\"\"\"\ndef standard_deviation(y_true, y_pred):\n    u = keras.backend.mean(y_pred-y_true)\n    return keras.backend.sqrt(keras.backend.mean(keras.backend.square((y_pred-y_true) - u)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"回调函数\"\"\"\n#保存迭代周期内最好的模型\ncheckpoint_filepath = r'./model_struction.h5'\nSave_epochs = 10 #迭代多少层保存一次模型\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    # save_weights_only=True,\n    monitor='val_Sbp_mean_absolute_error',\n    mode='min',\n    save_best_only=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    model = create_model()\n    \n    model.compile(loss={'Sbp':\"mse\",'Dbp':\"mse\"}, optimizer=keras.optimizers.Adam(lr=0.0001),metrics=[tf.keras.metrics.MeanAbsoluteError(),standard_deviation])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"保存模型结构图片\"\"\"\ntf.keras.utils.plot_model(model, to_file=r'./model_graph.png', show_shapes=True, show_layer_names=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_data,{'Sbp':train_label_Sbp,'Dbp':train_label_Dbp},\n                    batch_size=128*8,\n                    epochs=500,\n                    callbacks=model_checkpoint_callback,\n                    validation_data=(validation_data,{'Sbp':validation_label_Sbp,'Dbp':validation_label_Dbp})\n                    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(r'./last_model.h5.pickle', 'wb') as file_pi:\n \tpickle.dump(history.history, file_pi)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}