{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Kensho Derived Wikimedia Dataset - Hugging Face Language Model from Scratch\n\nbased on this colab notebook https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from collections import Counter\nimport json\nimport os\nfrom pprint import pprint\nimport string\nimport time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport spacy\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom unidecode import unidecode\n\nsns.set()\nsns.set_context('talk')\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split corpus file\n!split -l 100000 -d /kaggle/input/kdwd-make-sentences/wikipedia_intros_sentences.txt wikipedia_intros_sentences_chunks_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -lh /kaggle/*/*","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip uninstall -y tensorflow\n!pip install transformers==2.5.1\n!pip install tokenizers==0.5.2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tokenizers import ByteLevelBPETokenizer\nfrom tokenizers.processors import BertProcessing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"VOCAB_SIZE = 50_000\nMAX_FREQUENCY = 2\nMAX_LENGTH = 512\nWORK_DIR = 'WikipediaRoberta'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir {WORK_DIR}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -lh /kaggle/*/*","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus_fname = '/kaggle/input/kdwd-make-sentences/wikipedia_intros_sentences.txt'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!head /kaggle/input/kdwd-make-sentences/wikipedia_intros_sentences.txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = ByteLevelBPETokenizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.train(\n    files=[corpus_fname], \n    vocab_size=VOCAB_SIZE, \n    min_frequency=MAX_FREQUENCY, \n    special_tokens=[\n        \"<s>\",\n        \"<pad>\",\n        \"</s>\",\n        \"<unk>\",\n        \"<mask>\",\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.save(WORK_DIR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -lh /kaggle/*/*","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded = tokenizer.encode('I am a sentence.')\nprint(encoded.ids)\nprint(encoded.tokens)\nprint(encoded.type_ids)\nprint(encoded.offsets)\nprint(encoded.attention_mask)\nprint(encoded.special_tokens_mask)\nprint(encoded.overflowing)\nprint(encoded.original_str)\nprint(encoded.normalized_str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = json.load(open(os.path.join(WORK_DIR, 'vocab.json'), 'r'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(vocab.keys())[0:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!head {WORK_DIR}/merges.txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = ByteLevelBPETokenizer(\n    f\"{WORK_DIR}/vocab.json\",\n    f\"{WORK_DIR}/merges.txt\",\n)\ntokenizer._tokenizer.post_processor = BertProcessing(\n    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n)\ntokenizer.enable_truncation(max_length=MAX_LENGTH)\n\nencoded = tokenizer.encode(unidecode(\"My name is Gabriel.\"))\nprint(encoded)\nprint(encoded.tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\ntorch.cuda.is_available()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget -c https://raw.githubusercontent.com/huggingface/transformers/v2.5.1/examples/run_language_modeling.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -lh /kaggle/*/*","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\nconfig = {\n    \"architectures\": [\"RobertaForMaskedLM\"],\n    \"attention_probs_dropout_prob\": 0.1,\n    \"hidden_act\": \"gelu\",\n    \"hidden_dropout_prob\": 0.1,\n    \"hidden_size\": 768,\n    \"initializer_range\": 0.02,\n    \"intermediate_size\": 3072,\n    \"layer_norm_eps\": 1e-05,\n    \"max_position_embeddings\": 514,\n    \"model_type\": \"roberta\",\n    \"num_attention_heads\": 12,\n    \"num_hidden_layers\": 6,\n    \"type_vocab_size\": 1,\n    \"vocab_size\": VOCAB_SIZE\n}\nwith open(os.path.join(WORK_DIR, 'config.json'), 'w') as fp:\n    json.dump(config, fp)\n\ntokenizer_config = {\n    \"max_len\": MAX_LENGTH\n}\nwith open(os.path.join(WORK_DIR, 'tokenizer_config.json'), 'w') as fp:\n    json.dump(tokenizer_config, fp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -lh /kaggle/*/*","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cmd = \"\"\"\n  python run_language_modeling.py\n  --train_data_file {}\n  --output_dir {}\n  --model_type roberta\n  --mlm\n  --config_name {}\n  --tokenizer_name {}\n  --do_train\n  --line_by_line\n  --learning_rate 2.5e-5\n  --num_train_epochs 1\n  --save_total_limit 2\n  --save_steps 2000\n  --per_gpu_train_batch_size 4\n  --seed 42\n\"\"\".replace(\"\\n\", \" \").format(\n    '/kaggle/working/wikipedia_intros_sentences_chunks_00', \n    WORK_DIR + '-small-v1', \n    WORK_DIR, \n    WORK_DIR\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -rf WikipediaRoberta-small-v1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n!{cmd}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -lh /kaggle/*/*","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import pipeline\n\nfill_mask = pipeline(\n    \"fill-mask\",\n    model = WORK_DIR + '-small-v1',\n    tokenizer = WORK_DIR + '-small-v1'\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = fill_mask(\"The sun <mask>.\")\nresult","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = fill_mask(\"This is the beginning of a beautiful <mask>.\")\nresult","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = fill_mask(\"Playing music is <mask> for your ears.\")\nresult","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}