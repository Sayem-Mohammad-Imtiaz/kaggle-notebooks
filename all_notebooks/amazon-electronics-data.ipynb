{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom surprise import Dataset,Reader\nfrom surprise.model_selection import train_test_split\n\nfrom surprise import KNNWithMeans\nfrom surprise import accuracy\n\nfrom surprise import SVD\nfrom surprise import accuracy\n\nfrom sklearn.model_selection import train_test_split as tt_split\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.listdir('../input/amazon-product-reviews')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_data=pd.read_csv('../input/amazon-product-reviews/ratings_Electronics (1).csv',names=['CustomerID','ItemID','Rating','Timestamp'])\ninput_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_data['CustomerID'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_data['ItemID'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Since the input data has more than 7 million records, To avoid processing difficulties we filter data which only contains the customers who have given ratings more than 50 times.\n\nSince we are providing the recommendation of the Products to the customers, it is better to remove data based on the customerID rather itemID."},{"metadata":{"trusted":true},"cell_type":"code","source":"data=input_data.groupby('CustomerID').filter(lambda x : len(x) > 100)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"We have reduced the records from 7 million to 43k.\n\nSince it requres more compute capacity for KNNMeans algorithm in the later part, we reduced it to very less records.\n\nSince we have removed many data, the model wont be much effective that the original one.\n\nSince we are not using this as production ready model and also the processing capacity is low, it is acceptable to do this."},{"metadata":{},"cell_type":"raw","source":"data.to_csv('data.csv')"},{"metadata":{},"cell_type":"raw","source":"data=pd.read_csv('data.csv')\ndata.drop(columns='Unnamed: 0',inplace=True)"},{"metadata":{},"cell_type":"markdown","source":"### Univariant Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['Rating'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.reset_index(inplace=True)\ndata.drop(columns=['index'],inplace=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Since the timestamp column is not needed, we can drop it","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(columns='Timestamp',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Unique items in DF\nprint(\"The no of unique items in the data is\", len(data['ItemID'].unique()))\n\n#Unique Customers in DF\nprint(\"The no of unique customers in the data is\", len(data['CustomerID'].unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Active customers - Those who given more no of ratings\ndata['CustomerID'].value_counts().head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Transforming data to SURPRISE format"},{"metadata":{"trusted":true},"cell_type":"code","source":"reader = Reader(rating_scale=(1, 5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"surp_data=Dataset.load_from_df(data[['CustomerID','ItemID','Rating']],reader)\nsurp_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"We see above that the dataset here is a object and not a DF"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainset,testset =train_test_split(surp_data, test_size=0.3,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(type(testset))\nprint(type(trainset))\n\n#Raw ids are normal data. the Raw ids are mapped to inner ids\n#trainset contain these inner ids","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"About trainset:\n    \n    1) Trainset is no longer a pandas dataframe. Rather, it's a specific datatypes defined by the Surprise library\n    2) CustomerID and ItemId in the pandas dataframe can contain any value (either string/integer etc). However, Trainset convert these raw ids into numeric indexes called as \"inner id\"\n    3) Methods are provided to convert rw id to inner id and vice verca.\n    \nhttps://surprise.readthedocs.io/en/stable/trainset.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"#From the above link we see that .ur represent user ratings\n\nuser_ratings=trainset.ur\ntype(user_ratings)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for keys in user_ratings.keys():\n    print(keys)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#For first user\n#These are inner ids\n\nuser_ratings[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# However the ids are the inner ids and not the raw ids\n# raw ids can be obatined as follows\n\n#Convert a user inner id to a raw id.\nprint(trainset.to_raw_uid(0))\n\n#Convert an item inner id to a raw id.\nprint(trainset.to_raw_iid(0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training and Performance Testing of Collaborative Filtering "},{"metadata":{},"cell_type":"markdown","source":"##### Using KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_model = KNNWithMeans(k=10,sim_options={'name':'cosine' , 'user_based':False})\nknn_model.fit(trainset)\n\n#Item Item similarity matrix has been created now","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(testset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluation on testset\n\ntest_pred_knn=knn_model.test(testset)\n\n# compute RMSE\naccuracy.rmse(test_pred_knn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets predict for 11th one\n\ntest_pred_knn[10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Convert result to DF\n\ntest_pred_df = pd.DataFrame(test_pred_knn)\ntest_pred_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#was_impossible=false: are only calculated\n\ntestset_new = trainset.build_anti_testset()\nlen(testset_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets fetch top 5 values\n\ntestset_new[0:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = knn_model.test(testset_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_df = pd.DataFrame([[x.uid,x.iid,x.est] for x in predictions])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_df.columns = [\"CustomerID\",\"ItemID\",\"Est_rating\"]\npredictions_df.sort_values(by = [\"CustomerID\",\"ItemID\", \"Est_rating\"],ascending=False,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Representing top 5 Recommendations for each Customers\n\n\ntop_5_recos = predictions_df.groupby(\"CustomerID\").head(5).reset_index(drop=True)\ntop_5_recos","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"This represents the top 5 recommendations for each customers."},{"metadata":{},"cell_type":"markdown","source":"## Training and Performance Testing of Popularity based Model "},{"metadata":{"trusted":true},"cell_type":"code","source":"#No of ratings for each Item\n\nitem_group = data.groupby(['ItemID']).agg({'Rating' : 'count'}).reset_index()\nitem_group.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Rating's total\n\ngrouped_sum = item_group['Rating'].sum()\nprint(grouped_sum)\n\n#Thus we have a rating sum of 43309 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_group['Percentage'] = item_group['Rating'].div(grouped_sum)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_group.sort_values(['Rating'], ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data, test_data = train_test_split(data, test_size = 0.3, random_state = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Class for Popularity based Recommender System model\nclass popularity_recommender_py():\n    def __init__(self):\n        self.train_data = None\n        self.user_id = None\n        self.item_id = None\n        self.popularity_recommendations = None\n        \n    #Create the popularity based recommender system model\n    def create(self, train_data, user_id, item_id):\n        self.train_data = train_data\n        self.user_id = user_id\n        self.item_id = item_id\n\n        #Get a count of user_ids for each unique song as recommendation score\n        train_data_grouped = train_data.groupby([self.item_id]).agg({self.user_id: 'count'}).reset_index()\n        train_data_grouped.rename(columns = {'CustomerID': 'score'},inplace=True)\n    \n        #Sort the songs based upon recommendation score\n        train_data_sort = train_data_grouped.sort_values(['score', self.item_id], ascending = [0,1])\n    \n        #Generate a recommendation rank based upon score\n        train_data_sort['Rank'] = train_data_sort['score'].rank(ascending=0, method='first')\n        \n        #Get the top 10 recommendations\n        self.popularity_recommendations = train_data_sort.head(10)\n\n    #Use the popularity based recommender system model to\n    #make recommendations\n    def recommend(self, user_id):    \n        user_recommendations = self.popularity_recommendations\n        \n        #Add user_id column for which the recommendations are being generated\n        user_recommendations['CustomerID'] = user_id\n    \n        #Bring user_id column to the front\n        cols = user_recommendations.columns.tolist()\n        cols = cols[-1:] + cols[:-1]\n        user_recommendations = user_recommendations[cols]\n        \n        return user_recommendations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pm = popularity_recommender_py()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pm.create(train_data,'CustomerID','ItemID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"customers = data['CustomerID'].unique()\nlen(customers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items = data['ItemID'].unique()\nlen(items)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Find Recommendation for a particular Customer\n\ncust_id=customers[7]\npm.recommend(cust_id)\n\n\n#This represents the top 10 items recommended for the customer A3PD8JD9L4WEII","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Item based Recommendation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Class for Item similarity based Recommender System model\nclass item_similarity_recommender_py():\n    def __init__(self):\n        self.train_data = None\n        self.user_id = None\n        self.item_id = None\n        self.cooccurence_matrix = None\n        self.songs_dict = None\n        self.rev_songs_dict = None\n        self.item_similarity_recommendations = None\n        \n    #Get unique items (songs) corresponding to a given user\n    def get_user_items(self, user):\n        user_data = self.train_data[self.train_data[self.user_id] == user]\n        user_items = list(user_data[self.item_id].unique())\n        \n        return user_items\n        \n    #Get unique users for a given item (song)\n    def get_item_users(self, item):\n        item_data = self.train_data[self.train_data[self.item_id] == item]\n        item_users = set(item_data[self.user_id].unique())\n            \n        return item_users\n        \n    #Get unique items (songs) in the training data\n    def get_all_items_train_data(self):\n        all_items = list(self.train_data[self.item_id].unique())\n            \n        return all_items\n        \n    #Construct cooccurence matrix\n    def construct_cooccurence_matrix(self, user_songs, all_songs):\n            \n        ####################################\n        #Get users for all songs in user_songs.\n        ####################################\n        user_songs_users = []        \n        for i in range(0, len(user_songs)):\n            user_songs_users.append(self.get_item_users(user_songs[i]))\n            \n        ###############################################\n        #Initialize the item cooccurence matrix of size \n        #len(user_songs) X len(songs)\n        ###############################################\n        cooccurence_matrix = np.matrix(np.zeros(shape=(len(user_songs), len(all_songs))), float)\n           \n        #############################################################\n        #Calculate similarity between user songs and all unique songs\n        #in the training data\n        #############################################################\n        for i in range(0,len(all_songs)):\n            #Calculate unique listeners (users) of song (item) i\n            songs_i_data = self.train_data[self.train_data[self.item_id] == all_songs[i]]\n            users_i = set(songs_i_data[self.user_id].unique())\n            \n            for j in range(0,len(user_songs)):       \n                    \n                #Get unique listeners (users) of song (item) j\n                users_j = user_songs_users[j]\n                    \n                #Calculate intersection of listeners of songs i and j\n                users_intersection = users_i.intersection(users_j)\n                \n                #Calculate cooccurence_matrix[i,j] as Jaccard Index\n                if len(users_intersection) != 0:\n                    #Calculate union of listeners of songs i and j\n                    users_union = users_i.union(users_j)\n                    \n                    cooccurence_matrix[j,i] = float(len(users_intersection))/float(len(users_union))\n                else:\n                    cooccurence_matrix[j,i] = 0\n                    \n        \n        return cooccurence_matrix\n\n    \n    #Use the cooccurence matrix to make top recommendations\n    def generate_top_recommendations(self, user, cooccurence_matrix, all_songs, user_songs):\n        print(\"Non zero values in cooccurence_matrix :%d\" % np.count_nonzero(cooccurence_matrix))\n        \n        #Calculate a weighted average of the scores in cooccurence matrix for all user songs.\n        user_sim_scores = cooccurence_matrix.sum(axis=0)/float(cooccurence_matrix.shape[0])\n        user_sim_scores = np.array(user_sim_scores)[0].tolist()\n \n        #Sort the indices of user_sim_scores based upon their value\n        #Also maintain the corresponding score\n        sort_index = sorted(((e,i) for i,e in enumerate(list(user_sim_scores))), reverse=True)\n    \n        #Create a dataframe from the following\n        columns = ['CustomerID', 'ItemID', 'score', 'rank']\n        #index = np.arange(1) # array of numbers for the number of samples\n        df = pd.DataFrame(columns=columns)\n         \n        #Fill the dataframe with top 10 item based recommendations\n        rank = 1 \n        for i in range(0,len(sort_index)):\n            if ~np.isnan(sort_index[i][0]) and all_songs[sort_index[i][1]] not in user_songs and rank <= 10:\n                df.loc[len(df)]=[user,all_songs[sort_index[i][1]],sort_index[i][0],rank]\n                rank = rank+1\n        \n        #Handle the case where there are no recommendations\n        if df.shape[0] == 0:\n            print(\"The current user has no item for training the item similarity based recommendation model.\")\n            return -1\n        else:\n            return df\n \n    #Create the item similarity based recommender system model\n    def create(self, train_data, user_id, item_id):\n        self.train_data = train_data\n        self.user_id = user_id\n        self.item_id = item_id\n\n    #Use the item similarity based recommender system model to\n    #make recommendations\n    def recommend(self, user):\n        \n        ########################################\n        #A. Get all unique songs for this user\n        ########################################\n        user_songs = self.get_user_items(user)    \n            \n        print(\"No. of unique items for the user: %d\" % len(user_songs))\n        \n        ######################################################\n        #B. Get all unique items (songs) in the training data\n        ######################################################\n        all_songs = self.get_all_items_train_data()\n        \n        print(\"no. of unique items in the training set: %d\" % len(all_songs))\n         \n        ###############################################\n        #C. Construct item cooccurence matrix of size \n        #len(user_songs) X len(songs)\n        ###############################################\n        cooccurence_matrix = self.construct_cooccurence_matrix(user_songs, all_songs)\n        \n        #######################################################\n        #D. Use the cooccurence matrix to make recommendations\n        #######################################################\n        df_recommendations = self.generate_top_recommendations(user, cooccurence_matrix, all_songs, user_songs)\n                \n        return df_recommendations\n    \n    #Get similar items to given items\n    def get_similar_items(self, item_list):\n        \n        user_songs = item_list\n        \n        ######################################################\n        #B. Get all unique items (songs) in the training data\n        ######################################################\n        all_songs = self.get_all_items_train_data()\n        \n        print(\"no. of unique items in the training set: %d\" % len(all_songs))\n         \n        ###############################################\n        #C. Construct item cooccurence matrix of size \n        #len(user_songs) X len(songs)\n        ###############################################\n        cooccurence_matrix = self.construct_cooccurence_matrix(user_songs, all_songs)\n        \n        #######################################################\n        #D. Use the cooccurence matrix to make recommendations\n        #######################################################\n        user = \"\"\n        df_recommendations = self.generate_top_recommendations(user, cooccurence_matrix, all_songs, user_songs)\n         \n        return df_recommendations\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_model = item_similarity_recommender_py()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_model.create(train_data,'CustomerID','ItemID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Find recommendation for User with id 5\n\ncusto_id = customers[5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_items = item_model.get_user_items(custo_id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('--------------------------------------------------------')\nprint(\"Training data items for the user userId: %s:\" %custo_id)\nprint('--------------------------------------------------------')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for user_item in user_items:\n    print(user_item)\n    \nprint('--------------------------------------------------------')\nprint(\"Recommendation process is going on:\" )\nprint('--------------------------------------------------------')\n\n#Recommend items for the user using personalized model\nitem_model.recommend(custo_id)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"From the above we get the top 10 items which are recommended for the customer A3HPCRD9RX351S."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}