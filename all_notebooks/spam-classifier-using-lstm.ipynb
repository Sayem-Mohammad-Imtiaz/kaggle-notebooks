{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is a project to classify spam and ham messages using LSTM. This is a basic and simple model that will take one through a detailed steps to approach a NLP problem.","metadata":{}},{"cell_type":"markdown","source":"**Steps involved in the Project:**\n\n*  **Data Cleaning:**\n1. Removing unwanted columns.\n2. Exploring & comparing length of messages.\n3. Performing undersampling on dataset.\n\n* **Text preparation:**\n1. Tokenization of Messages.\n2. One hot implementation on tokenized message(corpus)\n3. Perform word embedding\n\n* **Data preparation/Data Splitting:**\n1. Split the data into training+validation(85%) & testing(15%) data.\n2. Further split the training+validation data into training(85%) and validation(15%) data.\n\n* **Model Building:**\n1. Build a Sequential model: Embedding Layer->LSTM->Dense(output layer)\n2. Fit and Validate model on training and validation model\n\n* **Model Evaluation:**\n1. Evaluate the model on test dataset.\n2. Get the model accuracy score and visualize confusion matrix\n\n\n* **Model Testing:**\n1. Created a function that would classifiy the messages using the model\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:10:47.254256Z","iopub.execute_input":"2021-06-05T13:10:47.254699Z","iopub.status.idle":"2021-06-05T13:10:47.262944Z","shell.execute_reply.started":"2021-06-05T13:10:47.254608Z","shell.execute_reply":"2021-06-05T13:10:47.261906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import seaborn and matplotlib for visualization","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:10:52.898387Z","iopub.execute_input":"2021-06-05T13:10:52.898768Z","iopub.status.idle":"2021-06-05T13:10:53.246153Z","shell.execute_reply.started":"2021-06-05T13:10:52.898733Z","shell.execute_reply":"2021-06-05T13:10:53.245441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Read the .csv file and display the first entries of the data","metadata":{}},{"cell_type":"code","source":"data=pd.read_csv(\"../input/sms-spam-collection-dataset/spam.csv\",encoding=\"latin\")\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:10:57.914387Z","iopub.execute_input":"2021-06-05T13:10:57.91474Z","iopub.status.idle":"2021-06-05T13:10:57.94415Z","shell.execute_reply.started":"2021-06-05T13:10:57.914708Z","shell.execute_reply":"2021-06-05T13:10:57.943275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset contains 5 columns of which 3 Unnamed columns are not important for us","metadata":{}},{"cell_type":"code","source":"data.columns","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:11:05.122563Z","iopub.execute_input":"2021-06-05T13:11:05.122892Z","iopub.status.idle":"2021-06-05T13:11:05.128489Z","shell.execute_reply.started":"2021-06-05T13:11:05.122861Z","shell.execute_reply":"2021-06-05T13:11:05.127666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Drop these three columns","metadata":{}},{"cell_type":"code","source":"data=data.drop(columns=[\"Unnamed: 2\",\"Unnamed: 3\",\"Unnamed: 4\"])","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:11:15.588597Z","iopub.execute_input":"2021-06-05T13:11:15.588981Z","iopub.status.idle":"2021-06-05T13:11:15.597736Z","shell.execute_reply.started":"2021-06-05T13:11:15.588947Z","shell.execute_reply":"2021-06-05T13:11:15.596902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"columns **v1** & **v2** does not make any sense so rename the columns into meaningful **Category** & **Message** names","metadata":{}},{"cell_type":"code","source":"data=data.rename(\n{\n    \"v1\":\"Category\",\n    \"v2\":\"Message\"\n},\n    axis=1\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:11:40.818555Z","iopub.execute_input":"2021-06-05T13:11:40.818926Z","iopub.status.idle":"2021-06-05T13:11:40.826147Z","shell.execute_reply.started":"2021-06-05T13:11:40.818878Z","shell.execute_reply":"2021-06-05T13:11:40.823791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Display head of the new dataset","metadata":{}},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:11:44.235926Z","iopub.execute_input":"2021-06-05T13:11:44.236291Z","iopub.status.idle":"2021-06-05T13:11:44.24934Z","shell.execute_reply.started":"2021-06-05T13:11:44.236258Z","shell.execute_reply":"2021-06-05T13:11:44.248559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check if the dataset contains any **null** values, luckily we got dataset without any null values","metadata":{}},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:11:55.683769Z","iopub.execute_input":"2021-06-05T13:11:55.68411Z","iopub.status.idle":"2021-06-05T13:11:55.691866Z","shell.execute_reply.started":"2021-06-05T13:11:55.684078Z","shell.execute_reply":"2021-06-05T13:11:55.691038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:12:04.664084Z","iopub.execute_input":"2021-06-05T13:12:04.664738Z","iopub.status.idle":"2021-06-05T13:12:04.685731Z","shell.execute_reply.started":"2021-06-05T13:12:04.664694Z","shell.execute_reply":"2021-06-05T13:12:04.684758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create new column called **Message Length** that would compute the message lengths","metadata":{}},{"cell_type":"code","source":"data[\"Message Length\"]=data[\"Message\"].apply(len)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:12:12.975746Z","iopub.execute_input":"2021-06-05T13:12:12.97608Z","iopub.status.idle":"2021-06-05T13:12:12.985211Z","shell.execute_reply.started":"2021-06-05T13:12:12.976048Z","shell.execute_reply":"2021-06-05T13:12:12.984319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualize** the Messages length using **histogram**\n\nIt is evident form the plot that **spam** messages on an average are usually **lengthier** than the **non-spam(ham)** messages.","metadata":{}},{"cell_type":"code","source":"fig=plt.figure(figsize=(12,8))\nsns.histplot(\n    x=data[\"Message Length\"],\n    hue=data[\"Category\"]\n)\nplt.title(\"ham & spam messege length comparision\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:13:28.316863Z","iopub.execute_input":"2021-06-05T13:13:28.317219Z","iopub.status.idle":"2021-06-05T13:13:28.88653Z","shell.execute_reply.started":"2021-06-05T13:13:28.317186Z","shell.execute_reply":"2021-06-05T13:13:28.885498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Display the **description** of length of **ham** and **spam** messages seperately on an individual series.\n\nFrom the statistics of the two description we can see that the ham contains the longest message of 910 length. However more than 70% of the ham messages contains messages of length less than 90.\n\nOn the other hand 75% spam messages have messages length more than 130. Hence can conclude than the spam messages are usually lenthier ","metadata":{}},{"cell_type":"code","source":"ham_desc=data[data[\"Category\"]==\"ham\"][\"Message Length\"].describe()\nspam_desc=data[data[\"Category\"]==\"spam\"][\"Message Length\"].describe()\n\nprint(\"Ham Messege Length Description:\\n\",ham_desc)\nprint(\"************************************\")\nprint(\"Spam Message Length Description:\\n\",spam_desc)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:14:04.524195Z","iopub.execute_input":"2021-06-05T13:14:04.524551Z","iopub.status.idle":"2021-06-05T13:14:04.541629Z","shell.execute_reply.started":"2021-06-05T13:14:04.524516Z","shell.execute_reply":"2021-06-05T13:14:04.540291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the overall data Description we knew that label **Category** contains 2 unique values and hence is a categorical variable and a binary classification problem","metadata":{}},{"cell_type":"code","source":"data.describe(include=\"all\")","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:18:23.14075Z","iopub.execute_input":"2021-06-05T13:18:23.141104Z","iopub.status.idle":"2021-06-05T13:18:23.168172Z","shell.execute_reply.started":"2021-06-05T13:18:23.141069Z","shell.execute_reply":"2021-06-05T13:18:23.167519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The two unique values are **ham** & **spam** and **ham** contains **4825** & **spam** with **747** entries which is a vast difference","metadata":{}},{"cell_type":"code","source":"data[\"Category\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:18:35.459375Z","iopub.execute_input":"2021-06-05T13:18:35.459757Z","iopub.status.idle":"2021-06-05T13:18:35.468073Z","shell.execute_reply.started":"2021-06-05T13:18:35.459714Z","shell.execute_reply":"2021-06-05T13:18:35.467235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualizing the **Category** using countplot shows our **spam** messages are relatively less compared to the **ham** messages in the dataset","metadata":{}},{"cell_type":"code","source":"sns.countplot(\n    data=data,\n    x=\"Category\"\n)\nplt.title(\"ham vs spam\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:19:55.442467Z","iopub.execute_input":"2021-06-05T13:19:55.44282Z","iopub.status.idle":"2021-06-05T13:19:55.548826Z","shell.execute_reply.started":"2021-06-05T13:19:55.442786Z","shell.execute_reply":"2021-06-05T13:19:55.547975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Ham** contains **86.6%** while **spam** constitute only **13.4%** of the total dataset, and thus we can conclude that the data is **imbalanced**.\n\n**Imbalanced Data:** Imbalanced data are dataset with an unequal class distribution. Since the ham contains more than 86% of dataset, the model can plainly have an accuracy score of **86%** by just classifying all the entries as ham message. However we just dont want to have a more accuracy score but also a model that can **generalize** well.\n\n","metadata":{}},{"cell_type":"code","source":"ham_count=data[\"Category\"].value_counts()[0]\nspam_count=data[\"Category\"].value_counts()[1]\n\ntotal_count=data.shape[0]\n\nprint(\"Ham contains:{:.2f}% of total data.\".format(ham_count/total_count*100))\nprint(\"Spam contains:{:.2f}% of total data.\".format(spam_count/total_count*100))","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:20:13.724651Z","iopub.execute_input":"2021-06-05T13:20:13.724981Z","iopub.status.idle":"2021-06-05T13:20:13.737515Z","shell.execute_reply.started":"2021-06-05T13:20:13.724949Z","shell.execute_reply":"2021-06-05T13:20:13.736817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since our dataset is imbalanced I have used undersampling technique to make a balanced dataset.\n\n**Undersampling**: It is a technique of obtaining an equivalent sample from the dataset by simply **deleting** some of the examples of **majority** class","metadata":{}},{"cell_type":"code","source":"#compute the length of majority & minority class\nminority_len=len(data[data[\"Category\"]==\"spam\"])\nmajority_len=len(data[data[\"Category\"]==\"ham\"])\n\n#store the indices of majority and minority class\nminority_indices=data[data[\"Category\"]==\"spam\"].index\nmajority_indices=data[data[\"Category\"]==\"ham\"].index\n\n#generate new majority indices from the total majority_indices\n#with size equal to minority class length so we obtain equivalent number of indices length\nrandom_majority_indices=np.random.choice(\n    majority_indices,\n    size=minority_len,\n    replace=False\n)\n\n#concatenate the two indices to obtain indices of new dataframe\nundersampled_indices=np.concatenate([minority_indices,random_majority_indices])\n\n#create df using new indices\ndf=data.loc[undersampled_indices]\n\n#shuffle the sample\ndf=df.sample(frac=1)\n\n#reset the index as its all mixed\ndf=df.reset_index()\n\n#drop the older index\ndf=df.drop(\n    columns=[\"index\"],\n)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:22:20.729414Z","iopub.execute_input":"2021-06-05T13:22:20.729782Z","iopub.status.idle":"2021-06-05T13:22:20.75762Z","shell.execute_reply.started":"2021-06-05T13:22:20.729748Z","shell.execute_reply":"2021-06-05T13:22:20.756363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The resulting dataframes have **1494** rows and **4** columns ","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:45:13.231804Z","iopub.execute_input":"2021-06-05T13:45:13.232213Z","iopub.status.idle":"2021-06-05T13:45:13.240441Z","shell.execute_reply.started":"2021-06-05T13:45:13.232181Z","shell.execute_reply":"2021-06-05T13:45:13.239614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can see that the category cotains **747** entries for **ham** and **spam** and now we have a **balanced** dataset.","metadata":{}},{"cell_type":"code","source":"df[\"Category\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:46:09.001912Z","iopub.execute_input":"2021-06-05T13:46:09.002247Z","iopub.status.idle":"2021-06-05T13:46:09.00948Z","shell.execute_reply.started":"2021-06-05T13:46:09.002215Z","shell.execute_reply":"2021-06-05T13:46:09.008652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Both **ham** and **spam** message bars are now **equal** and hence obtained a **balanced** dataset.","metadata":{}},{"cell_type":"code","source":"sns.countplot(\n    data=df,\n    x=\"Category\"\n)\nplt.title(\"ham vs spam\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:45:38.646758Z","iopub.execute_input":"2021-06-05T13:45:38.647111Z","iopub.status.idle":"2021-06-05T13:45:38.75603Z","shell.execute_reply.started":"2021-06-05T13:45:38.64708Z","shell.execute_reply":"2021-06-05T13:45:38.754931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Display the head of new **df**","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:22:33.463998Z","iopub.execute_input":"2021-06-05T13:22:33.46432Z","iopub.status.idle":"2021-06-05T13:22:33.475066Z","shell.execute_reply.started":"2021-06-05T13:22:33.46429Z","shell.execute_reply":"2021-06-05T13:22:33.47353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Created new column **Label** and encode **ham** as **0** and **spam** as **1**","metadata":{}},{"cell_type":"code","source":"df[\"Label\"]=df[\"Category\"].map(\n    {\n        \"ham\":0,\n        \"spam\":1\n    }\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:22:50.526646Z","iopub.execute_input":"2021-06-05T13:22:50.526958Z","iopub.status.idle":"2021-06-05T13:22:50.532357Z","shell.execute_reply.started":"2021-06-05T13:22:50.526929Z","shell.execute_reply":"2021-06-05T13:22:50.53153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"display head to see the new column","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:22:54.880754Z","iopub.execute_input":"2021-06-05T13:22:54.881098Z","iopub.status.idle":"2021-06-05T13:22:54.891675Z","shell.execute_reply.started":"2021-06-05T13:22:54.881065Z","shell.execute_reply":"2021-06-05T13:22:54.89063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import libraries to perform word **tokenization**","metadata":{}},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\nstemmer=PorterStemmer()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:24:48.925752Z","iopub.execute_input":"2021-06-05T13:24:48.926081Z","iopub.status.idle":"2021-06-05T13:24:49.247359Z","shell.execute_reply.started":"2021-06-05T13:24:48.92605Z","shell.execute_reply":"2021-06-05T13:24:49.246596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Perform word **tokenization** using the below block of code","metadata":{}},{"cell_type":"code","source":"#declare empty list to store tokenized message\ncorpus=[]\n\n#iterate through the df[\"Message\"]\nfor message in df[\"Message\"]:\n    \n    #replace every special characters, numbers etc.. with whitespace of message\n    #It will help retain only letter/alphabets\n    message=re.sub(\"[^a-zA-Z]\",\" \",message)\n    \n    #convert every letters to its lowercase\n    message=message.lower()\n    \n    #split the word into individual word list\n    message=message.split()\n    \n    #perform stemming using PorterStemmer for all non-english-stopwords\n    message=[stemmer.stem(words)\n            for words in message\n             if words not in set(stopwords.words(\"english\"))\n            ]\n    #join the word lists with the whitespace\n    message=\" \".join(message)\n    \n    #append the message in corpus list\n    corpus.append(message)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:24:53.085764Z","iopub.execute_input":"2021-06-05T13:24:53.086095Z","iopub.status.idle":"2021-06-05T13:24:56.919253Z","shell.execute_reply.started":"2021-06-05T13:24:53.086063Z","shell.execute_reply":"2021-06-05T13:24:56.918272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Perform one_hot on the corpus","metadata":{}},{"cell_type":"markdown","source":"I have initialized the **vocabulary** size to **10,000.**\n\n**oneHot_doc** will contain the list of indices of words in the corpus whose indices will range in bw **0-10,000**.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import one_hot\nvocab_size=10000\n\noneHot_doc=[one_hot(words,n=vocab_size)\n           for words in corpus\n           ]","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:24:59.704054Z","iopub.execute_input":"2021-06-05T13:24:59.704375Z","iopub.status.idle":"2021-06-05T13:25:01.242911Z","shell.execute_reply.started":"2021-06-05T13:24:59.704344Z","shell.execute_reply":"2021-06-05T13:25:01.242043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After **one_hot** we will then perform **word embedding** \n\nThe resulting list of one_hot will contain uneven indices length because of uneven length of tokenized words in the corpus.\n\nTo perform word embedding we have to consider a sentence length and hence to define a fixed sentence length for our dataset we will try to visualize and understand the patterns of Messege length of dataset.","metadata":{}},{"cell_type":"code","source":"df[\"Message Length\"].describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:25:04.880455Z","iopub.execute_input":"2021-06-05T13:25:04.880791Z","iopub.status.idle":"2021-06-05T13:25:04.893007Z","shell.execute_reply.started":"2021-06-05T13:25:04.880759Z","shell.execute_reply":"2021-06-05T13:25:04.892104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**visualizing the Messages length using kdeplot.**\n\nFrom the plot we can see that spam messages usually are lengthier than ham messages, however both the messages are concentrated bw the lenth of 0-200, of course there are few messages with lengths more than 200. However we will take a length of 200 to make an even distribution amongst the two messages.","metadata":{}},{"cell_type":"code","source":"fig=plt.figure(figsize=(12,8))\nsns.kdeplot(\n    x=df[\"Message Length\"],\n    hue=df[\"Category\"]\n)\nplt.title(\"ham & spam messege length comparision\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:25:10.487744Z","iopub.execute_input":"2021-06-05T13:25:10.488078Z","iopub.status.idle":"2021-06-05T13:25:10.678681Z","shell.execute_reply.started":"2021-06-05T13:25:10.488048Z","shell.execute_reply":"2021-06-05T13:25:10.677784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will use **pad_sequences** from keras to perform **word embedding**.\n\nThis will make every list to an equal length(**sentence length**) which we can later be fed to our model.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\nsentence_len=200\nembedded_doc=pad_sequences(\n    oneHot_doc,\n    maxlen=sentence_len,\n    padding=\"pre\"\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:25:53.935821Z","iopub.execute_input":"2021-06-05T13:25:53.936162Z","iopub.status.idle":"2021-06-05T13:25:53.966208Z","shell.execute_reply.started":"2021-06-05T13:25:53.93613Z","shell.execute_reply":"2021-06-05T13:25:53.965465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us make a data frame using embedded document, and a target using **Label** column from df","metadata":{}},{"cell_type":"code","source":"extract_features=pd.DataFrame(\n    data=embedded_doc\n)\ntarget=df[\"Label\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:26:02.468199Z","iopub.execute_input":"2021-06-05T13:26:02.468569Z","iopub.status.idle":"2021-06-05T13:26:02.474669Z","shell.execute_reply.started":"2021-06-05T13:26:02.468521Z","shell.execute_reply":"2021-06-05T13:26:02.473607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I have concatenated two dataframes to get the final dataframe","metadata":{}},{"cell_type":"code","source":"df_final=pd.concat([extract_features,target],axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:26:06.739888Z","iopub.execute_input":"2021-06-05T13:26:06.740223Z","iopub.status.idle":"2021-06-05T13:26:06.748741Z","shell.execute_reply.started":"2021-06-05T13:26:06.740192Z","shell.execute_reply":"2021-06-05T13:26:06.747777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Resulting dataframe contains 201 columns where 200 are independent features and 1 is our target class.","metadata":{}},{"cell_type":"code","source":"df_final.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:26:09.554478Z","iopub.execute_input":"2021-06-05T13:26:09.554851Z","iopub.status.idle":"2021-06-05T13:26:09.575275Z","shell.execute_reply.started":"2021-06-05T13:26:09.554818Z","shell.execute_reply":"2021-06-05T13:26:09.574227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split the dataframe into **dependent(y)** & **independent(X)** variables ","metadata":{}},{"cell_type":"code","source":"X=df_final.drop(\"Label\",axis=1)\ny=df_final[\"Label\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:27:37.351838Z","iopub.execute_input":"2021-06-05T13:27:37.352209Z","iopub.status.idle":"2021-06-05T13:27:37.358487Z","shell.execute_reply.started":"2021-06-05T13:27:37.352174Z","shell.execute_reply":"2021-06-05T13:27:37.357374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will now split the dataset for **training**,**validataing** and **testing** sets.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:27:48.603866Z","iopub.execute_input":"2021-06-05T13:27:48.604217Z","iopub.status.idle":"2021-06-05T13:27:48.608681Z","shell.execute_reply.started":"2021-06-05T13:27:48.604185Z","shell.execute_reply":"2021-06-05T13:27:48.607396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code will split the whole data into two parts **trainval** & **testing**.\n\n**Trainval** dataset will contain the dataset for training and validation sets, which will constitute 85% of the data and rest as test data(15%).\n\nThis is done so to stop **data leakage**. If we use test data as both validation and testing purpose its very likely that our model will just **memorize** the dataset while validating using test data and we will get a **overfitted model**.\n\nSo I just wanted to completely **isolate** the **test** data from **train** and **validation** dataset, so we get a model with better **generilization**.","metadata":{}},{"cell_type":"code","source":"X_trainval,X_test,y_trainval,y_test=train_test_split(\n    X,\n    y,\n    random_state=42,\n    test_size=0.15\n)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:28:45.282993Z","iopub.execute_input":"2021-06-05T13:28:45.283318Z","iopub.status.idle":"2021-06-05T13:28:45.291687Z","shell.execute_reply.started":"2021-06-05T13:28:45.283286Z","shell.execute_reply":"2021-06-05T13:28:45.290768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have splitted **85**% dataset into **training** and **validataion** so lets futher split our **trainval** data into **training**(**85**%) and **validation**(**15**%) dataset.","metadata":{}},{"cell_type":"code","source":"X_train,X_val,y_train,y_val=train_test_split(\n    X_trainval,\n    y_trainval,\n    random_state=42,\n    test_size=0.15\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:29:35.634597Z","iopub.execute_input":"2021-06-05T13:29:35.634929Z","iopub.status.idle":"2021-06-05T13:29:35.642135Z","shell.execute_reply.started":"2021-06-05T13:29:35.6349Z","shell.execute_reply":"2021-06-05T13:29:35.641045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"import libraries to create **model**","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.models import Sequential\n","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:29:44.988862Z","iopub.execute_input":"2021-06-05T13:29:44.989202Z","iopub.status.idle":"2021-06-05T13:29:44.993532Z","shell.execute_reply.started":"2021-06-05T13:29:44.98917Z","shell.execute_reply":"2021-06-05T13:29:44.992645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Initialize the **Sequential** model","metadata":{}},{"cell_type":"code","source":"model=Sequential()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:29:53.107535Z","iopub.execute_input":"2021-06-05T13:29:53.107878Z","iopub.status.idle":"2021-06-05T13:29:54.0202Z","shell.execute_reply.started":"2021-06-05T13:29:53.107847Z","shell.execute_reply":"2021-06-05T13:29:54.019216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"create a model using **Embedding layer->LSTM->Dense**\n\n**Embedding_layer**: We will declare input_dimesion as our vocaburaly size(10,000),input_length as the sentence length and output dimension as 100.\n\n**LSTM**: Add **128** units to the layers whose output will be fed as an input to our output Dense layer.\n\n**Dense**: Add 1 unit(neurons) to the dense layers and with an **sigmoid** activation, since we have binary classification problem. Else if you are to performing multi-class classification problem **softmax** activation with **units=no. of classes** would perform pretty well.","metadata":{}},{"cell_type":"code","source":"feature_num=100\nmodel.add(\n    Embedding(\n        input_dim=vocab_size,\n        output_dim=feature_num,\n        input_length=sentence_len\n    )\n)\nmodel.add(\n    LSTM(\n    units=128\n    )\n)\n\nmodel.add(\n    Dense(\n        units=1,\n        activation=\"sigmoid\"\n    )\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:30:45.465732Z","iopub.execute_input":"2021-06-05T13:30:45.466057Z","iopub.status.idle":"2021-06-05T13:30:45.707322Z","shell.execute_reply.started":"2021-06-05T13:30:45.466027Z","shell.execute_reply":"2021-06-05T13:30:45.706352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets **compile** the model built above.\n\nI have used a **adam** optimizer with **learning rate of 0.001** & **binary_crossentropy** as loss funtion","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nmodel.compile(\n    optimizer=Adam(\n    learning_rate=0.001\n    ),\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\"]\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:31:04.941964Z","iopub.execute_input":"2021-06-05T13:31:04.942314Z","iopub.status.idle":"2021-06-05T13:31:04.95832Z","shell.execute_reply.started":"2021-06-05T13:31:04.94228Z","shell.execute_reply":"2021-06-05T13:31:04.957573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once the model is complied we will **fit** the model using **train** and **validation** dataset.","metadata":{}},{"cell_type":"code","source":"model.fit(\n    X_train,\n    y_train,\n    validation_data=(\n        X_val,\n        y_val\n    ),\n    epochs=10\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:31:08.675223Z","iopub.execute_input":"2021-06-05T13:31:08.678177Z","iopub.status.idle":"2021-06-05T13:31:19.143604Z","shell.execute_reply.started":"2021-06-05T13:31:08.678125Z","shell.execute_reply":"2021-06-05T13:31:19.142831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the model is fitted using required datasets, its time that how our model predict test data we have isolated earlier.\n\nThe prediction will be stored in array of boolean where prediction value **greater than** **0.5** will be assigned **True(Spam)** else **lesser than 0.5 will be False(Ham)**","metadata":{}},{"cell_type":"code","source":"y_pred=model.predict(X_test)\ny_pred=(y_pred>0.5)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:31:46.530346Z","iopub.execute_input":"2021-06-05T13:31:46.530699Z","iopub.status.idle":"2021-06-05T13:31:46.913894Z","shell.execute_reply.started":"2021-06-05T13:31:46.530667Z","shell.execute_reply":"2021-06-05T13:31:46.913058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets import **metrics** to **evaluate** our **model**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score,confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:32:01.177402Z","iopub.execute_input":"2021-06-05T13:32:01.177782Z","iopub.status.idle":"2021-06-05T13:32:01.183681Z","shell.execute_reply.started":"2021-06-05T13:32:01.177749Z","shell.execute_reply":"2021-06-05T13:32:01.182865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model predicts pretty well on the **test** data as evident from the accuracy score.","metadata":{}},{"cell_type":"code","source":"score=accuracy_score(y_test,y_pred)\nprint(\"Test Score:{:.2f}%\".format(score*100))","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:32:06.089772Z","iopub.execute_input":"2021-06-05T13:32:06.090112Z","iopub.status.idle":"2021-06-05T13:32:06.098875Z","shell.execute_reply.started":"2021-06-05T13:32:06.090078Z","shell.execute_reply":"2021-06-05T13:32:06.09591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets **visualize** our **confusion_matrix** using **heatmap**\n\nOur Model also gives a better generalization since the number of **False Positive(FP)** and **False Negative(FN)** are relatively lesser than the **True Postive(TP)** and **True Negative(TN)**","metadata":{}},{"cell_type":"code","source":"cm=confusion_matrix(y_test,y_pred)\nfig=plt.figure(figsize=(12,8))\nsns.heatmap(\n    cm,\n    annot=True,\n)\nplt.title(\"Confusion Matrix\")\ncm","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:50:04.410304Z","iopub.execute_input":"2021-06-05T13:50:04.410699Z","iopub.status.idle":"2021-06-05T13:50:04.625951Z","shell.execute_reply.started":"2021-06-05T13:50:04.410666Z","shell.execute_reply":"2021-06-05T13:50:04.625282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is the final code(**function**) that would take a **raw message** and classfiy the message using the model.","metadata":{}},{"cell_type":"code","source":"#The function take model and message as parameter\ndef classify_message(model,message):\n    \n    #We will treat message as a paragraphs containing multiple sentences(lines)\n    #we will extract individual lines\n    for sentences in message:\n        sentences=nltk.sent_tokenize(message)\n        \n        #Iterate over individual sentences\n        for sentence in sentences:\n            #replace all special characters\n            words=re.sub(\"[^a-zA-Z]\",\" \",sentence)\n            \n            #perform word tokenization of all non-english-stopwords\n            if words not in set(stopwords.words('english')):\n                word=nltk.word_tokenize(words)\n                word=\" \".join(word)\n    \n    #perform one_hot on tokenized word            \n    oneHot=[one_hot(word,n=vocab_size)]\n    \n    #create an embedded documnet using pad_sequences \n    #this can be fed to our model\n    text=pad_sequences(oneHot,maxlen=sentence_len,padding=\"pre\")\n    \n    #predict the text using model\n    predict=model.predict(text)\n    \n    #if predict value is greater than 0.5 its a spam\n    if predict>0.5:\n        print(\"It is a spam\")\n    #else the message is not a spam    \n    else:\n        print(\"It is not a spam\")","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:50:58.441459Z","iopub.execute_input":"2021-06-05T13:50:58.441781Z","iopub.status.idle":"2021-06-05T13:50:58.449467Z","shell.execute_reply.started":"2021-06-05T13:50:58.44175Z","shell.execute_reply":"2021-06-05T13:50:58.448569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"message1=\"I am having a bad day and I would like to have a break today\"\nmessage2=\"This is to inform you had won a lottery and the subscription will end in a week so call us.\"\n","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:51:52.388218Z","iopub.execute_input":"2021-06-05T13:51:52.388577Z","iopub.status.idle":"2021-06-05T13:51:52.392153Z","shell.execute_reply.started":"2021-06-05T13:51:52.388543Z","shell.execute_reply":"2021-06-05T13:51:52.391317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model predicts **message1** as **not a spam message**","metadata":{}},{"cell_type":"code","source":"classify_message(model,message1)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:52:36.309095Z","iopub.execute_input":"2021-06-05T13:52:36.309411Z","iopub.status.idle":"2021-06-05T13:52:36.372543Z","shell.execute_reply.started":"2021-06-05T13:52:36.309379Z","shell.execute_reply":"2021-06-05T13:52:36.371602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model predicts **message2** as **spam message**","metadata":{}},{"cell_type":"code","source":"classify_message(model,message2)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:54:08.211574Z","iopub.execute_input":"2021-06-05T13:54:08.211913Z","iopub.status.idle":"2021-06-05T13:54:08.291463Z","shell.execute_reply.started":"2021-06-05T13:54:08.211881Z","shell.execute_reply":"2021-06-05T13:54:08.290334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}