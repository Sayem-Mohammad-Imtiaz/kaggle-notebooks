{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# This notebook contains some code inspired from Sequences, Time Series and Prediction deeplearning.ai course which is a great course to start learning Deep Learning on Tensorflow.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport datetime\nimport tensorflow as tf\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-14T13:15:17.522542Z","iopub.execute_input":"2021-06-14T13:15:17.522891Z","iopub.status.idle":"2021-06-14T13:15:26.078295Z","shell.execute_reply.started":"2021-06-14T13:15:17.522862Z","shell.execute_reply":"2021-06-14T13:15:26.077262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Import Data\ndf = pd.read_csv('../input/sales-forecasting/train.csv')\n","metadata":{"execution":{"iopub.status.busy":"2021-06-14T13:15:32.805379Z","iopub.execute_input":"2021-06-14T13:15:32.805703Z","iopub.status.idle":"2021-06-14T13:15:32.879242Z","shell.execute_reply.started":"2021-06-14T13:15:32.805669Z","shell.execute_reply":"2021-06-14T13:15:32.878413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"Create a data frame containing sales grouped by day. ","metadata":{}},{"cell_type":"code","source":"#Create a data frame containing daily sales\ndf['Order Date'] = pd.to_datetime(df['Order Date'])\ndf.sort_values(['Order Date'],inplace=True)\ndaily_sales = pd.DataFrame(df.groupby('Order Date',sort=False)['Sales'].sum())\ndaily_sales.reset_index(inplace=True)\ndaily_sales['Order Year'] = daily_sales['Order Date'].apply(lambda x:x.year)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T13:15:34.384936Z","iopub.execute_input":"2021-06-14T13:15:34.385295Z","iopub.status.idle":"2021-06-14T13:15:34.424257Z","shell.execute_reply.started":"2021-06-14T13:15:34.385259Z","shell.execute_reply":"2021-06-14T13:15:34.423356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split data into two sets, a Training Set and a Validation Set. The training set contains all observations except those occurring in year 2018 which are reserved for cross validation.","metadata":{}},{"cell_type":"code","source":"#Split Data into train and test sets :\nX = np.array(daily_sales['Sales'])\nX_train = np.array(daily_sales[daily_sales['Order Year']!=2018]['Sales'])\nX_test = np.array(daily_sales[daily_sales['Order Year']==2018]['Sales'])\nprint('Train set size : ',len(X_train))\nprint('Test set size : ',len(X_test))","metadata":{"execution":{"iopub.status.busy":"2021-06-14T13:15:35.442622Z","iopub.execute_input":"2021-06-14T13:15:35.442992Z","iopub.status.idle":"2021-06-14T13:15:35.465645Z","shell.execute_reply.started":"2021-06-14T13:15:35.442963Z","shell.execute_reply":"2021-06-14T13:15:35.464734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, data must be transformed into windowed dataset, this is easily done using tensorflow, the next function is a simple way of doing it.","metadata":{}},{"cell_type":"code","source":"def windowed_dataset(X, window_size, batch_size, shuffle_buffer):\n    #Expand dataset fir RNN input shape expectation\n    X = tf.expand_dims(X, axis=-1)\n    #Create a dataset \n    ds = tf.data.Dataset.from_tensor_slices(X)\n    #Windowing the data set, window_size lags (passed observations)\n    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n    #Batching and shuffling observations\n    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n    ds = ds.shuffle(shuffle_buffer)\n    #Decompose into predictors and predicted components\n    ds = ds.map(lambda w: (w[:-1], w[-1]))\n    return ds.batch(batch_size).prefetch(1)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T13:15:36.422754Z","iopub.execute_input":"2021-06-14T13:15:36.42314Z","iopub.status.idle":"2021-06-14T13:15:36.429146Z","shell.execute_reply.started":"2021-06-14T13:15:36.423096Z","shell.execute_reply":"2021-06-14T13:15:36.428134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apply the previous function on daily sales using a time window of 41 days.","metadata":{}},{"cell_type":"code","source":"window_size = 41\n#Split time defines the limit of training observations and start of validation observations.\nsplit_time = 908\n\n\nXp_train = windowed_dataset(X_train,window_size=window_size,batch_size=256,shuffle_buffer=len(X_train))\nXp_validation = windowed_dataset(X_test,window_size=window_size,batch_size=512,shuffle_buffer=len(X_test))","metadata":{"execution":{"iopub.status.busy":"2021-06-14T13:15:37.322789Z","iopub.execute_input":"2021-06-14T13:15:37.323176Z","iopub.status.idle":"2021-06-14T13:15:39.354709Z","shell.execute_reply.started":"2021-06-14T13:15:37.323137Z","shell.execute_reply":"2021-06-14T13:15:39.353819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model ","metadata":{}},{"cell_type":"code","source":"model1 = tf.keras.models.Sequential([\n            \n            \n            #1D Convolutional layer (Helps in smoothing out some noise)\n            tf.keras.layers.Conv1D(filters=3, kernel_size=5,\n                      strides=1, padding=\"valid\",\n                      activation=\"relu\",\n                      input_shape=[None, 1]),\n   \n            \n        \n     \n            # 2 LSTM layers\n            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(40, return_sequences=True)), \n    \n            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(40, return_sequences=False)),  \n    \n            # 3 Dense layers comprising the output layer\n            tf.keras.layers.Dense(30),\n            tf.keras.layers.Activation('relu'),\n     \n    \n            tf.keras.layers.Dense(10),\n            tf.keras.layers.Activation('relu'),\n\n            \n            \n            tf.keras.layers.Dense(1),\n    \n          tf.keras.layers.Lambda(lambda x: x * 10000.0)\n        ])\n\n#Compile the model\n\nmodel1.compile(loss=tf.keras.losses.Huber(),optimizer='Adam',metrics=['mae'])","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:07:43.931543Z","iopub.execute_input":"2021-06-14T14:07:43.931865Z","iopub.status.idle":"2021-06-14T14:07:44.795677Z","shell.execute_reply.started":"2021-06-14T14:07:43.931835Z","shell.execute_reply":"2021-06-14T14:07:44.794792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history1 = model1.fit(Xp_train, epochs=500,validation_data=Xp_validation)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-14T14:07:44.797221Z","iopub.execute_input":"2021-06-14T14:07:44.797728Z","iopub.status.idle":"2021-06-14T14:10:25.278909Z","shell.execute_reply.started":"2021-06-14T14:07:44.797688Z","shell.execute_reply":"2021-06-14T14:10:25.277961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mae=history1.history['mae']\nval_mae = history1.history['val_mae']\nepochs=range(len(mae)) \nfig = plt.figure(figsize=(12,6))\nax = fig.add_axes([0,0,1,1])\nax.plot(epochs, mae, 'b',label='Training mae')\nax.plot(epochs, val_mae, 'r',label='Validation mae')\nax.legend()\nplt.title('MAE')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"MAE\")","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:14:31.623657Z","iopub.execute_input":"2021-06-14T14:14:31.623987Z","iopub.status.idle":"2021-06-14T14:14:31.819029Z","shell.execute_reply.started":"2021-06-14T14:14:31.623958Z","shell.execute_reply":"2021-06-14T14:14:31.81802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clearly the model overfitts training set. ","metadata":{}},{"cell_type":"markdown","source":"# Regularization","metadata":{}},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"markdown","source":"Introduce L2 regularization and dropouts into the model","metadata":{}},{"cell_type":"code","source":"def Model(Lambda,drop_rate) :\n    \n    model = tf.keras.models.Sequential([\n            \n            \n            \n            tf.keras.layers.Conv1D(filters=3, kernel_size=5,\n                      strides=1, padding=\"causal\",\n                      activation=\"relu\",\n                      input_shape=[None, 1],kernel_regularizer=tf.keras.regularizers.l2(Lambda)),\n   \n            tf.keras.layers.Dropout(drop_rate),\n        \n     \n            \n            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(40, return_sequences=True,\n                                          kernel_regularizer=tf.keras.regularizers.l2(Lambda))), \n    \n            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(40, return_sequences=False,\n                                          kernel_regularizer=tf.keras.regularizers.l2(Lambda))),  \n            \n            tf.keras.layers.Dropout(drop_rate),\n            \n  \n            tf.keras.layers.Dense(30,kernel_regularizer=tf.keras.regularizers.l2(Lambda)),\n            tf.keras.layers.Activation('relu'),\n     \n    \n            tf.keras.layers.Dense(10,kernel_regularizer=tf.keras.regularizers.l2(Lambda)),\n            tf.keras.layers.Activation('relu'),\n\n            \n            \n            tf.keras.layers.Dense(1,kernel_regularizer=tf.keras.regularizers.l2(Lambda)),\n    \n          tf.keras.layers.Lambda(lambda x: x * 10000.0)\n        ])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-14T13:45:38.667282Z","iopub.execute_input":"2021-06-14T13:45:38.667621Z","iopub.status.idle":"2021-06-14T13:45:38.676667Z","shell.execute_reply.started":"2021-06-14T13:45:38.667593Z","shell.execute_reply":"2021-06-14T13:45:38.675695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameters space","metadata":{}},{"cell_type":"markdown","source":"Create a hyperparameter space, in this case it is the space of pairs of the form (regularization factor,dropout rate). Hence, to create this space, simply create a space of regularization factor and one other for dropout rates and multiply them.","metadata":{}},{"cell_type":"code","source":"#Regularization factor space using random logarithmic selection :[1e-3,1] \nr = -3*np.random.rand(20)\nregu_factors = 10**r\n\n#Dropout rate space : [0,0.5]\ndrop_rates = np.linspace(0,0.4,5)\n\n#Hyperparameters space : Product of both previous spaces\nimport itertools\nhyper_space = list(itertools.product(regu_factors,drop_rates))\n\nprint('Subset of hyperparameters space :')\nprint(hyper_space[0:10])","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:14:05.702348Z","iopub.execute_input":"2021-06-14T14:14:05.702741Z","iopub.status.idle":"2021-06-14T14:14:05.712241Z","shell.execute_reply.started":"2021-06-14T14:14:05.702699Z","shell.execute_reply":"2021-06-14T14:14:05.708528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Explore the hyperparameter space ","metadata":{}},{"cell_type":"code","source":"#Dictionaries to save results\nhistory_dict = dict()\nprediction_dict = dict()\ni=0\n\n#Loop over hyperparameter space components\nfor Lambda,rate in hyper_space :\n    print(f'Processing for ({Lambda},{rate})')\n    #Update parameters\n    model = Model(Lambda,rate)\n    #Compile model and train\n    model.compile(loss=tf.keras.losses.Huber(),optimizer='Adam',metrics=['mae'])\n    #Save training and validation history\n    history = model.fit(Xp_train, epochs=500,validation_data=Xp_validation,callbacks=[early_stop]) \n    history_dict[f'Training MAE {i}'] = history.history['mae']\n    history_dict[f'Validation MAE {i}'] = history.history['val_mae']\n    i+=1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It takes time to search for hyperparameters. Let's just proceed with some given model.","metadata":{}},{"cell_type":"markdown","source":"# Regularized model","metadata":{}},{"cell_type":"code","source":"model = Model(0.5,0.2)\nmodel.compile(loss=tf.keras.losses.Huber(),optimizer='Adam',metrics=['mae'])","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:01:46.883659Z","iopub.execute_input":"2021-06-14T14:01:46.884042Z","iopub.status.idle":"2021-06-14T14:01:48.525928Z","shell.execute_reply.started":"2021-06-14T14:01:46.884008Z","shell.execute_reply":"2021-06-14T14:01:48.524942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n#Recuparate best model\nearly_stop = EarlyStopping(monitor='val_mae',patience=500,restore_best_weights=True,mode='min')\nhistory = model.fit(Xp_train, epochs=500,validation_data=Xp_validation,callbacks=[early_stop]) ","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-14T14:01:50.322455Z","iopub.execute_input":"2021-06-14T14:01:50.322788Z","iopub.status.idle":"2021-06-14T14:04:34.300343Z","shell.execute_reply.started":"2021-06-14T14:01:50.322759Z","shell.execute_reply":"2021-06-14T14:04:34.299496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mae=history.history['mae']\nval_mae = history.history['val_mae']\nprint('min validation mae : ',min(history.history['val_mae']))\n\nmin_val = min(history.history['val_mae'])\nindex = history.history['val_mae'].index(min_val)\nepochs=range(len(mae)) \nfig = plt.figure(figsize=(12,6))\nax = fig.add_axes([0,0,1,1])\nax.plot(epochs, mae, 'b',label='Training mae')\nax.plot(epochs, val_mae, 'r',label='Validation mae')\nax.plot(index,min_val,marker='*',ms=20,markerfacecolor='yellow',markeredgewidth=3, markeredgecolor='green')\nax.legend()\nplt.title('MAE')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"MAE\")","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:04:45.684721Z","iopub.execute_input":"2021-06-14T14:04:45.685068Z","iopub.status.idle":"2021-06-14T14:04:45.889888Z","shell.execute_reply.started":"2021-06-14T14:04:45.685038Z","shell.execute_reply":"2021-06-14T14:04:45.888923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"#Given a series, this function predicts sales for each step.\ndef model_forecast(model, X, window_size):\n    #Creating a dataset\n    ds = tf.data.Dataset.from_tensor_slices(X)\n    #Windowing\n    ds = ds.window(window_size, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size))\n    ds = ds.batch(908).prefetch(1)\n    #Predict \n    forecast = model.predict(ds)\n    return forecast","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:04:54.340799Z","iopub.execute_input":"2021-06-14T14:04:54.34112Z","iopub.status.idle":"2021-06-14T14:04:54.346283Z","shell.execute_reply.started":"2021-06-14T14:04:54.341089Z","shell.execute_reply":"2021-06-14T14:04:54.345386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's apply this function to all Data and split results into training and testing results.","metadata":{}},{"cell_type":"code","source":"forecast = model_forecast(model,X[...,np.newaxis],window_size)\nforecast = forecast[:,-1].reshape((len(forecast),))","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:04:57.239782Z","iopub.execute_input":"2021-06-14T14:04:57.240108Z","iopub.status.idle":"2021-06-14T14:04:58.533421Z","shell.execute_reply.started":"2021-06-14T14:04:57.240078Z","shell.execute_reply":"2021-06-14T14:04:58.532558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_predictions = forecast[:split_time-window_size]\ntest_predictions = forecast [split_time-window_size:-1]\nMSE_train = tf.keras.metrics.mean_squared_error(X_train[window_size:], train_predictions)\nMSE_test = tf.keras.metrics.mean_squared_error(X_test, test_predictions)\nMAE_train = tf.keras.metrics.mean_absolute_error(X_train[window_size:], train_predictions)\nMAE_test = tf.keras.metrics.mean_absolute_error(X_test, test_predictions)\nprint('Train RMSE = ',np.sqrt(MSE_train))\nprint('Test RMSE = ',np.sqrt(MSE_test))\nprint('Train MAE = ',MAE_train.numpy())\nprint('Test MAE = ',MAE_test.numpy())","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:05:01.109859Z","iopub.execute_input":"2021-06-14T14:05:01.110201Z","iopub.status.idle":"2021-06-14T14:05:01.123601Z","shell.execute_reply.started":"2021-06-14T14:05:01.110167Z","shell.execute_reply":"2021-06-14T14:05:01.122419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Still a lot of work to do on regularization. Also note that daily sales contain a lot of noise and demonstrates no clear pattern...","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}