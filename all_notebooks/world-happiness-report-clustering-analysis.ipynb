{"nbformat_minor":1,"cells":[{"cell_type":"markdown","source":"The objective is to learn various types of clustering algorithms as available in sklearn\n\nData used is Kaggle- World Happiness Report Ref- ***https://www.kaggle.com/unsdsn/world-happiness/data***\n\nClustering techniques reference: ***http://scikit-learn.org/stable/modules/clustering.html#clustering***\n\nClustering techniques used:\n\n1 **K-Means** - KMeans algorithm clusters data by trying to separate samples in n groups of equal variance, inimizing a criterion known as the within-cluster sum-of-squares.\n\n2 **Mean Shift** - This clustering aims to discover blobs in a smooth density of samples.It is a centroid based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region.These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids.\n\n3 **Mini Batch K-Means** - Similar to kmeans but clustering is done in batches to reduce computation time\n\n4 **Spectral clustering** - SpectralClustering does a low-dimension embedding of the affinity matrix between samples, followed by a KMeans in the low dimensional space. It is especially efficient if the affinity matrix is sparse. SpectralClustering requires the number of clusters to be specified.It works well for a small number of clusters but is not advised when using many clusters.\n\n5 **DBSCAN** - The DBSCAN algorithm views clusters as areas of high density separated by areas of low density. Due to this rather generic view, clusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are convex shaped.\n\n6 **Affinity Propagation** - Creates clusters by sending messages between pairs of samples until convergence.A dataset is then described using a small number of exemplars, which are identified as those most representative of other samples. The messages sent between pairs represent the suitability for one sample to be the exemplar of the other, which is updated in response to the values from other pairs.\n\n7 **Birch** - The Birch builds a tree called the Characteristic Feature Tree (CFT) for the given data and clustering is performed as per the nodes of the tree\n\n8 **Gaussian Mixture modeling** - It treats each dense region as if produced by a gaussian process and then goes about to find the parameters of the process","metadata":{"collapsed":true,"_cell_guid":"494b0a6d-d9b8-4365-8bbb-52411b19a338","_uuid":"74d8e01e13a08fa184516bdeb837d83e65263463"}},{"cell_type":"markdown","source":"**Clustering Analysis**","metadata":{}},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"collapsed":true},"source":"#Call libraries\nimport time                   # To time processes\nimport warnings               # To suppress warnings\nimport numpy as np            # Data manipulation\nimport pandas as pd           # Dataframe manipulatio \nimport matplotlib.pyplot as plt  # For graphics\nimport os                     # For os related operations\nimport sys                    # For data size\n\nfrom sklearn import cluster, mixture              # For clustering\nfrom sklearn.preprocessing import StandardScaler  # For scaling dataset\n#%matplotlib inline            # To display plots inline\nwarnings.filterwarnings('ignore','UsageError')"},{"cell_type":"markdown","source":"Read and normalize data","metadata":{}},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"collapsed":true},"source":"os.chdir(\"../input\")\ndf= pd.read_csv(\"2017.csv\")\n\n# Taken a 10% sample for analysis\nX = df.sample(frac=0.1)\n\n# Explore and scale dataset\nX.columns.values\nX.shape                 # 155 X 12\nX = X.iloc[:, 2: ]      # Ignore Country and Happiness_Rank columns\nX.head(2)\nX.dtypes\n\n# Normalization of dataset for easier parameter selection\nss = StandardScaler() #Instantiate scaler object\nss.fit_transform(X)"},{"cell_type":"markdown","source":"Create and set Parameters used in different clustering","metadata":{}},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"collapsed":true},"source":"n_clusters = 2   #for K-means clustering,, Mini Batch K-Means. No of clusters to use\nbandwidth = 0.1  #for Mean-Shift Clustering. bandwidth dictates size of the region to search through\neps = 0.3 #for DBSCAN Clustering. eps decides the incremental search area within which density should be same\ndamping = 0.9; preference = -200  #for Affinity Propagation. preference - controls how many exemplars are used\n# damping factor - damps the responsibility and availability messages to avoid numerical oscillations when updating these messages\n"},{"cell_type":"markdown","source":"Create cluster objects","metadata":{}},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"collapsed":true},"source":"km = cluster.KMeans(n_clusters =n_clusters )\nkm_result = km.fit_predict(X)\nms = cluster.MeanShift(bandwidth=bandwidth)\nms_result = ms.fit_predict(X)\ntwo_means = cluster.MiniBatchKMeans(n_clusters=n_clusters)\ntwo_means_result = two_means.fit_predict(X)\nspectral = cluster.SpectralClustering(n_clusters=n_clusters)\nsp_result= spectral.fit_predict(X)\ndbscan = cluster.DBSCAN(eps=eps)\ndb_result= dbscan.fit_predict(X)\naffinity_propagation = cluster.AffinityPropagation(damping=damping, preference=preference) \naffinity_propagation.fit(X)\nbirch = cluster.Birch(n_clusters=n_clusters)\nbirch_result = birch.fit_predict(X)\ngmm = mixture.GaussianMixture( n_components=n_clusters, covariance_type='full')\ngmm.fit(X)"},{"cell_type":"markdown","source":"Create Clustering Algorithm","metadata":{}},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"collapsed":true},"source":"clustering_algorithms = (\n        ('KMeans', km),\n        ('MeanShift', ms),\n        ('MiniBatchKMeans', two_means),\n        ('SpectralClustering', spectral),\n        ('DBSCAN', dbscan),\n        ('AffinityPropagation', affinity_propagation),\n        ('Birch', birch),\n        ('GaussianMixture', gmm)\n    )\n"},{"cell_type":"markdown","source":"Execute the clusters in a for loop","metadata":{}},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"collapsed":true},"source":"result=algorithm.predict(X)\nplot_num = 1 #for iteration\nfor name,algorithm in clustering_algorithms:\n    y_pred = result\n    y_pred = result\n    plt.subplot(4, 2, plot_num)\n    plt.scatter(X.iloc[:, 4], X.iloc[:, 5],c=result)\n    plt.title(name, size=12)\n    plot_num += 1\nplt.show()"},{"cell_type":"markdown","source":"**Plot the world map with K-Means cluster**","metadata":{}},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"collapsed":true},"source":"import plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True) \n\n# Read data\nwhdata=pd.read_csv(\"2017.csv\")\nwhdata = whdata.iloc[:, 2: ] \n\n# Instantiate scaler object\nss = StandardScaler()\n# Use ot now to 'fit' &  'transform'\nss.fit_transform(whdata)\n\nn_clusters = 2\nkm = cluster.KMeans(n_clusters =n_clusters )\nkm_result = km.fit_predict(whdata)\n\n#Make a copy of the data set\nwhdata_map = whdata\nwhdata_map.head(2)\nwhdata.insert(0,'Country',df.iloc[:,0])\nout=km_result\n\nplt.subplot(4, 2, 1)\nplt.scatter(whdata.iloc[:, 4], whdata.iloc[:, 5],  c=km_result)\n\nwhdata_map['clusters'] = out\ndata = dict(type = 'choropleth', \n           locations = whdata_map['Country'],\n           locationmode = 'country names',\n           z =  whdata_map['clusters'],\n           text = whdata_map['Country'],\n           colorbar = {'title':'Happiness'})\nlayout = dict(title = 'World Happiness Using K Means Clustering Method', \n             geo = dict(showframe = False, \n                       projection = {'type': 'Mercator'}))\nchoromap3 = go.Figure(data = [data], layout=layout)\niplot(choromap3)"},{"cell_type":"markdown","source":"\n","metadata":{}}],"nbformat":4,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","mimetype":"text/x-python","pygments_lexer":"ipython3","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.4","nbconvert_exporter":"python"}}}