{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Import everything"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\nimport matplotlib.pyplot as plt\n\n# regex\nimport re\n\n# merge paths\nimport os\n\nimport numpy as np \n\n# create datasets\nimport pandas as pd \n\n# to get file names in a folder \nfrom glob import glob\n\n# The Image module provides a class with the same name which is used to represent a PIL image. The module also provides a number of factory functions, including functions to load images from files, and to create new images.\nfrom PIL import Image\n\n# to save a model as a file \nfrom pickle import load, dump\n\n# one-hot encoding \nfrom keras.utils import to_categorical\n\n# It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape).\nfrom keras.layers.merge import add\n\n# Loads a model saved via model.save(), Model is the actual model \nfrom keras.models import Model, load_model\n\n# Dense: dense layers perform classification on the features extracted by the convolutional layers and down-sampled by the pooling layers. \n# LSTM: RNN may suffer from the vanishing gradient problem - LSTM solves this problem, LSTM knows what to store and what to throw away. \n# Dropout: The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged.\n# Embedding: Use to create our own word embeddings using a tokenizer \n# Input: is used to instantiate a Keras tensor.\nfrom keras.layers import Dense, LSTM, Dropout, Embedding, Input\n\n# Pads sequences to the same length.\nfrom keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define global variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"start = \"<start>\"\nend = \"<end>\"\npathToImageFolder = '/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/flickr30k_images/'\npathToImageCaptionCSV = '/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/results.csv'\n\n# training params \npictures_used_when_training = 10000\n\n# In terms of artificial neural networks, an epoch refers to one cycle through the full training dataset\nepochs = 10\n\n# dimentions of the pictures\nxdim = 299\nydim = 299","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define functions used when preprocessing data"},{"metadata":{},"cell_type":"markdown","source":"We use the pretrained CNN model \"Xception\": https://keras.io/api/applications/xception/\n\nQuote from webbsite: \nOptionally loads weights pre-trained on ImageNet. \nthe default input image size for this model is 299x299."},{"metadata":{"trusted":true},"cell_type":"code","source":"# returns a list with the name of all vectors and a dictionary with the name as key and caption as value\ndef fetchData(): \n    \n    # read the captions to a panda datastructure \n    captions = pd.read_csv(pathToImageCaptionCSV, delimiter='|')\n    captions.columns = ['imageName', 'captionNumber', 'caption']\n\n    # stores all the file names in a list \n    all_img_name_vector = []\n    \n    # stores all the captions in a dictionary with filenames as key \n    all_captions = {}\n    \n    # parse the panda and fill the structures above \n    for index, row in captions.head(n=pictures_used_when_training).iterrows():\n        caption = start + \" \" + row[2].replace(\".\", \"\").strip() + \" \" + end\n        im_ID = row[0]\n        all_img_name_vector.append(im_ID)\n        \n        if im_ID not in all_captions:\n            all_captions[im_ID] = []\n        all_captions[im_ID].append(caption)\n    \n    all_img_name_vector = list(set(all_img_name_vector))\n    return all_img_name_vector, all_captions\n\n# creates the feature dictionary (feature representation of every picture)\ndef createFeatures(img_name_vector):\n    \n    # create a features.p file if it does not exist\n    if not os.path.exists('features.p'):\n        \n        # load the pretrained Xception model \n        # the pooling layers downsomples the image data extracted by the convolutional layers to reduce the dimentionality of the feaature map inorder to decrease processing time \n        # include_top false to excklude the top layer \n        model = tf.keras.applications.Xception(pooling = 'avg', include_top = False)\n        \n        # dictionary used to store the features images \n        features = {}       \n        \n        # create the feature of each picture (represent the picture as a vector) using the CNN\n        for im_ID in img_name_vector:       \n            image = loadPicture(im_ID)    \n            \n            \n            feature = model.predict(image)\n            features[im_ID] = feature\n\n        # store the feature representation fo every picture in the file feature.p\n        dump(features, open(\"features.p\",\"wb\"))  \n\n    features = load(open(\"features.p\", \"rb\"))\n   \n    # only return the images that are in the name vector \n    features = {im_ID:features[im_ID] for im_ID in img_name_vector}\n    \n    return features\n\n# transform picture to standard dimention  (299x299)\ndef loadPicture(im_ID):\n    imagePath = pathToImageFolder + im_ID\n    image = Image.open(imagePath).resize((xdim,ydim))\n    image = np.expand_dims(image, axis=0) \n    return normalize(image) \n\n# normalising the values to -1 to 1 \ndef normalize(image):\n    return (image/127.5) - 1.0\n\n# create and return tokenizer \n# This class allows to vectorize a text corpus, by turning each text into a sequence of integers \n\ndef extractTokens(all_captions, num_words):\n    tkz = tf.keras.preprocessing.text.Tokenizer(num_words = num_words, oov_token = \"<unk>\", filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n    # Updates internal vocabulary based on a list of texts.\n    tkz.fit_on_texts(all_captions)\n    dump(tkz, open('tokenizer.p', 'wb'))\n    return tkz \n\n# flatten dictionary\ndef flatten(dictionary):\n    flat = list()\n    for item in dictionary.keys():\n        [flat.append(d) for d in dictionary[item]]\n    return flat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocess data"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_img_name, all_captions = fetchData()\n\n# feature representation of the images\nfeatures = createFeatures(all_img_name)\n\n# tokenizer\nflat_all_captions = flatten(all_captions)\ntkz = extractTokens(flat_all_captions, 5000)\nvocabSize = len(tkz.word_index) + 1\n\n# max length used for the caption length\ncaption_max_length = max(len(t.split()) for t in flat_all_captions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"print(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"tkz.get_config()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define functions used when building and training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build the RNN \ndef buildRNN(vocabSize, caption_max_length):\n\n    # specify the shape (the number of features that each input sample has) \n    # 2048 because that if the shape that Imagenet uses \n    # Output from Imagenets is the input for the RNN model \n    inputs1 = Input(shape=(2048,))\n    \n    # the dropout layer randomly sets input units to 0 with a frequency of 50%\n    # to avoid overfitting \n    fe1 = Dropout(0.5)(inputs1)\n    \n    # create the dense layer and use the relu as the activation function\n    # relu because we have many outputs? \n    fe2 = Dense(256, activation='relu')(fe1)\n    \n    # Create the LSTM layer\n    # shape of the caption input \n    inputs2 = Input(shape=(caption_max_length,))\n    se1 = Embedding(vocabSize, 256, mask_zero=True)(inputs2)\n    # drop 50% of all neurons \n    se2 = Dropout(0.5)(se1)\n    # the layer for the picture \n    se3 = LSTM(256)(se2)\n    \n    # Merg the two models \n    decoder1 = add([fe2, se3])\n    decoder2 = Dense(256, activation='relu')(decoder1)\n    outputs = Dense(vocabSize, activation='softmax')(decoder2)\n    \n    # tie it together [image, seq] [word]\n    RNN_model = Model(inputs=(inputs1, inputs2), outputs=outputs)\n    RNN_model.compile(loss='categorical_crossentropy', optimizer='adam')\n    \n    return RNN_model\n\n# generator is used so that we dont have to give all data to the model at once (that will exhaust the model)\ndef data_generator(all_captions, features, tkz, caption_max_length):\n    \n    while True:\n        for im_ID, captions in all_captions.items():   \n            \n            # vector representation of the picture \n            feature = features[im_ID][0]\n            \n            # create the sequence \n            input_image, input_sequence, output_word = createSequences(tkz, caption_max_length, captions, feature)\n            \n            yield ([input_image, input_sequence], output_word)\n            \ndef createSequences(tkz, caption_max_length, captions, feature):\n    X1 = []\n    X2 = []\n    y = []\n    \n    for caption in captions:\n        # transform text to vector using the tokenizor\n        seq = tkz.texts_to_sequences([caption])[0]\n        \n        # loop every word in the sequence \n        for i in range(1, len(seq)):\n            \n            # all the words in the caption until letter i\n            inputSequence = seq[:i]\n            \n            # pad so that it always is the same length\n            inputSequence = pad_sequences([inputSequence], maxlen = caption_max_length)[0]\n\n            # the new word \n            outputSequence = seq[i]\n            \n            # go from words to indexes \n            outputSequence = to_categorical([outputSequence], num_classes=vocabSize)[0]\n            \n            # append to inputs and outputs \n            X1.append(feature)\n            X2.append(inputSequence)\n            y.append(outputSequence)\n    \n    X1 = np.array(X1)\n    X2 = np.array(X2)\n    y = np.array(y)\n    return X1, X2 , y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build and train the model"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"model = buildRNN(vocabSize, caption_max_length)\nsteps_per_epoch = len(all_captions)\n\nfor i in range(epochs):\n    filename = \"model_\" + str(i) + \".h5\"\n\n    gen = data_generator(all_captions, features, tkz, caption_max_length)\n    model.fit_generator(gen, epochs = epochs, steps_per_epoch = steps_per_epoch, verbose = 1)\n    \n    model.save(filename)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load pre-saved model (so we dont have to train it every time)"},{"metadata":{"trusted":true},"cell_type":"code","source":"tkz = load(open(\"../input/longtraining/tokenizer.p\",\"rb\"))\nmodel = load_model('../input/longtraining/model_8.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define functions used when testing the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# takes an index and returns a word\ndef word_for_id(integer, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == integer:\n            return word    \n    return None\n    \ndef generate_caption(model, tkz, img_feature, caption_max_length):    \n    # first word is always the start symbol \n    caption = start\n    \n    # loop until we get \"<end>\" or reached the max length\n    for i in range(caption_max_length):\n        \n        # convert the text to a vector using the tokenizer\n        seq = tkz.texts_to_sequences([caption])[0]\n        \n        # transform to the same length \n        seq = pad_sequences([seq], maxlen = caption_max_length)\n        \n        # gives the probability of each word given the picture and the previous sequence\n        predicted_id_list = model.predict([img_feature, seq], verbose=0)\n        \n        # gives the index of the word with the highest probability\n        pred_id = np.argmax(predicted_id_list)\n        \n        # gives the word of the above index \n        word = word_for_id(pred_id, tkz)\n        \n        # bug-fix when a tokenizer that does not match the captions have been used \n        if word is None:\n            break\n            \n        caption += ' ' + word\n        \n        # end if the word is end or it it wants to write start again \n        if word == end or word == start:\n            break\n            \n    return caption\n\ndef test_model(model, tkz, im_ID):        \n    \n    # CNN model\n    xc_model = tf.keras.applications.Xception(pooling = 'avg', include_top = False)\n    \n    # create the vector representation of the image \n    image = loadPicture(im_ID)\n    feature = xc_model.predict(image)\n    \n    # generate caption\n    print(generate_caption(model, tkz, feature, caption_max_length))\n    \n    # display image \n    img = Image.open(pathToImageFolder + im_ID)\n    plt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_model(model, tkz, '1001633352.jpg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_model(model, tkz, '1005216151.jpg')\nall_captions['1005216151.jpg']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_model(model, tkz, '1009434119.jpg')\nall_captions['1009434119.jpg']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_model(model, tkz, '1021293940.jpg')\nall_captions['1021293940.jpg']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_model(model, tkz, '1022975728.jpg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_model(model, tkz, '1032122270.jpg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"captions = pd.read_csv(pathToImageCaptionCSV, delimiter='|')\ni = 0\nfor cap in captions.image_name:\n    i += 1\n    if i > 10000:\n        print(cap)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_model(model, tkz, '1714937792.jpg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_model(model, tkz, '1891331926.jpg')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}