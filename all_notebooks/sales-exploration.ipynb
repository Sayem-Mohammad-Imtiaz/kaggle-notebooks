{"cells":[{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"de4c27d2-9572-4653-82c8-60d588f8071c","_uuid":"43c8421e0a66064a3351f577bf77c17eab146e37"},"source":"import numpy as np\nimport pandas as pd\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\nimport matplotlib.pyplot as plt\nsales=pd.read_csv('../input/Sales_Transactions_Dataset_Weekly.csv')\nprint(sales.describe().T)\nprint(sales.head())"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"546e8bf9-a5f6-42a3-a9ac-32381ff34b4e","collapsed":true,"_uuid":"359930f89859500ffaedee8a1be049b8f7d671e1"},"source":"def dddraw(X_reduced,name):\n    import matplotlib.pyplot as plt\n    from mpl_toolkits.mplot3d import Axes3D\n    # To getter a better understanding of interaction of the dimensions\n    # plot the first three PCA dimensions\n    fig = plt.figure(1, figsize=(8, 6))\n    ax = Axes3D(fig, elev=-150, azim=110)\n    ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=Y,cmap=plt.cm.Paired)\n    titel=\"First three directions of \"+name \n    ax.set_title(titel)\n    ax.set_xlabel(\"1st eigenvector\")\n    ax.w_xaxis.set_ticklabels([])\n    ax.set_ylabel(\"2nd eigenvector\")\n    ax.w_yaxis.set_ticklabels([])\n    ax.set_zlabel(\"3rd eigenvector\")\n    ax.w_zaxis.set_ticklabels([])\n\n    plt.show()"},{"metadata":{"_cell_guid":"fa2f0238-786e-4766-ba66-03d386cf0660","_uuid":"2a48e6d6ba4b40d17a60ba6e3e92f7ba4f43bcb1"},"cell_type":"markdown","source":"Clustering the products\n---"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"e0d0385d-9b3d-493b-b3a5-9103001a8d5e","_uuid":"0e29167ce92ccf48c5fe8815e84b4ab1b6a83644"},"source":"from sklearn.decomposition import PCA, FastICA,SparsePCA,NMF, LatentDirichletAllocation,FactorAnalysis\nfrom sklearn.random_projection import GaussianRandomProjection,SparseRandomProjection\nfrom sklearn.cluster import KMeans,Birch\nimport statsmodels.formula.api as sm\nfrom scipy import linalg\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler,PolynomialFeatures\ndef rmsle(y_predicted, y_real):\n    return np.sqrt(np.mean(np.power(np.log1p(y_predicted)-np.log1p(y_real), 2)))\ndef procenterror(y_predicted, y_real):\n     return ( np.mean(np.abs(y_predicted-y_real) )/ np.mean(y_real) *100 ).round()\n\nn_col=50\nX = sales.drop(['Product_Code','W51','Normalized 51'],axis=1)\ndef rmsle(y_predicted, y_real):\n    return np.sqrt(np.mean(np.power(np.log1p(y_predicted)-np.log1p(y_real), 2)))\ndef procenterror(y_predicted, y_real):\n     return np.round( np.mean(np.abs(y_predicted-y_real) )/ np.mean(y_real) *100 ,1)\n\n    \nY=sales['W51']\nX=X.fillna(value=0)  #nasty NaN\n#scaler = MinMaxScaler()\n#scaler.fit(X)\n#X=scaler.transform(X)\n#poly = PolynomialFeatures(2)\n#X=poly.fit_transform(X)\n\n\nnames = [\n         'PCA',\n         'FastICA',\n         'Gauss',\n         'KMeans',\n         'SparsePCA',\n         'SparseRP',\n         'Birch',\n         'NMF',    \n       #  'LatentDietrich',    \n        ]\n\nclassifiers = [\n    \n    PCA(n_components=n_col),\n    FastICA(n_components=n_col),\n    GaussianRandomProjection(n_components=3),\n    KMeans(n_clusters=n_col),\n    #SparsePCA(n_components=n_col),\n    SparseRandomProjection(n_components=n_col, dense_output=True),\n    Birch(branching_factor=10, n_clusters=7, threshold=0.5),\n    NMF(n_components=n_col),    \n  #  LatentDirichletAllocation(n_topics=n_col),\n    \n]\ncorrection= [1,1,0,0,0,0,0,0,0]\n\ntemp=zip(names,classifiers,correction)\nprint(temp)\n\nfor name, clf,correct in temp:\n    Xr=clf.fit_transform(X,Y)\n    dddraw(Xr,name)\n    res = sm.OLS(Y,Xr).fit()\n    #print(res.summary())  # show OLS regression\n    #print(res.predict(Xr).round()+correct)  #show OLS prediction\n    #print('Ypredict',res.predict(Xr).round()+correct)  #show OLS prediction\n    \n    print('Ypredict',res.predict(Xr).round()+correct*Y.mean())  #show OLS prediction\n    print(name,'%error',procenterror(res.predict(Xr)+correct*Y.mean(),Y),'rmsle',rmsle(res.predict(Xr)+correct*Y.mean(),Y)) #\n    \n    \n    \n    "},{"metadata":{"_cell_guid":"700c9fd1-fa2b-48a0-a697-8da12579693d","_uuid":"3a7edcef09b97055fb3e991a7f62cb906466c44d"},"cell_type":"markdown","source":"Clustering the weeks\n---\n"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"6902ca05-75a8-46b6-803e-d9341f03d0f2","_uuid":"eff699de5f62517de62134c0d3bd2551a5349f3a"},"source":"n_col=4\nkolom=sales.Product_Code\nX = (sales.drop(['Product_Code'],axis=1).T)[:51] #sales 51weeks \nX.columns=kolom\n\ndef rmsle(y_predicted, y_real):\n    return np.sqrt(np.mean(np.power(np.log1p(y_predicted)-np.log1p(y_real), 2)))\ndef procenterror(y_predicted, y_real):\n     return np.round( np.mean(np.abs(y_predicted-y_real) )/ np.mean(y_real) *100 ,1)\n\n    \nY=X['P33']\nX=X.fillna(value=0)  #nasty NaN\n\nnames = [\n         'PCA',\n         'FastICA',\n         'Gauss',\n         'KMeans',\n         'SparsePCA',\n         'SparseRP',\n         'Birch',\n         'NMF',    \n       #  'LatentDietrich',    \n        ]\n\nclassifiers = [\n    \n    PCA(n_components=n_col),\n    FastICA(n_components=n_col),\n    GaussianRandomProjection(n_components=3),\n    KMeans(n_clusters=n_col),\n    #SparsePCA(n_components=n_col),\n    SparseRandomProjection(n_components=n_col, dense_output=True),\n    Birch(branching_factor=10, n_clusters=3, threshold=0.5),\n    NMF(n_components=n_col),    \n  #  LatentDirichletAllocation(n_topics=n_col),\n    \n]\ncorrection= [1,1,0,0,0,0,0,0,0]\n\ntemp=zip(names,classifiers,correction)\nprint(temp)\n\nfor name, clf,correct in temp:\n    Xr=clf.fit_transform(X,Y)\n    dddraw(Xr,name)\n    res = sm.OLS(Y,Xr).fit()\n    #print(res.summary())  # show OLS regression\n    #print(res.predict(Xr).round()+correct)  #show OLS prediction\n    #print('Ypredict',res.predict(Xr).round()+correct)  #show OLS prediction\n    \n    print('Ypredict',res.predict(Xr).round()+correct*Y.mean())  #show OLS prediction\n    print(name,'%error',procenterror(res.predict(Xr)+correct*Y.mean(),Y),'rmsle',rmsle(res.predict(Xr)+correct*Y.mean(),Y)) #\n    \n    \n    "},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{},"source":"from sklearn.linear_model import OrthogonalMatchingPursuit,RANSACRegressor,LogisticRegression,ElasticNetCV,HuberRegressor, Ridge, Lasso,LassoCV,Lars,BayesianRidge,SGDClassifier,LogisticRegressionCV,RidgeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import MinMaxScaler,PolynomialFeatures\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nparam_grid = {'C': [0.1,1, 10, 100, 1000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']}\n\n    \n# import some data to play with\n#X = df_new[df_new['split']==0]\nX = X.drop(['P33'],axis=1)\ndef rmsle(y_predicted, y_real):\n    return np.sqrt(np.mean(np.power(np.log1p(y_predicted)-np.log1p(y_real), 2)))\ndef procenterror(y_predicted, y_real):\n     return np.round( np.mean(np.abs(y_predicted-y_real) )/ np.mean(y_real) *100 ,1)\n\n#Y=df_new[df_new['split']==0]\n\n\n#X=X.replace([np.inf, -np.inf], np.nan).fillna(value=0)\n#print(X) #nasty NaN\n#scaler = MinMaxScaler()\n#scaler.fit(X)\n#X=scaler.transform(X)\n#poly = PolynomialFeatures(2)\n#X=poly.fit_transform(X)\n\n\nnames = [\n         #'ElasticNet',\n         'SVC',\n         'kSVC',\n         'KNN',\n         'DecisionTree',\n         'RandomForestClassifier',\n         #'GridSearchCV',\n         'HuberRegressor',\n         'Ridge',\n         'Lasso',\n         'LassoCV',\n         'Lars',\n         #'BayesianRidge',\n         'SGDClassifier',\n         'RidgeClassifier',\n         'LogisticRegression',\n         'OrthogonalMatchingPursuit',\n         #'RANSACRegressor',\n         ]\n\nclassifiers = [\n    #ElasticNetCV(cv=10, random_state=0),\n    SVC(),\n    SVC(kernel = 'rbf', random_state = 0),\n    KNeighborsClassifier(n_neighbors = 1),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators = 200),\n    #GridSearchCV(SVC(),param_grid, refit = True, verbose = 1),\n    HuberRegressor(fit_intercept=True, alpha=0.0, max_iter=100,epsilon=2.95),\n    Ridge(fit_intercept=True, alpha=0.0, random_state=0, normalize=True),\n    Lasso(alpha=0.05),\n    LassoCV(),\n    Lars(n_nonzero_coefs=10),\n    #BayesianRidge(),\n    SGDClassifier(),\n    RidgeClassifier(),\n    LogisticRegression(),\n    OrthogonalMatchingPursuit(),\n    #RANSACRegressor(),\n]\ncorrection= [0,0,0,0,0,0,0,0,0,0,0,0]\n\ntemp=zip(names,classifiers,correction)\nprint(temp)\n\nfor name, clf,correct in temp:\n    regr=clf.fit(X,Y)\n    #print( name,'% errors', abs(regr.predict(X)+correct-Y).sum()/(Y.sum())*100)\n    print(name,'%error',procenterror(regr.predict(X),Y),'rmsle',rmsle(regr.predict(X),Y))\n    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,f1_score, precision_score, recall_score\n\n    # Confusion Matrix\n    print(name,'Confusion Matrix')\n    print(confusion_matrix(Y, np.round(regr.predict(X) ) ) )\n    print('--'*40)\n\n    # Classification Report\n    print('Classification Report')\n    print(classification_report(Y,np.round( regr.predict(X) ) ))\n\n    # Accuracy\n    print('--'*40)\n    logreg_accuracy = round(accuracy_score(Y, np.round( regr.predict(X) ) ) * 100,2)\n    print('Accuracy', logreg_accuracy,'%')"}],"nbformat":4,"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"mimetype":"text/x-python","version":"3.6.1","pygments_lexer":"ipython3","name":"python","nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py"}},"nbformat_minor":1}