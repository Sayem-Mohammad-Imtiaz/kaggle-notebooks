{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* > # EE226 - Coding 2\n## Streaming algorithm & Locality Sensitive Hashing","metadata":{}},{"cell_type":"markdown","source":"### Streaming: DGIM","metadata":{}},{"cell_type":"markdown","source":"DGIM is an efficient algorithm in processing large streams. When it's infeasible to store the flowing binary stream, DGIM can estimate the number of 1-bits in the window. In this coding, you're given the *stream_data.txt* (binary stream), and you need to implement the DGIM algorithm to count the number of 1-bits. Write code and ask the problems below.","metadata":{}},{"cell_type":"markdown","source":"### Your task","metadata":{}},{"cell_type":"markdown","source":"1. Set the window size to 1000, and count the number of 1-bits in the current window.","metadata":{}},{"cell_type":"code","source":"# Your code here, you can add cells if necessary\n\n\n# the function method\nimport math\nimport time\nimport numpy as np\nfilename = \"../input/coding2/stream_data.txt\"\n\nbuckets = {}\nwindow_size = 1000\ncurrent_time = 0\nupdate_time = 1000# no larger than the window_size\nupdate_index = 0\n\nkeysnum = int(math.log(window_size, 2)) + 1\nkey_list = list()\n\n# initialize the buckets\nfor i in range(keysnum):\n    key = int(math.pow(2, i))\n    key_list.append(key)\n    buckets[key] = list()\n\n\ndef Update_buckets(inputdict, klist, numkeys):\n    for key in klist:\n        if len(inputdict[key]) > 2:\n            inputdict[key].pop(0)\n            tstamp = inputdict[key].pop(0)\n            if key != klist[-1]:\n                inputdict[key * 2].append(tstamp)\n        else:\n            break\n\ndef OutputResult(inputdict, klist, window_size):\n    cnt = 0\n    firststamp = 0\n\n    for key in klist:\n        if len(inputdict[key]) > 0:\n            firststamp = inputdict[key][0]\n\n    for key in klist:\n        for tstamp in inputdict[key]:\n            if tstamp != firststamp:\n                cnt += key\n            else:\n                cnt += 0.5 * key\n    print (\"The estimated number of 1 s by DGIM in the last %d bits: %d\" % (window_size, cnt))\n\nwith open(filename, 'r') as rfile:\n    start_time = time.time()\n    while True:\n        char = rfile.read(1)\n        #print (\"char\", char)\n        \n        if not char:# no more input\n            OutputResult(buckets, key_list, window_size)\n            print (\"The end of the document.\")\n            break\n        \n        #print (\"buckets.keys()\", buckets.keys())\n        if char == \"1\" or char == \"0\" :\n            current_time = (current_time + 1) % window_size\n            update_index = (update_index + 1) % update_time\n            for k in buckets.keys():\n                for itemstamp in buckets[k]:\n                    if itemstamp == current_time:# remove record which is out of the window\n                        buckets[k].remove(itemstamp)\n            if update_index == 0:\n                OutputResult(buckets, key_list, window_size)\n\n        if char == \"1\":# add it to the buckets\n            buckets[1].append(current_time)\n            Update_buckets(buckets, key_list, keysnum)\n        \n        \n\n    end_time = time.time()\n    time_spent_DGIM = end_time - start_time\n    print (\"time_spent_DGIM\",  time_spent_DGIM)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Write a function that accurately counts the number of 1-bits in the current window, and compare the difference between its running time and space and the DGIM algorithm.","metadata":{}},{"cell_type":"code","source":"# Your code here, you can add cells if necessary\n\n# Your code here, you can add cells if necessary\n\n# the normal method \nimport math\nfilename = \"../input/coding2/stream_data.txt\"\nwindow_size = 1000\ncurrent_time = 0\nupdateindex = 0\n\nsum_bit = 0\nkeep_num = np.zeros((window_size))\n\nwith open(filename, 'r') as rfile:\n    start_time = time.time()\n    while True:\n        char = rfile.read(1)\n        \n        if not char:# no more input\n            print (\"The number of 1 s in the last %d bits: %d\" % (current_time, sum_bit))\n            print (\"The end of the document.\")\n            break \n        if current_time==0 :\n            sum_bit = 0\n\n        if char == \"1\" or char == \"0\" :\n            keep_num_new = np.zeros((window_size))\n            for i in range(1, window_size):\n                keep_num_new[i] = keep_num[i-1]\n                keep_num_new[0] = int(char)\n\n            keep_num = keep_num_new\n            current_time = (current_time + 1) \n\n\n        if current_time == window_size :\n            sum_bit = 0\n            for i in range( window_size ):\n                sum_bit = sum_bit + keep_num[i]\n            print (\"The number of 1 s in the last %d bits: %d\" % (window_size, sum_bit))\n            current_time = 0\n\n    end_time = time.time()\n    time_spent_NORMAL = end_time - start_time\n    print (\"time_spent_NORMAL\", time_spent_NORMAL)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (\"time_spent_NORMAL =\", time_spent_NORMAL , \"s , while time_spent_DGIM =\", time_spent_DGIM, \"s \")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Locality Sensitive Hashing","metadata":{}},{"cell_type":"markdown","source":"The locality sensitive hashing (LSH) algorithm is efficient in near-duplicate document detection. In this coding, you're given the *docs_for_lsh.csv*, where the documents are processed into set of k-shingles (k = 8, 9, 10). *docs_for_lsh.csv* contains 201 columns, where column 'doc_id' represents the unique id of each document, and from column '0' to column '199', each column represents a unique shingle. If a document contains a shingle ordered with **i**, then the corresponding row will have value 1 in column **'i'**, otherwise it's 0. You need to implement the LSH algorithm and ask the problems below.","metadata":{}},{"cell_type":"markdown","source":"### Your task","metadata":{}},{"cell_type":"markdown","source":"Use minhash algoirthm to create signature of each document, and find 'the most similar' documents under Jaccard similarity. \nParameters you need to determine:\n1) Length of signature (number of distinct minhash functions) *n*. Recommanded value: n > 20.\n\n2) Number of bands that divide the signature matrix *b*. Recommanded value: b > n // 10.","metadata":{}},{"cell_type":"code","source":"# Your code here, you can add cells if necessary\n\n# input the document \nimport pandas as pd\nimport numpy as np\ndf=pd.read_csv('../input/coding2/docs_for_lsh.csv',sep=',') \nprint (df.head())\nprint (df.tail())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (\"df['doc_id'] \", df['198'])\ndoc_num = df.shape[1] - 1\nprint (\"doc_num\", doc_num)\n# print (\"doc_num\", df['198'][999998])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the min_hashing function\ndef min_hashing(df,  sig_length):\n    doc_num = df.shape[0]\n    print (\"doc_num\", doc_num)\n    shingle_num = df.shape[1] - 1\n    print (\"shingle_num\", shingle_num)\n    signature = np.zeros((sig_length, doc_num))\n    for i in range(sig_length):\n        index = np.arange(shingle_num)\n        np.random.shuffle(index)\n        #print (\"index\", index)\n        for j in range(doc_num):  # j_th document\n            if np.mod(j, 500000) == 0 :\n                print (\"j / 500000\", j / 500000)\n            for k in range(shingle_num): \n                tmp_index = index[k]\n                tmp_search = df[str(tmp_index)][j]\n                if tmp_search == 1:\n                    signature[i, j] = k\n                    #print (\"k\", k)\n                    break\n        print (\"i in range(sig_length)\", i)\n        print (\"signature[i,:]\", signature[i,:])\n    return signature\n\n# the hashing for the band\ndef hash_band(signature, element_band, hash_interval):\n    sig_length = signature.shape[0]\n    doc_num = signature.shape[1]\n    band_num = int (sig_length / element_band)\n    print (band_num)\n    hash_result = np.zeros((band_num, doc_num))\n    \n    for i in range(band_num):\n        print (\"band_num = \", i)\n        for j in range(doc_num):\n            tmp_signature = signature[i * element_band:i * element_band + element_band, j ]\n            tmp_sum = np.sum(tmp_signature)\n            tmp_hash_result = int(tmp_sum / hash_interval )\n#             print (\"tmp_signature\", tmp_signature, \"j = \", j )\n#             print (\"tmp_hash_result\", tmp_hash_result)\n            hash_result [i,j] = tmp_hash_result\n    return hash_result\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sig_length = 45  # the length of the sig_length\nelement_band = 5  # the length of the band\nhash_interval = 3  # the interval of the hashing\n\n# establish the signature\nsignature = min_hashing(df,  sig_length)\nprint (\"signature\",signature )\nprint (\"signature\",signature )\nprint (\"signature\",signature[:, 0] )\n\n# get the band hashing result\nhash_result =  hash_band(signature, element_band, hash_interval)\nprint (\"hash_result\", hash_result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_similar_document(hash_result, compare_index, top_k ):\n    search_num = len(compare_index)\n    similar_document_matrix = np.zeros((search_num, top_k ))\n    for i in range(search_num):\n        compare_doc = compare_index[i]\n        tmp_index = hash_result[:, compare_doc]\n        compare_matrix = np.zeros_like(hash_result)\n        for j in range(compare_matrix.shape[1]):\n            compare_matrix[:, j] = (hash_result[:, j]  == tmp_index)\n            #print (\"compare_matrix[:, j]\", compare_matrix[:, j], \" j = \", j)\n        # caculate the Jaccard_sim\n        Jaccard_sim = np.sum(compare_matrix, axis = 0) / compare_matrix.shape[0]\n#         print (\"Jaccard_sim.shape\", Jaccard_sim.shape)\n        sim_doc = Jaccard_sim.argsort()[::-1]\n#         print (\"sim_doc.shape\", sim_doc.shape)\n        print (sim_doc[0:31])\n        print (Jaccard_sim[sim_doc[0:31]])\n        similar_document_matrix[i,:] = sim_doc[1:31]\n        print (\" compare_doc = \", compare_doc ,  \" similar_document_matrix = \", similar_document_matrix[i,:])\n    return similar_document_matrix\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the result\ncompare_index = [0, 3] # searching document \ntop_k = 30 \nsimilar_document_matrix = get_similar_document(hash_result, compare_index , top_k)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# examine the result\nfrom sklearn import metrics\ndata = (np.loadtxt(\"../input/coding2/docs_for_lsh.csv\", delimiter=',', skiprows=1, usecols=range(1,201)))\n\n\nresult = np.zeros((top_k))\ncompare_doc = 0\nfor i in range(top_k):\n    compare = int (similar_document_matrix[compare_doc][i])\n    #print (\"compare\", compare)\n    result[i] = metrics.jaccard_score(data[compare,:],data[compare_doc,:])\n    print(metrics.jaccard_score(data[compare,:],data[compare_doc,:]))\n\nprint (\"mean effect = \", np.mean(result))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Problem: For document 0 (the one with id '0'), list the **30** most similar document ids (except document 0 itself). You can valid your results with the [sklearn.metrics.jaccard_score()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html) function.\n\nTips: You can adjust your parameters to hash the documents with similarity *s > 0.8* into the same bucket.","metadata":{}},{"cell_type":"code","source":"# Your code here, you can add cells if necessary\n\n# the result\ncompare_index = [0]\ntop_k = 30 \nsimilar_document_matrix = get_similar_document(hash_result, compare_index , top_k)\n# print (\"similar_document_matrix\", similar_document_matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# examine the result\nfrom sklearn import metrics\n# data = (np.loadtxt(\"../input/coding2/docs_for_lsh.csv\", delimiter=',', skiprows=1, usecols=range(1,201)))\n\ncompare_doc = 0\nfor i in range(top_k):\n    compare = int (similar_document_matrix[compare_doc][i])\n    result[i] = metrics.jaccard_score(data[compare,:],data[0,:])\n    print(metrics.jaccard_score(data[compare,:],data[0,:]))\n\nprint (\"mean effect = \", np.mean(result))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}