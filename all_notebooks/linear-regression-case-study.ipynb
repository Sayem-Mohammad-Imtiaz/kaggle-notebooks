{"cells":[{"metadata":{},"cell_type":"markdown","source":"# LINEAR REGRESSION","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Table of Content\n\n1. [Problem Statement](#section1)<br>\n2. [Data Loading and Description](#section2)<br>\n3. [Exploratory Data Analysis](#section3)<br>\n4. [Training and testing the Model](#section4)<br>\n    - 4.1 [Splitting data into training and test datasets](#section401)<br>\n    - 4.2 [Linear regression in scikit-learn](#section402)<br>\n    - 4.3 [Interpreting Model Coefficients](#section403)<br>\n    - 4.4 [Using the Model for Prediction](#section404)<br>\n    \n5. [Model evaluation](#section5)<br>\n    - 5.1 [Model evaluation using metrics](#section501)<br>\n6. [Handling Categorical Features](#section6)<br>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=section1></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1. Problem Statement\n\n__Sales__ (in thousands of units) for a particular product as a __function__ of __advertising budgets__ (in thousands of dollars) for _TV, radio, and newspaper media_. Suppose that in our role as __Data Scientist__ we are asked to suggest.\n\n- We want to find a function that given input budgets for TV, radio and newspaper __predicts the output sales__.\n\n- Which media __contribute__ to sales?\n\n- Visualize the __relationship__ between the _features_ and the _response_ using scatter plots.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=section2></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 2. Data Loading and Description\n\nThe adverstising dataset captures sales revenue generated with respect to advertisement spends across multiple channels like radio, tv and newspaper.\n- TV        - Spend on TV Advertisements\n- Radio     - Spend on radio Advertisements\n- Newspaper - Spend on newspaper Advertisements\n- Sales     - Sales revenue generated","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"__Importing Packages__","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nfrom sklearn import metrics\n\nimport numpy as np\n\n# allow plots to appear directly in the notebook\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Importing the Dataset","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/advertising.csv/Advertising.csv', index_col=0)\ndata.head(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What are the **features**?\n- TV: advertising dollars spent on TV for a single product in a given market (in thousands of dollars)\n- Radio: advertising dollars spent on Radio\n- Newspaper: advertising dollars spent on Newspaper\n\nWhat is the **response**?\n- Sales: sales of a single product in a given market (in thousands of widgets)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=section3></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 3. Exploratory Data Analysis","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 200 **observations**, and thus 200 markets in the dataset.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"__Distribution of Features__","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"f, axes = plt.subplots(2, 2, figsize=(15, 7), sharex=False)            # Set up the matplotlib figure\nsns.despine(left=True)\n\nsns.distplot(data.sales, color=\"b\", ax=axes[0, 0])\n\nsns.distplot(data.TV, color=\"r\", ax=axes[0, 1])\n\nsns.distplot(data.radio, color=\"g\", ax=axes[1, 0])\n\nsns.distplot(data.newspaper, color=\"m\", ax=axes[1, 1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Is there a relationship between sales and spend various advertising channels?","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"JG1 = sns.jointplot(\"newspaper\", \"sales\", data=data, kind='reg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"JG2 = sns.jointplot(\"radio\", \"sales\", data=data, kind='reg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"JG3 = sns.jointplot(\"TV\", \"sales\", data=data, kind='reg')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Observation__<br/>\n_Sales and spend on newpaper_ is __not__ highly correlaed where are _sales and spend on tv_ is __highly correlated__.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Visualising Pairwise correlation","execution_count":null},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"sns.pairplot(data, height = 2, aspect = 1.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.pairplot(data, x_vars=['TV', 'radio', 'newspaper'], y_vars='sales', size=5, aspect=1, kind='reg')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Observation__\n\n- Strong relationship between TV ads and sales\n- Weak relationship between Radio ads and sales\n- Very weak to no relationship between Newspaper ads and sales\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Calculating and plotting heatmap correlation","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"data.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(7,5))\nsns.heatmap(round(data.corr(),2),annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Observation__\n\n- The diagonal of the above matirx shows the auto-correlation of the variables. It is always 1. You can observe that the correlation between __TV and Sales is highest i.e. 0.78__ and then between __sales and radio i.e. 0.576__.\n\n- correlations can vary from -1 to +1. Closer to +1 means strong positive correlation and close -1 means strong negative correlation. Closer to 0 means not very strongly correlated. variables with __strong correlations__ are mostly probably candidates for __model builing__.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=section4></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 4. Training and Testing the Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=section401></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 4.1 Splitting data into training and test datasets. \"WHY?\"","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"features = ['TV', 'radio', 'newspaper']                # create a Python list of feature names\ntarget = ['sales']                                     # Define the target variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data[features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data[target]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data[features], data[target], test_size=0.05, random_state=5000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Train cases as below')\nprint('X_train shape: ',X_train.shape)\nprint('y_train shape: ',y_train.shape)\nprint('\\nTest cases as below')\nprint('X_test shape: ',X_test.shape)\nprint('y_test shape: ',y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"y_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=section402></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 4.2 Linear regression in scikit-learn","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"To apply any machine learning algorithm on your dataset, basically there are 4 steps:\n1. Load the algorithm\n2. Instantiate and Fit the model to the training dataset\n3. Prediction on the test set\n4. Evaluate - Calculate RMSE and R square\n\nThe code block given below shows how these steps are carried out:<br/>\n\n``` from sklearn.linear_model import LinearRegression\n    lr_model = LinearRegression()\n    ll_model.fit(X_train, y_train) \n    RMSE_test = np.sqrt(metrics.mean_squared_error(y_test, y_pred_test))\n    r2_test = metrics.r2_score(y_test, y_pred_test)```","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"#Instantiating the model\nfrom sklearn.linear_model import LinearRegression\nlr_model = LinearRegression(fit_intercept=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"lr_model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=section403></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 4.3 Interpreting Model Coefficients","execution_count":null},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"print('Intercept:',lr_model.intercept_)          # print the intercept \nprint('Coefficients:',lr_model.coef_)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"(lr_model.coef_).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pd.DataFrame((lr_model.coef_).T,index=X_train.columns,\\\n             columns=['Co-efficients']).sort_values('Co-efficients',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__y = 2.9 + 0.0468 `*` TV + 0.1785 `*` radio + 0.00258 `*` newspaper__","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"How do we interpret the TV coefficient (0.0468)\n- A \"unit\" increase in TV ad spending is **associated with** a _\"0.0468_ unit\" increase in Sales.\n- Or more clearly: An additional $1,000 spent on TV ads is **associated with** an increase in sales of ~0.0468 * 1000 = 47 widgets.\n\nImportant Notes:\n- This is a statement of __association__, not __causation__.\n- If an increase in TV ad spending was associated with a __decrease__ in sales,  β1  would be __negative.__","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=section404></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 4.4 Using the Model for Prediction","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred_train = lr_model.predict(X_train)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred_train                                                         # make predictions on the training set","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred_test = lr_model.predict(X_test)                                  # make predictions on the testing set","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"y_pred_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We need an evaluation metric in order to compare our predictions with the actual values.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=section5></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 5. Model evaluation ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"__Error__ is the _deviation_ of the values _predicted_ by the model with the _true_ values.<br/>\nFor example, if a model predicts that the price of apple is Rs75/kg, but the actual price of apple is Rs100/kg, then the error in prediction will be Rs25/kg.<br/>\nBelow are the types of error we will be calculating for our _linear regression model_:\n- Mean Absolute Error\n- Mean Squared Error\n- Root Mean Squared Error","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=section501></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 5.1 Model Evaluation using __metrics.__","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"__Mean Absolute Error__ (MAE) is the mean of the absolute value of the errors:\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\nComputing the MAE for our Sales predictions","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"MAE_train = metrics.mean_absolute_error(y_train, y_pred_train)\nMAE_test = metrics.mean_absolute_error(y_test, y_pred_test)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"print('MAE for training set is {}'.format(MAE_train))\nprint('MAE for test set is {}'.format(MAE_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Mean Squared Error__ (MSE) is the mean of the squared errors:\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n\nComputing the MSE for our Sales predictions","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"MSE_train = metrics.mean_squared_error(y_train, y_pred_train)\nMSE_test = metrics.mean_squared_error(y_test, y_pred_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('MSE for training set is {}'.format(MSE_train))\nprint('MSE for test set is {}'.format(MSE_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Root Mean Squared Error__ (RMSE) is the square root of the mean of the squared errors:\n\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n\nComputing the RMSE for our Sales predictions","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"RMSE_train = np.sqrt( metrics.mean_squared_error(y_train, y_pred_train))\nRMSE_test = np.sqrt(metrics.mean_squared_error(y_test, y_pred_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('RMSE for training set is {}'.format(RMSE_train))\nprint('RMSE for test set is {}'.format(RMSE_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data['sales'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"RMSE_test/data['sales'].mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comparing these metrics:\n- __RMSE__ is more popular than MSE, because RMSE is _interpretable_ in the \"y\" units.\n    - Easier to put in context as it's the same units as our response variable.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=section6></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 6.  Handling Categorical Features\n\nLet's create a new feature called **Area**, and randomly assign observations to be **rural, suburban, or urban** :","execution_count":null},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"np.random.seed(123456)                                                # set a seed for reproducibility\nnums = np.random.rand(len(data))\nmask_suburban = (nums > 0.33) & (nums < 0.66)                         # assign roughly one third of observations to each group\nmask_urban = nums > 0.66\ndata['Area'] = 'rural'\ndata.loc[mask_suburban, 'Area'] = 'suburban'\ndata.loc[mask_urban, 'Area'] = 'urban'\n\ndata.head(50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Some EDA of categorical variable","execution_count":null},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"data.groupby(['Area'])['sales'].mean().sort_values(ascending=False).plot(kind = 'bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"a = sns.scatterplot(x=\"TV\", y=\"sales\", data=data, hue='Area')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"a = sns.scatterplot(x=\"Area\", y=\"sales\", data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#data.to_csv(\"data_with_area.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We want to represent Area numerically, but we can't simply code it as:<br/>\n- 0 = rural,<br/>\n- 1 = suburban,<br/>\n- 2 = urban<br/>\nBecause that would imply an **ordered relationship** between suburban and urban, and thus urban is somehow \"twice\" the suburban category.<br/> Note that if you do have ordered categories (i.e., strongly disagree, disagree, neutral, agree, strongly agree), you can use a single dummy variable to represent the categories numerically (such as 1, 2, 3, 4, 5).<br/>\n\nAnyway, our Area feature is unordered, so we have to create **additional dummy variables**. Let's explore how to do this using pandas:","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"features = ['TV', 'radio', 'newspaper', 'Area']\ncat_cols = ['Area']                                           # Define the categorical variables","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data_with_dummies = pd.get_dummies(data, columns=cat_cols, drop_first=True)\ndata_with_dummies.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is how we interpret the coding:\n- **rural** is coded as  Area_suburban = 0  and  Area_urban = 0\n- **suburban** is coded as  Area_suburban = 1  and  Area_urban = 0\n- **urban** is coded as  Area_suburban = 0  and  Area_urban = 1\n\nIf this sounds confusing, think in general terms that why we need only __k-1 dummy variables__ if we have a categorical feature with __k \"levels\"__.\n\nAnyway, let's add these two new dummy variables onto the original DataFrame, and then include them in the linear regression model.","execution_count":null},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"feature_cols = ['TV', 'radio', 'newspaper', 'Area_suburban', 'Area_urban']             # create a Python list of feature names\nX = data_with_dummies[feature_cols]  \ny = data_with_dummies.sales\nlr_model_cat = LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"lr_model_cat.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred_cat = lr_model_cat.predict(X)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pd.DataFrame((lr_model_cat.coef_).T,index=X.columns,\\\n             columns=['Co-efficients']).sort_values('Co-efficients',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"print('Intercept:',lr_model_cat.intercept_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__y =  2.92 + 0.045802 `*` TV + 0.1876 `*` radio - 0.001018 `*` newspaper - 0.117890 `*` Area_suburban + 0.2535 `*` Area_urban__<br/>\nHow do we interpret the coefficients?<br/>\n- Holding all other variables fixed, being a **suburban** area is associated with an average **decrease** in Sales of 0.1178 widgets (as compared to the baseline level, which is rural).\n- Being an **urban** area is associated with an average **increase** in Sales of 0.2535 widgets (as compared to rural).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"ORIGINAL EQUATION\n\n__y = 2.9 + 0.0468 `*` TV + 0.1785 `*` radio + 0.00258 `*` newspaper__","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"data_with_dummies['predictions'] = y_pred_cat","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"data_with_dummies","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data_with_dummies['error'] = data_with_dummies['sales'] - data_with_dummies['predictions']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data_with_dummies['error'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data_with_dummies.plot.scatter(x='sales', y='predictions',\\\n                      figsize=(8,5), grid=True, title='Actual vs Predicted')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"sns.distplot(data_with_dummies['error'])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"data_with_dummies[data_with_dummies['error']<-4]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"data_with_dummies.plot.scatter(x='sales', y='error',\\\n                      figsize=(8,5), grid=True, title='Actual vs Predicted')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data_with_dummies.to_csv('data_with_predictions.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=section8></a>","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}