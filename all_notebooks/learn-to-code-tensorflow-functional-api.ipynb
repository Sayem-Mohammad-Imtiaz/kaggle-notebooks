{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Input\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.__version__","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using Energy Dataset from UCI"},{"metadata":{"trusted":true},"cell_type":"code","source":"file = '../input/eergy-efficiency-dataset/ENB2012_data.csv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### In the above sample data, we have inputs from X1,X2..X8 and we have two outputs Y1 and Y2.In the Multi Head Networks we essentially will have multiple outputs with the same sets of inputs"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the data into train and test with 80 train / 20 test\ntrain, test = train_test_split(df, test_size=0.2)\ntrain_stats = train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_stats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have data with different distributions and data ranges are differnt. So, obvious step here is to make the data normalize/standardise"},{"metadata":{"trusted":true},"cell_type":"code","source":"def format_output(data):\n    y1 = data.pop('Y1')\n    y1 = np.array(y1)\n    y2 = data.pop('Y2')\n    y2 = np.array(y2)\n    return y1, y2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get Y1 and Y2 as the 2 outputs and format them as np arrays\ntrain_stats.pop('Y1')\ntrain_stats.pop('Y2')\ntrain_stats = train_stats.transpose()\ntrain_Y = format_output(train)\ntest_Y = format_output(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def norm(x):\n    return (x - train_stats['mean']) / train_stats['std']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalize the training and test data\nnorm_train_X = norm(train)\nnorm_test_X = norm(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build the Model"},{"metadata":{},"cell_type":"markdown","source":"## We have three stages in building the model in functional API\n1. Declare Inputs\n2. Core Model Architecture (except inuput)\n3. Model Object with inptuts and  Outputs"},{"metadata":{},"cell_type":"markdown","source":"### Declare Input Layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"input = Input(shape=(len(train.columns),))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Core Model Architecture"},{"metadata":{"trusted":true},"cell_type":"code","source":"L1 = Dense(units='128', activation='relu')(input)\nL2 = Dense(units='128', activation='relu')(L1)\nL3 = Dense(units='64', activation='relu')(L2)\n\n# Y1 output will be fed directly from the second dense\ny1_output = Dense(units='1', name='y1_output')(L2)\n\n# Y2 output will come via the third dense\ny2_output = Dense(units='1', name='y2_output')(L3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can observe y1_output & y2_output has differnt no of intermediate layers, this is where the functional API is useful. And, each layers input is seperately mentioned as a parameter in the paranthesis after declaring the layer. For example, L3 can fed from L1 also just by changing the L1 in the paranthesis."},{"metadata":{},"cell_type":"markdown","source":"### Declaring Model Object"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Model(inputs=input, outputs=[y1_output, y2_output])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you have multiple inputs or outputs just pass them as a list to the model object"},{"metadata":{},"cell_type":"markdown","source":"### See the Architecture"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Compile the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = tf.keras.optimizers.SGD(lr=0.001)\nmodel.compile(optimizer=optimizer,\n              loss={'y1_output': 'mse', 'y2_output': 'mse'},\n              metrics={'y1_output': tf.keras.metrics.RootMeanSquaredError(),\n                       'y2_output': tf.keras.metrics.RootMeanSquaredError()})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(norm_train_X, train_Y,\n                    epochs=500, batch_size=10, validation_data=(norm_test_X, test_Y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, Y1_loss, Y2_loss, Y1_rmse, Y2_rmse = model.evaluate(x=norm_test_X, y=test_Y)\nprint(\"Loss = {}, Y1_loss = {}, Y1_mse = {}, Y2_loss = {}, Y2_mse = {}\".format(loss, Y1_loss, Y1_rmse, Y2_loss, Y2_rmse))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_diff(y_true, y_pred, title=''):\n    plt.scatter(y_true, y_pred)\n    plt.title(title)\n    plt.xlabel('True Values')\n    plt.ylabel('Predictions')\n    plt.axis('equal')\n    plt.axis('square')\n    plt.xlim(plt.xlim())\n    plt.ylim(plt.ylim())\n    plt.plot([-100, 100], [-100, 100])\n    plt.show()\n\n\ndef plot_metrics(metric_name, title, ylim=5):\n    plt.title(title)\n    plt.ylim(0, ylim)\n    plt.plot(history.history[metric_name], color='blue', label=metric_name)\n    plt.plot(history.history['val_' + metric_name], color='green', label='val_' + metric_name)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the loss and mse\nY_pred = model.predict(norm_test_X)\nplot_diff(test_Y[0], Y_pred[0], title='Y1')\nplot_diff(test_Y[1], Y_pred[1], title='Y2')\nplot_metrics(metric_name='y1_output_root_mean_squared_error', title='Y1 RMSE', ylim=6)\nplot_metrics(metric_name='y2_output_root_mean_squared_error', title='Y2 RMSE', ylim=7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### You can observe the true and predicted values are very close and the loss is very minimal "},{"metadata":{},"cell_type":"markdown","source":"### These are plots of loss from epoch to epoch during the trainnig process"},{"metadata":{},"cell_type":"markdown","source":"Credits to Laurence Moroney,Coursera"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}