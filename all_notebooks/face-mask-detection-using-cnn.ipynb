{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport cv2\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras import layers\nimport tensorflow as tf\nfrom keras.preprocessing.image import img_to_array\nfrom tensorflow.keras import backend\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"Most of the given data set images contain more than one person,for for our model we need area cover by face(with mask or without mask) only not the entire image. We use the csv data set to crop only the required part and than load it, in the train, test and valid data frames."},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_set(dir_data):\n    data=[]\n    target=[]\n    data_map = {\n    'with_mask':1,\n    'without_mask':0\n    }\n    skipped=0\n    root=dir_data+'_annotations.csv'\n    df1 = pd.read_csv(root)\n    df1.dataframeName = '_annotations.csv'\n    nRow, nCol = df1.shape\n    for i in range(len(df1)):\n        without_mask='without_mask'\n        k=dir_data+df1['filename'][i]\n        image=cv2.imread(k)\n        xmin=int(df1['xmin'][i])\n        ymin=int(df1['ymin'][i])\n        xmax=int(df1['xmax'][i])\n        ymax=int(df1['ymax'][i])\n        image=image[ymin:ymax,  xmin:xmax]\n        try:\n                # resizing to (70 x 70)\n                image = cv2.resize(image,(70,70))\n        except Exception as E:\n                skipped += 1\n                print(E)\n                continue\n        if(df1['class'][i]=='mask'):\n            without_mask='with_mask'\n        image=img_to_array(image)\n        data.append(image)\n        target.append(data_map[without_mask])\n    data = np.array(data, dtype=\"float\") / 255.0\n    target = tf.keras.utils.to_categorical(np.array(target), num_classes=2)\n    return data, target\ntraining_data, training_target=data_set('/kaggle/input/face-mask-detection/train/')\ntesting_data, testing_target=data_set('/kaggle/input/face-mask-detection/test/')\nvalid_data, valid_target=data_set('/kaggle/input/face-mask-detection/valid/')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nLet's take a quick look at what the data looks like:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(0, figsize=(100,100))\nfor i in range(1,10):\n    plt.subplot(10,5,i)\n    plt.imshow(training_data[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we check the format of the images\nchannels_last=(row,col,channels)\nchannels_first=(channel,row,col)"},{"metadata":{"trusted":true},"cell_type":"code","source":"img_shape=training_data[0].shape\ndepth, height, width=3, img_shape[0], img_shape[1]\nimg_shape=(height, width, depth)\nchanDim=-1\nif backend.image_data_format() == \"channels_first\": #Returns a string, either 'channels_first' or 'channels_last'\n        img_shape = (depth, height, width)\n        chanDim = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Building Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Sequential()\nmodel.add(layers.Conv2D(32,(3,3),input_shape=img_shape))\nmodel.add(layers.MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(layers.Conv2D(64,(3,3)))\nmodel.add(layers.Activation('relu'))\nmodel.add(layers.MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(layers.Conv2D(128,(3,3)))\nmodel.add(layers.Activation('relu'))\nmodel.add(layers.MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(layers.Conv2D(256,(3,3)))\nmodel.add(layers.Activation('relu'))\nmodel.add(layers.MaxPooling2D(pool_size=(2,2)))\n\n\nmodel.add(layers.Flatten())\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(64,activation='relu'))\nmodel.add(layers.Dropout(0.4))\n\nmodel.add(layers.Dense(2,activation='softmax'))\n\nadam =tf.keras.optimizers.Adam(0.001)\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"check the summary of model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we apply data augumentation for training our model with more data set which produce by modifying the same data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# augmenting dataset \naug = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,\n                         height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n                         horizontal_flip=True, fill_mode=\"nearest\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we are ready to train our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(aug.flow(training_data, training_target, batch_size=10),\n                   epochs=70,\n                   validation_data=(valid_data, valid_target),\n                   verbose=2,\n                   shuffle=True) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's plot a graph between accuracy of training and validation data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.ylabel(['accuracy'])\nplt.xlabel(['epoch'])\nplt.legend(['accuracy', 'val_accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's plot a graph between loss of training and validation data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.ylabel(['loss'])\nplt.xlabel(['epoch'])\nplt.legend(['loss', 'val_loss'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Find loss and accuracy of our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, accuracy = model.evaluate(testing_data,testing_target)\nprint('accuracy= ',loss,\" loss= \",loss)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's use our model on the testing data and get the report of our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"yhat = model.predict(testing_data)\ntest_pred=np.argmax(yhat,axis=1)\ntesting_target=np.argmax(testing_target,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nimport sklearn.metrics as metrics\nimport itertools\nreport = classification_report(testing_target, test_pred)\nprint(report)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"confusion matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.RdYlGn):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion = metrics.confusion_matrix(testing_target, test_pred)\nplt.figure()\nplot_confusion_matrix(confusion, classes=['without_mask','with_mask'], title='Confusion matrix')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}