{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Context"},{"metadata":{},"cell_type":"markdown","source":"League of Legends is a MOBA (multiplayer online battle arena) where 2 teams (blue and red) face off. There are 3 lanes, a jungle, and 5 roles. The goal is to take down the enemy Nexus to win the game."},{"metadata":{},"cell_type":"markdown","source":"### Content"},{"metadata":{},"cell_type":"markdown","source":"This dataset contains the first 10min. stats of approx. 10k ranked games (SOLO QUEUE) from a high ELO (DIAMOND I to MASTER). Players have roughly the same level.\n\nEach game is unique. The gameId can help you to fetch more attributes from the Riot API.\n\nThere are 19 features per team (38 in total) collected after 10min in-game. This includes kills, deaths, gold, experience, levelâ€¦ It's up to you to do some feature engineering to get more insights.\n\nThe column blueWins is the target value (the value we are trying to predict). A value of 1 means the blue team has won. 0 otherwise.\n\nSo far I know, there is no missing value"},{"metadata":{},"cell_type":"markdown","source":"### Glossary"},{"metadata":{},"cell_type":"markdown","source":"- Warding totem: An item that a player can put on the map to reveal the nearby area. Very useful for map/objectives control.\n- Minions: NPC that belong to both teams. They give gold when killed by players.\n- Jungle minions: NPC that belong to NO TEAM. They give gold and buffs when killed by players.\n- Elite monsters: Monsters with high hp/damage that give a massive bonus (gold/XP/stats) when killed by a team.\n- Dragons: Elite monster which gives team bonus when killed. The 4th dragon killed by a team gives a massive stats bonus. The - - 5th dragon (Elder Dragon) offers a huge advantage to the team.\n- Herald: Elite monster which gives stats bonus when killed by the player. It helps to push a lane and destroys structures.\n- Towers: Structures you have to destroy to reach the enemy Nexus. They give gold.\n- Level: Champion level. Start at 1. Max is 18."},{"metadata":{},"cell_type":"markdown","source":"### Importing major libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_csv('../input/league-of-legends-diamond-ranked-games-10-min/high_diamond_ranked_10min.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the shape of the data\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Basic EDA & Preprocessing the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking null values\ndata.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking data types of the columns\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking for quasi constants\ndata.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As seen above, there is no column with 1 or same value throughout"},{"metadata":{},"cell_type":"markdown","source":"#### Importing required libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import RFE\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=data.drop(['blueWins', 'gameId'], axis=1)\ny=data['blueWins']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dropping gameID column as it is only an ID and has different value for each row"},{"metadata":{},"cell_type":"markdown","source":"#### Splitting the data into train and test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Method 1: Feature Selection using different methods and checking with different models"},{"metadata":{},"cell_type":"markdown","source":"## `Feature Selection using Feature importance of Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"sel_rf=SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=1))\n\nsel_rf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sel_rf.get_support()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, lot of features have been set as False depicting that they are not as important as other features"},{"metadata":{},"cell_type":"markdown","source":"#### How many features remain after above procedure"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total number of features in the database: \", len(X_train.columns))\nprint(\"Total number of features after removing according to RF feature importances: \", sel_rf.get_support().sum())\nprint(\"Total features removed: \", int(len(X_train.columns)-sel_rf.get_support().sum()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's transformed the data now and check the accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_rfc=sel_rf.transform(X_train)\nX_test_rfc=sel_rf.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the shape of the data now to confirm that they have 16 features now\nX_train_rfc.shape, X_test_rfc.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's create a function with RandomForest and Gradient Boost Classifier, once we find the best classifier, we can further fine tune it using hyperparameter tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"def classifier_model(X_train, X_test, y_train, y_test, method, data):\n    rf_clf=RandomForestClassifier(n_estimators=1000, random_state=1)\n    rf_clf.fit(X_train, y_train)\n    y_pred_rf=rf_clf.predict(X_test)\n    score_rlf=accuracy_score(y_test, y_pred_rf)\n    print(\"---Feature Selection method: {}---\". format(method))\n    print(\"---Checking Accuracy with {}---\".format(data))\n    print(\"The accuracy score of Random Forest:\", score_rlf)\n    \n    \n    gb_clf=GradientBoostingClassifier(n_estimators=1000, random_state=1)\n    gb_clf.fit(X_train, y_train)\n    y_pred_gb=gb_clf.predict(X_test)\n    score_gb=accuracy_score(y_test, y_pred_gb)\n    print(\"The accuracy score of Gradient Boosting:\", score_rlf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Accuracy with Reduced features"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier_model(X_train_rfc, X_test_rfc, y_train, y_test, \"Random Forest Feature importance\", \"Reduced Features\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Accuracy with all features"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier_model(X_train, X_test, y_train, y_test, \"Random Forest Feature importance\", \"All Features\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### As you can see above, accuracy has reduced after feature removal, hence let's check some other method to reduce the feature space"},{"metadata":{},"cell_type":"markdown","source":"## Feature Selection using Recursive feature extraction (RFE)"},{"metadata":{"trusted":true},"cell_type":"code","source":"sel_rfe=RFE(RandomForestClassifier(n_estimators=100, random_state=1),n_features_to_select=20)\nsel_rfe.fit(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Total features selected:\nsel_rfe.get_support().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Let's transform the data now;\nX_train_rfe=sel_rfe.transform(X_train)\nX_test_rfe=sel_rfe.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's run the classifiers now"},{"metadata":{},"cell_type":"markdown","source":"### Accuracy with reduced features"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier_model(X_train_rfe, X_test_rfe, y_train, y_test, \"Recursive feature extraction with RF\", \"Reduced Features\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Accuracy with All features"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier_model(X_train, X_test, y_train, y_test, \"Recursive feature extraction with RF\", \"All Features\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Recursive Feature extraction using Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"sel_rfe_gb=RFE(GradientBoostingClassifier(n_estimators=100, random_state=1), n_features_to_select=22)\nsel_rfe_gb.fit(X_train, y_train)\n\nX_train_rfe_gb=sel_rfe_gb.transform(X_train)\nX_test_rfe_gb=sel_rfe_gb.transform(X_test)\n\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's run the model\n"},{"metadata":{},"cell_type":"markdown","source":"### Accuracy with reduced features"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier_model(X_train_rfe_gb, X_test_rfe_gb, y_train, y_test, \"Recursive feature extraction with GB\", \"Reduced Features\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gradient boosting algorithm had the highest accuracy. Now let's check how many number of features will give the best accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"for index in range(14,39):\n    sel_rfe_gb=RFE(GradientBoostingClassifier(n_estimators=100, random_state=1), n_features_to_select=index)\n    sel_rfe_gb.fit(X_train, y_train)\n\n    X_train_rfe_gb=sel_rfe_gb.transform(X_train)\n    X_test_rfe_gb=sel_rfe_gb.transform(X_test)\n    \n    clf_gb=GradientBoostingClassifier(n_estimators=200, random_state=1)\n    clf_gb.fit(X_train_rfe_gb, y_train)\n    y_pred_gb=clf_gb.predict(X_test_rfe_gb)\n    score_gb=accuracy_score(y_test, y_pred_gb)\n    print(\"Number of features: \", index)\n    print(\"Accuracy: \", score_gb)\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### It is clear from above that best selection of features are 16:"},{"metadata":{},"cell_type":"markdown","source":"#### Now transforming the data with 16 features only and then running on different models to select the best model"},{"metadata":{"trusted":true},"cell_type":"code","source":"sel_rfe_gb_new=RFE(GradientBoostingClassifier(n_estimators=1000, random_state=1), n_features_to_select=16)\nsel_rfe_gb_new.fit(X_train, y_train)\n\nX_train_final=sel_rfe_gb_new.transform(X_train)\nX_test_final=sel_rfe_gb_new.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GRADIENT BOOST CLASSIFIER"},{"metadata":{},"cell_type":"markdown","source":"### Checking with reduced and important features"},{"metadata":{"trusted":true},"cell_type":"code","source":"gb_clf_1=GradientBoostingClassifier(n_estimators=400, random_state=1)\n\ngb_clf_1.fit(X_train_final, y_train)\ny_pred_gb_1=gb_clf_1.predict(X_test_final)\n\nscore_gb_1=accuracy_score(y_test, y_pred_gb_1)\n\nprint(\"Accuracy:\" ,score_gb_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params_grid_gb={'n_estimators' : [100,200,400,600,1000,1200],\n                'min_samples_split': [100,200,300,400],\n                'min_samples_leaf' : [10,20,30,40,60,100],\n                'max_depth' : [2,4,6,8],\n                'learning_rate' : [0.01, 0.05, 0.1, 0.5, 1, 5, 10]\n               }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\ngridsearch_gb=RandomizedSearchCV(estimator=GradientBoostingClassifier(), param_distributions=params_grid_gb, cv=5, scoring='accuracy')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gridsearch_gb.fit(X_train_final, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gridsearch_gb.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gridsearch_gb.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Checking accuracy on Test set\ny_pred_final_gb=gridsearch_gb.predict(X_test_final)\n\nprint(\"Accuracy of GBM with accuracy_scoreced features on test set\", accuracy_score(y_test, y_pred_final_gb))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### checking the model with all features"},{"metadata":{"trusted":true},"cell_type":"code","source":"gridsearch_gb.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gridsearch_gb.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gridsearch_gb.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Checking accuracy on Test set\ny_pred_final_gb_all=gridsearch_gb.predict(X_test)\n\nprint(\"Accuracy of GBM with all features on test set\", accuracy_score(y_test, y_pred_final_gb_all))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### As we see that maximum accuracy achieved was , Now let's do some feature engineering to further improve the accuracy"},{"metadata":{},"cell_type":"markdown","source":"# Method 2: Performing feature Engineering and checking with different models now"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's calculate the difference of values b/w Blue and red teams in all the columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols=[x[4:] for x in data.columns if \"blue\" in x and x[4:]!= 'Wins']\ncols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are the columns which require to be differenced b/w Blue and Red teams"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Below columns to be dropped  because they are already the difference of blue and red\ncols_to_drop=['GoldDiff', 'ExperienceDiff']\nfinal_cols=[x for x in cols if x not in cols_to_drop]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_new=pd.DataFrame()\n\nfor col in final_cols:\n    data_new[f'Diff_{col}'] =data[f'blue{col}']-data[f'red{col}']\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Keeping values corresponding to only Red in ['GoldDiff', 'ExperienceDiff'] i.e redGoldDiff and redExperienceDiff\nfor col_ in cols_to_drop:\n    data_new[col_]=data[f'red{col_}']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_new.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now split the dataset into train and test\nX_new=data_new\ny_new=data['blueWins']\nX_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_new, y_new, test_size=0.2, random_state=1, stratify=y)   \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Create 2 datasets tuples in order to run the model easily on new dataset ( feature engineered) and old dataset( original)\n\n#Originaldata\ndataset_1=(X_train, X_test, y_train, y_test, 'dataset_1')\n\n#Featureengineered data\ndataset_2=(X_train_new, X_test_new, y_train_new, y_test_new, 'dataset_2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Create a function to test different classifiers"},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_classifier(model, dataset):\n    model.fit(dataset[0], dataset[2])\n    y_pred=model.predict(dataset[1])\n    score_=accuracy_score(dataset[3], y_pred)\n    return f'{round(score_, 4)*100}%'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### A quick run on different algorithms"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_dict={ 'Decision Tree' : DecisionTreeClassifier(max_depth=6,random_state=1),\n            'Random Forest' : RandomForestClassifier(n_estimators=100, random_state=1),\n           'Support Vector Classification': SVC(random_state=1), \n           'Gaussian Naive Bayes': GaussianNB(),\n           'Gradient Boosting Classifier': GradientBoostingClassifier(random_state=1),\n           'XG Boost Classifier': XGBClassifier()\n                 \n          }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### On original dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"for model in model_dict:\n    print(f'model:{model} -accuracy: {run_classifier(model_dict[model],dataset_1)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### On feature Engineered dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"for model in model_dict:\n    print(f'model:{model} -accuracy: {run_classifier(model_dict[model],dataset_2)}')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}