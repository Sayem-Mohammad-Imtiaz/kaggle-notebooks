{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pandas --quiet\n!pip install torchtext --quiet","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:15:55.026342Z","iopub.status.idle":"2021-08-19T06:15:55.027069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n\nfrom collections import Counter\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn import metrics\n\nimport torchtext\nfrom torchtext.data import get_tokenizer\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:15:55.028173Z","iopub.status.idle":"2021-08-19T06:15:55.028903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_device():\n  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n  if device != \"cuda\":\n    print(\"WARNING: For this notebook to perform best, \"\n          \"if possible, in the menu under `Runtime` -> \"\n          \"`Change runtime type.`  select `GPU` \")\n  else:\n    print(\"GPU is enabled in this notebook.\")\n\n  return device","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:15:55.029994Z","iopub.status.idle":"2021-08-19T06:15:55.030697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the device (check if gpu is available)\ndevice = set_device()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:15:55.031775Z","iopub.status.idle":"2021-08-19T06:15:55.032509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/english-contractions/df_processed.csv', delimiter=',')\n\n# Let's have a look at it\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:15:55.033746Z","iopub.status.idle":"2021-08-19T06:15:55.034558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Remove some classess: happiness, hate, fun, love, surprise**","metadata":{}},{"cell_type":"code","source":"indexForRemove = df[ (df['sentiment'] == 'happiness') | \n                    (df['sentiment'] == 'hate') | \n                    (df['sentiment'] == 'fun') | \n                    (df['sentiment'] == 'love') | \n                    (df['sentiment'] == 'surprise') | \n                    (df['sentiment'] == 'worry') ].index\nprint(len(indexForRemove))\ndf.drop(index=indexForRemove , inplace=True)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:15:55.035809Z","iopub.status.idle":"2021-08-19T06:15:55.036536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VOC_CLASSES = df['sentiment'].unique()\nLEN_CLASSES = len(VOC_CLASSES)\nencoding2label = dict(enumerate(VOC_CLASSES))\nlabel2encoding = {value: key for key, value in encoding2label.items()}\n\nprint(VOC_CLASSES, LEN_CLASSES)\nprint(encoding2label)\nprint(label2encoding)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:15:55.037638Z","iopub.status.idle":"2021-08-19T06:15:55.038387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.content.values\ny = [label2encoding[l] for l in df.sentiment.values]\nprint(len(X), len(y))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:15:55.039483Z","iopub.status.idle":"2021-08-19T06:15:55.04022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data into train and test\nx_train_text, x_test_text, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:15:55.041304Z","iopub.status.idle":"2021-08-19T06:15:55.042214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenaizer sentences\nthis part will be change if use BERT","metadata":{}},{"cell_type":"code","source":"for x, y in zip(x_train_text[:5], y_train[:5]):\n  print('{}: {}'.format(encoding2label[y], x))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:15:55.043344Z","iopub.status.idle":"2021-08-19T06:15:55.044075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = get_tokenizer(\"basic_english\")\n\nprint('Before Tokenize: ', x_train_text[1])\nprint('After Tokenize: ', tokenizer(x_train_text[1]))\nx_train_token = [tokenizer(s) for s in tqdm(x_train_text)]\nx_test_token = [tokenizer(s) for s in tqdm(x_test_text)]","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:15:55.04529Z","iopub.status.idle":"2021-08-19T06:15:55.046098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words = Counter()\nfor s in x_train_token:\n  for w in s:\n    words[w] += 1\n\nsorted_words = list(words.keys())\nsorted_words.sort(key=lambda w: words[w], reverse=True)\nprint(f\"Number of different Tokens in our Dataset: {len(sorted_words)}\")\nprint(sorted_words[:100])","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:15:55.047177Z","iopub.status.idle":"2021-08-19T06:15:55.047902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_occurences = sum(words.values())\n\naccumulated = 0\ncounter = 0\n\nwhile accumulated < count_occurences * 0.8:\n  accumulated += words[sorted_words[counter]]\n  counter += 1\n\nprint(f\"The {counter * 100 / len(words)}% most common words \"\n      f\"account for the {accumulated * 100 / count_occurences}% of the occurrences\")\nplt.bar(range(100), [words[w] for w in sorted_words[:100]])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:15:55.048999Z","iopub.status.idle":"2021-08-19T06:15:55.049702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_words_dict = 30000\n# We reserve two numbers for special tokens.\nmost_used_words = sorted_words[:num_words_dict-2]","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:15:55.050799Z","iopub.status.idle":"2021-08-19T06:15:55.051493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dictionary to go from words to idx \nword_to_idx = {}\n# dictionary to go from idx to words (just in case) \nidx_to_word = {}\n\n\n# We include the special tokens first\nPAD_token = 0   \nUNK_token = 1\n\nword_to_idx['PAD'] = PAD_token\nword_to_idx['UNK'] = UNK_token\n\nidx_to_word[PAD_token] = 'PAD'\nidx_to_word[UNK_token] = 'UNK'\n\n# We popullate our dictionaries with the most used words\nfor num,word in enumerate(most_used_words):\n  word_to_idx[word] = num + 2\n  idx_to_word[num+2] = word","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:15:55.052563Z","iopub.status.idle":"2021-08-19T06:15:55.053274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A function to convert list of tokens to list of indexes\ndef tokens_to_idx(sentences_tokens,word_to_idx):\n  sentences_idx = []\n  for sent in sentences_tokens:\n    sent_idx = []\n    for word in sent:\n      if word in word_to_idx:\n        sent_idx.append(word_to_idx[word])\n      else:\n        sent_idx.append(word_to_idx['UNK'])\n    sentences_idx.append(sent_idx)\n  return sentences_idx","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:15:55.054512Z","iopub.status.idle":"2021-08-19T06:15:55.055332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train_idx = tokens_to_idx(x_train_token,word_to_idx)\nx_test_idx = tokens_to_idx(x_test_token,word_to_idx)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:15:55.056494Z","iopub.status.idle":"2021-08-19T06:15:55.057204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"some_number = 1\nprint('Before converting: ', x_train_token[some_number])\nprint('After converting: ', x_train_idx[some_number])","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:15:55.058263Z","iopub.status.idle":"2021-08-19T06:15:55.059017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweet_lens = np.asarray([len(sentence) for sentence in x_train_idx])\nprint('Max tweet word length: ',tweet_lens.max())\nprint('Mean tweet word length: ',np.median(tweet_lens))\nprint('99% percent under: ',np.quantile(tweet_lens,0.99))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:15:55.060122Z","iopub.status.idle":"2021-08-19T06:15:55.060843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We choose the max length\nmax_length = 40\n\n# A function to make all the sequence have the same lenght\n# Note that the output is a Numpy matrix\ndef padding(sentences, seq_len):\n  features = np.zeros((len(sentences), seq_len),dtype=int)\n  for ii, tweet in enumerate(sentences):\n    len_tweet = len(tweet) \n    if len_tweet != 0:\n      if len_tweet <= seq_len:\n        # If its shorter, we fill with zeros (the padding Token index)\n        features[ii, -len(tweet):] = np.array(tweet)[:seq_len]\n      if len_tweet > seq_len:\n        # If its larger, we take the last 'seq_len' indexes\n        features[ii, :] = np.array(tweet)[-seq_len:]\n  return features","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:15:55.061918Z","iopub.status.idle":"2021-08-19T06:15:55.062611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We convert our list of tokens into a numpy matrix\n# where all instances have the same lenght\nx_train_pad = padding(x_train_idx,max_length)\nx_test_pad = padding(x_test_idx,max_length)\n\n# We convert our target list a numpy matrix\ny_train_np = np.asarray(y_train)\ny_test_np = np.asarray(y_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:15:55.063701Z","iopub.status.idle":"2021-08-19T06:15:55.064461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"some_number = 3\nprint('Before padding: ', x_train_idx[some_number])\nprint('After padding: ', x_train_pad[some_number])","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:15:55.065738Z","iopub.status.idle":"2021-08-19T06:15:55.066475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define dataloader and model","metadata":{}},{"cell_type":"code","source":"# create Tensor datasets\ntrain_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(y_train_np))\ntest_data = TensorDataset(torch.from_numpy(x_test_pad), torch.from_numpy(y_test_np))\n\n# Batch size (this is an important hyperparameter)\nbatch_size = 100\n\n# dataloaders\n# make sure to SHUFFLE your data\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last = True)\ntest_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last = True)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:15:55.067542Z","iopub.status.idle":"2021-08-19T06:15:55.068252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Obtain one batch of training data\ndataiter = iter(train_loader)\nsample_x, sample_y = dataiter.next()\n\nprint('x size: ', sample_x.size(), 'Y size', sample_y.size()) # batch_size, seq_length\n# print('Sample input: \\n', sample_x)\n# print('Sample output: \\n', sample_y)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:15:55.069318Z","iopub.status.idle":"2021-08-19T06:15:55.070085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SentimentRNN(nn.Module):\n    def __init__(self, no_layers, vocab_size, hidden_dim, embedding_dim, output_dim, drop_prob=0.55):\n        super(SentimentRNN, self).__init__()\n\n        self.hidden_dim = hidden_dim\n        self.no_layers = no_layers\n\n        # Embedding Layer\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n\n        # LSTM Layers\n        self.lstm = nn.LSTM(input_size=embedding_dim,\n                            hidden_size=hidden_dim,\n                            num_layers=no_layers, \n                            batch_first=True, \n                            dropout=drop_prob,\n                            bidirectional=True\n                            )\n                                  \n\n        # Dropout layer\n        self.dropout = nn.Dropout(drop_prob)\n\n        # Linear and Sigmoid layer\n        # why not no_layers*hidden_dim? and for biLSTM x2?\n        self.fc = nn.Linear(2*hidden_dim, output_dim)\n#         self.sig = nn.Sigmoid()\n    \n    def forward(self, x, hidden):\n        batch_size = x.size(0)\n\n        # Embedding out\n        embeds = self.embedding(x)\n        #Shape: [batch_size x max_length x embedding_dim]\n\n        # LSTM out\n        lstm_out, hidden = self.lstm(embeds, hidden)\n        # Shape: [batch_size x max_length x hidden_dim]\n\n        # Select the activation of the last Hidden Layer\n        lstm_out = lstm_out[:,-1,:].contiguous()\n        # Shape: [batch_size x hidden_dim]\n\n         ## You can instead average the activations across all the times\n        # lstm_out = torch.mean(lstm_out, 1).contiguous()\n\n        # Dropout and Fully connected layer\n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n\n        # Sigmoid function\n#         sig_out = self.sig(out)\n\n        # return last sigmoid output and hidden state\n        return out, hidden\n\n    def init_hidden(self, batch_size):\n        ''' Initializes hidden state '''\n        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n        # initialized to zero, for hidden state and cell state of LSTM\n        #random initialization is better, i think \n        h0 = torch.randn((2*self.no_layers, batch_size, self.hidden_dim)).to(device)\n        c0 = torch.randn((2*self.no_layers, batch_size, self.hidden_dim)).to(device)\n        hidden = (h0, c0)\n        return hidden\n","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:15:55.071237Z","iopub.status.idle":"2021-08-19T06:15:55.072013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's define our model\nmodel = SentimentRNN(no_layers=3,\n                     vocab_size=num_words_dict, # 43800 dictionary\n                     hidden_dim=64,\n                     embedding_dim=32,\n                     output_dim=LEN_CLASSES,    # 13 classes\n                     drop_prob=0.55\n                    )\n\n\n# Moving to gpu\nmodel.to(device)\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:15:55.07308Z","iopub.status.idle":"2021-08-19T06:15:55.073855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How many trainable parameters does our model have?\nmodel_parameters = filter(lambda p: p.requires_grad, model.parameters())\nparams = sum([np.prod(p.size()) for p in model_parameters])\nprint('Total Number of parameters: ',params)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:15:55.075052Z","iopub.status.idle":"2021-08-19T06:15:55.07588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loss and optimization functions\nlr = 0.0001\n\ncriterion = nn.CrossEntropyLoss().to(device)\n\n# We choose an Adam optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n# function to predict accuracy\ndef acc(pred, labels):\n    pred = torch.argmax(pred, dim=1)\n#     labels = torch.argmax(labels, dim=1)\n    return torch.sum((pred == labels).float())","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:15:55.076958Z","iopub.status.idle":"2021-08-19T06:15:55.077656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of training Epochs\nepochs = 25\n# Maximum absolute value accepted for the gradeint\nclip = 5\n\n# Initial Loss value (assumed big)\nvalid_loss_min = np.Inf\n\n# Lists to follow the evolution of the loss and accuracy\nepoch_tr_loss, epoch_vl_loss = [], []\nepoch_tr_acc, epoch_vl_acc = [], []\n\n# Train for a number of Epochs\nfor epoch in range(epochs):\n  train_losses = []\n  train_acc = 0.0\n  model.train()\n\n  for inputs, labels in train_loader:\n    # Initialize hidden state \n    h = model.init_hidden(batch_size)\n    \n    # Creating new variables for the hidden state\n    h = tuple([each.data.to(device) for each in h])\n    \n    # Move batch inputs and labels to gpu\n    inputs, labels = inputs.to(device), labels.to(device)   \n\n    # Set gradient to zero\n    model.zero_grad()\n    \n    # Compute model output\n    output, h = model(inputs, h)\n\n    # Calculate the loss and perform backprop\n    loss = criterion(output.squeeze(), labels.long())\n    loss.backward()\n    train_losses.append(loss.item())\n    \n    # calculating accuracy\n    accuracy = acc(output,labels)\n    train_acc += accuracy\n\n    #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n    nn.utils.clip_grad_norm_(model.parameters(), clip)\n    optimizer.step()\n    \n  # Evaluate on the validation set for this epoch \n  val_losses = []\n  val_acc = 0.0\n  model.eval()\n    \n  for inputs, labels in test_loader:\n    \n    # Initialize hidden state \n    val_h = model.init_hidden(batch_size)\n    val_h = tuple([each.data.to(device) for each in val_h])\n\n    # Move batch inputs and labels to gpu\n    inputs, labels = inputs.to(device), labels.to(device)\n\n    # Compute model output\n    output, val_h = model(inputs, val_h)\n\n    # Compute Loss\n    val_loss = criterion(output.squeeze(), labels.long())\n\n    val_losses.append(val_loss.item())\n\n    accuracy = acc(output, labels)\n    val_acc += accuracy\n\n  epoch_train_loss = np.mean(train_losses)\n  epoch_val_loss = np.mean(val_losses)\n  epoch_train_acc = train_acc/len(train_loader.dataset)\n  epoch_val_acc = val_acc/len(test_loader.dataset)\n  epoch_tr_loss.append(epoch_train_loss)\n  epoch_vl_loss.append(epoch_val_loss)\n  epoch_tr_acc.append(epoch_train_acc)\n  epoch_vl_acc.append(epoch_val_acc)\n  print(f'Epoch {epoch+1}') \n  print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n  print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n  if epoch_val_loss <= valid_loss_min:\n    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n    # torch.save(model.state_dict(), '../working/state_dict.pt')\n    valid_loss_min = epoch_val_loss\n  print(25*'==')","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:15:55.078775Z","iopub.status.idle":"2021-08-19T06:15:55.079491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize = (20, 6))\nplt.subplot(1, 2, 1)\nplt.plot(epoch_tr_acc, label='Train Acc')\nplt.plot(epoch_vl_acc, label='Validation Acc')\nplt.title(\"Accuracy\")\nplt.legend()\nplt.grid()\n\nplt.subplot(1, 2, 2)\nplt.plot(epoch_tr_loss, label='Train loss')\nplt.plot(epoch_vl_loss, label='Validation loss')\nplt.title(\"Loss\")\nplt.legend()\nplt.grid()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:15:55.080551Z","iopub.status.idle":"2021-08-19T06:15:55.081267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n Plotting results ... \\n\")\nplot_graphs(history, 'accuracy')\nplot_graphs(history, 'loss')\nprint(\"\\n Evaluating Model ... \\n\")\npredicted = model_RNN.predict_classes(X_test_Glove)\n#print(predicted)\nprint(metrics.classification_report(y_test, predicted))\n# print(\"\\n\")\n# logger = logging.getLogger(\"logger\")\n# result = compute_metrics(y_test, predicted)\n# for key in (result.keys()):\n#   logger.info(\"  %s = %s\", key, str(result[key]))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:15:55.082357Z","iopub.status.idle":"2021-08-19T06:15:55.083073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###Utility\nfrom sklearn import metrics\n\ndef get_eval_report(labels, preds):\n    mcc = matthews_corrcoef(labels, preds)\n    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n    precision = (tp)/(tp+fp)\n    recall = (tp)/(tp+fn)\n    f1 = (2*(precision*recall))/(precision+recall)\n    return {\n        \"mcc\": mcc,\n        \"tp\": tp,\n        \"tn\": tn,\n        \"fp\": fp,\n        \"fn\": fn,\n        \"pricision\" : precision,\n        \"recall\" : recall,\n        \"F1\" : f1,\n        \"accuracy\": (tp+tn)/(tp+tn+fp+fn)\n    }\n\ndef compute_metrics(labels, preds):\n    assert len(preds) == len(labels)\n    return get_eval_report(labels, preds)\n\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string], '')\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n\ndef class_balance(df, target):\n  cls = df[target].value_counts()\n  cls.plot(kind='bar')\n  plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:15:55.084148Z","iopub.status.idle":"2021-08-19T06:15:55.084902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n Plotting results ... \\n\")\nplot_graphs(history, 'accuracy')\nplot_graphs(history, 'loss')\nprint(\"\\n Evaluating Model ... \\n\")\npredicted = model.predict_classes(x_test_pad)\n#print(predicted)\nprint(metrics.classification_report(y_test_np, predicted))\n# print(\"\\n\")\n# logger = logging.getLogger(\"logger\")\n# result = compute_metrics(y_test, predicted)\n# for key in (result.keys()):\n#   logger.info(\"  %s = %s\", key, str(result[key]))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:15:55.086056Z","iopub.status.idle":"2021-08-19T06:15:55.086741Z"},"trusted":true},"execution_count":null,"outputs":[]}]}