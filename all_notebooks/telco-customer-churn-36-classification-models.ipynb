{"cells":[{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"# Telco Customer Churn Classification\n- Assignment 2\n- [Chris Tan](https://www.linkedin.com/in/christan/)\n- 4/28/2019"},{"metadata":{},"cell_type":"markdown","source":"In this notebook, I performed binary classification on [Telco Customer Churn](https://www.kaggle.com/blastchar/telco-customer-churn#WA_Fn-UseC_-Telco-Customer-Churn.csv) from Kaggle. The data set includes information about:\n- Customers who left within the last month – the column is called Churn\n- Services that each customer has signed up for – phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies\n- Customer account information – how long they’ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges\n- Demographic info about customers – gender, age range, and if they have partners and dependents"},{"metadata":{"toc-hr-collapsed":false},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"## Import Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n%matplotlib inline\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os as os\nimport sys\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# From Scikit Learn\nfrom sklearn import preprocessing, decomposition, tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import scale\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix, classification_report\nfrom astropy.table import Table, Column\n# Set DEBUG = True to produce debug results\nDEBUG = False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check Python Version"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The Python version is %s.%s.%s.\" % sys.version_info[:3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check Present Working Directory"},{"metadata":{"trusted":true},"cell_type":"code","source":"%pwd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import Dataset"},{"metadata":{},"cell_type":"markdown","source":"Using the [pandas.read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) method, I read the **WA_Fn-UseC_-Telco-Customer-Churn.csv** dataset and stored this in the **churn** DataFrame."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nchurn = pd.read_csv(\"../input/WA_Fn-UseC_-Telco-Customer-Churn.csv\", header=0, sep=\",\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html) method, I generated descriptive statistics that summarize the central tendency, dispersion and shape of a dataset’s distribution, excluding NaN values. This helps me analyzes both numeric and object series, as well as DataFrame column sets of mixed data types."},{"metadata":{"trusted":true},"cell_type":"code","source":"churn.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the [pandas.DataFrame.dropna](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html) method with the **how = 'all'** parameter, I removed all observations where all features were **NaN**."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html\nchurn = churn.dropna(axis = 0, how = 'all')\nif DEBUG:\n    #Dimensions of dataset\n    print(\"Shape of Data\", churn.shape)\n    #Colum names\n    print(\"Colums Names\", churn.columns)\n    #See bottol few rows of dataset\n    print(churn.tail())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Target Feature Designation"},{"metadata":{},"cell_type":"markdown","source":"Using the code below, I set the **churn** feature as the target feature and moved this to the beginning of my DataFrame."},{"metadata":{"trusted":true},"cell_type":"code","source":"# designate target variable name\ntargetName = 'Churn'\ntargetSeries = churn[targetName]\n#remove target from current location and insert in collum 0\ndel churn[targetName]\nchurn.insert(0, targetName, targetSeries)\n#reprint dataframe and see target is in position 0\nchurn.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html), I printed a concise summary of the **churn** DataFrame. This method prints information about a DataFrame including the index dtype and column dtypes, non-null values and memory usage."},{"metadata":{"trusted":true},"cell_type":"code","source":"churn.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the information above, we can see that we have a combination of **object** and numerical (**int64** and **float64**) data types in our dataframe. We will need to convert the **object** data types to dummy variables or drop them from the data frame depending on their predictive value."},{"metadata":{},"cell_type":"markdown","source":"In the **churn** dataset, we have two types of customers: those who stayed and those who left the telecom service. We can divide the data into two groups and compare their characteristics. Here, you can find the average of both the groups using the groupby() and mean() functions."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Basic bar chart since the target is binominal\ngroupby = churn.groupby(targetName)\ntargetEDA=groupby[targetName].aggregate(len)\nplt.figure()\ntargetEDA.plot(kind='bar', grid=False)\nplt.axhline(0, color='k')\nplt.title('Bar Chart of Churn')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"groupby.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the table above, we can see some statistics on those customers who stayed vs. those who left. On average, those left (churm = yes) had higher **MonthlyCharges** and lower **tenure**."},{"metadata":{"toc-hr-collapsed":false},"cell_type":"markdown","source":"## Cross-Tabulation of Paperless Billing, Contract, and Payment Method with Churn"},{"metadata":{},"cell_type":"markdown","source":"Using the [pandas.crosstab](https://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.crosstab.html) function, I computed a simple cross-tabulation of two (or more) factors. By default, this computes a frequency table of the factors unless an array of values and an aggregation function are passed."},{"metadata":{},"cell_type":"markdown","source":"### Churn by Paperless Billing using [pandas.DataFrame.plot](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.crosstab.html\nchurn_ac_cross = pd.crosstab(churn['PaperlessBilling'], churn['Churn'])\nif DEBUG:\n    print(churn_ac_cross)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html\nchurn_ac_cross.plot(kind='bar', stacked=True)\nplt.title('Churn by Paperless Billing')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the chart above, we can see that customers who have paperless billing are more likely to churn."},{"metadata":{},"cell_type":"markdown","source":"### Churn by Contract using [pandas.DataFrame.plot](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.crosstab.html\nchurn_ac_cross = pd.crosstab(churn['Contract'], churn['Churn'])\nif DEBUG:\n    print(churn_ac_cross)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html\nchurn_ac_cross.plot(kind='bar', stacked=True)\nplt.title('Churn by Contract')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the chart above, we can see that customers who have a short-term (month-to-month) contract are more likely to churn than those who have one or two year contracts."},{"metadata":{},"cell_type":"markdown","source":"### Churn by Online Security using [pandas.DataFrame.plot](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.crosstab.html\nchurn_st_cross = pd.crosstab(churn['OnlineSecurity'], churn['Churn'])\nif DEBUG:\n    print(churn_st_cross)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html\n#plt.rcParams[\"figure.figsize\"] = (20,3)\nchurn_st_cross.plot(kind='bar', stacked=True)\nplt.title('Churn by Online Security')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the chart above, customers who use an electronic check as their payment method are more likely to churn that those who do not."},{"metadata":{},"cell_type":"markdown","source":"Initial analysis indicates that customers who use paperless billing, have short-term contracts, and pay by electronic check are more likely to churn."},{"metadata":{},"cell_type":"markdown","source":"## Categorical Feature Conversion to Dummy Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(churn.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The code below turns the **churn** target feature into numeric so some scikit learn alogrythms can process it. **No** becomes 0 and **Yes** becomes 1 as the [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) function converts in alphabetical order."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nle_dep = preprocessing.LabelEncoder()\n#to convert into numbers\nchurn['Churn'] = le_dep.fit_transform(churn['Churn'])\nchurn['gender'] = le_dep.fit_transform(churn['gender'])\nchurn['Partner'] = le_dep.fit_transform(churn['Partner'])\nchurn['Dependents'] = le_dep.fit_transform(churn['Dependents'])\nchurn['PhoneService'] = le_dep.fit_transform(churn['PhoneService'])\nchurn['MultipleLines'] = le_dep.fit_transform(churn['MultipleLines'])\nchurn['InternetService'] = le_dep.fit_transform(churn['InternetService'])\nchurn['OnlineSecurity'] = le_dep.fit_transform(churn['OnlineSecurity'])\nchurn['OnlineBackup'] = le_dep.fit_transform(churn['OnlineBackup'])\nchurn['DeviceProtection'] = le_dep.fit_transform(churn['DeviceProtection'])\nchurn['TechSupport'] = le_dep.fit_transform(churn['TechSupport'])\nchurn['StreamingTV'] = le_dep.fit_transform(churn['StreamingTV'])\nchurn['StreamingMovies'] = le_dep.fit_transform(churn['StreamingMovies'])\nchurn['Contract'] = le_dep.fit_transform(churn['Contract'])\nchurn['PaperlessBilling'] = le_dep.fit_transform(churn['PaperlessBilling'])\nchurn['PaymentMethod'] = le_dep.fit_transform(churn['PaymentMethod'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For my analysis, I dropped the **customerID** feature. This is a descriptive feature for each customer and doesn't provide any predictive value."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Note: axis=1 denotes that we are referring to a column, not a row\nchurn=churn.drop(['customerID'],axis=1)\nif DEBUG:\n    print(churn.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"churn.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From my analysis above, I noticed that **TotalCharges** is listed as an object data type. I used the code below to convert this to a numeric type."},{"metadata":{"trusted":true},"cell_type":"code","source":"churn['TotalCharges'] = churn['TotalCharges'].convert_objects(convert_numeric=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(churn['TotalCharges'].describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Missing Value Replacement (NaN)"},{"metadata":{},"cell_type":"markdown","source":"From above, we can see 7032 observations for TotalCharges. Using the [pandas.DataFrame.fillna](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html) method, I filled the NA/NaN values using the specified method (median)."},{"metadata":{"trusted":true},"cell_type":"code","source":"churn['TotalCharges'].fillna(churn['TotalCharges'].median(), inplace=True)\nprint(churn['TotalCharges'].describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if DEBUG:\n    print(churn.shape)\n    print(churn.info())\n    print(churn.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"groupby = churn.groupby(targetName)\nprint(groupby.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After converting our dummy variables, we can see that gender and PhoneService do not have much impact on churn."},{"metadata":{},"cell_type":"markdown","source":"## Principal Component Analysis (PCA)"},{"metadata":{},"cell_type":"markdown","source":"Using PCA for dimensionality reduction involves zeroing out one or more of the smallest principal components, resulting in a lower-dimensional projection of the data that preserves the maximal data variance."},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Standardize the Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://etav.github.io/python/scikit_pca.html\nchurn_numeric = churn.select_dtypes(include=['number'])\nfeatures = list(churn_numeric)\nX = churn_numeric.loc[:, features].values\nX = scale(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create a Covariance Matrix"},{"metadata":{},"cell_type":"markdown","source":"In principal component analysis, this relationship is quantified by finding a list of the principal axes in the data, and using those axes to describe the dataset. Using Scikit-Learn's PCA estimator, we can compute this."},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=0.99, whiten=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calculate Eigenvalues"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.fit(X)\nvariance = pca.explained_variance_ratio_ #calculate variance ratios\nX_pca = pca.fit_transform(X)\nvar=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=3)*100)\nvar #cumulative sum of variance explained with [n] features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show results\nprint('Original number of features:', X.shape[1])\nprint('Reduced number of features:', X_pca.shape[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above array we see that the first feature explains roughly 21% of the variance within our data set while the first two explain 34% and so on."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.ylabel('% Variance Explained')\nplt.xlabel('# of Features')\nplt.title('PCA Analysis')\nplt.ylim(30,100.5)\nplt.style.context('seaborn-whitegrid')\n\nplt.plot(var)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\"explained variance\" returns the amount of variance explained by each of the selected components. This is equal to n_components largest eigenvalues of the covariance matrix of the dataset."},{"metadata":{},"cell_type":"markdown","source":"PCA dimensionality reduction means the information along the least important principal axis or axes is removed, leaving only the component(s) of the data with the highest variance. The fraction of variance that is cut out (proportional to the spread of points about the line formed in this figure) is roughly a measure of how much \"information\" is discarded in this reduction of dimensionality. Given the number of features, I chose to include all features in developing my models."},{"metadata":{},"cell_type":"markdown","source":"## Train Test Split (67/33)"},{"metadata":{},"cell_type":"markdown","source":"Below I created train and test (67/33) datasets to begin model evaluation using the [sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) method. I set the random number generator seed by keeping **random_state = 0** to replicate consistent results during my analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"# split dataset into testing and training\nfeatures_train, features_test, target_train, target_test = train_test_split(churn.iloc[:,1:].values, churn.iloc[:,0].values, test_size=0.33, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Normalization"},{"metadata":{},"cell_type":"markdown","source":"I standardized **features_train** and **features_test** using the [sklearn.preprocessing.MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) class to achieve this. This class ransforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = preprocessing.MinMaxScaler().fit(features_train)\nfeatures_train = scaler.transform(features_train)\nfeatures_test = scaler.transform(features_test)","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":false},"cell_type":"markdown","source":"# Model Instantiation, Fitting, and Analysis"},{"metadata":{"toc-hr-collapsed":false},"cell_type":"markdown","source":"## K-Nearest Neighbors Model"},{"metadata":{},"cell_type":"markdown","source":"Using the [sklearn.neighbors.KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) library, I created multiple K-Nearest Neighbors models. The code below interated over a k-range of 25 to 50. My goal is to maximize Recall for Churn = Yes."},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### KNN Model 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_results = []\ntest_results = []\n# search for an optimal value of max_depth for decision tree\nk_range = list(range(25,50))\nk_scores = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k, n_jobs=-1, algorithm='auto', weights='uniform')\n    scores = cross_val_score(knn, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    k_scores.append(scores.mean())\n    # Code for plotting results\n    knn.fit(features_train, target_train)\n    train_pred = knn.predict(features_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous train results\n    train_results.append(roc_auc)\n    y_pred = knn.predict(features_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous test results\n    test_results.append(roc_auc)\nif DEBUG:\n    print(k_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if DEBUG:\n    scores = pd.DataFrame(k_scores)\n    print(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(k_range, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(k_range, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('KNN Neighbors')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the results above, I chose K=27 for my first model. This value produced the highest AUC score for the Test data set (denoted by the red line)."},{"metadata":{"trusted":true},"cell_type":"code","source":"#KNN train model. Call up my model and name it clf\nclf_knn = KNeighborsClassifier(n_neighbors=27, n_jobs=-1, algorithm='auto', weights='uniform')\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_knn)\n#Fit clf to the training data\nclf_knn = clf_knn.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_knn = clf_knn.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### KNN Model 1 Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_knn = accuracy_score(target_test, target_predicted_knn)\nprec_knn = precision_score(target_test, target_predicted_knn)\nrecall_knn = recall_score(target_test, target_predicted_knn)\nf1_knn = f1_score(target_test, target_predicted_knn)\ncm_knn = confusion_matrix(target_test, target_predicted_knn)\nprint(\"KNN Accuracy Score\", acc_knn)\nprint(classification_report(target_test, target_predicted_knn))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_knn))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_knn, annot=True, fmt='d')\nplt.title('KNN Model 1 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_knn))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When testing my model against the Test Data, I noticed an overall Accuracy Score of 78.1% with a Recall of 0.53."},{"metadata":{},"cell_type":"markdown","source":"#### KNN Model 1 Cross Validation"},{"metadata":{},"cell_type":"markdown","source":"Cross-validation, sometimes called out-of-sample testing, is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set. Cross-validation can be used to compare the performances of different predictive modeling procedures."},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify KNN with Cross Validation\nscores_knn = cross_val_score(clf_knn, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_knn)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_knn.mean(), scores_knn.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the KNN Model 1 is 76% with a variance of 3%. A high variance indicates that a model is prone to overfitting."},{"metadata":{},"cell_type":"markdown","source":"#### KNN Model 1 Area Under Curve (AUC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_knn.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_knn = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_knn)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_knn)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('KNN Model 1 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"AUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. The AUC score of 0.808 shows that this model displays some class separation capacity. An excellent model has AUC closest to 1 which means it has good measure of separability. A poor model has AUC closer to 0 which means it has worse measure of separability."},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### KNN Model 2"},{"metadata":{},"cell_type":"markdown","source":"With the model below, I chose to keep KNN=27 but change the weights parameter from the default 'uniform' to 'distance'. ‘uniform’ indicates uniform weights. All points in each neighborhood are weighted equally. ‘distance’ indicates weight points by the inverse of their distance. In this case, closer neighbors of a query point will have a greater influence than neighbors which are further away."},{"metadata":{"trusted":true},"cell_type":"code","source":"#KNN train model. Call up my model and name it clf\nclf_knn1 = KNeighborsClassifier(n_neighbors=27, n_jobs=-1, algorithm='auto', weights='distance')\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_knn1)\n#Fit clf to the training data\nclf_knn1 = clf_knn1.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_knn1 = clf_knn1.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### KNN Model 2 Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_knn1 = accuracy_score(target_test, target_predicted_knn1)\nprec_knn1 = precision_score(target_test, target_predicted_knn1)\nrecall_knn1 = recall_score(target_test, target_predicted_knn1)\nf1_knn1 = f1_score(target_test, target_predicted_knn1)\ncm_knn1 = confusion_matrix(target_test, target_predicted_knn1)\nprint(\"KNN Accuracy Score\", acc_knn1)\nprint(classification_report(target_test, target_predicted_knn1))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_knn1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_knn1, annot=True, fmt='d')\nplt.title('KNN Model 2 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_knn1))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When testing my model against the Test Data, I noticed an overall Accuracy Score of 76.6% with a Recall of 0.48. This model performed similarly in terms of Accuracy but slighty worse in terms of Recall."},{"metadata":{},"cell_type":"markdown","source":"#### KNN Model 2 Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify KNN with Cross Validation\nscores_knn1 = cross_val_score(clf_knn1, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_knn1)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_knn1.mean(), scores_knn1.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the KNN Model 2 is 75% with a variance of 3%. This model had a slightly lower accuracy score with the same variance."},{"metadata":{},"cell_type":"markdown","source":"#### KNN Model 2 Area Under Curve (AUC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_knn1.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_knn1 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_knn1)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_knn1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('KNN Model 2 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The AUC score of 0.792 shows that this model displays some class separation capacity however not as good as KNN Model 1 (0.808)."},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### KNN Model 3"},{"metadata":{},"cell_type":"markdown","source":"For KNN Model 3, I chose to keep KNN=27, weights = 'uniform' but chose the algorithm 'kd_tree'. The KD tree is a binary tree structure which recursively partitions the parameter space along the data axes, dividing it into nested orthotropic regions into which data points are filed. The construction of a KD tree is very fast: because partitioning is performed only along the data axes, no D-dimensional distances need to be computed. Though the KD tree approach is very fast for low-dimensional (D < 20) neighbors searches, it becomes inefficient as D grows very large: this is one manifestation of the so-called “curse of dimensionality”. Since my feature set is < 20, I wanted to test if this algorithm improved my model."},{"metadata":{"trusted":true},"cell_type":"code","source":"#KNN train model. Call up my model and name it clf\nclf_knn2 = KNeighborsClassifier(p=1, n_neighbors=27, n_jobs=-1, algorithm='kd_tree', weights='uniform')\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_knn2)\n#Fit clf to the training data\nclf_knn2 = clf_knn2.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_knn2 = clf_knn2.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":false},"cell_type":"markdown","source":"#### KNN Model 3 Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_knn2 = accuracy_score(target_test, target_predicted_knn2)\nprec_knn2 = precision_score(target_test, target_predicted_knn2)\nrecall_knn2 = recall_score(target_test, target_predicted_knn2)\nf1_knn2 = f1_score(target_test, target_predicted_knn2)\ncm_knn2 = confusion_matrix(target_test, target_predicted_knn2)\nprint(\"KNN Accuracy Score\", acc_knn2)\nprint(classification_report(target_test, target_predicted_knn2))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_knn2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_knn2, annot=True, fmt='d')\nplt.title('KNN Model 3 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_knn2))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"#### KNN Model 3 Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify KNN with Cross Validation\nscores_knn2 = cross_val_score(clf_knn2, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_knn2)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_knn2.mean(), scores_knn2.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the KNN Model 3 is 77% with a variance of 4%. This model has produced the highest cross-validated accuracy but the variance has also increased by 1%."},{"metadata":{},"cell_type":"markdown","source":"#### KNN Model 3 Area Under Curve (AUC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_knn2.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_knn2 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_knn2)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_knn2)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('KNN Model 3 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The AUC score of 0.822 is the best of all three KNN models. This shows that this model displays some class separation capacity."},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### KNN Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: http://docs.astropy.org/en/stable/table/construct_table.html\nt = Table()\nt[''] = ['KNN Model 1','KNN Model 2','KNN Model 3']\nt['Cross Validation Score'] = [round(scores_knn.mean(),4),round(scores_knn1.mean(),4),round(scores_knn2.mean(),4)]\nt['Accuracy Score'] = [round(acc_knn,4),round(acc_knn1,4),round(acc_knn2,4)]\nt['Precision'] = [round(prec_knn,4),round(prec_knn1,4),round(prec_knn2,4)]\nt['Recall'] = [round(recall_knn,4),round(recall_knn1,4),round(recall_knn2,4)]\nt['F1 Score'] = [round(f1_knn,4),round(f1_knn1,4),round(f1_knn2,4)]\nt['ROC AUC'] = [round(roc_auc_knn,4),round(roc_auc_knn1,4),round(roc_auc_knn2,4)]\nt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the three KNN models, KNN Model 3 with the KD tree algorithm produced the best scores for all classification metrics. All scores for were close but the recall score was substantially higher than the first two KNN models. I would designate KNN Model 3 as the best KNN model from my analysis."},{"metadata":{"toc-hr-collapsed":false},"cell_type":"markdown","source":"## Decision Tree Model"},{"metadata":{},"cell_type":"markdown","source":"Using the [sklearn.tree.DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) library, I created three Decision Tree models. With the first model, I chose the best **max_depth** parameter that would maximize the Recall score."},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Decision Tree Model 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_results = []\ntest_results = []\n# search for an optimal value of max_depth for decision tree\nk_range = list(range(1,20))\nk_scores = []\nfor k in k_range:\n    dt = tree.DecisionTreeClassifier(max_depth=k)\n    scores = cross_val_score(dt, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    k_scores.append(scores.mean())\n    # Code for plotting results\n    dt.fit(features_train, target_train)\n    train_pred = dt.predict(features_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous train results\n    train_results.append(roc_auc)\n    y_pred = dt.predict(features_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous test results\n    test_results.append(roc_auc)\nif DEBUG:\n    print(k_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if DEBUG:\n    scores = pd.DataFrame(k_scores)\n    print(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(k_range, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(k_range, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('Tree depth')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that our model overfits for large depth values. The tree perfectly predicts all of the train data, however, it fails to generalize the findings for new data. Based on these findings, I chose **max_depth=5** to minimize overfitting."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Decision Tree train model. Call up my model and name it clf \nclf_dt = tree.DecisionTreeClassifier(max_depth=5)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_dt)\n#Fit clf to the training data\nclf_dt = clf_dt.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_dt = clf_dt.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Decision Tree Baseline Model Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_dt = accuracy_score(target_test, target_predicted_dt)\nprec_dt = precision_score(target_test, target_predicted_dt)\nrecall_dt = recall_score(target_test, target_predicted_dt)\nf1_dt = f1_score(target_test, target_predicted_dt)\ncm_dt = confusion_matrix(target_test, target_predicted_dt)\nprint(\"DT Accuracy Score\", acc_dt)\nprint(classification_report(target_test, target_predicted_dt))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_dt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_dt, annot=True, fmt='d')\nplt.title('Decision Tree Model 1 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_dt))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When testing my model against the Test Data, I noticed an overall Accuracy Score of 78.2% with a Recall of 0.60 (Churn = Yes). At this point, this model has already exceeded the recall scores of all three KNN models."},{"metadata":{},"cell_type":"markdown","source":"#### Decision Tree Baseline Model Cross Validation (CV = 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify DT with Cross Validation\nscores_dt = cross_val_score(clf_dt, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_dt)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_dt.mean(), scores_dt.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the Decision Tree Model 1 is 79% with a variance of 5%. The cross-validation method produces a more accurate estimate of out-of-sample accuracy and is a more efficient use of data (every observation is used for both training and testing)."},{"metadata":{},"cell_type":"markdown","source":"#### Decision Tree Baseline Model Area Under Curve (AUC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_dt.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_dt = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_dt)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_dt)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Decision Tree Model 1 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The AUC score of 0.819 is an excellent starting point as this value indicates that this model is fairly good and predicting class separation."},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Decision Tree Model 2"},{"metadata":{},"cell_type":"markdown","source":"I created Decision Tree Model 2 and focused on adding the **min_samples_leaf** parameter. **min_samples_split** represents the minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least **min_samples_split** training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_results = []\ntest_results = []\n# search for an optimal value of max_depth for decision tree\n#k_range = list(range(2,40))\nk_range = np.linspace(0.1, 0.3, 20, endpoint=True)\nk_scores = []\nfor k in k_range:\n    dt = tree.DecisionTreeClassifier(max_depth=5, min_samples_leaf=k)\n    scores = cross_val_score(dt, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    k_scores.append(scores.mean())\n    # Code for plotting results\n    dt.fit(features_train, target_train)\n    train_pred = dt.predict(features_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous train results\n    train_results.append(roc_auc)\n    y_pred = dt.predict(features_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous test results\n    test_results.append(roc_auc)\nif DEBUG:\n    print(k_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if DEBUG:\n    scores = pd.DataFrame(k_scores)\n    print(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(k_range, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(k_range, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('Min Samples Leaf')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the analysis above, I chose **min_samples_split=0.225** as this produced the highest AUC score for the Train and Test datasets. Since I chose to use a float instead of int, **min_samples_leaf** is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Decision Tree train model. Call up my model and name it clf \nclf_dt1 = tree.DecisionTreeClassifier(max_depth=5, min_samples_leaf=0.225)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_dt1)\n#Fit clf to the training data\nclf_dt1 = clf_dt1.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_dt1 = clf_dt1.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Decision Tree Model 2 Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_dt1 = accuracy_score(target_test, target_predicted_dt1)\nprec_dt1 = precision_score(target_test, target_predicted_dt1)\nrecall_dt1 = recall_score(target_test, target_predicted_dt1)\nf1_dt1 = f1_score(target_test, target_predicted_dt1)\ncm_dt1 = confusion_matrix(target_test, target_predicted_dt1)\nprint(\"DT Accuracy Score\", acc_dt1)\nprint(classification_report(target_test, target_predicted_dt1))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_dt1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_dt1, annot=True, fmt='d')\nplt.title('Decision Tree Model 2 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_dt1))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Decision Tree Model 2 produced a slightly lower accuracy than Model 1 but had a higher recall score (0.61) for Churn = Yes."},{"metadata":{},"cell_type":"markdown","source":"#### Decision Tree Model 2 Cross Validation (CV = 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify DT with Cross Validation\nscores_dt1 = cross_val_score(clf_dt1, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_dt1)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_dt1.mean(), scores_dt1.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the Decision Tree Model 2 is 75% with a variance of 4%. These values are consistent with what we've seen with all the models thus far."},{"metadata":{},"cell_type":"markdown","source":"#### Decision Tree Model 2 Area Under Curve (AUC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_dt1.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_dt1 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_dt1)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_dt1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Decision Tree Model 2 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The AUC score of 0.758 is a reduction in overall performance when compared to Model 1."},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Decision Tree Model 3"},{"metadata":{},"cell_type":"markdown","source":"For Decision Tree Model 3, I focused on changing the **min_samples_split** parameter. This parameter represents the minimum number of samples required to split an internal node. This can vary between considering at least one sample at each node to considering all of the samples at each node. When we increase this parameter, the tree becomes more constrained as it has to consider more samples at each node. Here we will vary the parameter from 10% to 100% of the samples."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_results = []\ntest_results = []\n# search for an optimal value of max_depth for decision tree\n#k_range = list(range(2,40))\nk_range = np.linspace(0.1, 1.0, 20, endpoint=True)\nk_scores = []\nfor k in k_range:\n    dt = tree.DecisionTreeClassifier(max_depth=5, min_samples_split=k)\n    scores = cross_val_score(dt, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    k_scores.append(scores.mean())\n    # Code for plotting results\n    dt.fit(features_train, target_train)\n    train_pred = dt.predict(features_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous train results\n    train_results.append(roc_auc)\n    y_pred = dt.predict(features_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous test results\n    test_results.append(roc_auc)\nif DEBUG:\n    print(k_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(k_range, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(k_range, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('Min Samples Split')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the results above, I set **min_samples_split=0.5**. Since I used a float, **min_samples_split** is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Decision Tree train model. Call up my model and name it clf \nclf_dt2 = tree.DecisionTreeClassifier(max_depth=5, min_samples_split=0.5)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_dt2)\n#Fit clf to the training data\nclf_dt2 = clf_dt2.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_dt2 = clf_dt2.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Decision Tree Model 3 Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_dt2 = accuracy_score(target_test, target_predicted_dt2)\nprec_dt2 = precision_score(target_test, target_predicted_dt2)\nrecall_dt2 = recall_score(target_test, target_predicted_dt2)\nf1_dt2 = f1_score(target_test, target_predicted_dt2)\ncm_dt2 = confusion_matrix(target_test, target_predicted_dt2)\nprint(\"DT Accuracy Score\", acc_dt2)\nprint(classification_report(target_test, target_predicted_dt2))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_dt2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_dt2, annot=True, fmt='d')\nplt.title('Decision Tree Model 3 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_dt2))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Decision Tree Model 3 produced the worst accuracy score compared to the first two DT models but did produce the highest recall score of 0.71."},{"metadata":{},"cell_type":"markdown","source":"#### Decision Tree Model 3 Cross Validation (CV = 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify DT with Cross Validation\nscores_dt2 = cross_val_score(clf_dt2, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_dt2)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_dt2.mean(), scores_dt2.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the Decision Tree Model 3 is 74% with a variance of 5%."},{"metadata":{},"cell_type":"markdown","source":"#### Decision Tree Model 3 Area Under Curve (AUC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_dt2.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_dt2 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_dt2)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_dt2)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Decision Tree Model 3 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Decision Tree Model 3 produced an overall performance similar to Model 2 but much less than Model 1."},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Decision Tree Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"t = Table()\nt[''] = ['Decision Tree Model 1','Decision Tree Model 2','Decision Tree Model 3']\nt['Cross Validation Score'] = [round(scores_dt.mean(),4),round(scores_dt1.mean(),4),round(scores_dt2.mean(),4)]\nt['Accuracy Score'] = [round(acc_dt,4),round(acc_dt1,4),round(acc_dt2,4)]\nt['Precision'] = [round(prec_dt,4),round(prec_dt1,4),round(prec_dt2,4)]\nt['Recall'] = [round(recall_dt,4),round(recall_dt1,4),round(recall_dt2,4)]\nt['F1 Score'] = [round(f1_dt,4),round(f1_dt1,4),round(f1_dt2,4)]\nt['ROC AUC'] = [round(roc_auc_dt,4),round(roc_auc_dt1,4),round(roc_auc_dt2,4)]\nt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Overall, the Decision Tree Models performed better than the KNN models, specicifically with regards to their Recall scores. I noticed a degredation in overall accuracy as I tuned the model but found an increase in the Recall scores. I chose Decision Tree Model 3 as the best DT model based on the highest Recall score thus far."},{"metadata":{"toc-hr-collapsed":false},"cell_type":"markdown","source":"## Random Forest Model"},{"metadata":{},"cell_type":"markdown","source":"Using the [sklearn.ensemble.RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) library, I created three Random Forest Models. For my first model, I focused on setting the best **max_depth** parameter for my dataset. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. This leads to overfitting."},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Random Forest Model 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_results = []\ntest_results = []\n# search for an optimal value of max_depth for decision tree\nk_range = list(range(1,20))\nk_scores = []\nfor k in k_range:\n    rf = RandomForestClassifier(max_depth=k, n_jobs=-1)\n    scores = cross_val_score(rf, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    k_scores.append(scores.mean())\n    # Code for plotting results\n    rf.fit(features_train, target_train)\n    train_pred = rf.predict(features_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous train results\n    train_results.append(roc_auc)\n    y_pred = rf.predict(features_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous test results\n    test_results.append(roc_auc)\nif DEBUG:\n    print(k_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(k_range, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(k_range, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('Max Depth')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the chart above, as **max_depth** increases, we see the blue line (train data) approach 1.0 while the red line (test data) start to level off at 7. As the lines become further apart, this is where overfitting starts to happen. I chose **max_depth=7** as my tuning parameter to best address this."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest train model. Call up my model and name it clf\nclf_rf = RandomForestClassifier(max_depth=7, n_jobs=-1)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_rf)\n#Fit clf to the training data\nclf_rf = clf_rf.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_rf = clf_rf.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Random Forest Model 1 Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_rf = accuracy_score(target_test, target_predicted_rf)\nprec_rf = precision_score(target_test, target_predicted_rf)\nrecall_rf = recall_score(target_test, target_predicted_rf)\nf1_rf = f1_score(target_test, target_predicted_rf)\ncm_rf = confusion_matrix(target_test, target_predicted_rf)\nprint(\"RF Accuracy Score\", acc_rf)\nprint(classification_report(target_test, target_predicted_rf))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_rf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_rf, annot=True, fmt='d')\nplt.title('Random Forest Model 1 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_rf))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Random Forest Model 1 produced an accuracy of 79.3% with a recall score of 0.49. This is comparable to previous models but not any better or worse."},{"metadata":{},"cell_type":"markdown","source":"#### Random Forest Model 1 Cross Validation (CV = 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify RF with Cross Validation\nscores_rf = cross_val_score(clf_rf, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_rf)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_rf.mean(), scores_rf.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the Random Forest Model 1 is 80% with a variance of 4%. So far, this is a good accuracy score but the variance leads me to believe we can continue to tune this model."},{"metadata":{},"cell_type":"markdown","source":"#### Random Forest Model 1 Area Under Curve (AUC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_rf.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_rf = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_rf)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_rf)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Random Forest Model 1 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The AUC score of 0.833 is a respectable starting point for the Random Forest Models and is consistent with DT and KNN."},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Random Forest Model 2"},{"metadata":{},"cell_type":"markdown","source":"For Random Forest Model 2, I retained **max_depth=7** and chose to determine the best **n_estimators** parameter. This number determines the number of trees in the forest (default=10)."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_results = []\ntest_results = []\n# search for an optimal value of max_depth for decision tree\nk_range = list(range(1,20))\nk_scores = []\nfor k in k_range:\n    rf = RandomForestClassifier(n_estimators=k, n_jobs=-1, max_depth=7)\n    scores = cross_val_score(rf, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    k_scores.append(scores.mean())\n    # Code for plotting results\n    rf.fit(features_train, target_train)\n    train_pred = rf.predict(features_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous train results\n    train_results.append(roc_auc)\n    y_pred = rf.predict(features_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous test results\n    test_results.append(roc_auc)\nif DEBUG:\n    print(k_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(k_range, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(k_range, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('N Estimators')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the analysis above, I choose **n_estimators=17** as this produced the highest AUC score against the Test Data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest train model. Call up my model and name it clf\nclf_rf1 = RandomForestClassifier(n_estimators=17, n_jobs=-1, max_depth=7)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_rf1)\n#Fit clf to the training data\nclf_rf1 = clf_rf1.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_rf1 = clf_rf1.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Random Forest Model 2 Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_rf1 = accuracy_score(target_test, target_predicted_rf1)\nprec_rf1 = precision_score(target_test, target_predicted_rf1)\nrecall_rf1 = recall_score(target_test, target_predicted_rf1)\nf1_rf1 = f1_score(target_test, target_predicted_rf1)\ncm_rf1 = confusion_matrix(target_test, target_predicted_rf1)\nprint(\"RF Accuracy Score\", acc_rf1)\nprint(classification_report(target_test, target_predicted_rf1))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_rf1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_rf1, annot=True, fmt='d')\nplt.title('Random Forest Model 2 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_rf1))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest Model 2 produced an overall accuracy of 79.7% with a recall score of 0.49."},{"metadata":{},"cell_type":"markdown","source":"#### Random Forest Model 2 Cross Validation (CV = 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify RF with Cross Validation\nscores_rf1 = cross_val_score(clf_rf1, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_rf1)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_rf1.mean(), scores_rf1.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the Random Forest Model 2 is 80% with a variance of 3%. This is consistent with the previous models."},{"metadata":{},"cell_type":"markdown","source":"#### Random Forest Model 2 Area Under Curve (AUC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_rf1.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_rf1 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_rf1)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_rf1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Random Forest Model 2 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The AUC score of 0.836 is a comparable with Model 1."},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Random Forest Model 3"},{"metadata":{},"cell_type":"markdown","source":"For Random Forest Model 3, I chose to tune **min_samples_leaf**. This parameter determines the minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least **min_samples_leaf** training samples in each of the left and right branches."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_results = []\ntest_results = []\n# search for an optimal value of max_depth for decision tree\nk_range = list(range(1,20))\nk_scores = []\nfor k in k_range:\n    rf = RandomForestClassifier(n_estimators=17, n_jobs=-1, max_depth=7, min_samples_leaf=k)\n    scores = cross_val_score(rf, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    k_scores.append(scores.mean())\n    # Code for plotting results\n    rf.fit(features_train, target_train)\n    train_pred = rf.predict(features_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous train results\n    train_results.append(roc_auc)\n    y_pred = rf.predict(features_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous test results\n    test_results.append(roc_auc)\nif DEBUG:\n    print(k_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(k_range, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(k_range, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('Min Samples Leaf')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the analysis above, I choose **min_samples_leaf=14**. This produced the highest AUC score for the Test Data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest train model. Call up my model and name it clf\nclf_rf2 = RandomForestClassifier(n_estimators=17, n_jobs=-1, max_depth=7, min_samples_leaf=14)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_rf2)\n#Fit clf to the training data\nclf_rf2 = clf_rf2.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_rf2 = clf_rf2.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Random Forest Model 3 Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_rf2 = accuracy_score(target_test, target_predicted_rf2)\nprec_rf2 = precision_score(target_test, target_predicted_rf2)\nrecall_rf2 = recall_score(target_test, target_predicted_rf2)\nf1_rf2 = f1_score(target_test, target_predicted_rf2)\ncm_rf2 = confusion_matrix(target_test, target_predicted_rf2)\nprint(\"RF Accuracy Score\", acc_rf2)\nprint(classification_report(target_test, target_predicted_rf2))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_rf2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_rf2, annot=True, fmt='d')\nplt.title('Random Forest Model 3 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_rf2))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Random Forest Model 3 produced an accuracy score of 80.2% with a recall of 0.51. So far, no significant change from the previous Random Forest Models."},{"metadata":{},"cell_type":"markdown","source":"#### Random Forest Model 3 Cross Validation (CV = 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify RF with Cross Validation\nscores_rf2 = cross_val_score(clf_rf2, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_rf2)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_rf2.mean(), scores_rf2.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the Random Forest Model 3 is 80% with a variance of 4%."},{"metadata":{},"cell_type":"markdown","source":"#### Random Forest Model 3 Area Under Curve (AUC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_rf2.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_rf2 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_rf2)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_rf2)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Random Forest Model 3 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The AUC score of 0.842 is comparable with the previous models."},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Random Forest Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"t = Table()\nt[''] = ['RF Model 1','RF Model 2','RF Model 3']\nt['Cross Validation Score'] = [round(scores_rf.mean(),4),round(scores_rf1.mean(),4),round(scores_rf2.mean(),4)]\nt['Accuracy Score'] = [round(acc_rf,4),round(acc_rf1,4),round(acc_rf2,4)]\nt['Precision'] = [round(prec_rf,4),round(prec_rf1,4),round(prec_rf2,4)]\nt['Recall'] = [round(recall_rf,4),round(recall_rf1,4),round(recall_rf2,4)]\nt['F1 Score'] = [round(f1_rf,4),round(f1_rf1,4),round(f1_rf2,4)]\nt['ROC AUC'] = [round(roc_auc_rf,4),round(roc_auc_rf1,4),round(roc_auc_rf2,4)]\nt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From my analysis, all three models produces very similar performance metrics. While no distinct separation could be made, I chose Random Forest Model 1 as my best model based on the highest Recall score."},{"metadata":{"toc-hr-collapsed":false},"cell_type":"markdown","source":"## Bagging Classifier Model"},{"metadata":{},"cell_type":"markdown","source":"Using the [sklearn.ensemble.BaggingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) library, I created three Bagging Classifier Model. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. For my first model, I choose KNN Model 3 as my base estimator."},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Bagging Classifier Model 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\n# Random Forest train model. Call up my model and name it clf\nclf_bag = BaggingClassifier(base_estimator=clf_knn2, n_jobs=-1)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_bag)\n#Fit clf to the training data\nclf_bag = clf_bag.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_bag = clf_bag.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Bagging Classifier Model 1 Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_bag = accuracy_score(target_test, target_predicted_bag)\nprec_bag = precision_score(target_test, target_predicted_bag)\nrecall_bag = recall_score(target_test, target_predicted_bag)\nf1_bag = f1_score(target_test, target_predicted_bag)\ncm_bag = confusion_matrix(target_test, target_predicted_bag)\nprint(\"Bag Accuracy Score\", acc_bag)\nprint(classification_report(target_test, target_predicted_bag))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_bag))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_bag, annot=True, fmt='d')\nplt.title('Bagging Classifier Model 1 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_bag))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Bagging Classifier Model 1 (KNN Model 3) produced an accuracy of 78.5% with a Recall score of 0.56. This is a great start, especially with the Recall score."},{"metadata":{},"cell_type":"markdown","source":"#### Bagging Classifier Baseline Model Cross Validation (CV = 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify RF with Cross Validation\nscores_bag = cross_val_score(clf_bag, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_bag)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_bag.mean(), scores_bag.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the Bagging Classifier Model 1 is 77% with a variance of 4%."},{"metadata":{},"cell_type":"markdown","source":"#### Bagging Classifier Model 1 Area Under Curve (AUC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_bag.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_bag = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_bag)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_bag)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Bagging Classifier Model 1 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This model produced an AUC Score of 0.822 and is consistent with the models we've seen thus far."},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Bagging Classifier Model 2"},{"metadata":{},"cell_type":"markdown","source":"For Bagging Classifier Model 2, I chose the **Decision Tree Model 1** as my base estimator model and added **n_estimators** as a tuning parameter. This parameter determines the number of base estimators in the ensemble."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_results = []\ntest_results = []\n# search for an optimal value of max_depth for decision tree\nk_range = list(range(1,20))\nk_scores = []\nfor k in k_range:\n    bag = BaggingClassifier(base_estimator=clf_dt, n_estimators=k, n_jobs=-1)\n    scores = cross_val_score(bag, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    k_scores.append(scores.mean())\n    # Code for plotting results\n    bag.fit(features_train, target_train)\n    train_pred = bag.predict(features_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous train results\n    train_results.append(roc_auc)\n    y_pred = bag.predict(features_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous test results\n    test_results.append(roc_auc)\nif DEBUG:\n    print(k_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(k_range, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(k_range, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('N Estimators')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the analysis above, I choose **n_estimators=4**."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest train model. Call up my model and name it clf\nclf_bag1 = BaggingClassifier(base_estimator=clf_dt, n_estimators=4, n_jobs=-1)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_bag1)\n#Fit clf to the training data\nclf_bag1 = clf_bag1.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_bag1 = clf_bag1.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Bagging Classifier Model 2 Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_bag1 = accuracy_score(target_test, target_predicted_bag1)\nprec_bag1 = precision_score(target_test, target_predicted_bag1)\nrecall_bag1 = recall_score(target_test, target_predicted_bag1)\nf1_bag1 = f1_score(target_test, target_predicted_bag1)\ncm_bag1 = confusion_matrix(target_test, target_predicted_bag1)\nprint(\"Bag Accuracy Score\", acc_bag1)\nprint(classification_report(target_test, target_predicted_bag1))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_bag1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_bag1, annot=True, fmt='d')\nplt.title('Bagging Classifier Model 2 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_bag1))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This model produced an accuracy of 79.1% and a recall score of 0.46. This did not perform as well as the original Decision Tree Model 1."},{"metadata":{},"cell_type":"markdown","source":"#### Bagging Classifier Model 2 Cross Validation (CV = 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify RF with Cross Validation\nscores_bag1 = cross_val_score(clf_bag1, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_bag1)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_bag1.mean(), scores_bag1.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the Bagging Classifier Model 2 is 79% with a variance of 4%."},{"metadata":{},"cell_type":"markdown","source":"#### Bagging Classifier Model 2 Area Under Curve (AUC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_bag1.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_bag1 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_bag1)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_bag1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Bagging Classifier Model 2 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Bagging Classifier Model 2 produced an AUC score of 0.830. From an overall performance perspective, this is consistent with the previous models."},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Bagging Classifier Model 3"},{"metadata":{},"cell_type":"markdown","source":"For the Bagging Classifier Model 3, I chose **Random Forest Model 1** as my base estimator and chose to tune **n_estimators**."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_results = []\ntest_results = []\n# search for an optimal value of max_depth for decision tree\nk_range = list(range(1,20))\nk_scores = []\nfor k in k_range:\n    bag = BaggingClassifier(base_estimator=clf_rf, n_estimators=k, n_jobs=-1)\n    scores = cross_val_score(bag, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    k_scores.append(scores.mean())\n    # Code for plotting results\n    bag.fit(features_train, target_train)\n    train_pred = bag.predict(features_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous train results\n    train_results.append(roc_auc)\n    y_pred = bag.predict(features_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous test results\n    test_results.append(roc_auc)\nif DEBUG:\n    print(k_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(k_range, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(k_range, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('N Estimators')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the analysis above, I choose **n_estimators=10**."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest train model. Call up my model and name it clf\nclf_bag2 = BaggingClassifier(base_estimator=clf_rf, n_estimators=10, n_jobs=-1)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_bag2)\n#Fit clf to the training data\nclf_bag2 = clf_bag2.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_bag2 = clf_bag2.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Bagging Classifier Model 3 Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_bag2 = accuracy_score(target_test, target_predicted_bag2)\nprec_bag2 = precision_score(target_test, target_predicted_bag2)\nrecall_bag2 = recall_score(target_test, target_predicted_bag2)\nf1_bag2 = f1_score(target_test, target_predicted_bag2)\ncm_bag2 = confusion_matrix(target_test, target_predicted_bag2)\nprint(\"Bag Accuracy Score\", acc_bag2)\nprint(classification_report(target_test, target_predicted_bag2))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_bag2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_bag2, annot=True, fmt='d')\nplt.title('Bagging Classifier Model 3 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_bag2))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Bagging Classifier Model 3 Cross Validation (CV = 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify RF with Cross Validation\nscores_bag2 = cross_val_score(clf_bag2, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_bag2)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_bag2.mean(), scores_bag2.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the Bagging Classifier Model 3 is 80% with a variance of 4%."},{"metadata":{},"cell_type":"markdown","source":"#### Bagging Classifier Model 3 Area Under Curve (AUC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_bag2.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_bag2 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_bag2)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_bag2)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Bagging Classifier Model 3 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Bagging Classifier Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"t = Table()\nt[''] = ['Bagging Classifier Model 1','Bagging Classifier Model 2','Bagging Classifier Model 3']\nt['Cross Validation Score'] = [round(scores_bag.mean(),4),round(scores_bag1.mean(),4),round(scores_bag2.mean(),4)]\nt['Accuracy Score'] = [round(acc_bag,4),round(acc_bag1,4),round(acc_bag2,4)]\nt['Precision'] = [round(prec_bag,4),round(prec_bag1,4),round(prec_bag2,4)]\nt['Recall'] = [round(recall_bag,4),round(recall_bag1,4),round(recall_bag2,4)]\nt['F1 Score'] = [round(f1_bag,4),round(f1_bag1,4),round(f1_bag2,4)]\nt['ROC AUC'] = [round(roc_auc_bag,4),round(roc_auc_bag1,4),round(roc_auc_bag2,4)]\nt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the analysis above, the Bagging Classifier Model 1 (KNN Model 3) produced the best recall score."},{"metadata":{"toc-hr-collapsed":false},"cell_type":"markdown","source":"## Extra Trees Model"},{"metadata":{},"cell_type":"markdown","source":"Using the [sklearn.ensemble.ExtraTreesClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html) library, I created an extra-trees classifier model. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. For the first model, I chose to optimize the maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than **min_samples_split** samples."},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Extra Trees Model 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\ntrain_results = []\ntest_results = []\n# search for an optimal value of max_depth for decision tree\nk_range = list(range(1,15))\nk_scores = []\nfor k in k_range:\n    xdt = ExtraTreesClassifier(max_depth=k, n_jobs=-1)\n    scores = cross_val_score(xdt, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    k_scores.append(scores.mean())\n    # Code for plotting results\n    xdt.fit(features_train, target_train)\n    train_pred = xdt.predict(features_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous train results\n    train_results.append(roc_auc)\n    y_pred = xdt.predict(features_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous test results\n    test_results.append(roc_auc)\nif DEBUG:\n    print(k_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(k_range, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(k_range, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('Max Depth')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the chart above, I chose **max_depth=8** as this value seemed to maximize the AUC score of the Test Data. As the tree grows deeper, this model becomes prone to overfitting."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest train model. Call up my model and name it clf\nclf_xdt = ExtraTreesClassifier(max_depth=8, n_jobs=-1)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_xdt)\n#Fit clf to the training data\nclf_xdt = clf_xdt.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_xdt = clf_xdt.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Extra Trees Model 1 Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_xdt = accuracy_score(target_test, target_predicted_xdt)\nprec_xdt = precision_score(target_test, target_predicted_xdt)\nrecall_xdt = recall_score(target_test, target_predicted_xdt)\nf1_xdt = f1_score(target_test, target_predicted_xdt)\ncm_xdt = confusion_matrix(target_test, target_predicted_xdt)\nprint(\"XDT Accuracy Score\", acc_xdt)\nprint(classification_report(target_test, target_predicted_xdt))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_xdt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_xdt, annot=True, fmt='d')\nplt.title('Extra Trees Model 1 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_xdt))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Extra Trees Baseline Model Cross Validation (CV = 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify RF with Cross Validation\nscores_xdt = cross_val_score(clf_xdt, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_xdt)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_xdt.mean(), scores_xdt.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the Extra Trees Model 1 is 80% with a variance of 3%."},{"metadata":{},"cell_type":"markdown","source":"#### Extra Trees Baseline Model Area Under Curve (AUC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_xdt.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_xdt = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_xdt)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_xdt)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Extra Trees Model 1 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Extra Trees Model 2"},{"metadata":{},"cell_type":"markdown","source":"For Extra Trees Model 2, I chose to keep **max_depth=8** and optimize **min_samples_leaf**."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_results = []\ntest_results = []\n# search for an optimal value of max_depth for decision tree\nk_range = list(range(1,20))\nk_scores = []\nfor k in k_range:\n    xdt = ExtraTreesClassifier(min_samples_leaf=k, max_depth=8, n_jobs=-1)\n    scores = cross_val_score(xdt, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    k_scores.append(scores.mean())\n    # Code for plotting results\n    xdt.fit(features_train, target_train)\n    train_pred = xdt.predict(features_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous train results\n    train_results.append(roc_auc)\n    y_pred = xdt.predict(features_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous test results\n    test_results.append(roc_auc)\nif DEBUG:\n    print(k_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(k_range, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(k_range, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('Min Samples Leaf')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the chart above, I chose **min_samples_leaf=7**."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest train model. Call up my model and name it clf\nclf_xdt1 = ExtraTreesClassifier(min_samples_leaf=7, max_depth=8, n_jobs=-1)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_xdt1)\n#Fit clf to the training data\nclf_xdt1 = clf_xdt1.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_xdt1 = clf_xdt1.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Extra Trees Model 2 Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_xdt1 = accuracy_score(target_test, target_predicted_xdt1)\nprec_xdt1 = precision_score(target_test, target_predicted_xdt1)\nrecall_xdt1 = recall_score(target_test, target_predicted_xdt1)\nf1_xdt1 = f1_score(target_test, target_predicted_xdt1)\ncm_xdt1 = confusion_matrix(target_test, target_predicted_xdt1)\nprint(\"XDT Accuracy Score\", acc_xdt1)\nprint(classification_report(target_test, target_predicted_xdt1))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_xdt1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_xdt1, annot=True, fmt='d')\nplt.title('Extra Trees Model 2 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_xdt1))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Extra Trees Model 2 Cross Validation (CV = 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify RF with Cross Validation\nscores_xdt1 = cross_val_score(clf_xdt1, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_xdt1)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_xdt1.mean(), scores_xdt1.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the Extra Trees Model 2 is 80% with a variance of 3%."},{"metadata":{},"cell_type":"markdown","source":"#### Extra Trees Model 2 Area Under Curve (AUC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_xdt1.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_xdt1 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_xdt1)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_xdt1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Extra Trees Model 2 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Extra Trees Model 3"},{"metadata":{},"cell_type":"markdown","source":"For Extra Trees Model 3, I kept **max_depth=8**, **min_samples_leaf=7**, and chose to optimize **max_leaf_nodes**. This parameter grows trees with **max_leaf_nodes** in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_results = []\ntest_results = []\n# search for an optimal value of max_depth for decision tree\nk_range = list(range(50,70))\nk_scores = []\nfor k in k_range:\n    xdt = ExtraTreesClassifier(max_leaf_nodes=k, min_samples_leaf=7, max_depth=8, n_jobs=-1)\n    scores = cross_val_score(xdt, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    k_scores.append(scores.mean())\n    # Code for plotting results\n    xdt.fit(features_train, target_train)\n    train_pred = xdt.predict(features_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous train results\n    train_results.append(roc_auc)\n    y_pred = xdt.predict(features_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous test results\n    test_results.append(roc_auc)\nif DEBUG:\n    print(k_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(k_range, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(k_range, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('Max Leaf Nodes')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the chart above, I chose **max_leaf_nodes=66**."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest train model. Call up my model and name it clf\nclf_xdt2 = ExtraTreesClassifier(max_leaf_nodes=66, min_samples_leaf=7, max_depth=8, n_jobs=-1)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_xdt2)\n#Fit clf to the training data\nclf_xdt2 = clf_xdt2.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_xdt2 = clf_xdt2.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Extra Trees Model 3 Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_xdt2 = accuracy_score(target_test, target_predicted_xdt2)\nprec_xdt2 = precision_score(target_test, target_predicted_xdt2)\nrecall_xdt2 = recall_score(target_test, target_predicted_xdt2)\nf1_xdt2 = f1_score(target_test, target_predicted_xdt2)\ncm_xdt2 = confusion_matrix(target_test, target_predicted_xdt2)\nprint(\"XDT Accuracy Score\", acc_xdt2)\nprint(classification_report(target_test, target_predicted_xdt2))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_xdt2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_xdt2, annot=True, fmt='d')\nplt.title('Extra Trees Model 3 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_xdt2))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Extra Trees Model 3 Cross Validation (CV = 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify RF with Cross Validation\nscores_xdt2 = cross_val_score(clf_xdt2, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_xdt2)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_xdt2.mean(), scores_xdt2.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the Extra Trees Model 3 is 80% with a variance of 4%."},{"metadata":{},"cell_type":"markdown","source":"#### Extra Trees Model 3 Area Under Curve (AUC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_xdt2.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_xdt2 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_xdt2)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_xdt2)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Extra Trees Model 3 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Extra Trees Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"t = Table()\nt[''] = ['XDT Model 1','XDT Model 2','XDT Model 3']\nt['Cross Validation Score'] = [round(scores_xdt.mean(),4),round(scores_xdt1.mean(),4),round(scores_xdt2.mean(),4)]\nt['Accuracy Score'] = [round(acc_xdt,4),round(acc_xdt1,4),round(acc_xdt2,4)]\nt['Precision'] = [round(prec_xdt,4),round(prec_xdt1,4),round(prec_xdt2,4)]\nt['Recall'] = [round(recall_xdt,4),round(recall_xdt1,4),round(recall_xdt2,4)]\nt['F1 Score'] = [round(f1_xdt,4),round(f1_xdt1,4),round(f1_xdt2,4)]\nt['ROC AUC'] = [round(roc_auc_xdt,4),round(roc_auc_xdt1,4),round(roc_auc_xdt2,4)]\nt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the analysis above,, the Extra Trees Model 3 proved to be the best performing model in terms of recall score."},{"metadata":{"toc-hr-collapsed":false},"cell_type":"markdown","source":"## Gradient Boost Model"},{"metadata":{},"cell_type":"markdown","source":"Using the [sklearn.ensemble.GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) library, I created three models. For the first model, I chose to use default values to develop a baseline."},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Gradient Boost Model 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n# Random Forest train model. Call up my model and name it clf\nclf_gbc = GradientBoostingClassifier()\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_gbc)\n#Fit clf to the training data\nclf_gbc = clf_gbc.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_gbc = clf_gbc.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Gradient Boost Model 1 Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_gbc = accuracy_score(target_test, target_predicted_gbc)\nprec_gbc = precision_score(target_test, target_predicted_gbc)\nrecall_gbc = recall_score(target_test, target_predicted_gbc)\nf1_gbc = f1_score(target_test, target_predicted_gbc)\ncm_gbc = confusion_matrix(target_test, target_predicted_gbc)\nprint(\"GBC Accuracy Score\", acc_gbc)\nprint(classification_report(target_test, target_predicted_gbc))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_gbc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_gbc, annot=True, fmt='d')\nplt.title('Gradient Boost Model 1 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_gbc))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Gradient Boost Model 1 Cross Validation (CV = 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify RF with Cross Validation\nscores_gbc = cross_val_score(clf_gbc, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_gbc)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_gbc.mean(), scores_gbc.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the Gradient Boost Model 1 is 80% with a variance of 3%."},{"metadata":{},"cell_type":"markdown","source":"#### Gradient Boost Model 1 Area Under Curve (AUC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_gbc.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_gbc = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_gbc)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_gbc)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Gradient Boost Model 1 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Gradient Boost Model 2"},{"metadata":{},"cell_type":"markdown","source":"For Gradient Boost Model 2, I chose to tune the maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_results = []\ntest_results = []\n# search for an optimal value of max_depth for decision tree\nk_range = list(range(1,10))\nk_scores = []\nfor k in k_range:\n    gbc = GradientBoostingClassifier(max_depth=k)\n    scores = cross_val_score(gbc, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    k_scores.append(scores.mean())\n    # Code for plotting results\n    gbc.fit(features_train, target_train)\n    train_pred = gbc.predict(features_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous train results\n    train_results.append(roc_auc)\n    y_pred = gbc.predict(features_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous test results\n    test_results.append(roc_auc)\nif DEBUG:\n    print(k_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(k_range, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(k_range, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('Max Depth')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest train model. Call up my model and name it clf\nclf_gbc1 = GradientBoostingClassifier(max_depth=2)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_gbc1)\n#Fit clf to the training data\nclf_gbc1 = clf_gbc1.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_gbc1 = clf_gbc1.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Gradient Boost Model 2 Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_gbc1 = accuracy_score(target_test, target_predicted_gbc1)\nprec_gbc1 = precision_score(target_test, target_predicted_gbc1)\nrecall_gbc1 = recall_score(target_test, target_predicted_gbc1)\nf1_gbc1 = f1_score(target_test, target_predicted_gbc1)\ncm_gbc1 = confusion_matrix(target_test, target_predicted_gbc1)\nprint(\"GBC Accuracy Score\", acc_gbc1)\nprint(classification_report(target_test, target_predicted_gbc1))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_gbc1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_gbc1, annot=True, fmt='d')\nplt.title('Gradient Boost Model 2 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_gbc1))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Gradient Boost Model 2 Cross Validation (CV = 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify RF with Cross Validation\nscores_gbc1 = cross_val_score(clf_gbc1, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_gbc1)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_gbc1.mean(), scores_gbc1.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the Random Forest Model is 95% with a variance of 1%."},{"metadata":{},"cell_type":"markdown","source":"#### Gradient Boost Model 2 Area Under Curve (AUC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_gbc1.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_gbc1 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_gbc1)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_gbc1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Gradient Boost Model 2 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Gradient Boost Model 3"},{"metadata":{},"cell_type":"markdown","source":"For Gradient Boost Model 3, I chose to keep **max_depth=2** and optimize **n_estimators** which controls the number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_results = []\ntest_results = []\n# search for an optimal value of max_depth for decision tree\nk_range = list(range(100,200))\nk_scores = []\nfor k in k_range:\n    gbc = GradientBoostingClassifier(n_estimators=k, max_depth=2)\n    scores = cross_val_score(gbc, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    k_scores.append(scores.mean())\n    # Code for plotting results\n    gbc.fit(features_train, target_train)\n    train_pred = gbc.predict(features_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous train results\n    train_results.append(roc_auc)\n    y_pred = gbc.predict(features_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous test results\n    test_results.append(roc_auc)\nif DEBUG:\n    print(k_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(k_range, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(k_range, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('N Estimators')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the analysis above, I choose **n_estimators=140**."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest train model. Call up my model and name it clf\nclf_gbc2 = GradientBoostingClassifier(n_estimators=140, max_depth=2)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_gbc2)\n#Fit clf to the training data\nclf_gbc2 = clf_gbc2.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_gbc2 = clf_gbc2.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Gradient Boost Model 3 Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_gbc2 = accuracy_score(target_test, target_predicted_gbc2)\nprec_gbc2 = precision_score(target_test, target_predicted_gbc2)\nrecall_gbc2 = recall_score(target_test, target_predicted_gbc2)\nf1_gbc2 = f1_score(target_test, target_predicted_gbc2)\ncm_gbc2 = confusion_matrix(target_test, target_predicted_gbc2)\nprint(\"GBC Accuracy Score\", acc_gbc2)\nprint(classification_report(target_test, target_predicted_gbc2))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_gbc2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_gbc2, annot=True, fmt='d')\nplt.title('Gradient Boost Model 3 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_gbc2))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Gradient Boost Model 3 Cross Validation (CV = 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify RF with Cross Validation\nscores_gbc2 = cross_val_score(clf_gbc2, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_gbc2)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_gbc2.mean(), scores_gbc2.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the Gradient Boost Model 3 is 80% with a variance of 4%."},{"metadata":{},"cell_type":"markdown","source":"#### Gradient Boost Model 3 Area Under Curve (AUC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_gbc2.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_gbc2 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_gbc2)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_gbc2)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Gradient Boost Model 3 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Gradient Boost Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"t = Table()\nt[''] = ['Gradient Boast Model 1','Gradient Boast Model 2','Gradient Boast Model 3']\nt['Cross Validation Score'] = [round(scores_gbc.mean(),4),round(scores_gbc1.mean(),4),round(scores_gbc2.mean(),4)]\nt['Accuracy Score'] = [round(acc_gbc,4),round(acc_gbc1,4),round(acc_gbc2,4)]\nt['Precision'] = [round(prec_gbc,4),round(prec_gbc1,4),round(prec_gbc2,4)]\nt['Recall'] = [round(recall_gbc,4),round(recall_gbc1,4),round(recall_gbc2,4)]\nt['F1 Score'] = [round(f1_gbc,4),round(f1_gbc1,4),round(f1_gbc2,4)]\nt['ROC AUC'] = [round(roc_auc_gbc,4),round(roc_auc_gbc1,4),round(roc_auc_gbc2,4)]\nt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Gradient Boost Model 3 produced the best recall score but only slightly. I chose this to be my best GBC model."},{"metadata":{"toc-hr-collapsed":false},"cell_type":"markdown","source":"## Stochastic Gradient Descent Model"},{"metadata":{},"cell_type":"markdown","source":"Using the [Stochastic Gradient Descent](https://scikit-learn.org/stable/modules/sgd.html) library, I created three Stochastic Gradient Descent Models. For the first model, I set **loss='modified_huber'**. **'modified_huber'** is a smooth loss that brings tolerance to outliers as well as probability estimates."},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Stochastic Gradient Descent \"Modified Huber\" Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n# Random Forest train model. Call up my model and name it clf\nclf_sgd_huber = SGDClassifier(loss='modified_huber', penalty='l2', n_jobs=-1, max_iter=1000)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_sgd_huber)\n#Fit clf to the training data\nclf_sgd_huber = clf_sgd_huber.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_sgd_huber = clf_sgd_huber.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Stochastic Gradient Descent \"Modified Huber\" Model Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_sgd_huber = accuracy_score(target_test, target_predicted_sgd_huber)\nprec_sgd_huber = precision_score(target_test, target_predicted_sgd_huber)\nrecall_sgd_huber = recall_score(target_test, target_predicted_sgd_huber)\nf1_sgd_huber = f1_score(target_test, target_predicted_sgd_huber)\ncm_sgd_huber = confusion_matrix(target_test, target_predicted_sgd_huber)\nprint(\"SGD Accuracy Score\", acc_sgd_huber)\nprint(classification_report(target_test, target_predicted_sgd_huber))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_sgd_huber))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_sgd_huber, annot=True, fmt='d')\nplt.title('Stochastic Gradient Descent \"Modified Huber\" Model Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_sgd_huber))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Stochastic Gradient Descent \"Modified Huber\" Model Cross Validation (CV = 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify RF with Cross Validation\nscores_sgd_huber = cross_val_score(clf_sgd_huber, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_sgd_huber)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_sgd_huber.mean(), scores_sgd_huber.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the Stochastic Gradient Descent \"Modified Huber\" Model is 80% with a variance of 4%."},{"metadata":{},"cell_type":"markdown","source":"#### Stochastic Gradient Descent \"Modified Huber\" Model Area Under Curve (AUC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_sgd_huber.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_sgd_huber = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_sgd_huber)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_sgd_huber)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Stochastic Gradient Descent \"Modified Huber\" Model ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Stochastic Gradient Descent \"Log\" Model"},{"metadata":{},"cell_type":"markdown","source":"For the SGD model below, I set **loss='log'**. The ‘log’ loss gives logistic regression, a probabilistic classifier. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest train model. Call up my model and name it clf\nclf_sgd_log = SGDClassifier(loss='log', n_jobs=-1, max_iter=1000)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_sgd_log)\n#Fit clf to the training data\nclf_sgd_log = clf_sgd_log.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_sgd_log = clf_sgd_log.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Stochastic Gradient Descent \"Log\" Model Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_sgd_log = accuracy_score(target_test, target_predicted_sgd_log)\nprec_sgd_log = precision_score(target_test, target_predicted_sgd_log)\nrecall_sgd_log = recall_score(target_test, target_predicted_sgd_log)\nf1_sgd_log = f1_score(target_test, target_predicted_sgd_log)\ncm_sgd_log = confusion_matrix(target_test, target_predicted_sgd_log)\nprint(\"SGD Accuracy Score\", acc_sgd_log)\nprint(classification_report(target_test, target_predicted_sgd_log))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_sgd_log))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_sgd_log, annot=True, fmt='d')\nplt.title('Stochastic Gradient Descent \"Log\" Model Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_sgd_log))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Stochastic Gradient Descent \"Log\" Model Cross Validation (CV = 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify RF with Cross Validation\nscores_sgd_log = cross_val_score(clf_sgd_log, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_sgd_log)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_sgd_log.mean(), scores_sgd_log.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the Stochastic Gradient Descent \"Log\" Model is 80% with a variance of 3%."},{"metadata":{},"cell_type":"markdown","source":"#### Stochastic Gradient Descent \"Log\" Model Area Under Curve (AUC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_sgd_log.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_sgd_log = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_sgd_log)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_sgd_log)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Stochastic Gradient Descent \"Log\" Model ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Stochastic Gradient Descent \"Log L1\" Model"},{"metadata":{},"cell_type":"markdown","source":"For this SGD model, I kept **loss='log'** and changed **penalty='l1'**. 'l1' might bring sparsity to the model (feature selection) not achievable with 'l2'."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest train model. Call up my model and name it clf\nclf_sgd_logl1 = SGDClassifier(loss='log', penalty='l1', n_jobs=-1, max_iter=1000)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_sgd_logl1)\n#Fit clf to the training data\nclf_sgd_logl1 = clf_sgd_logl1.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_sgd_logl1 = clf_sgd_logl1.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Stochastic Gradient Descent \"Log L1\" Model Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_sgd_logl1 = accuracy_score(target_test, target_predicted_sgd_logl1)\nprec_sgd_logl1 = precision_score(target_test, target_predicted_sgd_logl1)\nrecall_sgd_logl1 = recall_score(target_test, target_predicted_sgd_logl1)\nf1_sgd_logl1 = f1_score(target_test, target_predicted_sgd_logl1)\ncm_sgd_logl1 = confusion_matrix(target_test, target_predicted_sgd_logl1)\nprint(\"SGD Accuracy Score\", acc_sgd_logl1)\nprint(classification_report(target_test, target_predicted_sgd_logl1))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_sgd_logl1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_sgd_logl1, annot=True, fmt='d')\nplt.title('Stochastic Gradient Descent \"Log L1\" Model Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_sgd_logl1))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Stochastic Gradient Descent \"Log L1\" Model Cross Validation (CV = 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify RF with Cross Validation\nscores_sgd_logl1 = cross_val_score(clf_sgd_logl1, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_sgd_logl1)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_sgd_logl1.mean(), scores_sgd_logl1.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the Stochastic Gradient Descent \"Log L1\" Model is 80% with a variance of 3%."},{"metadata":{},"cell_type":"markdown","source":"#### Stochastic Gradient Descent \"Log L1\" Model Area Under Curve (AUC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_sgd_logl1.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_sgd_logl1 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_sgd_logl1)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_sgd_logl1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Stochastic Gradient Descent \"Log L1\" Model ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Stochastic Gradient Descent Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"t = Table()\nt[''] = ['SGD Modified Huber Model','SGD Log Model','SGD Log L1 Model']\nt['Cross Validation Score'] = [round(scores_sgd_huber.mean(),4),round(scores_sgd_log.mean(),4),round(scores_sgd_logl1.mean(),4)]\nt['Accuracy Score'] = [round(acc_sgd_huber,4),round(acc_sgd_log,4),round(acc_sgd_logl1,4)]\nt['Precision'] = [round(prec_sgd_huber,4),round(prec_sgd_log,4),round(prec_sgd_logl1,4)]\nt['Recall'] = [round(recall_sgd_huber,4),round(recall_sgd_log,4),round(recall_sgd_logl1,4)]\nt['F1 Score'] = [round(f1_sgd_huber,4),round(f1_sgd_log,4),round(f1_sgd_logl1,4)]\nt['ROC AUC'] = [round(roc_auc_sgd_huber,4),round(roc_auc_sgd_log,4),round(roc_auc_sgd_logl1,4)]\nt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the analysis above, the Stochastic Gradient Descent \"Log\" Model was the most performent in terms of recall score."},{"metadata":{"toc-hr-collapsed":false},"cell_type":"markdown","source":"## Linear Support Vector Classification Model"},{"metadata":{},"cell_type":"markdown","source":"Using the [Linear Support Vector Classification](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) library, I created three Linear Support Vector Classification Models. I started with a baseline model below with default values."},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Linear Support Vector Classification Model 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC\n# Random Forest train model. Call up my model and name it clf\nclf_lsvm = LinearSVC()\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_lsvm)\n#Fit clf to the training data\nclf_lsvm = clf_lsvm.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_lsvm = clf_lsvm.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Linear Support Vector Classification Model 1 Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_lsvm = accuracy_score(target_test, target_predicted_lsvm)\nprec_lsvm = precision_score(target_test, target_predicted_lsvm)\nrecall_lsvm = recall_score(target_test, target_predicted_lsvm)\nf1_lsvm = f1_score(target_test, target_predicted_lsvm)\ncm_lsvm = confusion_matrix(target_test, target_predicted_lsvm)\nprint(\"LSVM Accuracy Score\", acc_lsvm)\nprint(classification_report(target_test, target_predicted_lsvm))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_lsvm))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_lsvm, annot=True, fmt='d')\nplt.title('Linear Support Vector Classification Model 1 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_lsvm))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Linear Support Vector Classification Model 1 Cross Validation (CV = 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify RF with Cross Validation\nscores_lsvm = cross_val_score(clf_lsvm, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_lsvm)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_lsvm.mean(), scores_lsvm.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the Linear Support Vector Classification Model 1 is 80% with a variance of 4%."},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Linear Support Vector Classification Model 2"},{"metadata":{},"cell_type":"markdown","source":"For Linear Support Vector Classification Model 2, I set **penalty='l1'** and **dual=False**. The 'l1' leads to coef_ vectors that are sparse. The **dual** parameter selects the algorithm to either solve the dual or primal optimization problem. Prefer **dual=False** when n_samples > n_features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest train model. Call up my model and name it clf\nclf_lsvml1 = LinearSVC(penalty='l1',dual=False)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_lsvml1)\n#Fit clf to the training data\nclf_lsvml1 = clf_lsvml1.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_lsvml1 = clf_lsvml1.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Linear Support Vector Classification Model 2 Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_lsvml1 = accuracy_score(target_test, target_predicted_lsvml1)\nprec_lsvml1 = precision_score(target_test, target_predicted_lsvml1)\nrecall_lsvml1 = recall_score(target_test, target_predicted_lsvml1)\nf1_lsvml1 = f1_score(target_test, target_predicted_lsvml1)\ncm_lsvml1 = confusion_matrix(target_test, target_predicted_lsvml1)\nprint(\"LSVM Accuracy Score\", acc_lsvml1)\nprint(classification_report(target_test, target_predicted_lsvml1))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_lsvml1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_lsvml1, annot=True, fmt='d')\nplt.title('Linear Support Vector Classification Model 2 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_lsvml1))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Linear Support Vector Classification Model 2 Cross Validation (CV = 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify RF with Cross Validation\nscores_lsvml1 = cross_val_score(clf_lsvml1, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_lsvml1)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_lsvml1.mean(), scores_lsvml1.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the Linear Support Vector Classification Model 2 is 80% with a variance of 4%."},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Linear Support Vector Classification Model 3"},{"metadata":{},"cell_type":"markdown","source":"For Linear Support Vector Classification Model 3, I set **penalty='l2'** and used the code below to optimize the penalty parameter **C** of the error term."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_results = []\ntest_results = []\n# search for an optimal value of max_depth for decision tree\nk_range = list(range(1,10))\nk_scores = []\nfor k in k_range:\n    lsvm = LinearSVC(penalty='l2',C=k,dual=False,fit_intercept=False)\n    scores = cross_val_score(lsvm, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    k_scores.append(scores.mean())\n    # Code for plotting results\n    lsvm.fit(features_train, target_train)\n    train_pred = lsvm.predict(features_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous train results\n    train_results.append(roc_auc)\n    y_pred = lsvm.predict(features_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous test results\n    test_results.append(roc_auc)\nif DEBUG:\n    print(k_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(k_range, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(k_range, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('C Value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the chart above, I chose **C=3**."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest train model. Call up my model and name it clf\nclf_lsvml2 = LinearSVC(penalty='l2',C=3,dual=False,fit_intercept=False)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_lsvml2)\n#Fit clf to the training data\nclf_lsvml2 = clf_lsvml2.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_lsvml2 = clf_lsvml2.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Linear Support Vector Classification Model 3 Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_lsvml2 = accuracy_score(target_test, target_predicted_lsvml2)\nprec_lsvml2 = precision_score(target_test, target_predicted_lsvml2)\nrecall_lsvml2 = recall_score(target_test, target_predicted_lsvml2)\nf1_lsvml2 = f1_score(target_test, target_predicted_lsvml2)\ncm_lsvml2 = confusion_matrix(target_test, target_predicted_lsvml2)\nprint(\"LSVM Accuracy Score\", acc_lsvml2)\nprint(classification_report(target_test, target_predicted_lsvml2))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_lsvml2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_lsvml2, annot=True, fmt='d')\nplt.title('Linear Support Vector Classification Model 3 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_lsvml2))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Linear Support Vector Classification Model 3 Cross Validation (CV = 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify RF with Cross Validation\nscores_lsvml2 = cross_val_score(clf_lsvml2, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_lsvml2)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_lsvml2.mean(), scores_lsvml2.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the Linear Support Vector Classification Model 3 is 80% with a variance of 4%."},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Linear Support Vector Classification Model Evaluation"},{"metadata":{},"cell_type":"markdown","source":"From my analysis, the Random Forest and Decision Tree Models performed significantly better than the default KNN=3 Model. Below, I review the metrcs returned and go into some detail behind the metrics."},{"metadata":{"trusted":true},"cell_type":"code","source":"t = Table()\nt[''] = ['Linear SVC Model 1','Linear SVC Model 2','Linear SVC Model 3']\nt['Cross Validation Score'] = [round(scores_lsvm.mean(),4),round(scores_lsvml1.mean(),4),round(scores_lsvml2.mean(),4)]\nt['Accuracy Score'] = [round(acc_lsvm,4),round(acc_lsvml1,4),round(acc_lsvml2,4)]\nt['Precision'] = [round(prec_lsvm,4),round(prec_lsvml1,4),round(prec_lsvml2,4)]\nt['Recall'] = [round(recall_lsvm,4),round(recall_lsvml1,4),round(recall_lsvml2,4)]\nt['F1 Score'] = [round(f1_lsvm,4),round(f1_lsvml1,4),round(f1_lsvml2,4)]\nt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the analysis above, the Linear Support Vector Classification Model 3 produced the best recall score."},{"metadata":{"toc-hr-collapsed":false},"cell_type":"markdown","source":"## Support Vector Model RBF"},{"metadata":{},"cell_type":"markdown","source":"Using the [sklearn.svm.SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) library, I created a C-Support Vector Classification Model. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. For the first model, I set **gamma='auto'** which uses 1 / n_features."},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Support Vector RBF Model 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n# Random Forest train model. Call up my model and name it clf\nclf_svc = SVC(probability=True, gamma='auto', max_iter=-1)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_svc)\n#Fit clf to the training data\nclf_svc = clf_svc.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_svc = clf_svc.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Support Vector RBF Model 1 Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_svc = accuracy_score(target_test, target_predicted_svc)\nprec_svc = precision_score(target_test, target_predicted_svc)\nrecall_svc = recall_score(target_test, target_predicted_svc)\nf1_svc = f1_score(target_test, target_predicted_svc)\ncm_svc = confusion_matrix(target_test, target_predicted_svc)\nprint(\"SVC Accuracy Score\", acc_svc)\nprint(classification_report(target_test, target_predicted_svc))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_svc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_svc, annot=True, fmt='d')\nplt.title('SVC Mode 1 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_svc))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Support Vector RBF Model 1 Cross Validation (CV = 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify RF with Cross Validation\nscores_svc = cross_val_score(clf_svc, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_svc)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_svc.mean(), scores_svc.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the SVC Model 1 is 80% with a variance of 3%."},{"metadata":{},"cell_type":"markdown","source":"#### Support Vector RBF Model 1 Area Under Curve (AUC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_svc.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_svc = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_svc)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_svc)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('SVM Model 1 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Support Vector RBF Model 2"},{"metadata":{},"cell_type":"markdown","source":"For the model below, I changed **gamma='scale'**. The model now uses 1 / (n_features * X.var()) as value of gamma. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest train model. Call up my model and name it clf\nclf_svc1 = SVC(probability=True, gamma='scale', max_iter=-1)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_svc1)\n#Fit clf to the training data\nclf_svc1 = clf_svc1.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_svc1 = clf_svc1.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Support Vector RBF Model 2 Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_svc1 = accuracy_score(target_test, target_predicted_svc1)\nprec_svc1 = precision_score(target_test, target_predicted_svc1)\nrecall_svc1 = recall_score(target_test, target_predicted_svc1)\nf1_svc1 = f1_score(target_test, target_predicted_svc1)\ncm_svc1 = confusion_matrix(target_test, target_predicted_svc1)\nprint(\"SVC Accuracy Score\", acc_svc1)\nprint(classification_report(target_test, target_predicted_svc1))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_svc1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_svc1, annot=True, fmt='d')\nplt.title('SVC Model 2 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_svc1))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Support Vector RBF Model 2 Cross Validation (CV = 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify RF with Cross Validation\nscores_svc1 = cross_val_score(clf_svc1, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_svc1)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_svc1.mean(), scores_svc1.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the Support Vector RBF Model 2 is 79% with a variance of 3%."},{"metadata":{},"cell_type":"markdown","source":"#### Support Vector RBF Model 2 Area Under Curve (AUC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_svc1.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_svc1 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_svc1)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_svc1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Support Vector RBF Model 2 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Support Vector RBF Model 3"},{"metadata":{},"cell_type":"markdown","source":"To tune SVC Model 2, I kept the existing parameters but chose to optimize the penalty parameter **C** of the error term."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_results = []\ntest_results = []\n# search for an optimal value of max_depth for decision tree\n#k_range = list(range(1,100))\nk_range = np.linspace(0.1, 2.0, 20, endpoint=True)\nk_scores = []\nfor k in k_range:\n    svc = SVC(C=k, probability=True, gamma='scale', max_iter=-1)\n    scores = cross_val_score(svc, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    k_scores.append(scores.mean())\n    # Code for plotting results\n    svc.fit(features_train, target_train)\n    train_pred = svc.predict(features_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous train results\n    train_results.append(roc_auc)\n    y_pred = svc.predict(features_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous test results\n    test_results.append(roc_auc)\nif DEBUG:\n    print(k_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(k_range, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(k_range, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('C Value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest train model. Call up my model and name it clf\nclf_svc2 = SVC(C=1.25, probability=True, gamma='scale', max_iter=-1)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_svc2)\n#Fit clf to the training data\nclf_svc2 = clf_svc2.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_svc2 = clf_svc2.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Support Vector RBF Model 3 Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_svc2 = accuracy_score(target_test, target_predicted_svc2)\nprec_svc2 = precision_score(target_test, target_predicted_svc2)\nrecall_svc2 = recall_score(target_test, target_predicted_svc2)\nf1_svc2 = f1_score(target_test, target_predicted_svc2)\ncm_svc2 = confusion_matrix(target_test, target_predicted_svc2)\nprint(\"SVC Accuracy Score\", acc_svc2)\nprint(classification_report(target_test, target_predicted_svc2))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_svc2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_svc2, annot=True, fmt='d')\nplt.title('Support Vector RBF Model 3 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_svc2))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Support Vector RBF Model 3 Cross Validation (CV = 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify RF with Cross Validation\nscores_svc2 = cross_val_score(clf_svc2, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_svc2)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_svc2.mean(), scores_svc2.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the Support Vector RBF Model 3 is 79% with a variance of 3%."},{"metadata":{},"cell_type":"markdown","source":"#### Support Vector RBF Model 3 Area Under Curve (AUC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_svc2.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_svc2 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_svc2)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_svc2)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Support Vector RBF Model 3 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Support Vector RBF Model Evaluation"},{"metadata":{},"cell_type":"markdown","source":"From my analysis, the Random Forest and Decision Tree Models performed significantly better than the default KNN=3 Model. Below, I review the metrcs returned and go into some detail behind the metrics."},{"metadata":{"trusted":true},"cell_type":"code","source":"t = Table()\nt[''] = ['Support Vector RBF Model 1','Support Vector RBF Model 2','Support Vector RBF Model 3']\nt['Cross Validation Score'] = [round(scores_svc.mean(),4),round(scores_svc1.mean(),4),round(scores_svc2.mean(),4)]\nt['Accuracy Score'] = [round(acc_svc,4),round(acc_svc1,4),round(acc_svc2,4)]\nt['Precision'] = [round(prec_svc,4),round(prec_svc1,4),round(prec_svc2,4)]\nt['Recall'] = [round(recall_svc,4),round(recall_svc1,4),round(recall_svc2,4)]\nt['F1 Score'] = [round(f1_svc,4),round(f1_svc1,4),round(f1_svc2,4)]\nt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the analysis above, the Support Vector RBF Model 1 produced the best accuracy and recall scores."},{"metadata":{"toc-hr-collapsed":false},"cell_type":"markdown","source":"## Neural Network Classification Model"},{"metadata":{},"cell_type":"markdown","source":"Using the [sklearn.neural_network.MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier) library, I created three Multi-layer Perceptron Classification Model. The model below uses **hidden_layer_sizes=(20,8)**. This represents the ith element represents the number of neurons in the ith hidden layer. The activation function for the hidden layer has been set to ‘tanh’, the hyperbolic tan function, returns f(x) = tanh(x). The default **solver='adam'** is maintained."},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Multi-layer Perceptron Classifier Model 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\n# Random Forest train model. Call up my model and name it clf\nclf_NN = MLPClassifier(activation='tanh', solver='adam', hidden_layer_sizes=(20,8), max_iter=1000)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_NN)\n#Fit clf to the training data\nclf_NN = clf_NN.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_NN = clf_NN.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Multi-layer Perceptron Classifier Model 1 Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_NN = accuracy_score(target_test, target_predicted_NN)\nprec_NN = precision_score(target_test, target_predicted_NN)\nrecall_NN = recall_score(target_test, target_predicted_NN)\nf1_NN = f1_score(target_test, target_predicted_NN)\ncm_NN = confusion_matrix(target_test, target_predicted_NN)\nprint(\"MLP Accuracy Score\", acc_NN)\nprint(classification_report(target_test, target_predicted_NN))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_NN))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_NN, annot=True, fmt='d')\nplt.title('MLP Model 1 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_NN))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Multi-layer Perceptron Classifier Model 1 Cross Validation (CV = 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify RF with Cross Validation\nscores_NN = cross_val_score(clf_NN, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_NN)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_NN.mean(), scores_NN.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the Multi-layer Perceptron Classifier Model 1 is 79% with a variance of 3%."},{"metadata":{},"cell_type":"markdown","source":"#### Multi-layer Perceptron Classifier Model 1 Area Under Curve (AUC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_NN.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_NN = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_NN)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_NN)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('MLP Model 1 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Multi-layer Perceptron Classifier Model 2"},{"metadata":{},"cell_type":"markdown","source":"For Model 2, I changed acitivation to ‘relu’, the rectified linear unit function, returns f(x) = max(0, x). hidden_layer_sizes has been set to (15,)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest train model. Call up my model and name it clf\nclf_NN1 =MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',\n       beta_1=0.9, beta_2=0.999, early_stopping=False,\n       epsilon=1e-08, hidden_layer_sizes=(15,), learning_rate='constant',\n       learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n       warm_start=False)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_NN1)\n#Fit clf to the training data\nclf_NN1 = clf_NN1.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_NN1 = clf_NN1.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Multi-layer Perceptron Classifier Model 2 Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_NN1 = accuracy_score(target_test, target_predicted_NN1)\nprec_NN1 = precision_score(target_test, target_predicted_NN1)\nrecall_NN1 = recall_score(target_test, target_predicted_NN1)\nf1_NN1 = f1_score(target_test, target_predicted_NN1)\ncm_NN1 = confusion_matrix(target_test, target_predicted_NN1)\nprint(\"MLP Accuracy Score\", acc_NN1)\nprint(classification_report(target_test, target_predicted_NN1))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_NN1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_NN1, annot=True, fmt='d')\nplt.title('MLP Model 2 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_NN1))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Multi-layer Perceptron Classifier Model 2 Cross Validation (CV = 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify RF with Cross Validation\nscores_NN1 = cross_val_score(clf_NN1, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_NN1)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_NN1.mean(), scores_NN1.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the Multi-layer Perceptron Classifier Model 2 is 79% with a variance of 4%."},{"metadata":{},"cell_type":"markdown","source":"#### Multi-layer Perceptron Classifier Model 2 Area Under Curve (AUC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_NN1.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_NN1 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_NN1)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_NN1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('MLP Model 2 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Multi-layer Perceptron Classifier Model 3"},{"metadata":{},"cell_type":"markdown","source":"For MLP Model 3, I changed **solver='sgd'** and introduced three layers of 30 for hidden_layer_sizes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest train model. Call up my model and name it clf\nclf_NN2 = MLPClassifier(solver='sgd', hidden_layer_sizes=(30,30,30),max_iter=1000)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_NN2)\n#Fit clf to the training data\nclf_NN2 = clf_NN2.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_NN2 = clf_NN2.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Multi-layer Perceptron Classifier Model 3 Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_NN2 = accuracy_score(target_test, target_predicted_NN2)\nprec_NN2 = precision_score(target_test, target_predicted_NN2)\nrecall_NN2 = recall_score(target_test, target_predicted_NN2)\nf1_NN2 = f1_score(target_test, target_predicted_NN2)\ncm_NN2 = confusion_matrix(target_test, target_predicted_NN2)\nprint(\"MLP Accuracy Score\", acc_NN2)\nprint(classification_report(target_test, target_predicted_NN2))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_NN2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_NN2, annot=True, fmt='d')\nplt.title('MLP Model 3 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_NN2))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Multi-layer Perceptron Classifier Model 3 Cross Validation (CV = 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify RF with Cross Validation\nscores_NN2 = cross_val_score(clf_NN2, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_NN2)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_NN2.mean(), scores_NN2.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the Multi-layer Perceptron Classifier Model 3 is 80% with a variance of 3%."},{"metadata":{},"cell_type":"markdown","source":"#### Multi-layer Perceptron Classifier Model 3 Area Under Curve (AUC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_NN2.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_NN2 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_NN2)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_NN2)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('MLP Model 3 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Multiple-layer Perceptron Model Evaluation"},{"metadata":{},"cell_type":"markdown","source":"From my analysis, the Random Forest and Decision Tree Models performed significantly better than the default KNN=3 Model. Below, I review the metrcs returned and go into some detail behind the metrics."},{"metadata":{"trusted":true},"cell_type":"code","source":"t = Table()\nt[''] = ['MLP Classifier Model 1','MLP Classifier Model 2','MLP Classifier Model 3']\nt['Cross Validation Score'] = [round(scores_NN.mean(),4),round(scores_NN1.mean(),4),round(scores_NN2.mean(),4)]\nt['Accuracy Score'] = [round(acc_NN,4),round(acc_NN1,4),round(acc_NN2,4)]\nt['Precision'] = [round(prec_NN,4),round(prec_NN1,4),round(prec_NN2,4)]\nt['Recall'] = [round(recall_NN,4),round(recall_NN1,4),round(recall_NN2,4)]\nt['F1 Score'] = [round(f1_NN,4),round(f1_NN1,4),round(f1_NN2,4)]\nt['ROC AUC'] = [round(roc_auc_NN,4),round(roc_auc_NN1,4),round(roc_auc_NN2,4)]\nt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the analysis above, Multi-layer Perceptron Classifier Model 3 produced the highest accuracy and recall scores."},{"metadata":{"toc-hr-collapsed":false},"cell_type":"markdown","source":"## AdaBoost Classifier Model"},{"metadata":{},"cell_type":"markdown","source":"Using the [sklearn.ensemble.AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) library, I created an AdaBoost Classification Model. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. For the first boosting model, I chose the Decision Tree Model 1."},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### AdaBoost Classifier Model 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n# Random Forest train model. Call up my model and name it clf\nclf_ada = AdaBoostClassifier(base_estimator=clf_dt2)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_ada)\n#Fit clf to the training data\nclf_ada = clf_ada.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_ada = clf_ada.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### AdaBoost Classifier Model 1 Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_ada = accuracy_score(target_test, target_predicted_ada)\nprec_ada = precision_score(target_test, target_predicted_ada)\nrecall_ada = recall_score(target_test, target_predicted_ada)\nf1_ada = f1_score(target_test, target_predicted_ada)\ncm_ada = confusion_matrix(target_test, target_predicted_ada)\nprint(\"Ada Accuracy Score\", acc_ada)\nprint(classification_report(target_test, target_predicted_ada))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_ada))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_ada, annot=True, fmt='d')\nplt.title('AdaBoost Classifier Model 1 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_ada))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### AdaBoost Classifier Model 1 Cross Validation (CV = 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify RF with Cross Validation\nscores_ada = cross_val_score(clf_ada, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_ada)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_ada.mean(), scores_ada.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the AdaBoost Classifier Model 1 is 78% with a variance of 4%."},{"metadata":{},"cell_type":"markdown","source":"#### Support Vector RBF Model 1 Area Under Curve (AUC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_ada.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_ada = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_ada)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_ada)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('AdaBoost Classifier Model 1 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### AdaBoost Classifier Model 2"},{"metadata":{},"cell_type":"markdown","source":"For AdaBoost Classifier Model 2, I choose the Gradient Boost Model 3 as the base_estimator."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest train model. Call up my model and name it clf\nclf_ada1 = AdaBoostClassifier(base_estimator=clf_gbc2)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_ada1)\n#Fit clf to the training data\nclf_ada1 = clf_ada1.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_ada1 = clf_ada1.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### AdaBoost Classifier Model 2 Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_ada1 = accuracy_score(target_test, target_predicted_ada1)\nprec_ada1 = precision_score(target_test, target_predicted_ada1)\nrecall_ada1 = recall_score(target_test, target_predicted_ada1)\nf1_ada1 = f1_score(target_test, target_predicted_ada1)\ncm_ada1 = confusion_matrix(target_test, target_predicted_ada1)\nprint(\"Ada Accuracy Score\", acc_ada1)\nprint(classification_report(target_test, target_predicted_ada1))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_ada1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_ada1, annot=True, fmt='d')\nplt.title('AdaBoost Classifier Model 2 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_ada1))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### AdaBoost Classifier Model 2 Cross Validation (CV = 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify RF with Cross Validation\nscores_ada1 = cross_val_score(clf_ada1, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_ada1)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_ada1.mean(), scores_ada1.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the AdaBoost Classifier Model 2 is 77% with a variance of 3%."},{"metadata":{},"cell_type":"markdown","source":"#### AdaBoost Classifier Model 2 Area Under Curve (AUC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_ada1.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_ada1 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_ada1)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_ada1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('AdaBoost Classifier Model 2 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### AdaBoost Classifier Model 3"},{"metadata":{},"cell_type":"markdown","source":"For AdaBoost Classifier Model 3, I chose Support Vector RBF Model 1 as base_estimator."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest train model. Call up my model and name it clf\nclf_ada2 = AdaBoostClassifier(base_estimator=clf_svc)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_ada2)\n#Fit clf to the training data\nclf_ada2 = clf_ada2.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_ada2 = clf_ada2.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### AdaBoost Classifier Model 3 Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_ada2 = accuracy_score(target_test, target_predicted_ada2)\nprec_ada2 = precision_score(target_test, target_predicted_ada2)\nrecall_ada2 = recall_score(target_test, target_predicted_ada2)\nf1_ada2 = f1_score(target_test, target_predicted_ada2)\ncm_ada2 = confusion_matrix(target_test, target_predicted_ada2)\nprint(\"Ada Accuracy Score\", acc_ada2)\nprint(classification_report(target_test, target_predicted_ada2))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_ada2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_ada2, annot=True, fmt='d')\nplt.title('AdaBoost Classifier Model 3 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_ada2))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### AdaBoost Classifier Model 3 Cross Validation (CV = 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify RF with Cross Validation\nscores_ada2 = cross_val_score(clf_ada2, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_ada2)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_ada2.mean(), scores_ada2.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the AdaBoost Classifier Model 3 is 78% with a variance of 3%."},{"metadata":{},"cell_type":"markdown","source":"#### AdaBoost Classifier Model 3 Area Under Curve (AUC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_ada2.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_ada2 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_ada2)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_ada2)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('AdaBoost Classifier Model 3 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### AdaBoost Classifier Model Evaluation"},{"metadata":{},"cell_type":"markdown","source":"From my analysis, the Random Forest and Decision Tree Models performed significantly better than the default KNN=3 Model. Below, I review the metrcs returned and go into some detail behind the metrics."},{"metadata":{"trusted":true},"cell_type":"code","source":"t = Table()\nt[''] = ['AdaBoost Classifier Model 1','AdaBoost Classifier Model 2','AdaBoost Classifier Model 3']\nt['Cross Validation Score'] = [round(scores_ada.mean(),4),round(scores_ada1.mean(),4),round(scores_ada2.mean(),4)]\nt['Accuracy Score'] = [round(acc_ada,4),round(acc_ada1,4),round(acc_ada2,4)]\nt['Precision'] = [round(prec_ada,4),round(prec_ada1,4),round(prec_ada2,4)]\nt['Recall'] = [round(recall_ada,4),round(recall_ada1,4),round(recall_ada2,4)]\nt['F1 Score'] = [round(f1_ada,4),round(f1_ada1,4),round(f1_ada2,4)]\nt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the analysis above, the AdaBoost Classifier Model 3 was the most performant."},{"metadata":{"toc-hr-collapsed":false},"cell_type":"markdown","source":"## Stacking Model"},{"metadata":{},"cell_type":"markdown","source":"Using the [sklearn.ensemble.VotingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html) library, I created three Voting Classifier Models to stack previous models. The idea behind the VotingClassifier is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses. For the models below, the VotingClassifier (with voting='hard') would classify the sample as “class 1” based on the majority class label."},{"metadata":{},"cell_type":"markdown","source":"For the first model, I chose to stack KNN Model 3, Decision Tree Model 3, and Random Forest Model 1 as these were deemed the best for recall."},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Stacking Model 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n# Random Forest train model. Call up my model and name it clf\nclf1 = clf_knn2\nclf2 = clf_dt2\nclf3 = clf_rf\nclf_eclf = VotingClassifier(estimators=[('knn', clf1), ('dt', clf2), ('rf', clf3)], voting='hard', n_jobs=-1)\n\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_eclf)\n#Fit clf to the training data\nclf_eclf = clf_eclf.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_eclf = clf_eclf.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Stacking Model 1 Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_eclf = accuracy_score(target_test, target_predicted_eclf)\nprec_eclf = precision_score(target_test, target_predicted_eclf)\nrecall_eclf = recall_score(target_test, target_predicted_eclf)\nf1_eclf = f1_score(target_test, target_predicted_eclf)\ncm_eclf = confusion_matrix(target_test, target_predicted_eclf)\nprint(\"Stacking Accuracy Score\", acc_eclf)\nprint(classification_report(target_test, target_predicted_eclf))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_eclf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_eclf, annot=True, fmt='d')\nplt.title('Stacking Model 1 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_eclf))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Stacking Model 1 Cross Validation (CV = 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify RF with Cross Validation\nscores_eclf = cross_val_score(clf_eclf, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_eclf)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_eclf.mean(), scores_eclf.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the Stacking Model 1 is 78% with a variance of 4%."},{"metadata":{},"cell_type":"markdown","source":"#### Stacking Model 1 Recall Scores"},{"metadata":{"trusted":true},"cell_type":"code","source":"for MV, label in zip([clf1, clf2, clf3, clf_eclf], ['KNN', 'Decision Tree', 'Random Forest', 'Ensemble Model 1']):\n    scores2 = cross_val_score(MV, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    print(\"Recall: %0.2f (+/- %0.2f) [%s]\" % (scores2.mean(), scores2.std(), label))","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Stacking Model 2"},{"metadata":{},"cell_type":"markdown","source":"For Stacking Model 2, I chose Bagging Classifier Model 1, Extra Trees Model 3, and Gradient Boost Model 3 as these were deemed the best for recall."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest train model. Call up my model and name it clf\nclf1 = clf_bag\nclf2 = clf_xdt2\nclf3 = clf_gbc2\nclf_eclf1 = VotingClassifier(estimators=[('bag', clf1), ('xdt', clf2), ('sgd', clf3)], voting='hard', n_jobs=-1)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_eclf1)\n#Fit clf to the training data\nclf_eclf1 = clf_eclf1.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_eclf1 = clf_eclf1.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Stacking Model 2 Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_eclf1 = accuracy_score(target_test, target_predicted_eclf1)\nprec_eclf1 = precision_score(target_test, target_predicted_eclf1)\nrecall_eclf1 = recall_score(target_test, target_predicted_eclf1)\nf1_eclf1 = f1_score(target_test, target_predicted_eclf1)\ncm_eclf1 = confusion_matrix(target_test, target_predicted_eclf1)\nprint(\"Stacking Accuracy Score\", acc_eclf1)\nprint(classification_report(target_test, target_predicted_eclf1))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_eclf1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_eclf1, annot=True, fmt='d')\nplt.title('Stacking Model 2 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_eclf1))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Stacking Model 2 Cross Validation (CV = 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify RF with Cross Validation\nscores_eclf1 = cross_val_score(clf_eclf1, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_eclf1)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_eclf1.mean(), scores_eclf1.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the Stacking Model 2 is 80% with a variance of 3%."},{"metadata":{},"cell_type":"markdown","source":"#### Stacking Model 2 Recall Scores"},{"metadata":{"trusted":true},"cell_type":"code","source":"for MV, label in zip([clf1, clf2, clf3, clf_eclf1], ['Bagging', 'Extra Trees', 'Gradient Boost Classification', 'Ensemble Model 2']):\n    scores2 = cross_val_score(MV, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    print(\"Recall: %0.2f (+/- %0.2f) [%s]\" % (scores2.mean(), scores2.std(), label))","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Stacking Model 3"},{"metadata":{},"cell_type":"markdown","source":"For Stacking Model 3, I chose Stochastic Gradient Descenst \"Log\" Model, Linear Support Vector Classification Model 1, Support Vector RBF Model 1, and Multi-layer Perceptron Classifier 3 as these were deemed the best for recall."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest train model. Call up my model and name it clf\nclf1 = clf_sgd_log\nclf2 = clf_lsvm\nclf3 = clf_svc\nclf4 = clf_NN2\nclf_eclf2 = VotingClassifier(estimators=[('sgd', clf1), ('lsvm', clf2), ('svc', clf3), ('nn', clf4)], voting='hard', n_jobs=-1)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_eclf2)\n#Fit clf to the training data\nclf_eclf2 = clf_eclf2.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_eclf2 = clf_eclf2.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Support Vector RBF Model 3 Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_eclf2 = accuracy_score(target_test, target_predicted_eclf2)\nprec_eclf2  = precision_score(target_test, target_predicted_eclf2)\nrecall_eclf2 = recall_score(target_test, target_predicted_eclf2)\nf1_eclf2 = f1_score(target_test, target_predicted_eclf2)\ncm_eclf2 = confusion_matrix(target_test, target_predicted_eclf2)\nprint(\"Stacking Accuracy Score\", acc_eclf2)\nprint(classification_report(target_test, target_predicted_eclf2))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_eclf2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_eclf2, annot=True, fmt='d')\nplt.title('Stacking Model 3 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_eclf2))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Support Vector RBF Model 3 Cross Validation (CV = 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify RF with Cross Validation\nscores_eclf2 = cross_val_score(clf_eclf2, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_eclf2)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_eclf2.mean(), scores_eclf2.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the Stacking Model 3 is 80% with a variance of 3%."},{"metadata":{},"cell_type":"markdown","source":"#### Stacking Model 3 Recall Scores"},{"metadata":{"trusted":true},"cell_type":"code","source":"for MV, label in zip([clf1, clf2, clf3, clf4, clf_eclf2], ['Stochastic Gradient Descent', 'Linear Support Vector Classification', 'Support Vector Model RBF', 'Neural Network', 'Ensemble Model 3']):\n    scores2 = cross_val_score(MV, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    print(\"Recall: %0.2f (+/- %0.2f) [%s]\" % (scores2.mean(), scores2.std(), label))","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Stacking Model 4"},{"metadata":{},"cell_type":"markdown","source":"For Stacking Model 4, I chose only the models above that produce a recall score greater than or equal to 0.50 with a low variance (STD)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest train model. Call up my model and name it clf\nclf1 = clf_dt2\nclf2 = clf_rf\nclf3 = clf_xdt2\nclf4 = clf_gbc2\nclf5 = clf_lsvm\nclf6 = clf_NN2\nclf_eclf3 = VotingClassifier(estimators=[('dt', clf1), ('rf', clf2), ('xdt', clf3), ('gbc', clf4), ('lsvm', clf5), ('nn', clf6)], voting='hard', n_jobs=-1)\n#clf_eclf3 = VotingClassifier(estimators=[('knn', clf1), ('dt', clf2), ('rf', clf3), ('bag', clf4), ('xdt', clf5), ('gbc', clf6), ('sgd', clf7), ('lsvm', clf8), ('svc', clf9), ('nn', clf10)], voting='hard', n_jobs=-1)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_eclf3)\n#Fit clf to the training data\nclf_eclf3 = clf_eclf3.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_eclf3 = clf_eclf3.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Stacking Model 4 Accuracy Scores and Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_eclf3 = accuracy_score(target_test, target_predicted_eclf3)\nprec_eclf3 = precision_score(target_test, target_predicted_eclf3)\nrecall_eclf3 = recall_score(target_test, target_predicted_eclf3)\nf1_eclf3 = f1_score(target_test, target_predicted_eclf3)\ncm_eclf3 = confusion_matrix(target_test, target_predicted_eclf3)\nprint(\"Stacking Accuracy Score\", acc_eclf3)\nprint(classification_report(target_test, target_predicted_eclf3))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_eclf3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_eclf3, annot=True, fmt='d')\nplt.title('Stacking Model 4 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_eclf3))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Stacking Model 4 Cross Validation (CV = 10)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#verify RF with Cross Validation\nscores_eclf3 = cross_val_score(clf_eclf3, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_eclf3)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_eclf3.mean(), scores_eclf3.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The mean score and the 95% confidence interval of the score estimate for the Stacking Model 4 is 80% with a variance of 4%."},{"metadata":{},"cell_type":"markdown","source":"#### Stacking Model 4 Recall Scores"},{"metadata":{"trusted":true},"cell_type":"code","source":"for MV, label in zip([clf1, clf2, clf3, clf4, clf5, clf6, clf_eclf3], ['Decision Tree','Random Forest','Extra Trees','Gradient Boost Classification','Linear Support Vector Classificaiton','Neural Network','Ensemble Model 4']):\n    scores2 = cross_val_score(MV, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    print(\"Recall: %0.2f (+/- %0.2f) [%s]\" % (scores2.mean(), scores2.std(), label))","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"### Stacking Model Evaluation"},{"metadata":{},"cell_type":"markdown","source":"From my analysis, the Random Forest and Decision Tree Models performed significantly better than the default KNN=3 Model. Below, I review the metrcs returned and go into some detail behind the metrics."},{"metadata":{"trusted":true},"cell_type":"code","source":"t = Table()\nt[''] = ['Stacking Model 1','Stacking Model 2','Stacking Model 3','Stacking Model 4']\nt['Cross Validation Score'] = [round(scores_eclf.mean(),4),round(scores_eclf1.mean(),4),round(scores_eclf2.mean(),4),round(scores_eclf3.mean(),4)]\nt['Accuracy Score'] = [round(acc_eclf,4),round(acc_eclf1,4),round(acc_eclf2,4),round(acc_eclf3,4)]\nt['Precision'] = [round(prec_eclf,4),round(prec_eclf1,4),round(prec_eclf2,4),round(prec_eclf3,4)]\nt['Recall'] = [round(recall_eclf,4),round(recall_eclf1,4),round(recall_eclf2,4),round(recall_eclf3,4)]\nt['F1 Score'] = [round(f1_eclf,4),round(f1_eclf1,4),round(f1_eclf2,4),round(f1_eclf3,4)]\nt","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true},"cell_type":"markdown","source":"# Model Evaluation and Summary"},{"metadata":{},"cell_type":"markdown","source":"After reviewing all models, I chose the Decision Tree Model 3 as the best model for this business problem. This model produced the highest recall score compared to all models."},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_dt2 = accuracy_score(target_test, target_predicted_dt2)\nprec_dt2 = precision_score(target_test, target_predicted_dt2)\nrecall_dt2 = recall_score(target_test, target_predicted_dt2)\nf1_dt2 = f1_score(target_test, target_predicted_dt2)\ncm_dt2 = confusion_matrix(target_test, target_predicted_dt2)\nprint(\"DT Accuracy Score\", acc_dt2)\nprint(classification_report(target_test, target_predicted_dt2))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_dt2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://seaborn.pydata.org/examples/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_dt2, annot=True, fmt='d')\nplt.title('Decision Tree Model 3 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_dt2))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above, we can see that this model did produce a high number of False Positives (Predicted Churn = Yes, Actual Churn = No). From a busines perspective, there is not as much cost associated to this versus the False Negatives (Predicted Churn = No, Actual Churn = Yes). Below is a recap of the importance of Recall."},{"metadata":{},"cell_type":"markdown","source":"## Recall Definition"},{"metadata":{},"cell_type":"markdown","source":"![Recall](https://cdn-images-1.medium.com/max/800/1*dXkDleGhA-jjZmZ1BlYKXg.png)"},{"metadata":{},"cell_type":"markdown","source":"With regards to Recall, the consequence could impact business decisions to minimize customer churn. Given that False Negatives indicates that our model predicted Churn = No but actual Churn = Yes, this performance metric proved to be of great importance. The Decision Tree Model 3 exhibited the best Recall score for all the models. "},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pydotplus\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport pydotplus\nimport collections\n\ndata_feature_names = features[1:]\ndot_data = tree.export_graphviz(clf_dt2, feature_names=data_feature_names, class_names=True, rounded = True, proportion = False, precision = 2, filled = True)\ngraph = pydotplus.graphviz.graph_from_dot_data(dot_data)\ncolors = ('turquoise', 'orange')\nedges = collections.defaultdict(list)\n\nfor edge in graph.get_edge_list():\n    edges[edge.get_source()].append(int(edge.get_destination()))\n\nfor edge in edges:\n    edges[edge].sort()    \n    for i in range(2):\n        dest = graph.get_node(str(edges[edge][i]))[0]\n        dest.set_fillcolor(colors[i])\n\nImage(graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the Decision Tree above, **Contract** and **Online Security** are key factors in determining Churn. We saw this during our initial EDA. **OnlineSecurity** indicates whether the customer has online security or not (Yes, No, No internet service). **Contract** indicates the contract term of the customer (Month-to-month, One year, Two year)."},{"metadata":{},"cell_type":"markdown","source":"If Contract is Month-to-Month (transformed to 0), we move left to the Online Security leaf. If Contract is One Year or Two Year (1 or 2), we move to the right leaf. Of the 2119 samples, 1989 observations did not churn while 130 did (class = y[0] indicates Churn = No)."},{"metadata":{},"cell_type":"markdown","source":"For the Online Security leaf, if Online Security is No (transformed to 0), we move the left leaf, else right (class = y[1] indicates Churn = Yes)."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"toc-autonumbering":true},"nbformat":4,"nbformat_minor":1}