{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Task for Today  \n\n***\n\n## Bestseller Genre Prediction  \n\nGiven *data about Amazon's Top 50 best selling books from 2009-2019*, let's try to predict the **genre** of a given book.  \n  \nWe will use a TensorFlow ANN with two inputs to make our predictions."},{"metadata":{},"cell_type":"markdown","source":"# Getting Started"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport re\nfrom nltk.corpus import stopwords\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/amazon-top-50-bestselling-books-2009-2019/bestsellers with categories.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = stopwords.words('english')\n\ndef process_name(name):\n    name = re.sub(r'\\d+', ' ', name)\n    name = name.split()\n    name = \" \".join([word for word in name if word not in stop_words])\n    return name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names = data['Name'].apply(process_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(names)\n\nvocab_length = len(tokenizer.word_index) + 1\n\nprint(\"Vocabulary length:\", vocab_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names = tokenizer.texts_to_sequences(names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_seq_length = np.max(list(map(lambda name: len(name), names)))\n\nprint(\"Max sequence length:\", max_seq_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names = pad_sequences(names, maxlen=max_seq_length, padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop('Name', axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Encoding Other Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"genre_mapping = {'Non Fiction': 0, 'Fiction': 1}\n\ndata['Genre'] = data['Genre'].replace(genre_mapping)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of unique authors:\", len(data['Author'].unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def onehot_encode(df, column, prefix):\n    df = df.copy()\n    dummies = pd.get_dummies(df[column], prefix=prefix)\n    df = pd.concat([df, dummies], axis=1)\n    df = df.drop(column, axis=1)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = onehot_encode(data, 'Author', 'auth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting/Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data['Genre'].copy()\nX = data.drop('Genre', axis=1).copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\n\nX = scaler.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names_train, names_test, X_train, X_test, y_train, y_test = train_test_split(names, X, y, train_size=0.7, random_state=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling/Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"names.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dim = 64\n\n# Name features\nname_input = tf.keras.Input(shape=(20,), name=\"name_input\")\n\nembedding = tf.keras.layers.Embedding(\n    input_dim=vocab_length,\n    output_dim=embedding_dim,\n    input_length=max_seq_length,\n    name=\"name_embedding\"\n)(name_input)\n\nname_flatten = tf.keras.layers.Flatten(name=\"name_flatten\")(embedding)\n\n\n# Other features\nother_input = tf.keras.Input(shape=(252,), name=\"other_input\")\n\nhidden_1 = tf.keras.layers.Dense(256, activation='relu', name=\"other_dense_1\")(other_input)\nhidden_2 = tf.keras.layers.Dense(256, activation='relu', name=\"other_dense_2\")(hidden_1)\n\n# Concatenate and output\nconcat = tf.keras.layers.concatenate([name_flatten, hidden_2], name=\"concatenate\")\n\noutputs = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")(concat)\n\n\nmodel = tf.keras.Model(inputs=[name_input, other_input], outputs=outputs)\n\n\nprint(model.summary())\n\ntf.keras.utils.plot_model(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=[\n        'accuracy',\n        tf.keras.metrics.AUC(name='auc')\n    ]\n)\n\n\nbatch_size = 32\nepochs = 100\n\nhistory = model.fit(\n    [names_train, X_train],\n    y_train,\n    validation_split=0.12,\n    batch_size=batch_size,\n    epochs=epochs,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=3,\n            restore_best_weights=True\n        )\n    ]\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate([names_test, X_test], y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Every Day  \n\nThis notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  \n\n***\n\nCheck it out!  \nhttps://youtu.be/FIY53JthQD0"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}