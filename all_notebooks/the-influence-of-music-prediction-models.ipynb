{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Problem D Prediction Models for Task #3.5\n\nIncluded is also feature importance analysis."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport sys\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.offline as py\nimport math\nimport itertools\npy.init_notebook_mode(connected=True)\nimport random\nimport seaborn as sns\n\n\"\"\"XGBoost and Other Helper Functions\"\"\"\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold, RepeatedStratifiedKFold, KFold\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport gc\n\nLOOK_AT = 10\nSEED = 42\nnp.random.seed(SEED)\nrandom.seed(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_by_artist = pd.read_csv(\"../input/icm-problem-d/2021_ICM_Problem_D_Data/data_by_artist.csv\")\ndata_by_year = pd.read_csv(\"../input/icm-problem-d/2021_ICM_Problem_D_Data/data_by_year.csv\")\nfull_music_data = pd.read_csv(\"../input/icm-problem-d/2021_ICM_Problem_D_Data/full_music_data.csv\")\ninfluence_data = pd.read_csv(\"../input/icm-problem-d/2021_ICM_Problem_D_Data/influence_data.csv\")\nmusic_df = pd.read_csv(\"../input/icm-problem-d/music_genre.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop pop/rock potentially? Since there is a significant outlier in the amount of Pop/Rock songs in the dataset compared to all other songs."},{"metadata":{"trusted":true},"cell_type":"code","source":"#music_df = music_df.loc[music_df['Genre'] != \"Pop/Rock\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"genre_list = list(music_df[\"Genre\"].unique())\ngenre_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(music_df.groupby('Genre').size())\nfig.update_layout(title={'text': f\"Distribution of Each Song's Genre\", 'x': 0.5,\n                             'xanchor': 'center', 'font': {'size': 20}}, yaxis_title=\"Count\", showlegend=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xnn = music_df.select_dtypes(np.number).drop([\"year\", \"Unnamed: 0\"], axis=1)\nX_train = xnn.to_numpy()\n\ny_train = []\nfor genre in music_df[\"Genre\"]:\n    y_train.append(genre_list.index(genre))\ny_train = np.array(y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> Potential XGBoost CV Function </h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dmatrix = xgb.DMatrix(data=xnn, label=y_train)\nparams = {\"objective\":\"multi:softprob\", 'num_class': len(genre_list), 'min_child_weight': 3, 'colsample_bytree': 0.8, 'learning_rate': 0.17,\n                    'max_depth': 10, 'reg_lambda': 1.5, 'subsample': 0.8, 'reg_alpha': 0, 'gamma': 0, 'tree_method': 'gpu_hist'}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> XGBoost Implementation with some Hyperparameter Optimization </h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nkf = KFold(n_splits=5, shuffle=True, random_state=SEED)\nfor train_index, test_index in kf.split(X_train, y_train):\n    clf = xgb.XGBClassifier(\n        n_estimators=300,\n        learning_rate=0.17,\n        max_depth=10,\n        min_child_weight=3,\n        max_delta_step=0,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        missing=-999,\n        random_state=SEED,\n        n_jobs=-1,\n        reg_lambda=1.5,\n        reg_alpha=0,\n        gamma=0,\n        objective='multi:softprob',\n        tree_method='gpu_hist'\n    )\n\n    clf.fit(X_train[train_index], y_train[train_index], early_stopping_rounds=10, eval_set=[(X_train[test_index], y_train[test_index])], verbose=100)\n    print(np.count_nonzero(np.argmax(clf.predict_proba(X_train[test_index]), axis=1) == y_train[test_index])/len(y_train[test_index]))\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.argmax(clf.predict_proba(X_train[test_index]), axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> XGBoost Feature Importance </h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nperm = PermutationImportance(clf, random_state=SEED).fit(X_train, y_train)\neli5.show_weights(perm, feature_names = xnn.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> Simple Keras Neural Network </h3>\n\nThe following network architecture was copied from a Jane Street Market Prediction model. Although it is certainly not optimized for this task, it holds sufficient to test the efficacy of MLPs for this classification."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_mlp(\n    num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate\n):\n\n    inp = tf.keras.layers.Input(shape=(num_columns,))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n    for i in range(len(hidden_units)):\n        x = tf.keras.layers.Dense(hidden_units[i])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n        x = tf.keras.layers.Dropout(dropout_rates[i + 1])(x)\n\n    x = tf.keras.layers.Dense(num_labels)(x)\n    out = tf.keras.layers.Activation(\"sigmoid\")(x)\n\n    model = tf.keras.models.Model(inputs=inp, outputs=out)\n    model.compile(\n        optimizer=tfa.optimizers.RectifiedAdam(learning_rate=learning_rate),\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n        metrics=tf.keras.metrics.AUC(name=\"AUC\"),\n    )\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yy_train = []\nfor genre in music_df[\"Genre\"]:\n    my_list = [0 for x in range(len(genre_list))]\n    my_list[genre_list.index(genre)] = 1\n    yy_train.append(my_list)\nyy_train = np.array(yy_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 256\nhidden_units = [160, 160, 160]\ndropout_rates = [0.2, 0.2, 0.2, 0.2]\nlabel_smoothing = 1e-2\nlearning_rate = 1e-3\n\nclf = create_mlp(len(X_train[0]), len(yy_train[0]), hidden_units, dropout_rates, label_smoothing, learning_rate)\n\nFOLDS = 5\nes = tf.keras.callbacks.EarlyStopping(monitor='AUC', mode='max', verbose=0, patience=10, restore_best_weights=True)\nkf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\nfor train_index, test_index in kf.split(X_train, yy_train):\n    %time clf.fit(X_train[train_index], yy_train[train_index], validation_data=(X_train[test_index], yy_train[test_index]), callbacks=[es], epochs=128, batch_size=batch_size, verbose=1)\n    print(np.count_nonzero(np.argmax(clf(X_train[test_index]), axis=1) == y_train[test_index])/len(y_train[test_index]))\n    gc.collect()\n    break","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}