{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Life Expectancy (WHO)\n#### Regression Models for predicting Life Expectancy according to WHO dataset.\n\n### Context\nAlthough there have been lot of studies undertaken in the past on factors affecting life expectancy considering demographic variables, income composition and mortality rates. It was found that affect of immunization and human development index was not taken into account in the past. Also, some of the past research was done considering multiple linear regression based on data set of one year for all the countries. Hence, this gives motivation to resolve both the factors stated previously by formulating a regression model based on mixed effects model and multiple linear regression while considering data from a period of 2000 to 2015 for all the countries. Important immunization like Hepatitis B, Polio and Diphtheria will also be considered. In a nutshell, this study will focus on immunization factors, mortality factors, economic factors, social factors and other health related factors as well. Since the observations this dataset are based on different countries, it will be easier for a country to determine the predicting factor which is contributing to lower value of life expectancy. This will help in suggesting a country which area should be given importance in order to efficiently improve the life expectancy of its population.\n\n### Content\nThe project relies on accuracy of data. The Global Health Observatory (GHO) data repository under World Health Organization (WHO) keeps track of the health status as well as many other related factors for all countries The data-sets are made available to public for the purpose of health data analysis. The data-set related to life expectancy, health factors for 193 countries has been collected from the same WHO data repository website and its corresponding economic data was collected from United Nation website. Among all categories of health-related factors only those critical factors were chosen which are more representative. It has been observed that in the past 15 years , there has been a huge development in health sector resulting in improvement of human mortality rates especially in the developing nations in comparison to the past 30 years. Therefore, in this project we have considered data from year 2000-2015 for 193 countries for further analysis. The individual data files have been merged together into a single data-set. On initial visual inspection of the data showed some missing values. As the data-sets were from WHO, we found no evident errors. Missing data was handled in R software by using Missmap command. The result indicated that most of the missing data was for population, Hepatitis B and GDP. The missing data were from less known countries like Vanuatu, Tonga, Togo, Cabo Verde etc. Finding all data for these countries was difficult and hence, it was decided that we exclude these countries from the final model data-set. The final merged file(final dataset) consists of 22 Columns and 2938 rows which meant 20 predicting variables. All predicting variables was then divided into several broad categories:â€‹Immunization related factors, Mortality factors, Economical factors and Social factors.\n\n### Acknowledgements\nThe data was collected from WHO and United Nations website with the help of Deeksha Russell and Duan Wang.\n\n- Inspiration\n- The data-set aims to answer the following key questions:\n\n- Does various predicting factors which has been chosen initially really affect the Life expectancy? What are the predicting variables actually affecting the life expectancy?\n\n- Should a country having a lower life expectancy value(<65) increase its healthcare expenditure in order to improve its average lifespan?\n\n- How does Infant and Adult mortality rates affect life expectancy?\n\n- Does Life Expectancy has positive or negative correlation with eating habits, lifestyle, exercise, smoking, drinking alcohol etc.\n\n- What is the impact of schooling on the lifespan of humans?\n\n- Does Life Expectancy have positive or negative relationship with drinking alcohol?\n\n- Do densely populated countries tend to have lower life expectancy?\n\n- What is the impact of Immunization coverage on life Expectancy?","metadata":{}},{"cell_type":"markdown","source":"# 1. Importing Ml Libraries & Dataset","metadata":{}},{"cell_type":"code","source":"# basic libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings \nimport datetime\nimport math\n\n# librabries for data preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\n# libraries for ML Models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Ridge, RidgeCV,Lasso,LassoCV\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom mlxtend.regressor import StackingCVRegressor\n\n# libraries for model evaluation\nfrom sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, mean_absolute_error\n\n# libraries for hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\n# visualization\nfrom yellowbrick.regressor import residuals_plot,prediction_error\n\n# to ignore warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\nsns.set_style(\"whitegrid\")","metadata":{"execution":{"iopub.status.busy":"2021-06-19T04:50:45.404259Z","iopub.execute_input":"2021-06-19T04:50:45.404617Z","iopub.status.idle":"2021-06-19T04:50:45.85753Z","shell.execute_reply.started":"2021-06-19T04:50:45.404564Z","shell.execute_reply":"2021-06-19T04:50:45.856562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Importing data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/life-expectancy-who/Life Expectancy Data.csv')\ndf","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-19T04:50:45.862269Z","iopub.execute_input":"2021-06-19T04:50:45.862637Z","iopub.status.idle":"2021-06-19T04:50:45.921774Z","shell.execute_reply.started":"2021-06-19T04:50:45.862597Z","shell.execute_reply":"2021-06-19T04:50:45.920656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-19T04:50:45.923611Z","iopub.execute_input":"2021-06-19T04:50:45.923982Z","iopub.status.idle":"2021-06-19T04:50:45.930355Z","shell.execute_reply.started":"2021-06-19T04:50:45.923944Z","shell.execute_reply":"2021-06-19T04:50:45.929274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-19T04:50:45.932232Z","iopub.execute_input":"2021-06-19T04:50:45.932655Z","iopub.status.idle":"2021-06-19T04:50:45.953263Z","shell.execute_reply.started":"2021-06-19T04:50:45.932613Z","shell.execute_reply":"2021-06-19T04:50:45.952164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Data Cleaning\n## 2.1 Null Values","metadata":{}},{"cell_type":"code","source":"# Function to find out missing values\ndef check_na(data):\n    missing_values= data.isna().sum().reset_index()\n    missing_values.columns= [\"Features\", \"Missing_Values\"]\n    missing_values[\"Missing_Percent\"]= round(missing_values.Missing_Values/len(data)*100,2)\n    missing_values = missing_values[missing_values.Missing_Values > 0 ]\n\n    return missing_values.sort_values(\"Missing_Percent\", ascending=False).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T04:50:45.954757Z","iopub.execute_input":"2021-06-19T04:50:45.955139Z","iopub.status.idle":"2021-06-19T04:50:45.961923Z","shell.execute_reply.started":"2021-06-19T04:50:45.955099Z","shell.execute_reply":"2021-06-19T04:50:45.960895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List of Null value features\nmissing_values = check_na(df)\nmissing_values","metadata":{"execution":{"iopub.status.busy":"2021-06-19T04:50:45.963492Z","iopub.execute_input":"2021-06-19T04:50:45.964334Z","iopub.status.idle":"2021-06-19T04:50:45.991545Z","shell.execute_reply.started":"2021-06-19T04:50:45.964294Z","shell.execute_reply":"2021-06-19T04:50:45.990719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to impute null/missing values with Mean, Median, Mode\ndef missing_value_imputer(data, feature, method):\n    if method == \"mode\":\n        data[feature] = data[feature].fillna(data[feature].mode()[0])\n    elif method == \"median\":\n        data[feature] = data[feature].fillna(data[feature].median())\n    else:\n        data[feature] = data[feature].fillna(data[feature].mean())\n    return data","metadata":{"execution":{"iopub.status.busy":"2021-06-19T04:50:45.992835Z","iopub.execute_input":"2021-06-19T04:50:45.99337Z","iopub.status.idle":"2021-06-19T04:50:45.999952Z","shell.execute_reply.started":"2021-06-19T04:50:45.993333Z","shell.execute_reply":"2021-06-19T04:50:45.998802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Imputing the missing values for each column having missing values\nfor feature in missing_values[\"Features\"]:\n    missing_value_imputer(data= df, feature=feature, method=\"median\")","metadata":{"execution":{"iopub.status.busy":"2021-06-19T04:50:46.003673Z","iopub.execute_input":"2021-06-19T04:50:46.004409Z","iopub.status.idle":"2021-06-19T04:50:46.023536Z","shell.execute_reply.started":"2021-06-19T04:50:46.004368Z","shell.execute_reply":"2021-06-19T04:50:46.022762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check missing values\nmissing_values = check_na(df)\nmissing_values","metadata":{"execution":{"iopub.status.busy":"2021-06-19T04:50:46.025696Z","iopub.execute_input":"2021-06-19T04:50:46.026104Z","iopub.status.idle":"2021-06-19T04:50:46.043556Z","shell.execute_reply.started":"2021-06-19T04:50:46.026065Z","shell.execute_reply":"2021-06-19T04:50:46.042757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We have imputed the missing values with median of respective features with the help of missing_value_imputer() function ","metadata":{}},{"cell_type":"markdown","source":"## 2.2 Categorical and Numerical Data","metadata":{}},{"cell_type":"code","source":"# Finding out which features have categorical values and which one of them have numerical values.\ncategorical = df.select_dtypes(include=\"O\")\nnumerical = df.select_dtypes(exclude=\"O\")","metadata":{"execution":{"iopub.status.busy":"2021-06-19T04:50:46.045024Z","iopub.execute_input":"2021-06-19T04:50:46.045402Z","iopub.status.idle":"2021-06-19T04:50:46.054387Z","shell.execute_reply.started":"2021-06-19T04:50:46.045362Z","shell.execute_reply":"2021-06-19T04:50:46.053128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical","metadata":{"execution":{"iopub.status.busy":"2021-06-19T04:50:46.056035Z","iopub.execute_input":"2021-06-19T04:50:46.056769Z","iopub.status.idle":"2021-06-19T04:50:46.074083Z","shell.execute_reply.started":"2021-06-19T04:50:46.056695Z","shell.execute_reply":"2021-06-19T04:50:46.073013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Label encoding the categorical features \ncolumns = categorical.columns\ndef label_enoder(data, columns):\n    for feature in columns:\n        le = LabelEncoder()\n        data[feature]= le.fit_transform(data[feature])\n        data[feature].astype(\"int64\")\n    return data","metadata":{"execution":{"iopub.status.busy":"2021-06-19T04:50:46.075512Z","iopub.execute_input":"2021-06-19T04:50:46.075947Z","iopub.status.idle":"2021-06-19T04:50:46.081751Z","shell.execute_reply.started":"2021-06-19T04:50:46.075909Z","shell.execute_reply":"2021-06-19T04:50:46.08058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = label_enoder(df, categorical.columns)\ndf","metadata":{"execution":{"iopub.status.busy":"2021-06-19T04:50:46.083372Z","iopub.execute_input":"2021-06-19T04:50:46.083828Z","iopub.status.idle":"2021-06-19T04:50:46.13057Z","shell.execute_reply.started":"2021-06-19T04:50:46.083789Z","shell.execute_reply":"2021-06-19T04:50:46.129507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"### Correlation matrix\nFinding the feature correlation from heatmap to see which two features are highly correlated, so that unnecessary features could be deleted.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,20))\nsns.heatmap(df.corr(), annot=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-19T04:50:46.132045Z","iopub.execute_input":"2021-06-19T04:50:46.132426Z","iopub.status.idle":"2021-06-19T04:50:48.719222Z","shell.execute_reply.started":"2021-06-19T04:50:46.132388Z","shell.execute_reply":"2021-06-19T04:50:48.718399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to find and remove correlated features\ndef correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr","metadata":{"execution":{"iopub.status.busy":"2021-06-19T04:50:48.720371Z","iopub.execute_input":"2021-06-19T04:50:48.72072Z","iopub.status.idle":"2021-06-19T04:50:48.72729Z","shell.execute_reply.started":"2021-06-19T04:50:48.720683Z","shell.execute_reply":"2021-06-19T04:50:48.726302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"droppable_features = list(correlation(df, 0.8))\ndroppable_features","metadata":{"execution":{"iopub.status.busy":"2021-06-19T04:50:48.728919Z","iopub.execute_input":"2021-06-19T04:50:48.729288Z","iopub.status.idle":"2021-06-19T04:50:48.755515Z","shell.execute_reply.started":"2021-06-19T04:50:48.72925Z","shell.execute_reply":"2021-06-19T04:50:48.754528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(droppable_features, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T04:50:48.75693Z","iopub.execute_input":"2021-06-19T04:50:48.757271Z","iopub.status.idle":"2021-06-19T04:50:48.7637Z","shell.execute_reply.started":"2021-06-19T04:50:48.757234Z","shell.execute_reply":"2021-06-19T04:50:48.762679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we dropped the features which are highly correlated to another feature hence does not give cause information gain, so we can delete them to reduce dimensionality.\n\ncorrelation() function returns the columns which cn be dropped as they are having correlation higher than given threshold value","metadata":{}},{"cell_type":"markdown","source":"### Feature Selection","metadata":{}},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2021-06-19T04:50:48.765449Z","iopub.execute_input":"2021-06-19T04:50:48.766337Z","iopub.status.idle":"2021-06-19T04:50:48.807342Z","shell.execute_reply.started":"2021-06-19T04:50:48.766293Z","shell.execute_reply":"2021-06-19T04:50:48.806535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_regression\n\nx = df.drop([\"Life expectancy \"], axis=1)\ny = df[\"Life expectancy \"]\n\n# Function to get the features and their mutual information gain for regression\ndef select_features_mutual_info_regression(x, y):\n    mutual_info = mutual_info_regression(x,y)\n    mutual_data=pd.Series(mutual_info,index = x.columns)\n    return mutual_data.sort_values(ascending=False)\n\ntop_features = select_features_mutual_info_regression(x, y)\ntop_features","metadata":{"execution":{"iopub.status.busy":"2021-06-19T04:50:48.808596Z","iopub.execute_input":"2021-06-19T04:50:48.808989Z","iopub.status.idle":"2021-06-19T04:50:49.069527Z","shell.execute_reply.started":"2021-06-19T04:50:48.80895Z","shell.execute_reply":"2021-06-19T04:50:49.068545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Selecting top 15 features with highest mutual information gain\ntop_features = top_features.head(15)\ntop_features","metadata":{"execution":{"iopub.status.busy":"2021-06-19T04:50:49.071121Z","iopub.execute_input":"2021-06-19T04:50:49.071484Z","iopub.status.idle":"2021-06-19T04:50:49.079282Z","shell.execute_reply.started":"2021-06-19T04:50:49.071444Z","shell.execute_reply":"2021-06-19T04:50:49.078289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting Data into Features and Target","metadata":{}},{"cell_type":"code","source":"# Splitting data into Features\nx = df[top_features.index]\nx","metadata":{"execution":{"iopub.status.busy":"2021-06-19T04:50:49.080946Z","iopub.execute_input":"2021-06-19T04:50:49.081694Z","iopub.status.idle":"2021-06-19T04:50:49.117341Z","shell.execute_reply.started":"2021-06-19T04:50:49.081649Z","shell.execute_reply":"2021-06-19T04:50:49.116326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting data into Target\ny = df[\"Life expectancy \"]\ny","metadata":{"execution":{"iopub.status.busy":"2021-06-19T04:50:49.119344Z","iopub.execute_input":"2021-06-19T04:50:49.119836Z","iopub.status.idle":"2021-06-19T04:50:49.129304Z","shell.execute_reply.started":"2021-06-19T04:50:49.119793Z","shell.execute_reply":"2021-06-19T04:50:49.128294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Transformation","metadata":{}},{"cell_type":"code","source":"def scaler_transform(data):\n    columns = data.columns\n    sc = StandardScaler()\n    for i in columns:\n        data[[i]] = sc.fit_transform(df[[i]])\n        \n    return data","metadata":{"execution":{"iopub.status.busy":"2021-06-19T04:50:49.130762Z","iopub.execute_input":"2021-06-19T04:50:49.131135Z","iopub.status.idle":"2021-06-19T04:50:49.136597Z","shell.execute_reply.started":"2021-06-19T04:50:49.131095Z","shell.execute_reply":"2021-06-19T04:50:49.135645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = scaler_transform(x)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T04:50:49.138323Z","iopub.execute_input":"2021-06-19T04:50:49.139042Z","iopub.status.idle":"2021-06-19T04:50:49.214675Z","shell.execute_reply.started":"2021-06-19T04:50:49.139001Z","shell.execute_reply":"2021-06-19T04:50:49.21394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Predictive Model","metadata":{}},{"cell_type":"markdown","source":"### Splitting data into Training and Testing Datasets","metadata":{}},{"cell_type":"code","source":"x_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 0.3, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T04:50:49.215914Z","iopub.execute_input":"2021-06-19T04:50:49.216258Z","iopub.status.idle":"2021-06-19T04:50:49.223426Z","shell.execute_reply.started":"2021-06-19T04:50:49.21622Z","shell.execute_reply":"2021-06-19T04:50:49.222466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to apply Regression algorithms and return the results of models\ndef predictive_models():\n    algorithms = [LinearRegression(), Ridge(alpha=0.1), RidgeCV(alphas=[0.1,0.01,0.001,1],cv=10), Lasso(alpha=0.1), LassoCV(alphas=[0.1,0.01,0.001,1],cv=10), SVR(), KNeighborsRegressor(), DecisionTreeRegressor(), RandomForestRegressor(),GradientBoostingRegressor(), AdaBoostRegressor(), XGBRegressor(),\n                  StackingCVRegressor(regressors=(LinearRegression(), Ridge(alpha=0.1), RidgeCV(alphas=[0.1,0.01,0.001,1],cv=10), Lasso(alpha=0.1), LassoCV(alphas=[0.1,0.01,0.001,1],cv=10), SVR(), KNeighborsRegressor(), DecisionTreeRegressor(), RandomForestRegressor(),GradientBoostingRegressor(),AdaBoostRegressor(), XGBRegressor()),meta_regressor=Ridge(), use_features_in_secondary=True,cv=30)]\n    algorithm_names = [\"Linear Regression\",  \"Ridge\", \"RidgeCV\", \"Lasso\",\" LassoCV\", \"SVR\", \"KNeighbors Regressor\", \"Decision-Tree Regressor\", \"Random-Forest Regressor\", \"Gradient Boosting Regressor\", \"Ada-Boost Regressor\", \"XGB-Regressor\",\"Stacked Regressor\"]\n    \n    # Errors for training data\n    Mean_Squared_Error_Training = []\n    Mean_Absolute_Error_Training = []\n    Accuracy_Training = []\n    \n    # Errors for testing data\n    Mean_Squared_Error_Testing = []\n    Mean_Absolute_Error_Testing = []\n    Accuracy_Testing = []\n    \n    # Regression models\n    for i in algorithms:\n        model = i\n        model.fit(x_train,y_train)\n    \n        y_test_predict = model.predict(x_test)\n        y_train_predict = model.predict(x_train)\n            \n        mse_1 = round(mean_squared_error(y_train, y_train_predict),4)\n        mae_1 = round(mean_absolute_error(y_train, y_train_predict),4)\n        acc_1 = round((1-mean_absolute_percentage_error(y_train, y_train_predict))*100,4)\n        \n        mse_2 = round(mean_squared_error(y_test, y_test_predict),4)\n        mae_2 = round(mean_absolute_error(y_test, y_test_predict),4)\n        acc_2 = round((1-mean_absolute_percentage_error(y_test, y_test_predict))*100,4)\n        \n        # Appending the Errors into the list for training data\n        Mean_Squared_Error_Training.append(mse_1)\n        Mean_Absolute_Error_Training.append(mae_1)\n        Accuracy_Training.append(acc_1)\n                \n        # Appending the Errors into the list for training data\n        Mean_Squared_Error_Testing.append(mse_2)\n        Mean_Absolute_Error_Testing.append(mae_2)\n        Accuracy_Testing.append(acc_2)\n        \n    # Creating DataFrame for Logs of Models and their errors    \n    results = pd.DataFrame({\"Models\":algorithm_names,\n                            \"Mean Squared Error Training\":Mean_Squared_Error_Training,\n                            \"Mean Absolute Error Training\":Mean_Absolute_Error_Training,\n                            \"Accuracy_Training %\":Accuracy_Training,                          \n                            \"Mean Squared Error Testing\":Mean_Squared_Error_Testing,\n                            \"Mean Absolute Error Testing\":Mean_Absolute_Error_Testing,\n                            \"Accuracy Testing %\":Accuracy_Testing})\n\n    return results.sort_values(\"Accuracy Testing %\", ascending=False).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T04:50:49.229149Z","iopub.execute_input":"2021-06-19T04:50:49.229675Z","iopub.status.idle":"2021-06-19T04:50:49.243016Z","shell.execute_reply.started":"2021-06-19T04:50:49.229511Z","shell.execute_reply":"2021-06-19T04:50:49.242215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = predictive_models()\nresults","metadata":{"execution":{"iopub.status.busy":"2021-06-19T04:50:49.244669Z","iopub.execute_input":"2021-06-19T04:50:49.245288Z","iopub.status.idle":"2021-06-19T04:52:16.867671Z","shell.execute_reply.started":"2021-06-19T04:50:49.245236Z","shell.execute_reply":"2021-06-19T04:52:16.866555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBRegressor\n### Hyperparameter Tuning ","metadata":{}},{"cell_type":"markdown","source":"### RandomizedSearchCV for hyperparamter tuning","metadata":{}},{"cell_type":"code","source":"xgb_model = XGBRegressor()\n\n# Parameter dictionary for RandomizedSearchCV\nparameters = {'learning_rate': [.03, 0.05, .07], \n              'max_depth': [4, 5, 6, 7, 8, 9, 10],\n              'min_child_weight': [3, 4, 5, 6, 7, 8],\n              'subsample': [0.6,0.7,0.8],\n              'colsample_bytree': [0.6,0.7,0.8],\n              'n_estimators': [100,200,300,400,500]\n             }\n# Using RandomizedSearchCV()\nxgb_random_cv = RandomizedSearchCV(estimator=xgb_model, param_distributions=parameters, n_iter=100, cv=2, verbose=2)\nxgb_random_cv.fit(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T04:52:16.869263Z","iopub.execute_input":"2021-06-19T04:52:16.869614Z","iopub.status.idle":"2021-06-19T04:54:07.164301Z","shell.execute_reply.started":"2021-06-19T04:52:16.869576Z","shell.execute_reply":"2021-06-19T04:54:07.163433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Best Parameters for XGBregressor by RandomizedSearchCV\nbest_param = xgb_random_cv.best_params_\nbest_param","metadata":{"execution":{"iopub.status.busy":"2021-06-19T04:54:07.165603Z","iopub.execute_input":"2021-06-19T04:54:07.165976Z","iopub.status.idle":"2021-06-19T04:54:07.172761Z","shell.execute_reply.started":"2021-06-19T04:54:07.165936Z","shell.execute_reply":"2021-06-19T04:54:07.17172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### GridSearchCV for thorough search of best hyperparameters.\nWe will use GridSearchCV for thorough search of best parameters for XGBregressor","metadata":{}},{"cell_type":"code","source":"# Parameter grid for GridSearchCV\nparameters = {'learning_rate': [best_param[\"learning_rate\"]-0.01, best_param[\"learning_rate\"], best_param[\"learning_rate\"]+0.01], \n              'max_depth': [best_param[\"max_depth\"]-1, best_param[\"max_depth\"], best_param[\"max_depth\"]+1],\n              'min_child_weight': [best_param[\"min_child_weight\"]-1, best_param[\"min_child_weight\"], best_param[\"min_child_weight\"]+1],\n              'subsample': [best_param[\"subsample\"]-0.05, best_param[\"subsample\"], best_param[\"subsample\"]+0.05],\n              'colsample_bytree': [best_param[\"colsample_bytree\"]-0.1, best_param[\"colsample_bytree\"], best_param[\"colsample_bytree\"]+0.1],\n              'n_estimators': [best_param[\"n_estimators\"],best_param[\"n_estimators\"]+50,best_param[\"n_estimators\"]+100,best_param[\"n_estimators\"]+150]\n             }\n\n# Using GridSearchCV()\nxgb_grid = GridSearchCV(xgb_model, parameters, cv= 2, n_jobs=-1, verbose=3)\nxgb_grid.fit(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T04:54:07.174187Z","iopub.execute_input":"2021-06-19T04:54:07.174534Z","iopub.status.idle":"2021-06-19T06:57:46.892792Z","shell.execute_reply.started":"2021-06-19T04:54:07.174494Z","shell.execute_reply":"2021-06-19T06:57:46.89177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Best Hyperparameters for XGBRegressor by GridSearchCV\nbest_param_gridCV = xgb_grid.best_params_\nbest_param_gridCV","metadata":{"execution":{"iopub.status.busy":"2021-06-19T06:57:46.89446Z","iopub.execute_input":"2021-06-19T06:57:46.895009Z","iopub.status.idle":"2021-06-19T06:57:46.901073Z","shell.execute_reply.started":"2021-06-19T06:57:46.894962Z","shell.execute_reply":"2021-06-19T06:57:46.900108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_regressor = XGBRegressor(colsample_bytree = best_param_gridCV[\"colsample_bytree\"],\n                             learning_rate = best_param_gridCV[\"learning_rate\"], \n                             max_depth = best_param_gridCV[\"max_depth\"], \n                             min_child_weight = best_param_gridCV[\"min_child_weight\"], \n                             n_estimators = best_param_gridCV[\"n_estimators\"], \n                             subsample = best_param_gridCV[\"subsample\"])","metadata":{"execution":{"iopub.status.busy":"2021-06-19T06:57:46.902423Z","iopub.execute_input":"2021-06-19T06:57:46.902802Z","iopub.status.idle":"2021-06-19T06:57:46.914254Z","shell.execute_reply.started":"2021-06-19T06:57:46.902763Z","shell.execute_reply":"2021-06-19T06:57:46.91322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_regressor.fit(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T06:57:46.915557Z","iopub.execute_input":"2021-06-19T06:57:46.916058Z","iopub.status.idle":"2021-06-19T06:57:48.75236Z","shell.execute_reply.started":"2021-06-19T06:57:46.916021Z","shell.execute_reply":"2021-06-19T06:57:48.751547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = xgb_regressor.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T06:57:48.753512Z","iopub.execute_input":"2021-06-19T06:57:48.753821Z","iopub.status.idle":"2021-06-19T06:57:48.781366Z","shell.execute_reply.started":"2021-06-19T06:57:48.753792Z","shell.execute_reply":"2021-06-19T06:57:48.780636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation of Model ","metadata":{}},{"cell_type":"code","source":"mse = round(mean_squared_error(y_test, y_pred),4)\nmae = round(mean_absolute_error(y_test, y_pred),4)\nacc = round((1-mean_absolute_percentage_error(y_test, y_pred))*100,4)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T06:57:48.782492Z","iopub.execute_input":"2021-06-19T06:57:48.782824Z","iopub.status.idle":"2021-06-19T06:57:48.794763Z","shell.execute_reply.started":"2021-06-19T06:57:48.782789Z","shell.execute_reply":"2021-06-19T06:57:48.793823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\" Mean Squared Error = \",mse)\nprint(\"Mean Absolute Error = \",mae)\nprint(\"           Accuracy = \",acc)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-19T06:57:48.799001Z","iopub.execute_input":"2021-06-19T06:57:48.799254Z","iopub.status.idle":"2021-06-19T06:57:48.806131Z","shell.execute_reply.started":"2021-06-19T06:57:48.799229Z","shell.execute_reply":"2021-06-19T06:57:48.805056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Prediction v/s Actual values\")\nplt.scatter(y_pred, y_test)\nplt.title(\"Prediction v/s Actual values\")","metadata":{"execution":{"iopub.status.busy":"2021-06-19T06:57:48.807818Z","iopub.execute_input":"2021-06-19T06:57:48.80809Z","iopub.status.idle":"2021-06-19T06:57:49.01333Z","shell.execute_reply.started":"2021-06-19T06:57:48.808062Z","shell.execute_reply":"2021-06-19T06:57:49.012443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Prediction Error Plot\")\nprint(prediction_error(xgb_regressor, x_train, y_train, x_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2021-06-19T06:57:49.014555Z","iopub.execute_input":"2021-06-19T06:57:49.015087Z","iopub.status.idle":"2021-06-19T06:57:49.43206Z","shell.execute_reply.started":"2021-06-19T06:57:49.015046Z","shell.execute_reply":"2021-06-19T06:57:49.431076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,5))\n\nplt.subplot(1,2,1)\nsns.histplot(y_test)\n\nplt.subplot(1,2,2)\nsns.histplot(y_pred, color='r')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-19T06:57:49.43354Z","iopub.execute_input":"2021-06-19T06:57:49.433919Z","iopub.status.idle":"2021-06-19T06:57:49.8443Z","shell.execute_reply.started":"2021-06-19T06:57:49.433878Z","shell.execute_reply":"2021-06-19T06:57:49.843332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predictions using XGBoost Regression","metadata":{}},{"cell_type":"markdown","source":"#### - Feature List to give as input","metadata":{}},{"cell_type":"code","source":"feature_list = list(x.columns)\nprint(\"Number of Features = \", len(feature_list))\nfeature_list","metadata":{"execution":{"iopub.status.busy":"2021-06-19T06:57:49.845748Z","iopub.execute_input":"2021-06-19T06:57:49.846101Z","iopub.status.idle":"2021-06-19T06:57:49.853703Z","shell.execute_reply.started":"2021-06-19T06:57:49.846062Z","shell.execute_reply":"2021-06-19T06:57:49.852855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### - Input to model","metadata":{}},{"cell_type":"code","source":"def input_data():\n    sc = StandardScaler()\n    input_distionary = {}\n    \n    for i in feature_list:\n        print(\"Enter \", i)\n        input_distionary[i] = eval(input())\n        \n        data = pd.DataFrame(input_distionary, columns=feature_list, index=[0])\n        data = sc.fit_transform(data)\n        \n        prediction = xgb_regressor.predict(data)\n        return prediction","metadata":{"execution":{"iopub.status.busy":"2021-06-19T06:57:49.855195Z","iopub.execute_input":"2021-06-19T06:57:49.855829Z","iopub.status.idle":"2021-06-19T06:57:49.862133Z","shell.execute_reply.started":"2021-06-19T06:57:49.855792Z","shell.execute_reply":"2021-06-19T06:57:49.861241Z"},"trusted":true},"execution_count":null,"outputs":[]}]}