{"cells":[{"metadata":{"id":"zej-aX2A3J3L","outputId":"accd671f-c417-4e72-f235-6cd762c888c7","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\nimport os\nimport io\nimport string\nimport re\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"id":"Ta745Oud5Zuw","outputId":"61808c45-762f-4122-e02b-e1f7eeafce97","trusted":true},"cell_type":"code","source":"reviews = pd.read_csv('../input/zomato-restaurants-hyderabad/Restaurant reviews.csv')\nreviews.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"zoAidGwe63_r","outputId":"9b61f4d4-3e37-442e-c033-5442f21ee1dc","trusted":true},"cell_type":"code","source":"reviews.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"ch7pXWlU7Aw8","outputId":"3b606ee2-97f1-44c6-ea63-dd6356c1252b","trusted":true},"cell_type":"code","source":"reviews.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"YiH47uHa7Bw7","outputId":"8e8c8238-21e4-4369-8bb5-164c2c12a4e0","trusted":true},"cell_type":"code","source":"reviews.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"LRYh_FdC_I0x","trusted":true},"cell_type":"code","source":"reviews =reviews.dropna()","execution_count":null,"outputs":[]},{"metadata":{"id":"nxZoiKBm_Rl8","outputId":"2b822f5c-e925-4024-d17b-4d60b08f2043","trusted":true},"cell_type":"code","source":"reviews.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"QzopdTjd_VKC","outputId":"8bc7efa6-cb57-419f-c02f-c066751ec7b2","trusted":true},"cell_type":"code","source":"reviews_txt = reviews[['Review', 'Rating']]\nreviews_txt.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"mNL3OLDLA2ci","outputId":"afe8d692-a4ec-4a13-99de-85a8dff2c1db","trusted":true},"cell_type":"code","source":"reviews_txt['Rating'] = reviews_txt['Rating'].replace('Like', 5)\nreviews_txt['Rating'] = reviews_txt['Rating'].astype('float')","execution_count":null,"outputs":[]},{"metadata":{"id":"F0op0Yhn_46e","outputId":"914c5f28-781b-463d-d019-efb61ea8745e","trusted":true},"cell_type":"code","source":"reviews_txt['Rating'] = np.where(reviews_txt['Rating']<4, 0, 1) #0 for BAD rating and 1 for Good rating","execution_count":null,"outputs":[]},{"metadata":{"id":"1XLFP1UmBmZI","outputId":"0a51c91d-e615-4126-92a4-8021c441538b","trusted":true},"cell_type":"code","source":"reviews_txt.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"NX_YT00sB7YQ","outputId":"56c20dbd-451f-4401-f09c-3e2921c8fe6e","trusted":true},"cell_type":"code","source":"reviews_txt['Rating'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"9eX40fVpCwB0","outputId":"4e03b96d-7bff-409d-e566-4b3a2e36a12f","trusted":true},"cell_type":"code","source":"reviews['Restaurant'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"bKS_fSKACIs2"},"cell_type":"markdown","source":"Data Visualization","execution_count":null},{"metadata":{"id":"9GGBFH_-CFAB","outputId":"cf5cb309-6670-4fd2-e20e-5a10ba843b6a","trusted":true},"cell_type":"code","source":"reviews.columns","execution_count":null,"outputs":[]},{"metadata":{"id":"wgHhx0eic7d0","outputId":"db23eb80-1c98-4d98-a15d-6a218a585113","trusted":true},"cell_type":"code","source":"sns.countplot(reviews_txt['Rating']) #0 for bad and 1 for good","execution_count":null,"outputs":[]},{"metadata":{"id":"yAmLwLXgdIst","outputId":"c01705ca-8e77-43bb-dba6-dc997529c3f4","trusted":true},"cell_type":"code","source":"cleanup_re = re.compile('[^a-z]+')\ndef clean(sentence): \n  sentence = str(sentence)\n  sentence = sentence.lower()\n  sentence = cleanup_re.sub(' ', sentence).strip()\n  return sentence\nreviews_txt['Review'] = reviews_txt['Review'].apply(clean)","execution_count":null,"outputs":[]},{"metadata":{"id":"xhaxr2PSeSGd","outputId":"3fe3e455-b1ac-43b7-b7eb-bd9aca1154d1","trusted":true},"cell_type":"code","source":"nltk.download('popular')","execution_count":null,"outputs":[]},{"metadata":{"id":"9YLQEm2Qefqt","trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize","execution_count":null,"outputs":[]},{"metadata":{"id":"u32qXkZPe5bX","outputId":"11fe663f-1888-495d-a723-3934beafb953","trusted":true},"cell_type":"code","source":"def preprocess(sentence):\n  sentence = str(sentence)\n  word_tokens = word_tokenize(sentence)\n  stop_words = set(stopwords.words('english'))\n  sentence = ' '.join([i for i in word_tokens if not i in stop_words])\n  return sentence\n\nreviews_txt['Review'] = reviews_txt['Review'].apply(preprocess)","execution_count":null,"outputs":[]},{"metadata":{"id":"USeSlOoPfDpb","outputId":"abd0a99b-dd30-455d-8f34-a3b9e4cf966b","trusted":true},"cell_type":"code","source":"reviews_txt.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"zxKBo11CkMq2","outputId":"039a0741-8cb6-46d3-d085-a8f287fbf1a4","trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nlemma = WordNetLemmatizer()\ndef preprocess4(sentence):\n  input_str=word_tokenize(sentence)\n  lemmatized_output = ' '.join([lemma.lemmatize(w) for w in input_str])\n  return lemmatized_output\n\nreviews_txt['Review'] = reviews_txt['Review'].apply(preprocess4)","execution_count":null,"outputs":[]},{"metadata":{"id":"wytc9-wwj5wB","outputId":"2ef48032-dd0f-48ef-fb45-8880d64a78c7","trusted":true},"cell_type":"code","source":"reviews_txt.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"Vky3_Zi9keOq","trusted":true},"cell_type":"code","source":"X = reviews_txt['Review']\ny = reviews_txt['Rating']","execution_count":null,"outputs":[]},{"metadata":{"id":"3F2Dc6T5kUJA","trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 100)","execution_count":null,"outputs":[]},{"metadata":{"id":"hRFN5HSlk6X7","outputId":"fdd546f2-2612-4853-caeb-6d6c341a1965","trusted":true},"cell_type":"code","source":"X_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"iJp0Bsbhk_fR","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\ncount_vect.fit(X_train)\n\n#transform the train and test dataset\nX_train_countvect = count_vect.transform(X_train)\nX_test_countvect = count_vect.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"ZGWNQ_c9lTUr","outputId":"a1b093b9-e5de-4814-9f0b-49330654474e","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n# word level tf-idf\ntfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\ntfidf_vect.fit(X_train)\nxtrain_tfidf =  tfidf_vect.transform(X_train)\nxtest_tfidf =  tfidf_vect.transform(X_test)\n \n # ngram level tf-idf \ntfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\ntfidf_vect_ngram.fit(X_train)\nxtrain_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\nxtest_tfidf_ngram =  tfidf_vect_ngram.transform(X_test)\n\n# characters level tf-idf\ntfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\ntfidf_vect_ngram_chars.fit(X_train)\nxtrain_tfidf_ngram_char =  tfidf_vect_ngram_chars.transform(X_train)\nxtest_tfidf_ngram_char =  tfidf_vect_ngram_chars.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"W3RdwY-SngLN","trusted":true},"cell_type":"code","source":"def train_model(classifier, feature_vector_train, label, feature_vector_valid):\n    # fit the training dataset on the classifier\n    classifier.fit(feature_vector_train, label)\n    \n    # predict the labels on validation dataset\n    predictions = classifier.predict(feature_vector_valid)\n    \n    return accuracy_score(predictions, y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"VpdhSDtLn00f","outputId":"0247a9d2-148e-4d1c-eb8d-7a603c9ca26b","trusted":true},"cell_type":"code","source":"# Naive Bayes on Count Vectors\naccuracy = train_model(MultinomialNB(), X_train_countvect, y_train, X_test_countvect)\nprint(\"NB, Count Vectors: \", accuracy)\n\n# Naive Bayes on Word Level TF IDF Vectors\naccuracy = train_model(MultinomialNB(), xtrain_tfidf, y_train, xtest_tfidf)\nprint(\"NB, WordLevel TF-IDF: \", accuracy)\n\n# Naive Bayes on Ngram Level TF IDF Vectors\naccuracy = train_model(MultinomialNB(), xtrain_tfidf_ngram, y_train, xtest_tfidf_ngram)\nprint(\"NB, N-Gram Vectors: \", accuracy)\n\n# Naive Bayes on Character Level TF IDF Vectors\naccuracy = train_model(MultinomialNB(), xtrain_tfidf_ngram_char, y_train, xtest_tfidf_ngram_char)\nprint(\"NB, CharLevel Vectors: \", accuracy)","execution_count":null,"outputs":[]},{"metadata":{"id":"VC2RpjCumz09","outputId":"7da1ec16-4a1c-4860-c33d-5ecf2ce1cb89","trusted":true},"cell_type":"code","source":"# Logistic on Count Vectors\naccuracy = train_model(LogisticRegression(), X_train_countvect, y_train, X_test_countvect)\nprint(\"Count Vectors: \", accuracy)\n\n# Logistic on Word Level TF IDF Vectors\naccuracy = train_model(LogisticRegression(), xtrain_tfidf, y_train, xtest_tfidf)\nprint(\"WordLevel TF-IDF: \", accuracy)\n\n# Logistic on Ngram Level TF IDF Vectors\naccuracy = train_model(LogisticRegression(), xtrain_tfidf_ngram, y_train, xtest_tfidf_ngram)\nprint(\"N-Gram Vectors: \", accuracy)\n\n# Logistic on Character Level TF IDF Vectors\naccuracy = train_model(LogisticRegression(), xtrain_tfidf_ngram_char, y_train, xtest_tfidf_ngram_char)\nprint(\"CharLevel Vectors: \", accuracy)","execution_count":null,"outputs":[]},{"metadata":{"id":"hCD2tyPTnTfL","outputId":"28e38061-7b0d-42bd-b978-b2e486faf316","trusted":true},"cell_type":"code","source":"# SVM on Ngram Level TF IDF Vectors\naccuracy = train_model(SVC(), xtrain_tfidf_ngram, y_train, xtest_tfidf_ngram)\nprint(\"NB, N-Gram Vectors: \", accuracy)","execution_count":null,"outputs":[]},{"metadata":{"id":"I5m9W8wcnesw","outputId":"9e2c5a84-badb-4457-a555-7cdc2528851b","trusted":true},"cell_type":"code","source":"accuracy = train_model(RandomForestClassifier(n_estimators=250, random_state=100), X_train_countvect, y_train, X_test_countvect)\nprint(\"Count Vectors: \", accuracy)\n\n# Random Forest on Word Level TF IDF Vectors\naccuracy = train_model(RandomForestClassifier(), xtrain_tfidf, y_train, xtest_tfidf)\nprint(\"WordLevel TF-IDF: \", accuracy)","execution_count":null,"outputs":[]},{"metadata":{"id":"zL8k8V2ZoIwr","outputId":"bd6added-993c-41d7-c4f6-fd43e037b890","trusted":true},"cell_type":"code","source":"accuracy = train_model(BaggingClassifier(), X_train_countvect, y_train, X_test_countvect)\nprint(\"Count Vectors: \", accuracy)\n\n# Bagging on Word Level TF IDF Vectors\naccuracy = train_model(BaggingClassifier(), xtrain_tfidf, y_train, xtest_tfidf)\nprint(\"WordLevel TF-IDF: \", accuracy)","execution_count":null,"outputs":[]},{"metadata":{"id":"qVEtJVl3oWYE","outputId":"d5c7b1b7-559b-4576-9b59-167bfb76f33c","trusted":true},"cell_type":"code","source":"import xgboost as xgb\naccuracy = train_model(xgb.XGBClassifier(), X_train_countvect, y_train, X_test_countvect)\nprint(\"Count Vectors: \", accuracy)\n\n# Boosting on Word Level TF IDF Vectors\naccuracy = train_model(xgb.XGBClassifier(), xtrain_tfidf, y_train, xtest_tfidf)\nprint(\"WordLevel TF-IDF: \", accuracy)","execution_count":null,"outputs":[]},{"metadata":{"id":"IVtxZn8gsC8K"},"cell_type":"markdown","source":"Further imporovemnts can be done using hyperparameter tuning","execution_count":null},{"metadata":{"id":"WOq1zswdsOzd"},"cell_type":"markdown","source":"Classification using LSTM and tensorflow","execution_count":null},{"metadata":{"id":"vD2fon9asKab","outputId":"3c0b012f-0d17-4de4-e000-abe36775678f","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Embedding\nfrom keras.layers import SpatialDropout1D\nfrom keras.layers import LSTM\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import Bidirectional\nfrom keras.layers import Dropout","execution_count":null,"outputs":[]},{"metadata":{"id":"DBHEgNY1u5lk","outputId":"d4fac9f2-d910-42db-890d-87655e92c6bd","trusted":true},"cell_type":"code","source":"#max words to be used\nMAX_WORDS = 10000\n#max length of the sequence\nMAX_LEN = 50\n#embedding dimension should be between 50 to 300\nEMBEDDING_DIM = 100\ntokenizer = Tokenizer(num_words=MAX_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(X)\nword_index = tokenizer.word_index\nprint('number of unique tokens are: ', len(word_index))","execution_count":null,"outputs":[]},{"metadata":{"id":"m9_eKdzwwlfu","trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{"id":"Ltiqxo1exKhw","outputId":"807b41e0-13e9-43f6-c1a7-ce031b45548a","trusted":true},"cell_type":"code","source":"X = tokenizer.texts_to_sequences(X)\nX = pad_sequences(X, maxlen=MAX_LEN)\nprint('shape of data tensor is', X.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"oyC2Ks49z5WB","outputId":"fa00c8c4-b7e2-4e26-c384-9d05049b4966","trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X,y, test_size = 0.2, random_state = 100)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"Z_bchCeM0MZe"},"cell_type":"markdown","source":"Building LSTM model","execution_count":null},{"metadata":{"id":"AXazB6Zp0LwO","outputId":"cb9c20f1-f1cb-4540-9c44-27f8a99dfed1","trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(input_dim=MAX_WORDS, output_dim= EMBEDDING_DIM, input_length=MAX_LEN))\nmodel.add(LSTM(300, recurrent_dropout=0.1))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\nhistory =model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=10, batch_size=64)","execution_count":null,"outputs":[]},{"metadata":{"id":"O5zOz-IL0Lzi","outputId":"ef6a17b0-9b50-4d22-e937-c003a537b4f1","trusted":true},"cell_type":"code","source":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()\n\nplt.figure()\nplt.title('Training and validation loss')\nplt.plot(epochs, loss, 'red', label='Training loss')\nplt.plot(epochs, val_loss, 'blue', label='Validation loss')\n\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}