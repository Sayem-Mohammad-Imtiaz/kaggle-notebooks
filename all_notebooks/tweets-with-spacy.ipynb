{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#importing libraries\n\nimport spacy\nimport pandas as pd\nimport random","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = spacy.load('en')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Paste the following code in command line to download the English version of spaCy. \n\n#python -m spacy download en"},{"metadata":{"trusted":true},"cell_type":"code","source":"type(nlp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets = pd.read_csv(\"../input/5000-justdoit-tweets-dataset/justdoit_tweets_2018_09_07_2.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### For our NLP task, we are only interested in tweets_full_text."},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets['tweet_full_text'][:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random 20 tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"#random tweets\nrandom.seed(1024)\n\nrandom_tweets = tweets['tweet_full_text'][random.sample(range(1,5000), 20)]\nrandom_tweets","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Combining text as nlp variable created using spaCy will only use str"},{"metadata":{"trusted":true},"cell_type":"code","source":"#combined text\n\ncombined_text = str(random_tweets)\ncombined_text\n# len(combined_text)\n# type(combined_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc = nlp(combined_text)\ndoc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(doc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Naive approach for splitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"#tokenization using split as space\ndoc.text.split()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Naive approach just breaks the sentence into parts based on the splitting criteria."},{"metadata":{},"cell_type":"markdown","source":"## Tokenization using spaCy"},{"metadata":{"trusted":true},"cell_type":"code","source":"[token.orth_ for token in doc]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Methods with underscore suffix in spaCy returns strings whereas methods without underscore suffix returns numbers"},{"metadata":{"trusted":true},"cell_type":"code","source":"[(token.orth_, token.orth) for token in doc if not token.is_punct | token.is_space | token.is_stop]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Tokens after removing punctuations, space and stop words"},{"metadata":{"trusted":true},"cell_type":"code","source":"extracted_tokens = [token.orth_ for token in doc if not token.is_punct | token.is_space | token.is_stop]\nextracted_tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"only_word_tokens = [i for i in extracted_tokens if i.isalpha()]\nonly_word_tokens","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tokenization based on sentences"},{"metadata":{"trusted":true},"cell_type":"code","source":"list(doc.sents)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lemmatization"},{"metadata":{"trusted":true},"cell_type":"code","source":"[word.lemma_ for word in doc]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## POS Tagging"},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_tag = [(word, word.tag_, word.pos_) for word in doc]\npos_tag","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[i for i in pos_tag if i[1] == 'POS']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### No 'POS' tags in our data so we do not have the option to exploit the owner and the possession information."},{"metadata":{"trusted":true},"cell_type":"code","source":"[j for j in pos_tag if j[2] == 'PART']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 'PART' tag suggests that these words are parts of the previous words."},{"metadata":{},"cell_type":"markdown","source":"### Noun chunks in the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"nouns = list(doc.noun_chunks)\nnouns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Syntactic dependency between tokens"},{"metadata":{"trusted":true},"cell_type":"code","source":"[(token, token.dep_) for token in doc]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Named Entity Recognition"},{"metadata":{"trusted":true},"cell_type":"code","source":"[i for i in doc.ents]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Entities alongwith their labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"[(i, i.label_, i.label) for i in doc.ents]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing named entities alongwith labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"#named entities along with text labels\nspacy.displacy.render(doc, style='ent', jupyter=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dependency Parser Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"spacy.displacy.render(doc, style='dep', jupyter=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}