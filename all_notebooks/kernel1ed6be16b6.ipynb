{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Traffic sign classification\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Transfer Learning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"transfer learning the process of reusing the model trained for anothor task.there are two approaches to perform transfer learning .\n\n1.feature Extraction\n2.fine tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os # work with directory\nimport cv2\nfrom tqdm import tqdm\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimg_size = 224\nfrom keras.utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"/kaggle/input/belgiumts-dataset/BelgiumTSC_Training/Training\"))\nprint(os.listdir(\"/kaggle/input/belgiumts-dataset/BelgiumTSC_Testing/Testing\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"load_dataset method which will be used for loading the traning dataset as well as testing dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_dataset(file_name):\n    if file_name == \"Training\":\n        directory = \"/kaggle/input/belgiumts-dataset/BelgiumTSC_Training/Training\"\n    elif file_name ==\"Testing\":\n        directory = \"/kaggle/input/belgiumts-dataset/BelgiumTSC_Testing/Testing\"\n        \n    list_images =[]\n    list_labels=[]\n    \n    count = 0\n        \n    for sub_dir in tqdm(os.listdir(directory)):\n\n        \n        if sub_dir == 'Readme.txt':\n            pass\n        \n        else:\n            inner_directory = os.path.join(directory,sub_dir)\n            for image_file in os.listdir(inner_directory):\n                \n                if image_file.endswith(\".ppm\"):\n                    \n                    img = cv2.imread(os.path.join(inner_directory,image_file))\n                    img = cv2.resize(img,(img_size,img_size))\n                    \n                    list_images.append(img)\n                    \n                    list_labels.append(count)\n            count +=1   \n            \n    list_images = np.array(list_images).reshape(-1,img_size,img_size,3)        \n                    \n    list_labels = np.array(list_labels)\n    list_labels= to_categorical(list_labels,count)        \n  \n      \n    return list_images , list_labels              \n                    \n                    \n                    \n                    \n            \n            \n            ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**training_dadaset contain image features which will be used to train the model and training_label contain outputs.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"training_dataset , training_label =load_dataset(\"Training\")\ntesting_dataset , testing_label =load_dataset(\"Testing\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now training dataset is devided into two part .","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1.training_set\n2.validation_set\n\ntraining_set is used to train the model and validation dataset is used to ckeck whether overfiting problem is occur or not.\n\noverfitting problem --> when our model give more correctly result on training set but not give correct result on validation set then we can say that there is overfitting problem.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_val, Y_train, Y_val = train_test_split(training_dataset,training_label ,test_size=0.2, random_state=101,shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. I am using second type of transfer learning --> finetuning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"a. i will use pretrained VGG-16 model .and remove last layer (output layer) from vgg-16.i will add some dense layer and only train these dense layer .\nb. all vgg-16 model will remain freeze.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Input ,Dense,Flatten\nfrom keras.models import Model\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vgg = VGG16(input_shape = [img_size,img_size,3],weights='imagenet',include_top = False) # include_top = false means remove output layer\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for layer in vgg.layers:\n    layer.trainable = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = Flatten()(vgg.output)\n\nlayer1 = Dense(512,activation = \"relu\")(X)\n\nlayer2 = Dense(1536,activation = \"relu\")(layer1)\n\nlayer3 = Dense(3072,activation = \"relu\")(layer2)\n\nlayer4 = Dense(6144,activation = \"relu\")(layer3)\n\nlayer5 = Dense(12288,activation = \"relu\")(layer4)\n\nprediction = Dense(Y_train.shape[1],activation = \"softmax\")(layer5) # output layer ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"above these two layers are added with vgg16 model .but only these two layer are used to train.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the model object\n\nmodel = Model(inputs = vgg.input,outputs = prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"opt = Adam(learning_rate=0.0001)\nmodel.compile(loss = \"categorical_crossentropy\",optimizer=opt,metrics =['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ans = model.fit(X_train ,Y_train , batch_size = 16,epochs =30,validation_data=(X_val,Y_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now we can see it gave 100% accuracy .","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = ans.history['accuracy']\nval_acc = ans.history['val_accuracy']\nloss = ans.history['loss']\nval_loss = ans.history['val_loss']\nepochs = range(1,len(acc)+1)\n\nplt.plot(epochs,acc,'bo',label ='Training_acc')\nplt.plot(epochs,val_acc,'b',label ='validation_acc')\nplt.title(\"training and validation accuracy\")\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs,loss,'bo',label ='Training_loss')\nplt.plot(epochs,val_loss,'b',label ='validation_loss')\nplt.title(\"training and validation accuracy\")\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"predict = np.argmax(model.predict(testing_dataset), axis = 1)\ncount = 0\nfor i in range(0,predict.shape[0]):\n    if (predict[i] == np.argmax(testing_label[i])):\n        count +=1\nprint ('Accuracy on Test ',100 * count/predict.shape[0],'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}