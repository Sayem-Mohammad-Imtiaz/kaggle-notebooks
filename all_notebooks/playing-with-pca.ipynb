{"cells":[{"metadata":{"_uuid":"9ef226e4f30a3cbc332b09e337cbe53d7963387d","_cell_guid":"b375623a-64f7-46c5-8092-a6e03b9e5960"},"cell_type":"markdown","source":"# Playing with PCA"},{"metadata":{"_uuid":"b0f17c390101d24442c511635257eff609bc89be","_cell_guid":"afedc130-a17c-46e0-badf-50444de06425"},"cell_type":"markdown","source":"In this notebook I want to explore a bit how PCA can help with predictions in a regression setting across several models. We'll be using 10 fold cross validation to compare between Linear Regression, Lasso, Ridge and Random Forest.  \n  \n** Because our main concern is to explore PCA and what it can do, we will not be comparing between individual models but, between the mean error and standard error of errors of different  algorithms' predictions.  **  \nThat means that instead of selecting a specific model and saying it is the best one for the problem, we'll be using cross validation to aggregate predictions from each of the algorithms: Linear Regression, LassoCV, RidgeCV and RandomForestRegressor and try and see how PCA affects each of these aggregated predictions.  \n  \nWe'll compare between all models before PCA, with all possible numbers of PCA components and also repeat the process after adjusting for multicollinearity to better understand the relationship between PCA and mullticollinearity regarding predictions."},{"metadata":{"collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"59aded9567b8ac8d80c1dc1e11383dd44bc2748c","_cell_guid":"16329f7f-d2d4-4a8f-8e04-283d415b062d","trusted":false},"cell_type":"code","source":"df=pd.read_csv('../input/Financial Distress.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc3fcb8f336eef3d8d4e308578f8b8af658479e4","_cell_guid":"25065feb-d454-403e-af89-925b834754a4"},"cell_type":"markdown","source":"Take a quick look at our data:"},{"metadata":{"collapsed":true,"_uuid":"78896effc571bdc310496b5f144979086a5d7e28","_cell_guid":"8d1d477b-7159-4a79-9ca8-7d59bf5c8139","trusted":false},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"a3f2c25f9486c27f6e175a91e6845b1a2f629b05","_cell_guid":"193c68ce-e1eb-49a9-b988-6fdb0d106ef9","trusted":false},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c603fcd1ae5f6d49a5b919012fee661dc2239a1","_cell_guid":"d86a6bf8-d133-4dec-9d57-f4c2c7fd830b"},"cell_type":"markdown","source":"For the purpose of this notebook we'll assume each line represents a different, independent measurement:"},{"metadata":{"collapsed":true,"_uuid":"e44355fa9199f88ea873e489c17094a322612e8b","_cell_guid":"77700ad2-f24f-4141-a1ba-9c25d81a9c11","trusted":false},"cell_type":"code","source":"cdf=df.drop(labels=['Company','Time'], axis=1)\ncdf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe628c99ba05805ec7626f322c3efebd9dd733ec","_cell_guid":"66e22401-3b36-4662-9ccc-24dc02d1bf47"},"cell_type":"markdown","source":"Next, we'll shamelessly remove outliers to get a smaller variation in y and nicer predictions:"},{"metadata":{"collapsed":true,"_uuid":"ffd7a4a8360fcd743a7de8d71d2131780066c378","_cell_guid":"61bc6554-4abd-4c37-965d-3973a682f47e","trusted":false},"cell_type":"code","source":"cdf = cdf[cdf['Financial Distress']>-2.5]\ncdf = cdf[cdf['Financial Distress']<4]\ncdf.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d43a983cfae1393284437f74345b16a67465399c","_cell_guid":"2fab9e10-e8e9-4392-a4c3-95251a6862a9"},"cell_type":"markdown","source":"Scale the data:"},{"metadata":{"collapsed":true,"_uuid":"4e2d0b0dabea411df68e88c2e5b586ead3b7f45e","_cell_guid":"22ba9a34-e8f8-4f32-8944-135a70c40110","trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX = pd.DataFrame(scaler.fit_transform(cdf.drop('Financial Distress', axis=1)))  \ny = cdf['Financial Distress']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1957cd4d6c9c1a8730356114ddb1890bd8b5984","_cell_guid":"7b369331-9088-4c05-9559-c64ab9f33794"},"cell_type":"markdown","source":"We'll be using cross validation scored by root mean squared error to asses the models:"},{"metadata":{"collapsed":true,"_uuid":"f81f1e5ff4245f483fb158943b5b83b015b1a89e","_cell_guid":"78c288c2-d53a-4fe2-8af3-a337cd708ef0","trusted":false},"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import make_scorer\n\nMSE = metrics.make_scorer(metrics.mean_squared_error)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a9cfa1a904fdd43e63ae6c8365a6dcddd3d1892","_cell_guid":"b02b148c-b70f-4276-ad1d-c0f75afb88df"},"cell_type":"markdown","source":"Now, let's start with some predictions.  \nWe'll be trying Linear Regression, Lasso, Ridge and Random Forest and than, all of them again after applying PCA with different components.  \nLet's start naively, without considering multicollinearity."},{"metadata":{"collapsed":true,"_uuid":"49968ffb8e0e48406a2fb32aba624bc45087700c","_cell_guid":"7a1e6ee0-f252-48e8-91c9-0e58a0d226e3","trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\nlrCV=cross_val_score(lr,X,y,scoring=MSE,cv=10)\nprint('Mean RMSE is:',np.sqrt(lrCV).mean() )\nprint('Std is:', np.sqrt(lrCV).std() )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a95a91037c04743a84689bb66104f877f4e74fb","_cell_guid":"3e6fc6a8-2536-4f23-8512-4e181fca81a4"},"cell_type":"markdown","source":"Remebering that our y's std is ~ 1 this score is not very impressive...  \nLet's see how Lasso Regression handles this:"},{"metadata":{"collapsed":true,"_uuid":"db3651b4e8bc3753c43affe7874aa95640b0fe73","_cell_guid":"5e5233e5-9c0b-4ed5-bae6-b1195b620f28","trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LassoCV\n\nlas = LassoCV(n_alphas=500, tol=0.001)\nlasCV=cross_val_score(las,X,y,scoring=MSE,cv=10)\nprint('Mean RMSE is:',np.sqrt(lasCV).mean() )\nprint('Std is:', np.sqrt(lasCV).std() )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70c6267df4fd74b7b8659e5822af0f1ae09a878d","_cell_guid":"9925c72a-e088-48cb-b8d0-1b2459e24724"},"cell_type":"markdown","source":"That's a huge improvement over regular regression!  \nLet's see what Ridge Regression can do:"},{"metadata":{"collapsed":true,"_uuid":"623e0e26c42fd98521e69b7d7ba6109085816874","_cell_guid":"286a0c40-9f40-46fb-a2ce-3160cb9828f9","trusted":false},"cell_type":"code","source":"from sklearn.linear_model import RidgeCV\n\nri = RidgeCV(alphas=(0.1,1,10,100,1000,10000))\nriCV=cross_val_score(ri,X,y,scoring=MSE,cv=10)\nprint('Mean RMSE is:',np.sqrt(riCV).mean() )\nprint('Std is:', np.sqrt(riCV).std() )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6d3a5c9a2ace4ab0cdcb4fb52e847c1ab215af7","_cell_guid":"cdc08d82-9202-4d2d-b2cc-b5e792970131"},"cell_type":"markdown","source":"Wuah! Even better!  \nHow about Random Forest?"},{"metadata":{"collapsed":true,"_uuid":"8b696db1059918c9e439c12e6c5c6b01ca5f68d5","_cell_guid":"ca35b421-0abb-457e-b4fa-b2896b68120c","trusted":false},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor()\nrfCV=cross_val_score(rf,X,y,scoring=MSE,cv=10)\nprint('Mean RMSE is:',np.sqrt(rfCV).mean() )\nprint('Std is:', np.sqrt(rfCV).std() )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"648e669252d3734a48a216a84c1fc74aee5b2150","_cell_guid":"24d95670-cd3b-4cf4-9878-360dfd2848f1"},"cell_type":"markdown","source":"That's actually pretty decent!  \nAnd such a low std is pretty cool.  \nGreat.  \nNow that we got a basic feel for our data we can take a look at what PCA can do for us.  \nI want to get a prediction using each of our models for any number of PCA components and put all that in a nice graph.  "},{"metadata":{"collapsed":true,"_uuid":"b63d4261c5ada15c3d901cc26a67356c062652db","_cell_guid":"d74cf4a5-6dc3-477d-ab8a-e26bdf5140c7","trusted":false},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\n# This will be our PCA calculation function.\ndef calc_pca_models(_X,_y,_cv=10):\n    \n    # This will be our results DataFrame:\n    RMSE_PCA = pd.DataFrame(0, dtype=float, index=range(len(_X.columns)), \n                                columns=['lr','lr_std', 'las', 'las_std', 'ri', 'ri_std',\n                                         'rf', 'rf_std','PCA'])\n    \n    # PCA of the data.\n    pca = PCA(n_components=len(_X.columns))\n    pca.fit(_X)\n    X_pca=pd.DataFrame(pca.transform(_X))\n\n    #Start crunching the numbers!\n\n    for i in range (len(_X.columns)):\n        \n        X_pca_c = X_pca[X_pca.columns[:i+1]] # Choose how many components to consider.\n        \n        RMSE_PCA['PCA'][i] = i+1     # Write down how many components we're discussing.\n\n        # Regression\n        lrCV=cross_val_score(lr,X_pca_c,y,scoring=MSE,cv=_cv)\n        RMSE_PCA['lr'][i] = np.sqrt(lrCV).mean() # Write mean RSME.\n        RMSE_PCA['lr_std'][i] = np.sqrt(lrCV).std() # Write down RSME std. \n        \n        #Lasso\n        lasCV=cross_val_score(las,X_pca_c,y,scoring=MSE,cv=_cv)\n        RMSE_PCA['las'][i] = np.sqrt(lasCV).mean() # Write mean RSME.\n        RMSE_PCA['las_std'][i] = np.sqrt(lasCV).std() # Write down RSME std.\n    \n        #Ridge\n        riCV=cross_val_score(ri,X_pca_c,y,scoring=MSE,cv=_cv)\n        RMSE_PCA['ri'][i] = np.sqrt(riCV).mean() # Write mean RSME.\n        RMSE_PCA['ri_std'][i] = np.sqrt(riCV).std() # Write down RSME std.\n    \n        # Random Forest!\n        rfCV=cross_val_score(rf,X_pca_c,y,scoring=MSE,cv=_cv)\n        RMSE_PCA['rf'][i] = np.sqrt(rfCV).mean() # Write mean RSME.\n        RMSE_PCA['rf_std'][i] = np.sqrt(rfCV).std() # Write down RSME std.\n        \n    return RMSE_PCA","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"453bc6d2572429c9ca680798f0ae644cc972b3b3","_cell_guid":"8c4783e0-6472-4e3c-ab85-c4f8c86ec0fb","trusted":false},"cell_type":"code","source":"df_precol = calc_pca_models(X,y,)  # Get the numbers.\n\no_las = np.sqrt(lasCV).mean()*np.ones(df_precol.shape[0]) \no_ri = np.sqrt(riCV).mean()*np.ones(df_precol.shape[0]) \no_rf = np.sqrt(rfCV).mean()*np.ones(df_precol.shape[0]) \n\n# Plot the numbers.\nfig = plt.figure()\nax = fig.add_axes([0,0,2,2])\nx=df_precol['PCA']\nplt.ylim(0.5,1.5)\nplt.xlim(0,df_precol.shape[0]+1)\n\n#ax.plot(x, df_precol['lr']-df_precol['lr_std'], '--', 'b')\nax.plot(x, df_precol['lr'], 'b', label='Regression',)  \nax.plot(x, df_precol['lr']+df_precol['lr_std'], 'bv')\n\n#ax.plot(x, df_precol['las']-df_precol['las_std'], '--', color='orange')\nax.plot(x, df_precol['las'],'orange', label='Lasso')\nax.plot(x, df_precol['las']+df_precol['las_std'], 'v', color='orange', label = 'Upper Lasso std')\nax.plot(x, o_las, '.', color='orange', label = 'Original Lasso')\n\n\nax.plot(x, df_precol['ri']-df_precol['ri_std'], 'g^')\nax.plot(x, df_precol['ri'], 'g', label='Ridge')\n#ax.plot(x, df_precol['ri']+df_precol['ri_std'], 'g--')\nax.plot(x, o_ri, '.', color='g', label = 'Original Ridge')\n\nax.plot(x, df_precol['rf']-df_precol['rf_std'], 'r^', label = 'Lower RF std')\nax.plot(x, df_precol['rf'], 'r', label='RandomForest')\n#ax.plot(x, df_precol['rf']+df_precol['rf_std'], 'r--')\nax.plot(x, o_rf, '.', color='r', label = 'Orginal Random Forest')\n\n\nax.set_xlabel('Number of PCA Vectors')\nax.set_ylabel('RMSE')\nax.plot()\n\nax.legend()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e45dee6c1d5db2d33058232224640b748b320c34","_cell_guid":"b7b614f0-7583-4cb5-a221-4dc752f4af10"},"cell_type":"markdown","source":"That's a bit of a messy graph but it's good enough to tell us  a couple of things:  \n1. Random forest doesn't seem to be affected at all by PCA on this data set.\n2. Our regressions (specifically their std) seem to improve drastically with less than 10 PCAs.\n\nLet's remove Random Forest and take a closer look!\n"},{"metadata":{"collapsed":true,"_uuid":"cd87a8e91c8098581e9e49be12d6519ae19ecda8","_cell_guid":"109e6787-4383-4646-89d5-c5bd6ef9299f","trusted":false},"cell_type":"code","source":"o_las = np.sqrt(lasCV).mean()*np.ones(df_precol.shape[0]) \no_las_std = np.sqrt(lasCV).std()*np.ones(df_precol.shape[0]) \no_ri = np.sqrt(riCV).mean()*np.ones(df_precol.shape[0]) \no_ri_std = np.sqrt(riCV).std()*np.ones(df_precol.shape[0])\n\n# Plot the numbers.\nfig = plt.figure()\nax = fig.add_axes([0,0,2,2])\nx=df_precol['PCA']\nplt.ylim(0.5,1.5)\nplt.xlim(0.8,11)\n\nax.plot(x, df_precol['lr']-df_precol['lr_std'], 'b^')\nax.plot(x, df_precol['lr'], 'b', label='Regression',)  \nax.plot(x, df_precol['lr']+df_precol['lr_std'], 'bv')\n\nax.plot(x, df_precol['las']-df_precol['las_std'], '^', color='orange')\nax.plot(x, df_precol['las'],'orange', label='Lasso')\nax.plot(x, df_precol['las']+df_precol['las_std'], 'v', color='orange', label = 'Lasso std')\n\nax.plot(x, o_las+o_las_std, '--', color='orange', label = 'Original Lasso std')\nax.plot(x, o_las, '.', color='orange', label = 'Original Lasso')\nax.plot(x, o_las-o_las_std, '--', color='orange')\n\nax.plot(x, df_precol['ri']-df_precol['ri_std'], 'g^', label = 'Ridge std')\nax.plot(x, df_precol['ri'], 'g', label='Ridge')\nax.plot(x, df_precol['ri']+df_precol['ri_std'], 'gv')\n\nax.plot(x, o_ri+o_ri_std, '--', color='g', label = 'Original Ridge std')\nax.plot(x, o_ri, '.', color='g', label = 'Original Ridge')\nax.plot(x, o_ri-o_ri_std, '--', color='g')\n\n\nax.set_xlabel('Number of PCA Vectors')\nax.set_ylabel('RMSE')\nax.plot()\n\nax.legend()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51a448ca5a88077cc746c6ee99be9375c226c8c8","_cell_guid":"3be0bd15-6404-4727-9584-bc81eac8ff29"},"cell_type":"markdown","source":"So, the triangles mark the std of the PCAs and the dashed lines the std's of the original regressions and we can see that with 4-9 PCA components we get a similar prediction to the one we had with all the data but with much lower std.  \nCool.  \n  \n  Can we improve this results by adjusting for multicollinearity?  \n   Let's first asses how much multicollinearity we have:"},{"metadata":{"collapsed":true,"_uuid":"1e99113cb88fc6a7c824ca35e0cecbcaa2823ecf","_cell_guid":"fda8535a-ca40-46b9-a91c-976c254bf415","trusted":false},"cell_type":"code","source":"corrmat = cdf.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b2339adf3b3e4dfec59cf57fad1f11969e71dd0","_cell_guid":"0f8ae231-4806-4099-bf1f-1d0a3e2e80d4"},"cell_type":"markdown","source":"Quite a bit!  \nLet's deal with that with a little script I adapted from [here](https://stats.stackexchange.com/questions/155028/how-to-systematically-remove-collinear-variables-in-python).  \nIt removes variables with VIF over a treshold.\n"},{"metadata":{"collapsed":true,"_uuid":"d14d2260b4a460f48bde6096fef075b9e6b1e160","_cell_guid":"dfa998aa-3888-4658-8269-a8782ca4e9f1","trusted":false},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor    \n\ndef remove_multicol(X, thresh=5.0):\n    variables = list(range(X.shape[1]))\n    dropped = True\n    while dropped:\n        dropped=False\n        vif = [variance_inflation_factor(X[variables].values, ix) for ix in range(X[variables].shape[1])]\n\n        maxloc = vif.index(max(vif))\n        if max(vif) > thresh:\n            del variables[maxloc]\n            dropped=True\n\n    return X[variables]\n\nX_colli = remove_multicol(X,5)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"87eb165d7dc15cedd631313218152a91a49fbaa3","_cell_guid":"ccc01d31-40af-4c77-bc2e-5e102d7f5897","trusted":false},"cell_type":"code","source":"corrmat = X_colli.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9577e6b83cf18de5f2047317204580f5dbf7f9b6","_cell_guid":"baf57df9-b42d-4bfe-be39-4cc403fe03f0"},"cell_type":"markdown","source":"Looks way better!  \nLet's see how it helped with our models:"},{"metadata":{"collapsed":true,"_uuid":"a0e7f90c9fc4580f3e90ff23f2959e87cdd6d58f","_cell_guid":"30ae6893-93a9-4edc-9ad7-a0930bfb6771","trusted":false},"cell_type":"code","source":"print('___Before adjusting for multicollinearity___')\nlrCV=cross_val_score(lr,X,y,scoring=MSE,cv=10)\nprint('Regression')\nprint('Mean RMSE is:',np.sqrt(lrCV).mean() )\nprint('Std is:', np.sqrt(lrCV).std() )\n\nlasCV=cross_val_score(las,X,y,scoring=MSE,cv=10)\nprint('Lasso')\nprint('Mean RMSE is:',np.sqrt(lasCV).mean() )\nprint('Std is:', np.sqrt(lasCV).std() )\n\nriCV=cross_val_score(ri,X,y,scoring=MSE,cv=10)\nprint('Ridge')\nprint('Mean RMSE is:',np.sqrt(riCV).mean() )\nprint('Std is:', np.sqrt(riCV).std() )\n\nrfCV=cross_val_score(rf,X,y,scoring=MSE,cv=10)\nprint('Random Forest')\nprint('Mean RMSE is:',np.sqrt(rfCV).mean() )\nprint('Std is:', np.sqrt(rfCV).std() )\n\nprint('___After adjusting for multicollinearity___')\nlrCV=cross_val_score(lr,X_colli,y,scoring=MSE,cv=10)\nprint('Regression')\nprint('Mean RMSE is:',np.sqrt(lrCV).mean() )\nprint('Std is:', np.sqrt(lrCV).std() )\n\nlasCV=cross_val_score(las,X_colli,y,scoring=MSE,cv=10)\nprint('Lasso')\nprint('Mean RMSE is:',np.sqrt(lasCV).mean() )\nprint('Std is:', np.sqrt(lasCV).std() )\n\nriCV=cross_val_score(ri,X_colli,y,scoring=MSE,cv=10)\nprint('Ridge')\nprint('Mean RMSE is:',np.sqrt(riCV).mean() )\nprint('Std is:', np.sqrt(riCV).std() )\n\nrfCV=cross_val_score(rf,X_colli,y,scoring=MSE,cv=10)\nprint('Random Forest')\nprint('Mean RMSE is:',np.sqrt(rfCV).mean() )\nprint('Std is:', np.sqrt(rfCV).std() )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52b9384306b6043eb137469c440bcac9b3f5aea5","_cell_guid":"1b35cb32-2297-41d5-8e3f-4b81fecf50ba"},"cell_type":"markdown","source":"Makes quite a difference!  (Although Random Forest still doesn't seem to be impressed..)  \nLet's see how it would look like with all the PCA stuff:  \n(I'll remove Random Forest because it doesn't seem to be affected and because it takes quite a while to run..)"},{"metadata":{"collapsed":true,"_uuid":"e0ed3da72073c0946bb6b842a424618f05a1c9e9","_cell_guid":"8dbed74c-0e2e-48bf-b1b6-df2509e2d30f","trusted":false},"cell_type":"code","source":"df_col = calc_pca_models(X_colli,y,)  # Get the numbers.\n\no_las = np.sqrt(lasCV).mean()*np.ones(df_col.shape[0]) \no_las_std = np.sqrt(lasCV).std()*np.ones(df_col.shape[0]) \no_ri = np.sqrt(riCV).mean()*np.ones(df_col.shape[0]) \no_ri_std = np.sqrt(riCV).std()*np.ones(df_col.shape[0])\n\n\n# Plot the numbers.\nfig = plt.figure()\nax = fig.add_axes([0,0,2,2])\nx=df_col['PCA']\nplt.ylim(0.5,1.5)\nplt.xlim(0,df_col.shape[0]+1)\n\nax.plot(x, df_col['lr'], 'b', label='Regression',)  \nax.plot(x, df_col['lr']+df_col['lr_std'], 'bv')\n\nax.plot(x, df_col['las']-df_col['las_std'], '^', color='orange')\nax.plot(x, df_col['las'],'orange', label='Lasso')\n\nax.plot(x, df_col['ri'], 'g', label='Ridge')\nax.plot(x, df_col['ri']+df_col['ri_std'], 'gv', label = 'Ridge std')\n\nax.plot(x, o_ri+o_ri_std, '--', color='g', label = 'Original Ridge std')\nax.plot(x, o_ri, '.', color='g', label = 'Original Ridge')\nax.plot(x, o_ri-o_ri_std, '--', color='g')\n\nax.plot(x, o_las+o_las_std, '--', color='orange', label = 'Original Lasso std')\nax.plot(x, o_las, '.', color='orange', label = 'Original Lasso')\nax.plot(x, o_las-o_las_std, '--', color='orange')\n\nax.set_xlabel('Number of PCA Vectors')\nax.set_ylabel('RMSE')\nax.plot()\n\nax.legend()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3ef1f2569ece43adcca2ae3b372a3d80b1d61f8","_cell_guid":"00723a55-9961-477b-8967-7067c00e415f"},"cell_type":"markdown","source":"Apart from improving regular regression PCA had no effect here.\nOn both Ridge and Lasso best results are acheived only while using all vectors and are than equal to those prior to using PCA.  "},{"metadata":{"_uuid":"ee2187c414b9a0b8e4f419a346a55e82bc716cc6","_cell_guid":"2fd2781d-546e-4723-8f79-64e60ab7b89c"},"cell_type":"markdown","source":"# In Conclusion  \nIn this data set, PCA is effective in improving predictions using several components in Linear Regression, Ridge and Lasso.  \nBut, after adjusting for multicollinearity, PCA only improved Linear Regression and had inferior results to those achieved by using all of the data using Ridge and Lasso.  \nRandom Forest did not seem to be affected by PCA or multicollinearity on this data set and it seems to yield the best results regardless."},{"metadata":{"collapsed":true,"_uuid":"a82e389fb0c697e0a94556f5c5666414c67bc9f4","_cell_guid":"5c1e75f4-7572-479a-9c30-759d7b7ddbd3","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}