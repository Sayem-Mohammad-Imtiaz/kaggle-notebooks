{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"font-size: 30px; margin-left:50px\">SPAM detector</h1>\n\n<img src=\"https://gifimage.net/wp-content/uploads/2018/05/spam-gif-6.gif\" style=\"width:20%; float:center;\">\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import libraries\nimport numpy as np\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.metrics import confusion_matrix\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\nimport xgboost\nfrom sklearn import svm,tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n\n\nimport tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding,Input,LSTM,Dense,Bidirectional,Dropout, Activation\nfrom keras.models import Model\nfrom tensorflow.keras.models import Sequential\ntf.__version__\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the data\ndf = pd.read_csv('../input/spam-filter/emails.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking the number of duplicate columns\nprint('Number of duplicate rows in the data are : ',df[df.duplicated(subset=None, keep='first') == True].shape[0], '\\nSo we drop them')\n\n# dropping the duplicate columns\ndf.drop_duplicates(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Describing the values in the Spam column\ndf.groupby('spam').describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the "},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating a column with the length of each message\ndf['mail_len'] = df.text.apply(len)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"Plotting the histogram of data for the count of Ham and Spam mails"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6,6))\n\ndf.spam[df.spam==1].plot(bins=4, kind='hist', color='blue', \n                                       label='Spam Mails', alpha=0.6)\n\ndf.spam[df.spam==0].plot(bins=4, kind='hist', color='red', \n                                       label='Ham Mails', alpha=0.6)\nplt.legend()\nplt.xlabel(\"Ham/Spam\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We cam see that the Ham mails are more almost 4 times in number than the spam mails"},{"metadata":{},"cell_type":"markdown","source":"Plottin the distplot to see the distribution of mail length...bigger the mail lenght, higher the plot goes."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"plt.style.use('seaborn-darkgrid')\nplt.figure(figsize=(10,5))\nsns.distplot(df['mail_len'],kde=True,color='red',hist=True)\nplt.xlabel(\"Message Length\",size=15)\nplt.ylabel(\"Frequency\",size=15)\nplt.title(\"Length Histogram\",size=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the initial rows have got bigger mails than the later ones"},{"metadata":{},"cell_type":"markdown","source":"Lets see the histogram of length of mails for both the labels in the same plot one over the other"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\ndf[df.spam==1].mail_len.plot( kind='hist', color='blue',label='Spam Mails', alpha=0.6)\ndf[df.spam== 0].mail_len.plot(kind='hist', color='red',label='Ham Mails', alpha=0.6)\nplt.legend()\nplt.xlabel(\"Mail Length\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing of the text data\nIn preprocessing we will remove the punctuations and stopwords and lower case all the mails data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#1.Punctuations are [!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~]\n#2.Stop words in natural language processing, are useless words (data).\n\ndef process_text(text):\n    \n    #1 Remove Punctuationa\n    nopunc = [char for char in text if char not in string.punctuation]\n    nopunc = ''.join(nopunc)\n    \n    #2 Remove Stop Words\n    clean_words = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n    \n    #3 Return a list of clean words\n    return clean_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Show the processed data\ndf.text = df.text.apply(process_text)\ndf.text.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Vectorization of the text data \n\nWe can not feed text data directly to the models. So we will vectorize each mail into a matrix by tokenizing it, then converting  into numerial vectors and finally padding it to create a matrix of numbers for each mail input."},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = 10000\nmax_len = 250\n\n# Tokenize the mails\ntok = Tokenizer(num_words=vocab_size)\ntok.fit_on_texts(df.text)\n\n# Use text_to_sequence to convert it into vectors\nsequences = tok.texts_to_sequences(df.text)\n\n# pad seqence to create a matrix of equal length mails\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets see how the mails look like now....."},{"metadata":{"trusted":true},"cell_type":"code","source":"sequences_matrix[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay...So finally the data is ready for training."},{"metadata":{},"cell_type":"markdown","source":"### Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(sequences_matrix, df.spam, test_size = 0.2, random_state = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will now train the data on the below ML Models.\nLets make a list of the classification models, fit them on training data and check for their respective accuracies."},{"metadata":{"trusted":true},"cell_type":"code","source":"models=[RandomForestClassifier(),\n        GaussianNB(),\n        AdaBoostClassifier(),\n        xgboost.XGBClassifier(),\n        svm.SVC(),\n        tree.DecisionTreeClassifier(),\n        KNeighborsClassifier()]\n\nmodel_names=['Random Forest Classifier',\n             'Gaussian Naive Bayes Classifier',\n             'Adaboost Classifier',\n             'XGBoost Classifier',\n             'Support Vector Classifier',\n             'Decision Tree Classifier',\n             'K Nearest Neighbour Classifier']\naccuracy=[]\nd={}\nfor model in range (len(models)):\n    clf=models[model]\n    clf.fit(X_train,y_train)\n    y_pred=clf.predict(X_test)\n    accuracy.append(accuracy_score(y_test,y_pred))\nd={'Modelling Algo':model_names,'Accuracy':accuracy} ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"lets put all the models with their accuracies and compare to see whcih one has the highest score."},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_frame=pd.DataFrame.from_dict(d, orient='index').transpose()\naccuracy_frame","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGBoost Classifier has the highest score, we will do hyperparameter tuning and see how much the accuracy improves"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(random_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = xgboost.XGBClassifier()\n# Random search of parameters, using 2 fold cross validation, \n# search across 5 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 5, cv = 2, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_random.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_random.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Evaluating the score with best parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = rf_random.best_estimator_\nrfc.fit(X_train, y_train)\ny_pred1 = rfc.predict(X_test) \nprint(confusion_matrix(y_test,y_pred1))\nprint(accuracy_score(y_test,y_pred1))\nprint(classification_report(y_test,y_pred1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Accuracy using XGBoost with best parameters does improve improved the accuracy score but its still not satisfactory\n"},{"metadata":{},"cell_type":"markdown","source":"### Now lets use a simple single layered LSTM model"},{"metadata":{},"cell_type":"markdown","source":"https://en.wikipedia.org/wiki/Long_short-term_memory\n## What is LSTM and why it is used..\n\nLong short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning.\n\nIn theory, classic (or \"vanilla\") RNNs can keep track of arbitrary long-term dependencies in the input sequences. The problem with vanilla RNNs is computational (or practical) in nature: when training a vanilla RNN using back-propagation, the gradients which are back-propagated can \"vanish\" (that is, they can tend to zero) or \"explode\" (that is, they can tend to infinity), because of the computations involved in the process, which use finite-precision numbers. RNNs using LSTM units partially solve the vanishing gradient problem, because LSTM units allow gradients to also flow unchanged.\nWe also have Bi-directional LSTM which overcomes the drwbacks of LSTM model"},{"metadata":{},"cell_type":"markdown","source":"[https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/](http://)\n## Embedding layer - \nAlso we are using an embedding layer before giving the data to the LSTM layer\n\nThe Embedding layer is defined as the first hidden layer of a network. It must specify 3 arguments: \n1. input_dim: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n2. output_dim: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word.\n3. input_length: This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(vocab_size, 200, input_length=max_len))\nmodel.add(LSTM(32))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.summary()\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=5,batch_size=64)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = model.evaluate(X_test, y_test, verbose=0)\ny_pred = model.predict_classes(X_test)\n\nprint('Test loss:', scores[0])\nprint('Test accuracy:', scores[1])\nprint('confusion matrix:\\n', confusion_matrix(y_pred,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## So we can see that a simple LSTM model gives an accuracy of 0.98 whereas best ML model had just 0.89."},{"metadata":{},"cell_type":"markdown","source":"## So finally we have our machine ready....You feed the message and it will tell you whether its a SPAM or HAM\n\n<img src=\"https://digitalmarketingbypsk.files.wordpress.com/2017/05/21.gif\" style=\"width:30%; float:center;\">\n"},{"metadata":{},"cell_type":"markdown","source":"## Thank You........"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}