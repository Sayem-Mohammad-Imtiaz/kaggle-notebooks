{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"5c11cebc-996d-44f5-e022-3f34fa33f9c5"},"source":"#Searching for A Classifier\n\n\n----------"},{"cell_type":"markdown","metadata":{"_cell_guid":"9de35311-0d5e-1d54-6795-ccb22ded359e"},"source":"## Set Up Dataset\n\n\n----------\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ffeaa166-8434-36c8-13c7-125c449132da"},"outputs":[],"source":"from pandas import read_csv\ndata = read_csv(\"../input/WA_Fn-UseC_-HR-Employee-Attrition.csv\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"f3094246-c4f9-6380-c82f-cf9ec199b994"},"source":"##Prepare Data\n\n\n----------"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d0e26e32-79fe-d929-f047-2ff4402d08ec"},"outputs":[],"source":"target = \"Attrition\"\nfeature_by_dtype = {}\nfor c in data.columns:\n    \n    if c == target: continue\n    \n    data_type = str(data[c].dtype)\n    \n    if data_type not in feature_by_dtype.keys():\n         feature_by_dtype[data_type] = [c]\n    else:\n        feature_by_dtype[data_type].append(c)\n\nobjects = feature_by_dtype[\"object\"]\nremove = [\"Over18\"]\ncategorical_features = [f for f in objects if f not in remove]\nint64s = feature_by_dtype[\"int64\"]\ncount_features = []\nfor i in [i for i in int64s if len(data[i].unique()) < 20 and i not in remove]:\n    count_features.append(i)\nremove.append(\"StandardHours\")\nremove.append(\"EmployeeCount\")\ncount_features += [\"TotalWorkingYears\", \"YearsAtCompany\", \"HourlyRate\"]\nremove.append(\"EmployeeNumber\")\nnumerical_features = [i for i in int64s if i not in remove]\nfeatures = categorical_features + numerical_features\n\nfor c in categorical_features:\n    data[c] = data[c].apply(str)\n\n# Global variables\nfeatures, target, categorical_features, numerical_features, count_features\npass"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ac52db1c-fa52-b9df-1fd5-7cc04f720584"},"outputs":[],"source":"from pandas import get_dummies,concat\nonehot_encoded_categorical_data = get_dummies(data[categorical_features])\n\nX = concat([data[numerical_features], onehot_encoded_categorical_data], axis=1)\ny = data[target]"},{"cell_type":"markdown","metadata":{"_cell_guid":"c5cb0f9f-4d6b-e82d-2be0-fc6dc7a53c82"},"source":"# On To Finding A Classifier!\n\n\n----------\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"20a644c5-6443-6331-854b-dd5e7790cc8e"},"source":"10 K Folds Cross Validation"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7ec2afc9-93c6-0ba1-94d3-33519e4e6f28"},"outputs":[],"source":"def get_results(model, X, y):\n\n    import warnings\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        from sklearn.model_selection import cross_val_score\n        compute = cross_val_score(model, X, y, cv=10)\n        mean = compute.mean()\n        std = compute.std()\n        return mean, std\n\ndef display_classifier_results(X,y):\n\n    models = []\n\n    from sklearn.neighbors import KNeighborsClassifier\n    models = [KNeighborsClassifier()]\n    \n    from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n    models += [GaussianNB(), MultinomialNB(), BernoulliNB()]\n\n    from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier#, VotingClassifier\n    models += [RandomForestClassifier(), AdaBoostClassifier(), GradientBoostingClassifier(), ExtraTreesClassifier()]\n\n    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n    models += [LinearDiscriminantAnalysis(), QuadraticDiscriminantAnalysis()]\n\n    from sklearn.svm import SVC, LinearSVC\n    models += [SVC(),LinearSVC()]\n\n    from sklearn.linear_model import SGDClassifier\n    models += [SGDClassifier()]\n\n    from sklearn.neighbors.nearest_centroid import NearestCentroid\n    models += [NearestCentroid()]\n\n    from sklearn.neural_network import MLPClassifier\n    models += [MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(30, 2), random_state=1)]\n\n    output = {}\n\n    for m in models:\n        try:\n            model_name = type(m).__name__\n            scores = get_results(m,X,y)\n            row = {\"Average Score\" : scores[0], \"Standard Deviation\" : scores[1]}\n            output[model_name] = row\n        except:\n            pass\n\n    from pandas import DataFrame\n    from IPython.display import display\n\n    display(DataFrame(data=output).T.round(2).sort_values(\"Average Score\", ascending=False))\n\ndisplay_classifier_results(X,y)"},{"cell_type":"markdown","metadata":{"_cell_guid":"5a801cfc-dabe-8d49-4866-8282fa214d9d"},"source":"Awesome, so just with no feature engineering and classifiers on default settings, we can get a **Linear Discriminant Analysis classifier** or **AdaBoostClassifier** with **88% accuracy score**, just by sklearn's default settings."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}