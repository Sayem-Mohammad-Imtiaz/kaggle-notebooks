{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns \n%matplotlib inline\n\n# Import data and select \"id\" column as index\n\npath = '../input/breast-cancer-csv/breastCancer.csv'\ndf = pd.read_csv(path, index_col = \"id\")\n\n# Examine the dataframe\n\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, the datatype information from the dataframe shows that our target variable for the classifier model is integer type. We need to convert it to categorical. A further step needs to be made with our categorical data, but we will save this step for before implementing the model. If you are curious as I would be at this point, the further step is to convert categorical data into one-hot encoding."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert the \"class\" column (target feature for classifier) to categorical data\n\ndf[\"class\"] = df[\"class\"].astype(\"category\")\n\n# Examine dataframe again\n\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The column \"bare_nucleoli\" has to be changed to integer data type. Here we have two problems. The first one is that the datatypes are strings (object), while the second one is that missing values are reported as \"?\". First the Series is converted to \"integer\" datatype, and \"?\" to \"None\". Second, we make use of scikit-learn's imputer function, and lastly we save the output as its correspondant column. We store the values as integers. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nimport numpy as np\n\n# apply lambda function to change \"?\" for None values\n\ndf[\"bare_nucleoli\"] = df[\"bare_nucleoli\"].apply(lambda x: None if x is \"?\" else x)\n\n# Convert column to numeric type data\n\ndf[\"bare_nucleoli\"] = pd.to_numeric(df.bare_nucleoli)\n\n# Initialize SimpleImputer\n\nimputer = SimpleImputer(missing_values = np.nan, strategy = \"median\")\n\n# Reshape the imputer input \n\nimp_input = df.bare_nucleoli.to_numpy().reshape(-1,1)\n\n# Fit and transform imputer\n\nimputer.fit(imp_input)\nimp_input_transformed = imputer.transform(imp_input)\n\n# Save the imputer output into the dataframe column and convert to integer datatype\n\ndf[\"bare_nucleoli\"] = imp_input_transformed.astype(int)\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Having our clean data, we collect further insight through descriptive statistics and Exploratory Data Analysis. Descriptive statistics provides valuable information about the range of values within a feature, and how similar/distant these features are. These insights are useful for model selection, however we still need visual insight."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our next step in the process is EDA. We will see how the data is distributed among categories through a heatmap from the feature correlation matrix, and bee-swarm plots to further explore our data."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns \nimport matplotlib.pyplot as plt\n\n# Define target \n\ny = df[\"class\"]\n\n# compute corelation matrix\n\ncorr = df.corr()\n\n# display heatmap from correlation matrix\n\nsns.heatmap(corr,cmap=\"Blues\",  annot=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the heatmat from the correlation matrix, we observe features that are highly correlated. We want to eliminate redundant features, and also to extract features from the data. Having a ratio of shape uniformity and size uniformity extracts data from both features, so instead of eliminating either one, we extract the ratio between both. This renders size uniformity and shape uniformity as redundant, so they are removed."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract ratio between shape uniformity and size uniformity\n\ndf[\"shape_size_uniformity\"] = df[\"shape_uniformity\"]/df[\"size_uniformity\"]\n\n# Drop redundant features\n\ndf.drop([\"shape_uniformity\", \"size_uniformity\"], axis = 1, inplace = True)\n\n# Compute new correlation matrix\n\ncorr = df.corr()\n\n# Plot heatmap from the correlation matrix\n\nsns.heatmap(corr, cmap = \"Blues\", annot = True)\n\n# Check the distribution of the newly created feature in a bee-swarm plot\n\nplt.figure()\nsns.swarmplot(x = df[\"class\"], y = df[\"shape_size_uniformity\"], data = df )\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"An important fact to consider is that according to the Wisconsin Breast Cancer Dataset, the \"class\" column shows 2 for a benign tumor and 4 for a malignant tumor. These categories are replaced by \"Benign\", and \"Malignant\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Prepare features and target for the model\n\ny = df[\"class\"].replace({2:0, 4:1})\nX = df.drop([\"class\"], axis = 1)\n\n# Generate train and test data\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\n\n# Instantiate Random Forest Classifier\n\nrfc = RandomForestClassifier(n_estimators = 100, max_depth = 4, random_state = 42)\n\n# Fit model into training data\n\nrfc.fit(X_train,y_train)\n\n# Predict test data\n\ny_pred_train = rfc.predict(X_train)\ny_pred_test = rfc.predict(X_test)\n\n# Compute scores predicted train and test data\n\naccuracy_train = accuracy_score(y_train, y_pred_train)\naccuracy_test = accuracy_score(y_test, y_pred_test)\n\n# Compute confusion matrix\n\nconf_matrix_rf = confusion_matrix(y_test,y_pred_test)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our Random Forest Classifier is evaluated through accuracy and confusion matrix.\nAccuracy score computes the ratio of correct predictions with the total, while the confusion matrix displays the results as a 2 x 2 matrix where:\n\n                predicted no | predicted yes\n    actual no   _____TN______|______FP______|              TN = True Negative      FP = False Positive       \n    actual yes  _____FN______|______TP______|              FN = False Negative     TP = True Positive\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"##### Display results\n\nprint(\"The accuracy score of the Random Forest Classifier on train data is {:.2f}\".format(accuracy_train))\nprint(\"The accuracy score of the Random Forest Classifier on test data is {:.2f}\".format(accuracy_test))\nprint(\"\")\nprint(\"Confusion matrix:\")\nprint(\"\")\nprint(conf_matrix_rf)\n\n# Compute a panda series of feature importances\n\nimportances = pd.Series(data=rfc.feature_importances_, index= X_train.columns)\n\n# Sort importances\n\nimportances_sorted = importances.sort_values()\n\n# Plot a horizontal bar plot of the feature importances\n\nimportances_sorted.plot(kind = \"barh\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The confusion matrix shows that there are 7 false negatives and 5 false positives. It is the first one we want to take care of, since this case calls for minimizing false negatives. We do not want to tell patients they do not have cancer when they actually do. This model still has not been tuned. Next step, we use scikit-learn's GridSearchCV for finding the best parameters for our Random Forest Classifier out of a neighborhood of chosen parameters and values."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# Establish parameters for GridSearchCV, as a dictionary\n\nrfc_params = {\"n_estimators\":[50, 70, 90, 100, 110, 130, 140], \n              \"max_features\":[\"log2\", \"sqrt\", \"auto\"],\n              \"min_samples_leaf\":[2, 4, 8, 10]}\n\n# Obtain the model with the optimal hyperparameters found in GridSearchCV\n\nrfc_gscv = GridSearchCV(estimator = rfc,\n                       param_grid = rfc_params,\n                       cv = 5,\n                       scoring = \"accuracy\",\n                       verbose = 2,\n                       n_jobs = -1)\n\n# Fit model found with GridSearchCV on train data \n\nrfc_gscv.fit(X_train,y_train)\n\n# show results\nprint(\"Best parameters for Grid Search CV Random Forest Classifier model:\")\nprint(rfc_gscv.best_params_)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Results from Grid Search Cross Validation method gave the best parameters for the Random Forest Classifier. \nWith the new confusion matrix, two false negatives and four false positives were found. This is positive, considering the confusion matrix from the previous model. We must approach this particular problem with a degree of sensitivity, since we do not want to diagnose a patient with a benign tumor, where they actually have a malignant tumor.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute predictions from new model\n\ny_pred = rfc_gscv.predict(X_test)\n\n# Compute confusion matrix from test data and prediction\n\nconf_matrix_rfc_gscv = confusion_matrix(y_test, y_pred)\n\n# show results\n\nprint(conf_matrix_rfc_gscv)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}