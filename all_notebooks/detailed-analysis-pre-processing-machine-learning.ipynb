{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport plotly.figure_factory as ff\nimport category_encoders as ce\nfrom sklearn.preprocessing import LabelEncoder\nimport copy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn import svm\nfrom sklearn import metrics\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import BaggingRegressor\nimport statsmodels.api as sm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train=pd.read_csv('../input/big-mart-sales-prediction/Train.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### CHAPTER 2 Explore the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_modified(df,pred=None):\n    obs=df.shape[0] # return the number of rows\n    types=df.dtypes #return the type of data\n    counts=df.apply(lambda x:x.count()) #store the number of not null values for eac column\n    nulls=df.apply(lambda x:x.isnull().sum())#store the total number of nulls for each column\n    distincts=df.apply(lambda x:x.unique().shape[0])#sotre the unique memeber of each column\n    missing_ratio=round(df.isnull().sum()/obs*100,2) \n    skewness=round(df.skew(),2)\n    kurtosis=round(df.kurt(),2)\n    if pred is None:\n        cols=['types','counts','nulls','distincts','missing_ratio','skewness','kurtosis']\n        result=pd.concat([types,counts,nulls,distincts,missing_ratio,skewness,kurtosis],axis=1)\n    else:\n        corr=round(df.corr()[pred],2) #computing correlation between each column and SalePrice\n        corr_name='corr '+pred\n        result=pd.concat([types,counts,nulls,distincts,missing_ratio,skewness,kurtosis,corr],axis=1)\n        cols=['types','counts','nulls','distincts','missing_ratio','skewness','kurtosis',corr_name]\n    result.columns=cols\n    result=result.sort_values(by=corr_name,ascending=False)\n    result=result.reset_index()\n    return result\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_modified(train,pred='Item_Outlet_Sales')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Except for item visibility, none of the features are deemed to be heavily asymmetrical since thier values are below 1 or -1. Item visibility is the one we should be cautious of managing it where we have solid evidence to conclude that its distribution is rightly skewed with outliers appearing.  "},{"metadata":{},"cell_type":"markdown","source":"### Chapter 3 Transforming Categorical Variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"object_column_number=[i for i,j in enumerate(train.dtypes) if j=='object']\nobject_columns=train.iloc[:,object_column_number]\n#drop item_idtentifier which does not help in our predicting model\nobject_columns=object_columns.drop(['Item_Identifier','Outlet_Identifier'],axis=1)\nobject_columns_labels=object_columns.columns\ntable=[[object_columns_labels[i],list(object_columns.iloc[:,i].unique())] for i in range(5)]\ntable.insert(0,['columns','members'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prepare a table containing columns and their members\nresult=ff.create_table(table)\nresult.layout.annotations[5].font.size=10\nresult.layout.annotations[11].font.size=10\nresult.layout.update(width=1700)\nresult.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Chapter 3 Filling Null Values \n\nWe have checked that two features have missing observatiosn whose missing ratio is quite high. \nOutlet_Size is a categorical data so that we can easily predict the missing values with machine learning algorithms for classification. Since data has a low number of dimension of faetures, I will use the most simple but powerful model,K-nearest neighbors. Even though some may argue that there is too much cost complexity associated with the higher dimensions, it does not assume anything about the data, this\nfeature making the model itesel most prominent among others. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data=train.copy()\ndata.pivot_table('Item_Outlet_Sales',index='Outlet_Size',columns='Outlet_Location_Type',aggfunc='count',margins=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the table above, we have seen that\n\n- The medium or big sized outlets are located in tier 3\n- Only small shops are avaialbe in tier 2\n- Either Small or Medium are operating but none of large shops are operating in tier 1\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.pivot_table('Item_Outlet_Sales',index='Outlet_Type',columns='Outlet_Location_Type',aggfunc='count',margins=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compare two tables and there are couple of facts we could find are\n\n- In the district of tier 1, Only either small or medium sized shops are currently running and alos belong to one of two kinds, grocery store and supermarket Type . \n\n\n- All the stores in the distict 2 belong to the supermarket type 2. We have 1855 rows left with unknown size.\n\n- It is the only in the tier 3 where consumer can visit all business types.With a simple calculation, we could understand that the      \n  grocery stores are currently not assigned thier size.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby(['Outlet_Location_Type','Outlet_Type'])['Outlet_Size'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.1 Outlet_Size\n#### 3.1.1 Grocery store in Tier 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"display(data.pivot_table(['Item_Outlet_Sales','Item_MRP'],index=[data.Outlet_Type,data.Outlet_Location_Type],aggfunc='mean'))\n#to find out the unique member of business belong to Grocery Store currently operating in tier 1\ndata[(data.Outlet_Location_Type=='Tier 1')&(data.Outlet_Type=='Grocery Store')].loc[:,'Outlet_Size'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems to be a safeguard for us to assume that all the grocery stores currently operating in district 3 is a small size one.  This is because we have only one comparison to estimate the size of them."},{"metadata":{"trusted":true},"cell_type":"code","source":"index=data[(data.Outlet_Size.isnull())&(data.Outlet_Type=='Grocery Store')].loc[:,'Outlet_Size'].index\n#assign 'small'\ndata.loc[index,'Outlet_Size']='Small'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.1.2 Missing Values in Tier 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"index=data[data.Outlet_Size.isnull()].loc[:,'Outlet_Size'].index\ndata.loc[index,'Outlet_Size']='Small'\ndata.isnull().any()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2 Outlet_Size\n\nOutlet_Size is a continuous varialbe and we should take a somehow different approach to takle this problem from the previous case. This is a regression task and the model we will rely on is the forest regrssion, the most widespread model in this century. \n\n### 3.2.1 Establish the base line\n\nBefore actually making and evaluating predictions, it is always a good practice to establish a baseline, a sensible measure we could beat with our predicting model. If our model can not improve upon the baseline, we must turn down our model and try a different model or conclude that the model is not suitable for our problem. The base line I will adopt is simply the avearage value of Item_Weight.\n\nSteps to set up the base line\n\n1. Generate the random indicies of original data without missing values\n2. Store the real value in the separate object named as _testvalue_\n3. Assign the average value to the feature in the random incidies and store the values in the objected _predictVAlue_\n4. Calculate the error rate by subtracting the test_value from predict_value (Make sure you put an absoulte value to each error)\n5. Calcaute the average of error rates"},{"metadata":{"trusted":true},"cell_type":"code","source":"base_data=data.drop(['Item_Identifier','Outlet_Identifier',],axis=1)\nbase_columns=base_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Chosee every row with Item_Weight having some value \nbase_data=base_data[base_data.Item_Weight.isnull()==False]\npredict_value=base_data.Item_Weight.mean()\n#Generate random indicies\nrandom_rows=np.random.choice(base_data.index,np.int(base_data.index.shape[0]*0.25))\n#Store up the true value of Item_Weight\ntest_value=base_data.loc[random_rows,'Item_Weight']\nerror=test_value.map(lambda x:np.abs(x-predict_value))\nprint('Average Baseline Error:{0} degrees'.format(round(np.mean(error),2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have a guide to compare! If we can not achieve at least below the degress, then we need to rethink our approach"},{"metadata":{},"cell_type":"markdown","source":"###  3.2.2 Random Forest\n\nIn prediction of targe values with the radom forest, the most critical choice we should bear in mind is how many decision trees we need to\ntake in our model.From many text books, it is strongly recommended that over 2000 decision trees and the minimum number of samples required to splits an internal node is number of features/3 for regression model in order to meet the strong law of large number. "},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndata.Item_Fat_Content=data.Item_Fat_Content.map(lambda x: 'low fat' if x=='Low Fat'and 'LF'and 'low fat' else 'regular')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# mapping Item_Fat_Content to either 1 or 2\nce_ord=ce.OrdinalEncoder()\ndata.Item_Fat_Content=ce_ord.fit_transform(data.Item_Fat_Content)\n\n#maping Item_Type to ordinal category from 1 to 16\nce_ord=ce.OrdinalEncoder()\ndata.Item_Type=ce_ord.fit_transform(data.Item_Type)\n\n#Outlet location\n\ndata.Outlet_Location_Type=data.Outlet_Location_Type.map(lambda x:x[-1]).astype(int)\n\n#Outlet Type\nce_ord=ce.OrdinalEncoder()\ndata.Outlet_Type=ce_ord.fit_transform(data.Outlet_Type)\n\n#Outlet Size\ndata.Outlet_Size=data.Outlet_Size.map(lambda x: 0 if x=='Small' else 1 if x=='Medium' else  2 if x=='High' else x )\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_null_free=data[data.Item_Weight.isnull()==False]\ndata_null=data[data.Item_Weight.isnull()==True]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=data_null_free.drop(['Item_Identifier','Item_Weight','Outlet_Identifier'],axis=1)\ny=data_null_free['Item_Weight']\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25)\n#Instantiate model with 2000 decision trees \nrf=RandomForestRegressor(n_estimators=2000,random_state=42,min_samples_split=3)\n#train the model on training data\nrf.fit(X_train,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now,our model has been trained to learn the relations between the features and the targets. The next step is to find out how accruate our model is! "},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions=rf.predict(X_test)\nerrors=abs(predictions-y_test)\nprint('Mean Absoulte Error:{0} degrees'.format(round(np.mean(errors),2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our average estimate is off by about 1 degree. That is more than a 1 degree improvement over the baseline. To put our predictions in perspective, we can calucate the accuracy using the mean average error subtracted from 100 %."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculate mean absoulte the percentage error\nerror_percent=100*errors/y_test\naccuracy=100-np.mean(error_percent)\nprint(\"Accuracy: {0}%\".format(round(accuracy,2)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importance=list(rf.feature_importances_)\nfeatures=X.columns\nfeatures_importances=[(features,round(importances*100,2)) for features,importances in zip(features,importance)]\nfeatures_importances=sorted(features_importances,key=lambda x:x[1],reverse=True)\n[print('Variable: {:30} Importance: {}'.format(*pair)) for pair in features_importances];","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\nfig,ax=plt.subplots(figsize=(11,5))\nplt.bar(features,importance,alpha=0.7,color='coral')\nax.set_xticklabels(features,rotation=45)\nax.set_xlabel('features',fontsize=15)\nax.set_ylabel('importance(percentage)',fontsize=15)\nax.set_title('Features Importances',fontsize=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_null=data_null.drop(['Item_Identifier','Item_Weight','Outlet_Identifier'],axis=1)\n#store the indicies with missing values\ndata_null_index=data_null.index\n#predict the values with our random forest model\npredict=rf.predict(data_null)\n#assign them to the null values \ndata.iloc[data_null_index,1]=predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# double check wether we have removed all the missing values in every feature\ndata.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assigin the finding results to original data(train)\ntrain.Outlet_Size=data.Outlet_Size\ntrain.Outlet_Size=train.Outlet_Size.map(lambda x: 'small' if x==0 else 'medium' if x==1 else 'high')\ntrain.Outlet_Size=train.Outlet_Size.astype('category')\ntrain.Item_Weight=data.Item_Weight","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Chapter 4 Detailed Investigation on Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding out the correlation with Item_Outlet_Sales\nresult=data.corr()['Item_Outlet_Sales']\nprint('Correlation with Item_Outlet_Sales')\nprint('-'*100)\n#Sort the results in a descedning order\nresult=result.sort_values(ascending=False)\ndisplay(result)\nresult_columns=result.index\ndata=data.loc[:,result_columns]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.1 MRP \n\nWe are not given any information of what MRP stands for. However, one thing for sure is that there is an incresing trend of Sales as MRP rises. I wish the description of it were provied for the better analysis. One outstanding feature we could curiously see is that in some intervals observations are absent, thereby being grouped according to the specific range of MRP.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\nfig,axes=plt.subplots(figsize=(15,10))\nsns.scatterplot(x=data.Item_MRP,y=data.iloc[:,0],ax=axes,hue=data.Outlet_Size,palette='Spectral')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2 Item_Weight and Item_Visibility\n\nNormally, a lower correlation with the target variable is not into our consideration and we should drop the features before constructing a predicting model. However, before permanently discarding them, I always combine two varaibles to see if we can get some benefits from this interaction. \n\nI guess a sound guess that the heavier items become, the greater visible they are to consumers. With common sense, the heavier items should have a bigger size proportionally. Let's take some experiments in combing two varialbes by either multiplication or division."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax=plt.subplots(3,1,figsize=(15,20))\nweight_vis=data.Item_Weight*data.Item_Visibility\nweight_by_vis=data.Item_Weight/data.Item_Visibility\nsns.scatterplot(x=data.Item_Visibility,y=data.iloc[:,0],ax=ax[0],hue=data.Outlet_Location_Type,palette='Spectral')\nax[0].set_title('Correaltion with Sales: {0}'.format(data.Item_Outlet_Sales.corr(data.Item_Visibility)),fontsize=14)\nsns.scatterplot(x=weight_vis,y=data.Item_Outlet_Sales,hue=data.Outlet_Size,palette='Spectral',ax=ax[1])\nax[1].set_title('Correaltion with Sales: {0}'.format(data.Item_Outlet_Sales.corr(weight_vis)),fontsize=14)\nsns.scatterplot(x=weight_by_vis,y=data.iloc[:,0],hue=data.Outlet_Size,palette='Spectral',ax=ax[2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The interaction between the two features does not help to improve the correlation. But, the scatter plots give some clues of which points are to be removed. In fact, this has correctly identified outliers a single feature failed to detect."},{"metadata":{},"cell_type":"markdown","source":"### Removing Outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove the outliers\ndata=data[data.Item_Weight*data.Item_Visibility<4.7]\ndata=data[data.Item_Weight/data.Item_Visibility<2500]\ndata=data[data.Item_Outlet_Sales<12000]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3 Item Weight and Fat content\n\nTake another interesting experiment before building a predicting model. Why not we think there is a close relation between Item weight and fat content?  I have done the same procedure I did in the previous chapter. \nThe result is still disappointing even though a slight improvement over the correlation exists.  We are not seeing any outliers since we have removed a majority of them in the previous steps. "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax=plt.subplots(3,1,figsize=(15,20))\nweight_fat=data.Item_Weight*data.Item_Fat_Content\nweight_by_fat=data.Item_Weight/data.Item_Fat_Content\nsns.scatterplot(x=data.Item_Weight,y=data.iloc[:,0],ax=ax[0],hue=data.Outlet_Location_Type,palette='Spectral')\nax[0].set_title('Correlation between Item_Weight and Sale revenue',fontsize=20)\nax[0].text(x=4,y=11000,s='Correaltion:{0}'.format(data.Item_Outlet_Sales.corr(data.Item_Weight)),fontsize=14)\nsns.scatterplot(x=weight_fat,y=data.iloc[:,0],ax=ax[1],hue=data.Outlet_Location_Type,palette='Spectral')\nax[1].set_title('Correlation between Weight_fat and Sale revenue',fontsize=20)\nax[1].text(x=4,y=11000,s='Correaltion:{0}'.format(data.Item_Outlet_Sales.corr(weight_fat)),fontsize=14)\ndata.Item_Outlet_Sales.corr(weight_vis)\nsns.scatterplot(x=weight_by_fat,y=data.iloc[:,0],ax=ax[2],hue=data.Outlet_Location_Type,palette='Spectral')\nax[2].set_title('Correlation between Weight_by_fat and Sale revenue',fontsize=20)\nax[2].text(x=2.5,y=11000,s='Correaltion:{0}'.format(data.Item_Outlet_Sales.corr(weight_by_fat)),fontsize=14)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4 Other Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,axes=plt.subplots(2,2,figsize=(15,12))\nsns.boxplot(x='Outlet_Establishment_Year',y='Item_Outlet_Sales',ax=axes[0,0],data=data)\nsns.boxplot(x='Outlet_Size',y='Item_Outlet_Sales',ax=axes[0,1],data=data)\nsns.boxplot(x='Outlet_Location_Type',y='Item_Outlet_Sales',ax=axes[1,0],data=data)\nsns.boxplot(x='Outlet_Type',y='Item_Outlet_Sales',ax=axes[1,1],data=data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4 Conclusion\nTwo continuous columns demonstrate a much lower correlation with the target variable than what is normally accepted. Therefore, I decied to drop these two continuous features. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data=data.drop(['Item_Weight','Item_Visibility'],axis=1)\ny=data.Item_Outlet_Sales\nX=data.iloc[:,1:]\nX.iloc[:,1:6]=X.iloc[:,1:6].astype('category')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Constructing Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparing the data sets\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.1 Simple Linear Regression (Base Line)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train the model \nsl=LinearRegression()\nsl.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predict the taraget variable based on X_test\npredict_sl=sl.predict(X_test)\n#Calculate Mean Squared Error\nmse=np.mean((predict_sl-y_test)**2)\n#Score\nsl_score=np.sqrt(mse)\nprint('Score of Simple regrssion model : {0}'.format(sl_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2 Rigid Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"r=Ridge(alpha=0.5,solver='cholesky')\nr.fit(X_train,y_train)\npredict_r=r.predict(X_test)\nmse=np.mean((predict_r-y_test)**2)\nr_score=np.sqrt(mse)\nr_score\nprint('Score of Rigid Regression : {0}'.format(sl_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.3 Lasso"},{"metadata":{"trusted":true},"cell_type":"code","source":"l=Lasso(alpha=0.01)\nl.fit(X_train,y_train)\npredict_r=r.predict(X_test)\nmse=np.mean((predict_r-y_test)**2)\nl_score=np.sqrt(mse)\nl_score\nprint('Score of Lasso : {0}'.format(l_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.4 Elastic Net"},{"metadata":{"trusted":true},"cell_type":"code","source":"en=ElasticNet(alpha=0.01)\nen.fit(X_train,y_train)\npredict_r=en.predict(X_test)\nmse=np.mean((predict_r-y_test)**2)\nl_score=np.sqrt(mse)\nl_score\nprint('Score of Elastic Net: {0}'.format(l_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.5 Support Vector machine"},{"metadata":{"trusted":true},"cell_type":"code","source":"svm=SVR(epsilon=15,kernel='linear')\nsvm.fit(X_train,y_train)\npredict_r=svm.predict(X_test)\nmse=np.mean((predict_r-y_test)**2)\nl_score=np.sqrt(mse)\nl_score\nprint('Score of Support Vector machine: {0}'.format(l_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.6 Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"dtr=DecisionTreeRegressor()\ndtr.fit(X_train,y_train)\npredict_r=dtr.predict(X_test)\nmse=np.mean((predict_r-y_test)**2)\nl_score=np.sqrt(mse)\nl_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.7 Comments \n\nNo alternative models could beat the base model(multi regression model). Therefore,we will look at model to see if the further evelation steps are needed. Unfortunately, the adjusted R-squareds is really lower than what I expcet it to be. Thefore, I close my remark saying \nthat the data iteself is not sutiable for predicing the Item_Outlet Sales"},{"metadata":{"trusted":true},"cell_type":"code","source":"y=train.Item_Outlet_Sales\nX=train.iloc[:,[2,4,5,7,8,9,10]]\ncolumns=['Item_MRP',\n         'Item_Fat_Content',\n         'Item_Type',\n         'Outlet_Size',\n         'Outlet_Location_Type',\n         'Outlet_Type',\n        'Outlet_Establishment_Year']\n#rearrange the columns\nX=pd.DataFrame(X,columns=columns)\n#All the object varibles are converted into categories one\nX.iloc[:,1:]=X.iloc[:,1:7].astype('category')\nX=pd.get_dummies(X)\n#Create a OLS model\nmodel=sm.OLS(y,X)\nresults=model.fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}