{"cells":[{"metadata":{"id":"YCazTF5g_Wnq"},"cell_type":"markdown","source":"# Predict breast cancer by classification model - K-NearestNeighbors\n[Breast Cancer Wisconsin (Diagnostic) Dast Set on Kaggle](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data)","execution_count":null},{"metadata":{"id":"ejdizNJD_Wnr"},"cell_type":"markdown","source":"## 1. Import data for analysis","execution_count":null},{"metadata":{"id":"GIF2XTqg_Wnr","outputId":"ab43cac4-39e9-41e5-9ceb-7c7a82212caa","trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\n\nos.chdir('/kaggle/input')\nos.getcwd()","execution_count":null,"outputs":[]},{"metadata":{"id":"wpprI5Qs_Wnx","outputId":"b03a382f-b22c-4787-bc52-99b170e2e237","trusted":true},"cell_type":"code","source":"df=pd.read_csv('breast-cancer-wisconsin-data/data.csv')\n#df=pd.read_csv(\"../input/breast-cancer-wisconsin-data/data.csv\")\n#df.head()\n#df.columns\n#df.shape #569*33\ndf.info()  #no missing value ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Dataset information:**\nFeatures are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\n\n* Dataset Characteristics: Multivariate\n* Attribute Characteristics: Real\n* Attribute Characteristics: Classification\n* Number of Instances: 569\n* Number of Attributes: 33 \n* Missing Values: No","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Attribute Information**\n* id: ID number\n* diagnosis: The diagnosis of breast tissues (M = malignant, B = benign)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('diagnosis').size() #Diagnosis class distribution: 357 benign, 212 malignant","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature names and meanings (4dp)**\n* radius_mean: mean of distances from center to points on the perimeter\n* texture_mean: standard deviation of gray-scale values\n* perimeter_mean: mean size of the core tumor\n* area_mean: area of the tumor\n* smoothness_mean: mean of local variation in radius lengths\n* compactness_mean: mean of perimeter^2 / area - 1.0\n* concavity_mean: mean of severity of concave portions of the contour\n* concave_points_mean: mean for number of concave portions of the contour\n* symmetry_mean\n* fractal_dimension_mean: mean for \"coastline approximation\" - 1\n* radius_se: standard error for the mean of distances from center to points on the perimeter\n* texture_se: standard error for standard deviation of gray-scale values\n* perimeter_se\n* area_se\n* smoothness_se: standard error for local variation in radius lengths\n* compactness_se: standard error for perimeter^2 / area - 1.0\n* concavity_se: standard error for severity of concave portions of the contour\n* concave_points_se: standard error for number of concave portions of the contour\n* symmetry_se\n* fractal_dimension_se: standard error for \"coastline approximation\" - 1\n* radius_worst: \"worst\" or largest mean value for mean of distances from center to points on the perimeter\n* texture_worst: \"worst\" or largest mean value for standard deviation of gray-scale values\n* perimeter_worst\n* area_worst\n* smoothness_worst: \"worst\" or largest mean value for local variation in radius lengths\n* compactness_worst: \"worst\" or largest mean value for perimeter^2 / area - 1.0\n* concavity_worst: \"worst\" or largest mean value for severity of concave portions of the contour\n* concave_points_worst: \"worst\" or largest mean value for number of concave portions of the contour\n* symmetry_worst\n* fractal_dimension_worst: \"worst\" or largest mean value for \"coastline approximation\" - 1","execution_count":null},{"metadata":{"id":"HbQSF07z_Wn0"},"cell_type":"markdown","source":"## 2. Data Cleaning & Wrangling (EDA)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 2.1 Drop unnecessary columns\nGet rid of \"id\" and \"Unnamed: 32\" features since they are irrelevant to diagnose breast cancer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop([\"Unnamed: 32\",\"id\"],axis=1,inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.2 Descriptive Analysis\n**check decriptive statistics for features**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot outcome variable to see whether suitable for KNN Algorithm or not\nM = df[df.diagnosis == \"M\"]\nB = df[df.diagnosis == \"B\"]\n\nimport matplotlib.pyplot as plt\nplt.title(\"Malignant vs Benign Tumor\")\nplt.xlabel(\"Radius Mean\")\nplt.ylabel(\"Texture Mean\")\nplt.scatter(M.radius_mean, M.texture_mean, color = \"tomato\", label = \"Malignant\", alpha = 0.3)\nplt.scatter(B.radius_mean, B.texture_mean, color = \"olivedrab\", label = \"Benign\", alpha = 0.3)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Convert the diagnosis label from M and B to a dummy variable**\n* M (Malignant) = 1\n* B (Benign) = 0\n","execution_count":null},{"metadata":{"id":"VjX6K-E3_Wn1","trusted":true},"cell_type":"code","source":"df['diagnosis']=np.where(df['diagnosis']=='M',1,0)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df.info() #'diagnosis' has changed to int64\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"xLk7i1v3_Wn7"},"cell_type":"markdown","source":"**Check relationship between features and outcome variable**\n* **1.scatter plot**\n* According to the plots on the first row, M and B observations are clearly seperated out in terms of these features, suggesting these features are good predictors that we should put into the model later on.","execution_count":null},{"metadata":{"id":"Spkz3F-D_Wn7","outputId":"e585c92a-e439-4a7d-bab6-e08dede37eea","trusted":true},"cell_type":"code","source":"#df_plot=df[['radius_mean','texture_mean','perimeter_mean','area_mean','smoothness_mean','compactness_mean','diagnosis']]\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"whitegrid\")\ng=sns.PairGrid(df,hue='diagnosis')  \ng.map_offdiag(plt.scatter)  \ng.add_legend()\nplt.show()  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **2.correlation of coefficients**\n* First row suggests moderate or strong relationship between diagnosis label and other features","execution_count":null},{"metadata":{"id":"l8knhOC8_Wn-","outputId":"1b44e5c7-de60-4ef8-9241-2563b8477db0","trusted":true},"cell_type":"code","source":"#df.corr()\n#plot correlation heatmap\nplt.figure(figsize=(25, 12))\nsns.heatmap(df.corr(), vmin = -1, vmax = 1, center = 0, cmap = 'coolwarm', annot = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"4GljUOwe_WoB"},"cell_type":"markdown","source":"## 3. Build model - KNN model from Scikit-learn ","execution_count":null},{"metadata":{"id":"4hJspKT0_Wnq"},"cell_type":"markdown","source":"### Meaning of KNN Algorithm\n* Classify the label of a data point by looking at the 'K' nearest labeled data points\n* Taking the majority votes\n* :) high accuracy, insensitive to outliers\n* :( computationally heavy\n* only one parameter: n_neighors - 'K'\n* Train the model(Training dataset): .fit()    \n* Predict of new data (Testing dataset): .predict()","execution_count":null},{"metadata":{"id":"b64Xvw6W_WoC","outputId":"673ec4db-a5e8-40fd-d4f0-08764f873a37","trusted":true},"cell_type":"code","source":"#split the data into features X and label Y \n#df.info()\nX=df.iloc[:,1:30]\nY=df.iloc[:,0]","execution_count":null,"outputs":[]},{"metadata":{"id":"-WdkHXss_WoH"},"cell_type":"markdown","source":"### 3.1 Manually select 3 records as test dataset ","execution_count":null},{"metadata":{"id":"Duc3V-CE_WoI","outputId":"f36a7c58-3727-4756-ab2f-cbf4cd69e1e5","trusted":true},"cell_type":"code","source":"#select 3 data for prediction \nX_new=X.iloc[200:203]  \nY_new=Y.iloc[200:203]\n\n#KNN \nfrom sklearn.neighbors import KNeighborsClassifier\nknn1 = KNeighborsClassifier(n_neighbors=3)\nknn1.fit(X,Y) \nY_predict1=knn1.predict(X_new) \n\nprint('Prediction Result:{}'.format(Y_predict1))\nprint('Actual Result:{}'.format(Y_new))","execution_count":null,"outputs":[]},{"metadata":{"id":"xa1iAB5t_WoK"},"cell_type":"markdown","source":"### 3.2 Train/Test Split and Performance Metrics \n* *train_test_split*: helps to split the data for training and testing\n* Default Performance Metrics in Scikit-learn for KNN: **accuracy** \n* **accuracy=correct prediction/total no. of prediction**","execution_count":null},{"metadata":{"id":"48NVqx9J_WoL","outputId":"f4897740-8108-46d8-e31e-5bc5d0a4df3e","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split \nX_train, X_test, Y_train, Y_test=train_test_split(X,Y,test_size=0.2,random_state=1)  \n\nknn2=KNeighborsClassifier(n_neighbors=3)\nknn2.fit(X_train,Y_train)\n\nY_predict2=knn2.predict(X_test)\nprint(\"Test set predictions:{}\".format(Y_predict2))","execution_count":null,"outputs":[]},{"metadata":{"id":"PQab6ZSE_WoO","outputId":"98badf83-1c53-4eff-bf78-bbcb728a8742","trusted":true},"cell_type":"code","source":"#Evaluation: accuracy\nknn2.score(X_test, Y_test) ","execution_count":null,"outputs":[]},{"metadata":{"id":"Z2ZGIrAB_WoQ"},"cell_type":"markdown","source":"### 3.3 K-Fold Cross_Validation\n* Model performance is dependent on way the data is split\n* Not representative of the model's ability to generalize 用test_size去分出test set\n* **Solution: Cross-validation** \n* cv= no. of groups that a given data sample is to be split into\n* :) Reflect the true performance of a model\n* :( more folds, more computationally expensive","execution_count":null},{"metadata":{"id":"-YgHMPzp_WoR","outputId":"0b5cda36-b738-438d-95db-c124bbf94f90","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nknn=KNeighborsClassifier(n_neighbors=3)\ncv_results=cross_val_score(knn, X, Y, cv=5)  \nprint(cv_results)","execution_count":null,"outputs":[]},{"metadata":{"id":"KyQiEEY7_WoT","outputId":"1de3f5bd-5607-4aea-b918-573054e9f253","trusted":true},"cell_type":"code","source":"#Evaluation:The average accuracy rate of 5 test-train groups\nprint(\"The average accuracy rate is:{}\".format(np.mean(cv_results)))","execution_count":null,"outputs":[]},{"metadata":{"id":"9UoNKCI7_WoW"},"cell_type":"markdown","source":"## 4. Hyperparameter Tuning: Find out the optimal k\n* k-Nearest Neighbors: choosing optimal n_neighbors\n* Hyperparameters cannot be learned by fitting the model\n* **Solution1: GridSearchCV**\n* **Solution2: RandomizedSearch**","execution_count":null},{"metadata":{"id":"H5XRQ3Sw_WoX","outputId":"2cb3410a-1368-4c55-8b5b-1a2a790893b0","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nparam_grid={'n_neighbors':np.arange(1,50)}   \nknn=KNeighborsClassifier()  \nknn_best_k=GridSearchCV(knn, param_grid, cv=5) \n\nknn_best_k.fit(X,Y)","execution_count":null,"outputs":[]},{"metadata":{"id":"NF4ZYcnx_Woa","outputId":"35160b9b-30fc-4cba-fde8-ed47c8ecf895","trusted":true},"cell_type":"code","source":"knn_best_k.best_params_\nprint(\"Best parameter:\",knn_best_k.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"id":"gpWZdqtu_Woc","outputId":"9277b7d3-cc19-41c8-8747-893607e7afbe","trusted":true},"cell_type":"code","source":"#the accuracy rate for the best k \nknn_best_k.best_score_\nprint(\"Best score:\",knn_best_k.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"id":"eguK7Rir_Wof"},"cell_type":"markdown","source":"## 5. Classification Performance Metrics - Another way to evaluate the model: Confusion Matrix\n* Acurracy is not always a useful metric\n* If 99% of cancer are Malignant; 1% of cancer are Bengign,could build a classifier that predicts ALL cancer as Malignant - 99% accurate!\n* :( But horrible at actually classifying Benign cancer\n* :( Fails at its original purpose\n* **Solution: Confusion Matrix**\n* F1score=2*(precision x recall)/(precision+recall)","execution_count":null},{"metadata":{"scrolled":true,"id":"peyEcyRc_Wof","outputId":"f207127d-a36b-434f-b963-a098b3c1b3e4","trusted":true},"cell_type":"code","source":"#cross validation with confusion matrix\nfrom sklearn.metrics import confusion_matrix \nknn=KNeighborsClassifier(n_neighbors=14) #use the best k we computed above\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2,random_state=1) \nknn.fit(X_train,Y_train)\nY_predict3=knn.predict(X_test)\nprint(confusion_matrix(Y_test,Y_predict3))","execution_count":null,"outputs":[]},{"metadata":{"id":"nJsB-ol5_Wok","outputId":"5352ccb0-35ac-4281-8966-7b2b4ddac783","trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nknn=KNeighborsClassifier(n_neighbors=14)\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2,random_state=1) \nknn.fit(X_train,Y_train)\nY_predict3=knn.predict(X_test)\nprint(classification_report(Y_test,Y_predict3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Overall, with n_neighbors of 14, our KNN model gives the most accurate classifcation results.**","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}