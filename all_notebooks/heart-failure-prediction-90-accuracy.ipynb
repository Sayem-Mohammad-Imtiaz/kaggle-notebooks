{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/heart-disease/heart.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We can see using .head() that all features are numeric","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using .info() we see that none of the columns contain NaN values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can create a heatmap comparing the correlation bewteen all pairwise features since they are all numeric","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df.corr())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering and Model Building","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Before we apply scaling to some of the numeric features, we create a RandomForest model using the bagging technique. We split the data we have into 80% train and 20% test, and further split train into 80% train and 20% validation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.loc[:, df.columns != 'target']\ny = df.loc[:, 'target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n\nmodel1 = RandomForestRegressor()\nbags = 10\nseed = 1\nbagged_prediction = np.zeros(X_val.shape[0])\nfor n in range(0, bags):\n    model1.set_params(random_state = seed + n) \n    model1.fit(X_train, y_train)\n    preds1 = model1.predict(X_val)\n    bagged_prediction += preds1\nbagged_prediction /= bags\ntest_preds1 = model1.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we take replace some features that showed skewness in the pairplot above with its natural log","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['log_cp'] = np.log(df['cp'] + 1)\ndf['log_ca'] = np.log(df['ca'] + 1)\ndf['log_oldpeak'] = np.log(df['oldpeak'] + 1)\ndf['log_trestbps'] = np.log(df['trestbps'] + 1)\ndf['log_thalach'] = np.log(df['thalach'] + 1)\ndf.drop('cp', axis=1, inplace=True)\ndf.drop('ca', axis=1, inplace=True)\ndf.drop('oldpeak', axis=1, inplace=True)\ndf.drop('trestbps', axis=1, inplace=True)\ndf.drop('thalach', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our second model uses Linear Regression with the scaled values. Then we stack our predictions from the two models and take their average to create a meta model which we will use to generate our final predictions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.loc[:, df.columns != 'target']\ny = df.loc[:, 'target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n\nmodel2 = LinearRegression().fit(X_train, y_train)\npreds2 = model2.predict(X_val)\ntest_preds2 = model2.predict(X_test)\n\nstacked_predictions = np.column_stack((preds1, preds2))\nstacked_test_predictions = np.column_stack((test_preds1, test_preds2))\nmeta_model = LinearRegression()\nmeta_model.fit(stacked_predictions, y_val)\nfinal_predictions = meta_model.predict(stacked_test_predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the final predictions are averaged, it will fall on a continuous scale. The target variables are either 0 or 1 so we apply a threshold of 0.5 to determine its value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_predictions = (final_predictions >= 0.5).astype(int)\nfinal_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score(final_predictions, y_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}