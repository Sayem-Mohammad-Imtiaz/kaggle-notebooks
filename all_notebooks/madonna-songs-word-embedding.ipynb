{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Creating Word Embeddings","metadata":{}},{"cell_type":"code","source":"# need to install\n#!pip install pyemd","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:14:19.706582Z","iopub.execute_input":"2021-06-05T14:14:19.706994Z","iopub.status.idle":"2021-06-05T14:14:19.716202Z","shell.execute_reply.started":"2021-06-05T14:14:19.70696Z","shell.execute_reply":"2021-06-05T14:14:19.714709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing all the needed libraries for this project\nimport pandas as pd\nimport numpy as np\nimport gensim\nfrom gensim.models import KeyedVectors","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:14:19.718317Z","iopub.execute_input":"2021-06-05T14:14:19.718789Z","iopub.status.idle":"2021-06-05T14:14:19.730996Z","shell.execute_reply.started":"2021-06-05T14:14:19.718741Z","shell.execute_reply":"2021-06-05T14:14:19.730023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"TASK 0: \n\n-load the 80s_detailed songs using pandas\n- create a data series that contains only the lyrics of the songs by Madonna","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/80s-songs/80s_detailed(1).csv', delimiter = ',')\ninputdf = df.loc[df['artist'] == \"Madonna\"]['lyrics']\ninputdf","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:14:19.755715Z","iopub.execute_input":"2021-06-05T14:14:19.756361Z","iopub.status.idle":"2021-06-05T14:14:19.833579Z","shell.execute_reply.started":"2021-06-05T14:14:19.756316Z","shell.execute_reply":"2021-06-05T14:14:19.832209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"TIPS:\n\n- when creating word embeddings, we basically do not remove any tokens from our data. This means that we keep stopwords\n- we word at token level, so \"house\" and \"houses\" will results in two embeddicng vectors (hopefully very similar)\n- we can create lemma embeddings, nothing prevents us from doing this  - it is simply not done in practice (since we need hundreds of million of textual data to create good embeddings)\n- in some cases, we can apply some pre-processing to avoid too much sparseness (i.e., too high variation in our data). For example, when working with Twitter data, you may want to subsitute all URLs with a placeholder (e.g., URL). The same for all user mentions (e.g., @GRONlp --> @USER).\n\nIn our case, we can relax and take everythin into account.\n\nAs for the Topic Modeling, we will use `gensim`. The only important thing that is needed is that we pre-process the text and split into tokens. We will use the function `simple_preprocess()` in `gensim`.\nThis allows us to prepare our data.\n\nComplete the code below:","metadata":{}},{"cell_type":"code","source":"# Checking how many songs we have:\nlen(inputdf)\n# Apply gensim.utils.simple_preprocess\nmadonna_songs_tokens = [gensim.utils.simple_preprocess(line) for line in inputdf]\n# print the song at index 3 in the pre-processed list\nprint(madonna_songs_tokens[3])","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:14:19.836246Z","iopub.execute_input":"2021-06-05T14:14:19.836602Z","iopub.status.idle":"2021-06-05T14:14:19.849619Z","shell.execute_reply.started":"2021-06-05T14:14:19.83657Z","shell.execute_reply":"2021-06-05T14:14:19.847972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(madonna_songs_tokens)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:14:19.85149Z","iopub.execute_input":"2021-06-05T14:14:19.851937Z","iopub.status.idle":"2021-06-05T14:14:19.862941Z","shell.execute_reply.started":"2021-06-05T14:14:19.851903Z","shell.execute_reply":"2021-06-05T14:14:19.861283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Time to create our word2vec model\nNow that we have our songs we are ready to create our word2vec model using `gensim`\nWe can create (and train) a word2vec model by using the line of code in the block below.\n\n*IMPORTANT PARAMETERS* \n\nChanging any of these parameters affect your word embeddings (and their quality). What do they mean?\n\n### window\nThe window parameter indicates what the maximum distance between the target word and its neighboring word can be. This means that specifying a higher window value, will result in a neigboring words that are less related to the target word. Vice versa, a lower window value, will result in words that are closer related to the traget word. Previous work has shown that a window between 2 and 5 gives very good results. Experiment with this value to see how it impacts the word embeddings.\n### min_count\nMinimium frequency count of words. The model would ignore words that do not satisfy the `min_count`. Extremely infrequent words are usually unimportant, so its best to get rid of those. With very small datasets/corpora (up to 10 million words or so - as a rule of thumb), you keep everything (i.e., `min_count=1`). With very large corpora (from more than 100 million words upward) you may want to use a larger minimal frequency (e.g, `min_count=10`). This parameters affects the coverage (i.e., how many tokens in a texts your word embedding can find), the memory usage, and storage requirements of the model files.\n### workers\nWorkers specifies the number of independent threads doing simultaneous training. `worker = 2` (or more), data will be trained in two parallel ways. By default, `worker = 1` i.e, no parallelization. A higher amount of workers can make the training faster, provided your machine can handle it. As for now, we keep this parameter with the default value (`worker = 1`).\n### sg\n`sg` has only 2 values 0 or 1. It specificies which approach to use when creating the word embeddings. 0 = continuous bag-of-words; 1 = skip-gram models","metadata":{}},{"cell_type":"code","source":"# Training our word2vec model\n# Note -> make sure that madonna_songs_tokens contains lists in which each message is in its own list.\n# i.e madonna_songs_tokens = [['this', 'is', 'a', 'message'], [...], ....]\nw2v_model = gensim.models.Word2Vec(\n        madonna_songs_tokens,\n        window=5,\n        min_count=1,\n        workers=1,\n        sg = 1)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:21:58.888264Z","iopub.execute_input":"2021-06-05T14:21:58.888599Z","iopub.status.idle":"2021-06-05T14:21:58.959854Z","shell.execute_reply.started":"2021-06-05T14:21:58.88857Z","shell.execute_reply":"2021-06-05T14:21:58.958925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## What does our word2vec model look like?","metadata":{}},{"cell_type":"code","source":"# printing the model\nprint(w2v_model)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:22:03.074006Z","iopub.execute_input":"2021-06-05T14:22:03.074448Z","iopub.status.idle":"2021-06-05T14:22:03.080959Z","shell.execute_reply.started":"2021-06-05T14:22:03.074414Z","shell.execute_reply":"2021-06-05T14:22:03.079551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before moving forward, let see some useful functions and best practices:\n\n- once we have trained our model, we should save it. Normally, this models may take very long time to train. To save the model use the function `.save()` - inside `.save()` you can specify the name of your model\n\n- load a saved model: this can be done using the function `.KeyedVectors.load()` and specifying the name of the model your are loading. If the model is stored in a different folder, specify also the full path to the model\n\n- print the vocabulary of the model (this is important because if a word is not in your vocabulary the model will raise an error)with `wv.vocab`\n\n- print the vector representation of a specific word (e.g. `home`) using the function `.wv['home']` ","metadata":{}},{"cell_type":"code","source":"# save the model\nw2v_model.save('madonna_50.model')\n\n# load the model\nmy_model = KeyedVectors.load(\"madonna_50.model\")\n               \n# Printing the first 20 words that in our vocabulary\nwords = list(my_model.wv.index_to_key)\nprint(words[0:20])\n\ntarget_vector = my_model.wv['home']\nprint(target_vector)\n# print the dimensions of the vector\nprint(len(target_vector))","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:25:41.168213Z","iopub.execute_input":"2021-06-05T14:25:41.168566Z","iopub.status.idle":"2021-06-05T14:25:41.191226Z","shell.execute_reply.started":"2021-06-05T14:25:41.168538Z","shell.execute_reply":"2021-06-05T14:25:41.189715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## What can we do with our word2vec model?\nAs said before, word embeddings are created by looking at the context in which a word occurs.\nThis means that the meaning of a word can be obtained by the company it keeps (or its usage).\n\nWord embeddings are then a tool to do automatic semantic analysis of your data/corpus.\n\nEasy things we can do:\n- find the top N most similar words of a target word (`.wv.most_similar(positive='string',  topn=int)`)\n- determine how similar two words are with respect to each other (`.wv.similarity('string1', 'string2`))\n- directly compute the similarity of 2 sentences using similarity of the words that compose them (`.wv.wmdistance(sentence_1, sentence_2)`)\n- find analogies (.wv.most_similar_cosmul(positive=['word1', 'word2'], negative=['word3']))\n\n`wv` =  word2vec - it is used only for this model\n\n*KEEP IN MIND* : all of your results depends on 1) the size of your corpus; 2) the content of your data!","metadata":{}},{"cell_type":"code","source":"# retrieving 5 words that are similar to 'prayer' (according to our dataset!)\nword = 'prayer'\ntop5_sim = my_model.wv.most_similar(positive = word, topn = 5)\nprint(top5_sim)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:25:59.115038Z","iopub.execute_input":"2021-06-05T14:25:59.115406Z","iopub.status.idle":"2021-06-05T14:25:59.126872Z","shell.execute_reply.started":"2021-06-05T14:25:59.115376Z","shell.execute_reply":"2021-06-05T14:25:59.125388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# similarity of the word 'prayer' and the word 'home'\n# simialrity ranges between 0 (different words) to 1 (exactly the same meaning)\nsimilarity = my_model.wv.similarity(word, 'home')\nprint(similarity)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:26:01.67319Z","iopub.execute_input":"2021-06-05T14:26:01.67351Z","iopub.status.idle":"2021-06-05T14:26:01.678348Z","shell.execute_reply.started":"2021-06-05T14:26:01.673482Z","shell.execute_reply":"2021-06-05T14:26:01.67718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# similarity of 2 sentences\nsentence_1 = 'Look around everywhere you turn is heartache'.lower().split()\nsentence_2 = 'Gonna give you all my love, boy'.lower().split()\n\nsentence_similarity = my_model.wv.wmdistance(sentence_1, sentence_2)\nprint(sentence_similarity)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:26:04.313197Z","iopub.execute_input":"2021-06-05T14:26:04.313578Z","iopub.status.idle":"2021-06-05T14:26:04.3228Z","shell.execute_reply.started":"2021-06-05T14:26:04.313546Z","shell.execute_reply":"2021-06-05T14:26:04.32122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# analogies\nanalogies = my_model.wv.most_similar_cosmul(positive=['love', 'prayer'], negative=['virgin'])\nmost_similar_key, similarity = analogies[0]  # look at the first match\nprint(most_similar_key, similarity)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:26:07.331268Z","iopub.execute_input":"2021-06-05T14:26:07.331798Z","iopub.status.idle":"2021-06-05T14:26:07.342334Z","shell.execute_reply.started":"2021-06-05T14:26:07.331762Z","shell.execute_reply":"2021-06-05T14:26:07.341196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizing our model\nFor better understanding, it can be nice to visualize things!","metadata":{}},{"cell_type":"code","source":"X=my_model.wv.__getitem__(my_model.wv.index_to_key)\ndf=pd.DataFrame(X)\ndf.shape\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:03:03.896033Z","iopub.execute_input":"2021-06-05T15:03:03.89644Z","iopub.status.idle":"2021-06-05T15:03:03.927722Z","shell.execute_reply.started":"2021-06-05T15:03:03.8964Z","shell.execute_reply":"2021-06-05T15:03:03.92695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Computing the correlation matrix\nX_corr=df.corr()\n\n#Computing eigen values and eigen vectors\nvalues,vectors=np.linalg.eig(X_corr)\n\n#Sorting the eigen vectors coresponding to eigen values in descending order\nargs = (-values).argsort()\nvalues = vectors[args]\nvectors = vectors[:, args]\n\n#Taking first 2 components which explain maximum variance for projecting\nnew_vectors=vectors[:,:2]\n\n#Projecting it onto new dimesion with 2 axis\nneww_X=np.dot(X,new_vectors)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:03:16.605832Z","iopub.execute_input":"2021-06-05T15:03:16.606597Z","iopub.status.idle":"2021-06-05T15:03:16.645472Z","shell.execute_reply.started":"2021-06-05T15:03:16.606556Z","shell.execute_reply":"2021-06-05T15:03:16.644437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize=(13,7))\nplt.scatter(neww_X[:,0],neww_X[:,1],linewidths=10,color='blue')\nplt.xlabel(\"PC1\",size=15)\nplt.ylabel(\"PC2\",size=15)\nplt.title(\"Word Embedding Space\",size=20)\nvocab=list(my_model.wv.index_to_key)\nfor i, word in enumerate(vocab):\n    plt.annotate(word,xy=(neww_X[i,0],neww_X[i,1]))","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:03:41.287919Z","iopub.execute_input":"2021-06-05T15:03:41.288367Z","iopub.status.idle":"2021-06-05T15:03:43.584765Z","shell.execute_reply.started":"2021-06-05T15:03:41.288336Z","shell.execute_reply":"2021-06-05T15:03:43.58354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A last bit of info: people before you have created word embeddings models using very large collection of textula data (this is a methodology which is not free from some ethical concerns on what these large models do actually \"learn\" and how they keep promoting existing societal bias)\n\n**How do we load existing models**\n\nGensim comes with a bunch of models already available. This is a list of available models:\n- 'fasttext-wiki-news-subwords-300': FastText (Facebook Inc.) embeddings; 300 dimesions vectors; Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens) - English.\n- 'conceptnet-numberbatch-17-06-300': \n- 'word2vec-ruscorpora-300': trained on full Russian National Corpus (about 250M words). The model contains 185K words. dimension - 300 window_size - 10\n- 'word2vec-google-news-300': Pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. - English\n- 'glove-wiki-gigaword-50': Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/). dimension - 50 - English\n- 'glove-wiki-gigaword-100': Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/). dimension - 100 - English\n- 'glove-wiki-gigaword-200': Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/). dimension - 200 - English\n- 'glove-wiki-gigaword-300': Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/). dimension - 300 - English\n- 'glove-twitter-25': Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased) dimension 25; English\n- 'glove-twitter-50': Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased) dimension 50; English\n- 'glove-twitter-100': Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased) dimension 100; English\n- 'glove-twitter-200': Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased) dimension 200; English","metadata":{}},{"cell_type":"code","source":"import gensim.downloader\nprint(list(gensim.downloader.info()['models'].keys()))","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:03:58.549032Z","iopub.execute_input":"2021-06-05T15:03:58.549417Z","iopub.status.idle":"2021-06-05T15:03:58.658958Z","shell.execute_reply.started":"2021-06-05T15:03:58.549386Z","shell.execute_reply":"2021-06-05T15:03:58.657871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If this command raise an error, try to do the following:\n- go to https://github.com/RaRe-Technologies/gensim-data\n- click on 'code' and download as zip\n- put the zipped file in your home directory and unzip it\n- copy the info in this file (https://github.com/RaRe-Technologies/gensim-data/blob/master/list.json) and save it into a file called `information.json` in the same folder\n- run the command\n\nIf you are on a Mac and you get an error of the kind \"urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] \", go to Finder--> Application--> Python --> and double clik on the file \"Install Certificates.command\"","metadata":{}},{"cell_type":"code","source":"import gensim.downloader as api # import the downloader and allow us to download and use them\n# load the target model\nglove_vectors = api.load(\"glove-twitter-25\")  # load glove vectors","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:04:03.798132Z","iopub.execute_input":"2021-06-05T15:04:03.79852Z","iopub.status.idle":"2021-06-05T15:05:20.232466Z","shell.execute_reply.started":"2021-06-05T15:04:03.798488Z","shell.execute_reply":"2021-06-05T15:05:20.231463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now you can do the same things we have done with the madonna.model using an existing models.","metadata":{}},{"cell_type":"code","source":"# retrieving 5 words that are similar to 'prayer' (according to our dataset!)\nword = 'prayer'\ntop5_sim = glove_vectors.most_similar(positive = word, topn = 5)\nprint(top5_sim)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:06:33.698428Z","iopub.execute_input":"2021-06-05T15:06:33.698797Z","iopub.status.idle":"2021-06-05T15:06:33.806143Z","shell.execute_reply.started":"2021-06-05T15:06:33.698764Z","shell.execute_reply":"2021-06-05T15:06:33.805091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"similar = glove_vectors.similarity(word, 'home')\nprint(similar)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:06:42.723059Z","iopub.execute_input":"2021-06-05T15:06:42.723522Z","iopub.status.idle":"2021-06-05T15:06:42.729634Z","shell.execute_reply.started":"2021-06-05T15:06:42.723479Z","shell.execute_reply":"2021-06-05T15:06:42.728197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# similarity of 2 sentences\nsentence_1 = 'Look around everywhere you turn is heartache'.lower().split()\nsentence_2 = 'Gonna give you all my love, boy'.lower().split()\n\nsentence_similar = glove_vectors.wmdistance(sentence_1, sentence_2)\nprint(sentence_similar)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:07:39.911558Z","iopub.execute_input":"2021-06-05T15:07:39.912375Z","iopub.status.idle":"2021-06-05T15:07:39.92083Z","shell.execute_reply.started":"2021-06-05T15:07:39.91233Z","shell.execute_reply":"2021-06-05T15:07:39.920017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# analogies\nanalogies = glove_vectors.most_similar_cosmul(positive=['love', 'prayer'], negative=['virgin'])\nmost_similar_key, similarity = analogies[0]  # look at the first match\nprint(most_similar_key, similarity)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:08:00.131726Z","iopub.execute_input":"2021-06-05T15:08:00.132381Z","iopub.status.idle":"2021-06-05T15:08:00.225043Z","shell.execute_reply.started":"2021-06-05T15:08:00.132212Z","shell.execute_reply":"2021-06-05T15:08:00.223883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X= glove_vectors[glove_vectors.index_to_key] \ndf=pd.DataFrame(X)\ndf.shape\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T15:09:14.697279Z","iopub.execute_input":"2021-06-05T15:09:14.697871Z","iopub.status.idle":"2021-06-05T15:09:19.367884Z","shell.execute_reply.started":"2021-06-05T15:09:14.697835Z","shell.execute_reply":"2021-06-05T15:09:19.367176Z"},"trusted":true},"execution_count":null,"outputs":[]}]}