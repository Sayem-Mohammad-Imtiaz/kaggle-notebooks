{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Analyzing Sentiment and Predicting Recommendation and Rating"},{"metadata":{},"cell_type":"markdown","source":"1. [Looking at the Data](#looking-at-the-data)\n2. [Text Preprocessing](#text-preprocessing)\n3. [Creating a Model Pipeline](#creating-a- model-pipeline)\n4. [Testing the Model](#testing-the-model)\n5. [Tweaking the Model and Data](#tweaking-the-model-and-data)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nfrom __future__ import unicode_literals\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport nltk\nimport re\nimport string\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\nfrom sklearn.feature_extraction import text\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\nfrom sklearn.metrics import classification_report,confusion_matrix, roc_auc_score\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"looking-at-the-data\"></a>\n## Looking at the Data\n### Creating a Model to Predict Recommendation of Product\n#### For each review, the customer was asked if they would recommend the product. First, we will be taking only the text reviews to predict recommendation.\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"file_path = \"../input/Womens Clothing E-Commerce Reviews.csv\"\ndf = pd.read_csv(file_path,index_col = 0 )\ndf.info()\ndf.groupby('Recommended IND').describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"text-preprocessing\"></a>\n## Text Preprocessing\n\nthe column 'Review Text' stores object values rather than strings.\n\nThere are also some null values to be taken care of."},{"metadata":{"trusted":true},"cell_type":"code","source":"# df.info()\n# print(df.shape) # (23486, 10)\ndf = df.dropna(subset=['Review Text'])\ndf.isnull().sum() # no more null Review Text\nrec = df.filter(['Review Text','Recommended IND'])\nrec.columns = ['text','target']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### General Preprocessing Steps\nWe will turn the words from the reviews into something that the computer can comprehend.\nBefore we vectorize the words according to frequency, the words will require some preprocessing to increase the quality of words that will make up our vocabulary. \n\nThe basic ones are:\n1. Change all letters to **lowercase letter** so that Word, word, WORD are all recognized as the same entity.\n2. Remove all **digits and punctuations** (for the sake of simplicity in this case)\n3. Tokenize each word and **remove stop words**. Stop words are words that are the most commonly used, often providing no useful insight.\n4. **Stemming each word**. There are many forms of one word (organize, organization, organizing) that all hold the same meaning. We want the machine to recognize these different forms as one word.\n    * There are two ways to achieve this: stemming and lemmatization.\n        * Stemming is faster, following an algorithm to chop off ends of words.\n            * **PorterStemmer(), LancasterStemmer(), SnowballStemmer(), **\n        * Lemmatization is slower but uses a vocabulary and morphologically analyzes the word to provide the base form (lemma) of the word. The only issue is that lemmatiazation also requires defined parts-of-speech.\n            * **use WordNetLemmatizer()**\n5. Remove all words that consist of only one letter. They are likely not meaningful and therefore we will take them out.\n\nI believe that punctuation and looking at capitalized words can be good indication for varying levels of emotions, both negative and positive.\nBut for now, I will only be looking at words as indicators of positive or negative opinions, testing it by checking against whether the reviewer recommended the item or not.\n\nFor this, I will also not be spellchecking, which means that words like \"sooo\" are not going to be aligned with the correct spelling of \"so.\" Spelling correction could help improve accuracy, so it may be considered in the future."},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\nstemmer = PorterStemmer()\n# remove all tokens that are stop words or punctuations and perform lemmatization\ndef prep_clean(text):\n    text = text.lower()\n    text = re.sub(r'\\d+','',text)\n    tokens = word_tokenize(text)\n    words = [token for token in tokens if not token in stop_words]\n    words = [stemmer.stem(word) for word in words]\n    words = [word for word in words if not word in string.punctuation]\n    words = [word for word in words if len(word) > 1]\n    return words","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We clean up the text, and now we should do something with these words."},{"metadata":{},"cell_type":"markdown","source":"<a id = \"creating-a-model-pipeline\"></a>\n## Creating a Model Pipeline\n\nNow that we have a list of preprocessed tokens, let's look at different ways we can transform the words into something the model could use.\n\nFirst, the most simple thing to do is to build vocabulary of words by word count.\nWe can create a bag of words model using CountVectorizer"},{"metadata":{},"cell_type":"markdown","source":"Once we create our bow, we want to transform this count matrix into a normalized term-frequency representation.\ntf-idf is the term-ferquency times inverse document-frequency. It is a common term weighting scheme in information retrieval as well as document classification.\n\nThis will help scale down the impact of tokens that occur frequently in the corpus since features that occur n small fractions are more informative."},{"metadata":{},"cell_type":"markdown","source":"We are only comparing the review text and the recommendation values\n\nI want to explore several models to achieve this.\n\nFirst, I will be looking at the Multinoial Naive Bayes model.\nBy taking the word frequency for each review, the model will be able to learn which words are frequently associated with recommendation.\n\nThe model is naive because it takes an assumption that each word is independent of one another, therefore looking at individual words rather than the entire sentence.\n\nI will explain some ways to improve the model along the way."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(rec['text'], rec['target'], test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# build the model and test its accuracy\ndef model(mod, name, X_train, X_test, y_train, y_test):\n    mod.fit(X_train, y_train)\n    print(name)\n    acc = cross_val_score(mod, X_train, y_train, scoring = \"accuracy\", cv=5)\n    predictions = cross_val_predict(mod, X_train, y_train, cv=5)\n    print(\"Accuracy: \", round(acc.mean(),3))\n    cm = confusion_matrix(predictions, y_train)\n    print(\"Confusion Matrix: \\n\", cm)\n    print(\"Classification Report: \\n\", classification_report(predictions, y_train))\n    print(\"--------\")\n    print(predictions[:10])\n    print(y_train[:10])\n# the model and all the preprocessing steps\ndef pipeline(bow, tfidf, model):\n    return Pipeline([('bow', bow),\n               ('tfidf', tfidf),\n               ('classifier', model),\n              ])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A confusion matrix is an easy way to visualize the performance of this binary classification model.\n\nIt is divided to show \n\n\n$\\begin{array}\n{rrr}\n & predicted: no & predicted: yes \\\\\nactual: no  & correct & wrong \\\\\nactual: yes & wrong & correct \\\\\n\\end{array}\n$"},{"metadata":{"trusted":true},"cell_type":"code","source":"mnb = pipeline(CountVectorizer(analyzer=prep_clean, ngram_range=(1,2)), TfidfTransformer(), MultinomialNB())\nmnb = model(mnb, \"Multinomial Naive Bayes\", X_train, X_test, y_train, y_test)\n\nlog = pipeline(CountVectorizer(analyzer=prep_clean, ngram_range=(1,2)), TfidfTransformer(), LogisticRegression(solver='lbfgs'))\nlog = model(log, \"Logistic Regression\", X_train, X_test, y_train, y_test)\n\nsvc = pipeline(CountVectorizer(analyzer=prep_clean, ngram_range=(1,2)), TfidfTransformer(), LinearSVC())\nsvc = model(svc, \"Linear SVC\", X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"testing-the-model\"></a>\n## Testing the Model\n\n#### Multimodal Naive-Bayes\n* Accuracy:  0.821\n* Confusion Matrix:\n$\\left[\\begin{array}\n{rrr}\n45 & 1 \\\\\n3240 & 14826 \\\\\n\\end{array}\\right]\n$\n\n#### Logistic Regression\n* Accuracy:  0.884\n* Confusion Matrix:\n$\\left[\\begin{array}\n{rrr}\n1602 & 413 \\\\\n1683 & 14414 \\\\\n\\end{array}\\right]\n$\n\n#### Linear SVC\n* Accuracy:  0.884\n* Confusion Matrix:\n$\\left[\\begin{array}\n{rrr}\n1916 & 738 \\\\\n1369 & 14089 \\\\\n\\end{array}\\right]\n$\n\n"},{"metadata":{},"cell_type":"markdown","source":"We can analyze each of these to understand what they mean. Although the overall accuracy of the Logistic Regression model and the Linear SVC model is very similar, the makeup of the error is different.\n\nWe can look at false negatives and false positives.\n\n* predicted no, but was actually yes (FALSE NEGATIVE)\n    * log-reg:1683\n    * lin-svm:1369\n* predicted yes, but was actually no (FALSE POSITIVE)\n    * log-reg:413\n    * lin-svm:738\n\nAdding a stemming step to the preprocessor did increase the overall accuracy of each of the models. However, it also increased the false negatives.\n(The code now shows the accuracy of the pipeline including the preprocessor with stemming)"},{"metadata":{},"cell_type":"markdown","source":"#### Tuning the model\n\nSome of the reviews have titles, and the titles may be able to add more to the reviews."},{"metadata":{"trusted":true},"cell_type":"code","source":"rec[\"concat\"] = df[\"Title\"].fillna('') + \" \"+ df[\"Review Text\"]\nX_train, X_test, y_train, y_test = train_test_split(rec['concat'], rec['target'], test_size=0.2)\nmnb = pipeline(CountVectorizer(analyzer=prep_clean, ngram_range=(1,2)), TfidfTransformer(), MultinomialNB())\nmnb = model(mnb, \"Multinomial Naive Bayes\", X_train, X_test, y_train, y_test)\n\nlog = pipeline(CountVectorizer(analyzer=prep_clean, ngram_range=(1,2)), TfidfTransformer(), LogisticRegression(solver='lbfgs'))\nlog = model(log, \"Logistic Regression\", X_train, X_test, y_train, y_test)\n\nsvc = pipeline(CountVectorizer(analyzer=prep_clean, ngram_range=(1,2)), TfidfTransformer(), LinearSVC())\nsvc = model(svc, \"Linear SVC\", X_train, X_test, y_train, y_test)\n\n# rfc = pipeline(CountVectorizer(analyzer=prep_clean, ngram_range=(1,2)), TfidfTransformer(), LinearSVC())\n# rfc = model(rfc, \"Random Forest Classifier\", X_train, X_test, y_train, y_test)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adding the title data has increased the performance of all the models!\nThis makes sense since many of the titles summarize the reviewer's experience, with the review containing the details of the sentiment they expressed in their title.\nThe linear SVC now has an accuracy of 90%, with a 66% accuracy for not recommended.\nSince most of the data contains recommendation reviews, the focus of this is really to increase the accuracy of non-recommendation.\nThere is also some discrepency in the text reviews and the recommendation in some cases, and even human guesses may be wrong for those.\n\n<a id=\"tweaking-the-model-and-data\"></a>\n## Tweaking the Model and Data\n\n#### Data:\n1. Append the title and the review text together to create our X-value.\n2. The recommendation (binary value, 1=recommended) is our y-value.\n\n#### Preprocessing:\n1. Tokenize the text and stem the words\n2. Remove digits, stop words,  punctuations, and single-letter words\n\n#### Modeling:\n1. Split the dataset into training and testing set\n2. Create a pipeline, consisting of:\n    * CountVectorizer(analyzer=prep_clean, ngram_range=(1,2))\n    * TfidfTransformer()\n    * LinearSVC()\n3. Test the LinearSVC model's performance by\n    * Cross_Val_Score()\n    * Cross_Val_Predict()\n    * confusion_matrix()\n    * classification_report()"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}