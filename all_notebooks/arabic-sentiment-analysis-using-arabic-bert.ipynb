{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Arabic sentiment analysis","metadata":{}},{"cell_type":"markdown","source":"This notebook aims to explore the topic of Arabic sentiment analysis. First by attempting some classical ML models trained on the [arabic-sentiment-twitter-corpus](https://www.kaggle.com/mksaad/arabic-sentiment-twitter-corpus) (dataset 1 in the table below) and evaluated on 3 other sentiment-labelled Arabic datasets (datasets 2, 3, and 4 in the table below). I then  move to trying a deep learning approach through finetuning an [Arabic-BERT model](https://github.com/alisafaya/Arabic-BERT) trained on ~8.2 billion words. I do some iterations on variations of the model, evaluating on the same 3 datasets, and then attempt changing the training dataset (trying datasets 2, 3 and 4 as training datasets) and seeing how it affects the overall performance and the model's ability to generalize.","metadata":{}},{"cell_type":"markdown","source":"The following table summarizes the datasets used throughout this notebook.\n\n| dataset ID | dataset name| is_dialectical | is_MSA (Modern Standard Arabic) | is_balanced | num_of_tweets | num_of_pos_tweets | num_of_neg_tweets |\n|-- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 1 | [arabic-sentiment-twitter-corpus](https://www.kaggle.com/mksaad/arabic-sentiment-twitter-corpus) | Yes | No/minority | Yes | 58,751 | 29,849 | 28,902  \n| 2 |[SS2030](https://www.kaggle.com/snalyami3/arabic-sentiment-analysis-dataset-ss2030-dataset ) | Yes - Saudi dialect only | No/Minority | Yes | 4,252 | 2,436 | 1,816 \n| 3 |[100k Arabic Reviews](https://www.kaggle.com/abedkhooli/arabic-100k-reviews ) | No/Minority | Yes | Yes | 66,666 | 33,333 | 33,333\n| 4 | [ArSAS](https://homepages.inf.ed.ac.uk/wmagdy/resources.htm) | Yes - mixed dialects| No/Minority | Yes | 11,784 | 4,400 | 7,384\n\n*(For a more detailed analysis of the datasets see [this](https://www.kaggle.com/yasmeenhany/dataset-analysis) companion notebook. )*","metadata":{}},{"cell_type":"markdown","source":"# Table of Contents:\n* [Classic ML Approach](#1)\n    * [Compare classifiers](#1.1)\n    * [Evaluate best classifier's performance on other datasets](#1.2)\n    * [Summary of classic ML results](#1.3)\n* [Deep Learning Approach](#2)\n * [BERT-mini](#2.1)\n     * [Preprocessing](#2.1.1)\n     * [Create data loaders for test and validation sets](#2.1.2)\n     * [Define model initialization class and functions](#2.1.3)\n     * [Define model train and evaluate functions](#2.1.4)\n     * [Initialize and train model](#2.1.5)\n     * [Save model](#2.1.6)\n     * [Define prediction and test set evaluation functions](#2.1.7)\n     * [Predict and evaluate validation subset](#2.1.8)\n     * [Predict and evaluate test subset](#2.1.9)\n     * [Predict and evaluate on other test datasets](#2.1.10)\n     * [Summary of performance on test datasets](#2.1.11)\n * [BERT-mini without emojis](#2.2)\n    * [Define modified preprocessing function](#2.2.1)\n    * [Preprocess and create data loaders](#2.2.2)\n    * [Train](#2.2.3)\n    * [Evaluate on test datasets](#2.2.4)\n    * [Summary of performance on test datasets](#2.2.5)\n        \n * [BERT-base without emojis](#2.3)\n    * [Preprocess and create data loaders](#2.3.1)\n    * [Train](#2.3.2)\n    * [Save trained model](#2.3.3)\n    * [Evaluate on test datasets](#2.3.4)\n    * [Summary of performance on test datasets](#2.3.5)\n * [Training on other datasets (2, 3 and 4)](#2.4)\n    * [Train using the SS2030 dataset](#2.4.1)\n    * [Train using the 100k Arabic Reviews dataset](#2.4.2)\n    * [Train using the ArSAS dataset](#2.4.3)\n    * [Summary of performance on test datasets](#2.4.4)\n    * [Train BERT-base Using the ArSAS Dataset](#2.4.5)\n \n* [Summary](#3)\n","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport csv\n\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Load dataset 1","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', 280)\ntrain_neg = pd.read_csv(\"../input/arabic-sentiment-twitter-corpus/train_Arabic_tweets_negative_20190413.tsv\", sep=\"\\t\", header=None,  quoting=csv.QUOTE_NONE)\ntrain_neg.rename(columns={0:'label', 1:'tweet'}, inplace=True)\ntrain_neg['label'] = 0\n\ntrain_pos = pd.read_csv(\"../input/arabic-sentiment-twitter-corpus/train_Arabic_tweets_positive_20190413.tsv\", sep=\"\\t\", header=None,  quoting=csv.QUOTE_NONE)\ntrain_pos.rename(columns={0:'label', 1:'tweet'}, inplace=True)\ntrain_pos['label'] = 1\n\n\ntrain_df = pd.concat([train_neg, train_pos], axis=0).reset_index(drop=True)\n\n\nfrom sklearn.model_selection import train_test_split\nX = train_df.tweet.values\ny = train_df.label.values\n\n# The train val split is used by the DL approach but not classical ML\nX_train, X_val, y_train, y_val = train_test_split(X,y,test_size=0.1, random_state=2020)\n# Load test subset\ntest_pos = pd.read_csv(\"../input/arabic-sentiment-twitter-corpus/test_Arabic_tweets_positive_20190413.tsv\", sep=\"\\t\", header=None,  quoting=csv.QUOTE_NONE)\ntest_pos.rename(columns={0:'label', 1:'tweet'}, inplace=True)\ntest_pos['label']=1\n\ntest_neg = pd.read_csv(\"../input/arabic-sentiment-twitter-corpus/test_Arabic_tweets_negative_20190413.tsv\", sep=\"\\t\", header=None,  quoting=csv.QUOTE_NONE)\ntest_neg.rename(columns={0:'label', 1:'tweet'}, inplace=True)\ntest_neg['label']=0\n\ntest_df = pd.concat([test_neg, test_pos], axis=0).reset_index(drop=True)\nX_test = test_df.tweet.values\ny_test = test_df.label.values\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Load other test datasets (datasets 2, 3 and 4): (to test how well model generalizes on arabic tweets/short text)\n& preprocess such that each dataset's dataframe has two columns:\n1) **tweet**: tweet text \n\n2) **label**: representing sentiment, with 1 being positive and 0 negative","metadata":{}},{"cell_type":"markdown","source":"See this [companion notebook](https://www.kaggle.com/yasmeenhany/dataset-analysis?scriptVersionId=64595722) for a comparison of the datasets in terms of emoji presence, vocabulary similarity, vocabulary histograms, etc.","metadata":{}},{"cell_type":"code","source":"df_ss2030 = pd.read_csv(\"../input/arabic-sentiment-analysis-dataset-ss2030-dataset/Arabic Sentiment Analysis Dataset - SS2030.csv\")\n# Rename columns to match convention\ndf_ss2030 = df_ss2030.rename(columns = {\"text\":\"tweet\", \"Sentiment\": \"label\"})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_reviews = pd.read_csv(\"../input/arabic-100k-reviews/ar_reviews_100k.tsv\", delimiter=\"\\t\")\n# Create a mapping for the labels such that we use the same convention across all datasets\nlabel_mapping = {\"Positive\": 1, \"Negative\":0}\n# Filter to only have pos and neg tweets, i.e: remove mixed tweets\ndf_reviews = df_reviews[df_reviews.label != \"Mixed\"]\ndf_reviews[\"label\"] = df_reviews[\"label\"].map(label_mapping)\n# Rename columns to match convention\ndf_reviews = df_reviews.rename(columns = {\"text\":\"tweet\"})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_arsas = pd.read_csv('../input/arsas-dataset/ArSAS..txt', header = 0, delimiter = \"\\t\")\n# Filter to only have pos and neg tweets, i.e: remove mixed tweets\ndf_arsas_pos = df_arsas[df_arsas.Sentiment_label == 'Positive'] \ndf_arsas_neg = df_arsas[df_arsas.Sentiment_label == 'Negative'] \ndf_arsas = pd.concat([df_arsas_pos, df_arsas_neg], axis=0).reset_index(drop=True)\n# Create a mapping for the labels such that we use the same convention across all datasets\nlabel_mapping = {\"Positive\": int(1), \"Negative\":int(0)}\ndf_arsas[\"Sentiment_label\"] = df_arsas[\"Sentiment_label\"].map(label_mapping)\n# Rename columns to match convention\ndf_arsas = df_arsas.rename(columns = {\"Tweet_text\":\"tweet\", \"Sentiment_label\":\"label\"})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# Classical ML approach\n#### Using tf-idf features","metadata":{}},{"cell_type":"code","source":"# Helper functions \nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.pipeline import Pipeline\ndef train_model(model, data, targets):\n    text_clf = Pipeline([\n    ('vect', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('clf', model),\n    ])\n    text_clf.fit(data, targets)\n    return text_clf\ndef get_accuracy(trained_model,X, y):\n    predicted = trained_model.predict(X)\n    accuracy = np.mean(predicted == y)\n    return accuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1.1\"></a>\n### Compare classifiers","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ntrained_clf_decision_tree = train_model(DecisionTreeClassifier(), X, y)\naccuracy = get_accuracy(trained_clf_decision_tree,X_test, y_test)\nprint(f\"Test dataset accuracy with DecisionTreeClassifier: {accuracy:.2f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\ntrained_clf_multinomial_nb = train_model(MultinomialNB(), X, y)\naccuracy = get_accuracy(trained_clf_multinomial_nb,X_test, y_test)\nprint(f\"Test dataset accuracy with MultinomialNB: {accuracy:.2f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import LinearSVC\ntrained_clf_linearSVC = train_model(LinearSVC(), X, y)\naccuracy = get_accuracy(trained_clf_linearSVC,X_test, y_test)\nprint(f\"Test dataset accuracy with LinearSVC: {accuracy:.2f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\ntrained_clf_random_forest = train_model(RandomForestClassifier(), X, y)\naccuracy = get_accuracy(trained_clf_random_forest,X_test, y_test)\nprint(f\"Test dataset accuracy with RandomForestClassifier: {accuracy:.2f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Because no classifier is yielding a significantly better accuracy on the arabic-sentiment-twitter-corupus test subset, will evaluate all classifiers against the other test datasets, to get a more general idea about their performance (measured by accuracy)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1.2\"></a>\n### Evaluate classifiers on other datasets","metadata":{}},{"cell_type":"code","source":"def print_all_accuracies(dataset_name, dataset):\n  accuracy = get_accuracy(trained_clf_decision_tree,dataset.tweet.values, dataset.label.values)\n  print(f\"{dataset_name} dataset accuracy with Decision Tree: {accuracy:.2f}\")\n  accuracy = get_accuracy(trained_clf_multinomial_nb,dataset.tweet.values, dataset.label.values)\n  print(f\"{dataset_name} dataset accuracy with Multinomial NB: {accuracy:.2f}\")\n  accuracy = get_accuracy(trained_clf_linearSVC,dataset.tweet.values, dataset.label.values)\n  print(f\"{dataset_name} dataset accuracy with Linear SVC: {accuracy:.2f}\")\n  accuracy = get_accuracy(trained_clf_random_forest,dataset.tweet.values, dataset.label.values)\n  print(f\"{dataset_name} dataset accuracy with Random Forest: {accuracy:.2f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_all_accuracies(\"SS2030\", df_ss2030)\nprint_all_accuracies(\"100k Arabic Reviews\", df_reviews)\nprint_all_accuracies(\"ArSAS\", df_arsas)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id= \"1.3\"> </a>\n### Summary of Classic ML Results:\n- Best classfiers found for the `arabic-sentiment-twitter-corpus` dataset: **RandomForestClassifier** \n- Performance across test datasets (numbers represent accuracy):\n\n| Dataset | Decision Tree | Multinomial NB | Linear SVC | Random Forest\n| :---: | :---: | :---: | :---: | :---: |\n| arabic-sentiment-twitter-corpus test subset | 0.77 | 0.79 | 0.79 | **0.8** \n| SS2030 | 0.52 | **0.59** | 0.58 | 0.55\n| 100k reviews | 0.54 | **0.60** | 0.58 | 0.59\n| ArSAS | 0.51 | 0.65 | 0.61 | **0.66** \n    \n\n    \n- It appears that **Multinomial NB** can sometimes outperform Random Forest but the differences are insignificant. \n","metadata":{}},{"cell_type":"markdown","source":"# Deep Learning Approach\n- Given that the Random Forest Classifier model wasn't generalizing well for other datasets (possibly overfitting), I decided to try a DL approach using a pretrained model (i.e: increasing the dataset as a way of overcoming overfitting). For that I chose to use the [Arabic-BERT model](https://github.com/alisafaya/Arabic-BERT) By Ali Safaya.  \n> The models were pretrained on ~8.2 Billion words:\n> - Arabic version of OSCAR (unshuffled version of the corpus) - filtered from Common Crawl\n> - Recent dump of Arabic Wikipedia","metadata":{}},{"cell_type":"code","source":"import torch\n\nif torch.cuda.is_available():       \n    device = torch.device(\"cuda\")\n    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n    print('Device name:', torch.cuda.get_device_name(0))\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.1\"> </a>\n### BERT-mini\nCode adapted from https://skimai.com/fine-tuning-bert-for-sentiment-analysis/","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-mini-arabic\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.1.1\"> </a>\n##### Preprocessing","metadata":{}},{"cell_type":"code","source":"# Define preprocessing util function\ndef text_preprocessing(text):\n    \"\"\"\n    - Remove entity mentions (eg. '@united')\n    - Correct errors (eg. '&amp;' to '&')\n    @param    text (str): a string to be processed.\n    @return   text (Str): the processed string.\n    \"\"\"\n  \n\n    # Normalize unicode encoding\n    text = unicodedata.normalize('NFC', text)\n    # Remove '@name'\n    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n\n    # Replace '&amp;' with '&'\n    text = re.sub(r'&amp;', '&', text)\n\n    # Remove trailing whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    #Remove URLs\n    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '<URL>', text)\n\n\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a function to tokenize a set of texts\nimport emoji\nimport unicodedata\ndef preprocessing_for_bert(data, version=\"mini\", text_preprocessing_fn = text_preprocessing ):\n    \"\"\"Perform required preprocessing steps for pretrained BERT.\n    @param    data (np.array): Array of texts to be processed.\n    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n                  tokens should be attended to by the model.\n    \"\"\"\n    # Create empty lists to store outputs\n    input_ids = []\n    attention_masks = []\n    tokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-mini-arabic\") if version == \"mini\" else AutoTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\n\n    # For every sentence...\n    for i,sent in enumerate(data):\n        # `encode_plus` will:\n        #    (1) Tokenize the sentence\n        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n        #    (3) Truncate/Pad sentence to max length\n        #    (4) Map tokens to their IDs\n        #    (5) Create attention mask\n        #    (6) Return a dictionary of outputs\n        encoded_sent = tokenizer.encode_plus(\n            text=text_preprocessing_fn(sent),  # Preprocess sentence\n            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n            max_length=MAX_LEN,                  # Max length to truncate/pad\n            padding='max_length',        # Pad sentence to max length\n            #return_tensors='pt',           # Return PyTorch tensor\n            return_attention_mask=True,     # Return attention mask\n            truncation = True \n            )\n        \n        # Add the outputs to the lists\n        input_ids.append(encoded_sent.get('input_ids'))\n        attention_masks.append(encoded_sent.get('attention_mask'))\n    # Convert lists to tensors\n    input_ids = torch.tensor(input_ids)\n    attention_masks = torch.tensor(attention_masks)\n\n    return input_ids, attention_masks","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specify `MAX_LEN`\nMAX_LEN =  280\n\n# Print sentence 0 and its encoded token ids\ntoken_ids = list(preprocessing_for_bert([X[0]])[0].squeeze().numpy())\nprint('Original: ', X[0])\nprint('Token IDs: ', token_ids)\n\n# Run function `preprocessing_for_bert` on the train set and the validation set\nprint('Tokenizing data...')\ntrain_inputs, train_masks = preprocessing_for_bert(X_train)\nval_inputs, val_masks = preprocessing_for_bert(X_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.1.2\"> </a>\n##### Create data loaders for test and validation sets","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n# Convert other data types to torch.Tensor\ntrain_labels = torch.tensor(y_train)\nval_labels = torch.tensor(y_val)\n\n# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\nbatch_size = 16\n\n# Create the DataLoader for our training set\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# Create the DataLoader for our validation set\nval_data = TensorDataset(val_inputs, val_masks, val_labels)\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.1.3\"> </a>\n##### Define model initialization class and functions","metadata":{}},{"cell_type":"code","source":"%%time\nimport torch\nimport torch.nn as nn\nfrom transformers import BertModel\n\n# Create the BertClassfier class\nclass BertClassifier(nn.Module):\n    \"\"\"Bert Model for Classification Tasks.\n    \"\"\"\n    def __init__(self, freeze_bert=False, version=\"mini\"):\n        \"\"\"\n        @param    bert: a BertModel object\n        @param    classifier: a torch.nn.Module classifier\n        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n        \"\"\"\n        super(BertClassifier, self).__init__()\n        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n        D_in = 256 if version == \"mini\" else 768\n        H, D_out = 50, 2\n\n        # Instantiate BERT model\n        self.bert = AutoModel.from_pretrained(\"asafaya/bert-mini-arabic\") if version == \"mini\" else AutoModel.from_pretrained(\"asafaya/bert-base-arabic\")\n        # Instantiate an one-layer feed-forward classifier\n        self.classifier = nn.Sequential(\n            nn.Linear(D_in, H),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(H, D_out)\n        )\n\n        # Freeze the BERT model\n        if freeze_bert:\n            for param in self.bert.parameters():\n                param.requires_grad = False\n        \n    def forward(self, input_ids, attention_mask):\n        \"\"\"\n        Feed input to BERT and the classifier to compute logits.\n        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n                      max_length)\n        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n                      information with shape (batch_size, max_length)\n        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n                      num_labels)\n        \"\"\"\n        # Feed input to BERT\n        outputs = self.bert(input_ids=input_ids,\n                            attention_mask=attention_mask)\n        \n        # Extract the last hidden state of the token `[CLS]` for classification task\n        last_hidden_state_cls = outputs[0][:, 0, :]\n\n        # Feed input to classifier to compute logits\n        logits = self.classifier(last_hidden_state_cls)\n\n        return logits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup\n\nfrom torch.optim import SparseAdam, Adam\ndef initialize_model(epochs=4, version=\"mini\"):\n    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n    \"\"\"\n    # Instantiate Bert Classifier\n    bert_classifier = BertClassifier(freeze_bert=False, version=version)\n    # Tell PyTorch to run the model on GPU\n    bert_classifier.to(device)\n\n    # Create the optimizer\n    optimizer = AdamW(params=list(bert_classifier.parameters()),\n                      lr=5e-5,    # Default learning rate\n                      eps=1e-8    # Default epsilon value\n                      )\n\n    # Total number of training steps\n    total_steps = len(train_dataloader) * epochs\n\n    # Set up the learning rate scheduler\n    scheduler = get_linear_schedule_with_warmup(optimizer,\n                                                num_warmup_steps=0, # Default value\n                                                num_training_steps=total_steps)\n    return bert_classifier, optimizer, scheduler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.1.4\"> </a>\n##### Define model train and evaluate functions","metadata":{}},{"cell_type":"code","source":"import random\nimport time\nimport torch\nimport torch.nn as nn\n# Specify loss function\nloss_fn = nn.CrossEntropyLoss()\n\ndef set_seed(seed_value=42):\n    \"\"\"Set seed for reproducibility.\n    \"\"\"\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    torch.cuda.manual_seed_all(seed_value)\n\ndef train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n    \"\"\"Train the BertClassifier model.\n    \"\"\"\n    # Start training loop\n    print(\"Start training...\\n\")\n    for epoch_i in range(epochs):\n        # =======================================\n        #               Training\n        # =======================================\n        # Print the header of the result table\n        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n        print(\"-\"*70)\n\n        # Measure the elapsed time of each epoch\n        t0_epoch, t0_batch = time.time(), time.time()\n\n        # Reset tracking variables at the beginning of each epoch\n        total_loss, batch_loss, batch_counts = 0, 0, 0\n\n        # Put the model into the training mode\n        model.train()\n\n        # For each batch of training data...\n        for step, batch in enumerate(train_dataloader):\n            batch_counts +=1\n            # Load batch to GPU\n            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n\n            # Zero out any previously calculated gradients\n            model.zero_grad()\n\n            # Perform a forward pass. This will return logits.\n            logits = model(b_input_ids, b_attn_mask)\n\n            # Compute loss and accumulate the loss values\n            loss = loss_fn(logits, b_labels)\n            batch_loss += loss.item()\n            total_loss += loss.item()\n\n            # Perform a backward pass to calculate gradients\n            loss.backward()\n\n            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n            # Update parameters and the learning rate\n            optimizer.step()\n            scheduler.step()\n\n            # Print the loss values and time elapsed for every 20 batches\n            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n                # Calculate time elapsed for 20 batches\n                time_elapsed = time.time() - t0_batch\n\n                # Print training results\n                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n\n                # Reset batch tracking variables\n                batch_loss, batch_counts = 0, 0\n                t0_batch = time.time()\n\n        # Calculate the average loss over the entire training data\n        avg_train_loss = total_loss / len(train_dataloader)\n\n        print(\"-\"*70)\n        # =======================================\n        #               Evaluation\n        # =======================================\n        if evaluation == True:\n            # After the completion of each training epoch, measure the model's performance\n            # on our validation set.\n            val_loss, val_accuracy = evaluate(model, val_dataloader)\n\n            # Print performance over the entire training data\n            time_elapsed = time.time() - t0_epoch\n            \n            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n            print(\"-\"*70)\n        print(\"\\n\")\n    \n    print(\"Training complete!\")\n\n\ndef evaluate(model, val_dataloader):\n    \"\"\"After the completion of each training epoch, measure the model's performance\n    on our validation set.\n    \"\"\"\n    # Put the model into the evaluation mode. The dropout layers are disabled during\n    # the test time.\n    model.eval()\n\n    # Tracking variables\n    val_accuracy = []\n    val_loss = []\n\n    # For each batch in our validation set...\n    for batch in val_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n\n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids, b_attn_mask)\n\n        # Compute loss\n        loss = loss_fn(logits, b_labels)\n        val_loss.append(loss.item())\n\n        # Get the predictions\n        preds = torch.argmax(logits, dim=1).flatten()\n\n        # Calculate the accuracy rate\n        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n        val_accuracy.append(accuracy)\n\n    # Compute the average accuracy and loss over the validation set.\n    val_loss = np.mean(val_loss)\n    val_accuracy = np.mean(val_accuracy)\n\n    return val_loss, val_accuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.1.5\"> </a>\n##### Initialize and train model","metadata":{}},{"cell_type":"code","source":"set_seed(42) \nbert_classifier, optimizer, scheduler = initialize_model(epochs=2)\ntrain(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.1.6\"> </a>\n##### Save model","metadata":{}},{"cell_type":"code","source":"# Saving the model for future runs\n\nimport pickle\nfilename = 'trained_model_mini_with_emojis.sav'\npickle.dump(bert_classifier, open(filename, 'wb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load model (Uncomment to avoid retraining in future runs)","metadata":{}},{"cell_type":"code","source":"# # Loading the model (to avoid retraining in reruns)\n\n# import pickle\n# filename = 'trained_model_mini_with_emojis.sav'\n# f = open(filename, 'rb')\n# bert_classifier = pickle.load(f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.1.7\"> </a>\n##### Define prediction and test set evaluation functions","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef bert_predict(model, test_dataloader):\n    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n    on the test set.\n    \"\"\"\n    # Put the model into the evaluation mode. The dropout layers are disabled during\n    # the test time.\n    model.eval()\n\n    all_logits = []\n\n    # For each batch in our test set...\n    for batch in test_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n\n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids, b_attn_mask)\n        all_logits.append(logits)\n    \n    # Concatenate logits from each batch\n    all_logits = torch.cat(all_logits, dim=0)\n\n    # Apply softmax to calculate probabilities\n    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n\n    return probs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, roc_curve, auc\n\ndef evaluate_roc(probs, y_true, model_name, dataset_name, test_dataset_name):\n    \"\"\"\n    - Print AUC and accuracy on the test set\n    - Plot ROC\n    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n    \"\"\"\n    preds = probs[:, 1]\n    fpr, tpr, threshold = roc_curve(y_true, preds)\n    roc_auc = auc(fpr, tpr)\n    print(f'AUC: {roc_auc:.4f}')\n       \n    # Get accuracy over the test set\n    y_pred = np.where(preds >= 0.5, 1, 0)\n    accuracy = accuracy_score(y_true, y_pred)\n    print(f'Accuracy: {accuracy*100:.2f}%')\n    \n    # Plot ROC AUC\n    plt.title(f\" ROC of {model_name}  trained on {dataset_name} dataset & evaluated on the {test_dataset_name} dataset \")\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.1.8\"> </a>\n##### Predict and evaluate validation subset","metadata":{}},{"cell_type":"code","source":"# Compute predicted probabilities on the validation set\nprobs = bert_predict(bert_classifier, val_dataloader)\n\n# Evaluate the Bert classifier\nevaluate_roc(probs, y_val, \"BERT-mini\", \"arabic-sentiment-twitter-corpus\", \"arabic-sentiment-twitter-corpus validation\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.1.9\"> </a>\n##### Predict and evaluate test subset","metadata":{}},{"cell_type":"code","source":"# Run `preprocessing_for_bert` on the test set\nprint('Tokenizing data...')\ntest_inputs, test_masks = preprocessing_for_bert(X_test)\n\n# Create the DataLoader for our test set\ntest_dataset = TensorDataset(test_inputs, test_masks)\ntest_sampler = SequentialSampler(test_dataset)\ntest_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute predicted probabilities on the test set\nprobs = bert_predict(bert_classifier, test_dataloader)\n\n# Get predictions from the probabilities\nthreshold = 0.5\npreds = np.where(probs[:, 1] > threshold, 1, 0)\n\n# Number of tweets predicted non-negative\nprint(\"no-negative tweets ratio \", preds.sum()/len(preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the Bert classifier for unseen test data\nevaluate_roc(probs, y_test,\"BERT-mini\", \"arabic-sentiment-twitter-corpus\",\"arabic-sentiment-twitter-corpus test\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.1.10\"> </a>\n##### Predict and evaluate on other test datasets","metadata":{}},{"cell_type":"code","source":"# Evaluate the performance of a model on test datasets\ndef evaluate_dataset(sents, labels, model_name, dataset_name, test_dataset_name):\n    test_inputs, test_masks = preprocessing_for_bert(sents)\n\n    # Create the DataLoader for our test set\n    test_dataset = TensorDataset(test_inputs, test_masks)\n    test_sampler = SequentialSampler(test_dataset)\n    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)\n    # Compute predicted probabilities on the test set\n    probs = bert_predict(bert_classifier, test_dataloader)\n\n    # Get predictions from the probabilities\n    threshold = 0.5\n    preds = np.where(probs[:, 1] > threshold, 1, 0)\n    auc_graph = evaluate_roc(probs, labels, model_name, dataset_name, test_dataset_name )\n\n    return auc_graph","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate on the SS2030 Dataset\nevaluate_dataset(df_ss2030.tweet.values, df_ss2030.label.values,\"BERT-mini no emojis\", \"arabic-sentiment-twitter-corpus\", \"ss2030\" )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate on the 100k Arabic Reviews Dataset\nevaluate_dataset(df_reviews.tweet.values, df_reviews.label.values,\"BERT-mini\", \"arabic-sentiment-twitter-corpus\", \"100K Reviews\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_dataset(df_arsas.tweet.values, df_arsas.label.values,\"BERT-mini\", \"arabic-sentiment-twitter-corpus\", \"ArSAS\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.1.11\"> </a>\n##### Summary of performance on test datasets\n\n| Model | arabic-sentiment-twitter-corpus test subset | SS2030 | 100k reviews | ArSAS\n| :---: | :---: | :---: | :---: | :---: |\n| RandomForestClassifier | 0.798 | 0.554 | 0.587 | 0.660\n| BERT-mini | 0.900 | 0.639 | 0.599 | 0.691\n\n<center><i>numbers shown represent accuracy</i></center>","metadata":{}},{"cell_type":"code","source":"# Helper function to get the prediction of a single tweet's sentiment (can be used for random tweet testing)\ndef predict_tweet_sentiment(tweet):\n    df = pd.DataFrame([tweet])\n    df = df.rename(columns = {0:\"tweet\"})\n    print(df.tweet.values)\n    test_inputs, test_masks = preprocessing_for_bert(df.tweet.values)\n\n    # Create the DataLoader for our test set\n    test_dataset = TensorDataset(test_inputs, test_masks)\n    test_sampler = SequentialSampler(test_dataset)\n    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)\n    # Compute predicted probabilities on the test set\n    probs = bert_predict(bert_classifier, test_dataloader)\n    print(probs)\n    # Get predictions from the probabilities\n    threshold = 0.5\n    preds = np.where(probs[:, 1] > threshold, \"positive\", \"negative\")\n\n#     print(\"no-negative tweets ratio \", preds.sum()/len(preds))\n    return preds\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*While it seems like the DL approach with Arabic BERT improved generalization on other datasets, it seems like there's still a big gap between the performance on the arabic-sentiment-twitter-corpus dataset and the other datasets. I had a suspicion that the model's high accuracy on the first dataset (arabic-sentiment-twitter-corpus) was due to the fact that it uses emojis as cues (From the dataset analysis [notebook](https://www.kaggle.com/yasmeenhany/dataset-analysis?scriptVersionId=64595722) we can see that this dataset has emojis in almost 80% of the tweets while all other datasets' tweets/texts aren't as heavily saturated with emojis). To test this hypothesis, I decided to train the same model, but with removing emojis in the preprocessing step and seeing how it affects accuracy.*","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.2\"> </a>\n### BERT-mini without emojis","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.2.1\"> </a>\n##### Define modified preprocessing function","metadata":{}},{"cell_type":"code","source":"def remove_emojis(sent):\n    text =  emoji.demojize(sent)\n    text= re.sub(r'(:[!_\\-\\w]+:)', '', text)\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Redefine the text_processing function to include the remove emojis step\ndef text_preprocessing_no_emojis(text):\n    \"\"\"\n    - Remove entity mentions (eg. '@united')\n    - Correct errors (eg. '&amp;' to '&')\n    @param    text (str): a string to be processed.\n    @return   text (Str): the processed string.\n    \"\"\"\n  \n    # Remove emojis\n    text = remove_emojis(text)\n\n    return text_preprocessing(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.2.2\"> </a>\n##### Preprocess and create data loaders","metadata":{}},{"cell_type":"code","source":"# Specify `MAX_LEN`\nMAX_LEN =  280\n\n# Print sentence 0 and its encoded token ids\ntoken_ids = list(preprocessing_for_bert([X[0]], text_preprocessing_fn=text_preprocessing_no_emojis)[0].squeeze().numpy())\nprint('Original: ', X[0])\nprint('Token IDs: ', token_ids)\n\n# Run function `preprocessing_for_bert` on the train set and the validation set\nprint('Tokenizing data...')\ntrain_inputs, train_masks = preprocessing_for_bert(X_train, text_preprocessing_fn=text_preprocessing_no_emojis)\nval_inputs, val_masks = preprocessing_for_bert(X_val, text_preprocessing_fn=text_preprocessing_no_emojis)\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n# Convert other data types to torch.Tensor\ntrain_labels = torch.tensor(y_train)\nval_labels = torch.tensor(y_val)\n\n# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\nbatch_size = 16\n\n# Create the DataLoader for our training set\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# Create the DataLoader for our validation set\nval_data = TensorDataset(val_inputs, val_masks, val_labels)\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.2.3\"> </a>\n##### Train","metadata":{}},{"cell_type":"code","source":"set_seed(42) \nbert_classifier, optimizer, scheduler = initialize_model(epochs=2)\ntrain(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*As hypothesized, it seems like the presence/absence of emojis can greatly affect model performance in terms of accuracy, given how the accuracy went from 0.90 to 0.79 after only removing emojis in the preprocessing step*","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.2.4\"> </a>\n##### Evaluate on test datasets","metadata":{}},{"cell_type":"code","source":"# Evaluate on the unseen test data\nevaluate_dataset(X_test, y_test,\"BERT-mini no emojis\", \"arabic-sentiment-twitter-corpus\", \"arabic-sentiment-twitter-corpus test\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate on the SS2030 Dataset\nevaluate_dataset(df_ss2030.tweet.values, df_ss2030.label.values,\"BERT-mini no emojis\", \"arabic-sentiment-twitter-corpus\", \"SS2030\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate on the 100k Arabic Reviews Dataset\nevaluate_dataset(df_reviews.tweet.values, df_reviews.label.values,\"BERT-mini no emojis\",\"arabic-sentiment-twitter-corpus\", \"100k Arabic Reviews\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate on the ArSAS Dataset\nevaluate_dataset(df_arsas.tweet.values, df_arsas.label.values,\"BERT-mini no emojis\", \"arabic-sentiment-twitter-corpus\", \"ArSAS\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.2.5\"> </a>\n##### Summary of performance on test datasets\n\n| Model | arabic-sentiment-twitter-corpus test subset | SS2030 | 100k reviews | ArSAS\n| :---: | :---: | :---: | :---: | :---: |\n| RandomForestClassifier | 0.798 | 0.554 | 0.587 | 0.660\n| BERT-mini | 0.900 | 0.639 | 0.599 | 0.691\n| BERT-mini without emojis | 0.785 | 0.628 | 0.632 | 0.663\n\n<center><i>numbers shown represent accuracy</i></center>","metadata":{}},{"cell_type":"markdown","source":"Compared to BERT-mini with emojis, BERT-mini without emojis' accuracy has dropped across all datasets. This, however, is expected since the model was learning from emojis, which is undesired behavior (we want a text sentiment classifier). Compared to the Random Forest Classifier, it seems like the BERT-mini without emojis' performance has slightly dropped on the `arabic-sentiment-twitter-corpus` test subset, but improved on the other test datasets (2, 3 and 4). Given that this version has generalized better on other datasets, let's try to see how BERT-base without emojis performs in comparison.  ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.3\"> </a>\n### BERT-base","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.3.1\"> </a>\n##### Preprocess and create data loaders","metadata":{}},{"cell_type":"code","source":"# Specify `MAX_LEN`\nMAX_LEN =  280\n\n# Print sentence 0 and its encoded token ids\ntoken_ids = list(preprocessing_for_bert([X[0]], version=\"base\", text_preprocessing_fn=text_preprocessing_no_emojis)[0].squeeze().numpy())\nprint('Original: ', X[0])\nprint('Token IDs: ', token_ids)\n\n# Run function `preprocessing_for_bert` on the train set and the validation set\nprint('Tokenizing data...')\ntrain_inputs, train_masks = preprocessing_for_bert(X_train, version=\"base\", text_preprocessing_fn=text_preprocessing_no_emojis)\nval_inputs, val_masks = preprocessing_for_bert(X_val, version=\"base\", text_preprocessing_fn=text_preprocessing_no_emojis)\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n# Convert other data types to torch.Tensor\ntrain_labels = torch.tensor(y_train)\nval_labels = torch.tensor(y_val)\n\n# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\nbatch_size = 16\n\n# Create the DataLoader for our training set\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# Create the DataLoader for our validation set\nval_data = TensorDataset(val_inputs, val_masks, val_labels)\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.3.2\"> </a>\n##### Train","metadata":{}},{"cell_type":"code","source":"set_seed(42) \nbert_classifier, optimizer, scheduler = initialize_model(epochs=2, version=\"base\")\ntrain(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.3.3\"> </a>\n##### Save trained model","metadata":{}},{"cell_type":"code","source":"import pickle\nfilename = 'trained_model_base_without_emojis.sav'\npickle.dump(bert_classifier, open(filename, 'wb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.3.4\"> </a>\n##### Evaluate on test datasets","metadata":{}},{"cell_type":"code","source":"# Evaluate on the unseen test data\nevaluate_dataset(X_test, y_test,\"BERT-base\", \"arabic-sentiment-twitter-corpus\", \"arabic-sentiment-twitter-corpus test\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate on the SS2030 Dataset\nevaluate_dataset(df_ss2030.tweet.values, df_ss2030.label.values,\"BERT-base\", \"arabic-sentiment-twitter-corpus\", \"SS2030\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate on the 100k Arabic Reviews Dataset\nevaluate_dataset(df_reviews.tweet.values, df_reviews.label.values,\"BERT-base\", \"arabic-sentiment-twitter-corpus\", \"100K Arabic Reviews\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate on the ArSAS Dataset\nevaluate_dataset(df_arsas.tweet.values, df_arsas.label.values,\"BERT-base\", \"arabic-sentiment-twitter-corpus\", \"ArSAS\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.3.5\"> </a>\n##### Summary of performance of test datasets\n| Model | arabic-sentiment-twitter-corpus test subset | SS2030 | 100k reviews | ArSAS\n| :---: | :---: | :---: | :---: | :---: |\n| RandomForestClassifier | 0.798 | 0.554 | 0.587 | 0.660\n| BERT-mini | 0.900 | 0.639 | 0.599 | 0.691\n| BERT-mini without emojis | 0.785 | 0.628 | 0.632 | 0.663\n| BERT-base without emojis | 0.803 |  0.652 | 0.652 | 0.699\n\n<center><i>numbers shown represent accuracy</i></center>","metadata":{}},{"cell_type":"markdown","source":"*It looks like the BERT-base slightly improved the overall performance on the unseen datasets, but it still appears that the model is unable to generalize well after being trained on the arabic-sentiment-twitter-corpus dataset. To overcome this, we will attempt to train the model on the other datasets, and see on how that reflects on the model's ability to generalize*","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.4\"> </a>\n### DL approach trained on other datasets","metadata":{}},{"cell_type":"code","source":"# Helper function that encapsulates all training logic\nfrom sklearn.model_selection import train_test_split\ndef train_val_test_split(df):\n    X = df.tweet.values\n    y = df.label.values\n\n    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=2020)\n    X_val, X_test, y_val, y_test = train_test_split(X_test,y_test,test_size=0.5, random_state=2020)\n    return X_train, X_val, X_test, y_train, y_val, y_test\ndef preprocess_and_train(X_train, X_val, y_train,y_val):\n\n    # Print sentence 0 and its encoded token ids\n    token_ids = list(preprocessing_for_bert([X_train[0]], text_preprocessing_fn=text_preprocessing_no_emojis)[0].squeeze().numpy())\n    print('Original: ', X_train[0])\n    print('Token IDs: ', token_ids)\n\n    # Run function `preprocessing_for_bert` on the train set and the validation set\n    print('Tokenizing data...')\n    train_inputs, train_masks = preprocessing_for_bert(X_train, text_preprocessing_fn=text_preprocessing_no_emojis)\n    val_inputs, val_masks = preprocessing_for_bert(X_val, text_preprocessing_fn=text_preprocessing_no_emojis)\n    from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n    # Convert other data types to torch.Tensor\n    train_labels = torch.tensor(y_train)\n    val_labels = torch.tensor(y_val)\n\n    # For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n    batch_size = 16\n\n    # Create the DataLoader for our training set\n    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n    train_sampler = RandomSampler(train_data)\n    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n    # Create the DataLoader for our validation set\n    val_data = TensorDataset(val_inputs, val_masks, val_labels)\n    val_sampler = SequentialSampler(val_data)\n    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n    set_seed(42) \n    bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n    train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)\n    return bert_classifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenate all of the tweets from arabic-sentiment-twitter-corpus to treat it as 1 test dataset in this section\ndf_twitter_corpus = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.4.1\"> </a>\n#### Training using the SS2030 Dataset","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.model_selection import train_test_split\nMAX_LEN = 280\nX = df_ss2030.tweet.values\ny = df_ss2030.label.values\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=2020)\nX_val, X_test, y_val, y_test = train_test_split(X_test,y_test,test_size=0.5, random_state=2020)\n\n\n# Print sentence 0 and its encoded token ids\ntoken_ids = list(preprocessing_for_bert([X_train[0]], text_preprocessing_fn=text_preprocessing_no_emojis)[0].squeeze().numpy())\nprint('Original: ', X_train[0])\nprint('Token IDs: ', token_ids)\n\n# Run function `preprocessing_for_bert` on the train set and the validation set\nprint('Tokenizing data...')\ntrain_inputs, train_masks = preprocessing_for_bert(X_train, text_preprocessing_fn=text_preprocessing_no_emojis)\nval_inputs, val_masks = preprocessing_for_bert(X_val, text_preprocessing_fn=text_preprocessing_no_emojis)\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n# Convert other data types to torch.Tensor\ntrain_labels = torch.tensor(y_train)\nval_labels = torch.tensor(y_val)\n\n# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\nbatch_size = 16\n\n# Create the DataLoader for our training set\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# Create the DataLoader for our validation set\nval_data = TensorDataset(val_inputs, val_masks, val_labels)\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\nset_seed(42) \nbert_classifier, optimizer, scheduler = initialize_model(epochs=2)\ntrain(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate on the unseen dataset\nevaluate_dataset(X_test, y_test,\"BERT-mini no emojis\", \"SS2030\", \"SS2030 test\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate on the twitter corpus dataset\nevaluate_dataset(df_twitter_corpus.tweet.values, df_twitter_corpus.label.values,\"BERT-mini no emojis\", \"SS2030\", \"arabic-sentiment-twitter-corpus\" )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate on the 100k Arabic Reviews Dataset\nevaluate_dataset(df_reviews.tweet.values, df_reviews.label.values,\"BERT-mini no emojis\", \"SS2030\", \"100K Arabic Reviews\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate on the ArSAS Dataset\nevaluate_dataset(df_arsas.tweet.values, df_arsas.label.values,\"BERT-mini no emojis\", \"SS2030\", \"ArSAS\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.4.2\"> </a>\n#### Train using the 100k Arabic Reviews dataset","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.model_selection import train_test_split\nMAX_LEN = 280\nX = df_reviews.tweet.values\ny = df_reviews.label.values\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=2020)\nX_val, X_test, y_val, y_test = train_test_split(X_test,y_test,test_size=0.5, random_state=2020)\n\n\n# Print sentence 0 and its encoded token ids\ntoken_ids = list(preprocessing_for_bert([X_train[0]], text_preprocessing_fn=text_preprocessing_no_emojis)[0].squeeze().numpy())\nprint('Original: ', X_train[0])\nprint('Token IDs: ', token_ids)\n\n# Run function `preprocessing_for_bert` on the train set and the validation set\nprint('Tokenizing data...')\ntrain_inputs, train_masks = preprocessing_for_bert(X_train, text_preprocessing_fn=text_preprocessing_no_emojis)\nval_inputs, val_masks = preprocessing_for_bert(X_val, text_preprocessing_fn=text_preprocessing_no_emojis)\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n# Convert other data types to torch.Tensor\ntrain_labels = torch.tensor(y_train)\nval_labels = torch.tensor(y_val)\n\n# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\nbatch_size = 16\n\n# Create the DataLoader for our training set\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# Create the DataLoader for our validation set\nval_data = TensorDataset(val_inputs, val_masks, val_labels)\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\nset_seed(42) \nbert_classifier, optimizer, scheduler = initialize_model(epochs=2)\ntrain(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate on unseen test dataset\nevaluate_dataset(X_test, y_test,\"BERT-mini no emojis\", \"100K Arabic Reviews\", \"100K Arabic Reviews test\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate on the twitter corpus dataset\nevaluate_dataset(df_twitter_corpus.tweet.values, df_twitter_corpus.label.values,\"BERT-mini no emojis\", \"100K Reviews\", \"arabic-sentiment-twitter-corpus\" )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate on the SS2030 Dataset\nevaluate_dataset(df_ss2030.tweet.values, df_ss2030.label.values,\"BERT-mini no emojis\", \"100K Reviews\", \"SS2030\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate on the ArSAS Dataset\nevaluate_dataset(df_arsas.tweet.values, df_arsas.label.values,\"BERT-mini no emojis\", \"100K Reviews\", \"ArSAS\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.4.3\"> </a>\n#### Train using the ArSAS Dataset","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.model_selection import train_test_split\nMAX_LEN = 280\nX = df_arsas.tweet.values\ny = df_arsas.label.values\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=2020)\nX_val, X_test, y_val, y_test = train_test_split(X_test,y_test,test_size=0.5, random_state=2020)\n\n\n# Print sentence 0 and its encoded token ids\ntoken_ids = list(preprocessing_for_bert([X_train[0]], text_preprocessing_fn=text_preprocessing_no_emojis)[0].squeeze().numpy())\nprint('Original: ', X_train[0])\nprint('Token IDs: ', token_ids)\n\n# Run function `preprocessing_for_bert` on the train set and the validation set\nprint('Tokenizing data...')\ntrain_inputs, train_masks = preprocessing_for_bert(X_train, text_preprocessing_fn=text_preprocessing_no_emojis)\nval_inputs, val_masks = preprocessing_for_bert(X_val, text_preprocessing_fn=text_preprocessing_no_emojis)\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n# Convert other data types to torch.Tensor\ntrain_labels = torch.tensor(y_train)\nval_labels = torch.tensor(y_val)\n\n# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\nbatch_size = 16\n\n# Create the DataLoader for our training set\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# Create the DataLoader for our validation set\nval_data = TensorDataset(val_inputs, val_masks, val_labels)\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\nset_seed(42) \nbert_classifier, optimizer, scheduler = initialize_model(epochs=2)\ntrain(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate on the twitter corpus dataset\nevaluate_dataset(X_test, y_test,\"BERT-mini no emojis\", \"ArSAS\", \"ArSAS test\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate on the twitter corpus dataset\nevaluate_dataset(df_twitter_corpus.tweet.values, df_twitter_corpus.label.values,\"BERT-mini no emojis\", \"ArSAS\", \"arabic-sentiment-twitter-corpus\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate on the SS2030 Dataset\nevaluate_dataset(df_ss2030.tweet.values, df_ss2030.label.values,\"BERT-mini no emojis\", \"ArSAS\", \"SS2030\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate on the 100k reviews Dataset\nevaluate_dataset(df_reviews.tweet.values, df_reviews.label.values,\"BERT-mini no emojis\", \"ArSAS\", \"100K Arabic Reviews\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.4.4\"> </a>\n#### Summary of performance on test datasets\n\n\n| training dataset | test subset accuracy |  arabic-sentiment-twitter-corpus accuracy | SS2030 accuracy | 100k reviews accuracy | ArSAS accuracy\n| :---: | :---: | :---: | :---: | :---: | :---: |\n| arabic-sentiment-twitter-corpus | 0.785 | - | 0.579 | 0.570 | 0.697\n| SS2030 | 0.847 | 0.492 | - | 0.502 | 0.628\n| 100k reviews | 0.885 | 0.585 | 0.626 | - | 0.547\n| ArSAS | 0.879 | 0.616 | 0.641 | 0.641 | - |\n\n<center><i>numbers shown represent accuracy</i></center>","metadata":{}},{"cell_type":"markdown","source":"**The model trained on the ArSAS dataset appears to be generalizing better on the other test datasets (higher accuracy). Given that promising result, let's try using ArSAS to train bert-base**","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.4.5\"> </a>\n#### Train BERT-base Using the ArSAS Dataset","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.model_selection import train_test_split\nMAX_LEN = 280\nX = df_arsas.tweet.values\ny = df_arsas.label.values\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=2020)\nX_val, X_test, y_val, y_test = train_test_split(X_test,y_test,test_size=0.5, random_state=2020)\n\n\n# Print sentence 0 and its encoded token ids\ntoken_ids = list(preprocessing_for_bert([X_train[0]], version=\"base\", text_preprocessing_fn=text_preprocessing_no_emojis)[0].squeeze().numpy())\nprint('Original: ', X_train[0])\nprint('Token IDs: ', token_ids)\n\n# Run function `preprocessing_for_bert` on the train set and the validation set\nprint('Tokenizing data...')\ntrain_inputs, train_masks = preprocessing_for_bert(X_train, version=\"base\", text_preprocessing_fn=text_preprocessing_no_emojis)\nval_inputs, val_masks = preprocessing_for_bert(X_val, version=\"base\", text_preprocessing_fn=text_preprocessing_no_emojis)\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n# Convert other data types to torch.Tensor\ntrain_labels = torch.tensor(y_train)\nval_labels = torch.tensor(y_val)\n\n# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\nbatch_size = 16\n\n# Create the DataLoader for our training set\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# Create the DataLoader for our validation set\nval_data = TensorDataset(val_inputs, val_masks, val_labels)\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\nset_seed(42) \nbert_classifier, optimizer, scheduler = initialize_model(epochs=2, version=\"base\")\ntrain(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate on the ArSAS test set\nevaluate_dataset(X_test, y_test,\"BERT-base no emojis\", \"ArSAS\", \"ArSAS test\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate on the twitter corpus dataset\nevaluate_dataset(df_twitter_corpus.tweet.values, df_twitter_corpus.label.values,\"BERT-base no emojis\", \"ArSAS\", \"arabic-sentiment-twitter-corpus\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate on the SS2030 Dataset\nevaluate_dataset(df_ss2030.tweet.values, df_ss2030.label.values,\"BERT-base no emojis\", \"ArSAS\", \"SS2030\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate on the 100k reviews Dataset\nevaluate_dataset(df_reviews.tweet.values, df_reviews.label.values,\"BERT-base no emojis\", \"ArSAS\", \"100K Arabic Reviews\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As expected, BERT-base (no emojis) trained on ArSAS yields the best results yet in terms of generalizing on unseen test datasets","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3\"> </a>\n# Final summary of all experiments\n\n| model | with emojis| training dataset  |  arabic-sentiment-twitter-corpus accuracy | SS2030 accuracy | 100k reviews accuracy | ArSAS accuracy \n| :---: | :---: | :---: |  :---: | :---: | :---: | :---: |\n| RandomForestClassifier | Yes | arabic-sentiment-twitter-corpus | *0.798*| 0.550 | 0.585 | 0.659 \n| BERT-mini| Yes | arabic-sentiment-twitter-corpus | *0.900*| 0.639 | 0.599 | 0.691 \n| BERT-mini| No | arabic-sentiment-twitter-corpus | *0.785* | 0.579 | 0.570 | **0.697** \n| BERT-base | No | arabic-sentiment-twitter-corpus | *0.803*  |  0.652| 0.652| **0.699** \n| BERT-mini| No | SS2030  | 0.492 | *0.847* | 0.502 | 0.628 \n| BERT-mini| No | 100k reviews  | 0.585 | 0.626 | *0.885* | 0.547 \n| BERT-mini| No | ArSAS | **0.616** | **0.641** | **0.641** | *0.879*\n| BERT-base | No | ArSAS | ***0.648*** | ***0.679*** | ***0.741*** |*0.899*\n\n<center><i>numbers shown represent accuracy</i></center>\n","metadata":{}},{"cell_type":"markdown","source":"**Notes:**\n* *Italic numbers on the diagonal represent accuracies of the unseen test subsets (same dataset as training set)*\n* *Bold numbers represent the highest BERT-base and BERT-mini accuracies for each external dataset/column (excluding the test subset of training dataset)*","metadata":{}},{"cell_type":"markdown","source":"### Summary & Conclusion","metadata":{}},{"cell_type":"markdown","source":"#### Summary of experiments\nIn this notebook my goal was to train an Arabic sentiment analysis classifier that is robust and has consistent performance regardless of the dataset used to evaluate it. Here's a summary of the experiments I've done:\n- I first tried the classic ML approach and found that while it has a fast training time and good performance (measured by accuracy score) on the test subset of the dataset it's trained on, its performance significantly dropped when evaluated on other datasets.\n- Then I tried a finetuning approach on a DL model that is pretrained on a very large corpus of Arabic text. The first model I tried in this category was a BERT-mini model that *did not* discard emojis in its preprocessing step. Similarly to the classical ML approach, this model performed well on the test subset of the dataset it's trained on, but failed to generalize on the other test datasets.\n- I attempted a version of the same model that removes emojis in its preprocessing step. This caused the accuracy scores to drop both on the test subset and the other test datasets. This tells us that the model had been using emojis as sentiment cues. This is an undesired behavior because we want a model that infers sentiment from Arabic text, not from emojis. \n- The next step was to change the model to BERT-base which is a more complex version that has 10x more parameters than BERT-mini. This improved the performance on the ss2030 and 100k reviews datasets, but the accuracy on ArSAS didn't budge as much.\n- After trying different versions of the model, changing the training dataset seemed like a logical next experiment. The dataset that showed the best performance improvement in terms of accuracy on unseen datasets, was shown to be ArSAS. \n- Given that result, I next trained a BERT-base model using ArSAS, and this version ended up outperforming the BERT-base model trained on the arabic-sentiment-twitter-corpus.\n\n#### Conclusion:\n- Out of the different model/training dataset combinations I've tried in this notebook, BERT-base trained on ArSAS proved to be the best one for the task of Arabic text Sentiment analysis.\n- Even though the datasets all (except for the 100k Reviews Dataset) consist of dialectical Arabic tweets, they seem to have intrinsic differences in terms of topics and vocabulary, this is discussed in more details in the companion dataset analysis [notebook](https://www.kaggle.com/yasmeenhany/dataset-analysis). This makes it hard for a model trained on one to generalize well on others. ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}