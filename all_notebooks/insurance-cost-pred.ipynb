{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Prediction of Premium Charges based on Health factors","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The objective is to predict the premium based on bmi,age,sex,region and smoking status of the individuals.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np  \nimport pandas as pd  \nfrom sklearn.preprocessing import LabelEncoder\nimport seaborn as sns\nfrom scipy import stats\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split,KFold,learning_curve\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/insurance/insurance.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Wrangling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = df[['age','bmi','children','charges']]\ncols.isnull()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = df[['sex','smoker','region']]\ncols.isna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### label encoding for catgorical variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\ndf1 = df[['sex','smoker','region']]\ndf[['sex','smoker','region']] = df1.apply(lambda x: le.fit_transform(x))\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df3 = df.corr(method='pearson').round(2)\ndf3.loc[df3.index == 'charges']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above correlation values, it is observed that smoker feature alone has very good correlation with charges. While other features have very moderate or poor correlation values. However, age,bmi,children,sex are important features which could play a vital role in predicting the premium charges.\nHence, in order to consider these features, outlier presence has to be checked.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Outlier Detection via box plots","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=df['age'],y=df['charges'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=df['bmi'],y=df['charges'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=df['children'],y=df['charges'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=df['sex'],y=df['charges'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=df['smoker'],y=df['charges'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=df['region'],y=df['charges'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Upon observing the plot for each of the features, there is few or more outliers for each of the features. Hence, Outliers have to be minimised in order to avoid overfitting of the data model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Outlier Elimination based on Z-Score normalisation(Kurtosis)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Identifying Kurtosis to eradicate heavy tail records","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df4 = df[['age','bmi','children','charges','sex','smoker','region']]\nz_score = stats.zscore(df4)\nsns.distplot(z_score,hist=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.where(z_score>3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df4 = df4[(z_score<3).all(axis=1)]\ndf4.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(df4['age'],df4['charges'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Outlier elimination based on Interquartile range from upper and lower extremes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Q1 = df4.quantile(0.25)\nQ3 = df4.quantile(0.75)\nIQR = Q3-Q1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df5 = df4[~((df4 < (Q1-1.5*IQR))| (df4 > (Q3+1.5*IQR))).any(axis=1)]\ndf5.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(df5['age'],df5['charges'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By applying IQR range, the amount of outliers have decreased considerably.However,eradication of the outliers completely may not be a good practice. Since it might result in eradicating most of the records in the dataset.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Model Development and Evaluation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Since the objective of the dataset is not a classification type, a regression model can be used. Hence, the learning curve of this two model has to be visualised to look for overfitting or underfitting cases.Adding on this , the evaluation metric of each model is observed to get the suitable model. Below models are to be checked.\n1. XGBoost Regressor\n2. Random Forest Regressor\n3. Linear Regression\n4. Lasso Regression\n5. Ridge Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_curve(estimator,x,y,cv= KFold(),m=np.linspace(0.5,1,5)):\n                        \n                            size,score_train,score_test= learning_curve(estimator,x,y,train_sizes=m)\n                            mean_train = np.mean(score_train,axis=1)\n                            mean_test = np.mean(score_test,axis=1)\n                            std_train = np.std(score_train,axis=1)\n                            std_test = np.std(score_test,axis=1)\n                            plt.fill_between(size,mean_train - std_train,mean_train + std_train,alpha=0.1)\n                            plt.fill_between(size,mean_test - std_test,mean_test + std_test,alpha=0.1)\n                            plt.plot(size,mean_train,label='Training samples')\n                            plt.plot(size,mean_test,label='Cross-Validation set')\n                            plt.xlabel('Training samples')\n                            plt.ylabel('Error')\n                            plt.legend()\n                            plt.title('Learning curve')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prediction(model):\n                        x = df5[['age','bmi','children','sex','smoker','region']]\n                        y = df5['charges']\n                        x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)\n                        \n                        model = model.fit(x_train,y_train)\n                        yhat = model.predict(x_test)\n                        df_pred = x_test\n                        df_pred['charges'] = yhat\n                        df_pred['Actual values'] = y_test\n                        from sklearn.metrics import accuracy_score,mean_squared_error,r2_score,mean_absolute_error\n                        print('MSE:',mean_squared_error(y_test,yhat))\n                        print('R2:',r2_score(y_test,yhat))\n                        print('MAE:',mean_absolute_error(y_test,yhat))\n                        return df_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost Regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nre = XGBRegressor(reg_alpha=0.9)\nplot_curve(re,x,y)\nprediction(re)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nrtree = RandomForestRegressor()\nplot_curve(rtree,x,y)\nprediction(rtree)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lasso Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nl = Lasso(alpha=0.1)\nplot_curve(l,x,y)\nprediction(l)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlr = LinearRegression()\nplot_curve(lr,x,y)\nprediction(lr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ridge Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nr = Ridge(alpha=0.1)\nplot_curve(r,x,y)\nprediction(r)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Inference","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Based on the regression model's learning curve and prediction, below insights are enlisted.\n\n* **XGBoost Regressor**: This model has very less r2 score of 0.12 and the learning curve shows a hint of high variance/bias. Hence, this model may not be suitable.\n* **Random Forest Regressor** : This model has has better r2 score compared to the previous model. But the learning curve shows deviation. hence, this model is also not suitable.\n* **Ridge Regression**: This model has r2 score of 0.42 and the learning curve shows low bias and variance.\n* **Lasso Regression**: This model has r2 score of 0.44 and the learning curve shows low bias and variance.\n* **Linear Regression**: This model has r2 score of 0.26 and the learning curve shows low bias and variance.\n\nComparing the Ridge,Lasso and Linear models:\n* Linear model has less r2 score compared to other two. Hence, this can be eliminated.\n\nComparing Ridge and Lasso:\n* Lasso has better r2 score than Ridge. But when comparing the predicted values of both these models, due to the supressing behaviour of Lasso model, some values are predicted too less than the actual value. While in Ridge, there is a considerable match with the actual value.\n                    \nHence, Ridge Regression model is chosen for the prediction of charges.\n\n***Note: The MSE and MAE values of all the models are too high due to the presence of few outliers. Since eliminating all the outliers might result in removing most of the records, this error is considered as an exception***","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}