{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Clickbait detector using Naive Bayes Classifier\n\nThis kernel focuses on classifying News headlines into clickbaits and non-clickbaits.\n\nThe clickbaits are labelled as **1** and non-clickbaits as **0**.\nThe headlines are collected from different news sites.\n\nThe dataset consists of 32000 headlines of which 50% are clickbaits and the other 50% are non-clickbait.\n\nI have used a *Multinomial Naive Bayes* classification algorithm for text classification of the given dataset. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Importing different tools and libraries\n\nThe main libraries used are *Numpy*, *Pandas*, *NLTK*(Natural language toolkit) and *Scikit-learn*.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.probability import FreqDist\nimport string as s\nimport re\n\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading the Dataset","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"cb_data= pd.read_csv('/kaggle/input/clickbait-dataset/clickbait_data.csv')\ncb_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting into Train and Test sets\n\nThe dataset is splitted into training and testing sets. The percentage of training data is 75% and testing data is 25%.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x=cb_data.headline\ny=cb_data.clickbait\ntrain_x,test_x,train_y,test_y=train_test_split(x,y,test_size=0.25,random_state=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analyzing Train and Test Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"No. of elements in training set\")\nprint(train_x.size)\nprint(\"No. of elements in testing set\")\nprint(test_x.size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_y.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_x.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_y.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenization of Data\n\nThe data is tokenized i.e. split into tokens which are the smallest or minimal meaningful units. The data is split into words.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenization(text):\n    lst=text.split()\n    return lst\ntrain_x=train_x.apply(tokenization)\ntest_x=test_x.apply(tokenization)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Converting to lowercase\n\nThe data is converted into lowercase to avoid ambiguity between same words in different cases like 'NLP', 'nlp' or 'Nlp'. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def lowercasing(lst):\n    new_lst=[]\n    for i in lst:\n        i=i.lower()\n        new_lst.append(i)\n    return new_lst\ntrain_x=train_x.apply(lowercasing)\ntest_x=test_x.apply(lowercasing)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Removing punctuation\n\nThe punctuations are removed to increase the efficiency of the model. They are irrelevant because they provide no added information.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_punctuations(lst):\n    new_lst=[]\n    for i in lst:\n        for j in s.punctuation:\n            i=i.replace(j,'')\n        new_lst.append(i)\n    return new_lst\ntrain_x=train_x.apply(remove_punctuations)\ntest_x=test_x.apply(remove_punctuations)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Removing Numbers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_numbers(lst):\n    nodig_lst=[]\n    new_lst=[]\n    for i in lst:\n        for j in s.digits:    \n            i=i.replace(j,'')\n        nodig_lst.append(i)\n    for i in nodig_lst:\n        if i!='':\n            new_lst.append(i)\n    return new_lst\ntrain_x=train_x.apply(remove_numbers)\ntest_x=test_x.apply(remove_numbers)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Removing Stopwords","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"All stopwords of English language \")\n\", \".join(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stopwords(lst):\n    stop=stopwords.words('english')\n    new_lst=[]\n    for i in lst:\n        if i not in stop:\n            new_lst.append(i)\n    return new_lst\n\ntrain_x=train_x.apply(remove_stopwords)\ntest_x=test_x.apply(remove_stopwords)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Removing extra spaces","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_spaces(lst):\n    new_lst=[]\n    for i in lst:\n        i=i.strip()\n        new_lst.append(i)\n    return new_lst\ntrain_x=train_x.apply(remove_spaces)\ntest_x=test_x.apply(remove_spaces)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analyzing data after preprocessing\n\nAfter preprocessing the data i.e. after removing punctuation, stopwords, spaces and numbers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_x.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lemmatization\n\nLemmatization in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form. It involves the morphological analysis of words.\n\nIn lemmatization we find the root word or base form of the word rather than just clipping some characters from the end e.g. *is, are, am* are all converted to its base form *be* in Lemmatization\n\nHere lemmatization is done using NLTK library.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmatizer=nltk.stem.WordNetLemmatizer()\ndef lemmatzation(lst):\n    new_lst=[]\n    for i in lst:\n        i=lemmatizer.lemmatize(i)\n        new_lst.append(i)\n    return new_lst\ntrain_x=train_x.apply(lemmatzation)\ntest_x=test_x.apply(lemmatzation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x=train_x.apply(lambda x: ''.join(i+' ' for i in x))\ntest_x=test_x.apply(lambda x: ''.join(i+' ' for i in x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq_dist={}\nfor i in train_x.head(20):\n    x=i.split()\n    for j in x:\n        if j not in freq_dist.keys():\n            freq_dist[j]=1\n        else:\n            freq_dist[j]+=1\nfreq_dist","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TF-IDF (Term frequency-Inverse Data Frequency)\n\nThis method is used to convert the text into features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf=TfidfVectorizer()\ntrain_1=tfidf.fit_transform(train_x)\ntest_1=tfidf.transform(test_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of features extracted\")\nprint(len(tfidf.get_feature_names()))\nprint()\nprint(\"The 100 features extracted from TF-IDF \")\nprint(tfidf.get_feature_names()[:100])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of train set\",train_1.shape)\nprint(\"Shape of test set\",test_1.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_arr=train_1.toarray()\ntest_arr=test_1.toarray()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define Naive Bayes Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"NB_MN=MultinomialNB()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"NB_MN.fit(train_arr,train_y)\npred=NB_MN.predict(test_arr)\nprint('first 20 actual labels: ',test_y.tolist()[:20])\nprint('first 20 predicted labels: ',pred.tolist()[:20])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation of Result\n\nThe Accuracy and F1 score of the model are printed to evaluate the model for text classification.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score,accuracy_score\nprint(\"F1 score of the model\")\nprint(f1_score(test_y,pred))\nprint(\"Accuracy of the model\")\nprint(accuracy_score(test_y,pred))\nprint(\"Accuracy of the model in percentage\")\nprint(accuracy_score(test_y,pred)*100,\"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(test_y,pred))\n\nfrom sklearn.metrics import classification_report\nprint(\"Classification Report\")\nprint(classification_report(test_y,pred))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## A second model for TF-IDF with different n-grams and fixed feature size","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf=TfidfVectorizer(ngram_range=(1,3),max_features=6500)\ntrain_2=tfidf.fit_transform(train_x)\ntest_2=tfidf.transform(test_x)\n\nNB_MN.fit(train_2.toarray(),train_y)\npred2=NB_MN.predict(test_2.toarray())\n\nprint(\"Accuracy of the model in percentage\")\nprint(accuracy_score(test_y,pred2)*100,\"%\")\n\nfrom sklearn.metrics import confusion_matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(test_y,pred2))\n\nfrom sklearn.metrics import classification_report\nprint(\"Classification Report\")\nprint(classification_report(test_y,pred2))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}