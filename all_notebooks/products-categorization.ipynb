{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nfrom sklearn import model_selection, preprocessing\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nnp.random.seed(123) #for reprodicible results\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"filename = '/kaggle/input/walmart-product-dataset-usa/walmart_com-ecommerce_product_details.csv'\ndata = pd.read_csv(filename)\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_category(full_cat):\n    if '|' in str(full_cat):\n        return full_cat.split('|')[0]\n    return 'NA'\n    \ndata['General_Category'] = data.apply(lambda x: get_category(x['Category']), axis=1)\n\ndata['General_Category'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_categories = data['General_Category'].value_counts()[:10].index.tolist()\nselected_categories","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[data['General_Category'].isin(selected_categories)]\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[~data['Description'].isnull()]\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split the dataset into training and test datasets \ntrain_x, test_x, train_y, test_y = model_selection.train_test_split(data['Description'], data['General_Category'])\n\n# label encode the target variable, encode labels to 0, 1, 2\nencoder = preprocessing.LabelEncoder()\ntrain_y = encoder.fit_transform(train_y)\ntest_y = encoder.fit_transform(test_y)\n\ncategories_df = pd.DataFrame({\"category\": selected_categories}, index=encoder.transform(selected_categories))\ncategories_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nstopwords = set(stopwords.words('english'))\nstopwords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# word level tf-idf\ntfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', stop_words = stopwords,\n                             max_features=5000, ngram_range=(1,2))\ntfidf_vect.fit(data['Description'])\n\n# output of tfidf transform is sparse matrix, which is not allow us to apply normal matrix calculation\n# => we need to convert to normal matrix\nxtrain_tfidf =  tfidf_vect.transform(train_x).toarray()\nxtest_tfidf =  tfidf_vect.transform(test_x).toarray()\n\n# Getting transformed training and testing dataset\nprint('Number of training documents: %s' %str(xtrain_tfidf.shape[0]))\nprint('Number of testing documents: %s' %str(xtest_tfidf.shape[0]))\nprint('Number of features of each document: %s' %str(xtrain_tfidf.shape[1]))\nprint('xtrain_tfidf shape: %s' %str(xtrain_tfidf.shape))\nprint('train_y shape: %s' %str(train_y.shape))\nprint('xtest_tfidf shape: %s' %str(xtest_tfidf.shape))\nprint('test_y shape: %s' %str(test_y.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def top_tfidf_feats(row, features, top_n=10):\n    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n    topn_ids = np.argsort(row)[::-1][:top_n]\n    top_feats = [(features[i], row[i]) for i in topn_ids]\n    df = pd.DataFrame(top_feats)\n    df.columns = ['feature', 'tfidf']\n    return df\n\ndef top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=10):\n    ''' Return the top n features that on average are most important amongst documents in rows\n        indentified by indices in grp_ids. '''\n    if grp_ids:\n        D = Xtr[grp_ids]\n    else:\n        D = Xtr\n\n    D[D < min_tfidf] = 0\n    tfidf_means = np.mean(D, axis=0)\n    return top_tfidf_feats(tfidf_means, features, top_n)\n\ndef top_feats_by_topic(Xtr, y, topics_df, features, min_tfidf=0.1, top_n=10):\n    ''' Return a list of dfs, where each df holds top_n features and their mean tfidf value\n        calculated across documents with the same topic. '''\n    dfs = pd.DataFrame(index = range(0,top_n))\n    for i in topics_df.index:\n        ids = np.where(y==i)\n        feats_df = top_mean_feats(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n        dfs[topics_df.loc[i,][0]] = feats_df[\"feature\"]\n    return dfs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_vect.inverse_transform(xtrain_tfidf[13])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import linear_model\nfrom sklearn.metrics import accuracy_score\n\n# train\nlogreg = linear_model.LogisticRegression(C=1e5, \n        solver = 'sag', multi_class = 'multinomial')\nlogreg.fit(xtrain_tfidf, train_y)\n\n# test\ny_pred = logreg.predict(xtest_tfidf)\nprint(\"Accuracy: %.2f %%\" %(100*accuracy_score(test_y, y_pred.tolist())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_test = [\"\"\"Ginkgo Biloba Leaf Extract - Naturally Supports Brain, Nervous System and Memory\n120 MG Extract Per Serving, 60 Servings, 60 Capsules - By Revive Herbs\n\nGinkgo Biloba Leaf Extract Naturally Supports Brain, Nervous System and Memory Made Using High Quality Ingredients Unconditional 90-Day Return Policy\n\nREVIVE HERBS has been founded with one principle in mind -\nwe want to do our part in the revolution that is taking place. We\nare referring to the revolution in the use of herbs and other natural\ningredients. Allopathic medicine has its place, but in many cases,\nhave side effects. Compared to that products with natural products\nhave been increasing shown in clinical studies to produce similar\nor better results without the associated side effects. We care about\nour customers, humanity and our earth. A part of our profits goes to\ncharities to help the under privileged lead a healthy and better life.\n\nTAKE ACTION NOW: We encourage you to take control of your health and\ngive our product a chance. We promise we will do our best to support\nyou in your journey to a better health.\"\"\",\n                \n\"\"\"AmazonBasics Glass Electric Kettle\nHave hot water ready in an instant with the AmazonBasics Glass Electric Kettle. This 1.7 liter, 1500 watt kettle quickly brings water to a boil, allowing you to make herbal tea, cocoa, French press coffee, and other hot beverages in a fraction of the time. Perfect for serving friends, family, or yourself, the kettle smoothly detaches from its heating base for cord-free convenience. Enjoy a hot beverage minus the fuss with this modern, space-saving glass kettle.\"\"\",\n\"\"\"The Russell Athletic Menâ€™s Essential Tee delivers the comfort, style, and performance to fit your active lifestyle. This t-shirt features our signature Dri-Power moisture wicking technology, odor protection to keep the fabric fresh, and a UPF 30+ rating to protect you from harmful UV rays. This tee is a wardrobe essential, offering both the style and comfort of cotton with the benefits of performance.\n\"\"\"]\n\nx_tfidf =  tfidf_vect.transform(my_test).toarray()\n\ny_log = logreg.predict(x_tfidf)\nprint('prediction of logistic regression with SAG: {}'.format([categories_df.loc[i,][0] for i in y_log.tolist()]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train\nlogreg = linear_model.LogisticRegression(C=1e5, \n        solver = 'sag', multi_class = 'multinomial')\nlogreg.fit(xtrain_tfidf, train_y)\n\n# test\ny_pred = logreg.predict(xtest_tfidf)\nprint(\"Accuracy: %.2f %%\" %(100*accuracy_score(test_y, y_pred.tolist())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=17)\nforest.fit(xtrain_tfidf, train_y)\n# test\ny_pred = forest.predict(xtest_tfidf)\nprint(\"Accuracy: %.2f %%\" %(100*accuracy_score(test_y, y_pred.tolist())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=10)\nknn.fit(xtrain_tfidf, train_y)\n# test\ny_pred = knn.predict(xtest_tfidf)\nprint(\"Accuracy: %.2f %%\" %(100*accuracy_score(test_y, y_pred.tolist())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}