{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv(r\"../input/phishing-website-detector/phishing.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X= df.drop(columns='class')\nY=df['class']\nY=pd.DataFrame(Y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check if data is blanced (Legit vs Phising)","metadata":{}},{"cell_type":"code","source":"import seaborn as sns # data visualization library  \nimport matplotlib.pyplot as plt\npd.value_counts(Y['class']).plot.bar()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#correlation map\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(X.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> # Recursive feature elimination\nresource: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.preprocessing import MinMaxScaler\nX_norm = MinMaxScaler().fit_transform(X)\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nrfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=10, step=10, verbose=5)\nrfe_selector.fit(X_norm, Y)\nrfe_support = rfe_selector.get_support()\nrfe_feature = X.loc[:,rfe_support].columns.tolist()\nprint(str(len(rfe_feature)), 'selected features')\nprint(rfe_feature)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CHI2\nhttps://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.preprocessing import MinMaxScaler\nchi_selector = SelectKBest(chi2, k=10)\nchi_selector.fit(X_norm, Y)\nchi_support = chi_selector.get_support()\nchi_feature = X.loc[:,chi_support].columns.tolist()\nprint(str(len(chi_feature)), 'selected features')\nprint(chi_feature)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Embeded\n## Logistic Regression\nhttps://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LogisticRegression\nembeded_lr_selector = SelectFromModel(LogisticRegression(penalty=\"l2\"), '1.25*median')\nembeded_lr_selector.fit(X_norm, Y)\nembeded_lr_support = embeded_lr_selector.get_support()\nembeded_lr_feature = X.loc[:,embeded_lr_support].columns.tolist()\nprint(str(len(embeded_lr_feature)), 'selected features')\nprint(embeded_lr_feature)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Extraction\n## PCA\nhttp://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html \nWe will use principle component analysis (PCA) for feature extraction. Before PCA, we need to normalize data for better performance of PCA.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_norm = scaler.fit_transform(X)\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=25)\nY_sklearn = pca.fit_transform(X_norm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cum_sum = pca.explained_variance_ratio_.cumsum()\n\npca.explained_variance_ratio_[:10].sum()\n\ncum_sum = cum_sum*100\n\nfig, ax = plt.subplots(figsize=(8,8))\nplt.yticks(np.arange(0,110,10))\nplt.bar(range(25), cum_sum, label='Cumulative _Sum_of_Explained _Varaince', color = 'b',alpha=0.5)\nplt.title(\"Around 95% of variance is explained by the First 25 colmns \");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"explained_variance=pca.explained_variance_ratio_\nprint(explained_variance.shape)\nprint(explained_variance.sum())\nwith plt.style.context('dark_background'):\n    plt.figure(figsize=(6, 4))\n\n    plt.bar(range(25), explained_variance, alpha=0.5, align='center',\n            label='individual explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.legend(loc='best')\n    plt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SVD","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\nsvd = TruncatedSVD(n_components=20, n_iter=50, random_state=42)\nsvd.fit(X_norm)\nexplained_variance=svd.explained_variance_ratio_\nprint(explained_variance.shape)\nprint(svd.explained_variance_ratio_.sum())\nwith plt.style.context('dark_background'):\n    plt.figure(figsize=(6, 4))\n\n    plt.bar(range(20), explained_variance, alpha=0.5, align='center',\n            label='individual explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.legend(loc='best')\n    plt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}