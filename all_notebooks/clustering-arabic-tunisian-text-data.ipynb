{"cells":[{"metadata":{"_uuid":"bb71c155-522c-432c-a76a-c8498a9e12cd","_cell_guid":"0a35245b-29d7-4d54-9339-08f8e0ef9a4c","trusted":true},"cell_type":"code","source":"import collections\nimport random\nimport re\nfrom collections import Counter\nfrom itertools import islice\nimport nltk\nfrom nltk.corpus import stopwords\nimport numpy as np \nimport pandas as pd \npd.set_option('display.max_colwidth', -1)\nfrom time import time\nimport re\nimport string\nimport os\nimport emoji\nfrom pprint import pprint\nimport collections\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\nsns.set(font_scale=1.3)\nfrom sklearn.metrics import f1_score\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.metrics import classification_report\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\n\nimport gensim\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nimport warnings\nwarnings.filterwarnings('ignore')\nnp.random.seed(37)                             \nfrom gensim.models import Word2Vec\n  \nfrom nltk.cluster import KMeansClusterer\nimport nltk\nimport numpy as np \n  \nfrom sklearn import cluster\nfrom sklearn import metrics                           \narabic_diacritics = re.compile(\"\"\" ّ    | # Tashdid\n                             َ    | # Fatha\n                             ً    | # Tanwin Fath\n                             ُ    | # Damma\n                             ٌ    | # Tanwin Damm\n                             ِ    | # Kasra\n                             ٍ    | # Tanwin Kasr\n                             ْ    | # Sukun\n                             ـ     # Tatwil/Kashida\n                         \"\"\", re.VERBOSE)\n\n\ndef remove_diacritics(text):\n    text = re.sub(arabic_diacritics, '', str(text))\n    return text\n\n\n\ndef remove_repeating_char(text):\n    # return re.sub(r'(.)\\1+', r'\\1', text)     # keep only 1 repeat\n    return re.sub(r'(.)\\1+', r'\\1\\1', text)  # keep 2 repeat\n\ndef process_text(text, grams=False):\n    clean_text = remove_diacritics(text)\n    clean_text = remove_repeating_char(clean_text)\n    if grams is False:\n        return clean_text.split()\n    else:\n        tokens = clean_text.split()\n        grams = list(window(tokens))\n        grams = [' '.join(g) for g in grams]\n        grams = grams + tokens\n        return grams\n\n\ndef window(words_seq, n=2):\n    \"Returns a sliding window (of width n) over data from the iterable\"\n    \"   s -> (s0,s1,...s[n-1]), (s1,s2,...,sn), ...                   \"\n    it = iter(words_seq)\n    result = tuple(islice(it, n))\n    if len(result) == n:\n        yield result\n    for elem in it:\n        result = result[1:] + (elem,)\n        yield result\ndef document_features(document, corpus_features):\n    document_words = set(document)\n    features = {}\n    for word in corpus_features:\n        features['contains({})'.format(word)] = (word in document_words)\n    return features\n\n\nall_features = list()\ntexts = list()\ndata_labels = list()\n\nnegative_file = open(\"../input/ts-naim-mhedhbi/negative_tweets.txt\", encoding =\"utf8\")\npositive_file = open(\"../input/ts-naim-mhedhbi/positive_tweets.txt\", encoding =\"utf8\")\n\nn_grams_flag = False\nmin_freq = 13\n\nprint('read data ...')\nprint('read data ...')\n# read positive data\nfor line in positive_file:\n    \n    text_features = process_text(line, grams=n_grams_flag)\n    stop_words = set(stopwords.words('arabic'))\n    text_features = [w for w in text_features if not w in stop_words]\n    all_features += text_features\n    texts.append(text_features)\n    data_labels.append('pos')\n\nfor line in negative_file:\n    \n    text_features = process_text(line, grams=n_grams_flag)\n    stop_words = set(stopwords.words('arabic'))\n    text_features = [w for w in text_features if not w in stop_words]\n    all_features += text_features\n    texts.append(text_features)\n    data_labels.append('neg')\n\n \ndf = pd.DataFrame(list(zip(texts, data_labels)),\n              columns=['texts','data_labels'])\n\nfrom gensim.models import Word2Vec\nmodel = Word2Vec(df['texts'], min_count=10)\n\nprint (model.similarity('الصحة', 'الناس'))\nprint (model.most_similar(positive=['تونس'], negative=[], topn=10))\nprint (model['الصحة'])\nprint (list(model.wv.vocab))\n\nprint (len(list(model.wv.vocab)))\n\nX = model[model.wv.vocab]\nfrom nltk.cluster import KMeansClusterer\nimport nltk\nNUM_CLUSTERS=5\nkclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25)\nassigned_clusters = kclusterer.cluster(X, assign_clusters=True)\nprint (assigned_clusters)\nwords = list(model.wv.vocab)\nfor i, word in enumerate(words):\n    words_clusters = word + \":\" + str(assigned_clusters[i])\n    print (words_clusters)\n    \n\n          \nkmeans = cluster.KMeans(n_clusters=NUM_CLUSTERS)\nkmeans.fit(X)\n  \nlabels = kmeans.labels_\ncentroids = kmeans.cluster_centers_    \n\nprint (\"Cluster id labels for inputted data\")\nprint (labels)\nprint (\"Centroids data\")\nprint (centroids)\nprint (\"Score (Opposite of the value of X on the K-means objective which is Sum of distances of samples to their closest cluster center):\")\nprint (kmeans.score(X))\n \nsilhouette_score = metrics.silhouette_score(X, labels, metric='euclidean')\nprint (\"Silhouette_score: \")\nprint (silhouette_score)\n\nimport matplotlib.pyplot as plt\n \nfrom sklearn.manifold import TSNE\n \nmodel = TSNE(n_components=2, random_state=0)\nnp.set_printoptions(suppress=True)\n \nY=model.fit_transform(X)\n \n \nplt.scatter(Y[:, 0], Y[:, 1], c=assigned_clusters, s=290,alpha=.5)\n \n \nfor j in range(len(df['texts'])):    \n   plt.annotate(assigned_clusters[j],xy=(Y[j][0], Y[j][1]),xytext=(0,0),textcoords='offset points')\n   print (\"%s %s\" % (assigned_clusters[j],  df['texts'][j]))\n \n \nplt.show() \n\n\n\n\n    \n\"\"\"\nfrom nltk.cluster import KMeansClusterer\nimport nltk\ndef sent_vectorizer(sent, model):\n    sent_vec =[]\n    numw = 0\n    for w in sent:\n        try:\n            if numw == 0:\n                sent_vec = model[w]\n            else:\n                sent_vec = np.add(sent_vec, model[w])\n            numw+=1\n        except:\n            pass\n     \n    return np.asarray(sent_vec) / numw\n  \n  \nX=[]\nfor sentence in df['texts']:\n    X.append(sent_vectorizer(sentence, model))   \n \nprint (\"========================\")\nprint (X)    \n\n# note with some version you would need use this (without wv) \n#  model[model.vocab] \nprint (model[model.wv.vocab])\n\nprint (model.similarity('الراجحي', 'الاهلي'))\nprint (model.most_similar(positive=['العالمي'], negative=[], topn=2))\n\"\"\"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}