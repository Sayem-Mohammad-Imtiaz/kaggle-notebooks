{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Table of Contents\n\n* [0. Data loading](#section_0)\n* [1. EDA](#section_1)\n* [2. Setup validation and baselines](#section_2)\n* [3. Feature engineering](#section_3)\n* [4. Modeling](#section_4)\n    * [4.1 Logistic regression](#section_4_1)\n    * [4.2 Random Forest](#section_4_2)\n* [5. Other experiments](#section_5)\n    * [5.1 Using raw data + smaller window size](#section_5_1)\n    * [5.2 Median filter visualization](#section_5_2)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nsns.set(\n    style=\"whitegrid\",\n    font_scale=2\n);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 0. Data loading <a class=\"anchor\" id=\"section_0\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_FOLDER = Path('../input/data-for-activity-recognition/data/data')\nFRAME_LENGTH = 30\nclasses = [f.name for f in DATA_FOLDER.iterdir()]\nclasses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_acc_cols = [f'acc_x_{i}' for i in range(FRAME_LENGTH)]\ny_acc_cols = [f'acc_y_{i}' for i in range(FRAME_LENGTH)]\nz_acc_cols = [f'acc_z_{i}' for i in range(FRAME_LENGTH)]\n\nframes = []\nlabels = []\n\nfor class_name in classes:\n    class_folder = DATA_FOLDER / class_name\n    for file in class_folder.iterdir():\n        df = pd.read_csv(file)\n        flat_frame = np.hstack([df['accelerometer_X'], df['accelerometer_Y'], df['accelerometer_Z']]).astype(float)\n        frames.append(flat_frame)\n        labels.append(class_name)\n        \nraw_data = pd.DataFrame(frames, columns = x_acc_cols+y_acc_cols+z_acc_cols)\nraw_data = pd.concat([raw_data, pd.Series(labels, name='label')], axis=1)\nraw_data.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. EDA <a class=\"anchor\" id=\"section_1\"></a>"},{"metadata":{},"cell_type":"markdown","source":"We can see that there is class imbalance: only 165 samples of 'stairs' class"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.label.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distibution of readings across each axis"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 3, figsize=(16,6))\naxes = axes.flatten()\n\nfor label, cols, ax in zip(['X', 'Y', 'Z'], [x_acc_cols, y_acc_cols, z_acc_cols], axes):\n    sns.distplot(raw_data[cols].values.flatten(), ax=ax)\n    ax.set_title(f'Distr. of {label} axis readings');\n    ax.set_xlabel('m/$s^2$');\n    ax.set_ylabel('')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that all readings are centered around zero. X axis is approximately normally distributed, while Y and Z looks like bimodal distributions."},{"metadata":{},"cell_type":"markdown","source":"## t-SNE visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\ntsne_results = tsne.fit_transform(raw_data[x_acc_cols+y_acc_cols+z_acc_cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_subset = pd.DataFrame({\n    'tsne-2d-0': tsne_results[:,0],\n    'tsne-2d-1': tsne_results[:,1],\n    'label': raw_data['label'],\n})\n\nplt.figure(figsize=(16,10))\nsns.scatterplot(\n    x=\"tsne-2d-0\", y=\"tsne-2d-1\",\n    hue=\"label\",\n    palette=sns.color_palette(\"hls\", 4),\n    data=df_subset,\n    legend=\"full\",\n    alpha=0.3\n);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"t-SNE transformation, applied on raw accelerometer data in a frame of length 30, is visualized on 2d plane. We can see 3 clusters here:\n- running\n- idle\n- stairs + walking"},{"metadata":{},"cell_type":"markdown","source":"## Sample activities"},{"metadata":{},"cell_type":"markdown","source":"Visualize one sample for each activity"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(2, 2, figsize=(20, 20))\naxes = axes.flatten()\n\n\nsteps = np.arange(30)\n\nfor ax, label in zip(axes, classes):\n    sample = raw_data[raw_data['label'] == label].iloc[0]\n    sns.lineplot(y=sample[x_acc_cols].astype(float), x=steps, label='X', ax=ax)\n    sns.lineplot(y=sample[y_acc_cols].astype(float), x=steps, label='Y', ax=ax)\n    sns.lineplot(y=sample[z_acc_cols].astype(float), x=steps, label='Z', ax=ax).set_title(label)\n    ax.set_ylim(-40, 40)\n    ax.set_xlabel('time step')\n    ax.set_ylabel('m/$s^2$')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Mean for each axis"},{"metadata":{},"cell_type":"markdown","source":"We can see, that calculating mean for every axis is enough to separate all classes for each other, except for 'walking' vs 'stairs'"},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_x = raw_data[x_acc_cols].mean(axis=1)\nmean_y = raw_data[y_acc_cols].mean(axis=1)\nmean_z = raw_data[z_acc_cols].mean(axis=1)\n\nsimple_features = pd.DataFrame({\n    'mean_x': mean_x,\n    'mean_y': mean_y,\n    'mean_z': mean_z,\n    'label': raw_data['label']\n})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Pairplot of mean accelerometer value during frame of length 30');\nsns.pairplot(data=simple_features, hue='label', height=5);\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Setup validation and baselines <a class=\"anchor\" id=\"section_2\"></a>"},{"metadata":{},"cell_type":"markdown","source":"## StratifiedKFold"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\n\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = raw_data['acc_x_0']\ny = raw_data['label']\n\nfor i, (_, valid_ix) in enumerate(kfold.split(X, y)):\n    y_valid = y.loc[valid_ix]\n    value_counts = y_valid.value_counts()\n    print(f'split#{i}, we have {value_counts.stairs} \"stairs\", {value_counts.running} \"running\" samples')\n#     print(    (y_valid == 'stairs').index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, StandardScaler\n\n# target label encoder\ntarget_le = LabelEncoder()\ntarget_le.fit(classes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Baselines"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\nfrom sklearn import linear_model\nfrom sklearn.metrics import f1_score, confusion_matrix, multilabel_confusion_matrix, accuracy_score\nfrom IPython.display import display, HTML","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_metrics = ['accuracy', 'f1_macro', 'f1_min']\n\ndef evaluate_performance(y_true, y_pred):\n    \"\"\"Calculates metrics and returns result as dict\"\"\"\n    metrics = {\n        'accuracy': accuracy_score(y_true, y_pred),\n        'f1_macro': f1_score(y_true, y_pred, average='macro'),\n        'f1_min': f1_score(y_true, y_pred, average=None).min(),\n        'CM': confusion_matrix(y_true, y_pred, normalize='true'),\n        'CM_not_normalized': confusion_matrix(y_true, y_pred),\n    }\n    \n    return metrics\n\ndef plot_confusion_matrix(df, title=None):\n    # C_ij - i is true, predicted in j\n    cm = np.stack(df['CM'].values, axis=0).mean(axis=0)\n    cm_nn = np.stack(df['CM_not_normalized'].values, axis=0).mean(axis=0)\n    cm_print = np.empty_like(cm_nn).astype(str)\n    for i in range(len(cm_print)):\n        for j in range(len(cm_print)):\n            cm_print[i,j] = (\"%.2f\" % (cm[i,j]*100))+'%\\n'+str(cm_nn[i,j])\n    plt.figure(figsize=(6,6))\n    sns.set(font_scale=1.3)\n    labels_ordered = target_le.inverse_transform(range(4))\n    sns.heatmap(cm,\n                annot=cm_print,\n                fmt='',\n                cmap='Blues',\n                cbar=False,\n                xticklabels=labels_ordered,\n                yticklabels=labels_ordered,\n               )\n    plt.xlabel('predicted')\n    plt.ylabel('actual');\n    plt.title(title);\n    \ndef print_cv_metrics(cv_metrics, title=None):\n    \"\"\"Displays results on cross validation\"\"\"\n    df = pd.DataFrame(cv_metrics)\n    plot_confusion_matrix(df, title=title);\n    plt.show()\n    print_df = pd.concat([df.mean()[numerical_metrics], df.std()[numerical_metrics]], axis=1)\n    print_df = print_df.applymap(lambda x: round(x, 3))\n    print_df.columns = ['mean', 'std']\n    display(HTML(print_df.T.to_html()))    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = simple_features[['mean_x', 'mean_y', 'mean_z']]\ny = simple_features['label']\ncv_metrics = []\n\nfor train_ix, valid_ix in kfold.split(X, y):\n    X_train, X_valid = X.loc[train_ix], X.loc[valid_ix]\n    y_train, y_valid = y.loc[train_ix], y.loc[valid_ix]\n    \n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_valid = scaler.transform(X_valid)\n    \n    cls = linear_model.LogisticRegression()\n    cls.fit(X_train, y_train)\n    y_pred = cls.predict(X_valid)\n    \n    fold_metrics = evaluate_performance(y_valid, y_pred)\n    cv_metrics.append(fold_metrics)\n    print(fold_metrics['CM_not_normalized'])\nprint_cv_metrics(cv_metrics, 'Baseline logistic regression')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = simple_features[['mean_x', 'mean_y', 'mean_z']]\ny = simple_features['label']\ncv_metrics = []\n\nfor train_ix, valid_ix in kfold.split(X, y):\n    X_train, X_valid = X.loc[train_ix], X.loc[valid_ix]\n    y_train, y_valid = y.loc[train_ix], y.loc[valid_ix]\n    \n#     scaler = StandardScaler()\n#     X_train = scaler.fit_transform(X_train)\n#     X_valid = scaler.transform(X_valid)\n    \n    cls = RandomForestClassifier()\n    cls.fit(X_train, y_train)\n    y_pred = cls.predict(X_valid)\n    \n    fold_metrics = evaluate_performance(y_valid, y_pred)\n    cv_metrics.append(fold_metrics)\n    print(fold_metrics['CM_not_normalized'])\nprint_cv_metrics(cv_metrics, 'Baseline Random Forest')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Feature engineering <a class=\"anchor\" id=\"section_3\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_features(x_part, y_part, z_part):\n    \"\"\"\n    Extract features from accelerometer readings.\n    \n    First, magnitude across 3 axes is calculed and added as 4ht timeseries 'm'\n    Then, the followng features for each axis is calculated:\n    - mean: simple average for each axis\n    - minmax: difference between max value and min value for each axis\n    - min: minumum value for each axis\n    - rms: root mean square for each axis\n    - corr_xy, corr_yz, corr_xz - Pearson correlation coefs. for corresponding axis.\n    \n    as proposed in 'A Study on Human Activity Recognition Using Accelerometer Data from Smartphones' \n    https://www.sciencedirect.com/science/article/pii/S1877050914008643\n\n    Parameters:\n            x_part, y_part, z_part: Arrays of shape n_samples x FRAME_LENGTH\n            with accelerometer readings\n\n    Returns:\n            features (DataFrame): DataFrame with generated features (19 features in total)\n    \"\"\"\n    magnitude_part = np.sqrt(x_part**2 + y_part**2 + z_part**2)\n    ts_matrix = np.stack([x_part, y_part, z_part, magnitude_part], axis=1)\n    ts_ax_names = ['x', 'y', 'z', 'm']\n    assert len(ts_ax_names) == ts_matrix.shape[1]\n    \n    mean_features = pd.DataFrame(ts_matrix.mean(axis=2), columns=[f'mean_{ax}' for ax in ts_ax_names])\n    \n    minmax_m = ts_matrix.max(axis=2) - ts_matrix.min(axis=2)\n    minmax_features = pd.DataFrame(minmax_m, columns=[f'minmax_{ax}' for ax in ts_ax_names])\n    \n    std_m = ts_matrix.std(axis=2)\n    std_features = pd.DataFrame(std_m, columns=[f'std_{ax}' for ax in ts_ax_names])\n    \n    min_m = ts_matrix.min(axis=2)\n    min_features = pd.DataFrame(min_m, columns=[f'min_{ax}' for ax in ts_ax_names])\n    \n    rms_m = np.sqrt(np.square(ts_matrix).mean(axis=2))\n    rms_features = pd.DataFrame(rms_m, columns=[f'rms_{ax}' for ax in ts_ax_names])\n    \n    ix = np.arange(len(ts_matrix))\n    iy = ix + len(ts_matrix)\n    corr_xy = np.corrcoef(ts_matrix[:, 0, :], ts_matrix[:, 1, :])[ix,iy]\n    corr_yz = np.corrcoef(ts_matrix[:, 1, :], ts_matrix[:, 2, :])[ix,iy]\n    corr_xz = np.corrcoef(ts_matrix[:, 0, :], ts_matrix[:, 2, :])[ix,iy]\n    corr_features = pd.DataFrame({'corr_xy': corr_xy, 'corr_yz': corr_yz, 'corr_xz': corr_xz})\n    \n    features = pd.concat([mean_features, minmax_features, min_features, rms_features, corr_features], axis=1)\n    return features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_df = extract_features(\n    raw_data[x_acc_cols].values,\n    raw_data[y_acc_cols].values,\n    raw_data[z_acc_cols].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_df['label'] = target_le.transform(raw_data['label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_features = [c for c in feature_df.columns if c != 'label']\nprint('len(all_features): ', len(all_features))\nall_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Modeling <a class=\"anchor\" id=\"section_4\"></a>"},{"metadata":{},"cell_type":"markdown","source":"## 4.1 Logistic regression <a class=\"anchor\" id=\"section_4_1\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = feature_df[all_features]\ny = feature_df['label']\ncv_metrics = []\n\nfor train_ix, valid_ix in kfold.split(X, y):\n    X_train, X_valid = X.loc[train_ix], X.loc[valid_ix]\n    y_train, y_valid = y.loc[train_ix], y.loc[valid_ix]\n    \n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_valid = scaler.transform(X_valid)\n    \n    cls = linear_model.LogisticRegression(max_iter=1_000)\n    cls.fit(X_train, y_train)\n    y_pred = cls.predict(X_valid)\n    \n    fold_metrics = evaluate_performance(y_valid, y_pred)\n    cv_metrics.append(fold_metrics)\n    print(fold_metrics['CM_not_normalized'])\nprint_cv_metrics(cv_metrics, 'Logistic Regression on all features')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding polynomial features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\n\npoly_transform = PolynomialFeatures(2, include_bias=False)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"X = poly_transform.fit_transform(feature_df[all_features])\ny = feature_df['label'].values\ncv_metrics = []\n\nfor train_ix, valid_ix in kfold.split(X, y):\n    X_train, X_valid = X[train_ix], X[valid_ix]\n    y_train, y_valid = y[train_ix], y[valid_ix]\n    \n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_valid = scaler.transform(X_valid)\n    \n    cls = linear_model.LogisticRegression(max_iter=1_000)\n    cls.fit(X_train, y_train)\n    y_pred = cls.predict(X_valid)\n    \n    fold_metrics = evaluate_performance(y_valid, y_pred)\n    cv_metrics.append(fold_metrics)\n    print(fold_metrics['CM_not_normalized'])\nprint_cv_metrics(cv_metrics, 'Logistic Regressions on all features + Polynomial degree 2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature selection with L1 regularization"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = poly_transform.fit_transform(feature_df[all_features])\ny = feature_df['label'].values\n\ndef l1_objective(C=1.0):\n    cv_metrics = []\n\n    for train_ix, valid_ix in kfold.split(X, y):\n        X_train, X_valid = X[train_ix], X[valid_ix]\n        y_train, y_valid = y[train_ix], y[valid_ix]\n\n        scaler = StandardScaler()\n        X_train = scaler.fit_transform(X_train)\n        X_valid = scaler.transform(X_valid)\n\n        cls = linear_model.LogisticRegression(penalty='l1', solver='liblinear', C=C)\n        cls.fit(X_train, y_train)\n        y_pred = cls.predict(X_valid)\n\n        fold_metrics = evaluate_performance(y_valid, y_pred)\n        cv_metrics.append(fold_metrics)\n    return cv_metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.cli import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Cs = np.linspace(1e-3, 1.5, num=20)\nf1_values = []\nf1_stds   = []\n\nfor C in tqdm(Cs):\n    cv_metrics = l1_objective(C=C)\n    df = pd.DataFrame(cv_metrics)\n    print_df = pd.concat([df.mean()[numerical_metrics], df.std()[numerical_metrics]], axis=1)\n    print_df.columns = ['mean', 'std']\n    f1_macro = print_df.loc['f1_macro']\n    f1_values.append(f1_macro['mean'])\n    f1_stds.append(f1_macro['std'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(x=Cs, y=f1_values)\nsns.scatterplot(x=Cs, y=f1_values, color='green')\nplt.errorbar(Cs, f1_values, f1_stds, linestyle='None')\n\nplt.xlabel('C')\nplt.ylabel('F1 macro')\nplt.title('Choosing inverse regularization strength C for Lasso with CV');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Cs[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\n\nX_scaled = scaler.fit_transform(X)\n\nlasso_cls = linear_model.LogisticRegression(penalty='l1', C=.2, solver='liblinear')\nlasso_cls.fit(X_scaled, y)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"importance_coefs = np.abs(lasso_cls.coef_).max(axis=0)\nfeature_importance = list(zip(poly_transform.get_feature_names(all_features), importance_coefs))\n\nimportant_features = [(name, coef) for name, coef in feature_importance]\nimportance_df = pd.DataFrame(important_features, columns = ['feature_name', 'importance']\n                            ).sort_values(by='importance', ascending=False\n                            ).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importance_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importance_df[importance_df['importance'] > 1e-2].shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Choosing optimal number of features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def l2_objective_n_features(selected_features):\n    X = poly_transform.fit_transform(feature_df[all_features])\n    X = pd.DataFrame(X, columns = poly_transform.get_feature_names(all_features))\n    X = X[selected_features]\n    y = feature_df['label']\n\n    cv_metrics = []\n\n    for train_ix, valid_ix in kfold.split(X, y):\n        X_train, X_valid = X.loc[train_ix], X.loc[valid_ix]\n        y_train, y_valid = y.loc[train_ix], y.loc[valid_ix]\n\n        scaler = StandardScaler()\n        X_train = scaler.fit_transform(X_train)\n        X_valid = scaler.transform(X_valid)\n\n        cls = linear_model.LogisticRegression(penalty='l2', max_iter=2_000)\n        cls.fit(X_train, y_train)\n        y_pred = cls.predict(X_valid)\n\n        fold_metrics = evaluate_performance(y_valid, y_pred)\n        cv_metrics.append(fold_metrics)\n    return cv_metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_features = np.arange(1, 61)\n\nscores = {\n    'selected_features': [],\n    'f1_macro': [],\n    'f1_macro_std': [],\n    'f1_min': [],\n    'f1_min_std': [],\n}\n\nfor n in tqdm(n_features):\n    selected_features = importance_df['feature_name'].head(n).values\n    cv_metrics = l2_objective_n_features(selected_features)\n    df = pd.DataFrame(cv_metrics)\n    print_df = pd.concat([df.mean()[numerical_metrics], df.std()[numerical_metrics]], axis=1)\n    print_df.columns = ['mean', 'std']\n    f1_macro = print_df.loc['f1_macro']\n    scores['selected_features'].append(selected_features)\n    scores['f1_macro'].append(print_df.loc['f1_macro']['mean'])    \n    scores['f1_macro_std'].append(print_df.loc['f1_macro']['std'])\n    \n    scores['f1_min'].append(print_df.loc['f1_min']['mean'])    \n    scores['f1_min_std'].append(print_df.loc['f1_min']['std'])    \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nsns.lineplot(x=n_features, y=scores['f1_macro'])\nsns.scatterplot(x=n_features, y=scores['f1_macro'], label='f1_macro')\n\nsns.lineplot(x=n_features, y=scores['f1_min'])\nsns.scatterplot(x=n_features, y=scores['f1_min'], label='f1_min')\n\nplt.xlabel('Number of features')\nplt.ylabel('F1 score')\nplt.title('Choosing number of features');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_23_features = importance_df['feature_name'].head(23).values\ntop_23_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Final model"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = poly_transform.fit_transform(feature_df[all_features])\nX = pd.DataFrame(X, columns = poly_transform.get_feature_names(all_features))\nX = X[top_23_features]\ny = feature_df['label']\n\nfor train_ix, valid_ix in kfold.split(X, y):\n    X_train, X_valid = X.loc[train_ix], X.loc[valid_ix]\n    y_train, y_valid = y.loc[train_ix], y.loc[valid_ix]\n    \n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_valid = scaler.transform(X_valid)\n    \n    cls = linear_model.LogisticRegression(max_iter=1_000)\n    cls.fit(X_train, y_train)\n    y_pred = cls.predict(X_valid)\n    \n    fold_metrics = evaluate_performance(y_valid, y_pred)\n    cv_metrics.append(fold_metrics)\n    print(fold_metrics['CM_not_normalized'])\nprint_cv_metrics(cv_metrics, 'Logistic Regression on top 23 features')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2 RandomForest <a class=\"anchor\" id=\"section_4_2\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = feature_df[all_features]\ny = feature_df['label']\ncv_metrics = []\n\nfor train_ix, valid_ix in kfold.split(X, y):\n    X_train, X_valid = X.loc[train_ix], X.loc[valid_ix]\n    y_train, y_valid = y.loc[train_ix], y.loc[valid_ix]\n    \n    cls = RandomForestClassifier()\n    cls.fit(X_train, y_train)\n    y_pred = cls.predict(X_valid)\n    \n    fold_metrics = evaluate_performance(y_valid, y_pred)\n    cv_metrics.append(fold_metrics)\n    print(fold_metrics['CM_not_normalized'])\nprint_cv_metrics(cv_metrics, 'Random Forest on all features')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = feature_df[all_features]\ny = feature_df['label']\n\ncls = RandomForestClassifier()\ncls.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cls.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16, 6));\nplt.title('Impurity-based feature importances of the forest')\nplot = sns.barplot(x=all_features, y=cls.feature_importances_)\nfor item in plot.get_xticklabels():\n    item.set_rotation(45)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importance_df = pd.DataFrame(zip(all_features, cls.feature_importances_), columns = ['feature_name', 'importance']\n                            ).sort_values(by='importance', ascending=False\n                            ).reset_index(drop=True)\nimportance_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_10_features = importance_df['feature_name'].head(10).values\ntop_10_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def RF_objective_n_features(selected_features):\n    X = feature_df[selected_features]\n    y = feature_df['label']\n\n    cv_metrics = []\n\n    for train_ix, valid_ix in kfold.split(X, y):\n        X_train, X_valid = X.loc[train_ix], X.loc[valid_ix]\n        y_train, y_valid = y.loc[train_ix], y.loc[valid_ix]\n\n        cls = RandomForestClassifier()\n        cls.fit(X_train, y_train)\n        y_pred = cls.predict(X_valid)\n\n        fold_metrics = evaluate_performance(y_valid, y_pred)\n        cv_metrics.append(fold_metrics)\n    return cv_metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_features = np.arange(1, len(all_features)+1)\n\nscores = {\n    'selected_features': [],\n    'f1_macro': [],\n    'f1_macro_std': [],\n    'f1_min': [],\n    'f1_min_std': [],\n}\n\nfor n in tqdm(n_features):\n    selected_features = importance_df['feature_name'].head(n).values\n    cv_metrics = RF_objective_n_features(selected_features)\n    df = pd.DataFrame(cv_metrics)\n    print_df = pd.concat([df.mean()[numerical_metrics], df.std()[numerical_metrics]], axis=1)\n    print_df.columns = ['mean', 'std']\n    f1_macro = print_df.loc['f1_macro']\n    scores['selected_features'].append(selected_features)\n    scores['f1_macro'].append(print_df.loc['f1_macro']['mean'])    \n    scores['f1_macro_std'].append(print_df.loc['f1_macro']['std'])\n    \n    scores['f1_min'].append(print_df.loc['f1_min']['mean'])    \n    scores['f1_min_std'].append(print_df.loc['f1_min']['std'])    \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nsns.lineplot(x=n_features, y=scores['f1_macro'])\nsns.scatterplot(x=n_features, y=scores['f1_macro'], label='f1_macro')\n\nsns.lineplot(x=n_features, y=scores['f1_min'])\nsns.scatterplot(x=n_features, y=scores['f1_min'], label='f1_min')\n\nplt.xlabel('Number of features')\nplt.ylabel('F1 score')\nplt.title('Choosing number of features for Random Forest');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_7_features = importance_df['feature_name'].head(7).values\ntop_7_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = feature_df[top_7_features]\ny = feature_df['label']\ncv_metrics = []\n\nfor train_ix, valid_ix in kfold.split(X, y):\n    X_train, X_valid = X.loc[train_ix], X.loc[valid_ix]\n    y_train, y_valid = y.loc[train_ix], y.loc[valid_ix]\n    \n    cls = RandomForestClassifier()\n    cls.fit(X_train, y_train)\n    y_pred = cls.predict(X_valid)\n    \n    fold_metrics = evaluate_performance(y_valid, y_pred)\n    cv_metrics.append(fold_metrics)\n    print(fold_metrics['CM_not_normalized'])\nprint_cv_metrics(cv_metrics, 'Random Forest on top 7 features')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### out-of-bag errors"},{"metadata":{},"cell_type":"markdown","source":"Each tree in Random forest is trained on subsample of the original data. We expect that for each training sample there are trees in our forest that had not seen this sample during training. Thus, we can make a good estimate of **generalization ability** of our forest even without holdout set or crossvalidation."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = feature_df[top_7_features]\ny = feature_df['label']\n\ncls = RandomForestClassifier(n_estimators=100, oob_score=True)\ncls.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oob_preds = cls.oob_decision_function_.argmax(axis=1)\nconfusion_matrix(y, oob_preds)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"print_cv_metrics([evaluate_performance(y, oob_preds)], 'out-of-bag performance of RF on top 7 features')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tuning Random Forest on 5 features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from hyperopt import tpe, hp, fmin, STATUS_OK, Trials, space_eval\nfrom hyperopt.pyll.base import scope\nfrom hyperopt.pyll.stochastic import sample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"space = {\n    \"n_estimators\": scope.int(hp.quniform(\"n_estimators\", 20, 1000, 1)),\n    \"max_depth\": hp.choice('max_depth', [None, scope.int(hp.quniform(\"max_depth_int\", 1, 20,1))] ),\n    \"criterion\": hp.choice(\"criterion\", [\"gini\", \"entropy\"]),\n}\n\nMAX_EVALS = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_5_features = importance_df['feature_name'].head(5).values\ntop_5_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = feature_df[top_5_features]\ny = feature_df['label']\ncv_metrics = []\n\nfor train_ix, valid_ix in kfold.split(X, y):\n    X_train, X_valid = X.loc[train_ix], X.loc[valid_ix]\n    y_train, y_valid = y.loc[train_ix], y.loc[valid_ix]\n    \n    cls = RandomForestClassifier(\n        random_state=1\n    )\n    \n    cls.fit(X_train, y_train)\n    y_pred = cls.predict(X_valid)\n    \n    fold_metrics = evaluate_performance(y_valid, y_pred)\n    cv_metrics.append(fold_metrics)\n    print(fold_metrics['CM_not_normalized'])\nprint_cv_metrics(cv_metrics, 'Random Forest on top 5 features with default hyperparams')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = feature_df[top_5_features]\ny = feature_df['label']\n\ndef RF_objective_hyperparams(params):\n#     print(params)\n    cv_metrics = []\n\n    for train_ix, valid_ix in kfold.split(X, y):\n        X_train, X_valid = X.loc[train_ix], X.loc[valid_ix]\n        y_train, y_valid = y.loc[train_ix], y.loc[valid_ix]\n\n        cls = RandomForestClassifier(\n            n_estimators=params['n_estimators'],\n            max_depth=params['max_depth'],\n            criterion=params['criterion'],\n            random_state=1\n        )\n        cls.fit(X_train, y_train)\n        y_pred = cls.predict(X_valid)\n\n        fold_metrics = evaluate_performance(y_valid, y_pred)\n        cv_metrics.append(fold_metrics)\n        \n    loss = 1 - pd.DataFrame(cv_metrics)['f1_min'].mean()\n    return {'loss': loss, 'params': params, 'status': STATUS_OK}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nRF_objective_hyperparams({'n_estimators': 100, 'criterion': 'gini', 'max_depth': None})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bayes_trials = Trials()\n\n# Optimize\n# best = fmin(fn = RF_objective_hyperparams, space = space, algo = tpe.suggest, max_evals = MAX_EVALS, trials = bayes_trials)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# best_params = space_eval(space, best)\nbest_params = {'criterion': 'entropy', 'max_depth': None, 'n_estimators': 274}\nbest_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = feature_df[top_5_features]\ny = feature_df['label']\ncv_metrics = []\n\nfor train_ix, valid_ix in kfold.split(X, y):\n    X_train, X_valid = X.loc[train_ix], X.loc[valid_ix]\n    y_train, y_valid = y.loc[train_ix], y.loc[valid_ix]\n    \n    cls = RandomForestClassifier(\n        n_estimators=best_params['n_estimators'],\n        max_depth=best_params['max_depth'],\n        criterion=best_params['criterion'],\n        random_state=1\n    )\n    \n    cls.fit(X_train, y_train)\n    y_pred = cls.predict(X_valid)\n    \n    fold_metrics = evaluate_performance(y_valid, y_pred)\n    cv_metrics.append(fold_metrics)\n    print(fold_metrics['CM_not_normalized'])\nprint_cv_metrics(cv_metrics, 'Tuned Random Forest on top 5 features')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Other experiments <a class=\"anchor\" id=\"section_5\"></a>"},{"metadata":{},"cell_type":"markdown","source":"## 5.1 Using raw data + smaller window size <a class=\"anchor\" id=\"section_5_1\"></a>"},{"metadata":{},"cell_type":"markdown","source":"### With original window"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = raw_data[x_acc_cols+y_acc_cols+z_acc_cols]\ny = raw_data['label']\ncv_metrics = []\n\nfor train_ix, valid_ix in kfold.split(X, y):\n    X_train, X_valid = X.loc[train_ix], X.loc[valid_ix]\n    y_train, y_valid = y.loc[train_ix], y.loc[valid_ix]\n    \n    cls = RandomForestClassifier()\n    cls.fit(X_train, y_train)\n    y_pred = cls.predict(X_valid)\n    \n    fold_metrics = evaluate_performance(y_valid, y_pred)\n    cv_metrics.append(fold_metrics)\n    print(fold_metrics['CM_not_normalized'])\nprint_cv_metrics(cv_metrics, 'Random Forest on raw data')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = raw_data[x_acc_cols+y_acc_cols+z_acc_cols]\ny = raw_data['label']\ncv_metrics = []\n\nfor train_ix, valid_ix in kfold.split(X, y):\n    X_train, X_valid = X.loc[train_ix], X.loc[valid_ix]\n    y_train, y_valid = y.loc[train_ix], y.loc[valid_ix]\n    \n    cls = svm.SVC()\n    cls.fit(X_train, y_train)\n    y_pred = cls.predict(X_valid)\n    \n    fold_metrics = evaluate_performance(y_valid, y_pred)\n    cv_metrics.append(fold_metrics)\n    print(fold_metrics['CM_not_normalized'])\nprint_cv_metrics(cv_metrics, 'SVM (rbf) on raw data')","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"### Switch to binary classification"},{"metadata":{"hidden":true},"cell_type":"markdown","source":"Lets work with binary classification (stairs vs walking) for simplicity. And setup baseline on raw data."},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"raw_stairs = raw_data[raw_data['label'].isin(['stairs', 'walking'])].reset_index(drop=True).copy()\nraw_stairs['is_stairs'] = raw_stairs['label'] == 'stairs'\nraw_stairs.shape","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"X = raw_stairs[x_acc_cols+y_acc_cols+z_acc_cols]\ny = raw_stairs['is_stairs']\ncv_metrics = []\nf_scores = []\n\nfor train_ix, valid_ix in kfold.split(X, y):\n    X_train, X_valid = X.loc[train_ix], X.loc[valid_ix]\n    y_train, y_valid = y.loc[train_ix], y.loc[valid_ix]\n    \n    cls = svm.SVC()\n    cls.fit(X_train, y_train)\n    y_pred = cls.predict(X_valid)\n    f_scores.append(f1_score(y_valid, y_pred))\nprint(f'F1 score mean: {round(np.mean(f_scores), 2)}, std: {round(np.std(f_scores), 2)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split to smaller window, then assemble predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_frames_to_windows(X, window_size = 20):\n    \"\"\"Create windows of smaller size from batch of timeseries\"\"\"\n    frame_length = X.shape[-1]\n    n_windows = frame_length - window_size + 1\n    i = 0\n\n    X_extended = []\n    for i in range(0, n_windows):\n        X_extended.append(X[:, :, i:window_size+i])\n\n    X_extended = np.vstack(X_extended)\n    return X_extended","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(1)\nsample_X = np.random.randint(6, size=(1,3,4))\nsample_X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_windows = split_frames_to_windows(sample_X, window_size=3)\nassert len(sample_windows)      == 2 # we expect to get 2 windows\nassert sample_windows.shape[1:] == (3,3) # we expect each window to be 3x3\n\nsample_windows","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def assemble_ts_parts(X):\n    x_part = X[x_acc_cols].values\n    y_part = X[y_acc_cols].values\n    z_part = X[z_acc_cols].values\n    features = np.stack([x_part, y_part, z_part], axis=1)\n    \n    return features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fold_predictions(y_pred, n_windows):\n    y_pred = y_pred.reshape(n_windows, -1)\n    return y_pred.mean(axis=0)\n\nsample_preds = np.array([1, 0, 1, 1, 0, 0])\nexpected_preds = np.array([1., 0., .5])\n\nactual_preds = fold_predictions(sample_preds, n_windows=2)\n\nassert np.allclose(actual_preds, expected_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"window_size = 29\nthreshold = .5\nn_windows = FRAME_LENGTH - window_size + 1\n\nX = raw_stairs[x_acc_cols+y_acc_cols+z_acc_cols]\ny = raw_stairs['is_stairs']\n\nf_scores = []\n\nfor train_ix, valid_ix in kfold.split(X, y):\n    X_train, X_valid = X.loc[train_ix], X.loc[valid_ix]\n    y_train, y_valid = y.loc[train_ix], y.loc[valid_ix]\n    \n    ts_frames = assemble_ts_parts(X_train)\n    X_train_extended = split_frames_to_windows(ts_frames, window_size=window_size)\n    X_train_extended = X_train_extended.reshape(len(X_train_extended), -1)\n    y_train_extended = np.tile(y_train, n_windows)\n    \n    cls = svm.SVC()\n    cls.fit(X_train_extended, y_train_extended)\n    X_valid_extended = assemble_ts_parts(X_valid)\n    X_valid_extended = split_frames_to_windows(X_valid_extended, window_size=window_size)\n    X_valid_extended = X_valid_extended.reshape(len(X_valid_extended), -1)\n    \n    y_pred_extended = cls.predict(X_valid_extended)\n    y_pred = fold_predictions(y_pred_extended, n_windows=n_windows)\n    y_pred = y_pred > threshold\n    f_scores.append(f1_score(y_valid, y_pred))\n    \nprint(f'F1 score mean: {round(np.mean(f_scores), 2)}, std: {round(np.std(f_scores), 2)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"window_size = 20\nthreshold = .5\nn_windows = FRAME_LENGTH - window_size + 1\n\nX = raw_stairs[x_acc_cols+y_acc_cols+z_acc_cols]\ny = raw_stairs['is_stairs']\n\nf_scores = []\n\nfor train_ix, valid_ix in kfold.split(X, y):\n    X_train, X_valid = X.loc[train_ix], X.loc[valid_ix]\n    y_train, y_valid = y.loc[train_ix], y.loc[valid_ix]\n    \n    ts_frames = assemble_ts_parts(X_train)\n    X_train_extended = split_frames_to_windows(ts_frames, window_size=window_size)\n    X_train_extended = X_train_extended.reshape(len(X_train_extended), -1)\n    y_train_extended = np.tile(y_train, n_windows)\n    \n    cls = svm.SVC()\n    cls.fit(X_train_extended, y_train_extended)\n    X_valid_extended = assemble_ts_parts(X_valid)\n    X_valid_extended = split_frames_to_windows(X_valid_extended, window_size=window_size)\n    X_valid_extended = X_valid_extended.reshape(len(X_valid_extended), -1)\n    \n    y_pred_extended = cls.predict(X_valid_extended)\n    y_pred = fold_predictions(y_pred_extended, n_windows=n_windows)\n    y_pred = y_pred > threshold\n    f_scores.append(f1_score(y_valid, y_pred))\n    \nprint(f'F1 score mean: {round(np.mean(f_scores), 2)}, std: {round(np.std(f_scores), 2)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_SVM_F1_on_window_size(window_size):\n    threshold = .5\n    n_windows = FRAME_LENGTH - window_size + 1\n\n    X = raw_stairs[x_acc_cols+y_acc_cols+z_acc_cols]\n    y = raw_stairs['is_stairs']\n\n    f_scores = []\n\n    for train_ix, valid_ix in kfold.split(X, y):\n        X_train, X_valid = X.loc[train_ix], X.loc[valid_ix]\n        y_train, y_valid = y.loc[train_ix], y.loc[valid_ix]\n\n        ts_frames = assemble_ts_parts(X_train)\n        X_train_extended = split_frames_to_windows(ts_frames, window_size=window_size)\n        X_train_extended = X_train_extended.reshape(len(X_train_extended), -1)\n        y_train_extended = np.tile(y_train, n_windows)\n\n        cls = svm.SVC()\n        cls.fit(X_train_extended, y_train_extended)\n        X_valid_extended = assemble_ts_parts(X_valid)\n        X_valid_extended = split_frames_to_windows(X_valid_extended, window_size=window_size)\n        X_valid_extended = X_valid_extended.reshape(len(X_valid_extended), -1)\n\n        y_pred_extended = cls.predict(X_valid_extended)\n        y_pred = fold_predictions(y_pred_extended, n_windows=n_windows)\n        y_pred = y_pred > threshold\n        f_scores.append(f1_score(y_valid, y_pred))\n\n    return f_scores\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_SVM_F1_on_window_size(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"window_sizes = np.arange(30, 9, -1)\nf_scores_means = []\nf_scores_stds  = []\n \nfor window_size in tqdm(window_sizes):\n    f_scores_cv = get_SVM_F1_on_window_size(window_size)\n    f_scores_means.append(np.mean(f_scores_cv))\n    f_scores_stds.append(np.std(f_scores_cv))    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,6))\nplt.title('Binary classification performance (stairs vs walking) on raw data')\nplt.xlabel('Window size');\nplt.ylabel('F1 score')\nsns.lineplot(x=window_sizes, y=f_scores_means)\nsns.scatterplot(x=window_sizes, y=f_scores_means, s=100);\nplt.errorbar(window_sizes, f_scores_means, f_scores_stds, linestyle='None')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.2 Median filter visualization <a class=\"anchor\" id=\"section_5_2\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import signal as sig","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(2, 2, figsize=(20, 20))\naxes = axes.flatten()\n\n\nsteps = np.arange(30)\n\nfor ax, label in zip(axes, classes):\n    sample = raw_data[raw_data['label'] == label].iloc[0]\n    sns.lineplot(y=sample[x_acc_cols].astype(float), color='g', x=steps, alpha=.3, label='X', ax=ax)\n    sns.lineplot(y=sample[y_acc_cols].astype(float), color='r', x=steps, alpha=.3, label='Y', ax=ax)\n    sns.lineplot(y=sample[z_acc_cols].astype(float), color='b', x=steps, alpha=.3, label='Z', ax=ax).set_title(label)\n    \n    sns.lineplot(y=sig.medfilt(sample[x_acc_cols].astype(float), kernel_size=7), x=steps, label='medfilt7 X', color='g', ax=ax)\n    sns.lineplot(y=sig.medfilt(sample[y_acc_cols].astype(float), kernel_size=7), x=steps, label='medfilt7 Y', color='r', ax=ax)\n    sns.lineplot(y=sig.medfilt(sample[z_acc_cols].astype(float), kernel_size=7), x=steps, label='medfilt7 Z', color='b', ax=ax)\n    \n    ax.set_ylim(-40, 40)\n    ax.set_xlabel('time step')\n    ax.set_ylabel('m/$s^2$')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see, that median filter can be used for denoizing signal from accelerometer and provide great source for futher feature engineering"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}