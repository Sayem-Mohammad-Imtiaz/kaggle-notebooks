{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eb6328c9-b8ca-da46-7421-5dbb0ac28ad0"},"outputs":[],"source":"%matplotlib inline\n\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nfrom matplotlib import pyplot as plt\n\n\"\"\"\n    df: dataframe\n    vec: Object that convert a collection of text documents to a matrix of token counts\n    ascending: order creteria\n    qty: first n words to return\n    \n    Return list with all words and their global frequency (all samples)\n\"\"\"\ndef get_resume(df, vec, ascending = False, n = None):\n    X = vec.fit_transform(df[\"text\"].values)\n    feature_names = vec.get_feature_names()\n\n    resume = pd.DataFrame(columns = feature_names, data = X.toarray()).sum()\n\n    if(n):\n        return resume.sort_values(ascending = ascending)[:n]\n\n    return resume"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bdc61ef1-2b82-9454-a47a-ab70b7e5487b"},"outputs":[],"source":"df = pd.read_csv(\"../input/spam.csv\", encoding='latin-1')\n\ndf.columns = [\"sms_type\", \"text\", \"2\", \"3\", \"4\"]\n\ndf.drop(\"2\", axis=1, inplace=True)\ndf.drop(\"3\", axis=1, inplace=True)\ndf.drop(\"4\", axis=1, inplace=True)\n\ndf[\"text\"] = df[\"text\"].str.lower() # Convert to lowercase"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"187f2301-a336-bfba-bf0b-d46c4140bd49"},"outputs":[],"source":"df.describe()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2f1ae4dd-d160-d261-c283-ae8dfef0c950"},"outputs":[],"source":"df[\"sms_type\"].value_counts()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2a049410-d1f3-4e60-b734-6aaed1c35265"},"outputs":[],"source":"vec = CountVectorizer(decode_error = 'ignore', stop_words = \"english\")\nspam_resume = get_resume(df[df[\"sms_type\"] == \"spam\"], vec, n = 25)\nspam_resume"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b8eaf52c-5fd0-400b-8a00-3183fbf49401"},"outputs":[],"source":"spam_resume.plot(kind = 'bar', figsize = (8, 4), fontsize = 12)"},{"cell_type":"markdown","metadata":{"_cell_guid":"6a0f5652-843a-7a05-bcd8-f50955a9eb4f"},"source":"When we work with NLP, it's importat understand the context. Laws are too large and wording is correct and there is no spelling errors. Chats, tweets, text messages, emails (maybe) are very informal nature.\nMany times it requires that we inspect texts manually so I did it and I note that spams have phone numbers and urls (check that we have 98 \"www\" in our sms spams)."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"29d1e5fa-c5d8-934f-9017-9eab5d6b7947"},"outputs":[],"source":"df[\"sms_type\"][df[\"text\"].str.contains(\"\\d{4,}\")].value_counts()"},{"cell_type":"markdown","metadata":{"_cell_guid":"366e9fd7-1e61-1b4f-02d3-3647667555b6"},"source":"Here I use a regular expression to identify urls. You can download from https://github.com/rcompton/ryancompton.net/blob/master/assets/praw_drugs/urlmarker.py (Thanks to John Gruber and Ryan Compton)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"439be559-e389-4c62-d925-4deaab17ee28"},"outputs":[],"source":"WEB_URL_REGEX = r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\"\n\ndf[\"sms_type\"][df[\"text\"].str.contains(WEB_URL_REGEX)].value_counts()"},{"cell_type":"markdown","metadata":{"_cell_guid":"562b5133-7f80-47db-a175-8668c84159fb"},"source":"As you can see, that expressions seems to be keeps relation with spams texts, so we will replace them to a fix word\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"23cc3f7d-11a4-b3b8-98cd-55b0fa870a4b"},"outputs":[],"source":"df[\"text\"] = df[\"text\"].str.replace(WEB_URL_REGEX,\" someurl \")\ndf[\"text\"] = df[\"text\"].str.replace(\"\\d{4,}\",\" suspectnumber \")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1c8f3014-d82b-0881-98c4-6eaefa78e40b"},"outputs":[],"source":"vec = CountVectorizer(decode_error = 'ignore', stop_words = \"english\")\nspam_resume = get_resume(df[df[\"sms_type\"] == \"spam\"], vec, n = 25)\nspam_resume"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"684b4bdf-c0e5-7d55-6bd7-04088f8814b5"},"outputs":[],"source":"spam_resume.plot(kind = 'bar', figsize = (8, 4), fontsize = 12)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ee3276df-4a09-c7fa-90cc-05c9bbed5bd5"},"outputs":[],"source":"X = df[\"text\"].values\ny = df[\"sms_type\"].values\n    \nfolds = StratifiedKFold(n_splits = 5, shuffle = True)\ntest_scores, train_scores = np.array([]), np.array([])\n\nvocabulary = list(set(list(spam_resume.keys())))\n\nrelevant_vec = CountVectorizer(decode_error = 'ignore',\n                                stop_words = \"english\",  \n                                vocabulary = vocabulary)\n\nfor train_idx, test_idx in folds.split(X, y):\n\n    X_train, y_train = relevant_vec.fit_transform(X[train_idx]), y[train_idx]\n    X_test, y_test = relevant_vec.transform(X[test_idx]), y[test_idx]\n\n    log_model = LogisticRegression()\n    log_model.fit(X_train, y_train)\n    y_pred_test = log_model.predict(X_test)\n\n    c = confusion_matrix(y_test, y_pred_test)\n    print(c)\n    print(\"------------------------------\")\n\n    train_scores = np.append(train_scores, log_model.score(X_train, y_train))\n    test_scores = np.append(test_scores, log_model.score(X_test, y_test))\n\nprint(train_scores.mean(), test_scores.mean())"},{"cell_type":"markdown","metadata":{"_cell_guid":"f52e1f8b-956e-dd17-954f-dcd18ebc163d"},"source":"<ul>\n    <li> c[0][0] corresponds to True Nagatives -> ham text messages classified as ham (Correctly classified). </li>\n    <li> c[0][1] corresponds to False Negatives -> ham text messages classified as spam (wrong classified). </li>\n    <li> c[1][0] corresponds to False Positives -> spam text messages classified as ham (wrong classified). </li>\n    <li>c[1][1] corresponds to True Positives -> spam text messages classified as spam (Correctly classified). </li>\n</ul>\n\nSo... as you can see we have a good performance related with ham text messages classification, regarding spam messages we have approximately 10% of misclassification.\n\nIf you have any suggestion about how to improve the performace, feel free to comment :)\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"48afc3f1-cedc-4365-3cfd-99bef2e4dec4"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}