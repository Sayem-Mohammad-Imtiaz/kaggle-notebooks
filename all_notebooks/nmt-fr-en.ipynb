{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import necessary libraries","metadata":{}},{"cell_type":"code","source":"import collections\nimport numpy as np\nimport pandas as pd\nimport keras\nimport tensorflow as tf\nimport random\n\nfrom nltk.translate.bleu_score import corpus_bleu\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Model\nfrom keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\nfrom keras.layers.embeddings import Embedding\nfrom keras.optimizers import Adam\nfrom keras.losses import sparse_categorical_crossentropy\nfrom keras import Sequential","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reading the Data","metadata":{}},{"cell_type":"code","source":"DF = pd.read_csv(\"../input/language-translation-englishfrench/eng_-french.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Viewing the datasest","metadata":{}},{"cell_type":"code","source":"DF","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Renaming the columns of DataFrame","metadata":{}},{"cell_type":"code","source":"DF = DF.rename(columns={'English words/sentences': 'English', 'French words/sentences': 'French'})\nDF","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-Processing of the Data","metadata":{}},{"cell_type":"markdown","source":"## Separating the dataset into English and French","metadata":{}},{"cell_type":"code","source":"english = DF.English\nenglish","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"french = DF.French\nfrench","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenizing the English DataFrame","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import RegexpTokenizer\ntokenizer = RegexpTokenizer(r'\\w+')\nfor i,text in enumerate(english):\n    stri = \"\"\n    txt = tokenizer.tokenize(text)\n    for j in txt:\n        j = j.lower()\n        stri = stri + j\n        stri = stri + \" \"\n    english[i] = stri","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Printing the first Ten Tokenized words of English DataFrame","metadata":{}},{"cell_type":"code","source":"print(english[0:10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenizing the French DataFrame","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import RegexpTokenizer\ntokenizer = RegexpTokenizer(r'\\w+')\nfor i,text in enumerate(french):\n    stri = \"\"\n    txt = tokenizer.tokenize(text)\n    for j in txt:\n        j = j.lower()\n        stri = stri + j\n        stri = stri + \" \"\n    french[i] = stri","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Printing the first Ten Tokenized words of French DataFrame","metadata":{}},{"cell_type":"code","source":"print(french[0:10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Transforming data into arrays","metadata":{}},{"cell_type":"code","source":"n1 = 0\nn2 = 100\neng = list(english)\nfre = list(french)\n\n# for DF in english:eng.append(DF)\n\n# for DF in french:fre.append(DF)\n\neng = np.asarray(eng)\nfre = np.asarray(fre)\n\neng = eng[0:175000]\nfre = fre[0:175000]\n\nfor i in range(n1,n2):\n  print(eng[i] + \"\\t->\\t\" + fre[i] + \"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Counting English and French Words","metadata":{}},{"cell_type":"code","source":"e = [word for sentence in eng for word in sentence.split(\" \")]\nf = [word for sentence in fre for word in sentence.split(\" \")]\nenglish_word_counter = collections.Counter(e)\nfrench_word_counter = collections.Counter(f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('{} English words.'.format(len(e)))\nprint('{} French words.'.format(len(f)))\nprint(\"\\n\")\nprint('{} unique English words.'.format(len(english_word_counter)))\nprint('{} unique French words.'.format(len(french_word_counter)))\nprint(\"\\n\")\nprint('10 Most common words in the English dataset:')\nprint('\"' + '\" \"'.join(list(zip(*english_word_counter.most_common(10)))[0]) + '\"')\nprint(\"\\n\")\nprint('10 Most common words in the French dataset:')\nprint('\"' + '\" \"'.join(list(zip(*french_word_counter.most_common(10)))[0]) + '\"')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sorting the above data into table form","metadata":{}},{"cell_type":"code","source":"dict1 = {1: [\"English \", 1133720, 13917 ], \n     2: [\"French\", 1250733, 23918] \n     } \n# Print the names of the columns. \nprint (\"{:<15} {:<15} {:<15}\".format('LANGUAGE', 'TOTAL WORDS', 'UNIQUE WORDS')) \n  \n# print each data item. \nfor key, value in dict1.items(): \n    language, total_words, unique_words = value \n    print (\"{:<15} {:<15} {:<15}\".format(language, total_words, unique_words)) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Indexing of a smaple text with help of tokenization","metadata":{}},{"cell_type":"code","source":"def tokenize(x):\n    tokenizer = Tokenizer(char_level=False,oov_token=\" \")\n    tokenizer.fit_on_texts(x)\n    return tokenizer.texts_to_sequences(x), tokenizer\n\ntext_sentences = [\n    'An apple a day keeps a doctor away .',\n    'well, hope this letter of mine finds u in pink of your health .',\n    'This is a short sentence .']\n  \ntext , tokenizer = tokenize(text_sentences)\nprint(text)\nprint(tokenizer.word_index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pad(x,length=None):\n    if (length==None):\n        length = max([len(sentence) for sentence in x])\n    a = pad_sequences(x,maxlen=length,padding=\"post\")\n    return a\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Indexing of complete Dataset","metadata":{}},{"cell_type":"code","source":"def preprocess(x,y):\n    preprocess_x,x_tk = tokenize(x)\n    preprocess_y,y_tk = tokenize(y)\n\n    preprocess_x = pad(preprocess_x)\n    preprocess_y = pad(preprocess_y)\n    print(*preprocess_y.shape)\n    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n\n    return preprocess_x, preprocess_y, x_tk, y_tk\n\ndef preprocessing(x):\n    preprocess_x,x_tk = tokenize(x)\n    preprocess_x = pad(preprocess_x)\n    return preprocess_x, x_tk","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessed Information about the data","metadata":{}},{"cell_type":"code","source":"pre_eng,pre_fre,eng_tk,fre_tk = preprocess(eng,fre)\nmax_eng_seq_len = pre_eng.shape[1]\nmax_fr_seq_len = pre_fre.shape[1]\nenglish_vocab_size = len(eng_tk.word_index)\nfrench_vocab_size = len(fre_tk.word_index)\n\nprint('Data Preprocessed')\nprint(\"Max English sentence length:\", max_eng_seq_len)\nprint(\"Max French sentence length:\", max_fr_seq_len)\nprint(\"English vocabulary size:\", english_vocab_size)\nprint(\"French vocabulary size:\", french_vocab_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Implementation ","metadata":{}},{"cell_type":"code","source":"def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n\n    learning_rate = 0.001\n    model = keras.Sequential([\n                                Embedding(english_vocab_size+1, \n                                          128, \n                                          input_length = input_shape[1]),\n        \n                                Bidirectional(GRU(128, \n                                                  return_sequences=True)),\n        \n                                tf.keras.layers.Dropout(0.25),\n        \n                                TimeDistributed(Dense(french_vocab_size, \n                                                      activation='softmax'))\n                                ])\n    model.summary()\n    \n    model.compile(loss=sparse_categorical_crossentropy,\n                  optimizer=Adam(learning_rate),\n                  metrics=['accuracy'])\n    \n    return model\n\ntmp_x = pad(pre_eng, \n            max_fr_seq_len)\n\nrnn_model = embed_model(tmp_x.shape,\n                        max_fr_seq_len,\n                        english_vocab_size,\n                        french_vocab_size)\n\nrnn_model.fit(tmp_x, pre_fre, batch_size=1024, epochs=20, validation_split=0.2)\n\nrnn_model.save_weights(\"rnn_model_weights.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def logits_to_text(logits, tokenizer):\n    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n    index_to_words[0] = '<PAD>'\n\n    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking for translation ","metadata":{}},{"cell_type":"code","source":"for k in range(10):\n    predicted=[]\n    a = random.randint(0,100000)\n    print('Random Index: ', a)\n\n    print(\"PREDICTED:\\t\", end=' ')\n    for i in range(5):\n        x = logits_to_text(rnn_model.predict(tmp_x[a])[i], fre_tk)\n        print(x, end =' ')\n        if x!='<PAD>':\n            predicted.append(x)\n\n    \n    english = eng[a].split()\n    french = fre[a].split()\n\n    print(\"\\n\\nENGLISH:\\t\", eng[a] + \"\\nFRENCH:\\t\\t \" + fre[a] + \"\\n\")\n    print('\\nIndexing: ',tmp_x[a])\n    \n    print('\\n|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CALCULATING BLEU SCORE","metadata":{}},{"cell_type":"code","source":"n = []\nlst=[]\nk=0\nwhile k<1000:\n    \n    predicted=[]\n    a = random.randint(0,175000)\n    \n    for i in range(5):\n        x = logits_to_text(rnn_model.predict(tmp_x[a])[i], fre_tk)\n    \n        if x!='<PAD>':\n            predicted.append(x)\n\n    english = eng[a].split()\n    french = fre[a].split()\n\n    references = [[french]]\n    candidates = [predicted]\n    score = corpus_bleu(references, candidates, weights=(0.05, 0.25, 0.35, 0.35))\n    lst.append(score)\n    if score>.8:\n        n+=[score]\n    k+=1\n    \n    if k%100==0:\n        print(k)        \ndef average(lst):\n    print(sum(lst)/len(lst))   \naverage(n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"AVERAGE BLEU SCORE: \", end ='\\t') \naverage(n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}}]}