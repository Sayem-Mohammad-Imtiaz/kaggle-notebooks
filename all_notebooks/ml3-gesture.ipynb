{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.colors as mcolors\n\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score,plot_confusion_matrix\nfrom sklearn import tree\nfrom sklearn.decomposition import PCA\n\nimport matplotlib.pyplot as plt\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras import backend as K\n\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.ensemble import AdaBoostClassifier\nimport matplotlib.pyplot as plt\nimport copy\nfrom sklearn.model_selection import KFold\nimport time\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import f1_score\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n\nimport sys\nimport random\nfrom sklearn.cluster import KMeans\n\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.decomposition import FastICA\nfrom sklearn.decomposition import PCA, FastICA\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.decomposition import FactorAnalysis\nfrom sklearn.mixture import GaussianMixture\nfrom scipy.stats import kurtosis\nsns.set(context=\"notebook\", palette=\"Spectral\", style = 'darkgrid' ,font_scale = 1.5, color_codes=True)\ncolors = ['blue','green','purple','orange','m','y','forestgreen','darkorange','darkblue','indigo','brown','steelblue','chocolate','olivedrab']\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nrock_data = pd.read_csv(\"../input/emg-4/0.csv\", header=None )\nscissors_data = pd.read_csv(\"../input/emg-4/1.csv\", header=None )\npaper_data = pd.read_csv(\"../input/emg-4/2.csv\", header=None )\nok_data = pd.read_csv(\"../input/emg-4/3.csv\", header=None )\ndata = pd.concat([rock_data, scissors_data, paper_data, ok_data], axis = 0)\n\n\nX = data.drop(data.columns[-1],axis=1)\ny = data[data.columns[-1]]\n\n#Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\none_hot = OneHotEncoder()\ny_train = one_hot.fit_transform(y_train.values.reshape(-1, 1)).todense()\ny_test = one_hot.transform(y_test.values.reshape(-1, 1)).todense()\n\nmany_colors = [];\nfor i in range(200):\n    many_colors.append((min(0.8,random.random()),min(0.8,random.random()),min(0.8,random.random())))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"SHAPE:\",X.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Part 1: K-Means","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nwcss = []\nx_vals = []\nfor i in range(1, 20):\n    t = time.time()\n    x_vals.append(int(i**1.8))\n    kmeans = KMeans(n_clusters = int(i**1.8), init = 'k-means++', random_state = 42)\n    kmeans.fit(X_train)\n    # inertia method returns wcss for that model\n    wcss.append(kmeans.inertia_)\n    print(int(i**1.8), \"intertia:\",kmeans.inertia_,\"\\ttime:\",time.time()-t)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.lineplot(x_vals, wcss,marker='o',color='red')\nplt.title('Gesture Recognition: Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np_X_train = X_train.to_numpy()[0:300]\n\n\nclusts = 45\nkmeans = KMeans(n_clusters = clusts, init = 'k-means++', random_state = 42)\ny_kmeans = kmeans.fit_predict(np_X_train)\n\nfor i in range(1,2):\n    # Visualising the clusters\n    plt.figure(figsize=(12,8))\n    \n    x_variable = 0\n    for k in range(clusts):\n        sns.scatterplot(np_X_train[(y_kmeans == k), x_variable], np_X_train[y_kmeans == k, i+1], color= many_colors[k],s=50)#label = 'Cluster '+str(k+1)\n    #sns.scatterplot(np_X_train[y_kmeans == 1, x_variable], np_X_train[y_kmeans == 1, i+1], color = 'blue', label = 'Cluster 2',s=50)\n    #sns.scatterplot(np_X_train[y_kmeans == 2, x_variable], np_X_train[y_kmeans == 2, i+1], color = 'green', label = 'Cluster 3',s=50)\n    #sns.scatterplot(np_X_train[y_kmeans == 3, x_variable], np_X_train[y_kmeans == 3, i+1], color = 'purple', label = 'Cluster 4',s=50)\n    #sns.scatterplot(np_X_train[y_kmeans == 4, x_variable], np_X_train[y_kmeans == 4, i+1], color = 'orange', label = 'Cluster 5',s=50)\n    sns.scatterplot(kmeans.cluster_centers_[:, x_variable], kmeans.cluster_centers_[:, i+1], color = 'red', alpha=0.3,\n                    label = 'Centroids',s=500,marker='o')\n    \n    plt.grid(False)\n    plt.title('K-Means Clustering')\n    plt.xlabel('Feature '+str(x_variable))\n    plt.ylabel('Feature '+str(i))\n    #plt.legend()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Expecting Maximization","metadata":{}},{"cell_type":"code","source":"bics = [];\naics = [];\navgs = [];\nx_vals = []\n\nfor i in range(1,20):\n    this_x = int(i**1.4)\n    print(this_x)\n    mixture = GaussianMixture(n_components=this_x, init_params='kmeans')\n    mixture.fit(X_train);\n    bics.append(mixture.bic(X_train));\n    aics.append(mixture.aic(X_train));\n    avgs.append((mixture.bic(X_train)+mixture.aic(X_train))/2)\n    x_vals.append(this_x);\n\nplt.figure(figsize=(8,5))\nsns.lineplot(x_vals, bics,marker='o',color='red', label=\"BIC\")\nsns.lineplot(x_vals, aics,marker='o',color='blue', label=\"AIC\")\n#sns.lineplot(x_vals, avgs,marker='o',color='green', label=\"Avg\")\nplt.title('Gesture Recognition: Expecting Maximization')\nplt.xlabel('Number of clusters')\nplt.ylabel('Criterion Value')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np_X_train = X_train.to_numpy()[0:300]\n\ncolors = ['blue','green','purple','orange','m','y','forestgreen','darkorange','darkblue','indigo','brown','steelblue','chocolate','olivedrab']\n\nclusts = 6\nkmeans = GaussianMixture(n_components=clusts, init_params='kmeans')\ny_kmeans = kmeans.fit_predict(np_X_train);\n\nfor i in range(1,2):#3,4\n    # Visualising the clusters\n    plt.figure(figsize=(12,8))\n    \n    x_variable = 0\n    for k in range(clusts):\n        sns.scatterplot(np_X_train[(y_kmeans == k), x_variable], np_X_train[y_kmeans == k, i+1], color= colors[k], label = 'Cluster '+str(k+1),s=50)\n    #sns.scatterplot(np_X_train[y_kmeans == 1, x_variable], np_X_train[y_kmeans == 1, i+1], color = 'blue', label = 'Cluster 2',s=50)\n    #sns.scatterplot(np_X_train[y_kmeans == 2, x_variable], np_X_train[y_kmeans == 2, i+1], color = 'green', label = 'Cluster 3',s=50)\n    #sns.scatterplot(np_X_train[y_kmeans == 3, x_variable], np_X_train[y_kmeans == 3, i+1], color = 'purple', label = 'Cluster 4',s=50)\n    #sns.scatterplot(np_X_train[y_kmeans == 4, x_variable], np_X_train[y_kmeans == 4, i+1], color = 'orange', label = 'Cluster 5',s=50)\n    sns.scatterplot(kmeans.means_[:, x_variable], kmeans.means_[:, i+1], color = 'red', alpha=0.3,\n                    label = 'Centroids',s=500,marker='o')\n    \n    plt.grid(False)\n    plt.title('Expecting Maximization')\n    plt.xlabel('Feature 0')\n    plt.ylabel('Feature 1')\n    plt.legend()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PCA INITIAL","metadata":{}},{"cell_type":"code","source":"losses = [];\nfor i in range(1,64):\n    pca = PCA(n_components=i)\n    X_train_ica = pca.fit_transform(X_train)\n    X_projected = pca.inverse_transform(X_train_ica)\n\n    loss = np.mean((X_train - X_projected) ** 2)\n    #print(i,\"LOSS:\",np.mean(list(loss)))\n    losses.append(np.mean(list(loss)))\n    \nplt.plot(range(1,64),losses,color='g')\nplt.rcParams[\"figure.figsize\"] = (10,6)\nplt.xlabel('New Features')\nplt.ylabel('Eigenvalues')\nplt.title(\"PCA Eigenvalues by Number of New Features\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"losses = []\n\nfor n in range(1,64):\n    total_kurtosis = [];\n    for i in range(1,n):\n        ica = FastICA(n_components=i)\n        X_train_ica = ica.fit_transform(X_train)\n        dt = tree.DecisionTreeClassifier(random_state=0)\n\n        this_x = np.array(X_train)[:,i]\n        new_x = []\n        for j in range(len(this_x)):\n            new_x.append([this_x[j]])\n        #print(\"TX:\",new_x[0:5])\n        dt.fit(new_x,y_train)\n        preds = dt.predict(new_x)\n        #print(i,\"Kurtosis:\",kurtosis(preds))\n        total_kurtosis.append(kurtosis(preds))\n    print(n,\"Avg kurtosis:\", np.mean(total_kurtosis),end=\"\\t\")\n    if n > 1: losses.append(np.mean(total_kurtosis))\nplt.plot(range(2,64),losses,marker='s',color='g')\nplt.rcParams[\"figure.figsize\"] = (10,6)\nplt.xlabel('New Features')\nplt.ylabel('Kurtosis')\nplt.title(\"ICA Kurtosis by Number of New Features\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_losses = []\n\nbest_loss = 999999;\nbest_index = 0;\nfor i in range(1,64):\n    losses = []\n    for j in range(100):\n        pca = GaussianRandomProjection(n_components=i)\n        pca.fit(X_train)\n        data_reduced = np.dot(X_train, pca.components_.T) # transform\n        X_projected = np.dot(data_reduced, pca.components_)# inverse_transform\n        loss = np.mean((X_train - X_projected) ** 2)\n        ##print(i,\"LOSS:\",np.mean(list(loss)))\n        losses.append(np.mean(list(loss)))\n    if i % 5 == 0: print(i,\"LOSS:\",np.mean(losses));\n    total_losses.append(np.mean(losses))\n    if np.mean(losses) < best_loss:\n        best_loss = np.mean(losses)\n        best_index = i;\nprint(\"Best loss, index:\",best_loss, best_index)\n\nplt.plot(range(1,64),total_losses,color='g')\nplt.rcParams[\"figure.figsize\"] = (10,6)\nplt.xlabel('New Features')\nplt.ylabel('Reconstruction Error')\nplt.title(\"Randomized Projections Reconstruction Error by Number of New Features\")\nplt.show()\n\n#Uses reconstructed Error\n#After running 100 times each, 12 is the best number of components.\n#Use 4 because of elbow method","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"losses = [];\nfor n in range(1,64):\n    total_kurtosis = [];\n    for i in range(1,n):\n        ica = FactorAnalysis(n_components=i)\n        X_train_ica = ica.fit_transform(X_train)\n        dt = tree.DecisionTreeClassifier(random_state=0)\n\n        this_x = np.array(X_train)[:,i]\n        new_x = []\n        for j in range(len(this_x)):\n            new_x.append([this_x[j]])\n        #print(\"TX:\",new_x[0:5])\n        dt.fit(new_x,y_train)\n        preds = dt.predict(new_x)\n        #print(i,\"Kurtosis:\",kurtosis(preds))\n        total_kurtosis.append(kurtosis(preds))\n    print(n,\"Avg kurtosis:\", np.mean(total_kurtosis),end=\"\\t\")\n    if i > 1: losses.append(np.mean(total_kurtosis))\n#Best kurtosis of -1.091 with 6 components","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(range(2,56),losses,marker='s',color='g')\nplt.rcParams[\"figure.figsize\"] = (10,6)\nplt.xlabel('New Features')\nplt.ylabel('Kurtosis')\nplt.title(\"Factor Analysis Kurtosis by Number of New Features\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Draw K-Means with Reduction:","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nwcss_list = [[],[],[],[]]\nx_vals = []\nX_train_list = [PCA(n_components=30).fit_transform(X_train),\n                FastICA(n_components=21).fit_transform(X_train),\n                GaussianRandomProjection(n_components=10).fit_transform(X_train),\n                FactorAnalysis(n_components=21).fit_transform(X_train)]\n\nfor i in range(1, 20):\n    t = time.time()\n    x_vals.append(int(i**1.5))\n    \n    for j in range(4):\n        kmeans = KMeans(n_clusters = int(i**1.5), init = 'k-means++', random_state = 42)\n        kmeans.fit(X_train_list[j])\n        wcss_list[j].append(kmeans.inertia_)\n    print(i,\"t:\",time.time()-t,end=\"\\t\")\n   \n#Plot the charts\nplt.figure(figsize=(8,5))\n##plt.yscale(\"log\")\nsns.lineplot(x_vals, wcss_list[0],marker='o',color='red',label=\"PCA\")\nsns.lineplot(x_vals, wcss_list[1],marker='o',color='blue',label=\"ICA\")\nsns.lineplot(x_vals, wcss_list[2],marker='o',color='orange',label=\"RP\")\nsns.lineplot(x_vals, wcss_list[3],marker='o',color='purple',label=\"FA\")\nplt.title('Gesture Recognition: Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()\nprint(\"ICA:\",wcss_list[1])\nprint(\"X vals\",x_vals)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reduction_models = [PCA(n_components=2), FastICA(n_components=2), GaussianRandomProjection(n_components=2), FactorAnalysis(n_components=2)]\nrandom.shuffle(many_colors)\n\nclusters = [4,4,4,4]\nf_list = [[0,1],[0,1],[0,1],[0,1]]\n#bests: 0,1   3,4  0,1  0,1\ntitles = [\"PCA\", \"ICA\", \"Random Projection\", \"Factor Analysis\"]\n\nnp_X_train = X_train.to_numpy()[0:300]\n\nfor i in range(len(reduction_models)):\n    model = reduction_models[i];\n    model.fit_transform(np_X_train);\n    model_X_train = model.transform(np_X_train);\n    model_X_test = model.transform(X_test)\n    \n    kmeans = KMeans(n_clusters = clusters[i], init = 'k-means++')\n    y_kmeans = kmeans.fit_predict(model_X_train)\n    #print(\"Y MEANS:\",y_kmeans)\n\n    plt.figure(figsize=(12,8))\n    for k in range(clusters[i]):#try with 3 and for, and 7 and 4.\n        sns.scatterplot(model_X_train[(y_kmeans == k), f_list[i][0]], model_X_train[y_kmeans == k, f_list[i][1]], color= many_colors[k],s=50)#, label = 'Cluster '+str(k+1)\n        \n    sns.scatterplot(kmeans.cluster_centers_[:, f_list[i][0]], kmeans.cluster_centers_[:, f_list[i][1]], color = 'red', alpha=0.3,\n                    label = 'Centroids',s=500,marker='o')\n    plt.grid(False)\n    plt.title('K-Means Clustering with '+titles[i])\n    plt.xlabel('Reduced Feature '+str(f_list[i][0]))\n    plt.ylabel('Reduced Feature '+str(f_list[i][1]))\n    #plt.legend()\n    plt.show()\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## NOW USE EM:","metadata":{}},{"cell_type":"code","source":"X_train_list = [PCA(n_components=30).fit_transform(X_train),\n                FastICA(n_components=21).fit_transform(X_train),\n                GaussianRandomProjection(n_components=10).fit_transform(X_train),\n                FactorAnalysis(n_components=21).fit_transform(X_train)]\nmodel_titles = [\"PCA\",\"ICA\",\"RP\", \"FA\"]\naic_colors = ['red','orange','brown','gray']\nbic_colors = ['blue', 'green', 'purple', 'black']\n\nplt.figure(figsize=(10,6))\n\nfor a in range(4):\n    bics = [];\n    aics = [];\n    avgs = [];\n    x_vals = []\n\n    for i in range(1,20):\n        this_x = int(i**1.4)\n        print(i, this_x,end=\"\\t\")\n        mixture = GaussianMixture(n_components=this_x, init_params='kmeans')\n        mixture.fit(X_train_list[a]);\n        bics.append(mixture.bic(X_train_list[a]));\n        aics.append(mixture.aic(X_train_list[a]));\n        avgs.append((mixture.bic(X_train_list[a])+mixture.aic(X_train_list[a]))/2)\n        x_vals.append(this_x);\n    sns.lineplot(x_vals, bics,marker='o',color=aic_colors[a], label=model_titles[a] + \" BIC\")\n    sns.lineplot(x_vals, aics,marker='o',color=bic_colors[a], label=model_titles[a] + \" AIC\")\n\n\n#sns.lineplot(x_vals, avgs,marker='o',color='green', label=\"Avg\")\nplt.title('Gesture Recognition: Expecting Maximization')\nplt.xlabel('Number of clusters')\nplt.ylabel('Criterion Value')\nplt.legend()\nplt.show()\nfor i in range(10):\n    print(int(i**1.4),end=\",\")\n#print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reduction_models = [PCA(n_components=2), FastICA(n_components=2), GaussianRandomProjection(n_components=2), FactorAnalysis(n_components=2)]\nrandom.shuffle(many_colors)\n\nclusters = [4,4,4,4]\nf_list = [[0,1],[0,1],[0,1],[0,1]]\n#bests: 0,1   3,4  0,1  0,1\ntitles = [\"PCA\", \"ICA\", \"Random Projection\", \"Factor Analysis\"]\n\nnp_X_train = X_train.to_numpy()[0:300]\n\nfor i in range(len(reduction_models)):\n    model = reduction_models[i];\n    model.fit_transform(np_X_train);\n    model_X_train = model.transform(np_X_train);\n    model_X_test = model.transform(X_test)\n    \n    kmeans = GaussianMixture(n_components=clusters[i], init_params='kmeans')\n    y_kmeans = kmeans.fit_predict(model_X_train)\n    #print(\"Y MEANS:\",y_kmeans)\n\n    plt.figure(figsize=(12,8))\n    for k in range(clusters[i]):#try with 3 and for, and 7 and 4.\n        sns.scatterplot(model_X_train[(y_kmeans == k), f_list[i][0]], model_X_train[y_kmeans == k, f_list[i][1]], color= many_colors[k],s=50)#, label = 'Cluster '+str(k+1)\n        \n    #sns.scatterplot(kmeans.cluster_centers_[:, f_list[i][0]], kmeans.cluster_centers_[:, f_list[i][1]], color = 'red', alpha=0.3,\n    #                label = 'Centroids',s=500,marker='o')\n    plt.grid(False)\n    plt.title('Expecting Maximization Clustering with '+titles[i])\n    plt.xlabel('Reduced Feature '+str(f_list[i][0]))\n    plt.ylabel('Reduced Feature '+str(f_list[i][1]))\n    #plt.legend()\n    plt.show()\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}