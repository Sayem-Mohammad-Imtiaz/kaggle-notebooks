{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n\nGLOVE_DIR = '../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt'\nDATA_DIR = '../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv'\n\ndf = pd.read_csv(DATA_DIR)\ndf.replace(['positive', 'negative'], [1, 0], inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n\n# Data preparing\n\ndata = list(df.review)\nlabels = list(df.sentiment)\n\ntokenizer = Tokenizer(num_words=10000) # Only 10000 most met words\ntokenizer.fit_on_texts(data)\nsequences = tokenizer.texts_to_sequences(data)\n\ndata = pad_sequences(sequences, maxlen=100) # Only 100 words in one review\nlabels = np.array(labels)\n\n\ntrain_data = data[:30000]\ntrain_labels = labels[:30000]\n\nvalidation_data = data[30000:40000]\nvalidation_labels = labels[30000:40000]\n\ntest_data = data[40000:]\ntest_labels = labels[40000:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_index = {}\nwith open(GLOVE_DIR) as file: # Preparing glove\n    for line in file:\n        values = line.split()\n        key = values[0]\n        value = values[1:]\n        embedding_index[key] = np.array(value, dtype='float32')\n        \n        \nembedding_matrix = np.zeros((10000, 100)) # 10000 - num_words, 100 - length of sequence\nfor word, i in tokenizer.word_index.items():\n    if i < 10000:\n        embedding_vector = embedding_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import losses, optimizers\nfrom tensorflow.keras.layers import Dense, Embedding, Flatten\n\nmodel = tf.keras.models.Sequential([\n    Embedding(10000, 100, input_length=100),\n    Flatten(),\n    Dense(32, activation=tf.nn.relu),\n    Dense(1, activation=tf.nn.sigmoid),\n])\n\nmodel.layers[0].set_weights([embedding_matrix]) # Add glove to our model\nmodel.layers[0].trainable = False # Freeze embedding layer\n\nmodel.compile(\n    loss = losses.binary_crossentropy,\n    optimizer = optimizers.RMSprop(lr=0.001),\n    metrics = ['accuracy'],\n)\n\nhistory = model.fit(\n    train_data, train_labels,\n    validation_data = [validation_data, validation_labels],\n    batch_size = 32, \n    epochs = 10,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\nplt.plot(history.history['accuracy'], 'r-', label='Train accuracy')\nplt.plot(history.history['val_accuracy'], 'b-', label='Validation accuracy')\nplt.legend()\nplt.grid()\nplt.show()\n\n_, accuracy = model.evaluate(test_data, test_labels, verbose=0)\nprint('Test accuracy: ', round(accuracy * 100, 2), '%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}