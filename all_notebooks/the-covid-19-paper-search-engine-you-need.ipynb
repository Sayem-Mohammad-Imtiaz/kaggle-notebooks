{"cells":[{"metadata":{},"cell_type":"markdown","source":"# The Covid-19 Paper Search Engine you Need (Word2Vec+BM25)"},{"metadata":{},"cell_type":"markdown","source":"As the DS team of Xpand IT, we faced this problem with two objectives in mind: help the community in a time of worldwide crisis and consolidate our NLP skillset. This notebook shows the work we have achieved so far.\n\nHere, we create a search engine that aims to represent a more general approach to solving the 10 tasks presented.\nOur goal with this search engine was to isolate the most relevant papers according to the search queue and present the abstract and relevant sentences of said papers.\nWe decided on a broader approach as it was our understanding that delving too deep into a certain task without the correct knowledge could lead to wrong findings that could mislead the community."},{"metadata":{},"cell_type":"markdown","source":"## Our Approach Description\n#### Abstract Preprocessing\n* Remove non english papers\n* Delete empty abstract entries\n* Remove stop words\n* Remove punctuation\n* Lemmatize\n* Remove digits\n* Tokenize\n   \n#### Create models\n* Word2Vec using tokenize abstracts\n* BM25 using tokenize abstracts\n    \n#### Query Pipeline\n* Preprocess query\n* Remove verbs from query\n* Remove tokens not in Word2Vec vocabulary\n* Use Word2Vec to find new_keywords (similar to tokens in query)\n* Use BM25 to find most relevant papers based on (query+new_keywords)\n* Get body text from thoose papers\n* Preprocess body text sentences\n* Create local BM25 model based on body text sentences\n* Use local BM25 to find most relevant sentences based on (query+new_keywords)\n* Display results as HTML"},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing Abstracts\nLoad metadata file to a dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_path = '/kaggle/input/CORD-19-research-challenge/'#'../../data/raw/'\n\nimport pandas as pd\ndf = pd.read_csv(data_path+\"metadata.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Detect title language and add it as a dataframe column."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install langdetect\nfrom langdetect import detect, DetectorFactory\nDetectorFactory.seed = 0\n\n\ndef get_language(text):\n    text = str(text)\n    try:\n        language = detect(text)\n    except:\n        language = \"error\"\n        print(\"This row throws and error:\", text)\n    return language\n\n\ndf[\"language\"] = df[\"title\"].apply(lambda x: get_language(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop all non english papers from the dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.shape)\ndf = df[df.language == 'en']\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create temporary dataframe to pre-process abstract text."},{"metadata":{"trusted":true},"cell_type":"code","source":"trim_data = df[[\"cord_uid\", \"abstract\"]]\nprint(trim_data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop all dataframe rows where the abstract is empty."},{"metadata":{"trusted":true},"cell_type":"code","source":"trim_data = trim_data.dropna()\nprint(trim_data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clean string given as argument.\n* Remove stopwords\n* Remove punctuation\n* Lemmatize\n* Remove digits\n* Tokenize (if True)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport re\nimport string\n\nnltk.download('stopwords')\nnltk.download('punkt')\n\n\ndef clean(doc, tokenize=False):\n    doc = str(doc)\n    stop_free = \" \".join([i for i in doc.lower().split() if i not in stopwords.words('english')])\n    punc_free = ''.join(ch for ch in stop_free if ch not in set(string.punctuation))\n    normalized = \" \".join(WordNetLemmatizer().lemmatize(word, pos=\"v\") for word in punc_free.split())\n    processed = re.sub(r\"\\d+\",\"\",normalized)\n    if tokenize is True:\n        processed = nltk.word_tokenize(processed)\n    return processed","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apply clean to abstract."},{"metadata":{"trusted":true},"cell_type":"code","source":"trim_data[\"processed\"] = trim_data[\"abstract\"].apply(clean)\ntrim_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add trim_data to df and delete it."},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"abstract_processed\"] = trim_data[\"processed\"]\ndel trim_data\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Extract tokens from previously processed abstract."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['tokens'] = df['abstract_processed'].apply(lambda x: nltk.word_tokenize(str(x)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a flag that identifies if paper contains covid-19 information. This is done by detecting one of the following list names in abstract or in title. This list was extracted from:\n* https://www.kaggle.com/maria17/cord-19-explore-drugs-being-developed\n* https://www.kaggle.com/rismakov/research-search-tool-and-article-summary"},{"metadata":{"trusted":true},"cell_type":"code","source":"covid19_names = {\n    'COVID19',\n    'COVID-19',\n    '2019-nCoV',\n    '2019-nCoV.',\n    # 'novel coronavirus',  # too ambiguous, may mean SARS-CoV\n    'coronavirus disease 2019',\n    'Corona Virus Disease 2019',\n    '2019-novel Coronavirus',\n    'SARS-CoV-2',\n    'covid-19', \n    'covid 19',\n    'covid-2019',\n    '2019 novel coronavirus', \n    'corona virus disease 2019',\n    'coronavirus disease 19',\n    'coronavirus 2019',\n    '2019-ncov',\n    'ncov-2019', \n    'wuhan virus',\n    'wuhan coronavirus',\n    'wuhan pneumonia',\n    #'NCIP', commented to fix priNCIPal problem\n    'sars-cov-2',\n    'sars-cov2'\n}\n\n\n# detect if text contains covid-19 terms\ndef has_covid19(text):\n    for name in covid19_names:\n        if text and str(name).lower() in str(text).lower():\n            return True\n    return False\n\n\ndf['title_has_covid19'] = df.title.apply(has_covid19)\ndf['abstract_has_covid19'] = df.abstract.apply(has_covid19)\n\ndf['has_covid19'] = df['title_has_covid19'] | df['abstract_has_covid19']\n\ndel df['title_has_covid19']\ndel df['abstract_has_covid19']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check number of papers with and without covid-19 terms."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('has_covid19').size()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating Models\nCreate word2vec model using the tokens extracted from abstracts.\nThis model will be used to detect similar keywords from a given query."},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import Word2Vec\nEMBEDDING_DIM = 50\nword2vec = Word2Vec(sentences=df['tokens'], size=EMBEDDING_DIM, window=5, workers=4, min_count=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a BM25 model using the tokens extracted from abstracts.\nThis model will be used to rank most similar papers to a given query."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install rank-bm25\nfrom rank_bm25 import BM25Okapi\nbm25 = BM25Okapi(df['tokens'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Function to extract body text from papers\nThe functions creates a dataframe with body text of the given papers. The dataframe created has 3 columns: sha, text and tokens. Each row is a different body text sentence."},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\nimport glob\n\ndef create_bodytext_dataframe(dataframe):\n\n    df_body = pd.DataFrame(columns=['sha', 'text', 'tokens'])\n\n    for index, row in dataframe.iterrows():\n        file_name = row['sha']\n        file_list = glob.glob(data_path+'**/**/**/'+str(file_name)+'.json')\n        if len(file_list) > 0:\n            file_path = file_list[0]\n        else:\n            #print('File '+str(file_name)+' not found... :(')\n            continue\n\n        with open(file_path) as json_data:\n            data = json.load(json_data)\n            body_list = [bt['text'] for bt in data['body_text']]\n\n            # each json has a series of segments, maybe paper pages...\n            for json_segment in body_list:\n                # split segments into sentences [this can be improved: (fig. 11) will be splitted]\n                sentences = json_segment.split(\". \")\n                for sentence in sentences:\n                    df_body = df_body.append({\n                        'sha': file_name,\n                        'text': sentence,\n                        'tokens': clean(sentence, tokenize=True)\n                    }, ignore_index=True)\n\n    return df_body","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Query pipeline\nThis function is used to make a search in our dataframe based in a given query.\n\n**Inputs:**\n* Query\n* Threshold (1-Search only the keywords from query / 0.5-Search query keywords plus similar keywords / 0-Search all  word2vec words)\n* N (Number of papers in result)\n* Covid19_only (Search filter)\n\n**Output:**\n* Dataframe with top N results"},{"metadata":{"trusted":true},"cell_type":"code","source":"def search(query, threshold=0.7, N=5, covid19_only=True):\n    tokenized = clean(query, tokenize=True)\n    \n    #remove verbs from tokens\n    verb_tags = ['VB' ,'VBD', 'VBG', 'VBN', 'VBZ'] #'VBP'\n    tags = nltk.pos_tag(tokenized)\n    for tuple_ in tags:\n        if tuple_[1] in verb_tags:\n            tokenized.remove(tuple_[0])\n\n    keywords_ = []\n    # add to keywords if tokens are in word2vec dictionary\n    for token in tokenized:\n        if token in word2vec.wv.vocab:\n            keywords_.append(token)\n\n    keywords = keywords_[:]\n    # search for keywords related to query\n    for kw in keywords_:\n        most_similar = word2vec.wv.most_similar(positive=[kw])\n        for word in most_similar:\n            if word[1] > threshold:\n                keywords.append(word[0])\n\n    # rank papers based on keywords similarity\n    doc_scores = bm25.get_scores(keywords)\n\n    # add scores to df\n    df['score'] = doc_scores\n\n    # create dataframe with Top N results (filter covid-19 terms)\n    if covid19_only is False:\n        df_result = df.sort_values(by=['score'])[::-1].head(N)\n    else:\n        df_result = df[df['has_covid19'] == True].sort_values(by=['score'])[::-1].head(N)\n\n    # create dataframe with the bodies text\n    df_bodies = create_bodytext_dataframe(df_result)\n\n    # create a local bm25 model with the tokens from body text\n    local_bm25 = BM25Okapi(df_bodies['tokens'])\n    sentence_scores = local_bm25.get_scores(keywords)\n    df_bodies['score'] = sentence_scores\n\n    # add top X sentences in each paper in df_results as highlight1, 2 and 3\n    X = 3\n    for index, row in df_result.iterrows():\n        best_sentences = df_bodies[df_bodies['sha'] == row['sha']].sort_values(by=['score'], ascending=False)['text'][:X]\n        for idx, text in enumerate(best_sentences):\n            df_result.at[index, 'highlight'+str(idx+1)] = text\n\n    return df_result, query, keywords","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Display the results dataframe as HTML inside the notebook."},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.core.display import display, HTML\n\n\ndef result_display(result, query, keywords):\n    display(HTML(\n        '<h2 style=\"color:#ff6600\">'+query+'</h2>' +\n        '<p><b>Keywords: </b>'+(','.join(keywords))+'</p>'\n    ))\n\n    for index, row in result.iterrows():\n        \n        # display title and abstract\n        display(HTML(\n            '<h3 style=\"color:#ffa64d\">' + row['title'] + '</h3>' +\n            '<p><b>' + str(row['publish_time']) + '</b><i> ' + str(row['journal']) +'</i></p>'+\n            '<p>' + str(row['abstract']) + '</p>'\n        ))\n        \n        # display highlights\n        if pd.isnull(row['highlight1']):\n            display(HTML('<p><b>No highlights to show :(</b></p>'))\n        else:\n            display(HTML(\n                '<p><b>Highlights:</b></p>' +\n                '<ul>' +\n                  '<li>'+str(row['highlight1'])+'</li>' +\n                  '<li>'+str(row['highlight2'])+'</li>' +\n                  '<li>'+str(row['highlight3'])+'</li>' +\n                '</ul>'\n            ))\n        \n        # display paper link\n        display(HTML(\n            '<p><a href='+str(row['url'])+'>Link to paper</a></p>' +\n            '<br>'\n        ))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**NOTE:** Some of the papers do not have highlights because 'sha' column in dataframe is null :(\nHowever we present the paper can still be relevant for your search, you can still check full paper by clicking in the provided link.\n\n### Query and Display Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"query_list = [\n    \"What do we know about virus genetics, origin, and evolution?\",\n    \"Evidence of whether farmers are infected, and whether farmers could have played a role in the origin.\",\n    \"Animal host(s) and any evidence of continued spill-over to humans\"\n]\n\nfor query_ in query_list:\n    result, query, keywords = search(query_)\n    result_display(result, query, keywords)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusions\n  As said before the team decided on a broader approach as it was our understanding that delving too deep into a certain task without the correct knowledge could lead to wrong findings that could mislead the community. And for that reason we developed a search engine that can help researchers.\n\n  Therefore, the code presented in this notebook can make the task of finding relevant papers a lot quicker when compared to manual searching, increasing the real productivity time that can be used to read the papers in more detail or to make another research task."},{"metadata":{},"cell_type":"markdown","source":"## Future Work\n* Create models and search using body text instead of abstracts\n* Develop a better way to split body text sentences\n\n##### Leave a comment if you have any suggestion and want to contribute to this notebook ;)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}