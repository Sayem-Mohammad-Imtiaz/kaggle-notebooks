{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **Table of Contents:**\n* Introduction\n* Exploratory data analysis\n* Data Preprocessing\n    - Converting Features\n    - Creating Categories\n    - Creating new Features\n* Building Machine Learning Models\n    - Training different models\n    - Which is the best model ?","metadata":{"_uuid":"6c2350adbb20ec2b769d7e819d89f87d62b86c59"}},{"cell_type":"markdown","source":"# **Introduction**\n\nThe database we are working with classifies people into 16 distinct personality types showing their last 50 tweets, separated by \"|||\". \n\nOur goal will be to create new columns based on the content of the tweets, in order to create a predictive model. As we will see, this can be quite tricky and our creativity comes into play when analysing the content of the tweets.\n\nWe begin by importing our dataset and showing some info, for an initial exploratory analysis.","metadata":{"_cell_guid":"14ff033f-691f-49e3-b1b2-260b28adfe58","_uuid":"9d5cdd7511862e240273f2687c2d69f3c9bc8568"}},{"cell_type":"code","source":"import re\nimport numpy as np\nimport collections\nfrom collections import Counter\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom bokeh.io import output_file, show\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource, HoverTool\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\n\ndf = pd.read_csv('../input/mbti_1.csv')\nprint(df.head(10))\nprint(\"*\"*40)\nprint(df.info())\n","metadata":{"_cell_guid":"c1984163-fffe-41aa-99a0-243673f1bf51","_uuid":"e1fd66d81a1166c3f8054857e1824c5de671bb09","execution":{"iopub.status.busy":"2021-06-03T13:43:01.899679Z","iopub.execute_input":"2021-06-03T13:43:01.900005Z","iopub.status.idle":"2021-06-03T13:43:04.902561Z","shell.execute_reply.started":"2021-06-03T13:43:01.899951Z","shell.execute_reply":"2021-06-03T13:43:04.900016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can thus see that there are no null inputs, which means there is no need for cleaning the data. \n\nThe first idea that pops up is checking if the words per tweet of each person shows us some information. For that reason, we can create a new column as shown below.\n","metadata":{"_cell_guid":"fb2b5c26-42e1-4b04-8ef4-16999b6ab6f1","_uuid":"84dec5efb299b0d3fb4988a295e8c03087539f26"}},{"cell_type":"code","source":"df['words_per_comment'] = df['posts'].apply(lambda x: len(x.split())/50)\nprint(df.head())","metadata":{"_cell_guid":"e8029602-1dc9-4a2a-b4e1-7072f81c565b","_uuid":"ffcbdf4d43d2017e187f58a69d58cde88f5206b2","execution":{"iopub.status.busy":"2021-06-03T13:43:04.903662Z","iopub.execute_input":"2021-06-03T13:43:04.903936Z","iopub.status.idle":"2021-06-03T13:43:05.601137Z","shell.execute_reply.started":"2021-06-03T13:43:04.903892Z","shell.execute_reply":"2021-06-03T13:43:05.600216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['words_per_comment'].mean()","metadata":{"execution":{"iopub.status.busy":"2021-06-03T13:43:05.60541Z","iopub.execute_input":"2021-06-03T13:43:05.605881Z","iopub.status.idle":"2021-06-03T13:43:05.615039Z","shell.execute_reply.started":"2021-06-03T13:43:05.605767Z","shell.execute_reply":"2021-06-03T13:43:05.614224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Exploratory data analysis**\n\nWe may use it for one reason or for another, but one thing we can do is printing a violin plot. \n\nAt the end I did not use it at all, but it is always nice to have the ability do some visual analysis for further investigations. ","metadata":{"_cell_guid":"f2a405e1-afa0-4dbb-9ee8-1164e1be07bb","_uuid":"d1a10157637263e2867e24564a19368ea371708a"}},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.violinplot(x='type', y='words_per_comment', data=df, inner=None, color='lightgray')\nsns.stripplot(x='type', y='words_per_comment', data=df, size=4, jitter=True)\nplt.ylabel('Words per comment')\nplt.show()","metadata":{"_cell_guid":"29fc9dd1-39f2-49fa-8034-a887def80113","_uuid":"903c6696d37f517ec51d35309e902e46daf2d789","execution":{"iopub.status.busy":"2021-06-03T13:43:05.6163Z","iopub.execute_input":"2021-06-03T13:43:05.616568Z","iopub.status.idle":"2021-06-03T13:43:06.322256Z","shell.execute_reply.started":"2021-06-03T13:43:05.616523Z","shell.execute_reply":"2021-06-03T13:43:06.321416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There's quite a lot of information there. \n\nCreating new columns showing the amount of questionmarks per comment, exclamations or other types will be useful later on, as we will see. This are the examples I came up with, but here is where creativity comes into play.\n\nWe can also perform joint plots, pair plots and heat maps to explore relationship between data, just for fun.","metadata":{"_cell_guid":"08bbe211-a91c-45b3-9330-016dbecfca96","_uuid":"a72478621467a2a67d0f561a220e26af50e37ea2"}},{"cell_type":"code","source":"df['http_per_comment'] = df['posts'].apply(lambda x: x.count('http')/50)\ndf['music_per_comment'] = df['posts'].apply(lambda x: x.count('music')/50)\ndf['question_per_comment'] = df['posts'].apply(lambda x: x.count('?')/50)\ndf['img_per_comment'] = df['posts'].apply(lambda x: x.count('jpg')/50)\ndf['excl_per_comment'] = df['posts'].apply(lambda x: x.count('!')/50)\ndf['ellipsis_per_comment'] = df['posts'].apply(lambda x: x.count('...')/50)\n\nplt.figure(figsize=(15,10))\nsns.jointplot(x='words_per_comment', y='ellipsis_per_comment', data=df, kind='kde')","metadata":{"_cell_guid":"a5365240-b4f1-45b9-801f-639155108f45","_uuid":"2a92888c4fae4668421f151609c47fde41764d22","execution":{"iopub.status.busy":"2021-06-03T13:43:06.323682Z","iopub.execute_input":"2021-06-03T13:43:06.324055Z","iopub.status.idle":"2021-06-03T13:43:13.475341Z","shell.execute_reply.started":"2021-06-03T13:43:06.32399Z","shell.execute_reply":"2021-06-03T13:43:13.474592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So it seems there's a large correlation between words per comment ant the ellipsis the user types per comment!","metadata":{"_cell_guid":"a14470ad-642a-4640-a64b-c6184c426f32","_uuid":"edf176de9553085fdecbd60a477ed77aa6f7ceeb"}},{"cell_type":"code","source":"i = df['type'].unique()\nk = 0\nfor m in range(0,2):\n    for n in range(0,6):\n        df_2 = df[df['type'] == i[k]]\n        sns.jointplot(x='words_per_comment', y='ellipsis_per_comment', data=df_2, kind=\"hex\")\n        plt.title(i[k])\n        k+=1\n","metadata":{"_cell_guid":"7820e7c7-a4c8-4249-ae87-dae21a39608f","_uuid":"7b86180aa6b7409252aecb45ad065fd3faa49718","execution":{"iopub.status.busy":"2021-06-03T13:43:13.476556Z","iopub.execute_input":"2021-06-03T13:43:13.476796Z","iopub.status.idle":"2021-06-03T13:43:18.139524Z","shell.execute_reply.started":"2021-06-03T13:43:13.476744Z","shell.execute_reply":"2021-06-03T13:43:18.138595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = df['type'].unique()\nk = 0\nTypeArray = []\nPearArray=[]\nfor m in range(0,2):\n    for n in range(0,6):\n        df_2 = df[df['type'] == i[k]]\n        pearsoncoef1=np.corrcoef(x=df_2['words_per_comment'], y=df_2['ellipsis_per_comment'])\n        pear=pearsoncoef1[1][0]\n        print(pear)\n        TypeArray.append(i[k])\n        PearArray.append(pear)\n        k+=1\n\n\nTypeArray = [x for _,x in sorted(zip(PearArray,TypeArray))]\nPearArray = sorted(PearArray, reverse=True)\nprint(PearArray)\nprint(TypeArray)\nplt.scatter(TypeArray, PearArray)","metadata":{"_cell_guid":"dcdb7444-2da6-4660-acfe-5a9321a3af35","_uuid":"831d063f8b916840403709f51736ad616aad99e8","execution":{"iopub.status.busy":"2021-06-03T13:43:18.14086Z","iopub.execute_input":"2021-06-03T13:43:18.141329Z","iopub.status.idle":"2021-06-03T13:43:18.347663Z","shell.execute_reply.started":"2021-06-03T13:43:18.141276Z","shell.execute_reply":"2021-06-03T13:43:18.346734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data preprocessing**\n\nTo get a further insight on our dataset, we can first create 4 new columns dividing the people by introversion/extroversion, intuition/sensing, and so on. \n\nWhen it comes to performing machine learning, trying to distinguish between two categories is much easier than distinguishing between 16 categories. We will check that later on. Dividing the data in 4 small groups will perhaps be more useful when it comes to accuracy.","metadata":{"_uuid":"32a81450ae9c91a8ff26106b8e05362543d74aff"}},{"cell_type":"code","source":"map1 = {\"I\": 0, \"E\": 1}\nmap2 = {\"N\": 0, \"S\": 1}\nmap3 = {\"T\": 0, \"F\": 1}\nmap4 = {\"J\": 0, \"P\": 1}\ndf['I-E'] = df['type'].astype(str).str[0]\ndf['I-E'] = df['I-E'].map(map1)\ndf['N-S'] = df['type'].astype(str).str[1]\ndf['N-S'] = df['N-S'].map(map2)\ndf['T-F'] = df['type'].astype(str).str[2]\ndf['T-F'] = df['T-F'].map(map3)\ndf['J-P'] = df['type'].astype(str).str[3]\ndf['J-P'] = df['J-P'].map(map4)\nprint(df.head(10))","metadata":{"_uuid":"2a2c8c86d9f4cfe5bac1a34b23eb1bc71cbaacd6","execution":{"iopub.status.busy":"2021-06-03T13:43:18.348856Z","iopub.execute_input":"2021-06-03T13:43:18.34929Z","iopub.status.idle":"2021-06-03T13:43:18.415379Z","shell.execute_reply.started":"2021-06-03T13:43:18.349238Z","shell.execute_reply":"2021-06-03T13:43:18.414565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Building machine learning algorithms**\n\nLet's do some machine learning now, first with the entire \"type\" column, with different models.","metadata":{"_cell_guid":"6441ac53-9d10-4312-9191-0f0ecd440f2e","_uuid":"1488ee7975b272f31b284e6b7e6c8580042c150d"}},{"cell_type":"code","source":"# Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, y_train)\n\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, y_train)\nacc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\nprint(round(acc_random_forest,2,), \"%\")","metadata":{"_uuid":"30168f35c770296823db30dfedad1146ee8042b7","execution":{"iopub.status.busy":"2021-06-03T13:44:22.898972Z","iopub.execute_input":"2021-06-03T13:44:22.89928Z","iopub.status.idle":"2021-06-03T13:44:23.257625Z","shell.execute_reply.started":"2021-06-03T13:44:22.899235Z","shell.execute_reply":"2021-06-03T13:44:23.256028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\nY_pred = logreg.predict(X_test)\n\nacc_log = round(logreg.score(X_train, y_train) * 100, 2)\nprint(round(acc_log,2,), \"%\")","metadata":{"_uuid":"4a28af6cbb1041fe234a6641d49c00d60fe9f5f0","execution":{"iopub.status.busy":"2021-06-03T13:44:24.058809Z","iopub.execute_input":"2021-06-03T13:44:24.059155Z","iopub.status.idle":"2021-06-03T13:44:24.415892Z","shell.execute_reply.started":"2021-06-03T13:44:24.059103Z","shell.execute_reply":"2021-06-03T13:44:24.414116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# KNN\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\n\nY_pred = knn.predict(X_test)\n\nacc_knn = round(knn.score(X_train, y_train) * 100, 2)\nprint(round(acc_knn,2,), \"%\")","metadata":{"_uuid":"015d0834932cb68d9f4062059fe97b3d0538cf28","execution":{"iopub.status.busy":"2021-06-03T13:44:24.468741Z","iopub.execute_input":"2021-06-03T13:44:24.469097Z","iopub.status.idle":"2021-06-03T13:44:24.826565Z","shell.execute_reply.started":"2021-06-03T13:44:24.469033Z","shell.execute_reply":"2021-06-03T13:44:24.824937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our simple model is only able to classify people with a 24% of right guesses, which is not too much. \n\nNow we will perform machine learning with the introverted/extroverted column, and we'll see if our model is able to classify if someone is introverted or extroverted with a higher precision.","metadata":{"_uuid":"68ede73124edd6cfa80fe215061c8d74ac125ed8"}},{"cell_type":"code","source":"XX = df.drop(['type','posts','I-E'], axis=1).values\nyy = df['I-E'].values\n\nprint(yy.shape)\nprint(XX.shape)\n\nXX_train,XX_test,yy_train,yy_test=train_test_split(XX,yy,test_size = 0.1, random_state=5)\n\nsgdd = SGDClassifier(max_iter=5, tol=None)\nsgdd.fit(XX_train, yy_train)\nY_predd = sgdd.predict(XX_test)\nsgdd.score(XX_train, yy_train)\nacc_sgdd = round(sgdd.score(XX_train, yy_train) * 100, 2)\nprint(round(acc_sgdd,2,), \"%\")","metadata":{"_uuid":"4ba8140f5e08778e0ab69fa000459a171c068d76","execution":{"iopub.status.busy":"2021-06-03T13:44:25.628638Z","iopub.execute_input":"2021-06-03T13:44:25.62899Z","iopub.status.idle":"2021-06-03T13:44:25.666237Z","shell.execute_reply.started":"2021-06-03T13:44:25.628931Z","shell.execute_reply":"2021-06-03T13:44:25.665482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_forestt = RandomForestClassifier(n_estimators=100)\nrandom_forestt.fit(XX_train, yy_train)\n\nY_predictionn = random_forestt.predict(XX_test)\n\nrandom_forestt.score(XX_train, yy_train)\nacc_random_forestt = round(random_forestt.score(XX_train, yy_train) * 100, 2)\nprint(round(acc_random_forestt,2,), \"%\")","metadata":{"_uuid":"a5dc51c11b33647a4eaa5fb23a50fe3068d2715a","execution":{"iopub.status.busy":"2021-06-03T13:44:25.928802Z","iopub.execute_input":"2021-06-03T13:44:25.929128Z","iopub.status.idle":"2021-06-03T13:44:27.266213Z","shell.execute_reply.started":"2021-06-03T13:44:25.929078Z","shell.execute_reply":"2021-06-03T13:44:27.265265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Logistic Regression\nlogregg = LogisticRegression()\nlogregg.fit(XX_train, yy_train)\n\nY_predd = logregg.predict(XX_test)\n\nacc_logg = round(logregg.score(XX_train, yy_train) * 100, 2)\nprint(round(acc_logg,2,), \"%\")","metadata":{"_uuid":"d016e4899884bd7df54f8a008ef16147828fcbae","execution":{"iopub.status.busy":"2021-06-03T13:44:32.088809Z","iopub.execute_input":"2021-06-03T13:44:32.089161Z","iopub.status.idle":"2021-06-03T13:44:32.13124Z","shell.execute_reply.started":"2021-06-03T13:44:32.089078Z","shell.execute_reply":"2021-06-03T13:44:32.130359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# KNN\nknnn = KNeighborsClassifier(n_neighbors = 3)\nknnn.fit(XX_train, yy_train)\n\nY_predd = knnn.predict(XX_test)\n\nacc_knnn = round(knnn.score(XX_train, yy_train) * 100, 2)\nprint(round(acc_knnn,2,), \"%\")","metadata":{"_uuid":"adf9af8db8ec105cc00273b0b494fc0f70d0f811","execution":{"iopub.status.busy":"2021-06-03T13:44:32.419203Z","iopub.execute_input":"2021-06-03T13:44:32.419505Z","iopub.status.idle":"2021-06-03T13:44:32.534303Z","shell.execute_reply.started":"2021-06-03T13:44:32.419449Z","shell.execute_reply":"2021-06-03T13:44:32.533201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So we see our model has an accuracy of 77%, not bad for such a simple model! Let's see what else we can do!","metadata":{"_cell_guid":"9375705f-79f3-4cb1-8d5a-16db200da250","_uuid":"4fc8d8f3cef8422c708cd10da7b38762ab729c01"}},{"cell_type":"code","source":"new_column=[]\nfor z in range(len(df['posts'])):\n    prov=df['posts'][z]\n    prov2= re.sub(r'[“€â.|,?!)(1234567890:/-]', '', prov)\n    prov3 = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', prov)\n    prov4 = re.sub(r'[|||)(?.,:1234567890!]',' ',prov3)\n    prov5 = re.sub(' +',' ', prov4)\n    prov6 = prov5.split(\" \")\n    counter = Counter(prov6)\n    counter2 = counter.most_common(1)[0][0]\n    new_column.append(counter2)\ndf['most_used_word'] = new_column\nprint(df.head())\nprint(df['most_used_word'].unique())","metadata":{"_cell_guid":"f5feb407-e16a-4bfd-ab8c-90426336d3b8","_uuid":"780fd3061b547a9c370c978648fde337bf47b332","execution":{"iopub.status.busy":"2021-06-03T13:44:33.158624Z","iopub.execute_input":"2021-06-03T13:44:33.15898Z","iopub.status.idle":"2021-06-03T13:44:50.152473Z","shell.execute_reply.started":"2021-06-03T13:44:33.15892Z","shell.execute_reply":"2021-06-03T13:44:50.15156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p=[\"I think its good and I can do it. Lets see I am tired of speaking.\"]\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-03T13:44:50.153811Z","iopub.execute_input":"2021-06-03T13:44:50.154321Z","iopub.status.idle":"2021-06-03T13:44:50.159012Z","shell.execute_reply.started":"2021-06-03T13:44:50.154261Z","shell.execute_reply":"2021-06-03T13:44:50.158076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_p = knn.predict(p)\n\nprint(y_p)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T13:45:39.27946Z","iopub.execute_input":"2021-06-03T13:45:39.279974Z","iopub.status.idle":"2021-06-03T13:45:39.30192Z","shell.execute_reply.started":"2021-06-03T13:45:39.27993Z","shell.execute_reply":"2021-06-03T13:45:39.300252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}