{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Read Data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/amazon-top-50-bestselling-books-2009-2019/bestsellers with categories.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check to see if any of our numeric variables have any negative values. This will help us determine if there are any irregularities in the data that we should be concerned with. Now, there are a couple of ways we can find negatives. Let's start with the \"slow and dumb\" way. We'll loop through each column individually and sum the total negative values we find in each one.","metadata":{}},{"cell_type":"code","source":"sum(x < 0 for x in df['Price'].values.flatten())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum(x < 0 for x in df['User Rating'].values.flatten())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum(x < 0 for x in df['Reviews'].values.flatten())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great! No negatives mean one less thing to worry about. But, notice how tedious it is to write out each loop and sum the values for each column. Let's build a function that does the exact same thing, but that we only need to write one time.","metadata":{}},{"cell_type":"code","source":"def count_negatives(data):\n    neg_count = 0\n    for n in data:\n        if type(data) == 'int':\n            if n < 0:\n               neg_count += 1\n    return neg_count\n\ncount_negatives(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We get the same correct result and if we want to check the dataset later on for whatever reason, we can just call this function again.","metadata":{}},{"cell_type":"markdown","source":"# Price","metadata":{}},{"cell_type":"markdown","source":"Now that we've made sure we loaded the data properly and that nothing seems to be wrong with it, we can begin our analysis. Let's first check if any of the variables are correlated.","metadata":{}},{"cell_type":"code","source":"sns.heatmap(\n    df[['Price', 'Reviews', 'User Rating', 'Year']].corr(),\n    annot = True,\n    cmap = 'BuPu'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A low correlation tells us that the variables don't have strong relationships with each other.","metadata":{}},{"cell_type":"markdown","source":"Financial data is often heavily skewed with higher prices having more influence than most of the other data points. Because of this, it is often helpful to take the log of financial data so that it is easier to read to work with. To start, I will make a copy of our dataset.","metadata":{}},{"cell_type":"code","source":"data = df.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's log transform our new dataset's price variable.","metadata":{}},{"cell_type":"code","source":"data['Price'] = data['Price'].map(lambda x: np.log(x) if x > 0 else 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize = (12,6))\n\nax[0].hist(\n    df['Price'],\n    color = 'pink',\n    bins = 25,\n    edgecolor = 'black',\n    label = \"Skewness: %.2f\"%df['Price'].skew()\n)\n\nax[0].legend()\nax[0].set_title('Original Data', fontsize = 14, fontweight = 'bold')\n\nax[1].hist(\n    data['Price'],\n    color = 'royalblue',\n    bins = 25,\n    edgecolor = 'black',\n    label = \"Skewness: %.2f\"%data['Price'].skew()\n)\n\nax[1].legend()\nax[1].set_title('Log-transformed Data', fontsize = 14, fontweight = 'bold')\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see from the above output that the original data has a long right tail, making it right-skewed. This skewness is evident by the legend, indicating that it is skewed by a factor of 3.69. This graph clearly follows a non-normal distribution. The graph on the right, however, shows a much more normal distribution as evidenced by its skewness being closer to 0 (in this case -0.48).\n\n** As a general rule rule of thumb, it is often beneficial to transform financial data by taking the logarithm of it for machine learning techniques, such as regression, that list normality as one of its assumptions.","metadata":{}},{"cell_type":"markdown","source":"Okay, now let's take a look at the highest and lowest priced books.","metadata":{}},{"cell_type":"code","source":"lowest_prices = df.groupby('Name', as_index = False)['Price'].mean().sort_values('Price').head(10)\nlowest_prices","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is interesting. To Kill a Mockingbird has a mean price of of 1.4. Let's take a closer look at this.","metadata":{}},{"cell_type":"code","source":"df[df['Name'] == 'To Kill a Mockingbird']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, we can see from this that To Kill a Mockingbird appears on the bestseller list five times and, for whatever reason, it costs $7 instead of the usual free pricetag. This is interesting, let's see what other titles appear in the list more than once.","metadata":{}},{"cell_type":"code","source":"import collections\n\nnames = [item for item, count in collections.Counter(df['Name']).items() if count > 1]\nnames","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is a bit clunky and hard to read, let's make this easier on the eyes by turning it into a dataframe.","metadata":{}},{"cell_type":"code","source":"new_df = pd.DataFrame(names)\nnew_df = new_df.rename(columns = {0: 'name'})\nnew_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above output shows us that 96 of the titles in the original dataset appear more than once. To avoid any confusion moving forward let's use the median for a better approximation of the book's price.","metadata":{}},{"cell_type":"code","source":"lowest_prices = df.groupby('Name', as_index = False)['Price'].median().sort_values('Price').head(10)\nlowest_prices","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"highest_prices = df.groupby('Name', as_index = False)['Price'].median().sort_values('Price', ascending = False).head(10)\nhighest_prices","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add breaks in long title names.\nx = ['Diagnostic and Statistical<br>Manual of Mental Disorders<br>,5th Edition:DSM-5', \n     'The Twilight Saga<br>Collection', 'Hamilton: The Revolution', \n     'The Book of Basketball:<br>The NBA According to<br>The Sports Guy', \n     'Harry Potter Paperback<br>Box Set (Books 1-7)', \n     'Publication Manual of the<br>American Psychological<br>Association, 6th Edition', \n     'Watchmen', 'The Official SAT<br>Study Guide', 'The Alchemist', \n     'The Official SAT<br>Study Guide, 2016 Edition<br>(Official Study Guide<br>for the New SAT)']\n\nlayout = go.Layout(\n    title = \"Highest Book Prices\",\n    plot_bgcolor = 'white', # Setting background color to white\n    xaxis = dict(\n        showgrid = False\n    ),\n    yaxis = dict(\n        showgrid = False\n    )\n)\n\nfig = go.Figure(layout = layout)\n\nfig.add_trace(\n    go.Bar(\n    x = x,\n    y = highest_prices['Price'],\n    marker_color = 'royalblue',\n    marker_line_color = 'black'\n    )\n)\n\n# Add text above each bar.\nfig.update_traces(\n    text = highest_prices['Price'], \n    textposition = 'outside', \n    texttemplate = '%{y:$.2f}', \n    textfont = {'size': 10}\n)\n\n# Increase the height.\nfig.update_layout(height = 620)\n\nfig.update_xaxes(\n    tickangle = 90,\n    title_text = \"Name of Book\"\n)\nfig.update_yaxes(\n    title_text = \"Price\"\n)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The bar graph shows us that the books with the highest prices seem to be educational of some form, in the case of non-fiction, or collections of stories, in the case of fiction.","metadata":{}},{"cell_type":"markdown","source":"Now, let's look at genre's effect on book price. The above graph shows an almost even split with non-fiction books just notching out the majority of the top ten most expensive books. Is this a trend that remains consistent throughout the entire dataset? Is there a difference in the distributions of each genre?","metadata":{}},{"cell_type":"code","source":"fiction = []\nnon_fiction = []\n\nfor index, row in df.iterrows():\n    if row['Genre'] == 'Fiction':\n        fiction.append(row['Price'])\n    else:\n        non_fiction.append(row['Price'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"layout = go.Layout(\n    title = \"Distribution of Prices by Genre\",\n    plot_bgcolor = \"white\",\n    xaxis = dict(\n        title = \"Price\",\n        showgrid = False\n    ),\n    yaxis = dict(\n        title = 'Count',\n        showgrid = False\n    )\n)\n\nfig = go.Figure(layout = layout)\n\nfig.add_trace(go.Histogram(x = fiction, name = 'Fiction', marker_color = 'salmon'))\nfig.add_trace(go.Histogram(x = non_fiction, name = 'Non Fiction', marker_color = 'royalblue'))\n\nfig.update_layout(barmode = 'stack')\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"layout = go.Layout(\n    title = \"Distribution of Prices by Genre\",\n    plot_bgcolor = \"white\",\n    xaxis = dict(\n        title = \"Price\",\n        showgrid = False\n    ),\n    yaxis = dict(\n        title = 'Count',\n        showgrid = False\n    )\n)\n\nfig = go.Figure(layout = layout)\n\nfig.add_trace(\n    go.Box(\n        y = fiction, \n        name = 'Fiction', \n        boxpoints = 'suspectedoutliers',\n        marker = dict(\n            color = 'salmon',\n            outliercolor = 'aquamarine'\n#         ),\n#         line = dict(\n#             outliercolor = 'green',\n#             outlierwidth = 2\n        )\n    )\n)\n\nfig.add_trace(\n    go.Box(\n        y = non_fiction,\n        name = 'Non FIction',\n        boxpoints = 'suspectedoutliers',\n        marker = dict(\n            color = 'royalblue',\n            outliercolor = 'aquamarine'\n        )\n    )\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The histograms and boxplots show that the two genres follow roughly similar distributions. The output also shows us that non-fiction books have higher average prices with more variance, as displayed by the outliers as evidenced by the boxplot.","metadata":{}},{"cell_type":"markdown","source":"# Genre","metadata":{}},{"cell_type":"markdown","source":"Let's go ahead and explore the genre variable some more. First, we'll make two new datasets with one representing just fiction books and the other representing non fiction books.","metadata":{}},{"cell_type":"code","source":"g1 = df[df['Genre'] == 'Fiction']\ng2 = df[df['Genre'] == 'Non Fiction']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at the total number of bestselling books from each genre across all years reported.","metadata":{}},{"cell_type":"code","source":"col = 'Year'\n\nfict = g1[col].value_counts().reset_index()\nfict = fict.sort_values('index')\nfict = fict.rename(columns = {'index': 'year', 'Year': 'count'})\nfict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nonfict = g2[col].value_counts().reset_index()\nnonfict = nonfict.rename(columns = {'index': 'year', 'Year': 'count'})\nnonfict = nonfict.sort_values('year')\nnonfict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"layout = go.Layout(\n    title = \"Number of Bestsellers Each Year by Genre\",\n    plot_bgcolor = \"white\",\n    xaxis = dict(\n        title = \"Year\",\n        showgrid = False\n    ),\n    yaxis = dict(\n        title = 'Count',\n        showgrid = False\n    )\n)\n\nfig = go.Figure(layout = layout)\nfig.add_trace(go.Scatter(x = fict['year'], y = fict['count'], name = 'Fiction', marker_color = 'salmon'))\nfig.add_trace(go.Scatter(x = nonfict['year'], y = nonfict['count'], name = 'Non Fiction', marker_color = 'royalblue'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The graph shows that in every year but 2014, more non fiction books were on the bestseller list compared with fiction books.","metadata":{}},{"cell_type":"markdown","source":"# User Rating","metadata":{}},{"cell_type":"code","source":"fig = px.histogram(df, x = 'User Rating', color = 'Genre', marginal = 'box', title = \"Distribution of User Ratings\")\nfig.update_layout({'plot_bgcolor': 'white'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distributions for user ratings follow about the same pattern regardless of genre. Fiction ratings are higher by 0.1 points.","metadata":{}},{"cell_type":"markdown","source":"Let's perform a hypothesis test on the two distributions to see if there is any significant difference between the two average ratings. We could transform the data to satisfy assumptions of normality for t- or z-tests, however, I believe it would be simpler to use a non-parametric test. Specifically, I will be using the Mann Whitney U test on the distributions. The Mann-Whitney U Test is a non-parametric test that works well with non-normal distributions and tests against the null hypothesis (H0) that the two distributions are the same.","metadata":{}},{"cell_type":"code","source":"genre_rating = df.groupby('Genre')['User Rating'].mean().reset_index()\ngenre_rating = genre_rating.rename(columns = {'Genre': 'genre', 'User Rating': 'mean'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupby('Genre')['User Rating'].apply(np.std)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"genre_rating['sdev'] = [0.264570, 0.189249]\ngenre_rating","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('##### Mann-Whitney U Test #####\\n')\nprint('###############################\\n')\nprint('H0: Distributions are equal\\n')\nprint('H1: Distributions are not equal\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import mannwhitneyu\n\nstat, p_value = mannwhitneyu(g1['User Rating'], g2['User Rating'])\nalpha = 0.05\n\nif p_value < alpha:\n    print('U Statistic: ', stat)\n    print('P-value: ', p_value)\n    print('Reject H0, Distributions are not equal.')\nelse:\n    print('U Statistic: ', stat)\n    print('P-value: ', p_value)\n    print('Fail to Reject H0, Distributions are equal.')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Indeed, the distributions are not the same and it is safe to conclude that user ratings for fiction books are statistically higher than non-fiction books.","metadata":{}},{"cell_type":"markdown","source":"# Reviews","metadata":{}},{"cell_type":"code","source":"genre_reviews = df.groupby('Genre')['Reviews'].sum().reset_index()\ngenre_reviews","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"go.Figure(go.Bar(x = genre_reviews['Genre'], y = genre_reviews['Reviews']), layout = go.Layout(plot_bgcolor = 'white'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.histogram(df, x = 'Reviews', color = 'Genre')\nfig.update_layout({'plot_bgcolor': 'white'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a quick look at what different data transformations would do to our reviews data.","metadata":{}},{"cell_type":"code","source":"log_r = df.copy()\nsqrt_r = df.copy()\ncube_r = df.copy()\n\nlog_r['Reviews'] = log_r['Reviews'].map(lambda x: np.log(x) if x > 0 else 0)\nsqrt_r['Reviews'] = sqrt_r['Reviews'].map(lambda y: np.sqrt(y) if y > 0 else 0)\ncube_r['Reviews'] = cube_r['Reviews'].map(lambda z: np.cbrt(z) if z > 0 else 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(2, 2, figsize = (12, 6))\n\nax[0,0].hist(\n    df['Reviews'],\n    color = 'pink',\n    bins = 25,\n    edgecolor = 'black',\n    label = \"Skewness: %.2f\"%df['Reviews'].skew()\n)\n\nax[0,0].legend()\nax[0,0].set_title('Original Data', fontsize = 14, fontweight = 'bold')\n\nax[0,1].hist(\n    log_r['Reviews'],\n    color = 'royalblue',\n    bins = 25,\n    edgecolor = 'black',\n    label = \"Skewness: %.2f\"%log_r['Reviews'].skew()\n)\n\nax[0,1].legend()\nax[0,1].set_title('Log Transformation ', fontsize = 14, fontweight = 'bold')\n\nax[1,0].hist(\n    sqrt_r['Reviews'],\n    color = 'darkcyan',\n    bins = 25,\n    edgecolor = 'black',\n    label = \"Skewness: %.2f\"%sqrt_r['Reviews'].skew()\n)\n\nax[1,0].legend()\nax[1,0].set_title(' Square Root Transformation', fontsize = 14, fontweight = 'bold')\n\nax[1,1].hist(\n    cube_r['Reviews'],\n    color = 'khaki',\n    bins = 25,\n    edgecolor = 'black',\n    label = \"Skewness: %.2f\"%cube_r['Reviews'].skew()\n)\n\nax[1,1].legend()\nax[1,1].set_title('Cube Root Transformation', fontsize = 14, fontweight = 'bold')\nfig.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above graphs show us that the cube root reduces the skewness of the data the most, resulting in the closest value to zero of all transformations tested, in this case 0.43. An interesting note: the square root distribtion is very similar to the distribution of the cube root.","metadata":{}},{"cell_type":"markdown","source":"As with user rating, let's run a Mann-Whitney U Test to test the difference of the distributions.\n\nRemember,\n\n* Null hypothesis: The Distributions are Equal\n* Alternative hypothesis: The Distributions are not Equal","metadata":{}},{"cell_type":"code","source":"stat, p_value = mannwhitneyu(g1['Reviews'], g2['Reviews'])\nprint('U Statistic: ', stat)\nprint('P-value: ', p_value)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The results show us that the distributions of reviews between fiction and non-fiction books are indeed different. Based on this, we can reject the null hypothesis and conclude that fiction reviews are significantly higher than non-fiction.","metadata":{}}]}