{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import networkx as nx\nimport pandas as pd\nimport numpy as np\nimport re\nimport seaborn as sns\nimport word2number\nimport community\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nfrom matplotlib.colors import ListedColormap\nfrom nltk.corpus import wordnet as wn\nfrom word2number import w2n\nfrom wordcloud import WordCloud, STOPWORDS\nfrom collections import Counter\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords #<- For calling the know stopwords in english (e.g, articles, connectors)\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:100% !important; }</style>\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# First apply cleansing\n\nRead in the network and apply the following cleansing:\n\n1. Remove numbers\n2. Apply stemming to take root words. This removes words such as compression and compressed, which fundamentally are the same as compress.\n3. Stopwords such as \"the\" and \"a\".","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"synonyms.csv\")\n\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def numbers(x):\n    try:\n        return w2n.word_to_num(x)\n    except (ValueError, TypeError, IndexError):\n        return np.nan","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"number\"] = df[\"lemma\"].apply(lambda x: numbers(x))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"lemma\"].unique().shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"th_rows = [f\"{i}th\" for i in range(1000)]\nrd_rows = [f\"{i}rd\" for i in range(1000)]\nnd_rows = [f\"{i}nd\" for i in range(1000)]\nst_rows = [f\"{i}st\" for i in range(1000)] \nnumber_rows = [f\"number {i}\" for i in range(1000)]\nyears_rows = [f\"{i}s\" for i in range(0,2021,10)]\ndays_rows = [f\"{i+1} days\" for i in range(366)]\nfinal_rows = [\"24-hour\",\"-karat\"]\n\nfinal_rows.extend(th_rows)\nfinal_rows.extend(number_rows)\nfinal_rows.extend(rd_rows)\nfinal_rows.extend(nd_rows)\nfinal_rows.extend(st_rows)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_numbers = df[(df[\"lemma\"].str.isnumeric() == False)&(df[\"number\"].isna() == True)&(df[\"lemma\"].str.contains('|'.join(final_rows)) == False)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_numbers[\"lemma\"].unique().shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ps = PorterStemmer()\nno_numbers[\"stemmed\"] = no_numbers[\"lemma\"].apply(lambda w: ps.stem(w) if isinstance(w, str) else None)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_numbers[no_numbers.duplicated(subset=[\"stemmed\",\"part_of_speech\"]) == True].shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_numbers[no_numbers.duplicated(subset=[\"stemmed\",\"part_of_speech\"]) == True][\"lemma\"].unique().shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"remove_stemming = no_numbers[no_numbers.duplicated(subset=[\"stemmed\",\"part_of_speech\"]) == False]","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"remove_stemming[\"lemma\"].unique().shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"remove_stemming[\"split\"] = remove_stemming[\"synonyms\"].str.replace(\"|\",\";\").apply(lambda x: x.split(\";\") if isinstance(x, str) else None)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Need to pair up words that are synonyms.","metadata":{}},{"cell_type":"code","source":"remove_stemming.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stops = stopwords.words('english')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# filter out obvious stop words such as \"have\" \"and\"\nno_stops = remove_stemming[remove_stemming[\"lemma\"].apply(lambda x: (x in list(STOPWORDS) or x in stops))==False]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_stops[\"lemma\"].unique().shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with_syns = no_stops[no_stops[\"synonyms\"].isna() ==False]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with_syns[\"lemma\"].unique().shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create pairs of words that are synonymous for edges file (which will be used in network analysis)\npairs = []\n\nfor lem in no_stops[\"lemma\"]:\n    \n    if with_syns[with_syns.split.apply(lambda x: lem in x)].shape[0] > 0:\n        \n        for comp_lem in with_syns[with_syns.split.apply(lambda x: lem in x)][\"lemma\"]:\n            if lem != comp_lem:\n                comp_pair = [lem, comp_lem]\n                pairs.append(comp_pair)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np_pairs = np.array(pairs)\nedges_raw = pd.DataFrame(np_pairs)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"edges_raw.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that all pairs are in a dataframe must get rid of duplicates.","metadata":{}},{"cell_type":"code","source":"df1 = pd.DataFrame(np.sort(edges_raw[[0,1]], axis=1))\n\nedges_raw1 = edges_raw[~df1.duplicated()]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"edges_raw1[0].unique().shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"edges_raw1.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"edges_raw1.rename(columns={0:\"Source\",1:\"Target\"},inplace=True)\nedges_raw1.to_csv(\"edges.csv\",index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# With edges created, create network of synonyms and perform analysis.\n\nAnalysis includes:\n\n1. Degree distribution analysis\n2. Analysis of ego network for highest degree node\n3. Community detection for ego networks","metadata":{}},{"cell_type":"code","source":"edges_raw = pd.read_csv(\"../input/network-edges/edges.csv\")\n\n# Convert columns into list of tuples\ntup_edges = list(zip(edges_raw[\"Source\"], edges_raw[\"Target\"]))\n\n# # list of nodes and associated connections\nedges = [t for t in (set(tuple(i) for i in tup_edges))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create networkx graph\nGx = nx.Graph()\nGx.add_edges_from(edges)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"degrees = pd.DataFrame([[tup[0], tup[1]]for tup in nx.degree(Gx)])\n\ndegrees.rename(mapper={0:\"Node ID\",1:\"Degrees\"},inplace=True,axis=1)\ndegrees.sort_values(\"Degrees\",inplace=True)\ndegrees.reset_index(inplace=True,drop=True)\ndegrees.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"degrees[\"Degrees\"].mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,12)) #<-- Create the subplots\n\ndist = sns.distplot(degrees[\"Degrees\"])\ndist.set(xlabel='Degrees', ylabel='Percentage of Nodes')\n\nplt.xlabel('Degree', fontsize=18)\nplt.ylabel('Percent of Nodes', fontsize=16)\nplt.title(\"Degree Distribution\", fontsize=16)\nax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"degree = nx.degree_centrality(Gx)\ndegree_df = pd.DataFrame.from_dict(degree, orient=\"index\", columns = [\"Degree Centrality\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"degree_df.sort_values(\"Degree Centrality\",ascending=False).head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From low centrality measure, this indicates many families of synonyms as network is largely disconnected given the large number of nodes","metadata":{}},{"cell_type":"code","source":"ego_graph = nx.ego_graph(Gx, \"pass\", radius=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"layout = nx.spring_layout(ego_graph)\nfig, ax = plt.subplots(figsize=(20,12)) #<-- Create the subplots\n    \nnx.draw_networkx_nodes(ego_graph,layout,node_size=10, alpha=0.5)\nnx.draw_networkx_edges(ego_graph,layout, alpha=0.5)\nnx.draw_networkx_nodes(ego_graph, layout, nodelist=[\"pass\"], node_color='r',node_size = 100)\nax.axis('off')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# community detection on words directly related to \"pass\"\ndepth1 = nx.ego_graph(Gx, \"pass\", radius=1)\npartition = community.best_partition(depth1,resolution=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show community distribution\ncommunities = pd.DataFrame(Counter(partition.values()),index=[0])\ntrans = communities.T\ntrans.rename(mapper={0:\"Count\"},axis=1,inplace=True)\n\nfig, ax = plt.subplots(figsize=(15,7)) #<-- Create the subplots\n\nsns.barplot(x=trans.sort_values(by=[\"Count\"],ascending=False).head(10).index,y=trans.sort_values(by=[\"Count\"],ascending=False).head(10)[\"Count\"])\nax.set_xlabel('Community')\n\nplt.title(\"Louvain's Community Detection for Pass Ego Network (depth 1)\")\nplt.xlabel('Community', fontsize=18)\nplt.ylabel('Count', fontsize=16)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Currently these communities do not mean a lot, so further analysis is performed to potentially explain meanings.","metadata":{}},{"cell_type":"code","source":"community_df = pd.DataFrame.from_dict(partition,orient=\"index\",columns=[\"Community\"])\ncommunity_df[community_df[\"Community\"]==0].head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Community 0 appears to be related to dying. This makes sense as a family of words connected to \"pass\", as one may \"pass away\".","metadata":{}},{"cell_type":"code","source":"community_df[community_df[\"Community\"]==1].head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Community 1 is less clear. By expanding the header from 10 to 20, it may indicate passing by something with words such as \"clear\" and \"make pass\", however this is tenuous.\n\nPotentially, the community detection algorithm parameters need tuning to split this community further.","metadata":{}},{"cell_type":"code","source":"community_df[community_df[\"Community\"]==2].head(10)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Community 2 appears to be related to time passing.","metadata":{}},{"cell_type":"markdown","source":"Plot the communities on the original ego network.","metadata":{}},{"cell_type":"code","source":"# plot network with communities highlighted\nn_clusters = len(np.unique([partition[key] for key in partition]))\n\nvmin = min([partition[key] for key in partition])\nvmax = max([partition[key] for key in partition])\n\ncmap = plt.get_cmap('viridis', n_clusters)\n\n\nfig, ax = plt.subplots(figsize=(20,12)) #<-- Create the subplots\nax.axis(\"off\")\nnx.draw_networkx_nodes(ego_graph,layout,node_size=5, alpha=0.2,)\nnx.draw_networkx_edges(ego_graph,layout, alpha=0.5)\n\n\nnx.draw_networkx_nodes(ego_graph, layout, nodelist = [key for key in partition.keys()], node_size=50,\n                       cmap=cmap, node_color=list(partition.values()))\nnx.draw_networkx_nodes(ego_graph, layout, nodelist=[\"pass\"], node_color='r',node_size = 100)\n\nsm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=vmin, vmax=vmax))\nsm.set_array([])\ncbar = plt.colorbar(sm,ticks=np.unique([partition[key] for key in partition]))\n\ncbar.set_ticklabels(np.arange(len([partition[key] for key in partition])))\ncbar.set_label('Community', rotation=90)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}