{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset analysed is called \"Sweden Insurance\". In it, we have:\n* the variable x, called \"Number of Claims\"\n* and y, or explained variable called \"Total Payments\". \n\nHere, we are going to construct a model that best explain Total_payments with Number of claims as input variable. \nLet's do this!\n\nFirst, we load main libraries as well as dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sbn\nsbn.set_style('darkgrid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv(r'/kaggle/input/auto-insurance-in-sweden/swedish_insurance.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After loading the dataset, we can see that there is no null or empty value. The following task that we are doing is cheking and erasing duplicated values: "},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset.drop_duplicates()\ndataset.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset.rename(columns = {'X':'Num_Claims', 'Y':'Total_payments'})\nX = pd.DataFrame(data = dataset['Num_Claims'])\ny = pd.DataFrame(data = dataset['Total_payments'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the resulting rows are the same (63) we can conclude that there were no duplicates rows in the initial dataset.\nSecondly, we are using a pairplot for: \n* Checking the relationship between the two variables.\n* Cheking the distribution of X and y."},{"metadata":{"trusted":true},"cell_type":"code","source":"sbn.pairplot(dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see in the previous plot that: \n* A linear relationship may exist.\n* The variables are not normal.\n* There are some values that could be considered as outliers."},{"metadata":{},"cell_type":"markdown","source":"1. Baseline model. \n\nPrior to any modification in the dataset, we are creating what could be considered as a naive or baseline regressor. That is to say,a model with the \"as is\" version of the dataset and no hyperparameter modified. \n\nWe load required functions from Sklearn."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we create a Kfold element and cross validation score:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = RepeatedKFold(n_splits = 2, n_repeats =15, random_state = 0)\nbaseline_score = cross_val_score(LinearRegression(), X, y, scoring = 'r2', cv = cv, n_jobs = -1)\nsbn.boxplot(baseline_score, orient = 'v').set_title('R2 baseline model')\nprint('Average R2 of baseline model: ', np.mean(baseline_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Results of Baseline model: R2 is 77% on average and IQR ranges between +50% and 90% aproximately."},{"metadata":{},"cell_type":"markdown","source":"Outliers. Here we use a boxplot in order to double-check what seemed to be outliers in the pairplot. "},{"metadata":{"trusted":true},"cell_type":"code","source":"sbn.boxplot(x = 'variable', y = 'value', data = pd.melt(dataset))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we spotted in the previous graph, there are some X values that could be considered as outliers. \n\nOne strategy can consist in erasing those values in order to better fit a model. However, since the dataset has only 63 rows, we should first consider what % we would be missing in case of dropping them.\n\nAs a rule of thumb, we could consider an outlier that point beyond the 75th percentile. However, I have considered different tresholds:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"number_outliers = []\nlimits = [0.75,0.80,0.90,0.95,0.99]\nfor i in  limits:\n    are_outliers = X > np.quantile(X, i)\n    X_noout = np.ma.masked_array(data = X, mask = are_outliers)\n    X_noout = np.ma.compressed(X_noout)\n    number_outliers.append(round((X.shape[0] - X_noout.shape[0])/X.shape[0],2)*100)\n\nnumber_outliers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The previous result means that if we consider and erase data values beyond: \n1. 75th percentile, we sacrifice 24% of our dataset.\n2. 80th percentile, we sacrifice 21% of our dataset.\n3. 90th percentile, we sacrifice 11% of our dataset.\n4. 95th percentile, we sacrifice 6% of our dataset.\n5. 99th percentile, we sacrifice 2% of our dataset.\n\nHere, we are considering outliers those beyond the 95% of Total Claims, meaning that we are sacrificing a 6% of available data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_noout = np.ma.masked_array(data = X, mask = X>np.quantile(X,0.95))\ny_noout = np.ma.masked_array(data = y, mask = X>np.quantile(X,0.95))\nX_noout = np.ma.compressed(X_noout)\ny_noout = np.ma.compressed(y_noout)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = RepeatedKFold(n_splits = 2, n_repeats =15, random_state = 0)\nnooutlier_score = cross_val_score(LinearRegression(), X_noout.reshape(-1,1), y_noout.reshape(-1,1), scoring = 'r2', cv = cv, n_jobs = -1)\nsbn.boxplot(data = [baseline_score,nooutlier_score]).set_title('R2 of baseline model (left), and no-outliers model (right)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see in the previous boxplot, eliminating extreme values here in this example results in an underfiting of the model.\nFor that reason, we will use the complete dataset when building the different models.\n\nThe following step will consist in finding the best regressors and comparing their goodness.\n\n2. Lasso model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV\n\nparams_lasso = {'alpha': [0,0.05,0.10, 0.20, 0.30, 0.50, 0.70, 1, 2, 3, 4, 5, 6, 8, 10, 12, 15, 18, 19, 20]}\ngrid_lasso = GridSearchCV(estimator = Lasso(), param_grid = params_lasso,scoring = 'r2',cv = 5, refit = True, verbose = 3,n_jobs = -1)\ngrid_lasso.fit(X, y)\nprint('Best Lasso parameter: ', grid_lasso.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. RandomTreeRegressor:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nparams_forest = {'n_estimators': [50, 100, 150],\n                 'criterion':['mse', 'mae'],\n                 'max_depth': [2, 5, 10, 20, 30], \n                 'min_samples_split':[2, 5, 8, 10, 15, 20, 25, 30, 50], \n                 'min_samples_leaf': [2, 3, 4, 5, 6, 10, 20] \n                }\ngrid_forest = GridSearchCV(estimator = RandomForestRegressor(), param_grid = params_forest,scoring = 'r2',cv = 5, refit = True, verbose = 3, n_jobs = -1)\ngrid_forest.fit(X, y)\nprint('Best Random Forest parameter: ', grid_forest.best_params_)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have constructed the three models ( baseline, Lasso and RandomForest), we are comparing: "},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\nmodels.append(('Baseline', LinearRegression()))\nmodels.append(('Lasso', Lasso(alpha = 18)))\nmodels.append(('RandomForest', RandomForestRegressor(criterion  = 'mae', max_depth = 5, min_samples_leaf = 3, min_samples_split = 15, n_estimators = 50)))\nresults = []\nnames = []\nscoring = 'r2'\nfor name, model in models:\n    kfold = RepeatedKFold(n_splits = 2, n_repeats = 10, random_state = 0)\n    cv_results = cross_val_score(model, X, y, cv = kfold, scoring = scoring)\n    results.append(cv_results)\n    names.append(name)\n    print('R2 of: ',name, '= ',cv_results.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_final = pd.DataFrame(results).T\nresults_final.set_axis(names, axis = 1, inplace = True)\nsbn.boxplot(x = 'variable', y = 'value', data = pd.melt(results_final))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have considered three possible models: \n1. Simple Linear Regression\n2. Lasso Regression\n3. Random Forest Regression\n\nand by searching the best parameters and conducting a kfold cross validation, we can conclude that, in this case, the baseline model is the best one in terms of R2. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}