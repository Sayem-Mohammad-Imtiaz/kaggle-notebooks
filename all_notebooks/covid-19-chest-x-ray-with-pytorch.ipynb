{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport os, random, torch, time, copy\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch import optim\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import Dataset\nfrom PIL import Image, ImageOps, ImageFilter\nfrom skimage.filters import threshold_local\n\nfrom torch.autograd import Variable","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:36:50.328545Z","iopub.execute_input":"2021-07-24T06:36:50.329111Z","iopub.status.idle":"2021-07-24T06:36:53.625736Z","shell.execute_reply.started":"2021-07-24T06:36:50.329076Z","shell.execute_reply":"2021-07-24T06:36:53.625057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create paths","metadata":{}},{"cell_type":"code","source":"# dataset_path = './dataset'\ncovid_dataset_path = '../input/covid-chest-xray'","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-07-24T06:36:53.627623Z","iopub.execute_input":"2021-07-24T06:36:53.628174Z","iopub.status.idle":"2021-07-24T06:36:53.632596Z","shell.execute_reply.started":"2021-07-24T06:36:53.628128Z","shell.execute_reply":"2021-07-24T06:36:53.631275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating a Dataset","metadata":{}},{"cell_type":"code","source":"classes = ['no-covid', 'covid']","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:36:53.633964Z","iopub.execute_input":"2021-07-24T06:36:53.634192Z","iopub.status.idle":"2021-07-24T06:36:53.646645Z","shell.execute_reply.started":"2021-07-24T06:36:53.634166Z","shell.execute_reply":"2021-07-24T06:36:53.645922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class image_dataset(Dataset):\n    \"\"\"Class creator for the x-ray dataset.\"\"\"\n\n    def __init__(self, csv_path, root_dir, transform=None, phase=None):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.df = pd.read_csv(csv_path)\n        self.root_dir = root_dir\n        self.transform = transform\n        # If not a PA view, drop the line \n        self.df.drop(self.df[self.df.view != 'PA'].index, inplace=True)\n        self.phase = phase\n\n    def __len__(self):\n        \n        return len(self.df)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        \n        if self.df['finding'].iloc[idx] != 'COVID-19':\n            finding = 0\n            img_path = os.path.sep.join([covid_dataset_path, 'images', self.df['filename'].iloc[idx]])\n            image = Image.open(img_path)\n            sample = {'image': image, 'finding': finding}\n            \n            if self.transform:\n                sample = {'image': self.transform[self.phase](sample['image']), 'finding': finding}\n\n        else:\n            finding = 1\n            img_path = os.path.sep.join([covid_dataset_path, 'images', self.df['filename'].iloc[idx]])\n            image = Image.open(img_path)\n            sample = {'image': image, 'finding': finding}\n\n            if self.transform:\n                sample = {'image': self.transform[self.phase](sample['image']), 'finding': finding}\n\n        return sample","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:36:53.648081Z","iopub.execute_input":"2021-07-24T06:36:53.648646Z","iopub.status.idle":"2021-07-24T06:36:53.667723Z","shell.execute_reply.started":"2021-07-24T06:36:53.648604Z","shell.execute_reply":"2021-07-24T06:36:53.666779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xray_dataset = image_dataset(csv_path=os.path.sep.join([covid_dataset_path, 'metadata.csv']), root_dir=covid_dataset_path)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:36:53.671851Z","iopub.execute_input":"2021-07-24T06:36:53.67233Z","iopub.status.idle":"2021-07-24T06:36:53.730207Z","shell.execute_reply.started":"2021-07-24T06:36:53.6723Z","shell.execute_reply":"2021-07-24T06:36:53.72948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating the data transformations","metadata":{}},{"cell_type":"code","source":"class HistEqualization(object):\n    \"\"\"Image pre-processing.\n\n    Equalize the image historgram\n    \"\"\"\n    \n    def __call__(self,image):\n        \n        return ImageOps.equalize(image, mask = None) ","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:36:53.732019Z","iopub.execute_input":"2021-07-24T06:36:53.732388Z","iopub.status.idle":"2021-07-24T06:36:53.736734Z","shell.execute_reply.started":"2021-07-24T06:36:53.73236Z","shell.execute_reply":"2021-07-24T06:36:53.735959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ContrastBrightness(object):\n    \"\"\"Image pre-processing.\n\n    alpha = 1.0 # Simple contrast control [1.0-3.0]\n    beta = 0    # Simple brightness control [0-100]\n    \"\"\"\n    \n    def __init__(self, alpha, beta):\n        self.alpha = alpha\n        self.beta = beta\n    \n    def __call__(self,image,):\n        image = np.array(image)\n        for y in range(image.shape[0]):\n            for x in range(image.shape[1]):\n                image[y,x] = np.clip(self.alpha*image[y,x] + self.beta, 0, 255)\n\n                return Image.fromarray(np.uint8(image)*255)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:36:53.737695Z","iopub.execute_input":"2021-07-24T06:36:53.738325Z","iopub.status.idle":"2021-07-24T06:36:53.751072Z","shell.execute_reply.started":"2021-07-24T06:36:53.738287Z","shell.execute_reply":"2021-07-24T06:36:53.749903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SmothImage(object):\n    \"\"\"Image pre-processing.\n\n    Smooth the image\n    \"\"\"\n    def __call__(self,image):\n        \n        return image.filter(ImageFilter.SMOOTH_MORE)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:36:53.752856Z","iopub.execute_input":"2021-07-24T06:36:53.753243Z","iopub.status.idle":"2021-07-24T06:36:53.768185Z","shell.execute_reply.started":"2021-07-24T06:36:53.753209Z","shell.execute_reply":"2021-07-24T06:36:53.767105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.Grayscale(1),\n        transforms.RandomRotation(30, fill=(0,)),\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        ContrastBrightness(1.2,25),\n        HistEqualization(),\n        SmothImage(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5],\n                             [0.25])\n    ]),\n    'test': transforms.Compose([\n        transforms.Grayscale(1),\n        transforms.Resize(240),\n        transforms.CenterCrop(224),\n        ContrastBrightness(1.2,25),\n        HistEqualization(),\n        SmothImage(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5],\n                             [0.25])\n    ]),\n}","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:36:53.769661Z","iopub.execute_input":"2021-07-24T06:36:53.769955Z","iopub.status.idle":"2021-07-24T06:36:53.780096Z","shell.execute_reply.started":"2021-07-24T06:36:53.769911Z","shell.execute_reply":"2021-07-24T06:36:53.779038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ploting some X-Rays samples","metadata":{}},{"cell_type":"code","source":"Nr = 4\nNc = 3\n\nfig, axs = plt.subplots(Nr, Nc,figsize=(20,10))\nfor i in range(Nr*Nc):\n    sample = xray_dataset[\n        i + random.randrange(\n            0, len(xray_dataset)-(Nr*Nc)\n        )]\n    \n    plt.subplot(Nr, Nc, i+1, aspect='auto')\n    plt.tight_layout()\n    plt.title('Finding: '+classes[sample['finding']])\n    plt.axis('off')\n    plt.imshow(sample['image'], cmap='gray')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:36:53.78157Z","iopub.execute_input":"2021-07-24T06:36:53.782043Z","iopub.status.idle":"2021-07-24T06:36:59.838465Z","shell.execute_reply.started":"2021-07-24T06:36:53.78201Z","shell.execute_reply":"2021-07-24T06:36:59.837357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exemple of transform application\n#### tsfm = Transform(params)\n#### transformed_sample = tsfm(sample)","metadata":{}},{"cell_type":"code","source":"scale = transforms.Resize(256)\ncrop = transforms.CenterCrop(400)\ncomposed = transforms.Compose([\n    transforms.Grayscale(1),\n    HistEqualization(),\n    transforms.RandomRotation(30, fill=(0,)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomResizedCrop(224),    \n])\n\n# Apply each of the above transforms on sample.\nfig = plt.figure(figsize=(18, 12))\nsample = xray_dataset[random.randrange(0, len(xray_dataset))]\nfor i, tsfrm in enumerate([scale, crop, composed]):\n    transformed_sample = tsfrm(sample['image'])\n\n    ax = plt.subplot(1, 3, i + 1)\n    plt.tight_layout()\n    plt.axis('off')\n    ax.set_title(type(tsfrm).__name__)\n    plt.imshow(transformed_sample, cmap='gray')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:36:59.839939Z","iopub.execute_input":"2021-07-24T06:36:59.840483Z","iopub.status.idle":"2021-07-24T06:37:00.455272Z","shell.execute_reply.started":"2021-07-24T06:36:59.840436Z","shell.execute_reply":"2021-07-24T06:37:00.454392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Applying transformations","metadata":{}},{"cell_type":"code","source":"xray_transform = image_dataset(\n    csv_path=os.path.sep.join([covid_dataset_path, 'metadata.csv']),\n    root_dir=covid_dataset_path,\n    transform=data_transforms,\n    phase='train'\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:37:00.457056Z","iopub.execute_input":"2021-07-24T06:37:00.457443Z","iopub.status.idle":"2021-07-24T06:37:00.479361Z","shell.execute_reply.started":"2021-07-24T06:37:00.457392Z","shell.execute_reply":"2021-07-24T06:37:00.478485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sampling the training images\n\n### Each iteration will give different images","metadata":{}},{"cell_type":"code","source":"Nr = 4\nNc = 3\n\nfig, axs = plt.subplots(Nr, Nc,figsize=(20,10))\nfor i in range(Nr*Nc):\n    sample = xray_transform[\n        i + random.randrange(\n            0, len(xray_transform)-(Nr*Nc)\n        )]\n    \n    plt.subplot(Nr, Nc, i+1, aspect='auto')\n    plt.tight_layout()\n    plt.title('Finding: '+classes[sample['finding']])\n    plt.axis('off')\n    plt.imshow(sample['image'].numpy().transpose(1, 2, 0).reshape(224,224),cmap='gray') #tensor is (ch, w, h)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:37:00.480785Z","iopub.execute_input":"2021-07-24T06:37:00.481152Z","iopub.status.idle":"2021-07-24T06:37:03.772Z","shell.execute_reply.started":"2021-07-24T06:37:00.481093Z","shell.execute_reply":"2021-07-24T06:37:03.770909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create a train and test dataset and the dataloaders","metadata":{}},{"cell_type":"code","source":"image_datasets = {\n    x: image_dataset(\n        csv_path=os.path.sep.join([covid_dataset_path, 'metadata.csv']),\n        root_dir=covid_dataset_path,\n        transform=data_transforms,\n        phase=x)\n    for x in ['train', 'test']\n}\n\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=16,\n                                              shuffle=True, num_workers=8)\n              for x in ['train', 'test']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:37:03.77341Z","iopub.execute_input":"2021-07-24T06:37:03.773669Z","iopub.status.idle":"2021-07-24T06:37:03.808548Z","shell.execute_reply.started":"2021-07-24T06:37:03.773641Z","shell.execute_reply":"2021-07-24T06:37:03.807607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create a train method","metadata":{}},{"cell_type":"code","source":"def train_model(model, criterion, optimizer, scheduler, num_epochs=25, device='cpu'):\n    \"\"\"\n    Support function for model training.\n\n    Args:\n      model: Model to be trained\n      criterion: Optimization criterion (loss)\n      optimizer: Optimizer to use for training\n      scheduler: Instance of ``torch.optim.lr_scheduler``\n      num_epochs: Number of epochs\n      device: Device to run the training on. Must be 'cpu' or 'cuda'\n    \"\"\"\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'test']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for data in dataloaders[phase]:\n                inputs = data['image']\n                labels = data['finding']\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                # zero the parameter gradients\n                optimizer.zero_grad()\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    out1 = model.classifier(outputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n            if phase == 'train':\n                scheduler.step()\n\n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n            # deep copy the model\n            if phase == 'test' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n      time_elapsed // 60, time_elapsed % 60))\n    print('Best validation Acc: {:4f}'.format(best_acc))\n    \n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:37:03.810185Z","iopub.execute_input":"2021-07-24T06:37:03.810575Z","iopub.status.idle":"2021-07-24T06:37:03.953717Z","shell.execute_reply.started":"2021-07-24T06:37:03.810532Z","shell.execute_reply":"2021-07-24T06:37:03.952701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CNN for classification\n\n### Transfer learning with Resnet18","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## Load the pre-trained model","metadata":{"trusted":true}},{"cell_type":"code","source":"model = models.resnet18(\n    pretrained=True,\n    progress=True\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:37:03.955032Z","iopub.execute_input":"2021-07-24T06:37:03.955299Z","iopub.status.idle":"2021-07-24T06:37:24.419281Z","shell.execute_reply.started":"2021-07-24T06:37:03.955261Z","shell.execute_reply":"2021-07-24T06:37:24.417289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Add a front layer to receive the black and white images from the X-Ray","metadata":{}},{"cell_type":"code","source":"model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:37:24.420516Z","iopub.status.idle":"2021-07-24T06:37:24.421058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Add a classification layer","metadata":{}},{"cell_type":"code","source":"num_ftrs = model.fc.in_features\n\nmodel.classifier = nn.Sequential(\n    nn.Dropout(p=0.5),\n    nn.Linear(1000, num_ftrs),\n    nn.Dropout(p=0.5),\n    nn.Linear(num_ftrs, 2),\n)\n\n# Freeze parameters so we don't backprop through them\nfor param in model.parameters():\n    param.requires_grad = True","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:37:24.422548Z","iopub.status.idle":"2021-07-24T06:37:24.423074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\n\n# Note that we are only training the head.\noptimizer_ft = optim.SGD(model.parameters(), lr=1e-3, momentum=0.9, weight_decay=0.1)\n\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.3)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:37:24.424332Z","iopub.status.idle":"2021-07-24T06:37:24.424835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fine tuning the model","metadata":{}},{"cell_type":"code","source":"model_ft = train_model(model.to(device), criterion, optimizer_ft, exp_lr_scheduler,\n                        num_epochs=50, device=device)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:37:24.426076Z","iopub.status.idle":"2021-07-24T06:37:24.42656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"correct = 0\ntotal = 0\nwith torch.no_grad():\n    for data in dataloaders['test']:\n        inputs = data['image']\n        labels = data['finding']\n        outputs = model_ft(inputs.float().to(device))\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels.to(device)).sum().item()\n\nprint('Accuracy of the network: %d %%' % (\n    100 * correct / total))","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:37:24.427935Z","iopub.status.idle":"2021-07-24T06:37:24.428429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_correct = list(0. for i in range(2))\nclass_total = list(0. for i in range(2))\n\n\nwith torch.no_grad():\n    for data in dataloaders['test']:\n        images = data['image']\n        labels = data['finding']\n        outputs = model_ft(images.to(device))\n        _, predicted = torch.max(outputs, 1)\n        c = (predicted == labels.to(device)).squeeze()\n        for i in range(images.shape[0]):\n            label = labels[i]\n            class_correct[label] += c[i].item()\n            class_total[label] += 1\n\n\nfor i in range(2):\n    print('Accuracy of %5s : %2d %%' % (\n        classes[i], 100 * class_correct[i] / class_total[i]))","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:37:24.430167Z","iopub.status.idle":"2021-07-24T06:37:24.43066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataloader = torch.utils.data.DataLoader(image_datasets['test'], batch_size=(len(image_datasets['test'])), num_workers=8)\n\ndataiter = iter(dataloader)\ndata = dataiter.next()\nimages = data['image']\nlabels = data['finding']\n\nmodel_ft.to('cpu')\n\noutput = torch.tensor(model_ft(images).detach().numpy())","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:37:24.432039Z","iopub.status.idle":"2021-07-24T06:37:24.432441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n# show a nicely formatted classification report\nprint(classification_report(labels, np.argmax(output,1), target_names=classes))","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:37:24.433415Z","iopub.status.idle":"2021-07-24T06:37:24.433817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\n# compute the confusion matrix and and use it to derive the raw\n# accuracy, sensitivity, and specificity\ncm = confusion_matrix(labels, np.argmax(output,1))\ntotal = sum(sum(cm))\nacc = (cm[0, 0] + cm[1, 1]) / total\nsensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\nspecificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n# show the confusion matrix, accuracy, sensitivity, and specificity\nprint(cm)\nprint(\"acc: {:.4f}\".format(acc))\nprint(\"sensitivity: {:.4f}\".format(sensitivity))\nprint(\"specificity: {:.4f}\".format(specificity))","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:37:24.43499Z","iopub.status.idle":"2021-07-24T06:37:24.435367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}