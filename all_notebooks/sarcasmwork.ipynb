{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport re\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n# Any results you write to the current directory are saved as output.\nfrom sklearn.ensemble import  RandomForestClassifier as RFC\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\nfrom keras.layers.embeddings import Embedding\nimport nltk\nimport string\nimport numpy as np\nimport pandas as pd\nfrom nltk.corpus import stopwords\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import f1_score,accuracy_score ,roc_auc_score\nfrom nltk.stem.snowball import SnowballStemmer\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split as tts\nfrom sklearn.model_selection import KFold,cross_val_score\nfrom sklearn.linear_model import LogisticRegression as LR","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df = pd.read_csv(\"/kaggle/input/yelp-reviews-dataset/yelp.csv\")\ndf = pd.read_csv(\"/kaggle/input/redditnew/reddit.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # df = df[[\"text\",\"funny\"]]\n# li = df[\"funny\"].values\n# op =[]\n# for i in li :\n#     if int(i) >2 :\n#             op.append(1)\n#     else :\n#             op.append(0)\n# df[\"funny\"] = op\n# df.columns = [\"tweet\",\"Sarcasm\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df= df.dropna()\ndf = df[df.tweet.apply(lambda x: x !=\"\")]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nanalyser = SentimentIntensityAnalyzer()\ndf.dropna(inplace =True)\nposi =[]\nnegi =[]\nneuu =[]\ncompi =[]\nfor i in df.tweet.values:\n    score = analyser.polarity_scores(str(i))\n    score = dict(score)\n    posi.append(score['pos'])\n    negi.append(score['neg'])\n    neuu.append(score['neu'])\n    compi.append(score['compound'])\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    \n    ## Remove puncuation\n    text = text.translate(string.punctuation)\n    \n    ## Convert words to lower case and split them\n    text = text.lower().split()\n    \n    \n    ## Remove stop words\n    stops = set(stopwords.words(\"english\"))\n    text = [w for w in text if not w in stops and len(w) >= 3]\n    \n    text = \" \".join(text)\n    ## Clean the text\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    ## Stemming\n    text = text.split()\n    stemmer = SnowballStemmer('english')\n    stemmed_words = [stemmer.stem(word) for word in text]\n    text = \" \".join(stemmed_words)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for i in df[\"tweet\"].values:\n#     print(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb =df\ngb = gb[\"tweet\"]\nmodified =[]\nfor i in gb.values :\n    url = re.sub('\\#sarcasm$', '', i)\n    modified.append(url)\ndf[\"tweet\"] = modified\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for i in df[\"tweet\"].values:\n#     print(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"tweet\"] = df[\"tweet\"].map(lambda x: clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Create sequence\nvocabulary_size = 20000\ntokenizer = Tokenizer(num_words= vocabulary_size)\ntokenizer.fit_on_texts(df[\"tweet\"])\nsequences = tokenizer.texts_to_sequences(df[\"tweet\"])\ndata = pad_sequences(sequences, maxlen=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Extract word embeddings from the Glove**"},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_index = dict()\nf = open('/kaggle/input/glove/glove.6B.50d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Create a weight matrix**"},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.zeros((vocabulary_size, 50))\nfor word, index in tokenizer.word_index.items():\n    if index > vocabulary_size - 1:\n        break\n    else:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[index] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom keras import backend as K\nK.tensorflow_backend._get_available_gpus()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nconfig = tf.ConfigProto( device_count = {'GPU': 1 } ) \nsess = tf.Session(config=config) \nkeras.backend.set_session(sess)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = df[\"Sarcasm\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range (0, (10)):\n    for j in range (0, len(data[i])):\n        print(data[i][j] , end = \" \")\n    print(\"\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" from sklearn.decomposition import PCA\npca = PCA(n_components=2)\nfrosty = pca.fit_transform(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range (0, (10)):\n    for j in range (0, len(frosty[i])):\n        print(frosty[i][j] , end = \" \")\n    print(\"\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import numpy as np\n# from numpy.linalg import norm\n\n\n# class Kmeans:\n#     '''Implementing Kmeans algorithm.'''\n\n#     def __init__(self, n_clusters, max_iter=100, random_state=123):\n#         self.n_clusters = n_clusters\n#         self.max_iter = max_iter\n#         self.random_state = random_state\n\n#     def initializ_centroids(self, X):\n#         np.random.RandomState(self.random_state)\n#         random_idx = np.random.permutation(X.shape[0])\n#         centroids = X[random_idx[:self.n_clusters]]\n#         return centroids\n\n#     def compute_centroids(self, X, labels):\n#         centroids = np.zeros((self.n_clusters, X.shape[1]))\n#         for k in range(self.n_clusters):\n#             centroids[k, :] = np.mean(X[labels == k, :], axis=0)\n#         return centroids\n\n#     def compute_distance(self, X, centroids):\n#         distance = np.zeros((X.shape[0], self.n_clusters))\n#         for k in range(self.n_clusters):\n#             row_norm = norm(X - centroids[k, :], axis=1)\n#             distance[:, k] = np.square(row_norm)\n#         return distance\n\n#     def find_closest_cluster(self, distance):\n#         return np.argmin(distance, axis=1)\n\n#     def compute_sse(self, X, labels, centroids):\n#         distance = np.zeros(X.shape[0])\n#         for k in range(self.n_clusters):\n#             distance[labels == k] = norm(X[labels == k] - centroids[k], axis=1)\n#         return np.sum(np.square(distance))\n    \n#     def fit(self, X):\n#         self.centroids = self.initializ_centroids(X)\n#         for i in range(self.max_iter):\n#             old_centroids = self.centroids\n#             distance = self.compute_distance(X, old_centroids)\n#             self.labels = self.find_closest_cluster(distance)\n#             self.centroids = self.compute_centroids(X, self.labels)\n#             if np.all(old_centroids == self.centroids):\n#                 break\n#         self.error = self.compute_sse(X, self.labels, self.centroids)\n    \n#     def predict(self, X):\n#         distance = self.compute_distance(X, old_centroids)\n#         return self.find_closest_cluster(distance)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# newfrosty = frosty[ 0:100 , 0:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# from sklearn.cluster import KMeans\n# from sklearn.preprocessing import LabelEncoder\n# from sklearn.preprocessing import MinMaxScaler\n# import seaborn as sns\n# import matplotlib.pyplot as plt\n# %matplotlib inline\n# km = Kmeans(n_clusters=2, max_iter=5)\n# km.fit(newfrosty)\n# centroids = km.centroids\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# # Plot the clustered data\n# fig, ax = plt.subplots(figsize=(6, 6))\n# plt.scatter(newfrosty[km.labels == 0], newfrosty[km.labels == 0],\n#             c='green', label='cluster 1')\n# plt.scatter(newfrosty[km.labels == 1], newfrosty[km.labels == 1],\n#             c='blue', label='cluster 2')\n# plt.scatter(centroids[:, 0], centroids[:, 0], marker='*', s=300,\n#             c='r', label='centroid')\n# plt.legend()\n# plt.xlim([-2, 2])\n# plt.ylim([-2, 2])\n# plt.xlabel('Eruption time in mins')\n# plt.ylabel('Waiting time to next eruption')\n# plt.title('Visualization of clustered data', fontweight='bold')\n# ax.set_aspect('equal')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for i in range (0, (100)):\n#     for j in range (0, len(embedding_matrix[i])):\n#         print(embedding_matrix[i][j] , end = \" \")\n#     print(\"\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def recall_m(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\n\ndef precision_m(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy.random import seed\nseed(1337)\nfrom keras import optimizers\nlabels = df[\"Sarcasm\"]\nmodel_glove = Sequential()\nmodel_glove.add(Embedding(vocabulary_size, 50, input_length=50, weights=[embedding_matrix], trainable=True))\nmodel_glove.add(Dropout(0.2))\nmodel_glove.add(Conv1D(64, 5, activation='relu'))\nmodel_glove.add(MaxPooling1D(pool_size=4))\n\n\nmodel_glove.add(LSTM(100))\nmodel_glove.add(Dense(1, activation='sigmoid'))\n\n\nsgd = keras.optimizers.SGD(learning_rate=0.001)\n\nmodel_glove.compile(loss='binary_crossentropy', optimizer=sgd, metrics=[f1_m ,'acc'] )\n## Fit train data\n\nhistory = model_glove.fit(data, np.array(labels), validation_split=0.1, epochs = 20  )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import plot_model\nplot_model(model_glove, to_file='model.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt \n%matplotlib inline\n# Plot training & validation accuracy values\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NN = model_glove","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"senti = pd.DataFrame()\nsenti['pos'] = posi\nsenti['neg'] = negi\nsenti['neu'] = neuu\nsenti['compund'] = compi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = tts(senti, labels, test_size=0.2,  random_state=13)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RFC(n_jobs= -1)\nrfc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LR()\nlr.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"huf_xgb = xgb.XGBClassifier(n_estimators = 500)\nhuf_xgb.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CROSS VALIDATION \n##cross_val_score(model, X, y , cv = 5)\n#model = model you want to validate \n#X - feature data \n#y - labels\n#cv = number of folds you want in cross validation\n# scores = cross_val_score(model, data, labels, cv=5,scoring =\"f1\")\n# print(\"SCORES : \" ,scores , \"\\nMEAN :\" , scores.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [lr,huf_xgb ,rfc] \nf1score =[]\nauc_score = []\naccuracy = []\nfor i in models :\n    pred = np.round(i.predict(X_test)).astype(int)\n    f1score.append(f1_score(pred,y_test))\n    auc_score.append(roc_auc_score(pred,y_test))\n    accuracy.append(accuracy_score(pred,y_test))\n    \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = pd.DataFrame()\nresults[\"MODEL\"] =[  \"Logistic Regression\",\"XGB\",\"rfc\"]\nresults[\"F1_Score\"] =f1score\nresults[\"Auc_Score\"] =auc_score\nresults[\"Accuracy %\"]=accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"nn_pred = np.round(NN.predict(X_test))\nlr_pred = huf_xgb.predict(X_test)\nnew_data = pd.DataFrame()\nnn_pred.reshape(-1,)\nnew_data[\"NN\"] = (nn_pred.reshape(-1,))\nnew_data[\"LR\"] = lr_pred\nnew_data[\"NN\"] = new_data[\"NN\"].astype(int)\nmeta_X_train, meta_X_test, meta_y_train, meta_y_test = tts(new_data, y_test, test_size=0.2,  random_state=13)\nfrom sklearn.naive_bayes import BernoulliNB as bayes\n\nmeta = xgb.XGBClassifier(n_estimators = 500)\nmeta.fit(meta_X_train,meta_y_train)\npred = np.round(meta.predict(meta_X_test)).astype(int)\nmeta_result = (f1_score(pred,meta_y_test))\nprint(\"STACKING RESULT :\" ,  meta_result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"average = []\nfor i in range(0 ,len(nn_pred)):\n       average.append((0.89*nn_pred[i] + 0.90*lr_pred[i]) /(0.89+0.90))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"AVERAGE RESULT : \" , f1_score(np.round(average) , y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}