{"cells":[{"metadata":{},"cell_type":"markdown","source":"The task at hand is to categorize songs into certain categories (trap, rap, pop etc) based off of given values such as danceability, acousticness and other metrics that can be readily obtained via Spotify's organize playlist. <br>\nThe datasets were obtained from https://www.kaggle.com/iamsumat/spotify-top-2000s-mega-dataset and https://www.kaggle.com/leonardopena/top-spotify-songs-from-20102019-by-year?select=top10s.csv\n\nTwo datasets are being used to increase the amount of data available. Note that both sets of data use the same metrics available off of Spotify's organize playlist feature. These metrics are as follows:\n1. **Genre** - the genre of the track\n2. **Year** - the release year of the recording. Note that due to vagaries of releases, re-releases, re-issues and general madness, sometimes the release years are not what you'd expect.\n3. **Added** - the earliest date you added the track to your collection.\n4. **Beats Per Minute** (BPM) - The tempo of the song.\n5. **Energy** - The energy of a song - the higher the value, the more energtic. song\n6. **Danceability** - The higher the value, the easier it is to dance to this song.\n7. **Loudness** (dB) - The higher the value, the louder the song.\n8. **Liveness** - The higher the value, the more likely the song is a live recording.\n9. **Valence** - The higher the value, the more positive mood for the song.\n10. **Length** - The duration of the song.\n11. **Acousticness** - The higher the value the more acoustic the song is.\n12. **Speechiness** - The higher the value the more spoken word the song contains.\n13. **Popularity** - The higher the value the more popular the song is.\n14. **Duration** - The length of the song.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h2> Obtaining Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import read_csv\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import scatter_matrix\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_2000 = pd.read_csv(\"../input/spotify-top-2000s-mega-dataset/Spotify-2000.csv\")\ndf_top10s = pd.read_csv(\"../input/top-spotify-songs-from-20102019-by-year/top10s.csv\", engine='python') # the engine needs to be changed otherwise UTF-8 error occurs\ndf_2000.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_top10s.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_2000.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_top10s.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df_2000[\"Top Genre\"].unique()), len(df_top10s[\"top genre\"].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_2000[\"Top Genre\"].value_counts(), df_top10s[\"top genre\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From a quick observation, we can see certain things about the data we are dealing with:<br>\n1. Both datasets use the same metrics, albeit with different column names so we will have to change those names. Fortunately, the same order is kept for both datasets\n2. Our target label is the top genre category, however there is a very large number of genre types, so many in fact that our model will be very inaccurate and inefficient if we attempt to label all categories as they are, especially since a large number of categories contain a single song only. \n3. The two datasets do not share the same # of genres.\n4. We don't care about the artist, title, index/Unamed: 0, and year. I simply don't think these would have a strong connection with the genre. (Arguably, artist would have a connection to the music genre, but I personally think there is too much variance and it wouldn't be a consistent pattern)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h2> Data Preparation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Dropping off all unecessary columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_top10s.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_2000.drop(columns = ['Index', 'Title', 'Artist', 'Year'], inplace = True)\ndf_top10s.drop(columns = ['Unnamed: 0', 'title', 'artist', 'year'], inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now both datasets have the same # of columsn in the same order, so we can now join them together after renaming","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_top10s.columns = df_2000.columns # setting column names as each other\ndf = df_2000.append(df_top10s, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"both X_train and X_test have several values that are string type numbers, separated with a comma. We have to get rid of these commas then turn them into floats","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"attributes = df.columns[1:]\nfor attribute in attributes:\n    temp = df[attribute]\n    for instance in range(len(temp)):\n        if(type(temp[instance]) == str):\n            df[attribute][instance] = float(temp[instance].replace(',',''))\n# check data types using df.dtype","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> Splitting Genre","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now that we have obtained our full dataset, we need a way to split the top genres. Two methods will be explored: <br>\n**Method 1** All related songs of a specific category will be placed in a broader category (i.e. celtic pop and indie pop will be placed under the larger theme of pop). The main assumption using this method is that there is minimal difference between differing types of a similar music genre. This will be multiclass classification <br>\n**Method 2** Songs will have their genres split by space and multilabel classification will be used. <br>\n**NOTE**: Method 2 has been pushed to the bottom after Method 1 is completed to prevent interference and distraction from cross coding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# first extracting the genre columns\n# getting rid of white spaces and turning it all into lower cases\ngenre = (df[\"Top Genre\"].str.strip()).str.lower()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4> Method 1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to split the genre column\ndef genre_splitter(genre):\n    result = genre.copy()\n    result = result.str.split(\" \",1)\n    for i in range(len(result)):\n        if (len(result[i]) > 1):\n            result[i] = [result[i][1]]\n    return result.str.join('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loop until the genre cannot be split any further\ngenre_m1 = genre.copy()\nwhile(max((genre_m1.str.split(\" \", 1)).str.len()) > 1):\n    genre_m1 = genre_splitter(genre_m1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(genre_m1.unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We reduced our target label by over 50%. Let's take a quick look at our new shortened labels","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"genre_m1.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that there are certain music genres that have a single value. These genres would make our model inefficient, since it does not have data to work off of, so these values and corresponding rows in the original dataframe will be taken out. Putting all of these instances into a broader \"Other\" category is another potential solution, but I decided against it because there is probably minimal similarity between all of these music genres. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"unique = genre_m1.unique()\nto_remove = [] \n\n# genres that have a single instance only will be placed within the to_remove array\nfor genre in unique:\n    if genre_m1.value_counts()[genre] < 20: # 10 was arbitrarily chosen\n        to_remove += [genre]\nlen(to_remove)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now replacing our original genre columns with our updated version","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Top Genre'] = genre_m1\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.set_index([\"Top Genre\"],drop = False, inplace = True)\nfor name in to_remove:\n    type(name)\n    df.drop(index = str(name), inplace = True)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Top Genre\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> Model Creation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Since we are dealing with a classification problem, several models will be tried:\n1. Random Forest\n2. Naive Bayes\n3. Stochastic Gradient Descent Classifier\n4. Logistic Regression","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h3> Method 1","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"first preparing the training and testing dataset with proper standardization using StandardScaler","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set, test_set = train_test_split(df, test_size = 0.2, random_state = 42)\n# training set\nX_train = train_set.values[:,1:]\ny_train = train_set.values[:,0]\n\n# test set\nX_test = test_set.values[:,1:]\ny_test = test_set.values[:,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nstandard_scaler = StandardScaler().fit(X_train)\n\n# Standard Scaler\nX_train_ST = standard_scaler.transform(X_train)\nX_test_ST = standard_scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The labels need to be converted into a form that can be understood by the models, One hot encoding will be used here","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# obtaining all unique classes\nunique = np.unique(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import label_binarize\nfrom sklearn.preprocessing import LabelEncoder\n# 1 hot encoding\ny_test_1hot = label_binarize(y_test, classes = unique)\ny_train_1hot = label_binarize(y_train, classes = unique)\n\n# labelling\ny_test_label = LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now creating the instances of the models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.multiclass import OneVsOneClassifier\n\nmodels = []\nmodels += [['Naive Bayes', GaussianNB()]]\nmodels += [['SGD', OneVsOneClassifier(SGDClassifier())]]\nmodels += [['Logistic', LogisticRegression(multi_class = 'ovr')]]\nrand_forest = RandomForestClassifier(random_state = 42, min_samples_split = 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now training the models using k cross validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"result_ST =[]\nkfold = StratifiedKFold(n_splits = 10, random_state = 1, shuffle = True)\n\n# Random Forest has to be done separately since it takes in one hot encoded labels instead\nRF_cross_val_score = cross_val_score(rand_forest, X_train_ST, y_train_1hot, cv = 10, scoring = 'accuracy')\nprint('%s: %f (%f)' % ('Random Forest', RF_cross_val_score.mean(), RF_cross_val_score.std()))\n\nfor name, model in models:\n    cv_score = cross_val_score(model, X_train_ST, y_train, cv = kfold, scoring = 'accuracy')\n    result_ST.append(cv_score)\n    print('%s: %f (%f)' % (name,cv_score.mean(), cv_score.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All of our models have very low accuracy, which is most likely due to the lack of available data. Let's move forward however, and select a model. While it appears that Logistic Regression has the highest accuracy (56%), let's calculate the respective recall and precision values. Note that micro averaging will be used for the models (w/ exception to Random Forest) bc of the imbalance in class examples.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score\n\nresult_precision_recall = []\n\n# same reasoning as before for Random Forest\ny_temp_randforest = cross_val_predict(rand_forest, X_train_ST, y_train_1hot, cv = 10)\nresult_precision_recall += [['Random Forest', precision_score(y_train_1hot, y_temp_randforest, average = \"micro\"), \n                            recall_score(y_train_1hot, y_temp_randforest, average = \"micro\")]]\n\nprint('%s| %s: %f, %s (%f)' % ('Random Forest', 'Precision Score: ', precision_score(y_train_1hot, y_temp_randforest, average = \"micro\"), \n                           'Recall Score: ', recall_score(y_train_1hot, y_temp_randforest, average = \"micro\")))\n\nfor name, model in models:\n    y_pred = cross_val_predict(model, X_train_ST, y_train, cv = kfold)\n    precision = precision_score(y_train, y_pred, average = \"micro\")\n    recall = recall_score(y_train, y_pred, average = \"micro\")\n    # storing the precision and recall values\n    result_precision_recall += [[name , precision, recall]]\n    print('%s| %s: %f, %s (%f)' % (name, 'Precision Score: ', precision, 'Recall Score: ', recall))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the given precision and recall scores, we can now find the respective f1 score and use the highest score to select our model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\n\nfor name, precision, recall in result_precision_recall:\n    print(\"%s: %f\" % (name, 2 * (precision * recall) / (precision + recall)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Given that Logistic Regression has the highest f1 score, we will move forward with that model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h2> Evaluation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h3> Method 1","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Our chosen model was logistic regression, so let's evaluate our trained model on the test data set now","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# training the models\nmodel_method1 = LogisticRegression(multi_class = 'ovr').fit(X_train_ST, y_train)\n\n# getting predictions\npredictions_method1 = model_method1.predict(X_test_ST)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now evaluating our accuracy and f1 score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nprint(f1_score(y_test, predictions_method1, labels = unique, average = 'micro' ))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This low f1_score is most definitely from the lack of data available","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h2> Data Preparation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h3> Method 2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# recalling our original dataframe\ndf = df_2000.append(df_top10s, ignore_index = True)\nattributes = df.columns[1:]\nfor attribute in attributes:\n    temp = df[attribute]\n    for instance in range(len(temp)):\n        if(type(temp[instance]) == str):\n            df[attribute][instance] = float(temp[instance].replace(',',''))\n# check data types using the following code\n# df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"genre = df['Top Genre'].str.split(\" \")\ngenre","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}