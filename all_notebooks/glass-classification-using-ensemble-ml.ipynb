{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"> # **Introduction** "},{"metadata":{"trusted":true,"_uuid":"c073febba31cfe242ab0304096c607e2db551cb5"},"cell_type":"markdown","source":"#### Glass classification is dataset which is to identify glass is made by several of chemical elements. The dataset has been provided by UCI Machine Learning. It contains 10 attributes including id. The response is glass type(discrete 7 values). Today I would like to try to analyze it using a ensemble machine learning approach"},{"metadata":{"_uuid":"6669189c5218a3cdfcb3dded01866698f0a2b8b6"},"cell_type":"markdown","source":"## **1. Import packages and Dataset** "},{"metadata":{"trusted":true,"_uuid":"1f276b6c1af08ac46d06f32f20bea6e204bc74ad"},"cell_type":"code","source":"#import python packages \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport sklearn\nfrom sklearn.model_selection import train_test_split  \nfrom sklearn.metrics import accuracy_score \n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8732f5601e2e4456922bf900b982e1a2bb925c6d"},"cell_type":"code","source":"#import dataset from draft environment\ndata = pd.read_csv('../input/glass.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9cf9eaeb75d30911088e13fc3880a6f88d3ae7e1"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60310182997f1cd173cf0b11d1929ec98f517c26"},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60666cdc8146911ef69a225e31fa1588f540ee5a"},"cell_type":"markdown","source":"## **2. Exploratory Data Analysis** "},{"metadata":{"trusted":true,"_uuid":"e2a250ab85c61a4871fbf3a3276e5bee41ccf1bd"},"cell_type":"code","source":"#correlation of each the datasets \ncorr = data.corr()\nplt.figure(figsize=(12,12))\nsns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 15}\n            , alpha = 0.7, cmap= 'coolwarm')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9faa3b300706b17d9a4bb2291fa839f0818c1764"},"cell_type":"code","source":"# make boxplot to correction is there outlier or no\n# you can repeat this code for all feature\nfig, axes = plt.subplots(nrows=2,ncols=2)\nfig.set_size_inches(10,10)\nsns.boxplot(x=data['RI'],color = 'blue', ax=axes[0][0])\nsns.boxplot(x=data['Na'],color = 'Red', ax=axes[0][1])\nsns.boxplot(x=data['Mg'],color = 'Green', ax=axes[1][0])\nsns.boxplot(x=data['Al'],color = 'Orange', ax=axes[1][1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f3d068ad31f0bdf5c8e062698f174edea6d8782"},"cell_type":"markdown","source":"## **3. Data Preproccessing **"},{"metadata":{"trusted":true,"_uuid":"085beebc4cde543c487248084c96f30269097e7c"},"cell_type":"code","source":"dt = data['Type'].value_counts()\nprint ('The number of each Type class = \\n')\nprint (dt)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"322c8e9b484c4ac9645bccde5e47c562a79d7e42"},"cell_type":"markdown","source":"We can see the dataset, the dataset has an imbalanced class. Therefore, we should handle first this issue. There are a few methods can handle it. Currently, I would like to try SMOTE ( Synthetic Minority Oversampling Technique) method to resampling the sample dataset. "},{"metadata":{"trusted":true,"_uuid":"0bde1aec6af3262fc5a687fdc381efcbb8b205da"},"cell_type":"code","source":"sns.countplot(data['Type'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4287671e9ce4564cd3f92266d740713d9209f1b"},"cell_type":"markdown","source":"## ** 4.  Oversampling using Pakages Imbalanced Learn (SMOTE)**"},{"metadata":{"trusted":true,"_uuid":"01bc4fac27673fe73f294030d1ea0a47b95a3a38"},"cell_type":"code","source":"#import packages for imbalance-learn for balancing class\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.model_selection import train_test_split\n\nx = data.drop('Type', axis=1)\ny = data['Type']\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n\nprint(\"Number  X_train dataset: \", x_train.shape)\nprint(\"Number y_train dataset: \", y_train.shape)\nprint(\"Number X_test dataset: \", x_test.shape)\nprint(\"Number y_test dataset: \", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"236c549454d1536815f901e668f802b90dd98cec"},"cell_type":"code","source":"print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Before OverSampling, counts of label '2': {}\".format(sum(y_train==2)))\nprint(\"Before OverSampling, counts of label '3': {}\".format(sum(y_train==3)))\nprint(\"Before OverSampling, counts of label '5': {}\".format(sum(y_train==5)))\nprint(\"Before OverSampling, counts of label '6': {}\".format(sum(y_train==6)))\nprint(\"Before OverSampling, counts of label '7': {} \\n\".format(sum(y_train==7)))\n\nsm = SMOTE(random_state=2)\nx_train_res, y_train_res = sm.fit_sample(x_train, y_train.ravel())\n\nprint('After OverSampling, the shape of train_X: {}'.format(x_train_res.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\nprint(\"After OverSampling, counts of label '2': {}\".format(sum(y_train_res==2)))\nprint(\"After OverSampling, counts of label '3': {}\".format(sum(y_train_res==3)))\nprint(\"After OverSampling, counts of label '5': {}\".format(sum(y_train_res==5)))\nprint(\"After OverSampling, counts of label '6': {}\".format(sum(y_train_res==6)))\nprint(\"After OverSampling, counts of label '7': {}\".format(sum(y_train_res==7)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1cc606cd753621c9c4493a50d726500fc4813e5"},"cell_type":"markdown","source":"## **A. Random Forest Classifier**"},{"metadata":{"trusted":true,"_uuid":"00b9b34cf3b38b2d5576e32353c0edd4e22a4869"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nRFC = RandomForestClassifier(n_estimators = 500, criterion = 'entropy', random_state = 42, max_depth = 10 )\nRFC.fit(x_train_res, y_train_res.ravel())\n\n#predict \npred_train = RFC.predict(x_train_res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a7ada6ccf480d6448cb4eaae377dc618706f5b3b"},"cell_type":"code","source":"#Confusion Matrik of train dataset  \nprint(confusion_matrix(y_train_res,pred_train))\nprint ('\\n')\nprint(classification_report(y_train_res,pred_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50cb3b81afb94ed193db4b6a3da7333f01c9e793"},"cell_type":"code","source":"# Confusion Matriks of Test Dataset  \nPred_RFC =RFC.predict(x_test)\n\nprint('Confusion Matrix : ','\\n',confusion_matrix(y_test,Pred_RFC))\nprint ('\\n')\nprint(classification_report(y_test,Pred_RFC))\nprint('\\n')\nprint ('Accuracy_R.Forest_Classifier : ', \n                     accuracy_score(y_test,Pred_RFC)*100,'%')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c82f4e83bf6f8d1a2c02e1884d391be7d1ddcb18"},"cell_type":"markdown","source":"The accuracy above  only produce less than 70 %. it indicate there is tendency for overfitting. What is overfitting ?\nOverfitting happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. This means that the noise or random fluctuations in the training data is picked up and learned as concepts by the model. Therefore, you must find the optimal method to solve it.  The following is another methods to solve this issue. \n"},{"metadata":{"_uuid":"0dcd803b0c6e7e2305747859d3eae632c30b939c"},"cell_type":"markdown","source":"## **B. Boostrap Aggregating Classifier**"},{"metadata":{"trusted":true,"_uuid":"ff22ce8538c29c86bb096569a350bbccb5067877"},"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nBS = BaggingClassifier(RandomForestClassifier(), n_estimators = 300 )\nBS.fit(x_train_res, y_train_res.ravel())\n\n#predict \npred_train_BS = BS.predict(x_train_res)\npred_test_BS = BS.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a640ea4cdbf340e0b500333c46c7a4b8fb5fb02a"},"cell_type":"code","source":"# Confusion Matriks of Test Dataset  \nprint('Confusion Matrix : ','\\n',confusion_matrix(y_test,pred_test_BS))\nprint ('\\n')\nprint(classification_report(y_test,pred_test_BS))\nprint('\\n')\nprint ('Accuracy_Bagging Classifier : ', \n                     accuracy_score(y_test,pred_test_BS)*100,'%')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a188acbf9db544b7489d3d80a823ac2a17702f12"},"cell_type":"markdown","source":" ## **C. AdaBoost Classifier**"},{"metadata":{"trusted":true,"_uuid":"cb32ee09e1b1f0d9ba277c1575355c0614173b0e"},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nAB = AdaBoostClassifier(DecisionTreeClassifier(), n_estimators = 300 )\nAB.fit(x_train_res, y_train_res.ravel())\n\n#predict \npred_train_AB = AB.predict(x_train_res)\npred_test_AB = AB.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5fe3c14fb46a3fa4687927068db0eeefa6cc0e11"},"cell_type":"code","source":"# Confusion Matriks of Test Dataset  \nprint('Confusion Matrix : ','\\n',confusion_matrix(y_test,pred_test_AB))\nprint ('\\n')\nprint(classification_report(y_test,pred_test_AB))\nprint('\\n')\nprint ('Accuracy_ AdaBoost Classifier : ', \n                     accuracy_score(y_test,pred_test_AB)*100,'%')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"80e6b96e92cc9fa11bf99076f3630b4ffc54af50"},"cell_type":"markdown","source":"## **5. Oversampling using the other method approach**\n"},{"metadata":{"_uuid":"078cf07325b1fa6bf74afa7c4b19de98e2e9592d"},"cell_type":"markdown","source":"in order to solve imbalanced class, you can use the other method, it like who I used that is doing duplicate dataset for every feature which has imbalance class. "},{"metadata":{"trusted":true,"_uuid":"ed3e451dfc6df57ff4469344360fcf530d8cdea0"},"cell_type":"code","source":"# split dataset to be train and test\ntrain = pd.concat([x_train,y_train], axis = 1)\nprint(train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e874dc741377f9504be22132c45d12f50ac7396"},"cell_type":"code","source":"test = pd.concat([x_test,y_test], axis = 1)\nprint(test.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8859275f2cd80685cd14eb828377134b7aa4a99"},"cell_type":"markdown","source":"Note : you must remember one thing, the dataset which oversampling that is only train dataset. "},{"metadata":{"trusted":true,"_uuid":"345b03c07326752042b5cdafc4566effcfc272cb"},"cell_type":"code","source":"dt=train['Type'].groupby(train['Type']).count()\n\nprint ('The number of each Type class = \\n')\nprint (dt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9fe66f29ca0b929f08bf1c12033a99abbc5276a0"},"cell_type":"code","source":"# we will calculate for each the number of class\nC3 = train[train['Type']==3]\nC3 = pd.concat([C3]*5)\n\nC5 = train[train['Type']==5]\nC5 = pd.concat([C5]*5)\n\nC6 = train[train['Type']==6]\nC6 =pd.concat([C6]*8)\n\nC7 = train[train['Type']==7]\nC7 = pd.concat([C7]*2)\n\nC1 = train[train['Type']==1]\n\nC2 = train[train['Type']==2]\n\n#Combain of every dataframe above with new variable name \ndata_balanced=pd.concat([C1,C2,C3,C5,C6,C7])\ndata_balanced.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad5a59d79db68184659fb1b936ff7d71f981c3e8"},"cell_type":"code","source":"data_balanced.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8afeaae9f0908a0b719cd11c66132f1136d5e191"},"cell_type":"code","source":"type=data_balanced['Type'].groupby(data_balanced['Type']).count()\ntype","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3b62586ea527b6ba847c2609c1574c4b6c3919a"},"cell_type":"markdown","source":"Output above show the result of oversampling with the other method. Then we will use the same method to predict a type of glass. "},{"metadata":{"_uuid":"3bcbdba563d402ba3852364d9e35eb7ffbcc4320"},"cell_type":"markdown","source":"## **A. Random Forest Classifier**"},{"metadata":{"trusted":true,"_uuid":"9dfa4d8727201eae94a82c756b16109b10e8b711"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nx_train_A = data_balanced.drop('Type', axis=1)\ny_train_A = data_balanced['Type']\n\nRFC = RandomForestClassifier(n_estimators = 300, criterion = 'entropy', random_state = 42, max_depth = 10 )\nRFC.fit(x_train_A, y_train_A)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a195cd361882b4337a824863219cf879a260c898"},"cell_type":"code","source":"pred_train = RFC.predict(x_train_A)\nprint(confusion_matrix(y_train_A,pred_train))\nprint ('\\n')\nprint(classification_report(y_train_A,pred_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8cc0f576f53cb8f7ea3717d1f20b6d653b5b287"},"cell_type":"code","source":"#predict test dataset \nx_test_A = test.drop('Type', axis=1)\ny_test_A = test['Type']\n\npred_test = RFC.predict(x_test_A)\nprint('Confusion Matrix : ','\\n',confusion_matrix(y_test_A,pred_test))\nprint ('\\n')\nprint(classification_report(y_test_A,pred_test))\n\nprint ('Accuracy_R.Forest_Classifier_B : ', \n                     accuracy_score(y_test_A,pred_test)*100,'%')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6cadb4c4ec8d23550ecbc929f68a2296f72778f2"},"cell_type":"markdown","source":"## ** B. Bagging Aggregating Classifier**"},{"metadata":{"trusted":true,"_uuid":"69f609b1ff9fb22eaa9b245bd6fb2a9b9093ca11"},"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nbs = BaggingClassifier(RandomForestClassifier(), n_estimators = 300 )\nbs.fit(x_train_res, y_train_res.ravel())\n\n#predict \npred_train_bs = bs.predict(x_train_res)\npred_test_bs = bs.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"552573f4a8640ff0b27bad40eb75b98b119812ff"},"cell_type":"code","source":"# Confusion Matriks of Test Dataset  \nprint('Confusion Matrix : ','\\n',confusion_matrix(y_test,pred_test_bs))\nprint ('\\n')\nprint(classification_report(y_test,pred_test_bs))\nprint('\\n')\nprint ('Accuracy_Bagging Classifier : ', \n                     accuracy_score(y_test,pred_test_bs)*100,'%')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ef6af2f044e2635d14a483f4835c8da10e9d5a0"},"cell_type":"markdown","source":"The result is producing same accuracy from both of method SMOTE and duplicate dataset as much 67 %. I think it is bad model. it can be improved with setting the hyperparameter or feature engineering and selection. "},{"metadata":{"trusted":true,"_uuid":"cea3de9df1de74c00f8a4e538c3762623780dc6d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}