{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Read my article on NLP and use this code for better understanding\n","metadata":{}},{"cell_type":"code","source":"# In any machine learning task, cleaning or preprocessing the data is as important as model building if not more. And when it comes to unstructured data like text, this process is even more important.\n\n# Objective of this code is to understand the various text preprocessing steps with examples.\n\n# Some of the common text preprocessing / cleaning steps are:\n\n# * Lower casing\n# * Removal of Punctuations\n# * Removal of Stopwords\n# * Removal of Frequent words\n# * Removal of Rare words\n# * Stemming\n# * Lemmatization\n\n# So these are the different types of text preprocessing steps which we can do on text data. But we need not do all of these all the times. We need to carefully choose the preprocessing steps based on our use case since that also play an important role.\n\n# For example, in sentiment analysis use case, we need not remove the emojis or emoticons as it will convey some important information about the sentiment. Similarly we need to decide based on our use cases.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport nltk\nimport spacy\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_df = pd.read_csv(\"/kaggle/input/pfizer-vaccine-tweets/vaccination_tweets.csv\")\ndf = full_df[[\"text\"]]\ndf[\"text\"] = df[\"text\"].astype(str)\nfull_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We will work only on how to do text preprocessing the tweet texts available in the dataset\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1. Lowercase conversion\ndf[\"text_lower\"] = df[\"text\"].str.lower()\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2. Remove stopwords\n# Import stopwords from nltk\nfrom nltk.corpus import stopwords\n\n# list of stopwords\n\", \".join(stopwords.words(\"english\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Stopwords = set(stopwords.words(\"english\"))\n\ndef remove_stopwords(text):\n    return \" \".join([words for words in str(text).split() if words not in Stopwords])\n\ndf[\"text_without_stopwords\"] = df[\"text_lower\"].apply(lambda text: remove_stopwords(text))\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 3. Remove Punctuations\n\nPunctuations = string.punctuation\nprint(Punctuations)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_punctuations(text):\n    return text.translate(str.maketrans(\"\", \"\", Punctuations))\n\ndf[\"text_without_punctuations\"] = df[\"text_without_stopwords\"].apply(lambda text: remove_punctuations(text))\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove Frequent words\nfrom collections import Counter\nCount = Counter()\nfor text in df[\"text_without_punctuations\"].values:\n    for word in text.split():\n        Count[word] += 1\n        \nCount.most_common(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removal of rare words\n\nFrequent_Words = set([w for (w, wc) in Count.most_common(10)])\ndef remove_freqwords(text):\n    return \" \".join([word for word in str(text).split() if word not in Frequent_Words])\n\ndf[\"text_without_stopfreq\"] = df[\"text_without_punctuations\"].apply(lambda text: remove_freqwords(text))\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Snowball Stemmer\nfrom nltk.stem.snowball import SnowballStemmer\n\n# Drop the two columns \n#df.drop([\"text_wo_stopfreq\", \"text_wo_stopfreqrare\"], axis=1, inplace=True) \nstemmer = SnowballStemmer(\"english\")\ndef stem_words(text):\n    return \" \".join([stemmer.stem(word) for word in text.split()])\n\ndf[\"text_stemmed\"] = df[\"text_without_stopfreq\"].apply(lambda text: stem_words(text))\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Lemmatization\n\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\ndef lemmatize_words(text):\n    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n\ndf[\"text_lemmatized\"] = df[\"text_without_punctuations\"].apply(lambda text: lemmatize_words(text))\ndf.head() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}