{"cells":[{"metadata":{"_uuid":"ec1a986cffa6f50a3de321c4c99d1b88b57d48d1"},"cell_type":"markdown","source":"In this kernel, I am going to build few clustering models. Clustering is used to group data points together into a group which we call as a cluster. Broadly, there are two types of clustering :\n1. Hard Clustering(each point belongs to only one cluster)\n2. Soft Clustering(each data point is associated with probabilistic values for each cluster)\n\nWe are going to use some common clustering algorithms like K-Means Clustering , Agglomerative Clustering and Gaussian Mixture.\nK-Means and Agglomerative are hard clustering methods whereas Gaussian Mixture is a soft clustering method."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Load csv into data frame\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\ndf = pd.read_csv('../input/Seed_Data.csv')\ndf.head() # first 5 rows","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"837085101069b90719cdb412aa177d34caa97b6b"},"cell_type":"code","source":"df.shape #(rows, columns)\n# 210 data points","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5af020a16adba02d0bbe16028e40ae6364a5f54"},"cell_type":"code","source":"#  Are the features strongly related ? \n# To know this, take each column as dependent variable and try to predict this column\n# from other columns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\ncols = df.columns\n\nfor col in cols:\n    X = df.drop([col], axis=1)\n    y = pd.DataFrame(df.loc[:, col])\n    X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42)\n    reg = LinearRegression()\n    reg.fit(X_train,y_train)\n    score = reg.score(X_test,y_test)\n    print('Score for {} as dependent variable is {}'.format(col,score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d71f073c462563ec7b12a69f2eb9f6092e80415c"},"cell_type":"code","source":"# Visualize how each feature is related to another feature\npd.scatter_matrix(df, diagonal='kde', figsize=(16,9))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e579b0e36d8c8965504aa1af8afdc440184fb46"},"cell_type":"markdown","source":"Now, I am going to pick two columns whose graph gives us some cluster like distribution i.e. I am not going to choose graphs with linear distribution(P and LK). For example, A and A_Coef have a non-linear distribution. Let's work on that!"},{"metadata":{"trusted":true,"_uuid":"dddfd24318439fee7f213494804b4d06f77eaca5"},"cell_type":"code","source":"df_A = df[['A','A_Coef']]\ndf_A.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f9b7793c62b96290be9893824fab398741f1e285"},"cell_type":"code","source":"import matplotlib.pyplot as plt\ndf_A.plot('A','A_Coef',kind='scatter',figsize=(7,5))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92ab40487324a63f8563658b16853a789c1bb115"},"cell_type":"markdown","source":"**How to find the number of clusters we need?**\n\nI am going to use elbow method and draw a graph that can give us the rough idea on the number of clusters that best suits for our data set."},{"metadata":{"trusted":true,"_uuid":"39080691c41f64fa471effa61756bbed1670edfa"},"cell_type":"code","source":"from sklearn.cluster import KMeans\n\nno_of_clusters = range(1, 10)\nkmeans = [KMeans(n_clusters=i) for i in no_of_clusters]\nscore = [kmeans[i].fit(df_A).score(df_A) for i in range(len(kmeans))]\nplt.plot(no_of_clusters,score)\nplt.xlabel('Number of Clusters')\nplt.ylabel('Score')\nplt.title('Elbow Curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4feefa3e4be3afa52c5ca4026d99315e9971aac3"},"cell_type":"markdown","source":"The above graph plots number of clusters against score. We can see that after 3, there isn't much increase in score."},{"metadata":{"_uuid":"9bbe7a8786baf8a6c540246949a9b7273f5bdc9a"},"cell_type":"markdown","source":"**K-Means Clustering on A and A_Coef**"},{"metadata":{"trusted":true,"_uuid":"c512cc2d98c8597a4647efe2741fd48efcd3703c"},"cell_type":"code","source":"kmeans = KMeans(n_clusters=3, random_state=42)\nkmeans.fit(df_A)\ncluster_labels = kmeans.predict(df_A)\nkmeans.cluster_centers_\n# 3 cluster centres","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e4ff946e06e891024cbece67759f218b5a9d22f"},"cell_type":"code","source":"kmeans.labels_ \n# 0, 1, 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"599e010ca4a65c8c260bb40bf238369b74897233"},"cell_type":"code","source":"plt.figure(figsize=(7,5))\nplt.scatter(df_A['A'],df_A['A_Coef'],c=kmeans.labels_)\nplt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1], color='red')\nplt.title('3 Means Clustering')\nplt.xlabel('A')\nplt.ylabel('A_Coef')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd81900598a5d9a6ec193c6ff29499703bfc520e"},"cell_type":"markdown","source":"**Agglomerative Clustering**"},{"metadata":{"trusted":true,"_uuid":"b38745f4e961c21d48fc2e003e1f43885bcea336"},"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\nhac_clustering = AgglomerativeClustering(n_clusters=3).fit(df_A)\nhac_clustering","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef02448c9a614eb4e50b9cefc3c22eac79983b02"},"cell_type":"code","source":"plt.figure(figsize=(7,5))\nplt.scatter(df_A['A'],df_A['A_Coef'],c=hac_clustering.labels_)\nplt.title('3 Means Clustering')\nplt.xlabel('A')\nplt.ylabel('A_Coef')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5e8454b50dd873e7ed26757f482da6e8cacc48c"},"cell_type":"markdown","source":"**K- Means Clustering on WK and LKG**"},{"metadata":{"trusted":true,"_uuid":"2650b5f84a9cc6bfcabc8a36506fb15ab374c6d9"},"cell_type":"code","source":"df.plot.scatter('WK','LKG')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a596a26b57ccfcbbafff735e3efeddcddbdc252f"},"cell_type":"code","source":"df_LKG_WK = df[['LKG','WK']]\nno_of_clusters=range(1,10)\nkmeans = [KMeans(n_clusters=i) for i in no_of_clusters]\nscore = [kmeans[i].fit(df_LKG_WK).score(df_LKG_WK) for i in range(len(kmeans))]\nscore\nplt.plot(no_of_clusters, score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e206aac74299dee6717fb8b5bae54e1c5a90ff76"},"cell_type":"code","source":"kmeans = KMeans(n_clusters=2,random_state=42)\nkmeans.fit(df_LKG_WK)\nkmeans.predict(df_LKG_WK)\nkmeans.cluster_centers_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4aa2fce5df0cfc44065c7a7826ae91315f2e1088"},"cell_type":"code","source":"plt.scatter(df_LKG_WK.iloc[:,0],df_LKG_WK.iloc[:,1],c=kmeans.labels_)\nplt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1], color='red')\nplt.title('2 Means Clustering')\nplt.xlabel('WK')\nplt.ylabel('LKG')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7aa2b27417b8ef8ea74d9e7be293870eaec7d2b0"},"cell_type":"markdown","source":"**GaussianMixture(Soft Clustering) on LKG and WK**"},{"metadata":{"trusted":true,"_uuid":"7895c0b6aba1a2ea9e0bcdc62c37938855e9919a"},"cell_type":"code","source":"from sklearn.mixture import GaussianMixture\ngmm = GaussianMixture(n_components=2).fit(df_LKG_WK)\ngmm_labels = gmm.predict(df_LKG_WK)\nplt.scatter(df_LKG_WK.iloc[:,0],df_LKG_WK.iloc[:,1],c=gmm_labels)\nplt.title('2 Means Clustering')\nplt.xlabel('WK')\nplt.ylabel('LKG')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5185c2ef73a6ef5a712423f164795e9a183a0c05"},"cell_type":"code","source":"# As we know that this soft clustering, we can find the probability\n# with which each data point belongs to the two clusters\ny_pred = gmm.predict_proba(df_LKG_WK)\ny_pred[50] # Probability that that data point at row 50 belongs to cluster 1 is 0.99674903 and cluster 2 is 0.00325097","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c5116fade114d95a1e2b2e196ec1b869ba39f33"},"cell_type":"code","source":"y_pred[100] #cluster 1 - 0.01608574, cluster 2 - 0.98391426","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}