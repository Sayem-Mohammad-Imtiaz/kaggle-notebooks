{"cells":[{"metadata":{},"cell_type":"markdown","source":"Thanks @junyan\n\n## Purpose\n\n* Compare GAM models and lightGBM@K-fold"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# データの前処理"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# ref: https://www.kaggle.com/mesadowski/moneyball-golf-scikit-learn-and-tf-estimator-api\n\n# read data\ndf = pd.read_csv('../input/pga-tour-20102018-data/PGA_Data_Historical.csv')\n\n# unstack\ndf = df.set_index(['Player Name', 'Variable', 'Season'])['Value'].unstack('Variable').reset_index()\n\n# drop non-numeric features\nkeep_columns = [\n    'Player Name',\n    'Season',\n    'Total Money (Official and Unofficial) - (MONEY)', # 年間獲得賞金 (ドル)\n    'Driving Distance - (AVG.)', # ドライバーの平均飛距離 (ヤード)\n    'Driving Accuracy Percentage - (%)', # ドライバーのフェアウェイキープ率\n    'Total Distance Efficiency - (AVERAGE DISTANCE (YARDS))', # ドライバーの、ボール スピードに対する飛距離 (高いほうが効率よく飛んでいる)\n    'Average Distance to Hole After Tee Shot - (AVG)', # 平均の平地ショット飛距離\n    'Ball Speed - (AVG.)', # 平均ボール速度\n    'Scrambling from the Sand - (%)', # バンカーからのスクランブル率 (パーオン出来なかったホールで、パー以上であがること)\n    'Scrambling from the Fringe - (%)', # グリーンのフリンジからのスクランブル率\n    'Scrambling from the Rough - (%)', # ラフからのスクランブル率\n    '3-Putt Avoidance - (%)', # 3 パットしたホールの割合\n    'Birdie or Better Conversion Percentage - (%)' # バーディーより良い成績で挙がるホールの割合\n]\ndf = df[keep_columns].dropna()\n\n# rename the columns to something shorter\ndf.rename(columns = {'Total Money (Official and Unofficial) - (MONEY)':'Money'}, inplace = True)\ndf.rename(columns = {'3-Putt Avoidance - (%)':'ThreePuttRate'}, inplace = True)\ndf.rename(columns = {'Average Distance to Hole After Tee Shot - (AVG)':'NonDrivingDistance'}, inplace = True)\ndf.rename(columns = {'Total Distance Efficiency - (AVERAGE DISTANCE (YARDS))':'DistanceEfficiency'}, inplace=True)\ndf.rename(columns = {'Ball Speed - (AVG.)':'BallSpeed'}, inplace=True)\ndf.rename(columns = {'Driving Distance - (AVG.)':'DrivingDistance'}, inplace = True)\ndf.rename(columns = {'Driving Accuracy Percentage - (%)':'DrivingAccuracy'}, inplace=True)\ndf.rename(columns = {'Scrambling from the Sand - (%)':'ScramblingSand'}, inplace = True)\ndf.rename(columns = {'Scrambling from the Fringe - (%)':'ScramblingFringe'}, inplace=True)\ndf.rename(columns = {'Scrambling from the Rough - (%)':'ScramblingRough'}, inplace=True)\ndf.rename(columns = {'Birdie or Better Conversion Percentage - (%)':'BirdieConversion'}, inplace=True)\n\n# remove $ and commas from Money\ndf['Money']= df['Money'].str.replace('$','')\ndf['Money']= df['Money'].str.replace(',','')\n\n# make all variables into number\nfor col in  df.columns[2:]:\n   df[col] = df[col].astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(0)\nindex = np.random.randint(df.shape[0], size=10)\ndf.iloc[index,:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 統計量の出力"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby(\"Season\").mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 各年度の最大賞金プレイヤー"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df.groupby(\"Season\")[\"Money\"].idxmax()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](https://pga-tour-res.cloudinary.com/image/upload/c_fill,f_auto,g_center,h_478,q_auto,w_850/v1/pgatour/editorial/2019/08/18/JustinThomasTrophyClean-847-KK.jpg)\n\nJustin Thomas はこんな人らしい。\n\nhttps://www.pgatour.com/news/2019/08/18/justin-thomas-shows-he-still-knows-how-to-win-bmw-championship-medinah-country-club-fedexcup.html"},{"metadata":{},"cell_type":"markdown","source":"## 各特徴量同士の相関"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df[df.columns[2:]].corr()\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)]= True\n\nf, ax = plt.subplots(figsize=(11, 15))\nheatmap = sns.heatmap(corr, \n                      square = True,\n                      mask = mask,\n                      linewidths = .5,\n                      cmap = 'coolwarm',\n                      cbar_kws = {'shrink': .4, \n                                'ticks' : [-1, -.5, 0, 0.5, 1]},\n                      vmin = -1, \n                      vmax = 1,\n                      annot = True,\n                      annot_kws = {\"size\": 12})\n\nax.set_yticklabels(corr.columns, rotation = 0)\nax.set_xticklabels(corr.columns)\nsns.set_style({'xtick.bottom': True}, {'ytick.left': True})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"賞金と正の相関が強いもの\n\n- ドライバーの距離\n- 飛距離の効率性 (スピン量)\n- ボールスピード\n- ラフからのリカバリの上手さ\n- **バーディよりよい成績でホールを終える率**\n\n賞金と負の相関が認められるもの\n\n- ドライバー以外のショット飛距離\n- 3 パットするホールの割合"},{"metadata":{},"cell_type":"markdown","source":"# GLM/GAM による分析"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 説明変数と目的変数\nX = df[df.columns[3:]]\ny = df[\"Money\"]\n\nprint('X', X.shape)\nprint('y', y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# 訓練データとテストデータに分離\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=2018)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## pyGAM をインストール\n\nhttps://pygam.readthedocs.io/en/latest/"},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"!pip install pygam","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## pyGAM による GAM モデル"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pygam import s, LinearGAM\n\ngam = LinearGAM(s(0) + s(1) + s(2) + s(3) + s(4) + s(5) + s(6) + s(7) + s(8) + s(9))\ngam = gam.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"各特徴量の予測値への寄与を見るために、スプライン関数をプロットする。"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_splines(gam):\n    fig, axes = plt.subplots(2, 5, figsize=(18, 12))\n    axes = np.array(axes).flatten()\n    for i, (ax, title, p_value) in enumerate(zip(axes, X_train.columns, gam.statistics_['p_values'])):\n        XX = gam.generate_X_grid(term=i)\n        ax.plot(XX[:, i], gam.partial_dependence(term=i, X=XX))\n        ax.plot(XX[:, i], gam.partial_dependence(term=i, X=XX, width=.95)[1], c='r', ls='--')\n        ax.axhline(0, c='#cccccc')\n        ax.set_title(\"{0:} (p={1:.2})\".format(title, p_value))\n        ax.set_yticks([])\n        \nplot_splines(gam)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## pyGAM による GLM モデル"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pygam import l, LinearGAM\n\nglm = LinearGAM(l(0) + l(1) + l(2) + l(3) + l(4) + l(5) + l(6) + l(7) + l(8) + l(9))\nglm = glm.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_splines(glm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# InterpretML\n\nhttps://github.com/microsoft/interpret\n\n## インストール"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install interpret","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"まずは、`interpret.data.Marginal` を使って、データの様子 (i.e. 各特徴量の分布) を見る"},{"metadata":{"trusted":true},"cell_type":"code","source":"from interpret import show\nfrom interpret.data import Marginal\n\nmarginal = Marginal().explain_data(X_test, y_test, name = 'test data')\nshow(marginal)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 説明可能なブースティング・モデル\n\nいよいよブースティングモデルを利用して、説明可能なモデルを学習する。"},{"metadata":{"trusted":true},"cell_type":"code","source":"from interpret.glassbox import ExplainableBoostingRegressor, LinearRegression, RegressionTree\n\nebm = ExplainableBoostingRegressor(random_state=0, scoring=\"mean_squared_error\")\nebm.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"学習したモデルを解釈するには、以下のように `explain_global` を用いる。\nパネルを操作すれば、各特徴量に対して、EBM がスコアへの寄与をノンパラメトリック モデルで推定していることが確認できる。"},{"metadata":{"trusted":true},"cell_type":"code","source":"ebm_global = ebm.explain_global(name='EBM')\nshow(ebm_global)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"さらにデータ点への予測の説明も行うことが出来る。\n最初の 5 つのデータセットに対して予測を行い、その説明をするには次のコードを実行する。"},{"metadata":{"trusted":true},"cell_type":"code","source":"ebm_local = ebm.explain_local(X_test[:5], y_test[:5], name='EBM')\nshow(ebm_local)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"最後に MSE の分布を表示する。\n残差分布が正規分布に近いことが確認できる。"},{"metadata":{"trusted":true},"cell_type":"code","source":"from interpret import show\nfrom interpret.perf import RegressionPerf\n\nebm_perf = RegressionPerf(ebm.predict).explain_perf(X_test, y_test, name='EBM')\nshow(ebm_perf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 精度の比較\n\n以下のモデルに対し、テストデータへの MSE (meas squared error) を報告する。\n\n- `pyGAM` の GLM\n- `pyGAM` の GAM\n- `interpret` の Boosting Model\n- `interpret` の Linear Regression\n- `interpret` の Regression Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"from interpret.glassbox import LinearRegression, RegressionTree\n\nlr = LinearRegression(random_state=0)\nlr.fit(X_train, y_train)\nrt = RegressionTree(random_state=0)\nrt.fit(X_train, y_train)\n\nmodel_names = [\n    (glm, \"GLM\"),\n    (gam, \"GAM\"),\n    (ebm, \"InterpretML (EBM)\"),\n    (lr, \"InterpretML (LR)\"),\n    (rt, \"InterpretML (RT)\")\n]\nresult = pd.DataFrame()\n\nfor (model, name) in model_names:\n    y_pred = model.predict(X_test)\n    series = pd.Series()\n    series[\"model\"] = name\n    series[\"MSE\"] = mean_squared_error(y_test, y_pred)\n    result = result.append(series, ignore_index=True)\nresult","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Appendix\n\nlightGBM と勝負!"},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMRegressor\nfrom sklearn.model_selection import KFold\n\ndef train_lgbm(params, log=False):\n    fold = KFold(n_splits=8)\n    oof = np.zeros_like(y_train)\n    models = []\n    \n    if log:\n        y = np.log1p(y_train.values)\n    else:\n        y = y_train.values\n\n    for idx_train, idx_valid in fold.split(X_train, y):\n        clf = LGBMRegressor(**params)\n        clf.fit(X_train.values[idx_train], y[idx_train], \n                eval_set=(X_train.values[idx_valid], y[idx_valid]), \n               early_stopping_rounds=100,\n               verbose=50)\n        oof[idx_valid] = clf.predict(X_train.values[idx_valid])\n        models.append(clf)\n    if log:\n        oof = np.expm1(oof)\n    return models, oof","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install git+https://gitlab.com/nyker510/vivid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'objective': 'poisson',\n    'learning_rate': .05,\n    'n_estimators': 1000,\n    'reg_lambda': 10.,\n    'colsample_bytree': .7,\n    'importance_type': 'gain',\n    'max_depth': 2 # 多重共線性が悪さしているようで depth が深いと精度が出ない\n}\n\nmodels, oof = train_lgbm(params, log=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from vivid.metrics import regression_metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regression_metrics(oof, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from vivid.visualize import visualize_feature_importance\n\nfig, ax, importance_df = visualize_feature_importance(models, columns=X_train.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* BirdieConversion が一番大事\n* NonDrivingDistance はあまり大事ではないみたい"},{"metadata":{"trusted":true},"cell_type":"code","source":"importance_df.groupby('column').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LGBMEnsumble:\n    def __init__(self, models, log=False):\n        self.models = models\n        self.log = log\n        \n    def predict(self, x):\n        p = np.mean([m.predict(x) for m in self.models], axis=0)\n        if self.log:\n            p = np.expm1(p)\n        return p","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_ens = LGBMEnsumble(models)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(6, 6))\nax.scatter(oof, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel_names = [\n    (glm, \"GLM\"),\n    (gam, \"GAM\"),\n    (ebm, \"InterpretML (EBM)\"),\n    (lr, \"InterpretML (LR)\"),\n    (rt, \"InterpretML (RT)\"),\n    (lgbm_ens, 'lgbm')\n]\nresult = pd.DataFrame()\n\nfig, axes = plt.subplots(figsize=(12, 8), nrows=2, ncols=3, sharex=True, sharey=True)\naxes = [a for x in axes for a in x]\n\nfor (model, name), ax in zip(model_names, axes):\n    y_pred = model.predict(X_test)\n    mse = mean_squared_error(y_test, y_pred)\n    ax.scatter(y_pred, y_test, label=name)\n    ax.set_title(f'{mse:.3e}')\n    ax.legend()\n    series = pd.Series()\n    series[\"model\"] = name\n    series[\"MSE\"] = mse\n    result = result.append(series, ignore_index=True)\n\nfig.tight_layout()\n\nresult","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.plot(kind='bar', x='model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from vivid.out_of_fold.ensumble import RFRegressorFeatureOutOfFold\nfrom vivid.out_of_fold.boosting import OptunaXGBRegressionOutOfFold, OptunaXGBRegressionOutOfFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RFRegressorFeatureOutOfFold(name='rf')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf.fit(X_train, y_train.values)\np = rf.predict(X_test)\nregression_metrics(y_test, p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optuna_lgbm = OptunaXGBRegressionOutOfFold(name='optuna_lgbm', n_trials=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optuna_lgbm.fit(X_train, y_train.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p = optuna_lgbm.predict(X_test)\n\nregression_metrics(y_test, p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optuna_xgb = OptunaXGBRegressionOutOfFold(n_trials=200, name='optuna_xgb')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optuna_xgb.fit(X_train, y_train.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = optuna_xgb.predict(X_test)\nregression_metrics(y_test, y)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}