{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n!pip install ktrain\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Simplest code for NLP - Sentiment analysis\nWe'll look out the most basic code block for **sentiment analysis** problem in **5 easy steps**\n\nFor this we are going to rely on **ktrain library** and the **Google's infamous NLP model: BERT**\n\n**As this is the basic guide, please don't expect multiple analysis steps or graphs**","metadata":{}},{"cell_type":"markdown","source":"# Step 1: Import libraries, dataset and some basic code cleaning\n\nWe'll clean the dataset for special characters and punctuations","metadata":{}},{"cell_type":"code","source":"import pandas\nimport ktrain\nimport re\nfrom ktrain import text\n\ndataset_real = pandas.read_csv('../input/fake-and-real-news-dataset/True.csv')\ndataset_fake = pandas.read_csv('../input/fake-and-real-news-dataset/Fake.csv')\n\ndataset_real['category'] = 1\ndataset_fake['category'] = 0\n\ndataset = pandas.concat([dataset_real, dataset_fake])\ndataset.text = dataset.text.apply(lambda x: re.sub('[^a-zA-Z]', ' ', x))\ndataset.text = dataset.text.apply(lambda x: re.sub('  ', ' ', x))\n\nprint(dataset.info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 2: Preprocess data, split dataset for training and test\n\nWe can use any of the ktrain library's below functions based on our needs:\n* texts_from_df - to read data from dataframe\n* texts_from_folder - to read data from a directory\n* texts_from_csv - to read data from a csv file\n\n**Arguments passed:**\n* text_column - independent variable/input data which is to be processed\n* label_columns - list of dependent variable/output result\n* preprocess_mode - preprocess the data to suit which model (takes in 3 params: standard, bert or distilbert)\n* maxlen - max features to be set for the model (need to be careful as it utilizes huge resource power)","metadata":{}},{"cell_type":"code","source":"(xtrain, ytrain), (xtest, ytest), preprocess = text.texts_from_df(dataset, text_column='title', label_columns=['category'], maxlen=128, preprocess_mode='bert')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 3: Create a model\n\nCreate a model of your choice, note that the name parameter passed here should match the preprocess_mode set above","metadata":{}},{"cell_type":"code","source":"model = text.text_classifier(name='bert', train_data=(xtrain, ytrain), preproc=preprocess)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 4: Create a Learner instance\n\nThis step creates a learner instance, which is to be trained in the next step\nValidation is passed so that the model can validate its prediction on validation set and re-adjusts its feature weights","metadata":{}},{"cell_type":"code","source":"learner = ktrain.get_learner(model=model, train_data=(xtrain, ytrain), val_data=(xtest, ytest), batch_size=32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Step 5: Train the model\n\nNow train the model by passing learning rate and epochs... its that simple and we are done\nI just didnt have patience to wait till it completes so just terminated it, hope you got the jist though\n\n**Note: Since BERT model is very powerful and already comes with huge preloaded data; in single epoch, you'd get accuracy above 90%**\n**Only downside to it is takes lot of time as the model is heavy, you can try its lite version distilbert **\n\nTo test the prediction, you can use predict method","metadata":{}},{"cell_type":"code","source":"learner.fit_onecycle(lr=2e-5, epochs=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}