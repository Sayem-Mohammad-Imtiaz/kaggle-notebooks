{"cells":[{"metadata":{},"cell_type":"markdown","source":"If you are new to Text processing, this is for you. Feel free to fork it and upvote if you find it helpful.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport sqlite3\nimport nltk \nimport string\n\n#from sklearn.feature_extraction.text import TfidTransformer\n#from sklearn.feature_extraction.text import TfidVectorizer\n#from sklearn.feature_extraction.text import CountVectorizer\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import accuracy_score\nfrom nltk.stem.porter import PorterStemmer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"con = sqlite3.connect(\"../input/amazon-fine-food-reviews/database.sqlite\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#selecting those reviews where score is not equal to 3\nfilt_data = pd.read_sql_query(\"select * from Reviews where score != 3 \", con)\n\ndef partition(x):\n    if x < 3:\n        return 'negative'\n    return 'positive'\n\nactual_score = filt_data['Score']\nposneg = actual_score.map(partition)\nfilt_data['Score'] = posneg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filt_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Exploratory Data Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning : Deduplication","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#sorting data according to ProductId\nsorted_data = filt_data.sort_values('ProductId' , axis = 0 , ascending = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#deduplication of data\nfinal = sorted_data.drop_duplicates(subset = {\"UserId\" , \"ProfileName\" , \"Text\" , \"Time\" } , keep = 'first' , inplace = False)\nfinal.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#finding how much % data still remains\n(final['Id'].size * 1.0) / (filt_data['Id'].size * 1.0) * 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning : Remove Error data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# we observe that Helpfullness numerator should alwyas be greater than helpfullness denominator\n# the products or tuples that d not follw this rule need to be removed\n\nfinal = final[final.HelpfulnessNumerator <= final.HelpfulnessDenominator]\nfinal.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,5))\nsns.countplot(final['Score'] , palette = 'gist_rainbow')\nplt.xlabel(\"Reviews\")\nplt.ylabel(\"No. of Reviews\")\nplt.show()\n\nprint(final['Score'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Processing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"While Pre processing, we'll do:-\n* Removing STOP words.\n* Stemming\n* Lemmetization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n#nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import f1_score , precision_score , recall_score\nfrom nltk.metrics.scores import (precision , recall , f_measure)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing stop words , stemming and lemmetization\n\n#set of stop words\nstop = set(stopwords.words('english'))\nwords_to_keep = set(('not'))\nstop -= words_to_keep\n\n#initialising snowball stemmer\nsno = nltk.stem.SnowballStemmer('english')\n\n\n#removing html tags\ndef cleanhtml(sentence):    #function cleans word of html tags\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr , ' ', sentence)\n    return cleantext\n\n#removing punctuation or special characters\ndef cleanpunc(sentence):    #function cleans words with these symbols\n    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n    return cleaned\n\n# text processing code\ni = 0\nstr1 = ' '\nfinal_string = []\nall_positive_words = []\nall_negative_words = []\nw = ''\n\nfor sent in final['Text'].values:\n    filtered_sentence = []\n    sent = cleanhtml(sent)\n    for s in sent.split():\n        for cleaned_words in cleanpunc(s).split():\n            if((cleaned_words.isalpha()) & (len(cleaned_words) > 2)):\n                if(cleaned_words.lower() not in stop):\n                    w = (sno.stem(cleaned_words.lower())).encode('utf8')\n                    filtered_sentence.append(w)\n                    if(final['Score'].values)[i] == 'positive':\n                        all_positive_words.append(w)\n                    if(final['Score'].values)[i] == 'negative':\n                        all_negative_words.append(w)\n                else:\n                    continue\n            else:\n                continue\n                \n    str1 = b\" \".join(filtered_sentence)\n\n    final_string.append(str1)\n    i += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#adding an extra column\nfinal['CleanedText'] = pd.Series(final_string)\nfinal['CleanedText'] = final['CleanedText'].str.decode(\"utf-8\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing all the rows with null reviews after pre processing\ncleaned_final = final\ncleaned_final.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(final.shape)\nprint(cleaned_final.shape)\ncleaned_final.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_final['CleanedText'].replace('', np.nan, inplace=True)\ncleaned_final.dropna(subset=['CleanedText'], inplace=True)\ncleaned_final.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sorting the data according to time in asc\ntimesortdata = cleaned_final.sort_values('Time' , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last')\nx = cleaned_final['CleanedText'].values\ny = cleaned_final['Score']\n\nX_train ,X_test ,Y_train ,Y_test = train_test_split(x ,y ,test_size = 0.3, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"countVector = CountVectorizer(min_df = 1000)\nxTrainVector = countVector.fit_transform(X_train)\nxTestVector = countVector.transform(X_test)\n\nstdScale = StandardScaler(with_mean = False)\nxTrVecStd = stdScale.fit_transform(xTrainVector)\nxTsVecStd = stdScale.transform(xTestVector)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building Decision Tree","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Depths = [3,4,5,6,7,8,9,10,11,12,13,14,15]\nparamGrid = {'max_depth':Depths}\n\nmodel = GridSearchCV(DecisionTreeClassifier() , paramGrid , scoring = 'accuracy' , cv = 3 , n_jobs = -1 , pre_dispatch = 2)\nmodel.fit(xTrVecStd,Y_train)\n\nprint(\"Accuracy : \", model.score(xTsVecStd , Y_test))\n\n#cross validation\ncv = [1-i for i in model.cv_results_['mean_test_score']]\n\n#optimum depth\nopt_depth = model.best_estimator_.max_depth\nprint(\"Optimal depth : \", opt_depth)\n\n#Decision Tree Classifier with optimal depth\nDT = DecisionTreeClassifier(max_depth = opt_depth)\nDT.fit(xTrVecStd , Y_train)\npredictions = DT.predict(xTsVecStd)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Performance Metrices","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"precision = precision_score(Y_test, predictions, pos_label = 'positive')\nprint(\"Precision = \" , precision)\n\nrecall = recall_score(Y_test, predictions , pos_label = 'positive')\nprint(\"\\nRecall = \", recall)\n\nf1 = f1_score(Y_test, predictions , pos_label = 'positive')\nprint(\"\\nf1 Score = \", f1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}