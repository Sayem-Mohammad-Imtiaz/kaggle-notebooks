{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook includes:\n- EDA\n- Outliers removal\n- Correlation\n- Featuer Engineering\n- PCA decoposition\n- Comparing baseline models\n- Model training and predictions"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom datetime import datetime\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check missing values\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This dataset onlu has numerical data, it doesn't contain any categorical data"},{"metadata":{},"cell_type":"markdown","source":"## The target value: quality"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import norm\nfrom scipy import stats\nimport seaborn as sns\n\n# Style\nsns.set_style('white')\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(10, 6))\n\n# Distibution plot\nsns.distplot(data['quality'], color='b', fit=norm)\n\n# Mean and variance\nmu, sigma = norm.fit(data['quality'])\n\n\n# Plot Details\nplt.legend([\"Normal dist. ($\\mu=$ {:.4f} and $\\sigma=$ {:.4f})\".format(mu, sigma)], loc='best')\nax.axes.grid(False)\nplt.ylabel('Frequency', fontsize=12)\nplt.xlabel('Quality', fontsize=12)\nplt.title('Wine Quality Distribution')\nsns.despine(trim=True, left=True)\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Skewness and Kurtosis"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Skewness: {:.4f}\".format(data['quality'].skew()))\nprint(\"Kurtosis: {:.4f}\".format(data['quality'].kurt()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The skewness of quality is close to zero, meaning the tails on both sides of the mean balance out overall.\nThe kurtosis is just 0.2967 (less than 3), Distributions with kurtosis less than 3 are said to be platykurtic, although this does not imply the distribution is \"flat-topped\" as is sometimes stated. Rather, it means the distribution produces fewer and less extreme outliers than does the normal distribution."},{"metadata":{},"cell_type":"markdown","source":"## QQ-plot\n- This plot provides a summary of whether the distributions of two variables are similar or not with respect to the locations."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nres = stats.probplot(data['quality'], plot=plt)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Numerical Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Look at numerical data\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.hist(figsize=(20, 15))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scatter plots of features vs Target value\nfix, axs = plt.subplots(ncols=2, nrows=0, figsize=(20, 50))\nsns.color_palette('husl', 8)\n\nfor i, feature in enumerate(data.columns, 1):\n    plt.subplot(len(data.columns), 3, i)\n    sns.scatterplot(x=feature, y='quality', hue='quality', data=data)\n    plt.xlabel('{}'.format(feature))\n    plt.ylabel('Quality')\n    plt.legend(loc='best', prop={'size': 10})\n    \n    for j in range(2):\n        plt.tick_params(axis='x', labelsize=12)\n        plt.tick_params(axis='y', labelsize=12)\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"General observations: The lower the volatile acidity; The higher the quality; The higher the alcohol, the higher the quality.\nDetails have to be confirmed with correlation studies.\n\nOutliers: There are many outliers in each feature. We will remove them by IQR method in the following"},{"metadata":{},"cell_type":"markdown","source":"# Correlation Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = data.corr()\n\n# Find out which feature has the highest correlation with target\ncorr_matrix['quality'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(10,8))\nsns.heatmap(corr_matrix, vmax=0.8, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Features with positive correlations: alcohol (0.4762) >> sulphates (0.2514) > citric acid (0.2264) >> other features\n- Features with negative correlations: volatile acidity (-0.3906) << total sulfur dioxide (-0.1851) < density (-0.1749) << other features \n"},{"metadata":{},"cell_type":"markdown","source":"# Baseline Models 1\n- With NO Feature Engineering\n- Simple Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n\ndata_base = data.copy()\n\nX = data_base.drop('quality', axis=1).values\ny = np.log1p(data_base['quality']).values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kfold = KFold(n_splits=5, random_state=42, shuffle=True)\n\n# Useful function to display scores\ndef display_scores(scores):\n    print(\"Scores: \", scores)\n    print(\"Mean of scores: {:.4f}\".format(scores.mean()))\n    print(\"Standard Deviation of scores: {:.4f}\".format(scores.std()))\n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nlin_reg = LinearRegression()\nridge_reg = Ridge(random_state=42)\nlasso_reg = Lasso(random_state=42)\nelasticnet_reg = ElasticNet(random_state=42)\ntree_reg = DecisionTreeRegressor(random_state=42)\nforest_reg = RandomForestRegressor(random_state=42)\ngb_reg = GradientBoostingRegressor(random_state=42)\nxgb_reg = XGBRegressor(random_state=42)\nlgmb_reg = LGBMRegressor(random_state=42)\n\nregressors = [lin_reg, ridge_reg, lasso_reg, elasticnet_reg, tree_reg, forest_reg, gb_reg, xgb_reg, lgmb_reg]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"basemodels_performance = []\nfor i in regressors:\n    start_time = datetime.now()\n    model_performance = {}\n    model_performance['model'] = type(i).__name__\n    scores = np.sqrt(-cross_val_score(i, X_train, y_train, cv=kfold, scoring='neg_mean_squared_error'))\n#     model_performance['scores'] = scores\n    model_performance['mean_score'] = round(scores.mean(), 4)\n    model_performance['standard_deviation'] = round(scores.std(), 4)\n    time_used = datetime.now() - start_time\n    model_performance['time_used'] = time_used.total_seconds()\n    basemodels_performance.append(model_performance)\n\ndf_basemodels_performance = pd.DataFrame(basemodels_performance)\ndf_basemodels_performance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Baseline Models 2\n- With NO Feature Engineering\n- Simple Linear Regression\n- Only select highly correlated featuers: alcohol, sulphates, citric acid, volatile acidity\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data_base[['alcohol', 'sulphates', 'citric acid', 'volatile acidity']].values\ny = np.log1p(data_base['quality']).values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lin_reg = LinearRegression()\nridge_reg = Ridge(random_state=42)\nlasso_reg = Lasso(random_state=42)\nelasticnet_reg = ElasticNet(random_state=42)\ntree_reg = DecisionTreeRegressor(random_state=42)\nforest_reg = RandomForestRegressor(random_state=42)\ngb_reg = GradientBoostingRegressor(random_state=42)\nxgb_reg = XGBRegressor(random_state=42)\nlgmb_reg = LGBMRegressor(random_state=42)\n\nregressors = [lin_reg, ridge_reg, lasso_reg, elasticnet_reg, tree_reg, forest_reg, gb_reg, xgb_reg, lgmb_reg]\n\nbasemodels_performance = []\nfor i in regressors:\n    start_time = datetime.now()\n    model_performance = {}\n    model_performance['model'] = type(i).__name__\n    scores = np.sqrt(-cross_val_score(i, X_train, y_train, cv=kfold, scoring='neg_mean_squared_error'))\n#     model_performance['scores'] = scores\n    model_performance['mean_score'] = round(scores.mean(), 4)\n    model_performance['standard_deviation'] = round(scores.std(), 4)\n    time_used = datetime.now() - start_time\n    model_performance['time_used'] = time_used.total_seconds()\n    basemodels_performance.append(model_performance)\n\ndf_basemodels_performance = pd.DataFrame(basemodels_performance)\ndf_basemodels_performance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Baseline Models 1 have better performance"},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"## Remove outliers by Standard Deviation Method\n- First, calculate the mean and standard deviation of data field\n- Second, remove outliers that are lower than Mean - 3 * S/D or greater than Mean + 3 * S/D"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [c for c in data.columns if c != 'quality']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of instances before removing outliers: \", len(data), \"\\n\")\n\ninstances = len(data)\n\nfor feature in features:\n    feature_values = data[feature].values\n    mean = feature_values.mean()\n    std = feature_values.std()\n    lowerbound, upperbound = mean - 3*std, mean + 3*std\n    print(\"Feature: %s\" % feature)\n    print(\"Mean: {:.4f}\".format(mean), \"\\tS.D: {:.4f}\".format(std))\n    print(\"Lower bound: {:.4f}\".format(lowerbound), \"\\tUpper bound: {:.4f}\".format(upperbound))\n    outliers = [x for x in feature_values if (x < lowerbound or x > upperbound)]\n    print(\"Outliers: \", outliers)\n    print(\"Remove {} outliers\".format(len(outliers)))\n    data = data.drop(data[(data[feature]<lowerbound)|(data[feature]>upperbound)].index)\n    print(\"Instances left: {}\".format(len(data)))\n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot scatters again to validate the results after outliers removal"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scatter plots of features vs Target value\nfix, axs = plt.subplots(ncols=2, nrows=0, figsize=(20, 50))\nsns.color_palette('husl', 8)\n\nfor i, feature in enumerate(data.columns, 1):\n    plt.subplot(len(data.columns), 3, i)\n    sns.scatterplot(x=feature, y='quality', hue='quality', palette='Blues', data=data)\n    plt.xlabel('{}'.format(feature))\n    plt.ylabel('Quality')\n    plt.legend(loc='best', prop={'size': 10})\n    \n    for j in range(2):\n        plt.tick_params(axis='x', labelsize=12)\n        plt.tick_params(axis='y', labelsize=12)\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data looks much better after removing outliers"},{"metadata":{},"cell_type":"markdown","source":"# Normalization\n- Using sklearn StandarScaler"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop('quality', axis=1).values\ny = np.log1p(data['quality']).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X=X)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lin_reg = LinearRegression()\nridge_reg = Ridge(random_state=42)\nlasso_reg = Lasso(random_state=42)\nelasticnet_reg = ElasticNet(random_state=42)\ntree_reg = DecisionTreeRegressor(random_state=42)\nforest_reg = RandomForestRegressor(random_state=42)\ngb_reg = GradientBoostingRegressor(random_state=42)\nxgb_reg = XGBRegressor(random_state=42)\nlgmb_reg = LGBMRegressor(random_state=42)\n\nregressors = [lin_reg, ridge_reg, lasso_reg, elasticnet_reg, tree_reg, forest_reg, gb_reg, xgb_reg, lgmb_reg]\n\nmodels_performance = []\n\nfor i in regressors:\n    start_time = datetime.now()\n    model_performance = {}\n    model_performance['model'] = type(i).__name__\n    scores = np.sqrt(-cross_val_score(i, X_train, y_train, cv=kfold, scoring='neg_mean_squared_error'))\n#     model_performance['scores'] = scores\n    model_performance['mean_score'] = round(scores.mean(), 4)\n    model_performance['standard_deviation'] = round(scores.std(), 4)\n    time_used = datetime.now() - start_time\n    model_performance['time_used'] = time_used.total_seconds()\n    models_performance.append(model_performance)\n\ndf_models_performance = pd.DataFrame(models_performance)\ndf_models_performance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PCA for dimensionality reduction\n- Find the right number of dimensions (d) to reduce down to, where the reduced data can explained over 95% of original data's variance\n- Then set PCA(n_components=d)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop('quality', axis=1).values\ny = np.log1p(data['quality']).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA()\npca.fit(X)\ncumsum = np.cumsum(pca.explained_variance_ratio_)\n\nd = np.argmax(cumsum >= 0.95) + 1\n\nprint(\"Original dataset dimensions: %s\" % X.shape[1])\nprint(\"The optimal dimensions to reduce down to: %s\" %d)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reduce\npca = PCA(n_components=d)\nX_reduced = pca.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Models with PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"lin_reg = LinearRegression()\nridge_reg = Ridge(random_state=42)\nlasso_reg = Lasso(random_state=42)\nelasticnet_reg = ElasticNet(random_state=42)\ntree_reg = DecisionTreeRegressor(random_state=42)\nforest_reg = RandomForestRegressor(random_state=42)\ngb_reg = GradientBoostingRegressor(random_state=42)\nxgb_reg = XGBRegressor(random_state=42)\nlgmb_reg = LGBMRegressor(random_state=42)\n\nregressors = [lin_reg, ridge_reg, lasso_reg, elasticnet_reg, tree_reg, forest_reg, gb_reg, xgb_reg, lgmb_reg]\n\nmodels_performance = []\n\nfor i in regressors:\n    start_time = datetime.now()\n    model_performance = {}\n    model_performance['model'] = type(i).__name__\n    scores = np.sqrt(-cross_val_score(i, X_train, y_train, cv=kfold, scoring='neg_mean_squared_error'))\n#     model_performance['scores'] = scores\n    model_performance['mean_score'] = round(scores.mean(), 4)\n    model_performance['standard_deviation'] = round(scores.std(), 4)\n    time_used = datetime.now() - start_time\n    model_performance['time_used'] = time_used.total_seconds()\n    models_performance.append(model_performance)\n\ndf_models_performance = pd.DataFrame(models_performance)\ndf_models_performance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Models without PCA (Better than PCA version)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop('quality', axis=1).values\ny = np.log1p(data['quality']).values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lin_reg = LinearRegression()\nridge_reg = Ridge(random_state=42)\nlasso_reg = Lasso(random_state=42)\nelasticnet_reg = ElasticNet(random_state=42)\ntree_reg = DecisionTreeRegressor(random_state=42)\nforest_reg = RandomForestRegressor(random_state=42)\ngb_reg = GradientBoostingRegressor(random_state=42)\nxgb_reg = XGBRegressor(random_state=42)\nlgmb_reg = LGBMRegressor(random_state=42)\n\nregressors = [lin_reg, ridge_reg, lasso_reg, elasticnet_reg, tree_reg, forest_reg, gb_reg, xgb_reg, lgmb_reg]\n\nmodels_performance = []\n\nfor i in regressors:\n    start_time = datetime.now()\n    model_performance = {}\n    model_performance['model'] = type(i).__name__\n    scores = np.sqrt(-cross_val_score(i, X_train, y_train, cv=kfold, scoring='neg_mean_squared_error'))\n#     model_performance['scores'] = scores\n    model_performance['mean_score'] = round(scores.mean(), 4)\n    model_performance['standard_deviation'] = round(scores.std(), 4)\n    time_used = datetime.now() - start_time\n    model_performance['time_used'] = time_used.total_seconds()\n    models_performance.append(model_performance)\n\ndf_models_performance = pd.DataFrame(models_performance)\ndf_models_performance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation\n- Baseline Models 1: Random Forest Regressor (without any feature engineering) has the better performance with the lowest RMSE at 0.0936"},{"metadata":{},"cell_type":"markdown","source":"# Model Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data_base.drop('quality', axis=1).values\ny = np.log1p(data_base['quality']).values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\nforest_reg = RandomForestRegressor(random_state=42)\nforest_reg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predictions on test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = forest_reg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert back to integers\npredictions = pd.DataFrame(list(zip(np.expm1(y_test), np.round(np.expm1(y_pred)))), columns=['y_test', 'y_pred'])\npredictions[:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusions & Future work\n- The results of RMSE are not really satisfying, the problem may be due to wrong linearity assumption, such that regression is the not the best wasy to solve this problem\n- Future works can be: \n    - Hyperparameters Tunning with Grid Search CV/ Randomized Search CV\n    - Ensemble modeling\n    - Multi-class classification"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}