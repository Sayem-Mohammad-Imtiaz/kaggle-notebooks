{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-26T04:56:57.642291Z","iopub.execute_input":"2021-05-26T04:56:57.642708Z","iopub.status.idle":"2021-05-26T04:56:58.542215Z","shell.execute_reply.started":"2021-05-26T04:56:57.642616Z","shell.execute_reply":"2021-05-26T04:56:58.54108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# I. Load dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/traffic-flow-data-in-ho-chi-minh-city-viet-nam/train.csv\", index_col=\"_id\", parse_dates=[\"date\"])\n\nprint(df.head())\nprint(df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T04:56:58.543828Z","iopub.execute_input":"2021-05-26T04:56:58.544191Z","iopub.status.idle":"2021-05-26T04:56:58.758489Z","shell.execute_reply.started":"2021-05-26T04:56:58.544159Z","shell.execute_reply":"2021-05-26T04:56:58.757463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Choose concerning cols\ncols = [\"segment_id\", \"street_id\", \"street_name\", \"date\", \"weekday\", \n        \"length\", \"max_velocity\", \"street_level\", \"street_type\", \n        \"long_snode\", \"lat_snode\", \"long_enode\", \"lat_enode\", \"period\", \"LOS\"]\ndf = df[cols]","metadata":{"execution":{"iopub.status.busy":"2021-05-26T04:56:58.760656Z","iopub.execute_input":"2021-05-26T04:56:58.761122Z","iopub.status.idle":"2021-05-26T04:56:58.771676Z","shell.execute_reply.started":"2021-05-26T04:56:58.761075Z","shell.execute_reply":"2021-05-26T04:56:58.770472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.head())","metadata":{"execution":{"iopub.status.busy":"2021-05-26T04:56:58.773255Z","iopub.execute_input":"2021-05-26T04:56:58.773598Z","iopub.status.idle":"2021-05-26T04:56:58.788558Z","shell.execute_reply.started":"2021-05-26T04:56:58.773567Z","shell.execute_reply":"2021-05-26T04:56:58.787319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Enrichment","metadata":{}},{"cell_type":"code","source":"import datetime\n\n# 6h-8h, 16h-19h\npeaks = [\"period_6_00\", \"period_6_30\", \n         \"period_7_00\", \"period_7_30\",\n         \"period_16_00\", \"period_16_30\", \n         \"period_17_00\", \"period_17_30\",\n         \"period_18_00\", \"period_18_30\"]\n\ndef is_special(date):\n    # holidays = [(day, month)]\n    holidays = [(1,1), (14,2), (8,3), (30,4), \n                (1,5), (1,6), (2,9), (20,10), \n                (20,11), (24,12), (25,12)]\n    for holiday in holidays:\n        if date.day == holiday[0] and\\\n           date.month == holiday[1]:\n            return True\n    return False","metadata":{"execution":{"iopub.status.busy":"2021-05-26T04:56:58.790145Z","iopub.execute_input":"2021-05-26T04:56:58.790457Z","iopub.status.idle":"2021-05-26T04:56:58.801169Z","shell.execute_reply.started":"2021-05-26T04:56:58.790426Z","shell.execute_reply":"2021-05-26T04:56:58.800062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"is_weekend\"] = df[\"weekday\"].apply(lambda x: int(x in [5, 6]))\ndf[\"is_peak\"] = df[\"period\"].apply(lambda p: int(p in peaks))\ndf[\"special_day\"] = df[\"date\"].apply(lambda date: int(is_special(date)))\nprint(df.head())","metadata":{"execution":{"iopub.status.busy":"2021-05-26T04:56:58.802102Z","iopub.execute_input":"2021-05-26T04:56:58.802405Z","iopub.status.idle":"2021-05-26T04:56:59.217597Z","shell.execute_reply.started":"2021-05-26T04:56:58.802376Z","shell.execute_reply":"2021-05-26T04:56:59.216457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# II. Simple EDA\n\n* LOS is the target we want to classify\n* Mostly data is categorical, except: velocity, max_velocity, long/lat of 2 nodes","metadata":{}},{"cell_type":"markdown","source":"## 1. Missing values","metadata":{}},{"cell_type":"code","source":"missing_df = pd.DataFrame((df.isna().sum() / df.shape[0]), columns=[\"missing_ratio\"]).sort_values(\"missing_ratio\", ascending=False)\nprint(missing_df)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T04:56:59.218998Z","iopub.execute_input":"2021-05-26T04:56:59.219281Z","iopub.status.idle":"2021-05-26T04:56:59.25788Z","shell.execute_reply.started":"2021-05-26T04:56:59.219253Z","shell.execute_reply":"2021-05-26T04:56:59.256887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A lot missing values in max_velocity, other columns are good.","metadata":{}},{"cell_type":"markdown","source":"## 2. Categorical columns","metadata":{}},{"cell_type":"code","source":"def plot_cat_cols_with_target(data, cols, target):\n    for col in cols:\n        pd.crosstab(data[col], data[target]).plot.bar(figsize=(20, 10), fontsize=18)\n    plt.show()\n        \ncat_cols = [\"weekday\", \"street_level\", \"street_type\",\n            \"period\", \"is_weekend\", \"is_peak\", \"special_day\"]\nplot_cat_cols_with_target(df, cat_cols, \"LOS\")","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:14:50.152628Z","iopub.execute_input":"2021-05-26T05:14:50.153036Z","iopub.status.idle":"2021-05-26T05:14:53.428256Z","shell.execute_reply.started":"2021-05-26T05:14:50.15299Z","shell.execute_reply":"2021-05-26T05:14:53.427317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The patterns show some similar distributions of LOS labels","metadata":{}},{"cell_type":"markdown","source":"## 2. Numerical columns","metadata":{}},{"cell_type":"code","source":"def plot_num_cols_with_target(data, cols, target):\n    for i, col in enumerate(cols):\n        plt.figure(figsize=(16, 8))\n        sns.violinplot(x=target, y=col, data=data)\n        plt.show()\n\n# Ignore 'max_velocity'\nnum_cols = [\"length\", \"long_snode\", \"lat_snode\", \"long_enode\", \"lat_enode\"]\nplot_num_cols_with_target(df, num_cols, \"LOS\")","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:02:00.177613Z","iopub.execute_input":"2021-05-26T05:02:00.177918Z","iopub.status.idle":"2021-05-26T05:02:02.392149Z","shell.execute_reply.started":"2021-05-26T05:02:00.177884Z","shell.execute_reply":"2021-05-26T05:02:02.391135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. Outliers!!!\n2. Worth noting that plots for **long_snode & long_enode** look familiar to each other, as well as **lat_snode & lat_enode**","metadata":{}},{"cell_type":"markdown","source":"## 4. Label","metadata":{}},{"cell_type":"code","source":"def check_label(target_cols):\n    target_cols.value_counts().plot.bar(figsize=(12, 8))\n    plt.show()\ncheck_label(df[\"LOS\"])","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:02:02.393351Z","iopub.execute_input":"2021-05-26T05:02:02.39365Z","iopub.status.idle":"2021-05-26T05:02:02.565675Z","shell.execute_reply.started":"2021-05-26T05:02:02.393619Z","shell.execute_reply":"2021-05-26T05:02:02.564193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dataset is imbalanced which is not good for classification.","metadata":{}},{"cell_type":"markdown","source":"## 5. Relationship","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nnum_cols = [\"length\", \"max_velocity\", \"long_snode\", \"lat_snode\", \"long_enode\", \"lat_enode\"]\ncat_cols = [\"street_id\", \"segment_id\", \"weekday\", \"street_level\", \"street_type\", \"period\", \"is_weekend\", \"is_peak\",\"special_day\"]\n\ndef plot_heatmap(data):\n    cols = [\"LOS\"] + num_cols + cat_cols\n    temp_df = data[cols].copy()\n    \n    encoder = LabelEncoder()\n    for col in cat_cols + [\"LOS\"]:\n        temp_df[col] = encoder.fit_transform(temp_df[col])\n        \n    corrmat = temp_df[cols].corr()\n    plt.figure(figsize=(12, 9))\n    sns.heatmap(corrmat, cbar=True, annot=True, square=True, \n                fmt='.2f', annot_kws={'size': 10}, yticklabels=cols, xticklabels=cols)\n    plt.show()\n\nplot_heatmap(df)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:23:38.681587Z","iopub.execute_input":"2021-05-26T05:23:38.682154Z","iopub.status.idle":"2021-05-26T05:23:40.09201Z","shell.execute_reply.started":"2021-05-26T05:23:38.682118Z","shell.execute_reply":"2021-05-26T05:23:40.091281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some features may affect classification of LOS:\n\n* All columns seems not to be related to LOS, maybe these features aren't enough for classification.\n* Some cells show that long_snode ~ long_enode, lat_snode ~ lat_enode with full correlation. Easy to understand because segments connected by nodes so that a start node of a segment can also be an end node of other. Therefore, remove a pair of long/lat of end node in each sample before train to prevent overestimating these features.\n* street_level somewhat relates to street_type, fair enough!\n\nThe dataset is pretty dark: outliers, unrelated features, severe imbalance; but diamond cuts diamond, I will try to mine it.","metadata":{}},{"cell_type":"markdown","source":"# III. Implement metric for evaluation multiclass classification","metadata":{}},{"cell_type":"markdown","source":"## Plot ROC curves for multiclass classification by computing macro-average ROC curve & ROC area","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nfrom itertools import cycle\n\ndef classification_report_df(y_true, y_pred):\n    classes = np.unique(y_true)\n    true = label_binarize(y_true, classes=classes)\n    pred = label_binarize(y_pred, classes=classes)\n    \n    fpr, tpr, roc_auc = dict(), dict(), dict()\n    for i, c in enumerate(classes):\n        fpr[c], tpr[c], _ = roc_curve(true[:, i], pred[:, i])\n        roc_auc[c] = auc(fpr[c], tpr[c])\n        \n    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(true.ravel(), pred.ravel())\n    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n    return fpr, tpr, roc_auc\n\ndef plot_multiclass_roc(y_true, y_pred, title=\"Extension ROC to multi-class\"):\n    fpr, tpr, roc_auc = classification_report_df(y_true, y_pred)\n    classes = fpr.keys()\n    all_fpr = np.unique(np.concatenate([fpr[c] for c in classes]))\n    mean_tpr = np.zeros_like(all_fpr)\n    for c in classes:\n        mean_tpr += np.interp(all_fpr, fpr[c], tpr[c])\n    mean_tpr /= len(classes)\n    \n    fpr[\"macro\"] = all_fpr\n    tpr[\"macro\"] = mean_tpr\n    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n    \n    plt.figure(figsize=(12, 8))\n    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n             label=f'micro-average ROC curve (area = {roc_auc[\"micro\"]:0.2f})',\n             color='deeppink', linestyle=':', linewidth=4)\n    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n             label=f'macro-average ROC curve (area = {roc_auc[\"macro\"]:0.2f})',\n             color='navy', linestyle=':', linewidth=4)\n    \n    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n    lw = 2\n    for c, color in zip(classes, colors):\n        plt.plot(fpr[c], tpr[c], color=color, lw=lw,\n                 label=f'ROC curve of class {c} (area = {roc_auc[c]:0.2f})')\n    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(title)\n    plt.legend(loc=\"lower right\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:02:03.584326Z","iopub.execute_input":"2021-05-26T05:02:03.584739Z","iopub.status.idle":"2021-05-26T05:02:03.657071Z","shell.execute_reply.started":"2021-05-26T05:02:03.584695Z","shell.execute_reply":"2021-05-26T05:02:03.656071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# IV. Try to mining data in desperation","metadata":{}},{"cell_type":"markdown","source":"## Baseline classification model","metadata":{}},{"cell_type":"code","source":"from sklearn.compose import make_column_transformer, make_column_selector\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, RobustScaler\nfrom sklearn.impute import SimpleImputer\n\nnum_features = make_column_selector(dtype_exclude=object)\ncat_features = make_column_selector(dtype_include=object)\n\nnum_pipeline = Pipeline([('numerical_imputer', SimpleImputer(strategy=\"median\")),\n                         ('numerical_scaler', RobustScaler())])\ncat_pipeline = Pipeline([('categorical_imputer', SimpleImputer(strategy=\"most_frequent\")),\n                         ('categorical_encoder', OneHotEncoder(handle_unknown=\"ignore\"))])\n\npreprocessor = make_column_transformer((num_pipeline, num_features), (cat_pipeline, cat_features))\n\n# Choose model\nfrom sklearn.tree import DecisionTreeClassifier\n\nmodel = Pipeline([(\"preprocessor\", preprocessor),\n                  (\"classifier\", DecisionTreeClassifier(random_state=0))])","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:02:03.658766Z","iopub.execute_input":"2021-05-26T05:02:03.65908Z","iopub.status.idle":"2021-05-26T05:02:03.901767Z","shell.execute_reply.started":"2021-05-26T05:02:03.659049Z","shell.execute_reply":"2021-05-26T05:02:03.900943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, learning_curve\nfrom sklearn.metrics import classification_report, plot_confusion_matrix\n\ndef train_and_evaluate_model(X, y, model):\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_val)\n    \n    plot_confusion_matrix(model, X_val, y_val)\n    print(classification_report(y_val, y_pred))\n    plot_multiclass_roc(y_val, y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:02:03.90291Z","iopub.execute_input":"2021-05-26T05:02:03.903522Z","iopub.status.idle":"2021-05-26T05:02:03.911701Z","shell.execute_reply.started":"2021-05-26T05:02:03.903476Z","shell.execute_reply":"2021-05-26T05:02:03.91049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = [\"weekday\", \"length\", \"street_level\", \"street_type\", \"long_snode\", \"lat_snode\", \"period\"]\n\ntrain_and_evaluate_model(df[features], df[\"LOS\"], model)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:02:03.914297Z","iopub.execute_input":"2021-05-26T05:02:03.914846Z","iopub.status.idle":"2021-05-26T05:02:05.992494Z","shell.execute_reply.started":"2021-05-26T05:02:03.914795Z","shell.execute_reply":"2021-05-26T05:02:05.99124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model classifies A and B pretty well, others are not good since the dataset as shown to be imbalance towards 'A', or maybe data is not well distinguishable among class C, D, E, F.","metadata":{}},{"cell_type":"markdown","source":"### Why care about Imbalanced Classification? ([this blog](https://machinelearningmastery.com/what-is-imbalanced-classification/) for details and further reading)\n* Most ML algorithms for classification were designed around the assumption of an equal number of examples for each class; therefore imbalanced model will prone to majority class, which is bad for generalization.\n* In real world, we're mostly interested in minority class so it's useless if a model shows poor performance on minor population.\n\n### 3 advises when addressing imbalance problem:\n1. Use other classification metrics rather than just 'accuracy': 'precision', 'recall', 'F1-score', ie.\n2. Preprocess the raw data before feeding it into model: data augmentation, ie.\n3. Use variant of models/ML algorithms that treat classification errors differently.","metadata":{}},{"cell_type":"markdown","source":"## SMOTE\n\nAuthor suggested: one approach to addressing imbalanced datasets is to oversample the minority class. Simplest approach involves duplicating samples in the minority class which leaves out generating unnecessary information.\n\nSynthetic Minor Oversampling Technique(SMOTE): select examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line.\n\n[SMOTE for Imbalanced Classification](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/)","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE()\nX, y = preprocessor.fit_transform(df[features]), df[\"LOS\"]\nprint(\"Before:\", X.shape, y.shape)\nX, y = smote.fit_resample(X, y)\nprint(\"After:\", X.shape, y.shape)\ny.value_counts().plot.bar(figsize=(12, 8))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:02:05.993761Z","iopub.execute_input":"2021-05-26T05:02:05.994054Z","iopub.status.idle":"2021-05-26T05:02:10.642031Z","shell.execute_reply.started":"2021-05-26T05:02:05.994023Z","shell.execute_reply":"2021-05-26T05:02:10.640936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is how SMOTE balances the distribution of classes. Now let's feed it into model.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\n\ndef preprocess_dataset(X, y, preprocessor, resampler, test_size=0.2):\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=0)\n    X_train = preprocessor.fit_transform(X_train)\n    X_val = preprocessor.transform(X_val)\n    X_train, y_train = resampler.fit_resample(X_train, y_train)\n    return X_train, X_val, y_train, y_val\n\ndef train_and_validate(X_train, X_val, y_train, y_val, model, plot_title=\"Extension ROC to multi-class\"):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_val)\n    print(classification_report(y_val, y_pred))\n    plot_confusion_matrix(model, X_val, y_val)\n    plot_multiclass_roc(y_val, y_pred, plot_title)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:02:10.643802Z","iopub.execute_input":"2021-05-26T05:02:10.644135Z","iopub.status.idle":"2021-05-26T05:02:10.652765Z","shell.execute_reply.started":"2021-05-26T05:02:10.644104Z","shell.execute_reply":"2021-05-26T05:02:10.651676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Normal SMOTE","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Use the previous defined preprocessor\nX_train, X_val, y_train, y_val = preprocess_dataset(df[features], df[\"LOS\"], preprocessor, SMOTE())\nmodel = DecisionTreeClassifier(random_state=0)\n\ntrain_and_validate(X_train, X_val, y_train, y_val, model, \"Decision Tree with normal SMOTE\")","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:02:10.654279Z","iopub.execute_input":"2021-05-26T05:02:10.654636Z","iopub.status.idle":"2021-05-26T05:02:17.814458Z","shell.execute_reply.started":"2021-05-26T05:02:10.654603Z","shell.execute_reply":"2021-05-26T05:02:17.81349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Weighting-SMOTE: Oversampling & Undersampling","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline as ImbPipeline\n\nresampler = ImbPipeline(steps=[('o', SMOTE(sampling_strategy={\"B\":5000, \"C\":5000, \"D\":5000, \"E\":5000, \"F\":5000})),\n                               ('u', RandomUnderSampler(sampling_strategy={\"A\":8000}))])\n# Use the previous defined preprocessor\nX_train, X_val, y_train, y_val = preprocess_dataset(df[features], df[\"LOS\"], preprocessor, resampler)\nmodel = DecisionTreeClassifier(random_state=0)\n\ntrain_and_validate(X_train, X_val, y_train, y_val, model, \"Decision Tree with Oversampling & Undersampling SMOTE\")","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:02:17.815639Z","iopub.execute_input":"2021-05-26T05:02:17.815914Z","iopub.status.idle":"2021-05-26T05:02:23.089501Z","shell.execute_reply.started":"2021-05-26T05:02:17.815886Z","shell.execute_reply":"2021-05-26T05:02:23.088191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Adaptive Synthetic Sampling (ADASYN)","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import ADASYN\n\n# Use the previous defined preprocessor\nX_train, X_val, y_train, y_val = preprocess_dataset(df[features], df[\"LOS\"], preprocessor, ADASYN())\nmodel = DecisionTreeClassifier(random_state=0)\n\ntrain_and_validate(X_train, X_val, y_train, y_val, model, \"Decision Tree with ADASYN\")","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:02:23.090828Z","iopub.execute_input":"2021-05-26T05:02:23.091206Z","iopub.status.idle":"2021-05-26T05:02:53.123613Z","shell.execute_reply.started":"2021-05-26T05:02:23.091172Z","shell.execute_reply":"2021-05-26T05:02:53.122116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SMOTE Tomek Links","metadata":{}},{"cell_type":"code","source":"from imblearn.combine import SMOTETomek\nfrom imblearn.under_sampling import TomekLinks\n\n# Resampler: SMOTE with Tomek Links\nresample = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'))\n\nX_train, X_val, y_train, y_val = preprocess_dataset(df[features], df[\"LOS\"], preprocessor, resampler)\nmodel = DecisionTreeClassifier(random_state=0)\n\ntrain_and_validate(X_train, X_val, y_train, y_val, model, \"Decision Tree with SMOTE Tomek Links\")","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:02:53.12486Z","iopub.execute_input":"2021-05-26T05:02:53.125166Z","iopub.status.idle":"2021-05-26T05:02:58.426748Z","shell.execute_reply.started":"2021-05-26T05:02:53.125136Z","shell.execute_reply":"2021-05-26T05:02:58.425655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from imblearn.combine import SMOTETomek\nfrom imblearn.under_sampling import TomekLinks\n\n# Resampler: SMOTE with Tomek Links\nresample = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'))\n\nX_train, X_val, y_train, y_val = preprocess_dataset(df[features], df[\"LOS\"], preprocessor, resampler)\nmodel = DecisionTreeClassifier(random_state=0)\n\ntrain_and_validate(X_train, X_val, y_train, y_val, model)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:02:58.42807Z","iopub.execute_input":"2021-05-26T05:02:58.428332Z","iopub.status.idle":"2021-05-26T05:03:03.80831Z","shell.execute_reply.started":"2021-05-26T05:02:58.428305Z","shell.execute_reply":"2021-05-26T05:03:03.807128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cost-sensitive random forest classifier\n\n[Multi-class Imbalanced Classification](https://machinelearningmastery.com/multi-class-imbalanced-classification/)\n\n[Cost-Sensitive Learning](https://machinelearningmastery.com/cost-sensitive-learning-for-imbalanced-classification/)","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nweights = {'F': 1.5, 'A': 0.8, 'B': 1.5, 'C': 1.5, 'D': 1.5, 'E': 1.5}\nforest = RandomForestClassifier(n_estimators=50, class_weight=weights)\nmodel = Pipeline([('preprocessor', preprocessor), ('classifier', forest)])\n\nX_train, X_val, y_train, y_val = train_test_split(df[features], df['LOS'], test_size=0.2, random_state=26)\n\ntrain_and_validate(X_train, X_val, y_train, y_val, model)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:03:03.88975Z","iopub.execute_input":"2021-05-26T05:03:03.890074Z","iopub.status.idle":"2021-05-26T05:03:17.938278Z","shell.execute_reply.started":"2021-05-26T05:03:03.890043Z","shell.execute_reply":"2021-05-26T05:03:17.936544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest with SMOTE Tomek Links","metadata":{}},{"cell_type":"code","source":"from imblearn.combine import SMOTETomek\nfrom imblearn.under_sampling import TomekLinks\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Resampler: SMOTE with Tomek Links\nresample = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'))\n# Model\nmodel = RandomForestClassifier(n_estimators=100)\n\nX_train, X_val, y_train, y_val = preprocess_dataset(df[features], df[\"LOS\"], preprocessor, resampler)\ntrain_and_validate(X_train, X_val, y_train, y_val, model, \"Random Forest with SMOTE Tomek Links\")","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:03:17.939925Z","iopub.execute_input":"2021-05-26T05:03:17.94035Z","iopub.status.idle":"2021-05-26T05:04:03.706844Z","shell.execute_reply.started":"2021-05-26T05:03:17.940307Z","shell.execute_reply":"2021-05-26T05:04:03.705677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# V. Check model","metadata":{}},{"cell_type":"markdown","source":"Create a list of models to test","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\n\ndef get_models():\n    models, names = [], []\n    # LDA\n    models.append(LinearDiscriminantAnalysis())\n    names.append('LDA')\n    # OAA SVC\n    # One-Against-All\n    models.append(OneVsRestClassifier(SVC()))\n    names.append('OAA-SVC')\n    # KNN\n    models.append(KNeighborsClassifier(n_neighbors=3))\n    names.append('KNN')\n    return zip(models, names)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:04:03.708389Z","iopub.execute_input":"2021-05-26T05:04:03.709014Z","iopub.status.idle":"2021-05-26T05:04:03.743832Z","shell.execute_reply.started":"2021-05-26T05:04:03.708969Z","shell.execute_reply":"2021-05-26T05:04:03.742637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = preprocess_dataset(df[features], df[\"LOS\"], preprocessor, resampler)\nfor model, name in get_models():\n    train_and_validate(X_train.toarray(), X_val.toarray(), y_train, y_val, model, name)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T05:04:03.745735Z","iopub.execute_input":"2021-05-26T05:04:03.74614Z","iopub.status.idle":"2021-05-26T05:14:50.145308Z","shell.execute_reply.started":"2021-05-26T05:04:03.7461Z","shell.execute_reply":"2021-05-26T05:14:50.143578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# VI. Reference for improvement\n\nhttps://www.reddit.com/r/MachineLearning/comments/12evgi/classification_when_80_of_my_training_set_is_of/\n\nhttps://www.tensorflow.org/tutorials/structured_data/imbalanced_data\n\n[Weka's CostSensitiveClassifier](https://www.youtube.com/watch?v=LbZ9ROR1tQ0)\n\n[Sample Imbalanced Multiclass Classification](https://machinelearningmastery.com/imbalanced-multiclass-classification-with-the-e-coli-dataset/)","metadata":{}}]}