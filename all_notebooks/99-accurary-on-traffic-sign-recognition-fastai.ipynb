{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 99% accurary on Traffic Sign Recognition using Fastai\n\n\n## Objective\n\nUse fastai and a pretrained model (resnet34) to classify traffic sign from the GTSRB dataset.\nWith a bit more code such a model can be used for automatic sign detection in a video feed.\n\nSOME TEXT TO WRITE HERE.\n\n<br>\n<strong style=\"color:red;\">Please Upvote my kernel and keep it in your favourite section if you think it is helpful.</strong>\n\n\n<div style=\"color:white;background-color:#337ab7;padding:15px;width:100%;margin-top:50px;\">Table of contents</div>\n\n\n* [**Introduction**](#introduction)\n  1. [What is Fastai?](#fastai)\n  1. [About the dataset](#gtsrb-dataset)\n\n\n* [**Library**](#library)\n  1. [Fastai installation](#installation)\n  1. [Import Libraries](#librairies)\n\n\n* [**Load and inspect your data**](#load-inspect-data)\n  1. [Convert numerical labels to human readable labels](#convert-labels)\n  1. [Load training data](#training-data)\n  1. [Inspect target class distribution](#dataset-distribution)\n  1. [Visualize target class](#target-class)\n  1. [Visualize real images for each class](#visualize-images-per-class)\n\n\n* [**Train a learner (model)**](#train-learner)\n  1. [Train the model](#train)\n  1. [Save the trained model](#save-model)\n  1. [Interpret the results](#interpretation)\n  1. [The most confused](#confusion-matrix)\n  1. [Can a human do better on wrong predictions?](#can-human-do-better)\n  1. [Predict using trained model](#test-prediction)\n\n\n* [**Predict unseen data**](#predict-unseen-data)\n  1. [Load test data](#test-data)\n  1. [Get predictions for test data](#predict-test-data)\n  1. [Test data accuracy](#test-data-accuracy)\n  1. [Is 99% accuracy enough for real world usage?](#real-world-usage)"},{"metadata":{},"cell_type":"markdown","source":"# Introduction <a name=\"introduction\"></a>\n\n### What is Fastai? <a name=\"fastai\"></a>\n\n\n> fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance.\n>\n> -- <cite>[About Fastai](https://docs.fast.ai/#About-fastai)</cite>\n\nBasically it's a library that allows both beginner and advanced deep learning practitioners to produce the best result possible with very few lines of codes.\n\n\n### About the dataset <a name=\"gtsrb-dataset\"></a>\n\nThe German Traffic Sign Benchmark (GTSRB) is a multi-class single-image classification dataset that was use during a challenge held at the International Joint Conference on Neural Networks (IJCNN) in 2011.  \nThe dataset has 43 classes and a little more than 50,000 images.\n\nPlease note that we are using the updated version of the dataset.  \nThe 2011 version of the data has some flaws which are fixed in the final version.\n\nThe original dataset is on the [INI Benchmark Website](http://benchmark.ini.rub.de/)\n\n\n# Library <a name=\"library\"></a>\n\n### Fastai installation <a name=\"installation\"></a>\n\nSince fastai is already installed in kaggle notebooks, you don't need to install it again.  \nHowever, if it's missing in your notebook, you can run the below command to intall it.\n\n```\n!pip install fastai\n```\n\n### Import Libraries <a name=\"librairies\"></a>\n\nIn normal python code `import *` is not recommended. But in fastai things are so optimized that it allows you to import everything you need to tackle your problem without slowing things down."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from fastai.vision.all import *\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load and inspect your data <a name=\"load-inspect-data\"></a>\n\n### Convert numerical labels to human readable labels <a name=\"convert-labels\"></a>\n\nDue to the fact that the categories (classes) are numbers, it can be quite hard to relate to them. Let's convert them.\n\nBut before we will load the dataframe holding our data."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/gtsrb-german-traffic-sign/Train.csv')\n# display a sneak peek of the data\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Number of classes: {train_df.ClassId.unique().shape[0]}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The column `ClassId` holds the target class for each sample. As you see it's an integer column. Thanks to [Mykola's notebook](https://www.kaggle.com/meowmeowmeowmeowmeow/road-signs-recognition) we can convert this to readable labels to make it easier to interpret the results of our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = ['20 km/h', '30 km/h', '50 km/h', '60 km/h', '70 km/h', '80 km/h', '80 km/h end', '100 km/h', '120 km/h', 'No overtaking',\n               'No overtaking for trucks', 'Crossroad with secondary way', 'Main road', 'Give way', 'Stop', 'Road up', 'Road up for truck', 'No entry',\n               'Other dangerous', 'Turn left', 'Turn right', 'Winding road', 'Hollow road', 'Slippery road', 'Narrowing road', 'Roadwork', 'Traffic light',\n               'Pedestrian', 'Children', 'Bike', 'Snow', 'Deer', 'End of the limits', 'Only right', 'Only left', 'Only straight', 'Only straight and right', \n               'Only straight and left', 'Take right', 'Take left', 'Circle crossroad', 'End of overtaking limit', 'End of overtaking limit for truck']\n# add column with readable labels\ntrain_df['Label'] = train_df['ClassId'].replace(sorted(train_df['ClassId'].unique()), labels)\n# print updated df\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"### Load training data <a name=\"training-data\"><a/>\n\nWe need to use part of our training for validation purposes. Fastai already handles that by keeping aside 20% of the data as validation dataset. \nSince the images are not the same size we'll transform them to have the size 224x224 using `item_tfms`."},{"metadata":{"trusted":true},"cell_type":"code","source":"dls = ImageDataLoaders.from_df(train_df, fn_col='Path', label_col='Label', path='../input/gtsrb-german-traffic-sign/', seed=42, item_tfms=Resize(224))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's inspect our training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"dls.train.show_batch(max_n=7, nrows=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also need to inspect the validation set.\n\n**N.B.**: One thing that is noticeable is that some images in the data are quite dark. My guess is that the model might struggle with these one. In the real world the lights from the car can help make the signs more visible at the night."},{"metadata":{"trusted":true},"cell_type":"code","source":"dls.valid.show_batch(max_n=7, nrows=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Inspect target class distribution <a name=\"dataset-distribution\"></a>\n\nThis is simply a way to give us an idea of how many samples each class has in the training data. This helps answer the question: how much of each class our model will learn from?"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(25, 6))\nax.set_title('Training classes distribution')\nax.set_xlabel('Class')\nax.set_ylabel('Count')\n\nchart = sns.countplot(train_df.Label, ax=ax, orient=\"v\")\nax.set_xlabel('Labels');\nax.set_xticklabels(chart.get_xticklabels(), rotation=45, horizontalalignment='right');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, some classes are less represented than others. If many of these classes are part of the classes the model has a hard time detecting, it might means that we need more or better samples for each in order to improve our results. "},{"metadata":{"trusted":true},"cell_type":"code","source":"label_counts = train_df.Label.value_counts()\nless_represented = label_counts[label_counts < 300]\nless_represented","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualize target class <a name=\"target-class\"></a>\n\nTo better understand the target class it might help to see what the traffic sign should look like."},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_df = pd.read_csv('../input/gtsrb-german-traffic-sign/Meta.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_images_with_labels(images, labels, ncols=7):\n    plt.figure(figsize=(25,12))\n    plt.subplots_adjust(hspace=0.5)\n    nrows = len(images) / ncols + 1\n    for i, image in enumerate(images):\n        img_idx = i + 1\n        ax = plt.subplot(nrows, ncols, img_idx, title=labels[i], frame_on=False)\n        ax.imshow(image)\n        ax.axis(\"off\")\n\n# build the list of images and display them\nmeta_df['ImgPath'] = \"../input/gtsrb-german-traffic-sign/\" + meta_df[\"Path\"]\nimages = meta_df['ImgPath'].apply(mpimg.imread)\nimg_labels = meta_df['ClassId'].replace(sorted(meta_df['ClassId'].unique()), labels)\ndisplay_images_with_labels(images, img_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualize real images for each class <a name=\"visualize-images-per-class\"></a>\n\nNow that we have a visual representation of each class, less see how each of them look like in our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# build the list of images to show\ntrain_df['ImgPath'] = \"../input/gtsrb-german-traffic-sign/\" + train_df[\"Path\"]\nuniq_train_df = train_df.drop_duplicates(subset=['Label'])\nimages = uniq_train_df['ImgPath'].apply(mpimg.imread)\nimg_labels = uniq_train_df['Label'].values\n\ndisplay_images_with_labels(images, img_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Train a learner (model) <a name=\"train-learner\"><a/>\n\n\n### Train the model <a name=\"train\"></a>\n\nWe'll use **error_rate** and **accuracy** as metrics for our model. The error rate will help us measure the performance of our model and interpret its results. Whereas the accuracy will help us understand how good is our model at choosing the right answer."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = cnn_learner(dls, resnet34, metrics=[error_rate, accuracy], model_dir=Path(\"/kaggle/working/model\"))\nlearn.fine_tune(8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Save the trained model <a name=\"save-model\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.export('/kaggle/working/export.pkl')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Interpret the results <a name=\"interpretation\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"interp = ClassificationInterpretation.from_learner(learn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`plot_top_losses` lets us visualize the samples with the higher loss during training. Meaning that we get to see on which images the performance of the model was worst. Keep in mind that a higher loss doesn't equate to a wrong prediction.\n\nAs you can see below, the higher the loss, the lower quality of the image.\n\nSome of the images were enlarged to ensure size uniformity. Others are probably not centered in the **224x224** square that was cropped from the original image."},{"metadata":{"trusted":true},"cell_type":"code","source":"interp.plot_top_losses(15, nrows=3)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The most confused <a name=\"confusion-matrix\"></a>\n\nIn most cases a confusion matrix helps understand why some classes are being confused with others. But in this case, there is just too many classes to make sense out of it."},{"metadata":{"trusted":true},"cell_type":"code","source":"interp.plot_confusion_matrix(figsize=(25,12))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Luckily, **fastai** provides a helper function that will allow us to take a look at the categories that are the most confused.\n\nThe returned list is presented as: **actual / predicted / # of occurences**."},{"metadata":{"trusted":true},"cell_type":"code","source":"mc = interp.most_confused()\nmc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The classes that are the most confused are among the classes where the model had the worst performances.  \nWe can possibly use data augmentation technique to improve the results, provide better images or make sure we grab the right portion of the image for our training.\n\nContrary to what we thought at the beginning, none of the `less_represented` classes are part of the `most_confused` thanks to transfert learning. With transfert learning we can have very few number of sample and still achieve great results.  \nThis also means that it's not only about the quantity of the samples but quality of it.\n\n\n### Can a human do better on wrong predictions? <a name=\"can-human-do-better\"></a>\n\nGiven this dataset, a short answer is **No**.\n\nFor many of the images where the model had it wrong, it will be difficult for the naked eye to recognize the sign due to the darkness of the image.\n\n\n### Predict using trained model <a name=\"test-prediction\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the model\nlearn_inf = load_learner('/kaggle/working/export.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# helper func to predict and provide probability.\ndef predict(img_path, proba=False):\n    pred,pred_idx,probs = learn_inf.predict(img_path)\n    return [pred,pred_idx,probs] if proba else pred\n\n# test predict function\npred,pred_idx,probs = predict('../input/gtsrb-german-traffic-sign/Test/00000.png', proba=True)\n\n# Test prediction\nprint(f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict unseen data <a name=\"predict-unseen-data\"></a>\n\n### Load test data <a name=\"test-data\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load test data\ntest_df = pd.read_csv('../input/gtsrb-german-traffic-sign/Test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add full path for each image\ntest_df['ImgPath'] = '../input/gtsrb-german-traffic-sign/' + test_df.Path","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Get predictions for test data <a name=\"predict-test-data\"></a>\n\nThe `%%capture` magic method captures the output printed by `apply`. In this case, it's an empty new line after each prediction. We don't need that."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n# Add prediction for each image in the test set\ntest_df['Category'] = test_df.ImgPath.apply(predict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test data accuracy <a name=\"test-data-accuracy\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert numerical category to labels\ntest_df['Label'] = test_df['ClassId'].replace(sorted(test_df['ClassId'].unique()), labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = accuracy_score(test_df['Label'], test_df['Category'])\n# acc = 0.9884402216943785\nprint(f\"Test data accuracy = {acc:.2f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Is 99% accuracy enough for real world usage? <a name=\"real-world-usage\"></a>\n\nYes and no. Yes if we have an optimal environment. This means the sign needs to be well lit and not obstructed (by an object, heavy rain or snowstorm).\n\nAlthough the model has a nearly perfect score with very poor images, we might need further testing. Possibly hooking it to a camera mounted in a car and see how it performs in different scenarios such as:\n  * **Night time**\n  * **Heavy rain**\n  * **Snow storm**\n  * **Obstructed by an object**\n  * **The car moving at high speed**\n\nWe might also need to test at which distance does the sign needs to be from a moving car for the model to give an accurate prediction. This might help set a safety threshold where the prediction is not to be trusted unless the distance is below that threshold."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}