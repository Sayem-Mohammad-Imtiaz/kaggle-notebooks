{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Download missing libraries\n\nKaggle's notebooks has several libraries already installed, but more especific libraries need to be installed manually, the following block code help to install those required libraries.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install contractions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import required libraries\n\nKeep all the import library code on a single cell to keep the notebook's organization and additional dowloads on this section.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# # Standard library imports\nimport collections\nimport re\nimport warnings\n\n# Third-party imports\nimport contractions\nimport matplotlib.pyplot as plt\nfrom mlxtend.evaluate import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix\nimport nltk\nimport numpy as np\nimport pandas as pd\nimport sklearn as sk\nfrom tensorflow import keras\nimport unidecode","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Suppress matplotlib user warnings,\n# necessary for newer version of matplotlib\nwarnings.filterwarnings(\n    \"ignore\",\n    category=UserWarning,\n    module=\"matplotlib\"\n)\n\n# Allows producing visualizations in notebook\n%matplotlib inline\n\n# Download required NLTK files \nnltk.download(['averaged_perceptron_tagger', 'punkt', 'wordnet'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Function definitions\n\nAll the functions are declared here with corresponding docstrings and references (if apply).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def bins_labels(bins, **kwargs):\n    ''' Plot histogram helper function\n    \n    The code was extracted from Stack Overflow, answer by @Pietro Battiston:\n    https://stackoverflow.com/questions/23246125/how-to-center-labels-in-histogram-plot\n    \n    Parameters\n    ----------\n    bins : list from start to end by given steps\n        description -> The xticks to fit.\n        format -> range(start, end, step)\n        options -> No apply\n    '''\n    bin_w = (max(bins) - min(bins)) / (len(bins) - 1)\n    plt.xticks(np.arange(min(bins)+bin_w/2, max(bins), bin_w), bins, **kwargs)\n    plt.xlim(bins[0], bins[-1])\n    \ndef print_text(dataframe, header='text', step=250):\n    '''Function to print some tweet samples.\n    \n    The code prints on the given step a tweet with\n    its index and sentiment classification.\n    \n    This function is for better understanding the most common\n    tweet sintaxis and special characters. \n    \n    Parameters\n    ----------\n    dataframe : Pandas Dataframe\n        description -> The data text and labels\n        format -> headers: [\"text\": string, \"airline_sentiment\": string]\n        options -> \"text\": No apply\n                   \"airline_sentiment\": [\"positive\", \"neutral\", \"negative\"]\n\n    header : string\n        description -> The object column to print\n        format -> No apply\n        options -> No apply\n\n    step : int\n        description -> The index separation desired to print\n        format -> No apply\n        options -> [0, len(dataframe) - 1]\n    '''\n    for index in range(0, len(dataframe), step):\n        print(f'Tweet[{index}]: {dataframe[header][index]}')\n        print(f'Sentiment: {dataframe[\"airline_sentiment\"][index]}')\n        print(f'{\"_\"*70}\\n')\n\ndef text_cleaning(input_text):\n    ''' Function including all the text clean process.\n    \n    The code includes all the steps required to clean\n    the tweet texts on a final and common format.\n    \n    The output text is in lowercase.\n    \n    Parameters\n    ----------\n    input_text : string\n        description -> The input text to clean\n        format -> 'string'\n        options -> No apply\n    \n    Returns\n    -------\n    output_text : string\n        description -> The cleaned output text\n        format -> 'string'\n        options -> No apply\n    '''\n    input_text = re.sub(u'http\\S+|@\\S+|#', ' ', input_text)\n    input_text = re.sub(u'^(.{140}).*$', '\\g<1>', input_text)\n    input_text = contractions.fix(input_text)\n    input_text = unidecode.unidecode(input_text)\n    input_text = re.sub(u'[^a-zA-Z]', ' ', input_text)\n    input_text = input_text.lower()\n    output_text = ' '.join(input_text.split())\n\n    return output_text\n\ndef stem_sentence(input_text):\n    ''' Function to stem a given sentence.\n    \n    The NLTK library is used on this function.\n    \n    Parameters\n    ----------\n    input_text : string\n        description -> The input text to stem\n        format -> 'string'\n        options -> Only cleaned text in lowercase\n    \n    Returns\n    -------\n    output_text : string\n        description -> The stemmed text\n        format -> 'string'\n        options -> No apply\n    '''\n    stem = nltk.stem.LancasterStemmer()\n    \n    words = nltk.word_tokenize(input_text)\n    output_text = list(map(stem.stem, words))\n    output_text = ' '.join(output_text)\n    \n    return output_text\n\n# The following functions help to completly lemmatize a given sentence\ndef nltk_tag_to_wordnet_tag(nltk_tag):\n    ''' Function to assign a tag to the detected word.\n    \n    The NLTK library is used on this function.\n\n    Each word is analized and determines the word\n    type as noun, verb, adverb or adjective.\n    \n    The function is based on the following link resources:\n    https://medium.com/@gaurav5430/using-nltk-for-lemmatizing-sentences-c1bfff963258\n    https://simonhessner.de/lemmatize-whole-sentences-with-python-and-nltks-wordnetlemmatizer/\n\n    Parameters\n    ----------\n    nltk_tag : tuple\n        description -> The tuple with the tokenized word and its type tag\n        format -> nltk.pos_tag object\n        options -> Only cleaned text in lowercase\n    \n    Returns\n    -------\n    tag : nltk.corpus.wordnet\n        description -> The required tag before the lemmatization process.\n        format -> nltk.corpus.wordnet\n        options -> No apply\n    '''\n    if nltk_tag.startswith('J'):\n        tag = nltk.corpus.wordnet.ADJ\n    elif nltk_tag.startswith('V'):\n        tag = nltk.corpus.wordnet.VERB\n    elif nltk_tag.startswith('N'):\n        tag = nltk.corpus.wordnet.NOUN\n    elif nltk_tag.startswith('R'):\n        tag = nltk.corpus.wordnet.ADV\n    elif nltk_tag.startswith('S'):\n        tag = nltk.corpus.wordnet.ADJ_SAT\n    else:\n        tag = None\n\n    return tag\n\ndef adverb_to_base(word):\n    ''' Lemmatization process on adverbs\n    \n    The NLTK library is used on this function.\n\n    If the word tokenized tag indicates that the \n    word is an adverb, this function must be used\n    to lemmatize the adverb.\n\n    The function is based on the following link resources:\n    https://stackoverflow.com/questions/17245123/getting-adjective-from-an-adverb-in-nltk-or-other-nlp-library\n\n    Parameters\n    ----------\n    word : string\n        description -> The tuple with the tokenized word and its type tag\n        format -> nltk.pos_tag object\n        options -> Only cleaned text in lowercase\n    \n    Returns\n    -------\n    tag : nltk.corpus.wordnet corresponding tag value\n        description -> The required tag before the lemmatization process.\n        format -> nltk.corpus.wordnet\n        options -> No apply\n    '''\n    try:\n        synonym = nltk.corpus.wordnet.synsets(word)\n        synonym = [lemma for syn_set in synonym \\\n                   for lemma in syn_set.lemmas()]\n        synonym = [pertain.name() for lemma in synonym \\\n                   for pertain in lemma.pertainyms()]\n        base_word = difflib.get_close_matches(word, synonym)[0]\n    except:\n        base_word = word\n\n    return base_word\n    \ndef lemmatize_word(wordnet_tagged):\n    ''' Lemmatization process on a single word\n    \n    The NLTK library is used on this function.\n\n    A tagged and tokenized word is lemmatize.\n\n    Parameters\n    ----------\n    wordnet_tagged : nltk.corpus.wordnet object\n        description -> The word tag and corresponding word to lemmatize\n        format -> nltk.corpus.wordnet corresponding tag value\n        options -> no apply\n    \n    Returns\n    -------\n    base_word : string\n        description -> The word on their base form\n        format -> no apply\n        options -> no apply\n    '''\n    lemma = nltk.stem.WordNetLemmatizer()\n    if wordnet_tagged[1] == None:\n        base_word = wordnet_tagged[0]\n    elif wordnet_tagged[1] == 'r':\n        base_word = adverb_to_base(wordnet_tagged[0])\n    else:\n        base_word = lemma.lemmatize(\n            wordnet_tagged[0],\n            wordnet_tagged[1]\n        )\n    \n    return base_word\n\ndef lemmatize_sentence(input_text):\n    ''' Lemmatization process in a sentence\n    \n    The NLTK library is used on this function.\n\n    Parameters\n    ----------\n    input_text : string\n        description -> A complete sentence to lemmatize cleaned and in lower case\n        format -> no apply\n        options -> no apply\n    \n    Returns\n    -------\n    output_text : string\n        description -> Lemmatize sentence\n        format -> no apply\n        options -> no apply\n    '''\n    #Pass the tokenize sentence and find the POS tag for each token\n    words = nltk.word_tokenize(input_text)\n    nltk_tagged = nltk.pos_tag(words)\n\n    #tuple of (token, wordnet_tag)\n    wordnet_tagged = map(\n        lambda tag: (tag[0], nltk_tag_to_wordnet_tag(tag[1])),\n        nltk_tagged\n    )\n\n    # Lemmatize sentence by tag\n    output_text = list(map(lemmatize_word, wordnet_tagged))\n    output_text = ' '.join(output_text)\n\n    return output_text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Data analysis\n\nThis notebook focuses on sentiment analysis and natural language processing, so the only desired attribute on the dataset is the **text** column, however, a simple data analysis is performed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_df = pd.read_csv('../input/twitter-airline-sentiment/Tweets.csv')\ntweets_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The information above shows that **text** and **airline_sentiment** has no missing values, so all the data on this columns can be used.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_df.describe(include=np.number)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_df.describe(include=np.object)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for attribute in list((tweets_df.describe(include=np.object)).keys()):\n    print('')\n    print(f'Analyzing the attribute \"{attribute}\"')\n    print(tweets_df[attribute].value_counts())\n    print(f'Actual length of dataframe {len(tweets_df)}')\n    print('')\n    print('*'*50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.1. Data description\n\nThe output cells above help to get a better understanding on the attributes:\n\n* [00] **tweet_id**: Tweet comment's identification.\n\n\n* [01] **airline_sentiment**: The output value of sentiment detected on the dataset, 3 unique values; **negative**, **neutral** and **positive**, no missing values or invalid data, categorical.\n\n\n* [02] **airline_sentiment_confidence**: Confidence on the detected sentiment on the dataset, no missing values or inavalid data, numerical interval value from **0.0** to **1.0**.\n\n\n* [03] **negativereason**: Text explaining the negative reason on sentiment airline output. 5462 missing values. \n\n\n* [04] **negativereason_confidence**: Confidence on the explanation of negative sentiment output. 4118 missing values, numeric interval value from **0.0** to **1.0**.\n\n\n* [05] **airline**: Categorical value, the airline's name, no missing values, the 6 airline's names are **United**, **US Airways**, **American**, **Southwest**, **Delta** and **Virgin America**.\n\n\n* [06] **airline_sentiment_gold**: There are sentiment values too but the amount of data is small, 14600 missign values.\n\n\n* [07] **name**: Probbly, the Twiter's username, no missing values.\n\n\n* [08] **negativereason_gold**: 14608 missing values, reason for negative gold sentiment.\n\n\n* [09] **retweet_count**: Number of retweets from comments, numerical value, entire positive number. No missing values.\n\n\n* [10] **text**: No missing values, the text content Tweet. Only strings on this attribute and possible only input data.\n\n\n* [11] **tweet_coord**: 13621 missing values, a pair number containing the coordinates where the Tweet came from **[latitude, longitude]**\n\n\n* [12] **tweet_created**: Date Tweet creation. No missing values, format **YYYY-MM-DD hh:mm:ss -0800**\n\n\n* [13] **tweet_location**: Geographic name where the Tweet was created. 4733 missing values.\n\n\n* [14] **user_timezone**: 4828 missing values, timezone where the Tweet was created.\n\nThe next step is to keep only the attributes containing the tweet text and sentiment label, so it is needed to keep only the following attributes:\n\n* **airline_sentiment**\n* **text**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1.2. Data visualization\n\nAdditionaly, the data visualization helps to identify main patterns and features on the filtered data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# It is easier to keep the desired columns than drop the columns.\ntweets_df = tweets_df[['text', 'airline_sentiment']].copy()\n\nprint(f'Shape of filtered data, columns: {tweets_df.shape[1]}')\nprint(f'Shape of filtered data, rows: {tweets_df.shape[0]}')\n\n# Only to display all the text\npd.set_option('display.max_colwidth', None)\ntweets_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The histogram bellow shows the most common tweet characters length, (**140 characters**) and according to [this research](https://techcrunch.com/2018/10/30/twitters-doubling-of-character-count-from-140-to-280-had-little-impact-on-length-of-tweets/) the text on Twitter are bellow 140 characters, the change on Twitter about the character length to **280 characters** did not affect the most common lenght on the tweets.\n\nThe suggested approach to pre-process this data is to truncate the text to keep only 140 characters to improve the training and inference process, the most common length is between 125 and 145 and is not common to have more than that value on the text length. It is common to identify if a given text has a specific connotation only reading the first few words on a sentence.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the max text length on the dataset\nlength_texts = list(map(len, tweets_df[\"text\"]))\nprint(f'The longest tweet has {max(length_texts)} characters')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(16,5))\nax = fig.add_subplot(111)\nbins = range(10, 195, 5)\nplt.hist(length_texts, bins=bins, rwidth= 0.9)  # `density=False` would make counts\nlabel_y = plt.ylabel('Frequency')\nlabel_y.set_color('gray')\nlabel_y.set_size(12)\nlabel_x = plt.xlabel('Tweet characters')\nlabel_x.set_color('gray')\nlabel_x.set_size(12)\nplt.xticks(list(bins))\nbins_labels(bins, fontsize=10, color='gray')\nax.tick_params(axis='x', colors='gray')\nax.tick_params(axis='y', colors='gray')\nplt.axis()\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before to clean the text, seeing some tweets samples could help to identify some patterns, useless words or notes to clean in the tweets. The function **print_text** accomplishes the task.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print_text(tweets_df, step=2000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(16,5))\nax = fig.add_axes([0.1, 0.1, 0.8, 0.8])\nbins = range(0, 4, 1)\nplt.hist(sorted(list(tweets_df['airline_sentiment'])), bins=bins, rwidth= 0.9)\nplt.ylabel('Frequency')\nplt.xlabel('Sentiment')\nplt.xticks(list(bins))\nbins_labels(bins, fontsize=9)\nax.set_xticklabels(['negative','neutral','positive'])\ntitle = plt.title('Balance on dataset visualization')\ntitle.set_color('gray')\ntitle.set_size(16)\nlabel_y = plt.ylabel('Frequency')\nlabel_y.set_color('gray')\nlabel_y.set_size(12)\nlabel_x = plt.xlabel('Label')\nlabel_x.set_color('gray')\nlabel_x.set_size(14)\nax.tick_params(axis='x', colors='gray')\nax.tick_params(axis='y', colors='gray')\nplt.show();\n\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Additionally, the dataset is imabalanced, some balancing methods could help to get better performance on the future models to test.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2. Text preprocessing\n\nThe samples helped to understand better the tweets, and this information determines how to clean the data. The function **text_cleaning** performs all the listed cleaning tasks:\n\n* Substitute the HTTP links to space.\n* Substitute the *@* tags and following text next to it to space.\n* Substitute the *#* tags to space.\n* Truncate the text length to 140 characters.\n* Normalize all the words (Example: Change \"ñ\" to \"n\" or \"ü\" to \"u\").\n* Delete all the contractions (Example: Change \"I'm\" to \"I am\").\n* Delete all the numbers.\n* Delete all the special characters, punctuations and emoticons.\n* Pass all the text to lowercase.\n* Delete unnecessary spaces.\n\nUsing the **text_cleaning** function is possible to map the column dataframe called \"**text**\" and perform the cleaning using only one sentence code. For analysis purposes the original text is kept and the cleaned text is saved in a different column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_df.insert(\n    loc=1,\n    column='cleaned_text',\n    value=tweets_df['text'].map(text_cleaning)\n)\n\ntweets_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The **airline_sentiment** has three categories that describe the sentiment on the tweets, but those categories are represented as objects (strings), so the labels must be converted to numeric.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_df['airline_sentiment'].replace(\n    {'negative': 0,\n     'neutral': 1,\n     'positive': 2},\n    inplace=True\n)\n\ntweets_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataframe has the original tweet texts and their corresponding cleaned text, on this point some process are suggested as next steps for reducing different forms of a word to a core root, some words are derived from another and it can be mapped to a central word or symbol, specially if they have the same core meaning. On this notebook two of them will be performed to check which processes get the better performance:\n\n* **Stemming**:  The words are reduced to their word stems that it is equal to or smaller form of the word (Example: \"cooking\", \"cooked\" and \"cook\" are reduced to the same stem of \"cook\").\n\n* **Lemmatization**: Involves resolving words to their dictionary form. A lemma of a word is its dictionary or canonical form (Example: \"quickly\" is lemmatized to \"quick\").","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Stemming text\n\nThe NLTK library has classes and methods that allow to do the stemming process easily, the following code blocks used the function called **stem_sentece**, it performs the stemming process by sentence, so mapping the cleaned text column **cleaned_text** return all the sentences stemmed on the new column **stem_text**. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_df.insert(\n    loc=2,\n    column='stem_text',\n    value=tweets_df['cleaned_text'].map(stem_sentence)\n)\n\ntweets_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Lemmatize text\n\nThe NLTK library has classes and methods that allow to do the lemmatization process, the following code blocks used the function called **lemmatize_sentence**, it performs the lemmatization process by sentence, so mapping the cleaned text column **cleaned_text** return all the sentences lemmatized on the new column **lemma_text**.\n\nThe function **lemmatize_sentence** requires other subfunctions (*nltk_tag_to_wordnwt_tag*, *adverb_to_base* and *lemmatize_word*) to perform adverbs, verbs, nouns and adjectives to their base form.\n\nThe function codes are based on the following resources:\n\n* [**Using NLTK for lemmatizing sentences**](https://medium.com/@gaurav5430/using-nltk-for-lemmatizing-sentences-c1bfff963258)\n* [**Stack Overflow post about adverbs handling in NLTK**](https://stackoverflow.com/questions/17245123/getting-adjective-from-an-adverb-in-nltk-or-other-nlp-library)\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_df.insert(\n    loc=3,\n    column='lemma_text',\n    value=tweets_df['cleaned_text'].map(lemmatize_sentence)\n)\n\ntweets_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The three pre-processed texts are used on the process as inputs to identify which one has better performance:\n\n* **cleaned_text**\n* **stem_text**\n* **lemma_text**\n\nBefore the word tokenization and training, the dataset is split into training (*70%*), validation (*15%*) and testing (*15%*) datasets, the purpose to do this at this point is to avoid that any model sees data from testing or validation set.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 3. Type of text preprocesed selection","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"It is time to focus only in the training dataset, the following codes help to identify the word lengths distribution on the training data set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"text_train, text_test, label_train, label_test = sk.model_selection.train_test_split(\n    tweets_df[['cleaned_text', 'stem_text', 'lemma_text']].copy(),\n    tweets_df[['airline_sentiment']].copy(),\n    train_size=0.7,\n    random_state=42 # To allow reproducible results\n)\n\ntext_test, text_validation, label_test, label_validation = sk.model_selection.train_test_split(\n    text_test.copy(),\n    label_test.copy(),\n    train_size=0.5,\n    random_state=42 # To allow reproducible results\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get a list of the number of words per sentence on all the training dataset\nlength_words = [\n    item \\\n    for header in text_train.keys() \\\n    for item in list(map(lambda sentence: len(sentence.split()), text_train[header]))\n]\n\nprint(f'Max number of words per sentence: {max(length_words)}')\nprint(f'Most frequent number of words per sentence: {max(set(length_words), key = length_words.count)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the distribution of words on the sentences\nfig = plt.figure(figsize=(16,5))\nax = fig.add_subplot(111)\nbins = range(0, max(length_words) + 2, 1)\nplt.hist(length_words, bins=bins, rwidth= 0.9)\ntitle = plt.title('Training dataset')\ntitle.set_color('gray')\ntitle.set_size(16)\nlabel_y = plt.ylabel('Frequency')\nlabel_y.set_color('gray')\nlabel_y.set_size(12)\nlabel_x = plt.xlabel('Number of words in the tweet text')\nlabel_x.set_color('gray')\nlabel_x.set_size(12)\nplt.xticks(list(bins))\nbins_labels(bins, fontsize=10, color='gray')\nax.tick_params(axis='x', colors='gray')\nax.tick_params(axis='y', colors='gray')\nplt.axis()\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The graphic showed that the max number of words per sentence tends to be above 20 words, the max number is 33, so set the limit of words to 30 (the number of words was chosen arbitrarily, only to keep a rounded number), this value is required to truncate the sentences above that value or padding the sentences bellow that value with zeros.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"maxlen = 30","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before going to preparate the data to probe on the models, map all the words on all the type of texts preprocessed, a histogram shows the most common words on the datasets, besides analyzing the less common words and their frequency on the training data set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"words_list_per_column = [list(text_train[header]) for header in text_train.keys()]\nwords_list_per_column = [list(map(lambda sentence: sentence.split(), set_)) for set_ in words_list_per_column]\nwords_list_per_column = list(map(lambda set_: [word for words in set_ for word in words] , words_list_per_column))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(25,10))\n\nfor index, words in enumerate(words_list_per_column, 1):\n    vocabulary = collections.Counter(words)\n    vocabulary = dict(vocabulary.most_common(50))\n    labels, values = zip(*vocabulary.items())\n    indSort = np.argsort(values)[::-1]\n    labels = np.array(labels)[indSort]\n    values = np.array(values)[indSort]\n    indexes = np.arange(len(labels))\n    bar_width = 0.1\n\n    ax = plt.subplot(1, 3, index)\n    plt.barh(indexes, values, align='center')\n    plt.yticks(indexes + bar_width, labels)\n    title = plt.title(f'Training dataset \"{text_train.keys()[index - 1]}\"')\n    title.set_color('gray')\n    title.set_size(16)\n    label_y = plt.ylabel('Most common words')\n    label_y.set_color('gray')\n    label_y.set_size(12)\n    label_x = plt.xlabel('Frequency')\n    label_x.set_color('gray')\n    label_x.set_size(12)\n    ax.tick_params(axis='x', colors='gray')\n    ax.tick_params(axis='y', colors='gray')\n    ax.invert_yaxis()\n\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get a dictionary with first and last index appearance by frequency words\nmax_words = 0\nfor index, words in enumerate(words_list_per_column):\n    vocabulary = collections.Counter(words)\n    frequencies = [word_counter[1] for word_counter in vocabulary.most_common()]\n    unique_values = list(set(frequencies))\n    unique_values.sort()\n    index_by_value = {\n        value: [frequencies.index(value), len(frequencies) - frequencies[::-1].index(value) - 1] \\\n        for value in unique_values\n    }\n\n    print(f'First and last index by frequency words in \"{text_train.keys()[index]}\":\\n{dict(list(index_by_value.items())[0:10])}\\n')\n    max_words += index_by_value[1][0]\n\nmax_words = int(max_words/3)\nprint(f'Max number of words for embedding layer: {max_words}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The previous results indicates that the suggested **number of words per sentence could be 30 words** and the suggested **max number of words to use on the embedding layer is 3989**.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert labels dataframe to numpy array\nlabel_train = np.array(label_train['airline_sentiment'])\nlabel_validation = np.array(label_validation['airline_sentiment'])\nlabel_test = np.array(label_test['airline_sentiment'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To select a possible preprocessed text to use a simple convolutional network model is tested on each type of text, the final results are the mean validation accuracy on 10 epochs witch a batch of 128 samples.\n\n> Note: The code bellow make a tiny word tokenization only in the training set (the purpose is to keep the notebook as a realistic problem where the validation and testing set are not seen by the model before), on each *for* cycle a type of preprocessed text is tested.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"results = {}\nmaxlen = 20\nfor probe_num in range(3):\n    # Past to array a selected type text\n    header = text_train.keys()[probe_num]\n    text_type_train = np.array(text_train[header].copy())\n    text_type_validation = np.array(text_validation[header].copy())\n    text_type_test = np.array(text_test[header].copy())\n\n    # Word tokenization\n    tokenizer = keras.preprocessing.text.Tokenizer(num_words=1000, lower=False)\n    tokenizer.fit_on_texts(text_type_train)\n\n    text_type_train = tokenizer.texts_to_sequences(text_type_train)\n    text_type_validation = tokenizer.texts_to_sequences(text_type_validation)\n    text_type_test = tokenizer.texts_to_sequences(text_type_test)\n\n    # Adding 1 because of reserved 0 index\n    vocab_size = len(tokenizer.word_index) + 1\n\n    text_type_train = keras.preprocessing.sequence.pad_sequences(text_type_train, padding='post', maxlen=maxlen)\n    text_type_validation = keras.preprocessing.sequence.pad_sequences(text_type_validation, padding='post', maxlen=maxlen)\n    text_type_test = keras.preprocessing.sequence.pad_sequences(text_type_test, padding='post', maxlen=maxlen)\n\n    model = keras.models.Sequential()\n    model.add(keras.layers.Embedding(vocab_size, 50, input_length=maxlen))\n    model.add(keras.layers.Conv1D(64, 3, activation='relu'))\n    model.add(keras.layers.GlobalMaxPooling1D())\n    model.add(keras.layers.Dense(3, activation='softmax'))\n\n    model.compile(\n        loss='sparse_categorical_crossentropy',\n        optimizer=keras.optimizers.Adam(),\n        metrics=['accuracy']\n    )\n\n    history = model.fit(\n        text_type_train,\n        label_train,\n        epochs=10,\n        batch_size=128,\n        use_multiprocessing=True,\n        validation_data=(text_type_validation, label_validation),\n        verbose=0\n    )\n    \n    results[header] = sum(history.history['val_accuracy'])/len(history.history['val_accuracy'])\n\nprint(results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The previous results show that **stem_tex** words have a tiny improvement compared to **cleaned_text** and **lemma_text**, so the following model improvements are going to be focused on the **stem_text** words.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"text_train_static = text_train.copy()\ntext_validation_static = text_validation.copy()\ntext_test_static = text_test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_train = np.array(text_train_static['stem_text'].copy())\ntext_validation = np.array(text_validation_static['stem_text'].copy())\ntext_test = np.array(text_test_static['stem_text'].copy())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Model training\n\nOn the model improvements is necessary to make a word tokenization (**the purpose is to keep this notebook as a realistic problem, so the tokenization process is made only on the words in the training set**), only then the training, validation and testing sets are converted to sequences and applied a padding to the list of words to keep a max number of 30 words.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"max_words=1000\nmax_len=30\n\n# Word tokenization only in the training set\ntokenizer = keras.preprocessing.text.Tokenizer(num_words=max_words, lower=False)\ntokenizer.fit_on_texts(text_train)\n\ntext_train = tokenizer.texts_to_sequences(text_train)\ntext_validation = tokenizer.texts_to_sequences(text_validation)\ntext_test = tokenizer.texts_to_sequences(text_test)\n\n# Adding 1 because of reserved 0 index\nvocab_size = len(tokenizer.word_index) + 1\n\n# \"maxlen\" set to 30 based on numbers of words on histogream per sentence (padding the sentences)\ntext_train = keras.preprocessing.sequence.pad_sequences(text_train, padding='post', maxlen=maxlen)\ntext_validation = keras.preprocessing.sequence.pad_sequences(text_validation, padding='post', maxlen=maxlen)\ntext_test = keras.preprocessing.sequence.pad_sequences(text_test, padding='post', maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model bellow is implemented.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = keras.models.Sequential()\nmodel.add(keras.layers.Embedding(vocab_size, 300, input_length=maxlen))\nmodel.add(keras.layers.Dropout(0.5))\nmodel.add(keras.layers.Conv1D(256, 7, activation='relu', padding='same'))\nmodel.add(keras.layers.GlobalMaxPooling1D())\nmodel.add(keras.layers.Flatten())\nmodel.add(keras.layers.Dense(128, activation='relu'))\nmodel.add(keras.layers.Dense(3, activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(),\n    optimizer=keras.optimizers.RMSprop(\n        learning_rate=0.0001,\n        rho=0.9,\n        centered=False\n    ),\n    metrics=['accuracy']\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stopping = keras.callbacks.EarlyStopping(\n    monitor='val_accuracy',\n    verbose=1,\n    patience=5,\n    mode='max',\n    restore_best_weights=True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_lr = keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.2,\n    patience=2,\n    verbose=1,\n    mode='auto',\n    min_delta=0.0005,\n    cooldown=0,\n    min_lr=1e-6\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> The model was trained using class weights because the imbalanced data, but the results dropped drastically, so no other balancing data process was made.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class_weights = sk.utils.class_weight.compute_class_weight(\n    'balanced',\n    classes=np.unique(label_train),\n    y=label_train\n)\n\nclass_weights = {label : class_weights[label] for label in range(3)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(\n    text_train,\n    label_train,\n    epochs=50,\n    batch_size=16,\n    use_multiprocessing=True,\n    #class_weight=class_weights,\n    validation_data=(text_validation, label_validation),\n    callbacks=[early_stopping, reduce_lr]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = pd.DataFrame(history.history).plot(figsize=(12,6))\nplt.grid(True)\nplt.gca().set_ylim(0.3, 1.0)\nlabel_y = plt.ylabel('Score')\nlabel_y.set_color('gray')\nlabel_y.set_size(12)\nlabel_x = plt.xlabel('Epoch')\nlabel_x.set_color('gray')\nlabel_x.set_size(12)\nax.tick_params(axis='x', colors='gray')\nax.tick_params(axis='y', colors='gray')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(text_validation, label_validation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Model evaluation\n\nOn the training process, the model got a validation accuracy above **82%**, some values were used as hyperparameters, like the learning rate, the network architecture or the number of nodes.\n\nThe final step is to verify the model performance using data never seen by the model (**testing set**).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(text_test, label_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model on the testing set has a performance slighly above **80%**, using a confussion matrix is possible to verify that the accuracy is not because the imbalance data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_test = model.predict(text_test)\n\n# Confusion matrix\nevaluation_cm = confusion_matrix(label_test, prediction_test.argmax(axis=1), binary=False)\nfig, ax = plot_confusion_matrix(\n    conf_mat=evaluation_cm,\n    figsize=(6,6),\n    class_names=['negative', 'neutral', 'positive'], \n)\nlabel_y = plt.ylabel('true label')\nlabel_y.set_color('gray')\nlabel_y.set_size(12)\nlabel_x = plt.xlabel('predicted label')\nlabel_x.set_color('gray')\nlabel_x.set_size(12)\n\nax.tick_params(axis='x', colors='gray')\nax.tick_params(axis='y', colors='gray')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Accuracy: {sk.metrics.accuracy_score(label_test, prediction_test.argmax(axis=1))}')\nprint(f'Precision: {sk.metrics.precision_score(label_test, prediction_test.argmax(axis=1), average=\"weighted\")}')\nprint(f'Recall: {sk.metrics.recall_score(label_test, prediction_test.argmax(axis=1), average=\"weighted\")}')\nprint(f'F1 Score: {sk.metrics.f1_score(label_test, prediction_test.argmax(axis=1), average=\"weighted\")}')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}