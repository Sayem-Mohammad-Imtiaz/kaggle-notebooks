{"cells":[{"metadata":{},"cell_type":"markdown","source":"## In this notebook, we'll show that using embeddings that employ the `hashing trick` will result in reduce performance, namely:\nEven with only 7,108 data items, using hashing vectors resulted in 73% accuracy, instead of 84-85% accuracy for the full sized embeddings.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### The hashing trick:\nIn machine learning, feature hashing, also known as the hashing trick (by analogy to the kernel trick), is a fast and space-efficient way of vectorizing features, i.e. turning arbitrary features into indices in a vector or matrix. It works by applying a hash function to the features and using their hash values as indices directly, rather than looking the indices up in an associative array. \nhttps://en.wikipedia.org/wiki/Feature_hashing\n\n### Employing the hashing trick creates smaller WordVector models. It is commonly used in Spacy medium sized models, and presently all non-English languages models use it. Beware! Let's prove the costs.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport spacy\nimport numpy as np  \nimport pandas as pd  \nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n# set random for reproducibility\nimport tensorflow as tf\nimport random as python_random\nnp.random.seed(12)\npython_random.seed(12)\ntf.random.set_seed(12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load Spacy for embeddings, big and small\n# The vectors in spacy medium use the hashing trick: \n# a number of keys map to the same vector\nnlp = spacy.load('/kaggle/input/spacymd225//en_core_web_md-2.2.5/en_core_web_md/en_core_web_md-2.2.5')\n# Spacy large has unique key vector mappings\nnlp_large = spacy.load('/kaggle/input/spacyen-core-web-lg225/en_core_web_lg-2.2.5/en_core_web_lg/en_core_web_lg-2.2.5')\n\nspacy_words = [tmp.text for tmp in nlp.vocab]\nprint('A few examples', spacy_words [:9])\nspacy_words = set(spacy_words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## We'll use the occupations and employers datasets (cleaned and labeled versions of the Kesho/Wikidata datasets)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"occupations_df = pd.read_csv('/kaggle/input/mlyoucanuse-wikidata-occupations-labeled/occupations.wikidata.all.labeled.tsv', sep='\\t')\noccupations_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"employers_df = pd.read_csv('/kaggle/input/mlyoucanuse-wikidata-employers-labeled/employers.wikidata.all.labeled.csv', sep='\\t')\nemployers_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_occupations =  occupations_df['occupation'].tolist()\noccs_in_spacy = [tmp for tmp in all_occupations \n                 if tmp in spacy_words]\nprint(f\"{len(occs_in_spacy):,} occupations in Spacy\")\nall_employers = employers_df['employer'].tolist()\nemps_in_spacy = [tmp for tmp in all_employers \n                 if tmp in spacy_words]\nprint(f\"{len(emps_in_spacy):,} employers in Spacy\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A good occupation and a good employer are mutually exclusive, \n# but a bad occupation might be a good employer, and vice versa, \n# so we'll carefully filter our negative examples\n\ngood_occs = set()\nbad_occs = set()\n\ngood_emps = set()\nbad_emps = set()\n\nclean_negative_eg = set()\n\nfor idx, row in occupations_df.iterrows():\n    if row['occupation'] in spacy_words :\n        if row['label'] ==1:\n            good_occs.add(row['occupation'])\n        else: \n            bad_occs.add(row['occupation'])\n            \nfor idx, row in employers_df.iterrows():\n    if row['employer'] in spacy_words :\n        if row['label'] ==1:\n            good_emps.add(row['employer'])\n        else: \n            bad_emps.add(row['employer'])\n            \nfor occ in bad_occs:\n    if occ not in good_emps:\n            clean_negative_eg.add(occ)\nfor emp in bad_emps:\n    if emp not in good_occs:\n            clean_negative_eg.add(emp)\n            \nprint(f\"{len(good_occs):,} good occupations, {len(good_emps):,} good employers, {len(clean_negative_eg):,} negative examples\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train and Predict using Spacy large: full-sized 300dim word vectors","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# We'll create our data using full size word vectors first\n\nXlg = []\nfor word in good_occs:\n    Xlg.append(nlp_large(word).vector)\nfor word in good_emps:\n    Xlg.append(nlp_large(word).vector)\nfor word in clean_negative_eg:\n    Xlg.append(nlp_large(word).vector)\n    \nXlg = np.asarray(Xlg)\nylg = np.hstack([np.ones(len(good_occs)), \n                 np.array([2] * len(good_emps), np.int32),\n                 np.zeros(len(clean_negative_eg))])\n\n# set random for reproducibility\n# np.random.seed(12) # already done above\nrandomize = np.arange(len(ylg))\nnp.random.shuffle(randomize)\nXlg = Xlg[randomize]\nylg = ylg[randomize]\n\nprint('X shape', Xlg.shape, 'y shape', ylg.shape)\n\nX_train, X_test, y_train, y_test = train_test_split(Xlg, ylg, test_size=0.3, random_state=12)\n# Partition equal sizes of test and validation sets\nX_test, X_validation, y_test, y_validation = train_test_split(X_test, y_test, test_size=0.5, random_state=12)\n\n# a create model method, which we'll reuse for the medium sized Spacy word embeddings\n\ndef get_model(input_dim=300):\n    np.random.seed(12) # set seeds for reproducibility\n    python_random.seed(12)\n    tf.random.set_seed(12)\n    model = Sequential(name='emp_occ_other_detector')\n    model.add(Dense(256, name='Dense_layer_1',\n                    input_dim=input_dim, activation='relu'))\n    model.add(Dense(256, name='Dense_layer_2',\n                    activation='relu'))\n    model.add(Dense(3, name='Dense_layer_3', activation='softmax'))\n    model.compile(loss='sparse_categorical_crossentropy', \n                  optimizer='adam',\n                  metrics=['accuracy'])  \n    return model\n\nmodel = get_model()\nhistory = model.fit(X_train, y_train,\n                     epochs=20,\n                     verbose=0,\n                     validation_data=(X_test, y_test),\n                     batch_size=32)\nloss, accuracy = model.evaluate(X_train, y_train, verbose=True)\nprint(f\"Training Accuracy: {accuracy:.4f} Loss: {loss:.4f}\")\nloss, accuracy = model.evaluate(X_test, y_test, verbose=True)\nprint(f\"Testing Accuracy: {accuracy:.4f} Loss: {loss:.4f}\")\nloss, accuracy = model.evaluate(X_validation, y_validation, verbose=True)\nprint(f\"Unseen Accuracy: {accuracy:.4f} Loss: {loss:.4f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now train and predict using Spacy medium-sized hashed word vectors","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Xmd = []\nfor word in good_occs:\n    Xmd.append(nlp(word).vector)\nfor word in good_emps:\n    Xmd.append(nlp(word).vector)\nfor word in clean_negative_eg:\n    Xmd.append(nlp(word).vector)\n    \nXmd = np.asarray(Xmd)\nymd = np.hstack([np.ones(len(good_occs)),\n                 np.array([2] * len(good_emps), np.int32), \n                 np.zeros(len(clean_negative_eg))])\n\n# we'll reuse the random array from before\n# randomize = np.arange(len(ymd))\n# np.random.shuffle(randomize)\nXmd = Xmd[randomize]\nymd = ymd[randomize]\n\nprint('X shape', Xmd.shape, 'y shape', ymd.shape)\n\nX_train, X_test, y_train, y_test = train_test_split(Xmd, ymd, test_size=0.3, random_state=12)\n# Partition equal sizes of test and validation sets\nX_test, X_validation, y_test, y_validation = train_test_split(X_test, y_test, test_size=0.5, random_state=12)\n\nmd_model = get_model()\n\nhistory = md_model.fit(X_train, y_train,\n                     epochs=20,\n                     verbose=False,\n                     validation_data=(X_test, y_test),\n                     batch_size=32)\nloss, accuracy = md_model.evaluate(X_train, y_train, verbose=True)\nprint(f\"Training Accuracy: {accuracy:.4f} Loss: {loss:.4f}\")\nloss, accuracy = md_model.evaluate(X_test, y_test, verbose=True)\nprint(f\"Testing Accuracy: {accuracy:.4f} Loss: {loss:.4f}\")\nloss, accuracy = md_model.evaluate(X_validation, y_validation, verbose=True)\nprint(f\"Unseen Accuracy: {accuracy:.4f} Loss: {loss:.4f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ## So using the hashing vectors resulted in 10% LESS accuracy than when using the full-sized embeddings.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Now, how bad would it be if we ran PCA and reduced the 300dim vector to 50dim?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=50)\nXmd50 = pca.fit_transform(Xmd)\n\nX_train, X_test, y_train, y_test = train_test_split(Xmd50, ymd, test_size=0.3, random_state=12)\n# Partition equal sizes of test and validation sets\nX_test, X_validation, y_test, y_validation = train_test_split(X_test, y_test, test_size=0.5, random_state=12)\n\nmd_model = get_model(input_dim=50) \nprint(\"Hashed embeddings reduced with PCA (300 -> 50 dim)\")\nhistory = md_model.fit(X_train, y_train,\n                     epochs=20,\n                     verbose=False,\n                     validation_data=(X_test, y_test),\n                     batch_size=32)\nloss, accuracy = md_model.evaluate(X_train, y_train, verbose=True)\nprint(f\"Training Accuracy: {accuracy:.4f} Loss: {loss:.4f}\")\nloss, accuracy = md_model.evaluate(X_test, y_test, verbose=True)\nprint(f\"Testing Accuracy: {accuracy:.4f} Loss: {loss:.4f}\")\nloss, accuracy = md_model.evaluate(X_validation, y_validation, verbose=True)\nprint(f\"Unseen Accuracy: {accuracy:.4f} Loss: {loss:.4f}\")    \n\npca = PCA(n_components=50)\nXlg50 = pca.fit_transform(Xlg)\n\nX_train, X_test, y_train, y_test = train_test_split(Xlg50, ymd, test_size=0.3, random_state=12)\n# Partition equal sizes of test and validation sets\nX_test, X_validation, y_test, y_validation = train_test_split(X_test, y_test, test_size=0.5, random_state=12)\n\nlg50_model = get_model(input_dim=50) \nprint(\"Full embeddings reduced with PCA (300 -> 50 dim)\")\nhistory = lg50_model.fit(X_train, y_train,\n                     epochs=20,\n                     verbose=False,\n                     validation_data=(X_test, y_test),\n                     batch_size=32)\nloss, accuracy = lg50_model.evaluate(X_train, y_train, verbose=True)\nprint(f\"Training Accuracy: {accuracy:.4f} Loss: {loss:.4f}\")\nloss, accuracy = lg50_model.evaluate(X_test, y_test, verbose=True)\nprint(f\"Testing Accuracy: {accuracy:.4f} Loss: {loss:.4f}\")\nloss, accuracy = lg50_model.evaluate(X_validation, y_validation, verbose=True)\nprint(f\"Unseen Accuracy: {accuracy:.4f} Loss: {loss:.4f}\")   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### PCA typically drops accuracy a percent or two when compressing 300 dimensions down to 50 dimensions.\n\n### Turns out, PCA isn't a sin, but word vectors using the hashing trick are. For what it's worth, they are very prevalent; currently all of the Spacy word vectors outside of English are medium sized, and use the hashing trick. \n## TLDR; Don't use the hashing trick.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Let's plot the Learning curve to show how a model learns with various amount of data, when the data is hashed or full-sized embeddings.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create CV training and test scores for various training set sizes, large and medium\ntrain_sizeslg, train_scoreslg, test_scoreslg = learning_curve(\n    RandomForestClassifier(), \n    Xlg, \n    ylg,\n    # Number of folds in cross-validation\n    cv=10,\n    # Evaluation metric\n    scoring='accuracy',\n    # Use all computer cores\n    n_jobs=-1, \n    # 20 different sizes of the training set\n    train_sizes=np.linspace(0.01, 1.0, 10))\n\n# medium\ntrain_sizesmd, train_scoresmd, test_scoresmd = learning_curve(\n    RandomForestClassifier(), \n    Xmd, \n    ymd,\n    # Number of folds in cross-validation\n    cv=10,\n    # Evaluation metric\n    scoring='accuracy',\n    # Use all computer cores\n    n_jobs=-1, \n    # 20 different sizes of the training set\n    train_sizes=np.linspace(0.01, 1.0, 10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create means and standard deviations of training set scores\ntrain_meanlg = np.mean(train_scoreslg, axis=1)\ntrain_stdlg = np.std(train_scoreslg, axis=1)\n# Create means and standard deviations of test set scores\ntest_meanlg = np.mean(test_scoreslg, axis=1)\ntest_stdlg = np.std(test_scoreslg, axis=1) \ntrain_meanmd = np.mean(train_scoresmd, axis=1)\ntrain_stdmd = np.std(train_scoresmd, axis=1)\ntest_meanmd = np.mean(test_scoresmd, axis=1)\ntest_stdmd = np.std(test_scoresmd, axis=1)\n\nplt.figure(figsize=(9,4))\nplt.plot(train_sizeslg, test_meanlg, color=\"#0000FF\", label=\"Full Size Embeddings\")\nplt.plot(train_sizesmd, test_meanmd, color=\"#FF0000\", label=\"Hashed Embeddings\")\nplt.title(\"Learning Curve: Full Size vs. Hashed Embeddings\")\nplt.xlabel(\"Training Set Size\")\nplt.ylabel(\"Accuracy\")\nplt.legend(loc=\"best\")\nplt.tight_layout(pad=1, w_pad=.5, h_pad=.5)\nplt.savefig('/kaggle/working/learningcurve_fullvshashedembeddings.png')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}