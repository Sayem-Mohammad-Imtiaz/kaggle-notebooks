{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1. **Import Libraries**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport time\nimport shutil\n\nimport random\n\nfrom tqdm import tqdm\nfrom glob import glob\nfrom typing import Union\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.utils import shuffle as sk_shuffle\n\nimport matplotlib.pyplot as plt\nfrom IPython.display import display, HTML","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras.backend as K\n\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.utils import Sequence, plot_model, to_categorical\nfrom tensorflow.keras.metrics import Mean\nfrom tensorflow.keras.losses import (\n    SparseCategoricalCrossentropy, CategoricalCrossentropy, \n    sparse_categorical_crossentropy, categorical_crossentropy\n)\nfrom tensorflow.keras.layers import (\n    Layer, \n    Input, InputLayer, Embedding, \n    Dropout, Dense, \n    Dot, Concatenate, Average, Add,\n    Bidirectional, LSTM,\n    Lambda, Reshape\n)\nfrom tensorflow.keras.callbacks import (\n    EarlyStopping, TensorBoard, \n    ModelCheckpoint, ReduceLROnPlateau, \n    LearningRateScheduler, Callback\n)\nfrom tensorflow.keras.activations import softmax, sigmoid\nfrom tensorflow.keras.initializers import Identity, GlorotNormal, TruncatedNormal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tpu_available = False\ntry:\n    # detect and init the TPU\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(f\"TPU: {tpu}\")\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n\n    # instantiate a distribution strategy\n    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    tpu_available = True\nexcept ValueError:\n    print('Cannot use TPU')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. **Concat Datasets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_for_concat = ['review', 'Negative_Review', 'Positive_Review']\nlist_of_review_dfs = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extra_data_dir = '../input/hotel-and-restaurant-reviews/'\nextra_datasets = os.listdir(extra_data_dir)\nfor dataset in extra_datasets:\n    dataset_path = os.path.join(extra_data_dir, dataset)\n    dataset_df = pd.read_csv(dataset_path)\n    \n    for column in columns_for_concat:\n        if column not in dataset_df.columns:\n            continue\n        \n        list_of_review_dfs.append(\n            dataset_df[column].rename({\n                column: 'review'\n            })\n        )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_dataset_path = '../input/hotel-comment/{}_data.csv'\nmain_datasets = ['training', 'valuating', 'testing']\nfor dataset in main_datasets:\n    dataset_df = pd.read_csv(main_dataset_path.format(dataset))\n    list_of_review_dfs.append(\n        dataset_df['Comment'].rename({\n            'Comment': 'review'\n        })\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences_df = pd.concat(list_of_review_dfs, ignore_index=True)\nn_sentences_raw = len(sentences_df)\n\nsentences_df.dropna(inplace=True)\nsentences_df.drop_duplicates(inplace=True)\nsentences_len = sentences_df.str.len()\nsentences_df = pd.concat((sentences_df, sentences_len), axis='columns', names=['review', 'seq_len'])\nsentences_df.rename({0: 'review', 1: 'seq_len'}, axis='columns', inplace=True)\nsentences_df = sentences_df.loc[\n    (sentences_df['seq_len']>32) & (sentences_df['seq_len']<512)\n]\nsentences_df.drop(['seq_len'], axis='columns', inplace=True)\nsentences_df = sentences_df.reset_index(drop=True)\nsentences_df = sentences_df['review']\nn_sentences = len(sentences_df)\n\nprint(f'Filter {n_sentences_raw} sentences down to {n_sentences} sentences')\n\nsentences_df.sample(n=7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. **BERT Word Embeddings**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install stellargraph","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from stellargraph.utils import plot_history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install keras-bert","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras_bert import (\n    PretrainedList, \n    get_pretrained, \n    get_checkpoint_paths,\n    load_trained_model_from_checkpoint, \n    load_vocabulary,\n    extract_embeddings,\n    Tokenizer\n)\n\nfrom keras_bert.bert import get_model\nfrom keras_bert.loader import load_trained_model_from_checkpoint\nfrom keras_bert.optimizers import AdamWarmup\n\n\n# model_path = get_pretrained(PretrainedList.multi_cased_base)\nmodel_path = '/kaggle/input/bert-pretrained/uncased_L-4_H-512_A-8'\npaths = get_checkpoint_paths(model_path)\nprint(f\"Config: {paths.config}\")\nprint(f\"Ckpt: {paths.checkpoint}\")\nprint(f\"Vocab: {paths.vocab}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocabs = load_vocabulary(paths.vocab)\ntokenizer = Tokenizer(vocabs, cased=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 512\nSPECIAL_TOKENS = ['[MASK]', '[PAD]', '[CLS]', '[SEP]', '[UNK]']\ntokens_dict = tokenizer._token_dict\ninverse_tokens_dict = tokenizer._token_dict_inv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pretrained_args = {\n    'config_file': paths.config,\n    'checkpoint_file': paths.checkpoint,\n    'training': True,\n    'seq_len': MAX_LEN,\n}\ncompile_args = {\n    'optimizer': AdamWarmup(decay_steps=100420,\n                            warmup_steps=8192,\n                            learning_rate=1e-4,\n                            weight_decay=0.01, \n                            weight_decay_pattern=['embeddings', 'kernel', 'W1', 'W2', 'Wk', 'Wq', 'Wv', 'Wo'],),\n    'metrics': [\"accuracy\"],\n    'loss': SparseCategoricalCrossentropy() \n}\n\nif tpu_available:\n    with tpu_strategy.scope():\n        model = load_trained_model_from_checkpoint(**pretrained_args)\n        model_args = {\n            'inputs': model.inputs,\n            'outputs': model.outputs[0],\n            'name': 'Masked-Language-Model',\n        }\n        bert_mlm = Model(**model_args)\n        bert_mlm.compile(**compile_args)\nelse:\n    model = load_trained_model_from_checkpoint(**pretrained_args)\n    model_args = {\n        'inputs': model.inputs,\n        'outputs': model.outputs[0],\n        'name': 'Masked-Language-Model',\n    }\n    bert_mlm = Model(**model_args)\n    bert_mlm.compile(**compile_args)\n\nbert_mlm.load_weights('../input/bertforhotelandrestaurant/ep029_acc1.000_val_acc1.000.h5')\nbert_mlm.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.inputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.outputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequence_embeddings  = model.layers[-6].output\nsequence_embeddings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(bert_mlm, show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. **Data Generator**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize_sentences(sentences,\n                       seq_len: int=512, \n                       use_cased: bool=False):\n    \n    if isinstance(sentences, str):\n        sentences = [sentences]\n    elif not isinstance(sentences, (list, tuple)):\n        raise ValueError(f'Wrong type of argument `sentences`: {type(sentences)}')\n    \n    sentences_tokenized, sentences_segmented, sentences_n_tokens = [], [], []\n    for sentence in sentences:\n        if not use_cased:\n            sentence = sentence.lower()\n        tokens, segments = tokenizer.encode(sentence, max_len=seq_len)\n        \n        # 0-padding\n        PAD_token = tokenizer._token_dict ['[PAD]']\n        n_pads = seq_len - len(tokens)\n        tokens.extend([PAD_token]*n_pads)\n        segments.extend([PAD_token]*n_pads)\n        sentences_tokenized.append(tokens)\n        sentences_segmented.append(segments)\n\n    return np.array(sentences_tokenized), np.array(sentences_segmented)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataGenerator(Sequence):\n\n    def __init__(self,\n                 sentences: pd.Series,\n                 max_samples=None,\n                 sentence_len: int=512,\n                 batch_size: int=16,\n                 mask_ratio: float=0.19,\n                 mask_step: float=0.0,\n                 shuffle: bool=True):\n        \n        self.sentences = sentences\n        self.indices = sentences.index.tolist()\n        self.sentence_len = sentence_len                \n        self.batch_size = batch_size\n        self.mask_ratio = min(0.89, max(mask_ratio, 0.02))\n        self.mask_step = mask_step\n        self.max_samples = len(sentences) if max_samples is None else max_samples\n        self.shuffle = shuffle\n        self.special_tokens = [\n            tokens_dict[tok] for tok in SPECIAL_TOKENS\n        ]\n        self.on_epoch_end()\n\n    def __len__(self):\n        \"\"\"\n        Denotes the number of batches per epoch\n        \"\"\"\n        return int(len(self.indices) // self.batch_size)\n\n    def __getitem__(self, index):\n        \"\"\"\n        Generate one batch of data\n        \"\"\"\n        # Generate indexes of the batch\n        start_index = self.batch_size * index\n        end_index = self.batch_size * (index+1)            \n        indices = self.indices[start_index:min(end_index, len(self.sentences))]\n\n        sentences_batch = self.sentences.loc[indices].tolist()\n\n        # Generate data\n        tokens_batch, segments_batch = tokenize_sentences(sentences_batch)\n        spec_tok_mask = np.zeros(tokens_batch.shape)\n        spec_tok = [\n            np.where(tokens_batch==tokens_dict[token]) \n            for token in SPECIAL_TOKENS[1:]\n        ]\n        for t in range(len(SPECIAL_TOKENS)-1):\n            spec_tok_mask[spec_tok[t][0], spec_tok[t][1]] = 1\n\n        rand_mask = np.random.rand(*tokens_batch.shape) < self.mask_ratio\n        bert_mask = rand_mask * (1-spec_tok_mask)\n        \n        mask_token = np.ones(tokens_batch.shape) * tokens_dict['[MASK]']\n        tokens_masked_batch = np.where(bert_mask, mask_token, tokens_batch)\n        \n        return [tokens_batch, segments_batch, bert_mask], tokens_masked_batch # to_categorical(tokens_masked_batch, num_classes=len(tokens_dict))\n\n    def on_epoch_end(self):\n        \"\"\"\n        Update indices after each epoch\n        \"\"\"\n        if self.shuffle:\n            self.indices = sk_shuffle(self.indices)\n        self.indices = self.indices[:self.max_samples]\n        self.mask_ratio += self.mask_step","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_set, test_set = train_test_split(sentences_df, test_size=41020, random_state=27)\nval_set, test_set = train_test_split(test_set, test_size=0.5, random_state=11)\n\ntrain_generator = DataGenerator(train_set, max_samples=41020, mask_step=0.03)\nval_generator = DataGenerator(val_set)\ntest_generator = DataGenerator(test_set)\nlen(train_generator), len(val_generator), len(test_generator)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. **Train**"},{"metadata":{"trusted":true},"cell_type":"code","source":"output_dir = '/kaggle/working/models'\nif not os.path.isdir(output_dir):\n    os.makedirs(output_dir)\n\nlog_dir = '/kaggle/working/logs'\nif not os.path.isdir(log_dir):\n    os.makedirs(log_dir)\n\nviz_dir = '/kaggle/working/visualizations'\nif not os.path.isdir(viz_dir):\n    os.makedirs(viz_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logger = TensorBoard(log_dir=log_dir)\n\nmodel_format = 'ep={epoch:03d}_loss={loss:.3f}_val_loss={val_loss:.3f}.h5'\ncheckpoint = ModelCheckpoint(\n    filepath=os.path.join(output_dir, model_format),\n    monitor='loss', \n    mode='min',\n    save_weights_only=True, \n    save_best_only=False, \n    save_freq='epoch'\n)\nlr_reducer = ReduceLROnPlateau(\n    monitor='loss', factor=0.1, patience=3, verbose=1)\nearly_stopper = EarlyStopping(\n    monitor='val_loss', mode='min', min_delta=0, patience=7, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_history = bert_mlm.fit_generator(\n    generator=train_generator,\n    steps_per_epoch=len(train_generator),\n    validation_data=val_generator,\n    validation_steps=len(val_generator),\n    callbacks=[checkpoint, lr_reducer, early_stopper], \n    epochs=50,\n    initial_epoch=29\n)\n\nhist_fig = plot_history(train_history, return_figure=True)\nhist_fig.savefig(f'{viz_dir}/train_history.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}