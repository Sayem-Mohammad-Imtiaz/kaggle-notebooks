{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Assumptions Of Linear Regression Algorithm","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I will analyze this dataset based on some assumptions of linear regression.\n\nLinear Relationship between the features and target - Linear regression requires the relationship between the independent and dependent variables to be linear. Linearity can be checked with the scatter plot and Pearson Correlation.\n\nNo Multicollinearity - Linear regression assumes that the independent variables are not highly correlated with each other. This assumption is tested using Variance Inflation Factor (VIF) values.\n\nHomoscedasticity – This assumption states that the variance of errors are similar across the values of the independent variables. A plot of standardized residuals versus predicted values can show whether points are equally distributed across all values of the independent variables.\n\nNormal distribution of errors – Linear regression assumes that the residuals are normally distributed.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **1. Reading And Understanding The Data**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv(\"../input/car-price-prediction/CarPrice_Assignment.csv\", index_col=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dataset.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dataset.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dataset.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no null values in the dataset. On the other hand, there are some categorical variables.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2. Cleaning The Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Some car names are badly written in the dataset. We need to fix them to get more accurate results.\n * maxda = mazda \n * toyouta = toyota\n * vokswagen = volkswagen\n * vw = volkswagen\n * porcshce = porsche\n * Nissan = nissan","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['CarName'] = dataset['CarName'].str.split(' ',expand =True)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dataset['CarName'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['CarName'] = dataset['CarName'].replace({'maxda': 'mazda', 'porcshce': 'porsche', 'toyouta': 'toyota', \n                                                 'vokswagen': 'volkswagen', 'vw': 'volkswagen', 'Nissan': 'nissan'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dataset['CarName'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dataset.duplicated().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no duplicated values in the dataset.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 3. Exploratory Data Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Firstly, it is better to check car price distribution.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,5))\n\nplt.subplot(1,2,1)\nsns.kdeplot(dataset['price'],  color='blue', shade=True)\nplt.xlabel(\"price\")\n\nplt.subplot(1,2,2)\nsns.boxplot(dataset['price'], palette=\"Set3\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Car price distribution is left skewed. On the other hand, there are some outliers in the dataset. Since linear regression is sensitive to outliers, I will analyze them later.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,5))\nsorted = dataset.groupby(['CarName'])['price'].median().sort_values()\nsns.boxplot(x=dataset['CarName'], y=dataset['price'], order = list(sorted.index))\nplt.title(\"Car Name vs Price\")\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Chevrolet is the cheapest car. Honda, Dodge, Plymouth has cheap prices, but these cars have some outlier prices. Bmw, Porsche, Buick, Jaguar has the highest prices.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Firstly, I will check the linear relationship between variables visually. After that, I will analyze Pearson Correlation and P-values to be able to understand the correlation. If the correlation is weak, I will drop related variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(dataset, diag_kind=\"kde\", vars=['symboling', 'wheelbase', 'carlength', 'carwidth', 'carheight', 'curbweight', 'enginesize', 'price'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Symboling and Carheight doesn't affect price. There is not a linear relationship.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(dataset, diag_kind=\"kde\", vars=['boreratio', 'stroke', 'compressionratio', 'horsepower', 'peakrpm', 'citympg', 'highwaympg', 'price'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Stroke, Compressionratio, Peakrpm doesn't affect price. There is not a linear relationship. I will drop these variables as well.\n\nFirstly, I will analyze the relationship between categorical variables and the price.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_columns = ['fueltype', 'aspiration', 'doornumber', 'carbody', 'drivewheel', 'enginelocation', 'enginetype', 'cylindernumber', 'fuelsystem']\ncategorical_data = dataset[categorical_columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,15))\nfor index, item in enumerate(categorical_columns, 1):\n    plt.subplot(3,3,index)\n    sns.barplot(x = item, y = 'price', data = dataset)\nplt.show()  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* diesel-powered cars more expensive than the gas-powered cars. \n* convertible and hardop is more expensive than sedan, hatchback, and wagon.\n* rwd drivewheel is more expensive than fwd and 4wd.\n* rear engine location is more expensive.\n* If the cylinder number is high, price is high.\n* mpfi and idi fuel systems is relatively expensive than other types.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 4. Data Preparation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I will check Pearson Correlation to investigate the linear relationship between two continuous variables. If the features have a weak relationship with the price, I will drop from the dataset. \n\nI will also check P-value to analyze the correlation is statistically significant or not. It is generally accepted that if the value is above 0.05, the correlation is not significant. If it is below 0.05, the correlation is significant.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nnumerical_columns = dataset.select_dtypes(exclude='object').columns\nfor i in (list(numerical_columns)):\n    pearson_coef, p_value = stats.pearsonr(dataset[i], dataset['price'])\n    print(i.capitalize(), \"Pearson Correlation:\", pearson_coef, \"P-value:\", p_value)\n    print(\"The correlation is not significant:\", p_value>0.05)\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Symboling, Carheight, Stroke, Compressionratio, Peakrpm has a weak relationship with Price. The correlation between these variables and the price is not statistically significant. I will drop them from the dataset.\n\nWheelbase, Boreratio has a moderate relationship with Price. The correlation is statistically significant.\n\nCarlength, Carwidth, Curbweight, Enginesize, Horsepower has a strong positive relationship with Price. The correlation is statistically significant.\n\nCitympg, Highwaympg has a strong negative relationship with Price. The correlation is statistically significant.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"On the other hand, door number has no relationship with the price at all. It is clear visually. I will drop door number from the dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.drop(['symboling', 'carheight', 'stroke', 'compressionratio', 'peakrpm', 'doornumber'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dataset.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Converting to numerical values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I will add \"cars category\" column to the dataset according to car prices. I will group cars as budget friendly, medium range, expensive cars. I will drop \"cars name\" column as I will add \"cars category\" column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_new = dataset.copy()\nt_price = data_new.groupby(['CarName'])['price'].mean()\ndata_new = data_new.merge(t_price.reset_index(), how='left', on='CarName')\nbins = [0,10000,20000,40000]\nlabel =['Budget_Friendly','Medium_Range','Expensive_Cars']\ndataset['Cars_Category'] = pd.cut(data_new['price_y'], bins, right=False, labels=label)\ndataset.drop(\"CarName\", axis=1, inplace=True)\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will convert categorical variables to numerical variables. Categorical variables in the dataset are nominal. I can apply OneHotEncoder.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"column = ['fueltype','aspiration','carbody', 'drivewheel', 'enginelocation', 'enginetype','cylindernumber', 'fuelsystem', 'Cars_Category']\ndummies = pd.get_dummies(dataset[column], drop_first = True)\ndataset = pd.concat([dataset, dummies], axis = 1)\ndataset.drop(column, axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dataset.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Rescaling","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Ordinary Least Squares method does not make normality assumptions about the data. It makes normality assumptions about the residuals. I will not transform the data to ensure Gaussian distribution. \n\nOn the other hand, linear regression is sensitive to outliers. Quantile Transformer is robust to outliers. It will transform the variables and handle the outliers in the dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import QuantileTransformer\ntransform =  QuantileTransformer(n_quantiles=205)\ncolumns = ['wheelbase', 'carlength', 'carwidth', 'curbweight','enginesize','boreratio','horsepower','citympg','highwaympg','price']\ndataset[columns] = transform.fit_transform(dataset[columns]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dataset.columns.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are all the columns in the dataset now.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Correlation Between Variables","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Linear regression assumes the independent variables are not related with each other. If the correlation degree is high, it will cause problems when we fit the model.\n\nTo check multicollinearity, I will use heatmap and VIF.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (40, 40))\nsns.heatmap(dataset.corr(method ='pearson'), cmap='PuBu', annot=True, linewidths=.5, annot_kws={'size':8})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dataset.corr(method ='pearson').unstack().sort_values().drop_duplicates())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nMulticollinearity exists among predictors. After even transforming the variables, there is a strong relationships between independent variables. For this reason, I will use Variation Inflation Factor (VIF) to detect multicollinearity and to eliminate these variables from the dataset.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Checking Pearson Correlation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Before eliminating correlated variables, I will check Pearson Correlation and p values. I will eliminate the features based on the accordingly.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = list(dataset.columns)\nfor i in data:\n    pearson_coef, p_value = stats.pearsonr(dataset[i], dataset['price'])\n    print(i.capitalize(), \"Pearson Correlation:\", pearson_coef, \"P-value:\", p_value)\n    print(\"The correlation is not significant:\", p_value>0.05)\n    if p_value>0.05:\n        dataset.drop(i, axis=1, inplace=True)\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dataset.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Variation Inflation Factor (VIF)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"A variance inflation factor(VIF) detects multicollinearity in regression analysis. I will select the features with VIF that is below 10.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = dataset.drop('price', axis=1)\ny = dataset['price']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif[\"variables\"] = X.columns\nvif['vif'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nfor index,column in enumerate(X.columns):\n    print(index, column, vif['vif'][index])\n    if vif['vif'][index]>10:\n        vif = vif.drop([index], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(vif)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(list(vif['variables']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Building the Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = list(vif['variables'])\ndata = dataset [columns]\ndata = pd.concat([data, dataset['price']], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = data.drop('price', axis=1)\ny = data ['price']\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.75, test_size = 0.25, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nlr = LinearRegression()\nlr.fit(X_train,y_train)\npred_test = lr.predict(X_test)\npred_train = lr.predict(X_train)\nprint(\"R Squared Value of Train Data: {}\".format(r2_score(y_train, pred_train)))\nprint(\"R Squared Value of Test Data: {}\".format(r2_score(y_test, pred_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Evaluating The Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I will check the residual normality assumption visually. Errors should be normally distributed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,5))\n\nplt.subplot(1,2,1)\nsns.distplot((y_train - pred_train))\nplt.title('Train Data Residual Analysis', fontsize = 20)                   \nplt.xlabel('Errors', fontsize = 18)\n\nplt.subplot(1,2,2)\nsns.distplot((y_test - pred_test))\nplt.title('Test Data Residual Analysis', fontsize = 20)              \nplt.xlabel('Errors', fontsize = 18)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will check ***Homoscedasticity***. There should not be specific pattern in the distribution of residuals. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from yellowbrick.regressor import ResidualsPlot\nvisualizer = ResidualsPlot(lr)\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.poof()\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}