{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, auc, roc_curve, roc_auc_score, balanced_accuracy_score, classification_report\nfrom sklearn.utils import resample\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\n\ndt = datetime.today()\n\n\ndef removeOutlier(dataSet, feature):    \n    q1=dataSet[feature].quantile(0.25)\n    q3=dataSet[feature].quantile(0.75)\n    IQR=q3-q1\n    lowerLimit = q1 - 1.5 * IQR\n    UpperLimit = q3 + 1.5 * IQR \n    dataSet = dataSet[dataSet[feature]< UpperLimit]\n    dataSet = dataSet[dataSet[feature]> lowerLimit]\n    return dataSet\n\ndef modelEvaluation(model, X_test, y_train,y_test, y_pred,cols=None):\n    print(\"Accurancy: {:.3f}\".format(accuracy_score(y_test, y_pred)))\n    print(\"ROC AUC Score: {:.3f}\".format(roc_auc_score(y_test, y_pred)))\n    print(\"F1 Score:: {:.3f} \".format(f1_score(y_test, y_pred)))\n    print(\"Balanced Accurancy Score:: {:.3f} \".format(balanced_accuracy_score(y_test, y_pred)))\n    print('\\n clasification report:\\n', classification_report(y_test,y_pred))\n    \n    fig = plt.figure(figsize=(10,6))\n    ax = fig.add_subplot(111)\n    prediction_probabilities = model.predict_proba(X_test[cols])[:,1]\n    fpr , tpr , thresholds = roc_curve(y_test,prediction_probabilities)\n    ax.plot(fpr,tpr,label = [\"Area under curve : \",auc(fpr,tpr)],linewidth=2,linestyle=\"dotted\")\n    ax.plot([0,1],[0,1],linewidth=2,linestyle=\"dashed\")\n    plt.legend(loc=\"best\")\n    plt.title(\"ROC-CURVE and AREA UNDER CURVE\")\n    ax.set_facecolor(\"k\")\n    \ndef calculate_age(row):\n    try:\n        yrs, mon = tuple(row['AVERAGE_ACCT_AGE'].split(' '))\n        age = round(float(yrs.replace('yrs', '')) + float(mon.replace('mon', ''))/12, 2)\n        row['AVERAGE_ACCT_AGE'] = age\n\n        yrs, mon = tuple(row['CREDIT_HISTORY_LENGTH'].split(' '))\n        age = round(float(yrs.replace('yrs', '')) + float(mon.replace('mon', ''))/12, 2)\n        row['CREDIT_HISTORY_LENGTH'] = age\n    except Exception as e:\n        print(row, e)\n        raise e\n    return row\n\n\ndef binning_by_depth_factor(df_column, factor):\n    divs, max_da, min_da= round(np.sqrt(len(df_column))/factor), df_column.max(), df_column.min()\n    step = (max_da - min_da)/divs\n    return (df_column/step).astype(int)*divs\n\n\ntraining_csv = '/kaggle/input/vehicle-loan-default-prediction/train.csv'\ntest_csv = '/kaggle/input/vehicle-loan-default-prediction/test.csv'\n\ntrain_df = pd.read_csv(training_csv)\ntrain_df = train_df[~train_df['LOAN_DEFAULT'].isna()]\nprint(train_df.columns)\n\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['LOAN_DEFAULT'].astype(bool).value_counts().plot.pie()\ntrain_df['LOAN_DEFAULT'].astype(bool).value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> The pie chart of the target class shows the skewness of data, leaning towards False and thus needs to be accounted for. \n>\n> Will be applying Resampling Technique to balance the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Before Resampling:')\nprint(train_df['LOAN_DEFAULT'].value_counts())\ndf_majority = train_df[train_df['LOAN_DEFAULT']==0]\ndf_minority = train_df[train_df['LOAN_DEFAULT']==1]\n\n\ndf_minority_upsampled = resample(df_minority, \n                                 replace=True,                  # sample with replacement\n                                 n_samples=len(df_majority),    # to match majority class\n                                 random_state=123)              # reproducible results\ndf_upsampled = pd.concat([df_majority, df_minority_upsampled])\nprint('After Resampling:')\nprint(df_upsampled['LOAN_DEFAULT'].value_counts())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"flags = ['MOBILENO_AVL_FLAG', 'AADHAR_FLAG', 'PAN_FLAG', 'VOTERID_FLAG', 'DRIVING_FLAG', 'PASSPORT_FLAG']\nfig, axs = plt.subplots(len(flags), 2, figsize=(10, 10))\n\nfor _i, flag in enumerate(flags):\n    for i, (vals, group) in enumerate(train_df.groupby([flag])):\n        group.groupby(['LOAN_DEFAULT']).size().plot.pie(ax=axs[_i][i])\n        axs[_i][i].set_title(str(flag)+' '+str(vals))\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncols = ['DISBURSED_AMOUNT', 'ASSET_COST', 'LTV']\nfig, axs = plt.subplots(len(cols), figsize=(10, 5*len(cols)))\n\nfor j, col in enumerate(cols):\n    for i, (val, group) in enumerate(train_df.groupby('LOAN_DEFAULT')):\n        column = binning_by_depth_factor(group[col], 10) \n        column.value_counts().to_frame('counts').reset_index().sort_values('index').plot(x='index', y='counts', label=val, ax=axs[j])\n        axs[j].set_xlabel(col)\n        axs[j].set_ylabel('counts')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Disbursed amount and asset cost in both both classes showing gaussian distribution but with different distribution frequencies. \n>\n> This shows that the defaulters are defaulting mostly in lesser amounts of loan and for lesser asset costs.\n>\n> Additionally as expected, lower LTV value customers are also defaulting."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['DOB'] = pd.to_datetime(train_df['DATE_OF_BIRTH'], format='%d-%m-%Y', errors='coerce')\ntrain_df['DD'] = pd.to_datetime(train_df['DISBURSAL_DATE'], format='%d-%m-%Y', errors='coerce')\n\n\ntrain_df = pd.DataFrame(train_df[~train_df['DOB'].isna()])\ntrain_df = pd.DataFrame(train_df[~train_df['DD'].isna()])\ntrain_df['APPLICANT_AGE'] = ((dt - train_df['DOB']).apply(lambda x: float(x.days)) / 365.0)\ntrain_df['DISBURSAL_AGE'] = ((dt - train_df['DD']).apply(lambda x: float(x.days)) / 365.0)\n\ncols = ['APPLICANT_AGE', 'DISBURSAL_AGE']\n\nfor j, col in enumerate(cols):\n    fig, axs = plt.subplots(1, len(cols), figsize=(20, 5), sharey=True)\n    for i, (val, group) in enumerate(train_df.groupby('LOAN_DEFAULT')):\n        group[col].plot.box( ax=axs[i], label=val)\n        axs[i].set_title(col)\n        axs[i].set_xlabel(\"Loan Default: %s\"%bool(val))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> As seen from the box plots, Applicant Age seems to have almost no impact on the defaulting, similar behaviour is seen in Disbursal Age"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.groupby('PERFORM_CNS_SCORE_DESCRIPTION').agg({'PERFORM_CNS_SCORE':[np.min, np.max]}).sort_values(('PERFORM_CNS_SCORE', 'amin'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> PERFORM_CNS_SCORE_DESCRIPTION seems to be a bucket label for PERFORM_CNS_SCORE"},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df[~train_df['LOAN_DEFAULT'].isna()]\n\ntotal_records = len(train_df)\n\nanalysis = []\n\ntrain_df = train_df.apply(calculate_age, axis=1)\ntrain_df['AVERAGE_ACCT_AGE'] = train_df['AVERAGE_ACCT_AGE'].astype(float)\ntrain_df['CREDIT_HISTORY_LENGTH'] = train_df['CREDIT_HISTORY_LENGTH'].astype(float)\n\ntrain_df['AADHAR_FLAG'] = train_df['AADHAR_FLAG'].astype(bool)\ntrain_df['PAN_FLAG'] = train_df['PAN_FLAG'].astype(bool)\ntrain_df['VOTERID_FLAG'] = train_df['VOTERID_FLAG'].astype(bool)\ntrain_df['DRIVING_FLAG'] = train_df['DRIVING_FLAG'].astype(bool)\ntrain_df['PASSPORT_FLAG'] = train_df['PASSPORT_FLAG'].astype(bool)\n\ntrain_df['DATE_OF_BIRTH'] = pd.to_datetime(train_df['DATE_OF_BIRTH'])\ntrain_df['DISBURSAL_DATE'] = pd.to_datetime(train_df['DISBURSAL_DATE'])\n\ntrain_df['APPLICANT_AGE'] = ((dt - train_df['DATE_OF_BIRTH']) / 365).apply(lambda x: float(x.days))\ntrain_df['DISBURSAL_AGE'] = ((dt - train_df['DISBURSAL_DATE']) / 365).apply(lambda x: float(x.days))\n\ntrain_df = train_df.drop('DATE_OF_BIRTH', axis=1)\ntrain_df = train_df.drop('DISBURSAL_DATE', axis=1)\ntrain_df = train_df.drop('PERFORM_CNS_SCORE_DESCRIPTION', axis=1)\ntrain_df = train_df.drop('MOBILENO_AVL_FLAG', axis=1)\ntrain_df = train_df.drop('UNIQUEID', axis=1)\n\ntrain_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Outlier Handling"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain_df = removeOutlier(train_df, 'DISBURSED_AMOUNT')\ntrain_df = removeOutlier(train_df, 'ASSET_COST')\ntrain_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(train_df.corr(), ax=ax, vmin=0, vmax=1, cmap=\"YlGnBu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_df = train_df.corr()\nfor col in corr_df.columns[:]:\n    correlated_fields = list(filter(lambda x: x!=col, corr_df[(np.abs(corr_df[col])>.75)][col].index.values))\n    correlated_values = list(filter(lambda x: x!=col, corr_df[(np.abs(corr_df[col])>.75)][col].values))\n    if correlated_fields:\n        print(col, ':', ', '.join(map(str, zip(correlated_fields, correlated_values))), sep='\\t')\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> From the Correlation Graph and the code filter above, we see that the below fields are highly correlated\n>\n> -\tPRI_NO_OF_ACCTS\t:\tPRI_ACTIVE_ACCTS\n> -\tPRI_ACTIVE_ACCTS\t:\tPRI_NO_OF_ACCTS\n> -\tPRI_CURRENT_BALANCE\t:\tPRI_SANCTIONED_AMOUNT, PRI_DISBURSED_AMOUNT\n> -\tPRI_SANCTIONED_AMOUNT\t:\tPRI_CURRENT_BALANCE, PRI_DISBURSED_AMOUNT\n> -\tPRI_DISBURSED_AMOUNT\t:\tPRI_CURRENT_BALANCE, PRI_SANCTIONED_AMOUNT\n> -\tSEC_NO_OF_ACCTS\t:\tSEC_ACTIVE_ACCTS\n> -\tSEC_ACTIVE_ACCTS\t:\tSEC_NO_OF_ACCTS\n> -\tSEC_CURRENT_BALANCE\t:\tSEC_SANCTIONED_AMOUNT, SEC_DISBURSED_AMOUNT\n> -\tSEC_SANCTIONED_AMOUNT\t:\tSEC_CURRENT_BALANCE, SEC_DISBURSED_AMOUNT\n> -\tSEC_DISBURSED_AMOUNT\t:\tSEC_CURRENT_BALANCE, SEC_SANCTIONED_AMOUNT\n> -\tAVERAGE_ACCT_AGE\t:\tCREDIT_HISTORY_LENGTH\n> -\tCREDIT_HISTORY_LENGTH\t:\tAVERAGE_ACCT_AGE\n>\n> From the above, we can remove the below columns from the feature set\n> - PRI_NO_OF_ACCTS\n> - PRI_SANCTIONED_AMOUNT\n> - SEC_SANCTIONED_AMOUNT\n> - AVERAGE_ACCT_AGE\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.drop(['PRI_NO_OF_ACCTS', 'PRI_SANCTIONED_AMOUNT', 'SEC_SANCTIONED_AMOUNT', 'AVERAGE_ACCT_AGE'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"state_counts_df = train_df.groupby(['STATE_ID', 'LOAN_DEFAULT']).size().to_frame('counts').reset_index()\n\nstates_counts =[(ld, group) for ld, group in state_counts_df.groupby(['LOAN_DEFAULT'])]\n\nfor ld, state_df in states_counts:\n    state_df.columns=['STATE_ID', 'LOAN_DEFAULT', 'COUNT_%s'%bool(ld)]\n    \nstates_counts_df = states_counts[0][1].merge(states_counts[1][1], on='STATE_ID')\nstates_counts_df = states_counts_df.drop(['LOAN_DEFAULT_x', 'LOAN_DEFAULT_y'], axis=1)\nstates_counts_df['total']= states_counts_df['COUNT_False']+ states_counts_df['COUNT_True']\nstates_counts_df['COUNT_False']= states_counts_df['COUNT_False']/ states_counts_df['total']\nstates_counts_df['COUNT_True']= states_counts_df['COUNT_True']/ states_counts_df['total']\nstates_counts_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predicting and filling back in the EMPLOYMENT_TYPE Columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"'Employment Type Blanks: %d'% len(train_df[train_df['EMPLOYMENT_TYPE'].isna()])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n### Using a RandomForestClassifier to fill in Employment Type Field for missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['EMPLOYMENT_TYPE'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_columns = [column for column in train_df.columns if column not in ['LOAN_DEFAULT', 'EMPLOYMENT_TYPE', 'EMPLOYMENT_TYPE_code', 'DD', 'DOB']]\ntarget_variable = 'EMPLOYMENT_TYPE'\n\nemp_type_df = pd.DataFrame(train_df[~train_df[target_variable].isna()])\n\ndf_majority = emp_type_df[emp_type_df['EMPLOYMENT_TYPE']=='Self employed']\ndf_minority = emp_type_df[emp_type_df['EMPLOYMENT_TYPE']=='Salaried']\n\ndf_minority_upsampled = resample(df_minority, \n                                 replace=True,                  # sample with replacement\n                                 n_samples=len(df_majority),    # to match majority class\n                                 random_state=123)              # reproducible results\n\nemp_type_df = pd.concat([df_majority, df_minority_upsampled])\n\nemp_type_df['%s_code'%target_variable], employment_type_map = emp_type_df[target_variable].factorize()\n\nX_df = emp_type_df[feature_columns]\nY_df = emp_type_df['%s_code'%target_variable]\n\nx_train, x_test, y_train, y_test = train_test_split(X_df, Y_df)\nclassifier = RandomForestClassifier()\nclassifier.fit(x_train, y_train)\ny_predict = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_predict), accuracy_score(y_test, y_predict), f1_score(y_test, y_predict))\n\n\nto_pred_df = pd.DataFrame(train_df[train_df[target_variable].isna()])\nto_pred_df['%s_code'%target_variable] = -1\nto_pred_df['%s_code'%target_variable] = classifier.predict(to_pred_df[feature_columns])\ntrain_df[train_df[target_variable].isna()][target_variable] = to_pred_df['%s_code'%target_variable].apply(lambda x: employment_type_map[x])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Final Transformation Function:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_employement_type(df):\n    df = pd.DataFrame(df)\n    feature_columns = [column for column in df.columns if column not in ['LOAN_DEFAULT', 'EMPLOYMENT_TYPE', 'EMPLOYMENT_TYPE_code', 'DD', 'DOB']]\n    target_variable = 'EMPLOYMENT_TYPE'\n\n    emp_type_df = pd.DataFrame(df[~df[target_variable].isna()])\n    df_majority = emp_type_df[emp_type_df['EMPLOYMENT_TYPE']=='Self employed']\n    df_minority = emp_type_df[emp_type_df['EMPLOYMENT_TYPE']=='Salaried']\n\n    df_minority_upsampled = resample(df_minority, \n                                     replace=True,                  # sample with replacement\n                                     n_samples=len(df_majority),    # to match majority class\n                                     random_state=123)              # reproducible results\n\n    emp_type_df = pd.concat([df_majority, df_minority_upsampled])\n    emp_type_df['%s_code'%target_variable], employment_type_map = emp_type_df[target_variable].factorize()\n\n    X_df = emp_type_df[feature_columns]\n    Y_df = emp_type_df['%s_code'%target_variable]\n\n    x_train, x_test, y_train, y_test = train_test_split(X_df, Y_df)\n    classifier = RandomForestClassifier()\n    classifier.fit(x_train, y_train)\n    y_predict = classifier.predict(x_test)\n    print(confusion_matrix(y_test, y_predict), accuracy_score(y_test, y_predict), f1_score(y_test, y_predict))\n\n    to_pred_df = pd.DataFrame(df[df[target_variable].isna()])\n    to_pred_df['%s_code'%target_variable] = -1\n    to_pred_df['%s_code'%target_variable] = classifier.predict(to_pred_df[feature_columns])\n    df[df[target_variable].isna()][target_variable] = to_pred_df['%s_code'%target_variable].apply(lambda x: employment_type_map[x])\n    return df\n\ndef prepare(df):\n    df = pd.DataFrame(df)\n    df = df.apply(calculate_age, axis=1)\n    df = removeOutlier(df, 'DISBURSED_AMOUNT')\n    df = removeOutlier(df, 'ASSET_COST')\n\n    # Set data types\n    df['AVERAGE_ACCT_AGE'] = df['AVERAGE_ACCT_AGE'].astype(float)\n    df['CREDIT_HISTORY_LENGTH'] = df['CREDIT_HISTORY_LENGTH'].astype(float)\n    df['AADHAR_FLAG'] = df['AADHAR_FLAG'].astype(bool)\n    df['PAN_FLAG'] = df['PAN_FLAG'].astype(bool)\n    df['VOTERID_FLAG'] = df['VOTERID_FLAG'].astype(bool)\n    df['DRIVING_FLAG'] = df['DRIVING_FLAG'].astype(bool)\n    df['PASSPORT_FLAG'] = df['PASSPORT_FLAG'].astype(bool)\n    \n    # Parse Dates\n    df['DATE_OF_BIRTH'] = pd.to_datetime(df['DATE_OF_BIRTH'])\n    df['DISBURSAL_DATE'] = pd.to_datetime(df['DISBURSAL_DATE'])\n    \n    df['APPLICANT_AGE'] = ((dt - df['DATE_OF_BIRTH']) / 365).apply(lambda x: float(x.days))\n    df['DISBURSAL_AGE'] = ((dt - df['DISBURSAL_DATE']) / 365).apply(lambda x: float(x.days))\n    emp_type_df = pd.get_dummies(df['EMPLOYMENT_TYPE'], prefix='EMPLOYMENT_TYPE', drop_first=True)\n    df[emp_type_df.columns.tolist()] = emp_type_df\n    \n    # Dropping Columns\n    columns_to_drop = ['DATE_OF_BIRTH', 'DISBURSAL_DATE', 'PERFORM_CNS_SCORE_DESCRIPTION','MOBILENO_AVL_FLAG','UNIQUEID', 'PRI_NO_OF_ACCTS', 'PRI_SANCTIONED_AMOUNT', 'SEC_SANCTIONED_AMOUNT', 'AVERAGE_ACCT_AGE']\n    df = df.drop(columns_to_drop, axis=1)\n\n    #Fill back Employment Type\n    df = predict_employement_type(df)\n    df = df.drop('EMPLOYMENT_TYPE', axis=1)\n\n    return df\n\ntrain_df = pd.read_csv(training_csv)\ntrain_df = prepare(train_df)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Model: RandomForestClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_columns = ['DISBURSED_AMOUNT', 'ASSET_COST', 'LTV', 'BRANCH_ID', 'SUPPLIER_ID',\n       'MANUFACTURER_ID', 'CURRENT_PINCODE_ID', 'STATE_ID',\n       'EMPLOYEE_CODE_ID', 'AADHAR_FLAG', 'PAN_FLAG', 'VOTERID_FLAG',\n       'DRIVING_FLAG', 'PASSPORT_FLAG', 'PERFORM_CNS_SCORE',\n       'PRI_ACTIVE_ACCTS', 'PRI_OVERDUE_ACCTS', 'PRI_CURRENT_BALANCE',\n       'PRI_DISBURSED_AMOUNT', 'SEC_NO_OF_ACCTS', 'SEC_ACTIVE_ACCTS',\n       'SEC_OVERDUE_ACCTS', 'SEC_CURRENT_BALANCE', 'SEC_DISBURSED_AMOUNT',\n       'PRIMARY_INSTAL_AMT', 'SEC_INSTAL_AMT', 'NEW_ACCTS_IN_LAST_SIX_MONTHS',\n       'DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS', 'CREDIT_HISTORY_LENGTH',\n       'NO_OF_INQUIRIES',  'APPLICANT_AGE', 'DISBURSAL_AGE',\n       'EMPLOYMENT_TYPE_Self employed']\ntarget_column = 'LOAN_DEFAULT'\n\nprint('Before Resampling:')\nprint(train_df['LOAN_DEFAULT'].value_counts())\ndf_majority = train_df[train_df['LOAN_DEFAULT']==0]\ndf_minority = train_df[train_df['LOAN_DEFAULT']==1]\n\n\ndf_minority_upsampled = resample(df_minority, \n                                 replace=True,                  # sample with replacement\n                                 n_samples=len(df_majority),    # to match majority class\n                                 random_state=123)              # reproducible results\ntrain_df = pd.concat([df_majority, df_minority_upsampled])\nprint('After Resampling:')\nprint(train_df['LOAN_DEFAULT'].value_counts())\n\n\nX_df = train_df[feature_columns]\nY_df = train_df[target_column]\n\nx_train, x_test, y_train, y_test = train_test_split(X_df, Y_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using LogisticRegression"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier  = LogisticRegression()\nclassifier.fit(x_train, y_train)\ny_predict = classifier.predict(x_test)\nmodelEvaluation(classifier,x_test, y_train, y_test, y_predict, x_train.columns)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using DecisionTreeClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier  = DecisionTreeClassifier()\nclassifier.fit(x_train, y_train)\ny_predict = classifier.predict(x_test)\nmodelEvaluation(classifier,x_test, y_train, y_test, y_predict, x_train.columns)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using RandomForestClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = RandomForestClassifier()\nclassifier.fit(x_train, y_train)\ny_predict = classifier.predict(x_test)\nmodelEvaluation(classifier,x_test, y_train, y_test, y_predict, x_train.columns)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}