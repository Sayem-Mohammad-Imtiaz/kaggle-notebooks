{"cells":[{"metadata":{},"cell_type":"markdown","source":"The dataset consists of 10000 individuals and whether their credit card has defaulted or not. The main aim is to build the model using Logistic Regression and predict the accuracy of it . <br>\n\nAttributes:<br>\n    \nDefault : Yes or No (Whether defaulted or Not). <br>\nStudent : Yes or Nor (Whether Student or not). <br>\nBalance : Total Balance for given credit card holder.<br>\nIncome : Gross Annual Income of credit card holder.<br>\n\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats, integrate\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\n%matplotlib inline\npd.options.display.float_format = '{:.2f}'.format\nplt.rcParams['figure.figsize'] = (8, 6)\nplt.rcParams['font.size'] = 14\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cred_df=pd.read_csv(\"../input/attachment_default.csv\")\nimport re\ncred_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cred_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cred_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cred_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above graph, we can easily classify the majority of student by simply drawing a horizontal line (from balance =1250) that separate Yes  & no (default).This is nothimg but a linear classifier describing the relationship between input and output variable and proves that its a condition for the logistic regression. "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='default', y='income', data=cred_df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot(x='balance', y='income', hue = 'default', data=cred_df, aspect=1.5, fit_reg = False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**A scatter plot and box and whisker diagram seem to suggest that there is a relationship between credit card balance and default, while income is not related. The diagram also suggest that the default rates are higher when balance is high.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(cred_df['default'], cred_df['student'], rownames=['Default'], colnames=['Student'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generating Dummy variable\n\nThe dummy variable <b>default_yes</b> reflecting the class value 0 or 1. When class value is 1 then we have the default case. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert Categorical to Numerical\ndefault_dummies = pd.get_dummies(cred_df.default, prefix='default')\ndefault_dummies.drop(default_dummies.columns[0], axis=1, inplace=True)\ncred_df = pd.concat([cred_df, default_dummies], axis=1)\ncred_df.head()\n#default_dummies","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Building Linear Regression Model\nfrom sklearn.linear_model import LinearRegression\n    \nX = cred_df[['balance']]\ny = cred_df['default_Yes']\n\nlinreg = LinearRegression()\nlinreg.fit(X, y)\n\nprint(linreg.coef_)\nprint(linreg.intercept_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Even though through linear regression, we are getting value of coefficient and intercept from the equation but this is not correct because our Output is in 0 or 1. Whereas the regression equation is generating a value between 0 to 1. So this doesnot make any sense and same has been suggested through the lmplot where datapoints are plotted as 0 and 1.</b> "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot(x='balance', y='default_Yes', data=cred_df, aspect=1.5, fit_reg = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating logistic model for demonstration purpose with all data."},{"metadata":{"trusted":true},"cell_type":"code","source":"#calling logistic regression  ( fitting all the data for demonstration purpose. The training & test data is excuted after this.)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nlogreg = LogisticRegression()\nlogreg.fit(X, y)\nprint(logreg.coef_)\nprint(logreg.intercept_)\n\ny_pred = logreg.predict_proba(X)\nplt.scatter(X.values, y_pred[:,1])\n#plt.scatter(X.values, y)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we are intersted in only default =1, So we have plotted the only the y_pred[:,1], which gives us a sigmoid. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# probability of  (class 0 , class 1)\n\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# probability of class 0 only.\n\ny_pred[:,0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating logistic model with Train & Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"cred_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting the data into train and test with 70:30 ratio\nfrom sklearn.model_selection import train_test_split\nxTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size=0.30, random_state=13)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calling logistic regression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nlogreg = LogisticRegression(class_weight='balanced')\nlogreg.fit(X, y)\nprint(logreg.coef_)\nprint(logreg.intercept_)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fitting the model with x and y attributes of train data\n#in this it is goin to learn the pattern\nlogreg.fit(xTrain, yTrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now applying our learnt model on test and also on train data\ny_log_pred_test = logreg.predict(xTest)\ny_log_pred_train = logreg.predict(xTrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_log_pred_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_log_pred_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_log_pred_test\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CONFUSION MATRIX"},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating a confusion matrix to understand the classification\nconf = metrics.confusion_matrix(yTest, y_log_pred_test)\nconf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save confusion matrix and slice into four pieces\nconfusion = metrics.confusion_matrix(yTest, y_log_pred_test)\nprint(confusion)\n#[row, column]\nTP = confusion[1, 1]\nTN = confusion[0, 0]\nFP = confusion[0, 1]\nFN = confusion[1, 0]\nprint (\"TP\",TP)\nprint (\"TN\",TN)\nprint(\"FN\",FN)\nprint (\"FP\",FP)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0, as_cmap=True)\nsns.heatmap(conf,cmap = cmap,xticklabels=['predicted_default_yes=0','predicted_default_yes=1'],yticklabels=['actual_default_yes=0','actual_default_yes=1'],annot=True, fmt=\"d\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Result:**\n\nTrueNegative(TN) = 2508 cases, which are nondefault and predicted as nondefault as well.\n\nTruePositive(TP) = 88 cases, which are default and predicted as default as well.\n\nFalseNegative(FN) = 21 cases, which are actually default but predicted as nondefault.\n\nFalsePositive(FP) = 383 cases, which are actually nondefault but predicted as default.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the first 25 true and predicted responses\nprint('True', yTest.values[0:15])\nprint('Pred', y_log_pred_test[0:15])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Metrics computed from a confusion matrix"},{"metadata":{},"cell_type":"markdown","source":"**1.Classification Accuracy: Overall, how often is the classifier correct?** This is discussed above in  detail under the classification accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"#comparing the metrics of predicted lebel and real label of test data\nprint('Accuracy_Score:', metrics.accuracy_score(yTest, y_log_pred_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This suggest that 86.5% observations of credit defaults rates are correctly or accurately observe by our model. "},{"metadata":{},"cell_type":"markdown","source":"<b>2.Classification Error: Overall, how often is the classifier incorrect?</b>. It is nothing but (1-classification accuracy)\n\n**Also known as \"Misclassification Rate\"**"},{"metadata":{"trusted":true},"cell_type":"code","source":" # Method to calculate Classification Error\n    \n\nprint('Classification Error:',1 - metrics.accuracy_score(yTest, y_log_pred_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>3.Sensitivity or Recall:</b> When the actual value is positive, how often is the prediction correct? ."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Method to calculate Sensitivity\n\nprint('Sensitivity or Recall:', metrics.recall_score(yTest, y_log_pred_test))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**4.Specificity: When the actual value is negative, how often is the prediction correct?**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"specificity = TN / (TN + FP)\n\nprint(specificity)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>7.Classification Report.</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(yTest, y_log_pred_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cred_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Defining a sample data to test the model\n# As we discussed earlier, income has no significance in default. So only balance is considered as input & X = cred_df[['balance']]\n\nfeature_cols = ['balance']\ndata =[817.18]\nstudentid_2=pd.DataFrame([data],columns=feature_cols)\nstudentid_2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_default=logreg.predict(studentid_2)\nprint(predictions_default)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**output is zero means, the Studentid_2 is a nondefault case and not going to have default anytime soon.**"},{"metadata":{},"cell_type":"markdown","source":"# Adjusting the classification threshold"},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the first 10 predicted responses\n# 1D array (vector) of binary values (0, 1)\nlogreg.predict(xTest)[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the first 10 predicted probabilities of class membership\nlogreg.predict_proba(xTest)[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the first 10 predicted probabilities for class 1   ( predicting diabetic cases =1)\nlogreg.predict_proba(xTest)[0:10, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# store the predicted probabilities for class 1\ny_pred_prob = logreg.predict_proba(xTest)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_prob[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting predicion through histogram of predicted probabilities\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\n# 8 bins\nplt.hist(y_pred_prob, bins=8)\n\n# x-axis limit from 0 to 1\nplt.xlim(0,1)\nplt.title('Histogram of predicted probabilities')\nplt.xlabel('Predicted probability of default')\nplt.ylabel('Frequency')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Histogram suggest that the predicted probabilities are positively-skewed distribution with a long tail on right side and most of the probabilities are <0.1. So we going to change the threshold for probability from 0.5 to 0.1 with binarize function."},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict diabetes if the predicted probability is greater than 0.1\nfrom sklearn.preprocessing import binarize\nfrom sklearn.preprocessing import Binarizer\nfrom sklearn import preprocessing\n# it will return 1 for all values above 0.1 and 0 otherwise\n# results are 2D so we slice out the first column\n\ny_pred = binarize(y_pred_prob.reshape(-1,1), 0.1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# probability with revised threshold =0.1\n\ny_pred_prob[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Outcome with revised threshold =0.3\n\ny_pred[0:10]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# previous confusion matrix (default threshold of 0.5)\nprint(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" #The new confusion matrix (threshold of 0.1)\n    \nprint(metrics.confusion_matrix(yTest, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> We can see that earlier we are able to correctly classified (TP) 88 cases of diabetic. Now, we are able to correctly classify 106 diabetic cases by lowering the threshold.</b>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# sensitivity has increased (used to be 0.81)\nprint (106 / float(3 + 106))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" # specificity has decreased (used to be 0.86)\nprint(1812 / float(1812 + 1079))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>We are more interested in higher sensitivity value because false positives (normal transactions that are flagged as possible fraud) are more acceptable than false negatives (fraudulent transactions that are not detected).</b>"},{"metadata":{},"cell_type":"markdown","source":"\n# Receiver Operating Characteristic (ROC) Curves\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# IMPORTANT: first argument is true values, second argument is predicted probabilities\n\n# we pass y_test and y_pred_prob\n# we do not use y_pred, because it will give incorrect results without generating an error\n# roc_curve returns 3 objects fpr, tpr, thresholds\n# fpr: false positive rate\n# tpr: true positive rate\nfpr, tpr, thresholds = metrics.roc_curve(yTest, y_pred_prob)\n\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for default classifier')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# AUC "},{"metadata":{"trusted":true},"cell_type":"code","source":"# IMPORTANT: first argument is true values, second argument is predicted probabilities\n\nprint(metrics.roc_auc_score(yTest, y_pred_prob))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# END"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}