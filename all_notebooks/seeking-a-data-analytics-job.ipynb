{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1. Importing the relevant libraries and data\n\nLet us import all the relevant libraries and also the data upon which we are gonna perform EDA.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.express as px","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/data-analyst-jobs/DataAnalyst.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Data Cleaning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"At the very start, we would like to check for the presence of any null values in our data. To get a quick glance, we can use a heatmap that will give us a good idea of what features require extra attention with respect to missing values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df.isnull(),cmap='gnuplot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great ! It looks like there aren't any or extremely negligible missing values in the dataframe. However, instead of a null value, the data maybe replaced by keywords such as 'NA' or 'null'. We shall check for those later in our data cleaning operation.\n\nLet us confirm that there aren't any null values in the dataframe.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Oh wow ! There seems to be a few missing values in the company name column. However, since our data analysis wouldn't require much use of company name, hence we aren't bothered by it. However, let us check the number of missing values for curiosity sake.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Company Name'].isna().value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, there is just one single company name which is missing. We can safely dismiss the presence of any missing values since we shall be dropping the Companu Name column anyway.\n\nNext, we shall check the various data types that we will be dealing with. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Droppable columns\n\nAs we look at the various columns, we see that not all the columns are important to us. These can be dropped immediately to make the data less cluttered. Let's see which columns may be removed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"unn_col=['Unnamed: 0','Job Description','Company Name']\nfor cols in unn_col:\n    df.drop(cols,axis=1,inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Salary Estimate\n\nUpon glancing at the above data types, we see that there is an issue with salary estimate. Here, since the estimate is given to us in a range figure, we need to split the lower and upper bounds of the salaries into the max salary and min salary column. \n\nLet us solve the salary range issue now.\n\n\n**But wait ! There is something weird about the data. One of the entries have -1 which is weird as it doesn't make sense. Let us assume that -1 actually refers to null value. We shall replace this data with the mode of the data.**\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Salary Estimate'].mode()[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Salary Estimate']=df['Salary Estimate'].replace('-1',df['Salary Estimate'].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us first remove the source of the salary listing i.e. Glassdoor.\n\n**split(separator,max_splits)**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Salary Estimate'],_=df['Salary Estimate'].str.split('(',1).str","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Salary Estimate']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Min_Salary'],df['Max_Salary']=df['Salary Estimate'].str.split('-').str","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, we have separated the salaries into Minimum salary and maximum salary. However, there is still some unncecessary info the columns which we need to clean. Let us try to do that using the split function. In addition, it is important to change the datatype of salary from string to int. Let us attempt to do that aswell. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Min_Salary']=df['Min_Salary'].str.strip(' ').str.lstrip('$').str.rstrip('K').fillna(0).astype('int')\ndf['Max_Salary']=df['Max_Salary'].str.strip(' ').str.lstrip('$').str.rstrip('K').fillna(0).astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, we have successfully cleaned the data for max salary and min salary in their integer forms. \n\n## Missing Values\n\nAlthough we initially claimed that there were no missing values, it is now seen that in the dataframe, instead of null values, -1 has been entered. Hence, what we can do is replace all the -1 with null values. This will let us fill the null values with other suitable values such as mean or mode.\n\nLet us now create the heatmap again that will give us an idea of the number of missing values in each column. \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.replace('-1',np.nan,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df.isna(),cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Oh boy ! Extremely high number of missing values in competitors and easy apply columns. Considerable missing values in a few more columns.\n\nLet us check the number of missing values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"miss_values=[]\ndef check_null(df):\n    for i in range(df.columns.shape[0]):\n        if df.iloc[:,i].isnull().any():\n            print('Missing values in {} : {} '.format(df.columns[i],df.iloc[:,i].isna().value_counts()[1]))\n            miss_values.append(df.iloc[:,i].isna().value_counts()[1])\n            i+=1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"miss_val_arr=np.array(miss_values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"miss_val_arr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_null(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us make this into a more readable dataframe.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"null_cols=[]\nfor i in range(df.columns.shape[0]):\n    if df.iloc[:,i].isnull().any():\n        null_cols.append(df.columns[i])\nnull_arr=np.array(null_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"miss_val_arr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"miss_val=pd.DataFrame(null_arr)\nmiss_val.rename(columns={0:'Column name'},inplace=True)\nmiss_val['Missing values']=miss_val_arr\nmiss_val['Percentage missing (%)']=np.round(100* miss_val['Missing values']/df.shape[0],1)\nmiss_val","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There we have it ! As we can see, 96.45% of values in Easy apply are null while 76.88% of Competitor values are high. These are quite high.\n\nEasy apply could be an indication are currently open for the particular role in that company. So far, only about 3.6% listings are open to hire. \n\nRegarding the competitors column, I believe we can consider this as an unimportant section since it doesn't provide us with any insights. Hence, we shall drop this value too.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('Competitors',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to make the Easy Apply data more readable, let us fill all the null values with false. This shall clearly indicate that the value is false and the company isn't hiring at the moment.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Easy Apply']=df['Easy Apply'].fillna('False')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With this, I believe we have cleaned the data just enough for us to perform some insightful data visualisations.\n\n# 3. Data Visualisation\n\nAlright, first off, let's check how the jobs pay.\n\n## Salary Estimate\n\nLet us see what are the most common salaries paid to the Data Analysts in the United States.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Count']=1\ndf_salaries=df.groupby('Salary Estimate')['Count'].sum().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_salaries.sort_values(by='Count',ascending=False,inplace=True)\ndf_salaries.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsns.catplot('Salary Estimate','Count',data=df_salaries,height=5,aspect=3)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the data is extremely cluttered. It is seen the maximum jobs are paying in the range of 41K-78k $ a year to Data Analysts.\n\nIn order to make the data less cluttered, we shall consider the data for only the top 10 salary values. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_salaries_top=df_salaries.head(10)\nplt.figure(figsize=(10,8))\nplt.bar(df_salaries_top['Salary Estimate'],df_salaries_top['Count'],color=['red','blue','green','orange','brown','purple'])\nplt.xticks(rotation=45)\nplt.xlabel('Salary estimates',size=15)\nplt.ylabel('Number of jobs',size=15)\nplt.title('Top 10 salary estimates',size=20)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us see how the minimum and maximum salaries are exactly distributed through the distplots below.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig1=plt.figure(figsize=(10,5))\nax1=fig1.add_subplot(121)\ng=sns.distplot(df['Min_Salary'],ax=ax1,color='green')\nax1.set_xlabel('Minimum Salary \\n Median min salary:$ {0:.1f} K'.format(df['Min_Salary'].median()))\nl1=g.axvline(df['Min_Salary'].median(),color='red')\n\nax2=fig1.add_subplot(122)\nh=sns.distplot(df['Max_Salary'],ax=ax2,color='Red')\nax2.set_xlabel('Maximum_Salary \\n Median min salary:$ {0:.1f} K'.format(df['Max_Salary'].median()))\nl2=h.axvline(df['Max_Salary'].median(),color='Blue')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the minimum and max salaries are distributed differently. The vertical red and blue lines in the above distplots show the median salary in each section.\n\nLet us infact superimpose the two distplots to understand their difference.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig2=plt.figure(figsize=(20,10))\nax1=fig2.add_subplot(111)\n\n#Plots\n\n\ng=sns.distplot(df['Min_Salary'],ax=ax1,color='green',label='Minimum salary')\nh=sns.distplot(df['Max_Salary'],ax=ax1,color='Red',label='Maximum salary')\n\n# Vertical median lines\nl1=g.axvline(df['Min_Salary'].median(),color='black',label='Median min salary')\nl2=h.axvline(df['Max_Salary'].median(),color='Blue',label='Median max salary')\n\n#Font descriptions\n\nplt.xlabel('Salary distribution',size=20)\nplt.title('Min/Max salary distribution',size=20)\n\n#Legend box\nplt.legend(fontsize='x-large', title_fontsize='40')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Current job openings\n\nCurrent job openings can be understood through the Easy Apply column. As we have already established in the past, there are very few companies (about 3.6 %) which are currently open to recruit. Let us check which industries and cities are reporting the maximum job openings right now.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ea_ind=df[df['Easy Apply']=='True']\ndf_ea_ind_grouped=df_ea_ind.groupby('Industry')['Easy Apply'].count().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ea_ind_grouped.sort_values(by='Easy Apply',ascending=False,inplace=True)\ndf_ea_ind_grouped","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsns.catplot('Industry','Easy Apply',data=df_ea_ind_grouped,kind='bar',height=10,aspect=2)\nplt.xticks(rotation=90,size=15)\nplt.ylabel('Job openings',size=20)\nplt.xlabel('Industry',size=20)\nplt.title('Current job openings in the industry',size=25)\nticks=np.arange(20)\nplt.yticks(ticks,fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see from the above barplot, Staffing & Outsourcing has the highest number of vacancies (about 18).\n\nLet us now check which cities are showing the highest number of job postings.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ea_loc=df_ea_ind.groupby('Location')['Easy Apply'].count().reset_index()\ndf_ea_loc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ea_loc.sort_values('Easy Apply',ascending=False,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot('Location','Easy Apply',data=df_ea_loc,kind='bar',height=10,aspect=2,palette='summer')\nplt.xticks(rotation=90,size=15)\nplt.ylabel('Job openings',size=20)\nplt.xlabel('Location',size=20)\nplt.title('Current job openings in different locations',size=25)\nticks=np.arange(20)\nplt.yticks(ticks,fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above barplot, we can see that NY,SF and Chicago are the top 3 cities with job postings.\n\n## Location wise mean salaries\n\nLet us check how the mean salaries in the cities range. We will try to find the top 10 locations with highest mean maximum salaries.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_salaries=df.groupby('Location')[['Min_Salary','Max_Salary']].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_salaries.sort_values(by='Max_Salary',ascending=False,inplace=True)\ndf_salaries=df_salaries.head(10)\ndf_salaries","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we have the top 10 cities with highest mean maximum salaries. Let us depict this through a visualisation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig3=go.Figure()\nfig3.add_trace(go.Bar(x=df_salaries.index,y=df_salaries['Min_Salary'],name='Minimum Salary'))\nfig3.add_trace(go.Bar(x=df_salaries.index,y=df_salaries['Max_Salary'],name='Maximum Salary'))\n\nfig3.update_layout(title='Top 10 cities with mean minimum and maximum salaries',barmode='stack')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above stacked graph, we see that Newark provides the highest mean maximum salary to their Data Analysts. The also provide the second highest minimum salary of the top 10 destinations for Data Analysts. Hence, Newark can be said to be the most idea location for Data Analysts.\n\n\n## Industry wise mean salaries\n\nLet us check the mean maximum and minimum salaries. We shall find the top 10 industries providing the highest mean maximum salaries to the data analysts.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sal_ind=df.groupby('Industry')[['Min_Salary','Max_Salary']].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf_sal_ind=df_sal_ind.sort_values('Max_Salary',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sal_ind=df_sal_ind.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig4=go.Figure()\nfig4.add_trace(go.Bar(x=df_sal_ind.index,y=df_sal_ind['Min_Salary'],name='Minimum Salary'))\nfig4.add_trace(go.Bar(x=df_sal_ind.index,y=df_sal_ind['Max_Salary'],name='Maximum Salary'))\n\nfig4.update_layout(title='Top 10 Industries with mean minimum and maximum salaries in $',barmode='stack')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Job Ratings\n\nJob ratings might be an important indicator for job searchers. Let us check how the various rated jobs are distributed. We will visualise the top 10 common job ratings.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_rate=df.groupby('Rating')['Count'].sum().reset_index()\ndf_rate.sort_values(by='Count',ascending=False,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_rate=df_rate.iloc[1:,:].head(10)  #Since we are discounting the null values given by -1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsns.catplot('Rating','Count',data=df_rate,kind='bar',palette='winter',height=5,aspect=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that most number of jobs have a rating of 3.9 . A fair number of jobs also have a rating of 5.\n\n\n# If you found the kernel useful, an upvote would be great ! :)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}