{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/churn-modelling/Churn_Modelling.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above it is clear that RowNumber; CustomerID, Surname are not required for training the model, and will not play any role. Also these are just a unique for a Customer.\nSo will ignore them, and consider rest of the features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.iloc[:,3:13]\ny = df.iloc[:, 13]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also observed that Geography and Gender are Categorical features, so we have to make it Numerical using dummies method.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X.Geography.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X['Gender'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"geo_cat = pd.get_dummies(X[\"Geography\"], drop_first = True)\ngender_cat = pd.get_dummies(X['Gender'], drop_first = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"geo_cat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge geo_cat and gender_cat into our X.\n\nX = pd.concat([X, geo_cat, gender_cat], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now have to drop original features, we now we have numerical format for those.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.drop(['Geography',\"Gender\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now splitting the dataset into the training and test split using sklearn","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature Scaling: ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 2: Working on DL","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n# from keras.layers import LeakyReLU, PReLU, ELU\n# from keras.layers import Dropout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initializing the ANN\nclf = Sequential()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While defining Dense layer we have to pass some parameters / arguments such as\n* `units` which is to define how many neurons we need in the hidden layer. So here we are taking it as 6. This is just a randon guess. But using Hyper Optimization we may get to the exact count of neurons required.\n* `kernel_initializer` - Initializer for the kernel weights matrix. Here we are going with `he_uniform` as this works well with relu activation function.\n* `activation` - using relu. Generally for hidden layer relu or leaky relu is uased.. and in the output layer we may use sigmoid or softmax. As relu helps in Vanishing Gradient Problem.\n* `input_dim` - Here we have taken it as 11, and this is the count of features which we are passing to the model. Check X_train.shape","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding the Input layer and the first hidden layer.\nclf.add(Dense(units = 6, kernel_initializer = 'he_uniform', activation = 'relu', input_dim = 11 ))\n# clf.add(Dense(output_dim = 6, init = 'he_uniform', activation = 'relu', input_dim = 11 )) # Parameter name chaned refer to https://keras.io/api/layers/core_layers/dense/","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we are defining the second layer we will not require the `input_dim`.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding the second hidden layer\nclf.add(Dense(units = 6, kernel_initializer = 'he_uniform', activation = 'relu' ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding Output Layer\nclf.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So till here we had made a model with 1 Input Layer; 2 Hidden Layers; and 1 Output Layer.\nIn Input Layer we have 11 Neurons (which are my features); and first Dense Layer we do have 6 Neurons; and in 2nd hidden layer as well we do have 6 neurons; finally in the output layer we have just one neuron.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classifier or Model Summary.\nclf.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once the model is created, you can config the model with losses and metrics with `model.compile()`, train the model with `model.fit()`, or use the model to do prediction with `model.predict()`.\nRefer https://keras.io/api/models/model/","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"So to compile as well we have to set some parameters such as \n* `optimizer` - adam is one of the most popular one so using it. Other optimizer can be found at https://keras.io/api/optimizers/\n* `loss` - The purpose of loss functions is to compute the quantity that a model should seek to minimize during training. Based on the problem we are solving there are various loss functions. Here we are using binary_crossentropy. Use this cross-entropy loss when there are only two label classes (assumed to be 0 and 1). For each example, there should be a single floating-point value per prediction. Refer site for more details https://keras.io/api/losses/ \n* `metrics` - The compile() method takes a metrics argument, which is a list of metrics. A metric is a function that is used to judge the performance of your model. Metric functions are similar to loss functions, except that the results from evaluating a metric are not used when training the model. Note that you may use any loss function as a metric. Various metrics are available, out of which we are using accuracy.Refer https://keras.io/api/metrics/","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compiling the model\nclf.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the `model.fit()` method we do have some parameters which we are going to use.\n   \n* X : our training data. Can be Vector, array or matrix      \n* Y : our training labels. Can be Vector, array or matrix   \n* validation_split : to split the provided dataset into 2 sets one for training and another for validation.    \n* Batch_size : it can take any integer value or NULL and by default, it will be set to 32. It specifies no. of samples per gradient.      \n* Epochs : an integer and number of epochs we want to train our model for.      \n* Verbose : specifies verbosity mode(0 = silent, 1= progress bar, 2 = one line per epoch).      \n* Shuffle : whether we want to shuffle our training data before each epoch.      \n* steps_per_epoch : it specifies the total number of steps taken before one epoch has finished and started the next epoch. By default it values is set to NULL.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the model.\nclf_history = clf.fit(X_train, y_train, validation_split = 0.33, batch_size = 10, epochs = 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above output we can see that the model is getting trained and also displaying the validation accuray `val_accuracy` after each epoch.\nAlso we have `loss` and `accuracy` calculated on train dataset, and on the validation dataset as `val_loss` and `val_accuracy`.\nOn comparing the two accuracy we can say that the model was doing pretty good.. and both are nearby.. and not huge difference. If having huge difference then some issue can be considered.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Lets see what our clf_history is hodling..","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_history","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"for `tf.keras.callbacks.History` refer https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_history.history.keys()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This looks something familiar... its a dictionary.\nand it has the same values which we were seeing loss, accuracy, val_loss, val_accuracy during rum-time.\nLets use it to plot a visualization graph.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(clf_history.history['accuracy'])\nplt.plot(clf_history.history['val_accuracy'])\n\nplt.title('Model Accuracy')\n\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\n\nplt.legend(['Train', 'Validation'], loc = 'best')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Other options for loc in legend:\n    best\n\tupper right\n\tupper left\n\tlower left\n\tlower right\n\tright\n\tcenter left\n\tcenter right\n\tlower center\n\tupper center\n\tcenter","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"From visualization we can see that it was increasing...","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Similar to Accuracy.. we can visualize for Loss.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(clf_history.history['loss'])\nplt.plot(clf_history.history['val_loss'])\n\nplt.title('Model Loss')\n\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\n\nplt.legend(['Train', 'Validation'], loc = 'best')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using Test Dataset to predict\nNow lets test the model with our test dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets set the threshold... if less than 0.5 than set it to false.\ny_pred = (y_pred > 0.5)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets see the accuracy of our Test Dataset.\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculate the accuracy on test dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nscore = accuracy_score(y_pred, y_test)\n\nscore","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we have seen that the accuracy on\n\n* Train Dataset was = 0.8690 \n* Validation Dataset = 0.8614\n* Test Dataset = 0.859\n\nand all are approx nearby... which is the best way to see if model is doing good.. or if not then there is ovefit issue.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We can play around with the parameters in the model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This time lets take 3 hidden layes and change the number of neurons in each hidden layer.\nAlso changing the kernel_initializer.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf2 = Sequential()\n\n# Adding the Input layer and the first hidden layer.\nclf2.add(Dense(units = 10, kernel_initializer = 'he_normal', activation = 'relu', input_dim = 11 ))\n\n\n# Adding the second hidden layer\nclf2.add(Dense(units = 20, kernel_initializer = 'he_normal', activation = 'relu' ))\n\n# Adding the third hidden layer\nclf2.add(Dense(units = 15, kernel_initializer = 'he_normal', activation = 'relu' ))\n\n\n# Adding Output Layer\nclf2.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))\n\n# Compiling the model\nclf2.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Fit the model.\nclf2_history = clf2.fit(X_train, y_train, validation_split = 0.33, batch_size = 10, epochs = 100)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = clf2.predict(X_test)\ny_pred = (y_pred > 0.5)\nscore = accuracy_score(y_pred, y_test)\nscore","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the change of parameters, we observe that the accuracy has changed.\n\n* Train Dataset was = 0.8690 --> 0.8750\n* Validation Dataset = 0.8614 --> 0.8504 (seems gone down.. but a very small difference.)\n* Test Dataset = 0.859 --> 0.845  (seems gone down.. but a very small difference.)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Besure not to add to many hidden layers.. as it will lead to overfitting of the data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We can also try using the dropout layer after each hidden layer.. so as to drop some neuorns based on some thereshold.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dropout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf3 = Sequential()\n\n# Adding the Input layer and the first hidden layer.\nclf3.add(Dense(units = 10, kernel_initializer = 'he_normal', activation = 'relu', input_dim = 11 ))\n\n# Add dropout layer\nclf3.add(Dropout(0.3)) # This is just a random threshold as of now\n\n# Adding the second hidden layer\nclf3.add(Dense(units = 20, kernel_initializer = 'he_normal', activation = 'relu' ))\n\n# Add dropout layer\nclf3.add(Dropout(0.4)) # This is just a random threshold as of now\n\n# Adding the third hidden layer\nclf3.add(Dense(units = 15, kernel_initializer = 'he_normal', activation = 'relu' ))\n\n# Add dropout layer\nclf3.add(Dropout(0.2)) # This is just a random threshold as of now\n\n# Adding Output Layer\nclf3.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))\n\n# Compiling the model\nclf3.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Fit the model.\nclf3_history = clf3.fit(X_train, y_train, validation_split = 0.33, batch_size = 10, epochs = 100)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}