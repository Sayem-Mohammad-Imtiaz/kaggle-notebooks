{"nbformat_minor":1,"metadata":{"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"name":"python","file_extension":".py","mimetype":"text/x-python","version":"3.6.1","pygments_lexer":"ipython3","nbconvert_exporter":"python"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"cells":[{"outputs":[],"metadata":{"_uuid":"9dbd8c3357b189fb73b9a67233cde3cd259e8241","_cell_guid":"d40080d3-9812-47ea-90ec-0eccd3e80846"},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1},{"metadata":{"_uuid":"7faa67b64357a808955b1ef067a3b1dd9d4d9664","_cell_guid":"63efeedc-0194-4c08-8b9f-d325491c688e"},"cell_type":"markdown","source":"# Introduction\nThe purpose of this kernel is to see how the neural network from the [first project](https://github.com/udacity/deep-learning/tree/master/first-neural-network) in the [Deep Learning Nanodegree](https://www.udacity.com/course/deep-learning-nanodegree-foundation--nd101) performs.\n\nThe purpose of that neural network was also to predict bike-sharing demand, but for the [UCI Bike Sharing Dataset](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset).\n\n# Data Preparation\n## Importing the Trips Data"},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"72a1b2214740dec411d26ae74130a41384d4058b","_cell_guid":"5bb234c1-491c-46f0-9829-06000819df82"},"cell_type":"code","source":"trips_df = pd.read_csv(\"../input/austin-bike/austin_bikeshare_trips.csv\")\nstations_df = pd.read_csv(\"../input/austin-bike/austin_bikeshare_stations.csv\")\nweather_df = pd.read_csv(\"../input/austin-weather/austin_weather.csv\")","execution_count":2},{"outputs":[],"metadata":{},"cell_type":"code","source":"weather_df.columns","execution_count":3},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"01821fb4f7ed88a2ec3e9c2a5207be40079d7ec2","_cell_guid":"b8e297b5-7068-46aa-8c3e-4ab111c5a123"},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":null},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"56594c4840165653237ee06ba3b8de36af436d13","_cell_guid":"9a364725-8787-42ad-9900-27c8e66ad406"},"cell_type":"code","source":"trips_df.head()","execution_count":null},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"bbd34e5a91a5215b9f17b6785489b451aec2b197","_cell_guid":"c44d59d1-8f83-4503-a18c-311bd468ccb3"},"cell_type":"code","source":"stations_df.head()","execution_count":null},{"metadata":{"_uuid":"2a5e3fc525cc70c5e832d5c24856f21c1d08854f","_cell_guid":"06f2aa35-e16e-4256-ab90-d82389cd60f9"},"cell_type":"markdown","source":"## Importing the Weather Data\nNext, we'd like to reshape this data so that it looks more like the UCI dataset.  \nThe head() of that data looks like:\n![UCI Bike Sharing Dataset](http://i.imgur.com/qtVLSZe.png)"},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"4b622e88af8ab1614dfb8218eb7935f08e048c98","_cell_guid":"0b550f43-8785-412d-bf60-ee27de0429dd"},"cell_type":"code","source":"trips_df.start_time.min(), trips_df.start_time.max()","execution_count":null},{"metadata":{"_uuid":"ca61a7ffed1635bb569bb37c446ff79ccfac75ef","_cell_guid":"ccbc31d0-bb8a-46c4-b6bf-9f4c4392213e"},"cell_type":"markdown","source":"Notably, this will include weaving in historical weather data, since that is not included in the Austing Bikeshare dataset.\n\nWe have some choices on which weather datasets to use, but for ease of downloading, and instructional value, let's work with the data that WeatherUnderground provides for the [Austin KATT](https://www.wunderground.com/history/airport/KATT/) station."},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"19450ed20889da51109aeae0ab54d93412d34b22","_cell_guid":"5adecd60-e6bd-4e76-ad43-14aa40fbbd05"},"cell_type":"code","source":"weather_df.head()","execution_count":null},{"metadata":{"_uuid":"cf09ff16f2bfd3aad4fe0e06fde288def25cb841","_cell_guid":"df0f7e3d-2e96-4d04-9a7c-06cb47104a52"},"cell_type":"markdown","source":"Unfortunately, we're not going to have the temperature resolution from the WeatherUnderground dataset (daily) that we have from the UCI Bikesharing Dataset (hourly), but hopefully it should still be useful to us."},{"metadata":{"collapsed":true,"_uuid":"f27f3f016cfd961927a8c384cd5b7692e2ed35fc","_cell_guid":"cdfe6887-aa72-4e1c-b1f6-df9d5ed97c6a"},"cell_type":"markdown","source":"## Preparing the trips data\nLet's see if we can get an idea of how the trip are distributed"},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"59dbd3ec3c692617febbdc688dce046906a1bd78","_cell_guid":"8d134608-6bda-4a29-a6d8-c5a4da7e2933"},"cell_type":"code","source":"trips_df.start_time.head()","execution_count":null},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"8c143eeb195b445fc7caa0da0f6140df9ea928ed","_cell_guid":"ac81205e-a92c-4da4-98bf-f907ab0df5e6"},"cell_type":"code","source":"type(trips_df.start_time[0])","execution_count":null},{"metadata":{"_uuid":"2a24fafa58ad4dc2e9181233c6094c9f5fd0dd77","_cell_guid":"7848f716-e4ba-46da-81db-eefe4beed483"},"cell_type":"markdown","source":"To better make use of this data, we'll want to convert it into a Python `datetime` object."},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"c57eaa3baa0c67ec23c76d1db2381b5213a3ccb9","_cell_guid":"9569e5cb-9deb-405f-a566-d046a17845b7"},"cell_type":"code","source":"import datetime","execution_count":null},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"1b6aef57cbfac6607f288f1bd12cdfd3c477b2e7","_cell_guid":"bec2bda3-559b-4897-95ac-49f9ae929635"},"cell_type":"code","source":"def toDatetime(s):\n    return datetime.datetime.strptime(s, \"%Y-%m-%d %H:%M:%S\")","execution_count":null},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"84e209381b1e6f2c5e502e460ba5de984a1d54fd","_cell_guid":"e23eb9ab-0567-4a5f-831d-8d378c4a565c"},"cell_type":"code","source":"trips_df[\"start_time\"] = trips_df.start_time.apply(toDatetime, input)","execution_count":null},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"cf0fa3d157d4709aeae13995e042cd11c41c34d7","_cell_guid":"dcd0f0c8-6562-48c3-b9ad-3e74b1028e2a"},"cell_type":"code","source":"trips_df.start_time.head()","execution_count":null},{"metadata":{"_uuid":"d002851f794ebe230563a8414b18f6c44ee0121e","_cell_guid":"68128767-01c1-499f-987c-a201b37cdea7"},"cell_type":"markdown","source":"Now, let's add a column that will allow us to easily sum the trips by the hour during which they started."},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"a9f0a2714d2c9b64994d74455c882de2799f5c43","_cell_guid":"c5e38963-9aba-4e6c-ab72-27754616ea88"},"cell_type":"code","source":"trips_df[\"start_hour\"] = trips_df.start_time.apply(lambda x: datetime.datetime(x.year, x.month, x.day, x.hour))","execution_count":null},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"27a70a5450e2e310b7e25921c02bc6e64d5b514b","_cell_guid":"b2ec8df3-1835-42c0-a7bf-ea32c191512f"},"cell_type":"code","source":"trips_df.start_hour.head()","execution_count":null},{"metadata":{"_uuid":"dc10b9776234e57878951a9eb0e08af279a9b3d2","_cell_guid":"7081e99f-5b57-4012-9352-e3b22156a12a"},"cell_type":"markdown","source":"### Adding Zeroes\nWe'll want to plot a distribution of the trip start times and frequencies, but we're missing some hours in our dataset (those hours where no one checked out a bike).\n\nSo, let's create a `date_range` to allow us to reindex the `start_hour` Series as a separate Series."},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"415fa70644a6802014655a9c6d6021879b0ee1e5","_cell_guid":"1d11f24d-1e17-43bd-9a7d-860c5d6428b4"},"cell_type":"code","source":"all_trip_hours = pd.date_range(start='2013-12-21', end='2017-07-31 23:00:00', freq='H')","execution_count":null},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"0be165df2b11c4b978e9e9bfd4f961734a3f328c","_cell_guid":"893ec146-55d9-4935-8568-eeb29120377f"},"cell_type":"code","source":"all_trip_hours[:5]","execution_count":null},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"5a58beb0d393def2ed6ca5508dd18efcde881784","_cell_guid":"5dd6b7ab-adb8-4fba-918e-91baef5313d8"},"cell_type":"code","source":"trips_starts_grouped = trips_df.start_hour.value_counts()","execution_count":null},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"f0ddb96ebd73cf020eb361d858952cd03adb550b","_cell_guid":"787dff10-bf60-477c-81cd-65188bc84de2"},"cell_type":"code","source":"trips_starts_grouped.sort_index(inplace=True)","execution_count":null},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"636a7943756e430cb811bde54d2ca306a7d95b27","_cell_guid":"fae49387-08e0-401f-b06e-9a30ea8a6fd4"},"cell_type":"code","source":"trips_starts_grouped.head()","execution_count":null},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"088bb066360b187dab04a4bd4bc144e7df7675bf","_cell_guid":"0649201c-5c62-468b-b48f-bcd9ce687a85"},"cell_type":"code","source":"trips_starts_grouped = trips_starts_grouped.reindex(all_trip_hours, fill_value=0)","execution_count":null},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"c270939472e0e4bcfa554c91b7dc897ab18b82c0","_cell_guid":"52af2c40-58ac-4b07-8a53-8098ce04147c","scrolled":true},"cell_type":"code","source":"trips_starts_grouped[:24*7].plot(figsize = (15, 10), ylim=(0,120))","execution_count":null},{"metadata":{"_uuid":"d01b987fa2ca34c7eee2e998645e66408d7af349","_cell_guid":"4d7dddb4-e490-43b8-9a0b-2ec20a4114fa"},"cell_type":"markdown","source":"The above is Christmas week, so perhaps that's not very representative, but it's enough to at least show that there is a cyclical nature to the data.\n\nWhat would some week in March look like?"},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"3c4a427bc6a83b3f028af71dfeac14ed3d936208","_cell_guid":"5c704b79-afb4-41b6-a170-833aec400d61"},"cell_type":"code","source":"offset = 24*30*3\ntrips_starts_grouped[offset : offset + 24*7].plot(figsize = (15, 10), ylim=(0,120))","execution_count":null},{"metadata":{"collapsed":true,"_uuid":"c676b9e2dd3a64f2f00e7ffc6a9615c1760c12bc","_cell_guid":"03cfa60f-83ad-4190-8434-5f7a44263e79"},"cell_type":"markdown","source":"That makes a lot more sense, but this is still a holiday week.  Let's check out a week in late September."},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"offset = 24*(30*9 - 3) \ntrips_starts_grouped[offset : offset + 24*7].plot(figsize = (15, 10), ylim=(0,120))","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's assume that the above is a pretty typical week for ridership.  \nIt looks like there's some after-midnight rentals, and typically higher weekend ridership than weekday ridership.  \n### Cleaning the Data\nFor the purposes of this analysis, we're after a dataset that approximates the UCI dataset:\n![UCI Bike Sharing Dataset](http://i.imgur.com/qtVLSZe.png)"},{"metadata":{},"cell_type":"markdown","source":"### Choosing Variables\nAccordingly, from our dataset, let's define the following **dummy variables**:\n* season (implicit)\n* month\n* hour\n* day of the week (implicit)\n* which weather events occurred\n\nAnd the following **quantitative variables**:\n* temperature\n* humidity\n* windspeed"},{"metadata":{},"cell_type":"markdown","source":"### Creating Seasons\nFirst, let's define a function that will allow us to map `datetime` objects to seasons, so that we can add that as a column to our rides dataframe."},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"def timestampToSeason(ts):\n    \"\"\"\n    Given a pandas Timestamp object, return which 'season' the date is in.\n    \n    Arbitrarily, to match the UCI dataset:\n    1 -> spring\n    2 -> summer\n    3 -> fall\n    4 -> winter\n    \n    returns:\n    integer\n    \"\"\"\n    dt = ts.to_pydatetime().date()\n    \n    # Our only dates in 2013 are from Dec 21 onward\n    year = dt.year\n    if year == 2013:\n        return 4\n    else:\n        # Check out the wonderful [wikipedia Seasons page]\n        # (https://en.wikipedia.org/wiki/Season)\n        eq_sol_by_year = {2014:\n                          [datetime.date(2014, 3, 20),\n                           datetime.date(2014, 6, 21),\n                           datetime.date(2014, 9, 23),\n                           datetime.date(2014, 12, 21)],\n                         2015:\n                          [datetime.date(2015, 3, 20),\n                           datetime.date(2015, 6, 21),\n                           datetime.date(2015, 9, 23),\n                           datetime.date(2015, 12, 22)],\n                         2016:\n                          [datetime.date(2016, 3, 20),\n                           datetime.date(2016, 6, 20),\n                           datetime.date(2016, 9, 22),\n                           datetime.date(2016, 12, 21)],\n                         2017:\n                          [datetime.date(2017, 3, 20),\n                           datetime.date(2017, 6, 21),\n                           datetime.date(2017, 9, 22),\n                           datetime.date(2017, 12, 21)]}\n        \n        if dt < eq_sol_by_year[year][0] or dt >= eq_sol_by_year[year][3]:\n            return 4\n        elif dt < eq_sol_by_year[year][1]:\n            return 1\n        elif dt < eq_sol_by_year[year][2]:\n            return 2\n        else: # dt < eq_sol_by_year[year][3]\n            return 3","execution_count":null},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"print(trips_df.start_time[0])\ntimestampToSeason(trips_df.start_time[0])","execution_count":null},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"trips_df[\"season\"] = trips_df.start_time.apply(timestampToSeason)","execution_count":null},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"trips_df.head()","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Creating Weekdays\nNext, let's use a similar approach from above to add a weekday column to our data."},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"def timestampToWeekday(ts):\n    \"\"\"\n    Given a pandas Timestamp object, return which weekday that corresponds to,\n    according to the python datetime library.\n    \n    https://docs.python.org/3.0/library/datetime.html\n        \n    returns:\n    integer\n    \"\"\"\n    return ts.to_pydatetime().weekday()","execution_count":null},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"trips_df[\"weekday\"] = trips_df.start_time.apply(timestampToWeekday)","execution_count":null},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"trips_df.head()","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Creating Categorical Data from Weather Events"},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"weather_df.Events.unique()","execution_count":null},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"# major props to https://datascience.stackexchange.com/a/14851\ncleaned = weather_df.Events.str.split(' , ', expand = True).stack()","execution_count":null},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"cleaned.tail(15)","execution_count":null},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"res = pd.get_dummies(cleaned).groupby(level=0).sum()","execution_count":null},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"res.columns","execution_count":null},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"del res[' ']","execution_count":null},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"res.tail(15)","execution_count":null},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"res1 = weather_df.join(res)","execution_count":null},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"weather_df.","execution_count":null},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"weather_df.head()","execution_count":null},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"weather_df.iloc[1]","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Dropping columns\nTo clean up our "},{"metadata":{},"cell_type":"markdown","source":"### Normalizing Quantitative Data\nNow, our last data preparation step will be to normalize our quantitative data [from above](#Choosing-Variables).\n\nWe'll do what the [UCI bikesharing dataset](https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset) did here.\n\nThat is:  \n* For temperature, we'll subtract t_min (19) from each temp, then divide by (t_max - t_min), where t_max here is 107.  \n  * We'll use `Temp-high-f` and `Temp-low-f`.\n* For humidity, we'll divide by 100, the max possible and in our dataset.\n  * We'll use `Humidity-high-percent` and `Humidity-low-percent`.\n* For wind speed, we'll divide by the max in our dataset (29).\n  * We'll use `Wind-high-mph` and `Wind-avg-mph`."},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"","execution_count":null}],"nbformat":4}