{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* > # EE226 - Coding 2\n## Streaming algorithm & Locality Sensitive Hashing","metadata":{}},{"cell_type":"markdown","source":"### Streaming: DGIM","metadata":{}},{"cell_type":"markdown","source":"DGIM is an efficient algorithm in processing large streams. When it's infeasible to store the flowing binary stream, DGIM can estimate the number of 1-bits in the window. In this coding, you're given the *stream_data.txt* (binary stream), and you need to implement the DGIM algorithm to count the number of 1-bits. Write code and ask the problems below.","metadata":{}},{"cell_type":"markdown","source":"### Your task","metadata":{}},{"cell_type":"markdown","source":"1. Set the window size to 1000, and count the number of 1-bits in the current window.","metadata":{}},{"cell_type":"code","source":"size_window = 1000\ntime_location = 1000 #假设当前窗从首个字符开始\n\ndef Count_bit_act():\n    bit_sum = 0  # 统计1-bit个数\n    with open('../input/coding2/stream_data.txt', 'r') as f:\n        f.seek(0 if time_location <= size_window else 2 * (time_location - size_window))  # 跳转到窗口大小之前行位置\n        num = f.read()\n        num = num.split()\n        for i in range(time_location if time_location <= size_window else size_window):\n            temp=num[i]\n            if temp and int(temp) == 1:\n                bit_sum += 1\n    return bit_sum\n\nbit_act_sum= Count_bit_act()\n\nprint(\"The number of \\\"1\\\" is:\",bit_act_sum)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Write a function that accurately counts the number of 1-bits in the current window, and compare the difference between its running time and space and the DGIM algorithm.","metadata":{}},{"cell_type":"code","source":"import time\nimport psutil\nimport os\n\nbucket_n = []  # 桶的列表\nn_max_bucket = 2\nsize_window = 1000 #假设窗长1000\ntime_location = 1000 #假设窗从首个字符开始\n\ndef show_info(start):\n    pid = os.getpid()\n    #模块名比较容易理解：获得当前进程的pid\n    p = psutil.Process(pid)\n    #根据pid找到进程，进而找到占用的内存值\n    info = p.memory_full_info()\n    memory = info.uss/1024\n    return memory\n\ndef Count_bit_act():\n    bit_sum = 0  # 统计1-bit个数\n    start_time = time.time()\n    start = show_info('start')\n\n    with open('../input/coding2/stream_data.txt', 'r') as f:\n        f.seek(0 if time_location <= size_window else 2 * (time_location - size_window))  # 跳转到窗口大小之前行位置\n        num = f.read()\n        num = num.split()\n        for i in range(time_location if time_location <= size_window else size_window):\n            temp=num[i]\n            if temp and int(temp) == 1:\n                bit_sum += 1\n    end = show_info('end')\n\n    return bit_sum, time.time() - start_time, str(end-start)\n\ndef Is_due(time_now):\n    if len(bucket_n) > 0 and time_now - size_window == bucket_n[0]['timestamp']:  # 最左边的桶的时间戳等于当前时间减去窗口大小，到期了\n        del bucket_n[0]\n\ndef Merge():\n    for i in range(len(bucket_n) - 1, n_max_bucket - 1, -1):\n        if bucket_n[i]['bit_sum'] == bucket_n[i - n_max_bucket]['bit_sum']:\n            # 存在n_max_bucket个大小相同的桶\n            bucket_n[i - n_max_bucket]['bit_sum'] += bucket_n[i - n_max_bucket + 1]['bit_sum']\n            bucket_n[i - n_max_bucket]['timestamp'] = bucket_n[i - n_max_bucket + 1]['timestamp']\n            del bucket_n[i - n_max_bucket + 1]\n\ndef Count_bit():\n    bit_sum = 0\n    start_time = time.time()\n    start=show_info('start')\n\n    with open('../input/coding2/stream_data.txt', 'r') as f:\n        num = f.read()\n        num = num.split()\n        for i in range(time_location):\n            temp = num[i] # 读取文件的值\n            if temp:\n                Is_due(i + 1)  # 判断是否有桶到期\n                if int(temp) == 1:\n                    bucket = {\"timestamp\": i + 1, \"bit_sum\": 1}  # 桶的结构\n                    bucket_n.append(bucket)\n                    Merge()  # 合并大小相同的桶\n    for i in range(len(bucket_n)):\n        bit_sum += bucket_n[i]['bit_sum']\n    bit_sum -= bucket_n[0]['bit_sum'] / 2\n    bit_sum=int(bit_sum)\n    end=show_info('end')\n\n    return bit_sum if len(bucket_n) > 0 else 0, time.time() - start_time,str(end-start)\n\n\nbit_sum, bit_time, bit_space= Count_bit()\nbit_act_sum, bit_act_time, bit_act_space = Count_bit_act()\n\nprint(\"The estimated number of \\\"1\\\" is:\",bit_sum,\", the processing time is:\",bit_time,\"s\",\", and the space is:\",bit_space,\"KB\")\nprint(\"The exact number of \\\"1\\\" is:\",bit_act_sum,\", the processing time is:\",bit_act_time,\"s\",\", and the space is:\",bit_act_space,\"KB\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Locality Sensitive Hashing","metadata":{}},{"cell_type":"markdown","source":"The locality sensitive hashing (LSH) algorithm is efficient in near-duplicate document detection. In this coding, you're given the *docs_for_lsh.csv*, where the documents are processed into set of k-shingles (k = 8, 9, 10). *docs_for_lsh.csv* contains 201 columns, where column 'doc_id' represents the unique id of each document, and from column '0' to column '199', each column represents a unique shingle. If a document contains a shingle ordered with **i**, then the corresponding row will have value 1 in column **'i'**, otherwise it's 0. You need to implement the LSH algorithm and ask the problems below.","metadata":{}},{"cell_type":"markdown","source":"### Your task","metadata":{}},{"cell_type":"markdown","source":"Use minhash algoirthm to create signature of each document, and find 'the most similar' documents under Jaccard similarity. \nParameters you need to determine:\n1) Length of signature (number of distinct minhash functions) *n*. Recommanded value: n > 20.\n\n2) Number of bands that divide the signature matrix *b*. Recommanded value: b > n // 10.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport csv\nimport random\n\nrow_num = 0\ndata = []\nwith open('../input/coding2/docs_for_lsh.csv') as f:\n    dataset = csv.reader(f)\n    for row in dataset:\n        row_num += 1\n        if row_num == 1:\n            pass\n        else:\n            data.append(row[1:])\ndata = np.array(data)\ndata = data.T\n\ndef MinHash(data, b, r):\n    '''\n    param: data: the ndarray type, for shingles and documents\n    param: b: the number of the bands in signature matrix\n    param: r: the number of rows in one single band in the signature matrix, b*r stands for the length of signature\n    '''\n    n = b * r\n    signature = []\n    for i in range(n):\n        permutation = []\n        signal_signature = []\n        for num in range(1, data.shape[0] + 1):\n            permutation.append(num)\n        random.shuffle(permutation)\n        for j in range(data.shape[1]):\n            for k in range(data.shape[0]):\n                index = permutation.index(k + 1)\n                if data[index][j] == '1':\n                    signal_signature.append(k + 1)\n                    break\n                else:\n                    pass\n        signature.append(signal_signature)\n    return np.array(signature)\n\nb = 10\nr = 5\nres_signature = MinHash(data, b, r)\nprint(res_signature)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Problem: For document 0 (the one with id '0'), list the **30** most similar document ids (except document 0 itself). You can valid your results with the [sklearn.metrics.jaccard_score()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html) function.\n\nTips: You can adjust your parameters to hash the documents with similarity *s > 0.8* into the same bucket.","metadata":{}},{"cell_type":"code","source":"import hashlib\nfrom sklearn.metrics import jaccard_score\n\ndef LSH(signature, b, r):\n    '''\n    param: signature: the ndarray type, signature matrix\n    param: b: the number of the bands in signature matrix\n    param: r: the number of rows in one single band in the signature matrix, b*r stands for the length of signature\n    '''\n    length, docnum = signature.shape\n    buckets = {}\n    start = 0\n\n    for i in range(b):\n        for j in range(docnum):\n            md5 = hashlib.md5()\n            signal_band = str(signature[start:start + r, j])\n            hashed_band = md5.update(signal_band.encode())\n            hashed_band = md5.hexdigest()\n\n            if hashed_band not in buckets:\n                buckets[hashed_band] = [j]\n            elif j not in buckets[hashed_band]:\n                buckets[hashed_band].append(j)\n        start += r\n    return buckets\n\nLSH_table = LSH(res_signature, b, r)\nprint('LSH over!')\n\ndef search(LSH_table, num):\n    '''\n    param: LSH_table: the dictionary type, LSH buckets\n    param: num: the document id to search\n    '''\n    res = {}\n    for key in LSH_table:\n        if num in LSH_table[key] and len(LSH_table) != 1:\n            for docnum in LSH_table[key]:\n                if docnum == num:\n                    pass\n                else:\n                    if docnum in res:\n                        res[docnum] += 1\n                    else:\n                        res[docnum] = 1\n    return res\n\nresult = search(LSH_table,0)\nresult = sorted(result.items(),key=lambda item:item[1])\n\nnearest_neighbor_num = 30\nnearest_neighbor = []\nfor i in range(len(result)-1,len(result)-nearest_neighbor_num-1,-1):   # find the nearest documents\n    nearest_neighbor.append(result[i])\n#print('The nearest documents with times in the buckets are {}. '.format(nearest_neighbor))\n\ncheck_data = data.T\n\nLSH_neighbor = []\n\nfor i in range(len(nearest_neighbor)):\n    check_doc = nearest_neighbor[i][0]\n    score = jaccard_score(check_data[check_doc],check_data[0], pos_label= '1', average = 'binary')\n    #print(\"doc {}'s score with doc 0 is : {}\".format(check_doc,score))\n    LSH_neighbor.append((check_doc,score))\n\nprint('Thenearest documents are {}'.format(LSH_neighbor))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}