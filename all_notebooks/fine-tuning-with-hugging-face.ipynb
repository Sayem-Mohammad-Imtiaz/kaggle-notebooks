{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\"In this example, we’ll show how to download, tokenize, and train a model on the IMDb reviews dataset. This task takes the text of a review and requires the model to predict whether the sentiment of the review is positive or negative. Let’s start by downloading the dataset from the Large Movie Review Dataset webpage.\"\n\n    - Hugging face tutorial (https://huggingface.co/transformers/master/custom_datasets.html)\n\n![](data:image/png;base64,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)","metadata":{}},{"cell_type":"markdown","source":"### 1. Getting the data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\ndf.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:25.482357Z","iopub.execute_input":"2021-07-02T01:58:25.482683Z","iopub.status.idle":"2021-07-02T01:58:25.992101Z","shell.execute_reply.started":"2021-07-02T01:58:25.482659Z","shell.execute_reply":"2021-07-02T01:58:25.991089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This data is organized into pos and neg folders with one text file per example. Let’s write a function that can read this in.","metadata":{}},{"cell_type":"code","source":"def give_me_text_and_labels(input_csv=\"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\"):\n    df = pd.read_csv(input_csv)\n    \n    df['label'] = [1 if x==\"positive\" else 0 for x in df['sentiment'] ]\n    return df['review'].values, df['label'].values\n    ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-02T01:58:25.993465Z","iopub.execute_input":"2021-07-02T01:58:25.993664Z","iopub.status.idle":"2021-07-02T01:58:25.999022Z","shell.execute_reply.started":"2021-07-02T01:58:25.993643Z","shell.execute_reply":"2021-07-02T01:58:25.997734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts, labels = give_me_text_and_labels()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:26.001481Z","iopub.execute_input":"2021-07-02T01:58:26.001814Z","iopub.status.idle":"2021-07-02T01:58:26.529812Z","shell.execute_reply.started":"2021-07-02T01:58:26.001783Z","shell.execute_reply":"2021-07-02T01:58:26.528875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts[0][:100], labels[0], len(labels)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:26.5313Z","iopub.execute_input":"2021-07-02T01:58:26.531617Z","iopub.status.idle":"2021-07-02T01:58:26.536288Z","shell.execute_reply.started":"2021-07-02T01:58:26.531587Z","shell.execute_reply":"2021-07-02T01:58:26.535612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_length = 40000\ntrain_texts, train_labels = texts[:train_length], labels[:train_length]\ntest_texts, test_labels = texts[train_length:], labels[train_length:]","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:26.537099Z","iopub.execute_input":"2021-07-02T01:58:26.537288Z","iopub.status.idle":"2021-07-02T01:58:26.557723Z","shell.execute_reply.started":"2021-07-02T01:58:26.537268Z","shell.execute_reply":"2021-07-02T01:58:26.557113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now have a train and test dataset, but let’s also also create a validation set which we can use for for evaluation and tuning without tainting our test set results. Sklearn has a convenient utility for creating such splits:","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:26.56086Z","iopub.execute_input":"2021-07-02T01:58:26.561186Z","iopub.status.idle":"2021-07-02T01:58:26.580763Z","shell.execute_reply.started":"2021-07-02T01:58:26.561162Z","shell.execute_reply":"2021-07-02T01:58:26.579932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Tokenizer\n\nAlright, we’ve read in our dataset. Now let’s tackle tokenization. We’ll eventually train a classifier using pre-trained DistilBert, so let’s use the DistilBert tokenizer.","metadata":{}},{"cell_type":"code","source":"from transformers import DistilBertTokenizerFast\n\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:26.582217Z","iopub.execute_input":"2021-07-02T01:58:26.582523Z","iopub.status.idle":"2021-07-02T01:58:28.936638Z","shell.execute_reply.started":"2021-07-02T01:58:26.582501Z","shell.execute_reply":"2021-07-02T01:58:28.935187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can simply pass our texts to the tokenizer. We’ll pass `truncation=True` and `padding=True`, which will ensure that all of our sequences are padded to the same length and are truncated to be no longer model’s maximum input length. This will allow us to feed batches of sequences into the model at the same time.","metadata":{}},{"cell_type":"code","source":"type(train_texts), type(list(train_texts))","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:28.940271Z","iopub.execute_input":"2021-07-02T01:58:28.940503Z","iopub.status.idle":"2021-07-02T01:58:28.948607Z","shell.execute_reply.started":"2021-07-02T01:58:28.940482Z","shell.execute_reply":"2021-07-02T01:58:28.94769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\ntest_encodings = tokenizer(list(test_texts), truncation=True, padding=True)\nval_encodings = tokenizer(list(val_texts), truncation=True, padding=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:28.949946Z","iopub.execute_input":"2021-07-02T01:58:28.950155Z","iopub.status.idle":"2021-07-02T01:58:50.837609Z","shell.execute_reply.started":"2021-07-02T01:58:28.950129Z","shell.execute_reply":"2021-07-02T01:58:50.83666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Creating a dataset object\n\nNow, let’s turn our labels and encodings into a Dataset object. In PyTorch, this is done by subclassing a torch.utils.data.Dataset object and implementing __len__ and __getitem__. In TensorFlow, we pass our input encodings and labels to the from_tensor_slices constructor method. We put the data in this format so that the data can be easily batched such that each key in the batch encoding corresponds to a named parameter of the forward() method of the model we will train.","metadata":{}},{"cell_type":"code","source":"import torch\n\nclass IMDBdataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:50.838716Z","iopub.execute_input":"2021-07-02T01:58:50.838996Z","iopub.status.idle":"2021-07-02T01:58:50.845047Z","shell.execute_reply.started":"2021-07-02T01:58:50.838968Z","shell.execute_reply":"2021-07-02T01:58:50.844546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = IMDBdataset(train_encodings, train_labels)\ntest_dataset = IMDBdataset(test_encodings,test_labels)\nval_dataset = IMDBdataset(val_encodings, val_labels)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:50.845818Z","iopub.execute_input":"2021-07-02T01:58:50.846094Z","iopub.status.idle":"2021-07-02T01:58:51.333571Z","shell.execute_reply.started":"2021-07-02T01:58:50.846072Z","shell.execute_reply":"2021-07-02T01:58:51.332878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# idx = 0\n# # print(train_encodings.items())\n# item = {key: torch.tensor(val[idx]) for key, val in train_encodings.items()}\n# print(item)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:51.334571Z","iopub.execute_input":"2021-07-02T01:58:51.334794Z","iopub.status.idle":"2021-07-02T01:58:51.33878Z","shell.execute_reply.started":"2021-07-02T01:58:51.334767Z","shell.execute_reply":"2021-07-02T01:58:51.338089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that our datasets our ready, we can fine-tune a model either with the 🤗 Trainer/TFTrainer or with native PyTorch/TensorFlow.","metadata":{}},{"cell_type":"markdown","source":"### 4. Fine Tuning with native pytorch\n\nThe steps above prepared the datasets in the way that the trainer is expected. Now all we need to do is create a model to fine-tune, define the TrainingArguments/TFTrainingArguments and instantiate a Trainer/TFTrainer.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom transformers import DistilBertForSequenceClassification, AdamW\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:51.339655Z","iopub.execute_input":"2021-07-02T01:58:51.339985Z","iopub.status.idle":"2021-07-02T01:58:51.353384Z","shell.execute_reply.started":"2021-07-02T01:58:51.33996Z","shell.execute_reply":"2021-07-02T01:58:51.352623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\nmodel = model.to(device=device)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:51.354219Z","iopub.execute_input":"2021-07-02T01:58:51.354453Z","iopub.status.idle":"2021-07-02T01:58:53.857217Z","shell.execute_reply.started":"2021-07-02T01:58:51.354426Z","shell.execute_reply":"2021-07-02T01:58:53.856592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.train()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:53.858547Z","iopub.execute_input":"2021-07-02T01:58:53.859077Z","iopub.status.idle":"2021-07-02T01:58:53.868156Z","shell.execute_reply.started":"2021-07-02T01:58:53.859038Z","shell.execute_reply":"2021-07-02T01:58:53.866908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:53.869567Z","iopub.execute_input":"2021-07-02T01:58:53.869831Z","iopub.status.idle":"2021-07-02T01:58:55.00495Z","shell.execute_reply.started":"2021-07-02T01:58:53.869803Z","shell.execute_reply":"2021-07-02T01:58:55.00377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optim = AdamW(model.parameters(),lr=5e-5)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:55.006658Z","iopub.execute_input":"2021-07-02T01:58:55.006893Z","iopub.status.idle":"2021-07-02T01:58:55.022033Z","shell.execute_reply.started":"2021-07-02T01:58:55.00687Z","shell.execute_reply":"2021-07-02T01:58:55.021121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is the all important training loop. It is giving me meory limit exceeded error! if it happens to you, comment the training loop.","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nfor epoch in range(3):\n    for batch in tqdm(train_dataloader):\n        optim.zero_grad()\n        input_ids= batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs[0]\n        loss.backward()\n        optim.step()\n    print(f\"Loss for epoch {epoch} is {loss}\")\n\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:55.023155Z","iopub.execute_input":"2021-07-02T01:58:55.023397Z","iopub.status.idle":"2021-07-02T01:58:55.034953Z","shell.execute_reply.started":"2021-07-02T01:58:55.02335Z","shell.execute_reply":"2021-07-02T01:58:55.034154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"once trained save it","metadata":{}},{"cell_type":"code","source":"save_directory = \"./\"","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:55.035884Z","iopub.execute_input":"2021-07-02T01:58:55.036107Z","iopub.status.idle":"2021-07-02T01:58:55.048427Z","shell.execute_reply.started":"2021-07-02T01:58:55.036085Z","shell.execute_reply":"2021-07-02T01:58:55.047542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.save_pretrained(save_directory)\nmodel.save_pretrained(save_directory)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:55.049359Z","iopub.execute_input":"2021-07-02T01:58:55.049618Z","iopub.status.idle":"2021-07-02T01:58:55.553797Z","shell.execute_reply.started":"2021-07-02T01:58:55.049573Z","shell.execute_reply":"2021-07-02T01:58:55.552558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"and loading it back","metadata":{}},{"cell_type":"code","source":"# from transformers import TFAutoModel, AutoTokenizer\n# tokenizer = AutoTokenizer.from_pretrained(save_directory)\n# model = TFAutoModel.from_pretrained(save_directory, from_pt=True)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:55.555088Z","iopub.execute_input":"2021-07-02T01:58:55.555479Z","iopub.status.idle":"2021-07-02T01:58:55.559742Z","shell.execute_reply.started":"2021-07-02T01:58:55.555445Z","shell.execute_reply":"2021-07-02T01:58:55.558397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. Building the classifier and using sequence selection\n\nNow lets do something that is not covered in the official tutorial","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\nclassifier =pipeline('sentiment-analysis',model=model, tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:55.561027Z","iopub.execute_input":"2021-07-02T01:58:55.561253Z","iopub.status.idle":"2021-07-02T01:58:55.582148Z","shell.execute_reply.started":"2021-07-02T01:58:55.56123Z","shell.execute_reply":"2021-07-02T01:58:55.58098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_test= test_dataset[0]\nsample_test","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:55.585744Z","iopub.execute_input":"2021-07-02T01:58:55.585975Z","iopub.status.idle":"2021-07-02T01:58:55.605886Z","shell.execute_reply.started":"2021-07-02T01:58:55.585953Z","shell.execute_reply":"2021-07-02T01:58:55.605432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier(\"I love it\")[0]","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:55.607352Z","iopub.execute_input":"2021-07-02T01:58:55.607689Z","iopub.status.idle":"2021-07-02T01:58:55.638496Z","shell.execute_reply.started":"2021-07-02T01:58:55.607659Z","shell.execute_reply":"2021-07-02T01:58:55.637594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_texts[1001], test_labels[1001]","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:55.639625Z","iopub.execute_input":"2021-07-02T01:58:55.639923Z","iopub.status.idle":"2021-07-02T01:58:55.645182Z","shell.execute_reply.started":"2021-07-02T01:58:55.639891Z","shell.execute_reply":"2021-07-02T01:58:55.644209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier(test_texts[1001])[0]","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:55.646327Z","iopub.execute_input":"2021-07-02T01:58:55.646655Z","iopub.status.idle":"2021-07-02T01:58:56.841485Z","shell.execute_reply.started":"2021-07-02T01:58:55.646625Z","shell.execute_reply":"2021-07-02T01:58:56.840568Z"},"trusted":true},"execution_count":null,"outputs":[]}]}