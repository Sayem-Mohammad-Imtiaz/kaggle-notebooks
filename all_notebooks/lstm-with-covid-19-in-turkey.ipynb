{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/covid19-in-turkey/covid_19_data_tr.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Province/State column is null\n# We work on Turkey data. We do not need Country column\ndata = data.drop([\"Province/State\",\"Country/Region\"],axis=1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confirmed_data = pd.read_csv(\"../input/covid19-in-turkey/time_series_covid_19_confirmed_tr.csv\")\n# confirmed_data = confirmed_data.drop([\"Province/State\", \"Country/Region\", \"Lat\", \"Long\"],axis=1)\n# confirmed_data = confirmed_data.transpose()\n# confirmed_data.head()\nconfirmed_data = data['Confirmed']\nconfirmed_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# total test numbers\ntested_data = pd.read_csv(\"../input/covid19-in-turkey/time_series_covid_19_tested_tr.csv\")\ntested_data = tested_data.drop([\"Province/State\", \"Country/Region\", \"Lat\", \"Long\"],axis=1)\ntested_data = tested_data.transpose()\ntested_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recovered_data = data['Recovered']\nrecovered_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"deaths_data = data['Deaths']\ndeaths_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"intubated_data = pd.read_csv(\"../input/covid19-in-turkey/time_series_covid_19_intubated_tr.csv\")\nintubated_data = intubated_data.drop([\"Province/State\", \"Country/Region\", \"Lat\", \"Long\"],axis=1)\nintubated_data = intubated_data.transpose()\nintubated_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dates_data = data['Last_Update']\ndates_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# death plot\nplt.plot(dates_data, deaths_data)\nplt.rcParams[\"figure.figsize\"] = [10,5]\nplt.xlabel(\"Date\")\nplt.ylabel(\"Deaths\",rotation=90)\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confirmed patient\nplt.plot(dates_data, confirmed_data)\nplt.rcParams[\"figure.figsize\"] = [15,5]\nplt.xlabel(\"Date\")\nplt.ylabel(\"Confirmed\")\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from plotly.offline import init_notebook_mode, iplot, plot\nconfirmed_scatter = go.Scatter(\n    x = dates_data,\n    y = confirmed_data,\n    mode = \"lines+markers\",\n    name = \"Confirmed\",\n    marker = dict(color = 'rgba(16, 112, 2, 0.8)'),\n    text= 'Confirmed'\n)\n\ndeath_scatter = go.Scatter(\n    x = dates_data,\n    y = deaths_data,\n    mode = \"lines+markers\",\n    name = \"Deaths\",\n    marker = dict(color = 'rgba(0, 255, 200, 0.8)'),\n    text = 'Deaths'\n)\n\ntest_scatter = go.Scatter(\n    x = dates_data,\n    y = recovered_data,\n    mode = \"lines+markers\",\n    name = \"Recovered\",\n    marker = dict(color = 'rgba(255, 128, 2, 0.8)'),\n    text = 'Recovered'\n)\n\nlayout = dict(title = 'Deaths & Confirmed & Recovered',\n              xaxis= dict(title= 'Dates',ticklen= 5,zeroline= True)\n             )\n\ndata_scatter = [confirmed_scatter, death_scatter, test_scatter]\nfig = dict(data = data_scatter, layout = layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LSTM Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We crate 3 different model. recovered, confirmed , deaths","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dates_data = dates_data.values.reshape(-1,1)\nrecovered_data = recovered_data.values.reshape(-1,1)\ndeaths_data = deaths_data.values.reshape(-1,1)\nconfirmed_data = confirmed_data.values.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scale deaths\nscaler_dead = MinMaxScaler(feature_range=(0, 1))\ndeaths_data_sc = scaler_dead.fit_transform(deaths_data)\n\n# scale recovered\nscaler_rec = MinMaxScaler(feature_range=(0, 1))\nrecovered_data_sc = scaler_rec.fit_transform(recovered_data)\n\n# scale confirmed\nscaler_con = MinMaxScaler(feature_range=(0, 1))\nconfirmed_data_sc = scaler_con.fit_transform(confirmed_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dead_data_size = int(len(deaths_data_sc) * 0.55)\ntest_dead_data_size = len(deaths_data_sc) - train_dead_data_size\ntrain_dead_data = deaths_data_sc[0:train_dead_data_size,:]\ntest_dead_data = deaths_data_sc[train_dead_data_size:len(deaths_data_sc),:]\nprint(\"Dead Train data size\", len(train_dead_data) , \"Dead Test data size\", len(test_dead_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_recovered_data_size = int(len(recovered_data_sc) * 0.55)\ntest_recovered_data_size = len(recovered_data_sc) - train_recovered_data_size\ntrain_recovered_data = recovered_data_sc[0:train_recovered_data_size,:]\ntest_recovered_data = recovered_data_sc[train_recovered_data_size:len(recovered_data_sc),:]\nprint(\"Recovered Train data size\", len(train_recovered_data) , \"Recovered Test data size\", len(test_recovered_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_confirmed_data_size = int(len(confirmed_data_sc) * 0.55)\ntest_confirmed_data_size = len(confirmed_data_sc) - train_confirmed_data_size\ntrain_confirmed_data = confirmed_data_sc[0:train_confirmed_data_size,:]\ntest_confirmed_data = confirmed_data_sc[train_confirmed_data_size:len(confirmed_data_sc),:]\nprint(\"Confirmed Train data size\", len(train_confirmed_data) , \"Confirmed Test data size\", len(test_confirmed_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"time_stemp = 4\ndatax_date = []\ndatay_deaths = []\ndatay_confirmed = []\ndatay_recovered = []\n\nfor i in range(len(train_dead_data)-time_stemp-1):\n    a = train_dead_data[i:(i+time_stemp), 0]\n    datax_date.append(a)\n    datay_deaths.append(train_dead_data[i + time_stemp, 0])\ntrainX_deaths = np.array(datax_date)\ntrainY_deaths = np.array(datay_deaths)  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datax_date = []\ndatay_deaths = []\n\nfor i in range(len(test_dead_data)-time_stemp-1):\n    a = test_dead_data[i:(i+time_stemp), 0]\n    datax_date.append(a)\n    datay_deaths.append(test_dead_data[i + time_stemp, 0])\ntestX_deaths = np.array(datax_date)\ntestY_deaths = np.array(datay_deaths)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainX_deaths = np.reshape(trainX_deaths, (trainX_deaths.shape[0], 1, trainX_deaths.shape[1]))\ntestX_deaths = np.reshape(testX_deaths, (testX_deaths.shape[0], 1, testX_deaths.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_model = Sequential()\nlstm_model.add(LSTM(10, input_shape=(1, time_stemp)))\nlstm_model.add(Dense(1))\nlstm_model.compile(loss='mean_squared_error', optimizer='adam')\nlstm_model.fit(trainX_deaths, trainY_deaths, epochs=50, batch_size=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainPredict = lstm_model.predict(trainX_deaths)\ntestPredict = lstm_model.predict(testX_deaths)\n\ntrainPredict = scaler_dead.inverse_transform(trainPredict)\ntrainY = scaler_dead.inverse_transform([trainY_deaths])\ntestPredict = scaler_dead.inverse_transform(testPredict)\ntestY = scaler_dead.inverse_transform([testY_deaths])\n# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrainPredictPlot = np.empty_like(deaths_data_sc)\ntrainPredictPlot[:, :] = np.nan\ntrainPredictPlot[time_stemp:len(trainPredict)+time_stemp, :] = trainPredict\n\ntestPredictPlot = np.empty_like(deaths_data_sc)\ntestPredictPlot[:, :] = np.nan\ntestPredictPlot[len(trainPredict)+(time_stemp*2)+1:len(deaths_data_sc)-1, :] = testPredict\n\nplt.plot(scaler_dead.inverse_transform(deaths_data_sc))\nplt.plot(trainPredictPlot)\nplt.plot(testPredictPlot)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will 2 more lstm models for confirmed and recovered datas.\nThank you","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}