{"cells":[{"metadata":{"_uuid":"a685aa1111c67ef34246e3f1dad528792c12e332"},"cell_type":"markdown","source":"## Import and Observe data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35eb7272c5c44d92ba8e680e5e7f14e01c7b9d5b"},"cell_type":"code","source":"os.listdir('../input/volcanoes_train/')\ntrain_images = pd.read_csv('../input/volcanoes_train/train_images.csv', header = None)\ntrain_labels = pd.read_csv('../input/volcanoes_train/train_labels.csv')\ntest_images = pd.read_csv('../input/volcanoes_test/test_images.csv', header = None)\ntest_labels = pd.read_csv('../input/volcanoes_test/test_labels.csv')\ntrain_images.shape, train_labels.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e34ff8ed666530230b3039900de0f7529593766"},"cell_type":"code","source":"train_images.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"406beff39d800b4086146e712ef54c5d0418b387","scrolled":true},"cell_type":"code","source":"train_labels.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a31de8c537f21cbb07826f98989de9445fa5a8c4"},"cell_type":"markdown","source":"The last three labels are only valid if the is a volcano in the photo. \nFor this project, we are focusing on predicting where there is a volcano, i.e. the label 'Volcano?'\n\nNow let's look at the label distribution"},{"metadata":{"trusted":true,"_uuid":"4ef02413c81b69c6f3b871891add6161446bf93c","scrolled":true},"cell_type":"code","source":"train_counts = train_labels['Volcano?'].value_counts()\ntest_counts = test_labels['Volcano?'].value_counts()\n\nplt.figure(figsize = (8,4))\nplt.subplot(121)\nsns.barplot(train_counts.index, train_counts.values)\nplt.title('volcanos in training set')\nplt.subplot(122)\nsns.barplot(test_counts.index, test_counts.values)\nplt.title('volcanos in testing set')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"379aae7016960a90900a62ed5c3f5b96e712f43a"},"cell_type":"markdown","source":"Plot a few photos with different labels. We can also see some 'no volvano' images are corrupted. We'll leave them as is for now."},{"metadata":{"trusted":true,"_uuid":"5a431e0d023b23541944b9b56da9a9623d7f8733","scrolled":true},"cell_type":"code","source":"pos_samples = train_images[train_labels['Volcano?'] == 1].sample(5)\nneg_samples = train_images[train_labels['Volcano?'] == 0].sample(5)\n\nplt.subplots(figsize = (15,6))\nfor i in range(5):\n    plt.subplot(2,5,i+1)\n    plt.imshow(pos_samples.iloc[i,:].values.reshape((110, 110)), cmap = 'gray')\n    if i == 0: plt.ylabel('Volcano')\nfor i in range(5):\n    plt.subplot(2,5,i+6)\n    if i == 0: plt.ylabel('No Volcano')\n    plt.imshow(neg_samples.iloc[i,:].values.reshape((110,110)), cmap = 'gray')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5de0faf9c0a68d2281821fff4ccddc517bb47335"},"cell_type":"markdown","source":"## Pre-Process Data"},{"metadata":{"_uuid":"df73aec4f2f20bae62442ffabe9e990816722506"},"cell_type":"markdown","source":"### Pixel Normalization\nWhen working with images, a generally good idea is to normalize the pixel values to between 0 and 1. In this case, divide the pixel values by 256."},{"metadata":{"trusted":true,"_uuid":"423a8e8fb839700ee8e5f9fe67786a5b6d701f07"},"cell_type":"code","source":"Xtrain_raw = train_images/256\nytrain_raw = train_labels['Volcano?']\nXtest_raw = test_images/256\nytest_raw = test_labels['Volcano?']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1dc99a904b203171dae88c9e7430eb39cebe3c1"},"cell_type":"markdown","source":"## Model"},{"metadata":{"_uuid":"e222c2f46a8050c1e5e632a894cd8bf183a0fd26"},"cell_type":"markdown","source":"### simple logistic regression"},{"metadata":{"trusted":true,"_uuid":"b347b7440b181c0bb18aff309f7f0f52248e5b0b","scrolled":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nXtrain, Xvali, ytrain, yvali = train_test_split(Xtrain_raw, ytrain_raw, test_size = 0.2, random_state = 3)\nXtest, ytest = Xtest_raw, ytest_raw\nmodelLR = LogisticRegression()\n\nfrom time import time\nstart = time()\nmodelLR.fit(Xtrain, ytrain)\nend = time()\nprint('training time: {} mins.'.format((end-start)/60))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4092cd0570436677056916cebb220e4bb8331faa"},"cell_type":"markdown","source":"Examine the result from logistic regression:\n\nThe accuracy score is good (>0.9), however a lot of it is contributed from the imbalance of the data, the recall score is only 0.59."},{"metadata":{"trusted":true,"_uuid":"c5b657d286100c4c2df03a9da98edf21a1083d9c"},"cell_type":"code","source":"from sklearn.metrics import classification_report\npredVali = modelLR.predict(Xvali)\npredTest = modelLR.predict(Xtest)\nprint('validation report:','\\n',classification_report(yvali, predVali))\nprint('testing report:', '\\n', classification_report(ytest, predTest))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69a9edc76d262da031f511db73550932d84aebbd"},"cell_type":"markdown","source":"## A simple CNN model"},{"metadata":{"trusted":true,"_uuid":"88a8e989dbba2d7bff1e713920112c42b258c9d1"},"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, MaxPool2D, Dropout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"643038e0127b3112961e74dcebff30517e375686"},"cell_type":"code","source":"#  can be used to fix the random seed to get reproducable result\n# from numpy.random import seed\n# from tensorflow import set_random_seed\n# seed(42)\n# set_random_seed(42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee093e8c7cc1afa8a94d7984207093ed5d40d7a0"},"cell_type":"markdown","source":"prepare the image data to appropriate dimention, and split the training data to train and validation data."},{"metadata":{"trusted":true,"_uuid":"384dda8eb3e385e023e196562785cccecf68d10a"},"cell_type":"code","source":"img_rows, img_cols = 110, 110\n\nX = Xtrain_raw.values.reshape((-1, img_rows, img_cols, 1))\ny = ytrain_raw.values\nX_train, X_vali, y_train, y_vali = train_test_split(X, y, test_size = 0.2, random_state = 3)\n\nX_test = Xtest_raw.values.reshape((-1, img_rows, img_cols, 1))\ny_test = ytest_raw.values\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa39bc675aa81ae0dbab4a3d2195f4bf7016f735"},"cell_type":"markdown","source":"build up the model with 3 convolution layers, each followed by a maxpooling and a drop out"},{"metadata":{"trusted":true,"_uuid":"26ee58c97d2950c3101263f91f00a3658aeae483"},"cell_type":"code","source":"# kernel_initializer can be tuned for the first conv2D layer\ninit = keras.initializers.RandomNormal(mean=0, stddev=0.1 )\nmodelCNN1 = Sequential()\nmodelCNN1.add(Conv2D(6, kernel_size = (3,3),kernel_initializer=init, activation = 'relu', input_shape = (img_rows, img_cols, 1)))\nmodelCNN1.add(MaxPool2D(pool_size=(2,2), strides=2))\nmodelCNN1.add(Dropout(0.5))\nmodelCNN1.add(Conv2D(12, kernel_size = (3,3), activation = 'relu'))\nmodelCNN1.add(MaxPool2D(pool_size=(2,2), strides=2))\nmodelCNN1.add(Dropout(0.5))\nmodelCNN1.add(Conv2D(24, kernel_size = (3,3), activation = 'relu'))\nmodelCNN1.add(MaxPool2D(pool_size=(2,2), strides=2))\nmodelCNN1.add(Dropout(0.5))\nmodelCNN1.add(Flatten())\nmodelCNN1.add(Dense(1, activation = 'sigmoid'))\n\nmodelCNN1.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a98e2e9435e2b950b2efb56a964e6a140e98e03f","scrolled":false},"cell_type":"code","source":"# the line bolow can be used for tuning the adam optimizer, e.g. different initial learning rate\n# adam = keras.optimizers.Adam(lr=1e-6, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\nmodelCNN1.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n\n# the callBack parameter can be added to model.fit as 'callbacks = [callBack]' for early termination\nfrom keras.callbacks import EarlyStopping\ncallBack = EarlyStopping(monitor='val_loss', min_delta=0, patience=15, verbose=0, mode='auto')\n\n\ndef reset_weights(model):\n    session = keras.backend.get_session()\n    for layer in model.layers: \n        if hasattr(layer, 'kernel_initializer'):\n            layer.kernel.initializer.run(session=session)\n\nepochs = 100\nbatch_size = 64\n                        \nreset_weights(modelCNN1)            \nhistory = modelCNN1.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, \n                        validation_data=(X_vali, y_vali),\n                        callbacks=[callBack]\n                       )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7438abad30296a1849e7e5e348f7bf1e73a591ee","scrolled":false},"cell_type":"markdown","source":"Let's Evaluate the result from CNN model by looking at the learning curve and classification report. Both the accuracy and recall improved."},{"metadata":{"trusted":true,"_uuid":"8af6b93e93b6e4da82b867230c42ca33d0cbd8b3","scrolled":false},"cell_type":"code","source":"def report(model):\n    predVali = model.predict_classes(X_vali)\n    predTest = model.predict_classes(X_test)\n    print('validation report:','\\n',classification_report(y_vali, predVali))\n    print('validation accuracy:', accuracy_score(y_vali, predVali))\n    print('testing report:', '\\n', classification_report(y_test, predTest))\n    print('test accuracy:', accuracy_score(y_test, predTest))\n\ndef plotLearningCurves(history):\n    fig, ax = plt.subplots(1,2, figsize = (14,6))\n    ax[0].plot(history.epoch, history.history['loss'], color='b', label=\"Training loss\")\n    ax[0].plot(history.epoch, history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\n    ax[0].legend(loc='best', shadow=True)\n    ax[0].set_title('loss vs epoch')\n\n    ax[1].plot(history.epoch, history.history['acc'], color='b', label=\"Training accuracy\")\n    ax[1].plot(history.epoch, history.history['val_acc'], color='r',label=\"Validation accuracy\")\n    ax[1].legend(loc='best', shadow=True)\n    ax[1].set_title('accuracy vs epoch')\n\nplotLearningCurves(history)    \nreport(modelCNN1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7427be480f5a1201c10e4fe55e3dd8d092e8b066"},"cell_type":"markdown","source":"Notice our label is not balanced, let's give the class different weights to account for the imbalance"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"75a9a71ef2881f8b9f5bf107f8d9b364a3f0127f"},"cell_type":"code","source":"modelCNN2 = keras.models.clone_model(modelCNN1)\nmodelCNN2.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\nreset_weights(modelCNN2)\n\n# the block below computes the class weights from the training set\nfrom collections import Counter\ncounter = Counter(y_train) \nmax_val = float(max(counter.values()))       \nclass_weight = {class_id : max_val/num_images for class_id, num_images in counter.items()}\n\nepochs = 80\nbatch_size = 64\nhistory2 = modelCNN2.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, \n                         validation_data=(X_vali, y_vali),\n                         # callbacks=[callBack],\n                         class_weight = class_weight\n                        )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"8c50c238af44e72a15f4041a46d55947bc0f6783"},"cell_type":"code","source":"plotLearningCurves(history2)\nreport(modelCNN2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6fc630b8e2f3b2ad4276d6f251436ce7ed44e666"},"cell_type":"markdown","source":"Although the accuracy did not increase much,  the recall rate for volcano increased a lot, which I consider a great improvement compared to the previous two models. Looking at the curve it seems like there is still a small room to improve modelCNN2."},{"metadata":{"_uuid":"aa84faa36f9b02cf14316f4a15c97bbd5c18e8b0"},"cell_type":"markdown","source":"Let's try to use image augumentation to increase the training data."},{"metadata":{"trusted":true,"_uuid":"e1d77e3ef80ff213530672e43bae9f2492039546"},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n\n\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = False, # Randomly zoom image \n        width_shift_range= False,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range= False,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=True)  # randomly flip images\n\n\ndatagen.fit(X_train)\ngenerator = datagen.flow(X_train, y_train, batch_size= batch_size)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db24dfa36ccce24ca73b85502365b21b262441f4","scrolled":true},"cell_type":"code","source":"modelCNN3 = keras.models.clone_model(modelCNN1)\nmodelCNN3.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n\nepochs = 60\nreset_weights(modelCNN3)\nhistory3 = modelCNN3.fit_generator(generator,epochs = epochs, validation_data = (X_vali,y_vali),\n                                   class_weight = class_weight,\n                                   #callbacks=[callBack]\n                                  )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e66b55da0282bd0ed8c709656ce0ce3f29e910f2"},"cell_type":"code","source":"plotLearningCurves(history3)\nreport(modelCNN3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b7137d67ca2c1e39b11259af430b9e261d377d3"},"cell_type":"markdown","source":"So for model3 with augumented data, I got similar result from model2, contray to some other kernals. "},{"metadata":{"_uuid":"7639c4ea03749ce240bcefb6a4c27b4d253f0557"},"cell_type":"markdown","source":"### CNN model 4, adding model complexity"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"377348ed6eeccf61d6df75cd7b7ec98ce8c59db1"},"cell_type":"code","source":"init = keras.initializers.RandomNormal(mean=0, stddev=0.1 )\nmodelCNN4 = Sequential()\nmodelCNN4.add(Conv2D(32, kernel_size = (3,3),kernel_initializer=init, activation = 'relu', input_shape = (img_rows, img_cols, 1)))\nmodelCNN4.add(MaxPool2D(pool_size=(2,2), strides=2))\nmodelCNN4.add(Dropout(0.5))\nmodelCNN4.add(Conv2D(64, kernel_size = (3,3), activation = 'relu'))\nmodelCNN4.add(MaxPool2D(pool_size=(2,2), strides=2))\nmodelCNN4.add(Dropout(0.5))\nmodelCNN4.add(Conv2D(128, kernel_size = (3,3), activation = 'relu'))\nmodelCNN4.add(MaxPool2D(pool_size=(2,2), strides=2))\nmodelCNN4.add(Dropout(0.5))\nmodelCNN4.add(Conv2D(64, kernel_size = (3,3), activation = 'relu'))\nmodelCNN4.add(MaxPool2D(pool_size=(2,2), strides=2))\nmodelCNN4.add(Dropout(0.5))\nmodelCNN4.add(Flatten())\nmodelCNN4.add(Dense(1, activation = 'sigmoid'))\n\nmodelCNN4.summary()\nmodelCNN4.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n\nreset_weights(modelCNN4)\nepochs = 80\nbatch_size = 64\nhistory4 = modelCNN4.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, \n                         validation_data=(X_vali, y_vali),\n                         callbacks=[callBack],\n                         class_weight = class_weight\n                        )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8f8f8e92264a6be23a665b488326a29313c5f42"},"cell_type":"code","source":"plotLearningCurves(history4)\nreport(modelCNN4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"395a61a64cd4e913a1778328b3c18e044e7b0aac","scrolled":true},"cell_type":"markdown","source":"Model 4 achieves near 0.97 accuracy and the recall rates are above 0.9. No overfitting yet. "},{"metadata":{"trusted":true,"_uuid":"58789a4aff0264ed7146b65d16697cf2cb8d7486"},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}