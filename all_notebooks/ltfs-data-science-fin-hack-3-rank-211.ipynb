{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', None)\nfrom sklearn.preprocessing import MinMaxScaler\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/analytics-vidhya-ltfs-data-science-finhack-3/Data/Data/train_Data.csv\")\ntrain_bureau = pd.read_csv(\"/kaggle/input/analytics-vidhya-ltfs-data-science-finhack-3/Data/Data/train_bureau.csv\")\n\ntest_data = pd.read_csv(\"/kaggle/input/analytics-vidhya-ltfs-data-science-finhack-3/Data/Data/test_Data.csv\")\ntest_bureau = pd.read_csv(\"/kaggle/input/analytics-vidhya-ltfs-data-science-finhack-3/Data/Data/test_bureau.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Working on Bureau Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_bureau = train_bureau.drop_duplicates()\ntest_bureau = test_bureau.drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['source'] = 'train'\ntest_data['source'] = 'test'\n\ntrain_bureau['source'] = 'train'\ntest_bureau['source'] = 'test'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau = pd.concat([train_bureau, test_bureau])\ndata = pd.concat([train_data, test_data])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in bureau.columns:\n    if bureau[col].dtype=='object':\n        print(bureau[col].value_counts())\n        print('')\n        \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Can drop MATCH-TYPE, since very few are of secondary type\nfix account status\nfix contributor type"},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau['ACCT-TYPE'] = bureau['ACCT-TYPE'].replace(bureau['ACCT-TYPE'].value_counts()[bureau['ACCT-TYPE'].value_counts()/bureau.shape[0]*100<3].index, 'Rest')\nbureau['ACCT-TYPE'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau['CONTRIBUTOR-TYPE'] = bureau['CONTRIBUTOR-TYPE'].replace(['HFC', 'MFI', 'FRB',\n       'SFB', 'ARC', 'OFI' ,'CCC'], 'OTHERS')\nbureau['CONTRIBUTOR-TYPE'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau['ACCOUNT-STATUS'] = bureau['ACCOUNT-STATUS'].replace(['SUIT FILED (WILFUL DEFAULT)',\n       'Written Off', 'Suit Filed', 'Restructured', 'Settled',\n       'WILFUL DEFAULT', 'Cancelled', 'Sold/Purchased'], 'Written Off')\nbureau['ACCOUNT-STATUS'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau.isnull().sum()[bureau.isnull().sum()>0]/bureau.shape[0]*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop columns with greater than 20% null values\nbureau.drop(bureau.isnull().sum()[bureau.isnull().sum()/bureau.shape[0]*100>20].index, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau.isnull().sum()[bureau.isnull().sum()>0]/bureau.shape[0]*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#can drop hist columns for now\nbureau = bureau.drop(['REPORTED DATE - HIST', 'DPD - HIST', 'CUR BAL - HIST',\n       'AMT OVERDUE - HIST', 'AMT PAID - HIST'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau['DISBURSED-DT'] = pd.to_datetime(bureau['DISBURSED-DT'])\nbureau['DATE-REPORTED'] = pd.to_datetime(bureau['DATE-REPORTED'])\nbureau['DATE-REPORTED'].max(), bureau['DISBURSED-DT'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau['DATE-REPORTED'] = bureau['DATE-REPORTED'].fillna(bureau['DATE-REPORTED'].max())\nbureau['DISBURSED-DT'] = bureau['DISBURSED-DT'].fillna(bureau['DATE-REPORTED'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau.drop('MATCH-TYPE', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau['CURRENT-BAL'] = bureau['CURRENT-BAL'].apply(lambda x: str(x).replace(',', ''))\nbureau['DISBURSED-AMT/HIGH CREDIT'] = bureau['DISBURSED-AMT/HIGH CREDIT'].apply(lambda x: str(x).replace(',', ''))\nbureau['WRITE-OFF-AMT'] = bureau['WRITE-OFF-AMT'].apply(lambda x: str(x).replace(',', ''))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#bureau['CURRENT-BAL'] = bureau['CURRENT-BAL'].fillna('0')\nbureau.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in bureau.columns[-4:-1]:\n    print(col)\n    bureau[col] = bureau[col].replace('nan', 0)\n    bureau[col] = bureau[col].apply(lambda x: float(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau = pd.get_dummies(data=bureau, columns=['ACCT-TYPE', 'OWNERSHIP-IND', 'CONTRIBUTOR-TYPE', \n                                    'ACCOUNT-STATUS'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau_data = bureau.groupby(['ID', 'source']).sum().reset_index()\nbureau_data.to_csv('bureau_data.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bureau_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('bureau_data.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols = ['DISBURSED-AMT/HIGH CREDIT', 'CURRENT-BAL']\nscaler = StandardScaler()\n\n\nfor col in num_cols:\n    df[col] = np.log(1+df[col])\n    print(col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Working on demographic data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#target variable is \"Top-up Month\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# can drop unwanted columns\ndata = data.drop(['Area', 'BranchID', 'City', 'ZiPCODE'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['DisbursalDate'] = pd.to_datetime(data['DisbursalDate'])\ndata['MaturityDAte'] = pd.to_datetime(data['MaturityDAte'])\ndata['AuthDate'] = pd.to_datetime(data['AuthDate'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in data.columns:\n    if data[col].dtype=='object':\n        print(data[col].value_counts())\n        print('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['State'] = data['State'].replace(data['State'].value_counts()[data['State'].value_counts()/data.shape[0]*100<3].index, 'Rest')\ndata['State'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['PaymentMode'] = data['PaymentMode'].replace(data['PaymentMode'].value_counts()[data['PaymentMode'].value_counts()/data.shape[0]*100<3].index, 'Rest')\ndata['PaymentMode'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = ['ManufacturerID', 'SupplierID', 'AssetID']\nfor col in cat_cols:\n    print('value counts')\n    print(data[col].value_counts())\n    print('number of unique values', data[col].nunique())\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(['SupplierID', 'AssetID'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['ManufacturerID'] = data['ManufacturerID'].replace(data['ManufacturerID'].value_counts()[data['ManufacturerID'].value_counts()/data.shape[0]*100<3].index, 1111)\ndata['ManufacturerID'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['MonthlyIncome'] = data['MonthlyIncome'].fillna(0)\ndata['SEX'] = data['SEX'].fillna(data['SEX'].mode()[0])\ndata['AGE'] = data['AGE'].fillna(data['AGE'].mode()[0])\ndata['ManufacturerID'] = data['ManufacturerID'].fillna(data['ManufacturerID'].mode()[0])\ndata['MaturityDAte'] = data['MaturityDAte'].fillna(data['MaturityDAte'].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\ncat_cols = ['Frequency', 'InstlmentMode', 'LoanStatus', 'PaymentMode', 'SEX', 'State']\n\nfor col in cat_cols:\n    print(col)\n    data[col] = le.fit_transform(data[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols = ['AssetCost', 'AmountFinance', 'DisbursalAmount', 'EMI', 'MonthlyIncome','Tenure']\nscaler = StandardScaler()\n\n\nfor col in num_cols:\n    data[col] = np.log(1+data[col])\n    print(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(['DisbursalDate', 'MaturityDAte', 'AuthDate'], axis=1, inplace=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['EMItoIncome'] = data['EMI']/data['MonthlyIncome']\ndata['AssettoIncome'] = data['AssetCost']/(data['MonthlyIncome']*12)\ndata['DifferenceAmount'] = data['AmountFinance'] - data['DisbursalAmount']\ndata['YearsOfService'] = 60 - data['AGE']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols = ['AssetCost', 'AmountFinance', 'DisbursalAmount', 'EMI', 'MonthlyIncome']\nscaler = StandardScaler()\nfor col in num_cols:\n    data[col] = np.log(1+data[col])\n    print(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop('AGE', 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.to_csv('cleaned_data.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final = pd.merge(data, df, on=['ID', 'source'], how='inner')\nfinal.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict ={' > 48 Months':6,'12-18 Months':1,'18-24 Months':2,'24-30 Months':3,'30-36 Months':4,'36-48 Months':5,'No Top-up Service':0}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = final[final['source']=='train']\ntest_df = final[final['source']=='test']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.drop('source',1, inplace=True)\ntest_df.drop(['source', 'Top-up Month'],1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.corr()\nimport seaborn as sns\nsns.heatmap(train_df.corr())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.shape, test_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['ManufacturerID'] = le.fit_transform(test_df['ManufacturerID'])\ntrain_df['ManufacturerID'] = le.fit_transform(train_df['ManufacturerID'])\ntrain_df=train_df.round(3)\ntest_df=test_df.round(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain = train_df.drop('ID', 1)\nprint(train_df['Top-up Month'].value_counts())\n\ntrain_df['Top-up Month'] = train_df['Top-up Month'].map(dict)\nX = train_df.drop(['ID', 'Top-up Month'], 1)\nprint(train_df['Top-up Month'].value_counts())\n\ny = train_df['Top-up Month']\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\ntest = test_df.drop(['ID'], 1)\nX_train.shape, y_train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix,  roc_curve, precision_recall_curve, accuracy_score, roc_auc_score\nimport lightgbm as lgbm\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier, RandomForestClassifier, BaggingClassifier\nfrom sklearn.linear_model import RidgeClassifier, Perceptron, PassiveAggressiveClassifier, LogisticRegression, SGDClassifier\n\nimport optuna\nfrom optuna.samplers import TPESampler\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.kernel_ridge import KernelRidge\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_folds = 5\n\ndef model_acc(model):\n    model.fit(X_train,y_train)\n    y_pred=model.predict(X_val)\n    accuracy=round(accuracy_score(y_pred,y_val)*100, 2)\n    return(accuracy)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel_xgb = XGBClassifier()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a=model_acc(model_xgb)\nimport lightgbm as lgb\nclf = lgb.LGBMClassifier()\nb=model_acc(clf)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble = float(a*0.60) + float(b*0.40)\nensemble","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = clf.predict(test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict1 ={6:' > 48 Months',1:'12-18 Months',2:'18-24 Months',3:'24-30 Months',4:'30-36 Months',5:'36-48 Months',0:'No Top-up Service'}\n\nsubm = pd.DataFrame({'ID': test_df['ID'], 'Top-up Month': pred})\nsubm['Top-up Month'] = subm['Top-up Month'].map(dict1)\nsubm.to_csv('ss2.csv', index=False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}