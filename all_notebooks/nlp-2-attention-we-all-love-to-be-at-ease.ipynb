{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Welcome\nThis is the second part of the series on NLP which is specially signifies the working of the `Attention Mechanism` in `Keras` for Natural Language Processing. If you want to start the NLP  or Deep journey, I have few notebooks for your use and please do watch then in sequence.\n\n1. [Deep Learning Beginner Tutorial (Using CNN with Keras)](https://www.kaggle.com/deshwalmahesh/bengali-ai-complete-beginner-tutorial-95-acc)\n2. [NLP Tutorial for Beginner describing Embeddings, RNN, LSTM, GRU](https://www.kaggle.com/deshwalmahesh/nlp-beginner-1-rnn-lstm-gru-embeddings-glove)\n\n**NOTE**: This notebook will use of the custom `Layer` in Keras as there is no `Attention` Layer given in the official package so if you do not how to make a custom layer, please find this very intutive notebook about [cutom layers, models in keras](https://www.kaggle.com/deshwalmahesh/keras-tensorflow-2-0-custom-layers-models)\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Problem\nWe have been given full articles from the BBC where these articles can belong to one of the 5 categories. We have to look at a new article and find which category does it belong to.\n\n\n# Solution\nWe'll won't make use of the sentiments here. Idea is very simple that there can be some words like `cricket`, `football` ,`goals`, `runs` and so on which can define the category of the news. Same goes with science and other genres as well. We'll try to build a modle which is able to predict these things for us.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Attention Here!!!\nSo what basically attention is? Why is it used? How is it used?......\nWell, to answer your question regarding attention, we have to **Roll Back in Time** (pun intended). Before the use of Attention in NLP tasks, we relied heavily on `LSTM` which used to **Roll Back in Time** to get the context and sementic of a word in a sentence. So what exactly is rolling back?? As we know that any `RNN` be it `GRU` or even `LSTM`, produces a hiden state `H_i` with every time step. Sate `H_i+1` defined that it has learned from the `Input + H_i`. So the new state has all the **Important** information needed up until now i.e it has all the information learned by previous cells, filtered it and read the new input to produce a new Hidden state which it'll pass to the next cell.\n\nThis is all fun and games until it's something with a deep meaning. LSTMs are good at finding relation when it comes to short sentences but what about a time when you have to find the hidden meaning? Now I got your Attention!!! or you can say we got Attention. Attention is very efficient at finding the hidden meanings of the words as well as the context of the word. A very popular example you'll see everywhere will be this one.\n<img src=\"https://i.stack.imgur.com/YNdXP.png\" width=\"350px\">\n\nSo What is this saying?\n\nIt is saying that the `Attention` Mechanism is able to get the part of English language that **it** is used in the context for **Animal** and it has decided to give this decision based after looking at the terms and making decision that  **The,animal,didn't** are more important than the **the,street,too**. So if you had change the last word from `tired -> narrow`, it would hve looked at `the,street,too` more than `the,animal,didn't` because it would have learned that **it** here is used for street but not the animal.\n\nGREATTTT!!!! But how does it do that??? \nWell it is doing by computing **self-attention**. Woah!!! is this even a term? Yes, if you pay enough attention to what you think and do ;)\n\n**NOTE**: The above discussion was to show you the advanced version of Attention called self attention. We'll use the simple attention. If you want to know more the paper [Attention is all you need](), then you have to refer to very awesome blogs given below:\n1. [Pytorch's seq2seq tutorial](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb)\n2. [Jay Alammar's highly intutive and most referenced blog](http://jalammar.github.io/illustrated-transformer/)\n3. [My Favourite blog on the paper](https://glassboxmedicine.com/2019/08/15/the-transformer-attention-is-all-you-need/)\n4. [Paper Dissected by mlexplained](https://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/#:~:text=The%20attention%20weight%20can%20be,simple%20feed%2Dforward%20neural%20network.&text=Attention%20basically%20gives%20the%20decoder,choose%20what%20information%20to%20use)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Workings\nTo get the illustrated knowledge of working of attention, please refer to [this very intutive blog](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3).\n\nLet us look at the image given below\n<img src=\"https://content.iospress.com/media/ifs/2020/38-2/ifs-38-2-ifs191257/ifs-38-ifs191257-g002.jpg?width=350\">\n\nSo in a `Many input, Many-output` style scenario, instead of loking at the current input and the previous hidden state like in any RNN type of architecture, it looks at all the instances. In simple terms, it looks at all the words in the sentence everytime and predicts that **if I have to predict a new `y_out`, then which words do I have to look for the most and bring out the most relatable output**. \nSo it finds the *Weight* and *Context Vector* at each time step to decide what should we be outputting at a given time step.\n\nWe can add Attention Layer to support LSTM layer for a very good Encoder-Decoder Architecture for Machine Translation as shown below:\n<img src=\"https://i.ytimg.com/vi/p3jmVkUMMuw/maxresdefault.jpg\" width=\"550px\">\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Imports\n\nPlease don't get intimidated by the size of imports. Some are used and some are not given different scenarios.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport nltk \nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom string import punctuation\n\nfrom keras import backend as K\nfrom keras import initializers, regularizers, constraints\n\nfrom keras.layers import Dense, Input, LSTM, Bidirectional,Dropout, Embedding, BatchNormalization, Layer\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint,EarlyStopping, ReduceLROnPlateau\nfrom keras.initializers import Constant\nfrom keras.layers.merge import add\nfrom keras.optimizers import Adam\n\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer, text_to_word_sequence\nfrom keras.utils import np_utils\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Set Defaults","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 13 # reproducible results\n\nnp.random.seed(seed) # numpy seed\n\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('stopwords')\n\nsigns = list(punctuation) # special characters \nstop_words = list(stopwords.words('english')) # stop words like `a,an,the,or,at` etc\n\nwordnet_lemmatizer = WordNetLemmatizer() # make the word cooking,cooks,cooked -> cook (in ideal case)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9bf30c58fb63b9e0aad9e36b3384ce5e91c83aef"},"cell_type":"markdown","source":"# Proprecess Data\nThere might be anomalies with the data we have. We have to remove those impurities so that it can makes our model faster and those impurities do not provide un necessary parameters to the model.\n\n**NOTE**: Cleaning and preprocessing methods are different for every data given What we need, what we want to achieve and what kind of impurities we have. ","execution_count":null},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/bbc-fulltext-and-category/bbc-text.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['category'].value_counts().plot(kind='pie',autopct='%.2f%%') # almost equally distributed","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Clean Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_articles(df):\n    '''\n    Method to clean the existing DataFrame\n    '''\n\n    # remove all the new lines and spaces if there are any : - \\r and \\n\n    print('Step 1: Replacing...')\n    \n    df['text'] = df['text'].str.replace(\"\\r\", \" \")\n    df['text'] = df['text'].str.replace(\"\\n\", \" \")\n    df['text'] = df['text'].str.replace(\"    \", \" \")\n    df['text'] = df['text'].str.replace('\"', '') # remove double quotes\n    df['text'] = df['text'].str.lower()  # make all the words as lower case\n    \n    for sign in signs:\n        df['text'] = df['text'].str.replace(sign, '') # remove any special punctuations\n        \n    # remove Deshwal's the trailing s as it does not add any information in classification\n    df['text'] = df['text'].str.replace(\"'s\", \"\")\n\n\n    print('Step 2: Lemmatizing......')\n    \n    nrows = len(df)\n    lemmatized_text_list = []\n\n    for row in range(nrows):\n        lemmatized_list = [] # Create an empty list containing lemmatized words\n        text = df.loc[row]['text'] # Save the text and its words into an object\n        text_words = text.split(\" \")\n\n        for word in text_words:  # Iterate through every word to lemmatize\n            lemmatized_list.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n\n        lemmatized_text = \" \".join(lemmatized_list)  # Join the list to get a string\n        lemmatized_text_list.append(lemmatized_text) # Append to the list containing the texts\n\n    df['text'] = lemmatized_text_list\n    \n    \n    print('Step 3: Removing Stop Words....')\n    \n    for stop_word in stop_words:\n        re_sw = r\"\\b\" + stop_word + r\"\\b\"\n        df['text'] = df['text'].str.replace(re_sw, '')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_articles(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tokenization","execution_count":null},{"metadata":{"trusted":true,"_uuid":"a5737eceacbb85f26e2cb640332a60881818b100"},"cell_type":"code","source":"def tokenize_articles(df):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(df.text)\n    df['words'] = tokenizer.texts_to_sequences(df.text)\n    \n    return tokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = tokenize_articles(df)\n\ndf['article_length'] = df.words.apply(lambda i: len(i)) # if article length is less than 10, drop it\ndf = df[df['article_length']>=10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51f586fa92aff7a859d606622788b11fc51fb197","scrolled":true},"cell_type":"code","source":"df.article_length.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Padding","execution_count":null},{"metadata":{"trusted":true,"_uuid":"ea5417ae1057812e360c413f83a44f029c57df8e"},"cell_type":"code","source":"maxlen = 275 # 75 percentile is 273\nX = list(sequence.pad_sequences(df.words, maxlen=maxlen)) # add padding to the short length articles\n\ndf['encoded_cat'] = LabelEncoder().fit_transform(df['category']) # convert category as 0,1,2,3,4","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b133fdfe331a1551e98440f479f9a02349ceb64"},"cell_type":"markdown","source":"# Glove embedding\nIf you want to know what are embeddings and why are they used and how come they are useful, please find the [**Embedding** part of this Kernel](https://www.kaggle.com/deshwalmahesh/nlp-beginner-1-rnn-lstm-gru-embeddings-glove) for a very intutive and real world examples of embeddings.","execution_count":null},{"metadata":{"trusted":true,"_uuid":"af4c3b7b877179ba628315d4dc1fe72bf9db8b1d"},"cell_type":"code","source":"word_index = tokenizer.word_index\n\nEMBEDDING_DIM = 100\n\nembeddings_index = {}\nwith open('/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.100d.txt') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n\nprint(f'Unique tokens: {len(word_index)}')\nprint(f'Total Word Vectors: {len(embeddings_index)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f23e667f067ac480412511082f758f12bf4caf7"},"cell_type":"code","source":"embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3216d87143804d30f2d2e51983c3310f46402dc3"},"cell_type":"markdown","source":"## Split Data","execution_count":null},{"metadata":{"trusted":true,"_uuid":"714f2d758c18a70dee1ba7c2d2ac4122da12c45b"},"cell_type":"code","source":"X = np.array(X)\nY = np_utils.to_categorical(df['encoded_cat'].tolist())\n\nx_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e148c03fd8c688fc4de4cbf23c9e764b3823a92"},"cell_type":"markdown","source":"# Attention Layer","execution_count":null},{"metadata":{"trusted":true,"_uuid":"d78cf159bb9befb7ab27b5be5954e9ec85fc997d"},"cell_type":"code","source":"class Attention(Layer):\n    def __init__(self,**kwargs):\n        super(Attention,self).__init__(**kwargs)\n\n    def build(self,input_shape):\n        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n        super(Attention, self).build(input_shape)\n\n    def call(self,x):\n        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n        at=K.softmax(et)\n        at=K.expand_dims(at,axis=-1)\n        output=x*at\n        return K.sum(output,axis=1)\n\n    def compute_output_shape(self,input_shape):\n        return (input_shape[0],input_shape[-1])\n\n    def get_config(self):\n        return super(Attention,self).get_config()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Custom Metrics\n**Accuracy** is a very decieving metric in terms of interpreting results. If I started talkimg about how it'll take a whole Kernel for this but in simplest terms, let us suppose you have a biased data of `90:10` of 100 images out of which 80 are Non-Spam and 20 are spams. Supposed you trained a model on this data and fount out that you have excellent `accuracy` of `90%`. When you looked at the classified data from model, you found out that it could not detect a single spam. How come?? \n\nBecuse even if it classified 100 images as `Non-Spam`, it was correct 90% of the times. Can you relate?? **A broken watch is also correct twice a day**. That's what happened.\n\n\nSo we need to find out `Recall`, `Precision` and Harmonic mean of both called `F-1 score`. Because these metrices are not given by Keras, we have to make these metrices.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build Attention Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_layer = Embedding(len(word_index)+1,EMBEDDING_DIM,\n                            embeddings_initializer=Constant(embedding_matrix),input_length=maxlen,\n                            trainable=False) # do not train as these are GloVe pre trained","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inp = Input(shape=(maxlen,), dtype='int32')\nembedding= embedding_layer(inp)\nattention_out = Attention()(embedding)\nflat = Dense(254, activation='relu')(attention_out)\nflat = Dropout(0.47)(flat)\nflat = BatchNormalization()(flat)\nout = Dense(5,activation='softmax')(flat)\n\nAttentionModel = Model(inputs=inp, outputs=out)\nAttentionModel.compile(loss='categorical_crossentropy', optimizer='adam', \n                       metrics=['acc',recall_m,precision_m,f1_m])\n\nAttentionModel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mcp = ModelCheckpoint(filepath='/kaggle/working/best_weights.h5',verbose=1,save_best_only=True,\n                      save_weights_only=True)\nes = EarlyStopping(min_delta=0.01,patience=2,verbose=1)\nrlp = ReduceLROnPlateau(factor=0.005,patience=1,verbose=1,min_delta=0.001,min_lr=1e-6)\n\ncallbacks = [mcp,es,rlp]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training","execution_count":null},{"metadata":{"trusted":true,"_uuid":"7c137a522a0113b183672fdd99939a2dd61d1f38"},"cell_type":"code","source":"training_history = AttentionModel.fit(x_train,y_train,batch_size=32,epochs=10,\n                                      validation_data=(x_val, y_val),callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Attention on top of LSTM\nLet us try to add a LSTM layer with Attention to see what happens. It surely will make the model slower.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"inp = Input(shape=(maxlen,), dtype='int32')\nembedding= embedding_layer(inp)\nlstm_out = LSTM(254, dropout=0.27, recurrent_dropout=0.25, return_sequences=True)(embedding)\nx = Dropout(0.49)(lstm_out)\nattention_out = Attention()(x)\nflat = Dense(512, activation='relu')(attention_out)\nflat = Dropout(0.69)(flat)\nflat = BatchNormalization()(flat)\nout = Dense(5,activation='softmax')(flat)\n\nWithLSTM = Model(inputs=inp, outputs=out)\nWithLSTM.compile(loss='categorical_crossentropy', optimizer='adam', \n                       metrics=['acc',recall_m,precision_m,f1_m])\n\nWithLSTM.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_history2 = WithLSTM.fit(x_train,y_train,batch_size=64,epochs=5,\n                                      validation_data=(x_val, y_val),callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Thank You\nAs you have seen that our models are producing good results with the methods. You can also try to make implement some new ideas by applyting `Bidirectional` layer wrapped around LSTM or GRU with the Attention Layer to find out how it goes. To improve model, you can change epochs, batch sizes, number of layers, number of neurons, dropout values and much more.\n\nHope you had some good understanding of the topic. Please let me know in case otherwise for corrections or improvements.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}