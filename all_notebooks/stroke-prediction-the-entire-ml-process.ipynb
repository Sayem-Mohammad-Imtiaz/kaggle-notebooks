{"cells":[{"metadata":{},"cell_type":"markdown","source":"# If you like the notebook please upvote it"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **LOADING THE DATASET**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**dropping the id column**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop('id', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DATA EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**lets look for some insights**"},{"metadata":{},"cell_type":"markdown","source":"**first I want to look if a person has a higher chance of geeting a stroke if he has hypertension**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot(x='stroke', col='hypertension', kind='count', data=data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**a person with no hyper tension has a slightly higher change of having a stroke**"},{"metadata":{},"cell_type":"markdown","source":"**Now I want to look if a person has a higher chance of geeting a stroke if he has heart disease**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot(x='stroke', col='heart_disease', kind='count', data=data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**a person who does not have a heart disease has a slightly higher chance of getting a stroke**"},{"metadata":{},"cell_type":"markdown","source":"**lets see if marriage effects the chances to stroke**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot(x='stroke', col='ever_married', kind='count', data=data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**a person who is married has a higher chance of stroke**"},{"metadata":{},"cell_type":"markdown","source":"**lets also see if gender effects the chances of stroke**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot(x='stroke', col='gender', kind='count', data=data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**females have a slightly higher chance of getting a stroke compared to male**"},{"metadata":{},"cell_type":"markdown","source":"**and finally lets see if smoking effects the chances of stroke**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot(x='stroke', col='smoking_status', kind='count', data=data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**a person who has never smoked has a higher chance of getting stroke**"},{"metadata":{},"cell_type":"markdown","source":"**what is the highest glucose level recorded among the people who had stroke?**"},{"metadata":{"trusted":true},"cell_type":"code","source":"avg = data[data['stroke']==1].avg_glucose_level\nmax(avg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**lets plot the histogram of the data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **DATA PROCESSING**"},{"metadata":{},"cell_type":"markdown","source":"**first lets check for missing values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**there are 201 missing values which is just 3% of the data and we can dop them**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**lets check for any duplicate values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.duplicated().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**there are no duplicate values**"},{"metadata":{},"cell_type":"markdown","source":"**Feature Binning**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import KBinsDiscretizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**creating all the binners**"},{"metadata":{"trusted":true},"cell_type":"code","source":"age_binner = KBinsDiscretizer(n_bins=5, encode='ordinal')\nglucode_lvl_binner = KBinsDiscretizer(n_bins=4, encode='ordinal')\nbmi_binner = KBinsDiscretizer(n_bins=5, encode='ordinal')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**binning the features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['age_bins'] = age_binner.fit_transform(data['age'].values.reshape(-1,1)).astype('int64')\ndata['avg_glucose_level_bins'] = glucode_lvl_binner.fit_transform(data['avg_glucose_level'].values.reshape(-1,1)).astype('int64')\ndata['weight'] = bmi_binner.fit_transform(data['bmi'].values.reshape(-1,1)).astype('int64')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Encoding categorical data"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = data.select_dtypes(object).columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\nfor col in cat_cols:\n    data[col] = le.fit_transform(data[col])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**scaling continuous data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler, StandardScaler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**robust scaler to remove outliars and standard scaler to scale the data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_scale = ['age', 'avg_glucose_level', 'bmi']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"robust = RobustScaler()\nstandard = StandardScaler()\n\ndata[cols_to_scale] = robust.fit_transform(data[cols_to_scale])\ndata[cols_to_scale] = standard.fit_transform(data[cols_to_scale])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature Correlation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = data.corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(corr, cmap='coolwarm', annot=True, square=True, fmt='.2f')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**stroke aka the target column has no highly correlated features but if your target column has highly correlated features then you gotta remove them or reduce their dimentions using PCA**"},{"metadata":{},"cell_type":"markdown","source":"**Feature Imbalance**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = data.drop('stroke', axis=1), data['stroke']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**there is a lot of imbalance in the data lets fix this**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smote = SMOTE()\n\nX, y = smote.fit_resample(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**now our data is balanced**"},{"metadata":{},"cell_type":"markdown","source":"# **SPLITTING DATA INTO TRAINING AND TESTING SETS**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SPLITTING TRAINING DATA INTO TRAINING AND VALIDATION SETS FOR MODEL SELECTION"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_, x_val, y_train_, y_val = train_test_split(x_train, y_train, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **MODEL SELECTION**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score, classification_report, mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_selection(x_train_, x_val, y_train_, y_val, model):\n  model = model()\n  model.fit(x_train_, y_train_)\n\n  pred = model.predict(x_val)\n\n  error = np.sqrt(mean_squared_error(y_val, pred))\n  acc = f1_score(y_val, pred)\n  report = classification_report(y_val, pred)\n  train_score = model.score(x_train_, y_train_)\n  val_score = model.score(x_val, y_val)\n\n  print('Error:', error*100)\n  print('\\n')\n  print('ACC:', acc*100)\n  print('\\n')\n  print('Classification report:', report)\n  print('\\n')\n  print('Train Score:', train_score*100)\n  print('\\n')\n  print('Val Score:', val_score*100)\n  print('\\n')\n  print('Is overfitting:', True if train_score>val_score else False)\n  print('\\n')\n  print('Overfitting by:',train_score*100-val_score*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extratrees = model_selection(x_train_, x_val, y_train_, y_val, ExtraTreesClassifier)\nextratrees ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gradient = model_selection(x_train_, x_val, y_train_, y_val, GradientBoostingClassifier)\ngradient","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"randomforest = model_selection(x_train_, x_val, y_train_, y_val, RandomForestClassifier)\nrandomforest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ada = model_selection(x_train_, x_val, y_train_, y_val, AdaBoostClassifier)\nada","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = model_selection(x_train_, x_val, y_train_, y_val, XGBClassifier)\nxgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm = model_selection(x_train_, x_val, y_train_, y_val, LGBMClassifier)\nlgbm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"catboost = model_selection(x_train_, x_val, y_train_, y_val, CatBoostClassifier)\ncatboost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree = model_selection(x_train_, x_val, y_train_, y_val, DecisionTreeClassifier)\ntree","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic = model_selection(x_train_, x_val, y_train_, y_val, LogisticRegression)\nlogistic","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sgd = model_selection(x_train_, x_val, y_train_, y_val, SGDClassifier)\nsgd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**I will use LGBMClassifier because it gives a good accuracy and the  error and overfitting rate is low**"},{"metadata":{},"cell_type":"markdown","source":"# **HYPER PARAMETER TUNING**"},{"metadata":{},"cell_type":"markdown","source":"**if you get an accuracy of like 95+ you dont need to do hyper parameter tuning because it may decrease your accuracy**"},{"metadata":{},"cell_type":"markdown","source":"**even though our accuracy is excellent i am including this part just to show how hyper parameter tuning works**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LGBMClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'num_leaves': sp_randint(6, 50), \n            'min_child_samples': sp_randint(100, 500), \n             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n             'subsample': sp_uniform(loc=0.2, scale=0.8), \n             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100],\n             'scale_pos_weight':[1,2,6,12],\n             'depth': sp_randint(3,10),\n             'learning_rate': sp_uniform()}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search = RandomizedSearchCV(model, params, n_iter=50, scoring='f1', n_jobs=-1, cv=5)\nsearch.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Best Score:', search.best_score_)\nprint('\\n')\nprint('Best Params:', search.best_params_)\nprint('\\n')\nprint('Best Estimator:', search.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We got our optimal parameters now lets build the model using them just copy the best estimator and paste it in another cell**"},{"metadata":{},"cell_type":"markdown","source":"# **MODEL BUILDING AND TRAINING**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LGBMClassifier(colsample_bytree=0.9040713748241764, depth=8,\n               learning_rate=0.887075684185765, min_child_samples=208,\n               min_child_weight=1e-05, num_leaves=35, reg_alpha=0.1,\n               reg_lambda=1, scale_pos_weight=2, subsample=0.507606910665195)\n\nmodel.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **PREDICTIONS**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(x_test)\npred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# METRIC CHECK"},{"metadata":{},"cell_type":"markdown","source":"**Mean Squared Error**"},{"metadata":{"trusted":true},"cell_type":"code","source":"error = np.sqrt(mean_squared_error(y_test, pred))*100\nerror","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**F1 Score**"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = f1_score(y_test, pred)\nacc*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Classification Report**"},{"metadata":{"trusted":true},"cell_type":"code","source":"classification_report(y_test, pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**OVER FITTING CHECK**"},{"metadata":{"trusted":true},"cell_type":"code","source":"over_fitting_rate = model.score(x_train, y_train)*100 - model.score(x_test, y_test)*100\nover_fitting_rate","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**the model is overfitting by 4% which is honestly not a lot so we can leave it there**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}