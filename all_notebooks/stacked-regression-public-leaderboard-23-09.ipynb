{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler,StandardScaler,OneHotEncoder\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error,auc\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/the-great-indian-hiring-hackathon/Participants_Data_TGIH/Train.csv')\ntest = pd.read_csv('/kaggle/input/the-great-indian-hiring-hackathon/Participants_Data_TGIH/Test.csv')\nsubmission = pd.read_csv('/kaggle/input/the-great-indian-hiring-hackathon/Participants_Data_TGIH/Sample Submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Removing the outliers\n\nIn the data exploration phase it has been found that there are some outliers present in the `Quantity` column so removing those outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"IQR = train['Quantity'].quantile(0.75) - train['Quantity'].quantile(0.25)\n\nlower = -10000\nupper = 6000\n\noutliers = np.where(train['Quantity'] > upper , True , np.where(train['Quantity'] < lower , True, False))\n\ntrain = train.loc[~(outliers),]\n\ntrain.drop_duplicates(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train.drop('UnitPrice',axis=1)\ny= train['UnitPrice']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_idx = len(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Combining train and test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['is_train'] = 1\ntest['is_train'] = 0\n\ndf_combine = pd.concat([train_df,test],axis=0,ignore_index=True)\nprint(df_combine.head())\n\ndf_combine['CustomerID'] = df_combine['CustomerID'].astype('int64')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding a feature named customer invoice id\ndf_combine['customer_invoice_id'] = df_combine['CustomerID'].astype(str)+\"_\"+df_combine['InvoiceNo'].astype(str)\n\n#Adding a feature named total visited - total number of transactions done by the customer\ntotal_visited = df_combine.groupby(by=['CustomerID'],as_index=False)['InvoiceNo'].count()\ndf_combine['visit_count'] = 0\n\nfor idx,count in zip(total_visited.CustomerID,total_visited.InvoiceNo):\n    cus_idx = df_combine[df_combine['CustomerID']==idx]['visit_count'].index\n    df_combine['visit_count'].loc[cus_idx] = count\n\n# Based on the feature visit count we will create a new column frequent - if he visits store more than 52times which is weekly once in a full year\ndf_combine['is_frequent'] = np.where(df_combine['visit_count']>=52,1,0)\n# if visited more than 52 times frequent customer \n# if visited less than 52 times non frequent customer\n\n\n# Adding a feature number_of_times_ordered - where it contains the number of orders \n# totally placed for that stock\norder_count = df_combine.groupby(by=['StockCode'],as_index=False)['InvoiceNo'].count()\ndf_combine['stock_order_count'] = 0\n\nfor idx,count in zip(order_count.StockCode,order_count.InvoiceNo):\n    stk_idx = df_combine[df_combine['StockCode']==idx]['stock_order_count'].index\n    df_combine['stock_order_count'].loc[stk_idx] = count\n    \n\n    \ndf_combine.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adding new features based on the `InvoiceDate`"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding new features using the InvoiceDate column\n\ndf_combine['InvoiceDate'] = pd.to_datetime(df_combine.InvoiceDate, format='%Y-%m-%d %H:%M:%S')\n\ndf_combine['day'] = df_combine['InvoiceDate'].dt.day\ndf_combine['month'] = df_combine['InvoiceDate'].dt.month_name()\ndf_combine['year'] = df_combine['InvoiceDate'].dt.year\ndf_combine['week'] = df_combine['InvoiceDate'].dt.week\ndf_combine['day_of_week'] = df_combine['InvoiceDate'].dt.dayofweek\ndf_combine['is_working_day'] = np.where(df_combine['day_of_week'].isin([0,1,2,3,4,6]),1,0)\ndf_combine['year_month'] = df_combine['year'].astype(str)+\"_\"+df_combine['month']\ndf_combine['is_month_start'] = df_combine['InvoiceDate'].dt.is_month_start\ndf_combine['is_month_end'] = df_combine['InvoiceDate'].dt.is_month_end\ndf_combine['quarter'] = df_combine['InvoiceDate'].dt.quarter\ndf_combine['is_year_start'] = df_combine['InvoiceDate'].dt.is_year_start\ndf_combine['is_year_end'] = df_combine['InvoiceDate'].dt.is_year_end\n\n#Extracting time features\ndf_combine['hour'] = df_combine['InvoiceDate'].dt.hour\n\n\ndf_combine = df_combine.drop('InvoiceDate',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_combine.drop('is_train',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## OneHotEncoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"ohe = OneHotEncoder()\ncols = df_combine.columns.values\n\ndf_combine_ohe = ohe.fit_transform(df_combine[cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = df_combine_ohe[:train_idx]\ntest_df = df_combine_ohe[train_idx:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmse(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model building\n\nBelow are the different base models which will be trained seperately on the whole training dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso = Lasso(alpha=0.0005, random_state = 1,max_iter=100)\n\nENet = ElasticNet(alpha = 0.0005, l1_ratio=0.9, random_state = 3,max_iter=100)\n\nGBoost = GradientBoostingRegressor(n_estimators = 100,learning_rate=0.05,\n                                   max_depth = 10, random_state=5)\n\nmodel_rf = RandomForestRegressor(max_depth=17,n_estimators=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [lasso,ENet,GBoost,model_rf]\nscores={}\nfor model in models:\n    print(model)\n    model.fit(X,y)\n    tr_pred = model.predict(X)\n    scores[model] = rmse(y,tr_pred)\n    print(scores[model])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stacked Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=3):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = y.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nstacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, model_rf),\n                                                 meta_model = lasso)\n\n\nstacked_averaged_models.fit(X, y_train)\n\nstacked_train_pred = stacked_averaged_models.predict(X)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test_df))\nprint(rmse(y_train, stacked_train_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predicting for test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"stacked_pred_ = stacked_averaged_models.predict(test_df)\nstacked_pred_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['UnitPrice'] = stacked_pred_.round(2)\n# scored 23.09\nsubmission.to_csv('avg_model_sub.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> The above submission scored 23.09 in the public leaderboard\n\nStacked Regression algorithm refernce - https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard"},{"metadata":{},"cell_type":"markdown","source":"### **If you find this notebook helpful please upvote**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}