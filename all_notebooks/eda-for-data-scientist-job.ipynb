{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"First import all the libraries we need and read the dataset:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import shapiro\nfrom scipy.stats import anderson\nfrom scipy.stats import normaltest\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport re\nimport warnings\nfrom pandas.api.types import is_string_dtype\nfrom pandas.api.types import is_numeric_dtype\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\ndata = pd.read_csv('../input/data-scientist-jobs/DataScientist.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like we don't need columns: \"Unnamed: 0\" and \"index\""},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = data.drop('Unnamed: 0', 1)\ndata = data.drop('index', 1)\n\nprint(data.shape)\nprint(data.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I see that in some columns we have \"-1\" value, which can be interpreted as null value, so let's check how many \"null\" values we have in every column"},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_missing_values():\n    for column in data:\n        nullAmount = None\n        if (is_numeric_dtype(data[column])):\n            nullAmount = data[data[column] == -1].shape[0]\n        else:\n            nullAmount = data[data[column] == \"-1\"].shape[0]\n        print('{}{},  \\t{:2.1f}%'.format(column.ljust(20),nullAmount, nullAmount*100/data[column].shape[0]))\n    \ncount_missing_values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that columns like \"Competitors\" and \"Easy Apply\" has 70.6% and 95.8% of null-values, so we can just delete this columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop('Competitors', 1)\ndata = data.drop('Easy Apply', 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can fill missing values for the \"Rating\" column using interpolation of the values we already had. I think we can just remove rows with missing values from columns like \"Headquarters\", \"Size\", \"Type of ownership\" and \"Revenue\". Let's do it and take one more looks at how many missing values we still have:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.replace(-1, np.nan)\ndata[\"Rating\"].interpolate(method='linear', direction = 'forward', inplace=True) \n\ndata.drop(data[data['Headquarters'] == \"-1\"].index, inplace=True)\ndata.drop(data[data['Size'].str.contains(\"-1\")].index, inplace=True)\ndata.drop(data[data['Type of ownership'].str.contains(\"-1\")].index, inplace=True)\ndata.drop(data[data['Revenue'].str.contains(\"-1\")].index, inplace=True)\nprint(data.shape)\ncount_missing_values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have only 8.4% of missing values in columns \"Industry\" and \"Sector\", it's relatively small value so I think we can also just remove this rows. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(data[data['Sector'].str.contains(\"-1\")].index, inplace=True)\ndata.drop(data[data['Industry'].str.contains(\"-1\")].index, inplace=True)\nprint(data.shape)\ncount_missing_values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So now we have filled dataframe with 13 columns and 3356 rows. Let's take a look at what we have there, let's do the EDA.\n\nFirst let's see which job positions we have:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Job Title'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's make analysis for the \"Data Scientist\" jobs:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data =  data[data['Job Title'].str.contains(\"Data Scientist\") | data['Job Title'].str.contains(\"Data Analyst\")]\nprint(data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next let's parse 'Salary Estimate' column to 'SalaryAverage' one:"},{"metadata":{"trusted":true},"cell_type":"code","source":"HOURS_PER_WEEK = 40\nWEEKS_PER_YEAR = 52\nTHOUSAND = 1000\n\ndef return_digits(x):\n    result = re.findall(r'\\d+', str(x))\n    result = int(result[0]) if result else 0\n    return result\n\ndef return_salary(string, isFrom):\n    patternMain = None\n    patternPerHour = None\n    if(isFrom):\n        patternMain = r'^\\$\\d+K';\n        patternPerHour = r'^\\$\\d+';\n    else:\n        patternMain = r'-\\$\\d+K';\n        patternPerHour = r'-\\$\\d+';\n    \n    result = None\n    if('Per Hour' in string):\n        result = re.findall(patternPerHour, str(string))\n        result = return_digits(result[0]) if result else 0\n        result = result * HOURS_PER_WEEK * WEEKS_PER_YEAR\n    else:\n        result = re.findall(patternMain, str(string))\n        result = return_digits(result[0]) if result else 0\n        result = result * THOUSAND\n    return result\n\ndef return_average_salary(x):\n    from_salary = return_salary(x, True)\n    to_salary = return_salary(x, False)\n    result = (from_salary+to_salary)/2\n    return result\n\ndata['SalaryAverage'] =  data['Salary Estimate'].apply(return_average_salary)\nprint(data['SalaryAverage'].describe())\nprint(sns.distplot(data['SalaryAverage']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The average salary distribution has 2 peaks, lets' try to see what can explain this peaks:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#SalaryAverage/Rating plot\nprint(sns.pairplot(x_vars=[\"Rating\"], y_vars=[\"SalaryAverage\"],data=data,  size=5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SalaryAverage/Sector plot\nprint(sns.pairplot(x_vars=[\"SalaryAverage\"], y_vars=[\"Sector\"],data=data,  size=5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SalaryAverage/Location plot\nprint(sns.pairplot(x_vars=[\"Location\"], y_vars=[\"SalaryAverage\"],data=data,  size=5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plots above shows us that this average salary distribution can be explained by location. Let's check it. First, let's get states or country where jobs are located:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def return_state(string):\n    patternMain = r',\\s[A-Z]{2}';    \n    result = re.findall(patternMain, str(string))\n    if result:\n        result = re.findall(r'[A-Z]{2}', str(result[0]))[0]\n    else:\n        result = string.split(r', ')[1]\n    return result\n\ndata['State'] =  data['Location'].apply(return_state)\nprint(data['State'].head())\nprint(data['State'].value_counts())\nprint(sns.pairplot(x_vars=[\"SalaryAverage\"], y_vars=[\"State\"],data=data,  size=5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plot above shows that most of the jobs position in the states like NY, NJ, CA are probably spread around bigger salary then in other places. Let's split data by state and see how our distribution will change. For example, let's take a look at average salary distribution in the states mentioned above:"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataBiggerSalary = data[data['State'].isin(['NY', 'NJ', 'CA'])] \nprint(sns.distplot(dataBiggerSalary['SalaryAverage'], fit=norm))\nprint(dataBiggerSalary.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import norm, expon, cauchy\ndataSmallerSalary = data[~data['State'].isin(['TX', 'NY', 'NJ', 'CA'])] \nprint(dataSmallerSalary.shape)\nprint(sns.distplot(dataSmallerSalary['SalaryAverage']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The average salary distribution in other states then NY, NJ and CA, looks different then normal, more like It looks like the average salary in the NY, NJ and CA states has an approximately bell-shape and can be  normal distributed, but it also could have an outliers. Let's check the outliers in the average salary in the NY, NJ and CA: "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sns.pairplot(x_vars=[\"SalaryAverage\"], y_vars=[\"State\"],data=dataBiggerSalary,  size=5))\nprint(dataBiggerSalary.boxplot(by ='State', column =['SalaryAverage']))\nprint(dataBiggerSalary[\"SalaryAverage\"].describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the salary less then 75000 can be outliers. Let's remove it:"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataBiggerSalary.drop(dataBiggerSalary[dataBiggerSalary['SalaryAverage'] < 75000].index, inplace=True)\nprint(dataBiggerSalary.shape)\nprint(sns.distplot(dataBiggerSalary['SalaryAverage'], fit=norm))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check normality with Shapiro Wilk test (as it's the most powerful test when testing for a normal distribution):"},{"metadata":{"trusted":true},"cell_type":"code","source":"def testNormality(data):\n    stat, p = shapiro(data)\n    print('Statistics=%.3f, p=%.3f' % (stat, p))\n    alpha = 0.05\n    if p > alpha:\n        print('Sample looks Gaussian (fail to reject H0)')\n    else:\n        print('Sample does not look Gaussian (reject H0)')\n        \ntestNormality(dataBiggerSalary['SalaryAverage'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So the results show that the average salary in this states (NY, NJ and CA) is not normal distributed.\n\nLet's see maybe we can find some interesting and usefull information from the data other then salary in our dataset: "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.columns)\nprint(sns.countplot(y='Sector',data=data, order = data['Sector'].value_counts().index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sns.countplot(y='State',data=data, order = data['State'].value_counts().index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sns.countplot(y='Size',data=data, order = data['Size'].value_counts().index))\nprint(data[\"Company Name\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,16))\nprint(sns.countplot(x='Rating',data=data, order = data['Rating'].value_counts().index))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the plots above we can see that most of the job positions is in IT and Business Services sectors, from companies like IBM, Amazon, Apple and Facebook and located in the states like CA and TX."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}