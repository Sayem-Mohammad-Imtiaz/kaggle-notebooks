{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math\nimport matplotlib.pyplot as plt\nimport keras\nimport pandas as pd\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.layers import *\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import EarlyStopping\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport sklearn.metrics as sm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Update the file path only here.\ndf = pd.read_csv(\"../input/vdisk1-for-dl/disk1DL.csv\")\n\n# Drop unnamed columns\ndf = df.loc[:, ~df.columns.str.contains('^Unnamed')]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = df[['startTid','lebel data']]\ndf1['startTid'] = pd.to_datetime(df.startTid).dt.strftime('%d-%m-%Y %H:%M')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convert Catg variable into numerical values","metadata":{}},{"cell_type":"code","source":"print (len(df['diskObjekt'].unique()))\ndf['diskObjekt'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Have to replace these 23 unique string with numbers to get this column into numerical format to input to model.\ndf.diskObjekt = pd.Categorical(df.diskObjekt)\ndf['diskObjekt_code'] = df.diskObjekt.cat.codes\n\n# Will do the same for column diskProgram\ndf.diskProgram = pd.Categorical(df.diskProgram)\ndf['diskProgram_code'] = df.diskProgram.cat.codes\n\n# Drop the old columns with catg values, we will retain the numerical code values.\ndf.drop(['diskObjekt','diskProgram'], axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Updated columns are added as last columns.\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Break Timestamp into further columns (to get data spread across the time in feature eng and model learning)","metadata":{}},{"cell_type":"code","source":"# Can split into Year, month, day, hour, min and seconds as individual columns.\ndf['startTid']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert the 'startTid' column to datetime format\ndf['startTid']= pd.to_datetime(df['startTid'])\n \n# Add new columns, with individual values\ndf['startTid_year'] = df['startTid'].dt.year\ndf['startTid_month'] = df['startTid'].dt.month\ndf['startTid_day'] = df['startTid'].dt.day\ndf['startTid_hour'] = df['startTid'].dt.hour\ndf['startTid_min'] = df['startTid'].dt.minute\ndf['startTid_sec'] = df['startTid'].dt.second\n\n# Drop the main old colum of 'startTid'\ndf.drop(['startTid'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Will follow the same steps for 'stoppTid' column","metadata":{}},{"cell_type":"code","source":"# convert the 'stoppTid' column to datetime format\ndf['stoppTid']= pd.to_datetime(df['stoppTid'])\n \n# Add new columns, with individual values\ndf['stoppTid_year'] = df['stoppTid'].dt.year\ndf['stoppTid_month'] = df['stoppTid'].dt.month\ndf['stoppTid_day'] = df['stoppTid'].dt.day\ndf['stoppTid_hour'] = df['stoppTid'].dt.hour\ndf['stoppTid_min'] = df['stoppTid'].dt.minute\ndf['stoppTid_sec'] = df['stoppTid'].dt.second\n\n# Drop the main old colum of 'stoppTid'\ndf.drop(['stoppTid'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['lebel data'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our target column have 1 and 0 as target value, so we will be using classification approach.","metadata":{}},{"cell_type":"markdown","source":"## 1- LSTM","metadata":{}},{"cell_type":"code","source":"df1.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Based on rows, split into training and testing set\ntraining_set = df1.iloc[:2000, 1:2].values\ntesting_set = df1.iloc[2000:, 1:2].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_set.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testing_set.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To scale/standarize the values to a normal range.\nfrom sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler(feature_range = (0, 1))\ntraining_set_scaled = sc.fit_transform(training_set)\n\ntesting__set_scaled = sc.fit_transform(testing_set)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Spliting data into x train and y train for training purpose, into 3d matrix form to feed into model\nX_train = []\ny_train = []\n\n# Setting 2000 rows, with 60 records interval window\nfor i in range(60, 2000):\n    X_train.append(training_set_scaled[i-60:i, 0])\n    y_train.append(training_set_scaled[i, 0])\n    \n# Converting into numpy array\nX_train, y_train = np.array(X_train), np.array(y_train)\n\nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n\nprint(X_train.shape)\nprint (y_train.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Spliting data into x test and y test for testing purpose, into 3d matrix form to feed into model\nX_test = []\ny_test = []\n# Setting 887 rows, with 60 records interval window\nfor i in range(60, 887):\n    X_test.append(testing__set_scaled[i-60:i, 0])\n    y_test.append(testing__set_scaled[i, 0])\n    \n# Converting into numpy array\nX_test, y_test = np.array(X_test), np.array(y_test)\n\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n\nprint(X_test.shape)\nprint (y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model object\nregressor = Sequential()\n\n\n# Adding lstm layer step by step and giving input data to each layer.\nregressor.add(LSTM(units = 50, return_sequences = True, activation = 'relu',input_shape = (X_train.shape[1], 1)))\nregressor.add(Dropout(0.2))\n\n# Adding drop out for better learning and to prevent overfitting\nregressor.add(LSTM(units = 50, activation = 'relu',return_sequences = True))\nregressor.add(Dropout(0.2))\n\nregressor.add(LSTM(units = 50, activation = 'relu',return_sequences = True))\nregressor.add(Dropout(0.2))\n\nregressor.add(LSTM(units = 1, activation = 'sigmoid'))\nregressor.add(Dropout(0.2))\n\n# Final output layer\nregressor.add(Dense(units = 1, activation = 'sigmoid'))\n\n# compile the model\nregressor.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics=['accuracy'])\n\n# Training the model on training data over here, wiht given batch size and number of iteration  (epochs)\nregressor.fit(X_train, y_train, epochs = 20, batch_size = 32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Used the trained model to predict on test data and make predictions, and show\nlstm_pred = regressor.predict(X_test)\nlstm_pred = np.round(sc.inverse_transform(lstm_pred))\ndf_res = pd.DataFrame()\ndf_res ['Actual'] = y_test\ndf_res ['Predicted'] = lstm_pred\n\ndf_res","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Mean absolute error =\", round(sm.mean_absolute_error(y_test, lstm_pred), 2)) \nprint(\"Mean squared error =\", round(sm.mean_squared_error(y_test, lstm_pred), 2)) \nprint(\"Median absolute error =\", round(sm.median_absolute_error(y_test, lstm_pred), 2)) \nprint(\"Explain variance score =\", round(sm.explained_variance_score(y_test, lstm_pred), 2)) \nprint(\"R2 score =\", round(sm.r2_score(y_test, lstm_pred), 2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RNN","metadata":{}},{"cell_type":"code","source":"# Creating RNN model with input and hidden layers.\n# Ouput layer have one node to give output\nmodel = keras.models.Sequential([\n    keras.layers.SimpleRNN(50, return_sequences=True, input_shape=[None, 1]),\n    keras.layers.SimpleRNN(45, return_sequences=True),\n    keras.layers.SimpleRNN(40, return_sequences=True),\n    keras.layers.SimpleRNN(35, return_sequences=True),\n    keras.layers.SimpleRNN(30, return_sequences=True),\n    keras.layers.SimpleRNN(25, return_sequences=True),\n    keras.layers.SimpleRNN(20, return_sequences=True),\n    keras.layers.SimpleRNN(1)\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Traing the model and evaluate the model performance on test data.\noptimizer = keras.optimizers.Adam(lr=0.005)\nmodel.compile(loss=\"mse\", optimizer=optimizer)\nhistory = model.fit(X_train, y_train, epochs=20,\n                    validation_data=(X_test, y_test))\nprint (\"Model Accu\")\nmodel.evaluate(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making predicitons and giving transformed predction array\ny_pred = model.predict(X_test)\ny_pred = np.round(sc.inverse_transform(y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Mean absolute error =\", round(sm.mean_absolute_error(y_test, y_pred), 2)) \nprint(\"Mean squared error =\", round(sm.mean_squared_error(y_test, y_pred), 2)) \nprint(\"Median absolute error =\", round(sm.median_absolute_error(y_test, y_pred), 2)) \nprint(\"Explain variance score =\", round(sm.explained_variance_score(y_test, y_pred), 2)) \nprint(\"R2 score =\", round(sm.r2_score(y_test, y_pred), 2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}