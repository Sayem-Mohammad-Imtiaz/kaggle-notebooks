{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import confusion_matrix\nfrom tensorflow.keras import backend as K\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices()) # list of DeviceAttributes\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# load data\ntrain = pd.read_csv('/kaggle/input/health-insurance-cross-sell-prediction/train.csv')\nsubmit = pd.read_csv('/kaggle/input/health-insurance-cross-sell-prediction/test.csv')\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check missing / duplicate values\nprint('# of missing values in train: %s' % train.isnull().sum().sum())\nprint('# of missing values in submit: %s' % submit.isnull().sum().sum())\n\n# check duplicate values\nprint('# of duplicate values in train: %s' % (train.id.unique().shape[0] - train.id.shape[0]))\nprint('# of duplicate values in submit: %s' % (submit.id.unique().shape[0] - submit.id.shape[0]))\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_mean_response(dimension):\n    response = train.groupby(dimension)[['Response']].mean()\n    print(response)\n    return\n\ndef plot_mean_response(dimension):\n    response = train.groupby(dimension)[['Response']].mean()\n    plt.figure()\n    plt.plot(response, 'o')\n    return\n\ndef plot_distribution(dimension):\n    plt.figure()\n    neg = train[train.Response==0]\n    pos = train[train.Response==1]\n    \n    sns.distplot(neg[dimension], hist=False, kde=True, \n             bins=int(180/5), color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4}, label='%s when Response=0' % dimension)\n    \n    sns.distplot(pos[dimension], hist=False, kde=True, \n             bins=int(180/5), color = 'red', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4}, label='%s when Response=1' % dimension)\n    \ndef plot_hist(dimension):\n    \n    fig, ax = plt.subplots(1,2)\n    fig.tight_layout()\n                    \n    neg = train[train.Response == 0][dimension].to_frame()\n    pos = train[train.Response == 1][dimension].to_frame()\n\n    pos_percentages = (pos.groupby(dimension).size() / pos.groupby(dimension).size().sum()) * 100\n    pos_percentages = pos_percentages.reset_index(name='frequencies')\n   \n    \n    neg_percentages = (neg.groupby(dimension).size() / neg.groupby(dimension).size().sum()) * 100\n    neg_percentages = neg_percentages.reset_index(name='frequencies')\n    \n    neg_plot = sns.barplot(x=neg_percentages[dimension], y=neg_percentages['frequencies'], ax=ax[0])\n    pos_plot = sns.barplot(x=pos_percentages[dimension], y=pos_percentages['frequencies'], ax=ax[1])\n    \n    ax[0].set(xlabel='%s (Response = 0)' % dimension, ylabel='Frequencies (%)')\n    ax[1].set(xlabel='%s (Response = 1)' % dimension, ylabel='Frequencies (%)')\n  \n    fig.show()\n\n    return\n    \n    \n# plot_distribution('Age')\n# plot_distribution('Vintage')\n# plot_distribution('Annual_Premium')\n\n# plot_hist('Gender')\n# plot_hist('Driving_License')\n# plot_hist('Previously_Insured')\n# plot_hist('Vehicle_Age')\n# plot_hist('Vehicle_Damage')\n\n\nprint(train.groupby('Response')['id'].count() / train.shape[0])\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(X):\n    df_dropped = X.drop(['Vintage', 'Annual_Premium'], axis=1)\n \n    X = pd.get_dummies(df_dropped, \n                         columns=['Gender', 'Driving_License', \n                                  'Region_Code', 'Previously_Insured',\n                                  'Vehicle_Age', 'Vehicle_Damage', \n                                  'Policy_Sales_Channel'])\n    return X\n    \n# construct X_train\n\n\n\n\nX_raw = train.iloc[:, 1:11]\ntrain_rows = train.shape[0]\ny = train[['Response']]\nX_submit_raw = submit.iloc[:, 1:11]\n\npreprocessed = preprocess(pd.concat([X_raw, X_submit_raw]))\n\nX = preprocessed.iloc[0:(train_rows), :]\nX_submit = preprocessed.iloc[train_rows:,: ]\n\n\nX_train, X_dev_test, y_train, y_dev_test = train_test_split(X, y, test_size=0.05)\nX_dev, X_test, y_dev, y_test = train_test_split(X_dev_test, y_dev_test, test_size=0.5)\n\n\nprint('Train size: %s' % X_train.shape[0])\nprint('Dev size: %s' % X_dev.shape[0])\nprint('Test size: %s' % X_test.shape[0])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import Callback\n\n#from s\n\nclass Metrics(Callback):\n    def __init__(self, validation):   \n        super(Metrics, self).__init__()\n        self.validation = validation    \n            \n        print('validation shape', len(self.validation[0]))\n        \n    def on_train_begin(self, logs={}):        \n        self.val_f1s = []\n        self.val_recalls = []\n        self.val_precisions = []\n     \n    def on_epoch_end(self, epoch, logs={}):\n        val_targ = self.validation[1]   \n        val_predict = (np.asarray(self.model.predict(self.validation[0]))).round()        \n    \n        val_f1 = f1_score(val_targ, val_predict)\n        val_recall = recall_score(val_targ, val_predict)         \n        val_precision = precision_score(val_targ, val_predict)\n        \n        self.val_f1s.append(round(val_f1, 6))\n        self.val_recalls.append(round(val_recall, 6))\n        self.val_precisions.append(round(val_precision, 6))\n \n        print(f' — val_f1: {val_f1} — val_precision: {val_precision}, — val_recall: {val_recall}')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef plot_confusion_matrix(y_real, y_pred):\n    cm = confusion_matrix(y_real, y_pred)\n\n    ax= plt.subplot()\n    sns.heatmap(cm, annot=True, ax = ax, fmt='g')\n\n    ax.set_xlabel('Predicted labels')\n    ax.set_ylabel('True labels')\n    return\n    \n    \n# build neural network\n\nmodel = tf.keras.models.Sequential(\n    [\n      tf.keras.layers.Dense(128, activation='relu'),\n      tf.keras.layers.Dropout(0.2),\n\n      tf.keras.layers.Dense(1, activation='sigmoid')\n    ]\n)\n\nclass_weight = {0:1, 1:8}\n\n\n# roc = RocCallback(training_data=(X_train, y_train),\n#                   validation_data=(X_dev, y_dev))\n\nmodel.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(1e-3), metrics=[])\nmodel.fit(X_train, y_train, epochs=30, validation_data=(X_dev, y_dev), callbacks=[Metrics( validation=(X_dev, y_dev))], batch_size=256, class_weight=class_weight)\n\n\n\ny_dev_predict = np.greater(model.predict(X_dev), 0.5).astype(int)\nplot_confusion_matrix(y_dev, y_dev_predict)\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Evaluate model on test set')\nmodel.evaluate(X_test, y_test, verbose=1)\n\ny_test_pred = model.predict(X_test)\ny_test_pred = np.greater(y_test_pred, 0.5).astype(int)\n\nval_f1 = f1_score(y_test, y_test_pred)\nval_recall = recall_score(y_test, y_test_pred)         \nval_precision = precision_score(y_test, y_test_pred)\n\nprint('F1 %s recall %s precision %s' % (str(val_f1), str(val_recall), str(val_precision)))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submit \npredictions = model.predict(X_submit)\npredictions = np.greater(predictions, 0.5).astype(int)\n\npred_df = pd.DataFrame()\npred_df['prediction'] = predictions.ravel()\nids = submit[['id']]\nresult = pd.concat([ids, pred_df], axis=1)\nprint(result.groupby('prediction').count())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.to_csv('submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}