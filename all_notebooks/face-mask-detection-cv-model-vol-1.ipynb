{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Image binary classification on DL (MobileNetV2). Face mask detection. Vol.1"},{"metadata":{},"cell_type":"markdown","source":"### (Diploma project by student @Pawel_MTW \"Skillfactory.ru\")"},{"metadata":{},"cell_type":"markdown","source":"###  Перед нами поставлена задача создать модель, которая будет предсказывать наличие маски на лице человека. Это задача  классификации, нужно научить нейросеть делить картинки  на 2 категории: \"С маской на лице\" и \"Без маски\". Это задача классификации элементов заданного множества в две группы (предсказание, какой из групп принадлежит каждый элемент множества) на основе правила классификации. \n### Используем бинарную классификацию. \n### Для этого мы используем картинки (предварительно поделенные на 2 категории) для обучения (у нас их более 14 тыс.) и используем глубокое обучение,    Вариант 1: построим сверточную нейросеть CNN и Вариант 2: построим нейросеть на базе предобученной нейросети MobileNetV2 (используя transfer learning). Обучим, сравним показатели, выберем лучший результат."},{"metadata":{},"cell_type":"markdown","source":"## Data importing / Загружаем данные\n### Установка  импорт библиотек/модулей. "},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --upgrade pip\n!pip install cvlib\n!pip install opencv-python\n!pip install xmltodict\n!pip install mtcnn \n!pip install plot-metric\n#!pip install face_recognition\n!pip install --upgrade imutils","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\nimport json\nimport os\nimport argparse\nimport xmltodict\nimport plot_metric\nimport pylab as pl\nimport cvlib as cv\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport sys, cv2, time\nfrom PIL import Image\nimport tensorflow as tf\nfrom tensorflow import keras\nimport scikitplot as skplt\nfrom mtcnn.mtcnn import MTCNN\nfrom matplotlib import pyplot\nimport matplotlib.pyplot as plt\nimport sklearn.metrics as metrics\nfrom keras.utils import plot_model\nfrom scipy.spatial import distance\nfrom keras.models import Sequential\nfrom warnings import filterwarnings\nfrom keras.models import load_model\nfrom tensorflow.keras.layers import *\nfrom matplotlib.patches import Rectangle\nfrom keras.optimizers import Adam, RMSprop\nfrom sklearn.metrics import roc_auc_score\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom plot_metric.functions import BinaryClassification\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\nfrom keras.layers import Conv2D, MaxPool2D, Dropout, Dense, Flatten\nfrom keras.losses import binary_crossentropy, categorical_crossentropy\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import SpatialDropout2D, BatchNormalization, Input, Activation\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Основные настройки"},{"metadata":{"trusted":true},"cell_type":"code","source":"IMG_SIZE = 124 # какого размера подаем изображения в сеть\nIMG_CHANNELS = 3   # у RGB 3 канала\ninput_shape  = (IMG_SIZE, IMG_SIZE, IMG_CHANNELS)\n\nbatch_size = 32 #16 \nMIN_DISTANCE = 80 # расстояние между лицами на фото\n\nnum_classes = 2\nlabels=[\"No Mask\",\"Mask\"]\nmask_label = {0:\"No Mask\",1:\"Mask\"}\ncolor_label = {0:(0,0,255),1:(0,255,0),2:(255,0,0),3:(0,0,0),4:(255,255,255)}\ndist_label = {0:(0,255,0),1:(255,0,0)}\n\nDATA_PATH = '../input/'\nPATH = \"../working/\" \nface_model = cv2.CascadeClassifier('../input/haar-cascades-for-face-detection/haarcascade_frontalface_default.xml')\n\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\n\nfilterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    pass   # print(dirname)\n    #print(os.path.join(dirname, filename))\n    \n#print(os.listdir(DATA_PATH))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip freeze > requirements.txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!nvidia-smi","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Будем использовать данные из нескольких датасетов, чтобы данных было много.\n### Объединим картинки в группы по предназначению: Train, Test, Validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dirs = [\"/kaggle/input/withwithout-mask/maskdata/maskdata/train\",\n             \"/kaggle/input/withwithout-mask/masks2.0/masks/train\",\n             \"/kaggle/input/face-mask-12k-images-dataset/Face Mask Dataset/Train\",\n             \"/kaggle/input/faces-with-masks\"]\n\ntest_dirs = [\"/kaggle/input/withwithout-mask/maskdata/maskdata/test\",\n            \"/kaggle/input/withwithout-mask/masks2.0/masks/test\",\n            \"/kaggle/input/face-mask-12k-images-dataset/Face Mask Dataset/Test\",\n            \"/kaggle/input/face-mask-detection/dataset\"]\n\nvalidation_dirs = [\"/kaggle/input/face-mask-12k-images-dataset/Face Mask Dataset/Validation\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fullimg = []\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        fullimg.append(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing / Обработка данных"},{"metadata":{},"cell_type":"markdown","source":"### Используем ImageDataGenerator для аугментации данных:"},{"metadata":{},"cell_type":"markdown","source":"#### Аугментацию можно провести разными способами,c разными настройками, но помним, что у нас на картинках лица, соответственно есть ряд ограничений (например не переворачивать 180\", не искажать слишком сильно)."},{"metadata":{},"cell_type":"markdown","source":"#### Аугментация вариант №1"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_generator = ImageDataGenerator(rescale=1./255, #финальный \n                                          rotation_range=10, #15 \n                                          zoom_range=0.15,\n                                          width_shift_range=0.1,\n                                          height_shift_range=0.1,\n                                          shear_range=0.1, horizontal_flip=True,\n                                          fill_mode=\"nearest\")\n\n#train_data_generator = ImageDataGenerator(rescale=1./255, #рабочий\n                              #       zoom_range=0.2,\n                              #       shear_range=0.2,\n                              #       rotation_range=0.2)\n            \ntest_data_generator = ImageDataGenerator(rescale=1./255)\n\nvalidation_data_generator = ImageDataGenerator(rescale=1./255, #финальный \n                                          rotation_range=10, #15 \n                                          zoom_range=0.15,\n                                          width_shift_range=0.1,\n                                          height_shift_range=0.1,\n                                          shear_range=0.1, horizontal_flip=True,\n                                          fill_mode=\"nearest\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Аугментация вариант №2 через albumentations."},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install git+https://github.com/mjkvaak/ImageDataAugmentor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from ImageDataAugmentor.image_data_augmentor import *\n#import albumentations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Аугментация вариант №2 через albumentations. \n\"\"\"\nAUGMENTATIONS = albumentations.Compose([\n    albumentations.HorizontalFlip(p=0.5),\n    albumentations.Rotate(limit=10, interpolation=1, border_mode=4, value=None, mask_value=None, always_apply=False, p=0.5),\n    albumentations.OneOf([\n        albumentations.CenterCrop(height=250, width=200),\n        albumentations.CenterCrop(height=200, width=250),\n    ],p=0.5),\n    albumentations.OneOf([\n        albumentations.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3),\n        albumentations.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1)\n    ],p=0.5),\n    albumentations.GaussianBlur(p=0.05),\n    albumentations.HueSaturationValue(p=0.5),\n    albumentations.RGBShift(p=0.5),\n    albumentations.FancyPCA(alpha=0.1, always_apply=False, p=0.5),\n    albumentations.Resize(120, 120)\n    ])\n\ntrain_data_generator = ImageDataAugmentor(\n        rescale=1./255,\n        augment = AUGMENTATIONS,\n        #validation_split=VAL_SPLIT,\n        )\n\nvalidation_data_generator = ImageDataAugmentor(\n        rescale=1./255,\n        augment = AUGMENTATIONS,\n        #validation_split=VAL_SPLIT,\n        )\n\ntest_data_generator = ImageDataAugmentor(rescale=1./255)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Генерация данных. Завернем наши данные в генератор:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Data image Train\")\ntrain_generator1 = train_data_generator.flow_from_directory(\n        train_dirs[0],\n        target_size=(IMG_SIZE,IMG_SIZE),\n        batch_size=77,\n        interpolation=\"nearest\",\n        class_mode='binary',\n        classes=[\"without_mask\",\"with_mask\"])\ntrain_generator2 = train_data_generator.flow_from_directory(\n        train_dirs[1],\n        target_size=(IMG_SIZE,IMG_SIZE),\n        batch_size=46,\n        interpolation=\"nearest\",\n        class_mode='binary',\n        classes=[\"0\",\"1\"])\ntrain_generator3 = train_data_generator.flow_from_directory(\n        train_dirs[2],\n        target_size=(IMG_SIZE,IMG_SIZE),\n        batch_size=80,\n        interpolation=\"nearest\",\n        class_mode='binary',\n        classes=[\"WithoutMask\",\"WithMask\"])\ntrain_generator4 = train_data_generator.flow_from_directory(\n        train_dirs[3],\n        target_size=(IMG_SIZE,IMG_SIZE),\n        batch_size=163,\n        interpolation=\"nearest\",\n        class_mode='binary',\n        classes=[\"_\",\"faces_with_mask\"])\n\n\nprint(\"\\nData image Test\")\ntest_generator1 = test_data_generator.flow_from_directory(\n        test_dirs[0],\n        target_size=(IMG_SIZE,IMG_SIZE),\n        batch_size=66,\n        interpolation=\"nearest\",\n        class_mode='binary',\n        classes=[\"without_mask\",\"with_mask\"])\ntest_generator2 = test_data_generator.flow_from_directory(\n        test_dirs[1],\n        target_size=(IMG_SIZE,IMG_SIZE),\n        batch_size=11,\n        interpolation=\"nearest\",\n        class_mode='binary',\n        classes=[\"0\",\"1\"])\ntest_generator3 = test_data_generator.flow_from_directory(\n        test_dirs[2],\n        target_size=(IMG_SIZE,IMG_SIZE),\n        batch_size=62,\n        interpolation=\"nearest\",\n        class_mode='binary',\n        classes=[\"WithoutMask\",\"WithMask\"])\ntest_generator4 = test_data_generator.flow_from_directory(\n        test_dirs[3],\n        target_size=(IMG_SIZE,IMG_SIZE),\n        batch_size=55,\n        interpolation=\"nearest\",\n        class_mode='binary',\n        classes=[\"without_mask\",\"with_mask\"])\n\nprint(\"\\nData image Validation\")\nvalidation_generator1 = validation_data_generator.flow_from_directory(\n        validation_dirs[0],\n        target_size=(IMG_SIZE,IMG_SIZE),\n        batch_size=80,\n        interpolation=\"nearest\",\n        class_mode='binary',\n        classes=[\"WithoutMask\",\"WithMask\"])\nwithWithoutMask = {\"0\":\"Without Mask\",\"1\":\"With Mask\"}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def genToTuple(gen):\n    templist = []\n    templist2 = []\n    for i in range(gen.__len__()):\n        tempnext = gen.next()\n        templist.append(tempnext[0])\n        templist2.append(tempnext[1])\n    x=np.concatenate(templist)\n    y=np.concatenate(templist2)\n    return (x,y)\n\ndef combine_tuple(*tuples):\n    x=np.concatenate([tuples[i][0] for i in range(len(tuples))])\n    y=np.concatenate([tuples[i][1] for i in range(len(tuples))])\n    return (x,y.astype(int))   \n\ntrain_generator1_t = genToTuple(train_generator1)\ntrain_generator2_t = genToTuple(train_generator2)\ntrain_generator3_t = genToTuple(train_generator3)\ntrain_generator4_t = genToTuple(train_generator4)\n\ntest_generator1_t = genToTuple(test_generator1)\ntest_generator2_t = genToTuple(test_generator2)\ntest_generator3_t = genToTuple(test_generator3)\ntest_generator4_t = genToTuple(test_generator4)\n\nx_train,y_train = combine_tuple(train_generator1_t,train_generator2_t,train_generator3_t,train_generator4_t)\n\nx_test,y_test = combine_tuple(test_generator1_t,test_generator2_t,test_generator3_t,test_generator4_t)\n\nx_val,y_val = genToTuple(validation_generator1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_train.shape,y_train.shape)\nprint(x_test.shape,y_test.shape)\nprint(x_val.shape,y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Посмотрим на равномерность распределения картинок по категориям.\n#### Распределение в целом равномерное, небольшой перевес  в обучающей выборке в категории \"With mask\", но поскольку мы будем использовать предобученную сеть MobileNet V2, то это не повлияет на конечный результат."},{"metadata":{"trusted":true},"cell_type":"code","source":"# смотрим распределение картинок по категориям в TRAIN:\nsns.countplot(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# смотрим кол-во картинок по категориям в TRAIN:\ncategory = pd.DataFrame(y_train)  \ncategory.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# смотрим кол-во картинок по категориям в TRAIN в %:\n#category = pd.DataFrame(y_train)  \ncategory.value_counts(normalize=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# смотрим распределение картинок по категориям в VALIDATION:\nsns.countplot(y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# смотрим распределение картинок по категориям в TEST:\nsns.countplot(y_test) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Вглянем на образцы картинок лиц на которых мы будем обучать модель:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(100,100))\ntempc = np.random.choice(x_train.shape[0],30,replace=False)\nd = 0\nfor i in tempc:\n    plt.subplot(7, 5, d+1)\n    d += 1\n    tempc = np.random.randint(x_train.shape[0])\n    plt.imshow(x_train[tempc])\n    plt.axis(\"off\")\nplt.subplots_adjust(wspace=-0.1, hspace=0.3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build Model / Построение модели."},{"metadata":{},"cell_type":"markdown","source":"### Вариант #1. Строим сверточную нейронную сеть CNN для классификации лиц по фото по категориям: Результат accuracy: 0.9416, roc_auc: 0.9918"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Вариант 1 построения сети: Результат accuracy_test: 0.9524, roc_auc: 0.9918\n\ndef build_model():\n        model = Sequential()\n        \n        model.add(Input(shape=input_shape))\n\n        model.add(Conv2D(filters=16,kernel_size=(2,2),padding=\"same\"))\n        model.add(Activation(\"relu\"))\n        model.add(SpatialDropout2D(0.25)) #0.45\n        \n        model.add(MaxPool2D(pool_size=(4,4)))\n\n        model.add(Conv2D(filters=32,kernel_size=(2,2),padding=\"same\"))\n        model.add(Activation(\"relu\"))\n        model.add(SpatialDropout2D(0.25)) # 0.45\n        \n        model.add(MaxPool2D(pool_size=(4,4),strides=(4,4)))\n        model.add(Dropout(0.2))        \n        model.add(Flatten())\n        \n        model.add(Dense(256)) # 1024\n        model.add(Activation(\"relu\"))\n        model.add(Dropout(0.25)) #0.45\n        \n        model.add(Dense(128)) # 256 1024\n        model.add(Activation(\"relu\"))\n        model.add(Dropout(0.2)) #0.4\n        \n        \n        model.add(Dense(1))\n        model.add(Activation(\"sigmoid\"))\n        \n        optimizer = Adam ()#(lr=0.001)\n        model.compile(optimizer = optimizer ,metrics=[\"accuracy\"], loss = binary_crossentropy) \n        \n        return model\n    \nmodel1 = build_model() \nmodel1.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Вариант #2 TransferLearning. Построение сети на базе предообученной сети MobileNet V2 (alpha=1.4)\n\nНа момент выполнения проекта нейросеть сеть MobileNet V2 (имеется несколько вариаций, мы выбрали Large) все ещё является оптимальным выбором, если брать во внимание размер сети, точность предсказаний и время работы, хотя уже появилась MobileNet V3. \nИтоговый результат обученной модели аccuracy: 0.9861%"},{"metadata":{},"cell_type":"markdown","source":"### Настраиваем обучение на всех слоях MobileNetV2 (сеть обучается быстро, и так будет точнее результат)."},{"metadata":{"trusted":true},"cell_type":"code","source":"mobilenet = MobileNetV2(alpha=1.4,weights='imagenet',include_top=False,input_shape=(input_shape))\nfor layer in mobilenet.layers:\n    layer.trainable = True #False\nmodel = Sequential()\nmodel.add(mobilenet)\nmodel.add(GlobalMaxPooling2D(),) # GlobalMaxPooling2D или GlobalAveragePooling2D не надо: Flatten())\nmodel.add(Dense(256)) #512\nmodel.add(Activation(\"relu\"))\nmodel.add(Dropout(0.45)) #0.25                   \nmodel.add(Dense(1))\nmodel.add(Activation(\"sigmoid\")) \nmodel.compile(optimizer=\"adam\",loss=binary_crossentropy, metrics =[\"accuracy\"]) #binary_accuracy accuracy\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# можно также вывести итоговую схему нейросети в виде красивой картинки: \nplot_model(model,show_shapes=True,show_layer_names=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Model / Обучение модели."},{"metadata":{},"cell_type":"markdown","source":"### Делаем последние настройки перед обучением модели,  используем ReduceLROnPlateau для гибкого использования LR,а также EarlyStopping для экономии времени и ресурсов обучения."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#  Используем ReduceLROnPlateau\n# ReduceLROnPlateau - уменьшаемый LR: когда результат уже не меняется, LR  уменьшается, и так до min предела указанного:\nreducer = ReduceLROnPlateau(monitor='loss',patience=3,factor=0.75,min_lr=0.000000001,verbose=1) #monitor='val_acc' loss\ncheckpoint = ModelCheckpoint(\"facemask.h5\",monitor=\"val_accuracy\",save_best_only=True,verbose=1) #\nstopSign = EarlyStopping(monitor = \"loss\",patience=10,min_delta=0.000000000001,mode=\"min\")\nepochs = 100 #120\nbatch_size = 32 \nsteps_per_epoch = x_train.shape[0] // batch_size\nhistory = model.fit(x_train, y_train,\n                    epochs = epochs, \n                    validation_data = (x_val,y_val),\n                    verbose = 1,\n                    batch_size=batch_size,\n                    steps_per_epoch = steps_per_epoch,\n                    callbacks=[stopSign,checkpoint\n                               ,reducer\n                              ])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training and Validation Visualizations / визуализация процесса обучения"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n \nepochs = range(len(acc))\n \nplt.plot(epochs, acc, 'b', label='Training acc')\nplt.plot(epochs, val_acc, 'r', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\n \nplt.figure()\n \nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.xlabel('epochs')\nplt.ylabel('loss')\n \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  Prediction / Предсказание"},{"metadata":{},"cell_type":"markdown","source":"### Делаем предсказания на тестовых данных. Выводим матрицу ошибок.\n\n### Результаты получились отличные: крайне небольшое кол-во ложно положительных и малое кол-во ложно отрицательных предсказаний."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_classes = model.predict_classes(x_test)\ny_pred = model.predict(x_test)\n\nplt.subplots(figsize=(8,7))\nsns.heatmap(confusion_matrix(y_test,y_pred_classes),xticklabels=labels,\n                                       yticklabels=labels, annot=True,fmt=\"1.0f\",cbar=False,annot_kws={\"size\": 20})\nplt.title(\"Confusion matrix\",fontsize=30)\nplt.xlabel(f\"CNN model Accuracy: {accuracy_score(y_test,y_pred_classes)}\",fontsize=30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Printing Classification report/ выводим Classification отчет:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred_classes, target_names = labels)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Вывод: precision и recall показатели очень высокие (почти избегаем ошибок I и II рода)."},{"metadata":{},"cell_type":"markdown","source":"В нашей проблематике хотелось бы в первую очередь снизить FN (ошибку II рода) - это когда \"без маски\", но модель говорит что \"в маске\"."},{"metadata":{},"cell_type":"markdown","source":"### Отчет по метрикам accuracy  и loss:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# считаем Score модели:\ntest_loss, test_acc = model.evaluate(x_test, y_test, steps=32)\nprint('The final test accuracy: ',test_acc)\nprint('The final test loss: ',test_loss)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cчитаем Auc-roс, строим кривую."},{"metadata":{},"cell_type":"markdown","source":"### Поскольку мы используем бинарную классификацию, то берем метрики Аuc_roc, Чувствительность и Специфичность для оценки работы сети (tpr - sensivity, fpr - specifity)."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_proba = model.predict_proba(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate the fpr and tpr for all thresholds of the classification\nfpr, tpr, threshold = metrics.roc_curve(y_test, y_pred_proba)\nroc_auc = metrics.auc(fpr, tpr)\nroc_auc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tpr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Результаты метрики Roc_auc (best = 0.9954) получились отличными. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_roc_cur(fper, tper):  \n    plt.plot(fper, tper, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_cur(fpr, tpr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Roc_curve кривую, матрицу ошибок и отчет по метрикам можно вывести вот такой единой красочной таблицей:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Визуализируем plot_metric\nbc = BinaryClassification(y_test,y_pred_proba, [\"No mask\",\"Mask\"]) \n# Figures\nplt.figure(figsize=(15,10))\nplt.subplot2grid(shape=(2,6), loc=(0,0), colspan=2)\nbc.plot_roc_curve()\nplt.subplot2grid((2,6), (0,2), colspan=2)\nbc.plot_precision_recall_curve()\n#plt.subplot2grid((2,6), (0,4), colspan=2)\n#bc.plot_class_distribution()\nplt.subplot2grid((2,6), (1,1), colspan=2)\nbc.plot_confusion_matrix()\nplt.subplot2grid((2,6), (1,3), colspan=2)\nbc.plot_confusion_matrix(normalize=True)\nplt.show()\n#bc.print_report()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Сохраним модель в файл:"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('model_mask.hdf5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Выведем примеры предсказаний применительно к картинкам  test датасета: \n### 1.Выборка: правильно предсказанные случаи"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(100,100))\ntempc = np.random.choice(x_test[y_test == y_pred_classes.ravel()].shape[0],20,replace=False)\nd = 0\nfor i in tempc:\n    plt.subplot(7, 5, d+1)\n    d += 1\n    tempc = np.random.randint(x_test[y_test == y_pred_classes.ravel()].shape[0])\n    plt.imshow(x_test[y_test == y_pred_classes.ravel()][tempc])\n    plt.title(f\"True:{withWithoutMask[str(y_test[y_test == y_pred_classes.ravel()][tempc])]}\\nPredicted:{withWithoutMask[str(y_pred_classes.ravel()[y_test == y_pred_classes.ravel()][tempc])]}\",\n              fontsize=60,color='g')\n    plt.axis(\"off\")\nplt.subplots_adjust(wspace=-0.1, hspace=0.3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.Выборка: неправильно предсказаные случаи."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(100,100))\ntempc = np.random.choice(x_test[y_test != y_pred_classes.ravel()].shape[0],20,replace=False)\nd = 0\nfor i in tempc:\n    plt.subplot(7, 5, d+1)\n    d += 1\n    tempc = np.random.randint(x_test[y_test != y_pred_classes.ravel()].shape[0])\n    plt.imshow(x_test[y_test != y_pred_classes.ravel()][tempc])\n    plt.title(f\"True:{withWithoutMask [str(y_test[y_test != y_pred_classes.ravel()][tempc])]}\\nPredicted:{withWithoutMask[str(y_pred_classes.ravel()[y_test != y_pred_classes.ravel()][tempc])]}\", fontsize=60, color='r')\n    plt.axis(\"off\")\nplt.subplots_adjust(wspace=-0.1, hspace=0.3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Итоги: \n### В данном ноутбуке мы сделали следующее: мы разработали модели согласно поставленной задаче, а именно, сверточную модель на основе нейронных сетей,и модель на базе предобученной нейросети MobilNetV2 large (которая показала лучший результат). Наша модель предсказывает отношения лица к одному из 2 классов (\"С Маской\"/\"Без маски\"). Обучили модель. Также мы проанализировали показатели модели. Сохранили модель для дальнейшего использования, сделали сериализацию. Учитывая все сказанное (исходя из полученных показателей), модель должна работать очень хорошо.\n\n"},{"metadata":{},"cell_type":"markdown","source":"### В Vol.2 ноутбуке мы займемся практическим использованием нашей обученной модели для фото, в Vol.3 - мы реализуем максимальные возможности нашей модели на видеопотоке, создадим прототип дальнейшего использования в виде системы контроля Video real-time."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}