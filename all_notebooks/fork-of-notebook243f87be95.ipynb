{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n#pip install smogn\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.style.use('ggplot')\n\n\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.feature_selection import VarianceThreshold, SelectKBest, f_regression, RFECV\nfrom mlxtend.feature_selection import SequentialFeatureSelector as sfs\nfrom sklearn.model_selection import train_test_split,StratifiedShuffleSplit, KFold\n\nfrom sklearn.linear_model import LinearRegression, RidgeCV,LassoCV,ElasticNetCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nimport statsmodels.api as sm\nfrom statsmodels.compat import lzip\nimport statsmodels.stats.api as sms\nfrom statsmodels.formula.api import ols\nfrom scipy.stats import zscore\nfrom statsmodels.stats.stattools import durbin_watson\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndf =  pd.read_csv(os.path.join(dirname, filenames[0]))\ndf.columns\n# data = np.loadtxt(\"/kaggle/input/forest-fires-data-set/forestfires.csv\", delimiter=',')\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info() \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" # Generate descriptive statistics that summarize the central tendency,dispersion and shape of a dataset's distribution\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Handling Missing Values"},{"metadata":{},"cell_type":"markdown","source":"Now we randomly simulate the presence of missing values by making a good chunk of our data as missing"},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy.random import default_rng\nrng = default_rng()\nprint(f\"Before missing values injection {df.isnull().sum()}\")\ncols_with_missing_data = ['wind', 'RH', 'DMC', 'ISI', 'rain']\nmissing_indices_rain = rng.choice(len(df['rain'])-1, size=int(0.6 * len(df['rain'])), replace=False)\nmissing_indices = list(map(lambda col: rng.choice(len(df[col])-1, size=int(0.15 * len(df[col])), replace=False), cols_with_missing_data[0:-1]))\nmissing_indices.append(missing_indices_rain)\n\nfor col in range(len(cols_with_missing_data)):\n    for missingIndex in missing_indices[col]:\n        df[cols_with_missing_data[col]][missingIndex] = np.nan\n\nprint(f\"After missing values injection {df.isnull().sum()}\")\n\ndf[1:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Imputing missing values using the most performant way"},{"metadata":{},"cell_type":"markdown","source":"Using SimpleImputer with all strategies"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef SimpleImputerImpute(X, y):\n    results = []\n    strategies = ['mean','median','most_frequent','constant']\n    for s in strategies:\n\n        split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\n        for train_index, test_index in split.split(df.values, df.temp_bins.values):\n            st_train_set = df.iloc[train_index]\n            st_test_set = df.iloc[test_index]\n\n        imputer = SimpleImputer(strategy=s)\n        X_train = st_train_set.drop(columns=['area', 'temp_bins', 'damage_category'])\n        imputer.fit(X_train)\n        X_train = imputer.transform(X_train)\n        X_train = pd.get_dummies(X_train)\n        y_train = st_train_set[['damage_category']]\n\n        X_test = st_test_set.drop(columns=['area', 'temp_bins', 'damage_category'])\n        imputer.fit(X_test)\n        X_test = imputer.transform(X_test)\n        X_test = pd.get_dummies(X_test)\n        y_test = st_test_set[['damage_category']]\n\n        model = RandomForestRegressor(max_depth=2, min_samples_split=2, min_samples_leaf=1, random_state=0, n_estimators=100)\n\n\n        model.fit(X_train, y_train.values.ravel())\n\n        y_pred = model.predict(X_test)\n        # print out the prediction scores\n        print(\"------------------------------------------------------------\")\n        print('RMSE: {}'.format(np.sqrt(mean_squared_error(y_test, y_pred))))\n        print('MAE: {}'.format(mean_absolute_error(y_test, y_pred)))\n        print('R-squared: {}'.format(r2_score(y_test, y_pred)))\n        print(\"------------------------------------------------------------\")\n\ndfa = pd.get_dummies(df)\ndata = dfa.values\ncols = dfa.columns\nix = [i for i in range(data.shape[1]) if  i != 10]\n\nX_train = data[:, ix] \ny_train = data[:, 10]\n\n\nfrom sklearn.utils.multiclass import type_of_target\ntype_of_target(df['area'].values)\n\nlist(SimpleImputerImpute(X_train, y_train))\n\nSimpleImputerImpute(X_train, )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using KNN imputer with and choosing the best features numbers/combinations"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef KNNImpute(X, y):\n    results = []\n    strategies = [1,3,5,7,8,9,15,18,21,25]\n    for s in strategies:\n        pipeline = Pipeline(steps=[('i', KNNImputer(n_neighbors=int(s))), ('m', RandomForestClassifier())] )\n\n\n        #evaluate model \n        cv = RepeatedStratifiedKFold(n_splits = 10,  n_repeats=3, random_state= 1)\n        scores = cross_val_score(pipeline, X , y ,scoring='accuracy', cv=cv, n_jobs=-1)\n\n        #store results \n        results.append(np.mean(scores))\n        \n        print('>%s %.3f (%.3f)'% (s, np.mean(scores), np.std(scores)))\n        if(s == 25):\n            strat = strategies[index(max(results))]\n            imputer = KNNImputer(n_neighbors=int(strat))\n            X = imputer.fit_transform(X)\n            df = pd.DataFrame(data= X)\n            \n        \nKNNImpute(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using Iterative imputation"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef iterativeImputation(X, y):\n\n    #evaluate strategy on our dataset\n    results = []\n\n    strategies = ['ascending', 'descending', 'roman', 'arabic', 'random']\n    for s in strategies:\n        pipeline = Pipeline(steps=[('i', IterativeImputer(imputation_order=s)),('m',RandomForestClassifier())])\n\n\n        #evaluate model \n        cv = RepeatedStratifiedKFold(n_splits = 10,  n_repeats=3, random_state= 1)\n        scores = cross_val_score(pipeline, X , y ,scoring='accuracy', cv=cv, n_jobs=-1)\n\n        #store results \n        results.append(scores)\n        print('>%s %.3f (%.3f)'% (s, np.mean(scores), np.std(scores)))\n        \niterativeImputation(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing columns with extremely low variance"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = dfHE.drop(columns=['area'])\ny = dfHE['area']\n\nresults = []\n# thresholds = np.arange(0.0, 0.5, 0.05)\n\n# for t in thresholds:\n#     transform = VarianceThreshold(threshold=t)\n#     print(X.shape)\n#     X_sel = transform.fit_transform(X)\n#     print(\"Threshold {} n_features {}\".format(t, X_sel.shape[1]))\n#     print(X_sel)\n#     results.append(X_sel.shape[1])\n    \ntransform = VarianceThreshold(threshold=0.125)\ntransform.fit(X)\n# print(\"Threshold {} n_features {}\".format(t, X_sel.shape[1]))\nX_sel = X[X.columns[transform.get_support(indices=True)]]\nX_sel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = data[:, 24]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Libraries and configurations for figure plotting\nplt.style.use('seaborn')\ndf.hist(bins=30, figsize=(20,15)) # plotting the histogram\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking for the occurences of fires in certain locations \ndf.plot(kind='scatter', x='X', y='Y', alpha=0.1, s=300) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting the graphs by increasing the size to see the affect of area over the datapoints\ndf.plot(kind='scatter', x='X', y='Y', alpha=0.2, s=20*df['area']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the histogram for the RH attribute\nattributes = ['RH']\nscatter_matrix(df[attributes], figsize=(15,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the histogram for the temp attribute\nattributes = ['temp']\nscatter_matrix(df[attributes], figsize=(15,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the histogram for the DMC attribute\n\nattributes = ['DMC']\nscatter_matrix(df[attributes], figsize=(15,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# analyzing categorical columns\n# graphing the distribution of the fires area\nplt.figure(figsize=(16,5))\nprint(\"Skew: {}\".format(df['area'].skew()))\nprint(\"Kurtosis: {}\".format(df['area'].kurtosis()))\nax = sns.kdeplot(df['area'],shade=True,color='g')\nplt.xticks([i for i in range(0,1200,50)])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"dfa = df.drop(columns='area')\ncat_columns = dfa.select_dtypes(include='object').columns.tolist()\nnum_columns = dfa.select_dtypes(exclude='object').columns.tolist()\n\n# analyzing categorical columns\nplt.figure(figsize=(16,10))\nfor i,col in enumerate(cat_columns,1):\n    plt.subplot(2,2,i)\n    sns.countplot(data = dfa,y = col)\n    plt.subplot(2,2,i+2)\n    df[col].value_counts(normalize=True).plot.bar()\n    plt.ylabel(col)\n    plt.xlabel('% distribution per category')\nplt.tight_layout()\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation analysis of the dataset\ndf.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(columns=['X', 'Y'])\n\nmonths_to_remove = ['nov','jan','may']\ndf = df.drop(df[df.month.isin(months_to_remove)].index ,axis=0)\n\ndf['temp_bins'] = pd.cut(df.temp, bins=[0, 15, 20, 25, 40], include_lowest=True, \n                                 labels=['0-15', '15-20', '20-25', '>25'])\n\n# split into train/test\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\nfor train_index, test_index in split.split(df.values, df.temp_bins.values):\n    st_train_set = df.iloc[train_index]\n    st_test_set = df.iloc[test_index]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = st_train_set.drop(columns=['area', 'temp_bins'])\nX_train = pd.get_dummies(X_train)\ny_train = st_train_set[['area']]\n\nX_test = st_test_set.drop(columns=['area', 'temp_bins'])\nX_test = pd.get_dummies(X_test)\ny_test = st_test_set[['area']]\n\nmodel = RandomForestRegressor(max_depth=2, min_samples_split=2, min_samples_leaf=1, random_state=0, n_estimators=100)\nmodel.fit(X_train, y_train.values.ravel())\n\ny_pred = model.predict(X_test)\n\n# print out the prediction scores\nprint('RMSE: {}'.format(np.sqrt(mean_squared_error(y_test, y_pred))))\nprint('MAE: {}'.format(mean_absolute_error(y_test, y_pred)))\nprint('R-squared: {}'.format(r2_score(y_test, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nWe see that the model performs bad. Ideally the dots should be near the line.\nThat's because of the numerous 0 values of our output variable, our model is trained to predict only low area values. We need to resample the dataset in the way our model will be able to learn to predict high values of the target variable because they are of interest."},{"metadata":{},"cell_type":"markdown","source":"**\nAddressing Extreme Rare Cases with SmoteR and SMOGN for Regression**"},{"metadata":{},"cell_type":"markdown","source":"# Implement SmoteR"},{"metadata":{},"cell_type":"markdown","source":"Let's start with SmoteR: a simple algorithm to generate more examples of extreme rare cases, we want to learn to predict. \nWe will implement it referencing the paper:https://www.researchgate.net/publication/220699419_Utility-Based_Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define sigmoid function\n# https://en.wikipedia.org/wiki/Sigmoid_function\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# plot sigmoid function\nplt.plot(np.linspace(-10, 10, 100), sigmoid(np.linspace(-10, 10, 100)))\nplt.title('Sigmoid Function')\nplt.xlabel('x')\nplt.axhline(y=0, c='black', linestyle=':')\nplt.axvline(x=0, c='black', linestyle=':')\nplt.ylabel('sigmoid(x)')\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# implement relevance function\ndef relevance(x):\n    x = np.array(x)\n    return sigmoid(x - 50)\n\n# plot relevance function\nplt.plot(np.linspace(30, 70, 1000), relevance(np.linspace(30, 70, 1000)))\nplt.title('Relevance Function')\nplt.xlabel('x')\nplt.axhline(y=0, c='gray', linestyle='--')\nplt.axvline(x=50, c='gray', linestyle='--')\nplt.ylabel('relevance(x)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we implement SmoteR:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# implement SMOTER\n# see paper: https://core.ac.uk/download/pdf/29202178.pdf\n\ndef get_synth_cases(D, target, o=200, k=3, categorical_col = []):\n    '''\n    Function to generate the new cases.\n    INPUT:\n        D - pd.DataFrame with the initial data\n        target - string name of the target column in the dataset\n        o - oversampling rate\n        k - number of nearest neighbors to use for the generation\n        categorical_col - list of categorical column names\n    OUTPUT:\n        new_cases - pd.DataFrame containing new generated cases\n    '''\n    new_cases = pd.DataFrame(columns = D.columns) # initialize the list of new cases \n    ng = o // 100 # the number of new cases to generate\n    for index, case in D.iterrows():\n        # find k nearest neighbors of the case\n        knn = KNeighborsRegressor(n_neighbors = k+1) # k+1 because the case is the nearest neighbor to itself\n        knn.fit(D.drop(columns = [target]).values, D[[target]])\n        neighbors = knn.kneighbors(case.drop(labels = [target]).values.reshape(1, -1), return_distance=False).reshape(-1)\n        neighbors = np.delete(neighbors, np.where(neighbors == index))\n        for i in range(0, ng):\n            # randomly choose one of the neighbors\n            x = D.iloc[neighbors[np.random.randint(k)]]\n            attr = {}          \n            for a in D.columns:\n                # skip target column\n                if a == target:\n                    continue;\n                if a in categorical_col:\n                    # if categorical then choose randomly one of values\n                    if np.random.randint(2) == 0:\n                        attr[a] = case[a]\n                    else:\n                        attr[a] = x[a]\n                else:\n                    # if continious column\n                    diff = case[a] - x[a]\n                    attr[a] = case[a] + np.random.randint(2) * diff\n            # decide the target column\n            new = np.array(list(attr.values()))\n            d1 = cosine_similarity(new.reshape(1, -1), case.drop(labels = [target]).values.reshape(1, -1))[0][0]\n            d2 = cosine_similarity(new.reshape(1, -1), x.drop(labels = [target]).values.reshape(1, -1))[0][0]\n            attr[target] = (d2 * case[target] + d1 * x[target]) / (d1 + d2)\n            \n            # append the result\n            new_cases = new_cases.append(attr,ignore_index = True)\n                    \n    return new_cases\n\ndef SmoteR(D, target, th = 0.999, o = 200, u = 100, k = 3, categorical_col = []):\n    '''\n    The implementation of SmoteR algorithm:\n    https://core.ac.uk/download/pdf/29202178.pdf\n    INPUT:\n        D - pd.DataFrame - the initial dataset\n        target - the name of the target column in the dataset\n        th - relevance threshold\n        o - oversampling rate\n        u - undersampling rate\n        k - the number of nearest neighbors\n    OUTPUT:\n        new_D - the resulting new dataset\n    '''\n    # median of the target variable\n    y_bar = D[target].median()\n    \n    # find rare cases where target less than median\n    rareL = D[(relevance(D[target]) > th) & (D[target] > y_bar)]  \n    # generate rare cases for rareL\n    new_casesL = get_synth_cases(rareL, target, o, k , categorical_col)\n    \n    # find rare cases where target greater than median\n    rareH = D[(relevance(D[target]) > th) & (D[target] < y_bar)]\n    # generate rare cases for rareH\n    new_casesH = get_synth_cases(rareH, target, o, k , categorical_col)\n    \n    new_cases = pd.concat([new_casesL, new_casesH], axis=0)\n    \n    # undersample norm cases\n    norm_cases = D[relevance(D[target]) <= th]\n    # get the number of norm cases\n    nr_norm = int(len(norm_cases) * u / 100)\n    \n    norm_cases = norm_cases.sample(min(len(D[relevance(D[target]) <= th]), nr_norm))\n    \n    # get the resulting dataset\n    new_D = pd.concat([new_cases, norm_cases], axis=0)\n    \n    return new_D","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Add SmoteR"},{"metadata":{"trusted":true},"cell_type":"code","source":"# construct the initial dataset for SmoteR\ncols = X_train.columns.tolist()\ncols.append('area')\nD = pd.DataFrame(np.concatenate([X_train, y_train], axis=1), columns = cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply SmoteR to get the new dataset\nnp.random.seed(43)\nXs = SmoteR(D, target='area', th = 0.999, o = 300, u = 100, k = 10, categorical_col = ['month_apr',\n       'month_aug', 'month_dec', 'month_feb', 'month_jan', 'month_jul',\n       'month_jun', 'month_mar', 'month_may', 'month_nov', 'month_oct',\n       'month_sep', 'day_fri', 'day_mon', 'day_sat', 'day_sun', 'day_thu',\n       'day_tue', 'day_wed'])\n\nX_train = Xs.drop(columns=['area'])\ny_train = Xs[['area']]\n\n\n# model and check the results\nmodel = RandomForestRegressor(max_depth=2, min_samples_split=5, min_samples_leaf=5, random_state=0, n_estimators=100)\n\nmodel.fit(X_train, y_train.values.ravel())\n\ny_pred = model.predict(X_test)\n\n# print out the prediction scores\nprint('RMSE: {}'.format(np.sqrt(mean_squared_error(y_test, y_pred))))\nprint('MAE: {}'.format(mean_absolute_error(y_test, y_pred)))\nprint('R-squared: {}'.format(r2_score(y_test, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(D.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the results\nplt.scatter(y_pred, y_test)\nplt.plot(np.linspace(0,400,400), np.linspace(0,400,400), c = 'orange', linestyle='--')\nplt.xlabel('prediction')\nplt.ylabel('true values')\nplt.xlim(0,400)\nplt.ylim(0,400)\nplt.title('Predicted vs True values')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our metrics haven't improved much. But now we are actually trying to predict something rather than zero.\n"},{"metadata":{},"cell_type":"markdown","source":"Implement SMOGN"},{"metadata":{},"cell_type":"markdown","source":"# Add SMOGN"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport smogn\n\nX_s = smogn.smoter(\n    \n    ## primary arguments / inputs\n    D,                     ## training set  (pandas dataframe)\n    'area',                        ## response variable y by name  (string)\n    k = 5,                    ## num of neighs for over-sampling  (pos int)\n    samp_method = \"extreme\",  ## % over / under sample  (\"balance\" or extreme\")\n    drop_na_col = True,       ## auto drop columns with nan's  (bool)\n    drop_na_row = True,       ## auto drop rows with nan's  (bool)\n    replace = False,          ## sampling replacement  (bool)\n    \n    ## phi relevance function arguments / inputs\n    rel_thres = 0.80,          ## relevance threshold considered rare  (pos real)\n    rel_method = \"auto\",      ## relevance method  (\"auto\" or \"manual\")\n    rel_xtrm_type = \"high\",   ## distribution focus  (\"high\", \"low\", \"both\")\n    rel_coef = 2.25,           ## coefficient for box plot  (pos real)\n    rel_ctrl_pts_rg = None    ## input for \"manual\" rel method  (2d array)\n    \n    )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = X_s['area'] ==0\nprint(len(res))\nprint(res.sum())\nprint(res.sum()/len(res))\n#Percentage of zero areas has drastically decreased due to resampling of minority into the majority\nplt.plot(X_s['area'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_s.drop(columns=['area'])\ny_train = X_s[['area']]\n\n\n# model and check the results\nmodel = RandomForestRegressor(max_depth=2, min_samples_split=2, min_samples_leaf=1, random_state=0, n_estimators=100)\n\nmodel.fit(X_train, y_train.values.ravel())\n\ny_pred = model.predict(X_test)\n\n# print out the prediction scores\nprint('RMSE: {}'.format(np.sqrt(mean_squared_error(y_test, y_pred))))\nprint('MAE: {}'.format(mean_absolute_error(y_test, y_pred)))\nprint('R-squared: {}'.format(r2_score(y_test, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the results\nplt.scatter(y_pred, y_test)\nplt.plot(np.linspace(0,400,400), np.linspace(0,400,400), c = 'orange', linestyle='--')\nplt.xlabel('prediction')\nplt.ylabel('true values')\nplt.xlim(0,400)\nplt.ylim(0,400)\nplt.title('Predicted vs True values')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now predict non-zero forest fire areas,and we improved our R^2 score which shows the effectiveness of using oversampling for rare cases for our dataset and undersampling the 0 hectares burn area. "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_s.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Handling Missing Values"},{"metadata":{},"cell_type":"markdown","source":"Now we randomly designate some instances as missing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from numpy.random import default_rng\n# rng = default_rng()\n# cols_with_missing_data = ['wind', 'RH', 'DMC', 'ISI', 'rain']\nmissing_indices_rain = rng.choice(len(X_s['rain'])-1, size=int(0.6 * len(X_s['rain'])), replace=False)\nmissing_indices = list(map(lambda col: rng.choice(len(X_s[col])-1, size=int(0.15 * len(X_s[col])), replace=False), cols_with_missing_data[0:-1]))\n# missing_indices.append(missing_indices_rain)\n\n# for col in range(len(cols_with_missing_data)):\n#     for missingIndex in missing_indices[col]:\n#         X_s[cols_with_missing_data[col]][missingIndex] = np.nan\n\n# X_s.isnull().sum()\nX_s\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(df['area'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using SimpleImputer with all strategies."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef SimpleImputerImpute(X, y):\n    results = []\n    strategies = ['mean','median','most_frequent','constant']\n    for s in strategies:\n        pipeline = Pipeline(steps=[('i', SimpleImputer(strategy=s)),('m',RandomForestClassifier())])\n\n\n        #evaluate model \n        cv = RepeatedStratifiedKFold(n_splits = 10,  n_repeats=3, random_state= 1)\n        scores = cross_val_score(pipeline, X , y ,scoring='accuracy', cv=cv, n_jobs=-1)\n\n        #store results \n        results.append(scores)\n        print('>%s %.3f (%.3f)'% (s, mean(scores), std(scores)))\n        return results\n\ndata = X_s.values\ncols = X_s.columns\nix = [i for i in range(data.shape[1]) if  i != 24]\n\nX_train = data[:, ix]\ny_train = data[:, 24]\n\n\n# from sklearn.utils.multiclass import type_of_target\n# type_of_target(df['area'].values)\nSimpleImputerImpute(X_train, y_train)\n\n# SimpleImputerImpute(X_train, )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_m, y_m = df_ml.drop(columns=['area']), df_ml['area']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# RFECV is a variant with inbuilt Cross validation\nmodel = LinearRegression()\nselector = RFECV(model,cv=5)\nselector = selector.fit(X_m, y_m)\nprint(f\"Out of {len(X_m.columns)} features, best number of features {selector.n_features_}\")\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score\")\nplt.plot(range(1, len(X_m.columns) + 1), selector.grid_scores_)\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfHE = pd.get_dummies(df)\n\nX = dfHE.drop(columns=['area'])\ny = dfHE['area']\nX= X.values\ny= y.values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n\n\nfs = SelectKBest(score_func=f_regression, k='all')\n# learn relationship from training data\nfs.fit(X_train, y_train)\n# transform train input data\nX_train_fs = fs.transform(X_train)\n# transform test input data\nX_test_fs = fs.transform(X_test)\n# print(X_train_fs, X_test_fs, fs, sep=\"\\n\")\nfor i in range(len(fs.scores_)):\n    print('Feature %d: %f' % (i, fs.scores_[i]))\n    \n# plot the scores\nplt.bar([i for i in range(len(fs.scores_))], fs.scores_)\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfHE.columns[26]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"X = dfHE.drop(columns=['area'])\ny = dfHE['area']\n\nresults = []\nthresholds = np.arange(0.0, 0.5, 0.05)\n\n# for t in thresholds:\n#     transform = VarianceThreshold(threshold=t)\n#     print(X.shape)\n#     X_sel = transform.fit_transform(X)\n#     print(\"Threshold {} n_features {}\".format(t, X_sel.shape[1]))\n#     print(X_sel)\n#     results.append(X_sel.shape[1])\n    \ntransform = VarianceThreshold(threshold=0.15)\ntransform.fit(X)\n# print(\"Threshold {} n_features {}\".format(t, X_sel.shape[1]))\nX_sel = X[X.columns[transform.get_support(indices=True)]]\nX_sel\n\n# results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Examining Clustering patterns using KMEANs  "},{"metadata":{"trusted":true},"cell_type":"code","source":"\nkmeans = KMeans(init=\"random\",n_clusters=3,n_init=10,max_iter=300,random_state=42)\n\nm = kmeans.fit(X_train.values)\n#print(m)\n# The lowest SSE value\nlowest = kmeans.inertia_\n#print(lowest)\n\n# Final locations of the centroid\ncentroid = kmeans.cluster_centers_\n#print(centroid)\n\n# The number of iterations required to converge\nconverge = kmeans.n_iter_\n\n\nl = kmeans.labels_[:5]\n\nkmeans_kwargs = {\"init\": \"random\", \"n_init\": 10, \"max_iter\": 300, \"random_state\": 42, }\n\nsse = []\nfor k in range(1, 11):\n     kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n     kmeans.fit(X_train.values)\n     sse.append(kmeans.inertia_)\n\nplt.style.use(\"fivethirtyeight\")\nplt.plot(range(1, 11), sse)\nplt.xticks(range(1, 11))\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"SSE\")\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}