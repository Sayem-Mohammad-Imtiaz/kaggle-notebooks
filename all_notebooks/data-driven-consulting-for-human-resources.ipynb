{"metadata":{"language_info":{"name":"python","version":"3.6.3","nbconvert_exporter":"python","pygments_lexer":"ipython3","mimetype":"text/x-python","file_extension":".py","codemirror_mode":{"name":"ipython","version":3}},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"cells":[{"cell_type":"markdown","metadata":{"_uuid":"d7d9366c97cbb050174e7a680ec43686d8e5d2a1","_cell_guid":"eb04e213-e09d-4cf3-acd8-0559fca1dbb6"},"source":"<img src = \"https://www.tilburguniversity.edu/upload/c67b70dd-9a4f-499b-82e0-0aee20dfe12a_jads%20logo.png\",width=500>\n\n<h1 align=center> Introduction to Data Science JBP010 </h1>\n<h1 align=center> Data Driven Consulting for Human Resources </h1>\n\n<h3 align=center> October 31th, 2017</h3>"},{"cell_type":"markdown","metadata":{"_uuid":"20649d842389d9980ec3da7872a31e570c576429","_cell_guid":"ab127ad3-9e27-4115-b7b5-a12aaed842b7"},"source":"*On a daily basis HR leaders and business managers face many complex challenges that can be overcome by adopting a data-driven approach. In this notebook it is illustrated how human resources could benefit from collecting and analyzing employee data to check for gender equality and workforce diversity and to predict employee turnover.*"},{"cell_type":"markdown","metadata":{"_uuid":"a1ab0b9f1f8c109ee7fba0126fdc5654b612dc0a","_cell_guid":"5efe5c28-94a4-41f3-918c-eb6212395735"},"source":"---"},{"cell_type":"markdown","metadata":{"_uuid":"9164ce6865049736dec3cf34fc2f2bc842760943","_cell_guid":"ebdf93f6-0968-4e66-9a8c-98a6d51089a1"},"source":"## Project Description\n<div align=\"justify\" style=\"width:95%;\">\nA large medicine manufacturer has requested a team of data science students **to identify historical trends for employee attrition and to predict employee turnover in advance** using quantitative methods on a dataset consisting of personal records from all current and past employees. Additionally, the management wonders *what would be effective Human Resource policies given the current workforce and to produce actionable insights in this regard*.\n<br/><br/>\n\nThe Society of Human Resource Managers [(Kantor, 2017)](https://www.huffingtonpost.com/julie-kantor/high-turnover-costs-way-more-than-you-think_b_9197238.html) estimates that replacing a salaried employee costs 6 to 9 months' salary on average. Given the median wage for workers in the United States was \\$44,148 per year in 2016 [(Doyle, 2017)](https://www.thebalance.com/average-salary-information-for-us-workers-2060808), this amounts to roughly \\$20K - \\$35K per employee. It follows that recruiting fees are significantly higher than the costs associated with measures to attain employees (e.g. promotion, salary increase, additional trainings etc.). In other words, it's a relevant problem because the financial stakes are high.  \n<br/>\nSince the manufacturer has only recently adopted a data-driven culture, they have very little experience with employee analytics: they used to only produce reports. For that reason the client specifically requested the data scientists to validate the correctness of their internal data and consider more advanced modelling techniques. \n\nGiven this project proposal we first need to establish whether this Data Science project proposal is *hype* or *hope*. Therefore, we answer the following three questions: \n\n<ol>\n    <li> **What main questions are you trying to answer?**</li>\n        <ul>\n            <li>*How can current and future employee attrition be minimized?*</li> \n            <li>*What actionable insights can you give on the current composition of our workforce?*</li>\n        </ul><br>\n\n    <li> **Do you have the data to answer the questions?**</li>\n        <br>The manufacturer supplies the following [dataset](https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset) which consists of 1470 rows and 35 columns. The exploratory analysis, together with the scientific literature, give us a basis to find actionable insights in the data. The predictive analysis shows what features influence attrition, by carefully selecting and tuning a classification model. A similar Kaggle [dataset](https://www.kaggle.com/ludobenistant/hr-analytics/data) will be used to validate the findings in aforementioned dataset.\n        Nevertheless, it's very important to note that both datasets are *simulated* which may affect a generalisation to real world data.\n<br><br>\n        We assume that the drawbacks of any resignation outweigh the potential benefits (e.g. better performance of a new hire). This is a reasonable assumption to make since all employees are evaluated either \"Excellent\" or \"Outstanding\" (though in reality is doubtful whether this is really the case). <br><br>\n\n    <li> **If we can answer the question, can we use the answer?**</li>\n        <br>Given that the predictive model produces good results, we are then able to predict what factors are more likely to make an employee quit. With this knowledge, the management and  Human Resources department can take action to minimize the number of resignations, both at company level and individual level. For example, adjustments in the working environment, hiring practises, number of trainings per year, work schedules or employees' salary.\n</ol>\n</div>"},{"cell_type":"markdown","metadata":{"_uuid":"7d6ae5d14b5115e1c96a23e36d9d344a711e4b43","_cell_guid":"27f9de30-3fb2-42fb-a42d-aee26ff320d1"},"source":"---"},{"cell_type":"markdown","metadata":{"_uuid":"e79f772126ff00e06ea8c801f6268dd76d837d75","_cell_guid":"17cd7956-3867-4173-a35d-994e67014511"},"source":"### Prepare the data\n<br>\n<div align=\"justify\" style=\"width:95%;\">\nThe dataset includes 6 features which don't add any meaningful value (for a complete overview of all features and their definitions see [Appendix A](#appendix-a)). Either all values are identical (`EmployeeCount`, `StandardHours`, `Over18`) or the attribute definition is unspecified (`HourlyRate`, `DailyRate`, `MonthlyRate`). In [this](https://www.kaggle.com/jamestollefson/modeling-the-business-cost-of-retention) kernel James Tellefson does an excellent job explaining why the latter type of attributes should not be taken into account. He shows that there is not any correlation between various rates of pay (e.g. `DailyRate` and `MonthlyRate`). Therefore, in the comments he concludes that there doesn't seem to be any meaningful relationship between those features at all. That's why all aforementioned features will be excluded from our analysis. Note that after these transformations we end up with a file without any missing values (`NAs`).</div>"},{"cell_type":"code","metadata":{"_uuid":"3ef7c80c8115c3d962782396a795c38742896e82","collapsed":true,"_cell_guid":"013ef0cd-53fe-4eaa-be12-0cbb99ade15c"},"source":"%matplotlib inline\nimport pandas as pd; pd.options.display.float_format = '{:,.3f}'.format\nimport numpy as np\nimport matplotlib.pyplot as plt; plt.rcParams['figure.figsize'] = (8,6)\nimport seaborn as sns; sns.set_style(\"dark\")","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"9a5ef54c2b2fae979db5464b9661d98c55aba909","collapsed":true,"_cell_guid":"fa121c30-8089-452e-ab6a-a60276780f88"},"source":"data = pd.read_csv(\"../input/WA_Fn-UseC_-HR-Employee-Attrition.csv\")\ncolumns_to_drop = ['EmployeeCount', 'StandardHours', 'Over18', 'DailyRate', \n                   'HourlyRate', 'MonthlyRate', \"EmployeeNumber\"]\n\ndata = data.drop(columns_to_drop, axis=1)\ncolumns = [\"Attrition\"] + [col for col in data.columns if col != \"Attrition\"]\ndata = data[columns].replace({\"Yes\": 1, \"No\": 0})\ndata.info()","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"6e902465b217b0017523511d8cfbb128fe92f8b6","_cell_guid":"bb0deb8a-0644-4032-ab8a-e996ba4952ba"},"source":"First, let's observe what data are available. Without considering the turnover rate the dataset consists of 27 features which can be grouped as follows:<br><br>\n\n<ul>\n<li>**Personal**:</li>\n<ul><li>Gender, Age, MaritalStatus</li> \n<li>TotalWorkingYears, NumCompaniesWorked</li>\n<li>Education, EducationField</li></ul><br>\n<li>**Job related**:</li> \n<ul><li>JobLevel, JobRole, Department</li>\n<li>MonthlyIncome, StockOptionLevel, PercentSalaryHike</li>\n<li>PerformanceRating, TrainingTimesLastYear, JobInvolvement</li></ul><br>\n<li>**Career at company**:</li> \n<ul><li>YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager</li></ul><br>\n<li>**Statisfaction**:</li>\n<ul><li> RelationshipSatisfaction, EnvironmentSatisfaction, JobSatisfaction</li></ul><br>\n<li>**Work intensivity**:</li> \n<ul><li>WorkLifeBalance, BusinessTravel, OverTime, DistanceFromHome</li></ul>\n</ul>"},{"cell_type":"markdown","metadata":{"_uuid":"48b675e71bb737197bf145708408f947f015a062","_cell_guid":"604c2e3b-47bc-4097-9c67-255cae95e294"},"source":"---"},{"cell_type":"markdown","metadata":{"_uuid":"4bcf9a98f44279872156ac2b9e07b3b45c820e31","_cell_guid":"dc79f23c-c67d-4fdb-aacd-63fe14c88cb6"},"source":"## Descriptive Analysis\n<br>\nWe will cover 3 themes in this analysis, namely (1) Workforce Composition, (2) Gender Equality and (3) Turnover, by answering the following 10 questions:\n\n* **Workforce composition**\n    - How loyal are the employees towards the company?\n    - How diverse is the workforce in terms of education? \n    - How does education relate to employee's performance?\n\n\n\n* **Gender equality**\n    - How are genders distributed across the departments of the company? (horizontal repartition) \n    - How are genders distributed across the hierarchy? (vertical repartition)\n    - What differences exist in the compensation of men and women at the same position? \n\n\n\n* **Turnover (Attrition)**\n    - What is the relation between employees satisfaction and attrition?\n    - What impact has the length of the career on the turnover?\n    - How are the different variables correlated?\n    - How good are our features for predicting attrition?   "},{"cell_type":"markdown","metadata":{"_uuid":"bc6b26f4c08d65d132e954a3322d874b01e236e5","_cell_guid":"dd5b32f9-9ba9-4615-8e68-20bd2173376f"},"source":"### 1.1 Workforce composition\n<h4>How loyal are the employees towards the company? </h4>\n<div align=\"justify\"style=\"width:95%;\">\nOur first observation is that the average employee is middle-aged (37 years-old). 50% of the employees are between 30 and 43 years old. The average seniority is 7 years but the standard deviation is important (6 years), mostly because of extreme values: the 3rd quartile is 9 years whereas the maximum is 40 years of tenure. In other words, there are some outliers. Figure 1.1 shows that a great share of the workforce stays loyal the company. That is, the majority of their `TotalWorkingYears` has been spent at the current company (i.e. points close to the diagonal).  \n</div> "},{"cell_type":"code","metadata":{"_uuid":"ccd64d8102c7300f3bb54ca64bb6da58bc35c011","scrolled":false,"collapsed":true,"_cell_guid":"70e8a867-16fd-4440-9b9d-b47aa446e51f"},"source":"sns.jointplot(x=\"TotalWorkingYears\", y=\"YearsAtCompany\", data=data).plot_joint(sns.kdeplot)\nplt.title(\"Figure 1.1: Employees seniority relative to their working years\", y=-.2)\nplt.subplots_adjust(bottom=.2)\ndata[[\"Age\", \"YearsAtCompany\", \"TotalWorkingYears\"]].describe()","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"e46c9004739b6fd325905bcbe959f44500b6c604","_cell_guid":"3991f5b8-4c2c-41ca-98b3-5ff6d639f33f"},"source":"<div align=\"justify\"style=\"width:95%;\">\nIn general, employees tend to stay loyal to the company. It is thus the duty of the management to make sure that their workforce is skilled enough to keep performances up for a number of years. One logical driver of employees' performance is their education. The HR-department should make sure that they select candidates with  appropriate educational backgrounds while keeping a diversified workforce. That is why we look into that in the next section.\n<br/><br />\n<hr>\n<h4>How diverse is the workforce in terms of education?</h4>\nOur data gives us 5 differents fields of study common in the company (Biology, Medical, Marketing, Technical, Human Resources) and 5 levels of education (No Degree, College, Bachelor, Master, PhD). In the next paragraphs we will analyse how the workforce is structured in terms of education and try to discover insights relative to performance.\n<br/><br />\nFigure 1.2 shows the distribution of employees according to their field of study (chart on the left) and the level of their education (chart on the right). The most common levels of education are bachelors and masters, followed by college degrees and college dropouts. The fraction of employees with a PhD. is the smallest which is in accordance with the average number of PhD researchers in the US population (<a href=\"https://www.census.gov/content/dam/Census/library/publications/2016/demo/p20-578.pdf\">Ryan & Bauman, 2016</a>).</div>"},{"cell_type":"code","metadata":{"_uuid":"54603ac96737280b8002d5784d9ab41d8f5ae129","scrolled":false,"collapsed":true,"_cell_guid":"c85ddb38-fda3-41ef-bcc4-599aecd595f7"},"source":"fig = plt.figure(figsize=(15, 6))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122, sharey=ax1)\n\nfor i, (ax, label) in enumerate(zip([ax1, ax2], [\"EducationField\", \"Education\"])):\n    vc = data[label].value_counts()\n    if i == 1: vc = vc.sort_index()\n    sns.barplot(x=vc.index, y=vc.values, palette=\"Blues_d\", ax=ax)\n    ax.set_xlabel(\"\")\n    [(label.set_fontsize(9), label.set_rotation(45)) for label in ax.get_xticklabels()]\n\nstudy_fields = [\"Biology\", \"Medical\", \"Marketing\", \"Technical\", \"Other\", \"H.R.\"]; ax1.set_xticklabels(study_fields)\ndiplomas = [\"No degree\", \"College\", \"Bachelor\", \"Master\", \"Ph.D\"]; ax2.set_xticklabels(diplomas)\nax2.yaxis.set_visible(False)\nplt.suptitle(\"Figure 1.2: Educational Backgrounds of the Employees\", size=14)\nplt.subplots_adjust(bottom=.15);","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"2778a18691f30639a7dfb9ed598b13b26cc6bf64","_cell_guid":"ec7d156f-adad-437f-b980-952b91467390"},"source":"<div align=\"justify\" style=\"width:95%;\">\nFigure 1.3 below decomposes the employees by department and field of study. We observe a large sales force and a even bigger pool of researchers. Interestingly enough, we see that a variety of  scientific backgrounds work in the Sales department. It would be interesting to know whether these employees are there from the start or whether they originally started as a researcher and moved to a sales position over the course of their career. Additionally, some educational backgrounds seem to be related to an employee's department. For example, employees with a background in Marketing are only present in the Sales department and similarly, people with a HR-diploma work for the HR-department.</div>"},{"cell_type":"code","metadata":{"_uuid":"4d44e75cf56b33e67beac52e5b2e4d36bbae94ae","scrolled":false,"collapsed":true,"_cell_guid":"e07ff4b4-d7da-4448-b28e-34602de592cf"},"source":"dept_field = (data.pivot_table(data, index=\"Department\", columns=\"EducationField\", aggfunc='size'))\nplt.figure(figsize=(12,6))\nsns.heatmap(dept_field, annot=True, cmap=\"Blues\", fmt=\".0f\")\nax = plt.gca()\nstudy_fields = [\"H.R.\", \"Biology\", \"Marketing\", \"Medical\", \"Other\", \"Technical\"]; ax.set_xticklabels(study_fields)\nax.set_xlabel(\"\"); ax.set_ylabel(\"\")\nplt.subplots_adjust(left=.2, bottom=.2)\nplt.title(\"Figure 1.3: Repartition of Employees in Departments\", y=-.2);","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"83e29b3b0108fb2d5445a18faa6deb91877a04be","_cell_guid":"2f566f09-9a0b-438c-b076-937c4005015e"},"source":"<div align=\"justify\" style=\"width:95%;\">\n<hr>\n<h4>How does education relate to employee's performance?</h4>\nNow, we are interested to observe how well certain types of degrees  perform in different job positions. Since we only have excellent and outstanding performance ratings (3's and 4's), we will look at the proportion of outstanding performers relative to the population with the same combination of education and job role. This allows us to draw conclusions regarding the best matches between job position and field of education.\n<br/><br/>\nThe table below shows the top ten performers. Remarkably, employees with a certain type of background may perform very well in one position while doing badly in another position. For example, managers with a medical diploma do best on average, whereas employees with the same background are performing relatively poorly in a Human Resources position. On the other hand, biologists seem to do quite well as Human Resources employees.\n</div>"},{"cell_type":"code","metadata":{"_uuid":"7bedd874d01d91aec54c5d1631b6f10d47130f24","collapsed":true,"_cell_guid":"fd4199ec-558b-46b9-a555-4aba8b3b6a78"},"source":"performance = data.pivot_table(index=['EducationField', 'JobRole'], \n                               columns='PerformanceRating', \n                               values='Attrition', aggfunc='count')\n\nperformance['PercentageTopPerformers'] = performance[4] / (performance[3] + performance[4])\nperformance.sort_values('PercentageTopPerformers', ascending=False).drop([3,4], axis=1).head(10)","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"d2b23554448261d40902bebaf975314b6113c9dd","_cell_guid":"931fd153-c9e6-486f-b94d-00a699d9cb5c"},"source":"---\n### 1.2. Gender equality\n<div align=\"justify\" style=\"width:95%;\">\nGender equality was, and still is, the topic of many debates. Some studies suggest that a more diverse workplace increases the performance of the company and the general satisfaction among employees. Nowadays most academics agree that our society should aim for a diverse workplace where male and female are equally represented and equally paid.\n<hr>\n<h4>How are genders distributed across the departments of the company? (horizontal repartition)</h4>\nWhile we could not demonstrate any differences in terms of wages between men and women, we have evidence that men are much more represented than women in the company. The ratio of men vs. women is 3:2 (meaning that there are 3 men for 2 women). Furthermore, it seems that the representation of women drops after reaching a certain job level. This may be a sign that a glass ceiling exists in this company.<br><br>\n\nFigure 1.5 below shows the repartition of men and women in the different Job Departments of the company. We observe that the department where the representation of men exceeds women's is the highest in Human Resource Department, with more than twice as many men as women. That being said, all departments have a larger proportion of men than women.\n</div>"},{"cell_type":"code","metadata":{"_uuid":"2aff40ccb09939198f8b8fabf749b5b4fae8ea28","collapsed":true,"_cell_guid":"a467573f-7bfb-49c9-b5e6-9533bf5d5269"},"source":"def plot_gender_repartition(data, **kwargs):\n    caption = kwargs[\"caption\"]; del kwargs[\"caption\"]\n    gender_repartition = data.groupby([kwargs[\"index\"], kwargs[\"columns\"]], as_index=False).count().pivot(**kwargs)\n    gender_repartition[\"Total\"] = gender_repartition[\"Male\"] + gender_repartition[\"Female\"]\n    gender_repartition[\"Female\"] = gender_repartition[\"Female\"] / gender_repartition[\"Total\"] * 100\n    gender_repartition[\"Male\"] = gender_repartition[\"Male\"] / gender_repartition[\"Total\"] * 100\n    gender_repartition[[\"Female\", \"Male\"]].plot(kind=\"barh\", stacked=True, legend=False)\n    ax = plt.gca()\n    ax.axvline(50, color=\"k\", zorder=0)\n    format_chart(ax, True)\n    label_barh_chart(ax)\n    plt.title(\"{}: Gender Repartition by {}\".format(caption, kwargs[\"index\"]), y=-.2)\n    plt.legend(bbox_to_anchor=(.7, 1.1), ncol=2)\n    plt.subplots_adjust(left=.16, bottom=.2)\n    \ndef format_chart(ax, multiline_labels=False, ticklabel_size=10):\n    [spine.set_visible(False) for spine in ax.spines.values()]\n    #[tl.set_visible(False) for tl in ax.get_xticklabels()]\n    ax.yaxis.set_label_text(\"\")\n    [tl.set(fontsize=ticklabel_size) for tl in ax.get_yticklabels()]\n    if multiline_labels:\n        ylabels = ax.get_yticklabels()\n        new_labels = [label.get_text()[::-1].replace(\" \", \"\\n\", 1)[::-1] for label in ylabels]\n        ax.set_yticklabels(new_labels)\n        \ndef label_barh_chart(ax):\n    text_settings = dict(fontsize=9, fontweight='bold', color=\"w\")\n    rects = ax.patches\n    for i, rect in enumerate(rects):\n        width = rect.get_width()\n        x_pos = width / 2 if i in range(len(rects) // 2) else 100 - width / 2\n        #color = \"pink\" if i in range(len(rects) // 2) else \"#2C6388\"\n        #rect.set_facecolor(color)\n        label = \"{:.1f}%\".format(rect.get_width())\n        ax.text(x_pos, rect.get_y() + rect.get_height()/2, label, ha='center', va='center', **text_settings)\n        \ndef label_barchart(ax):\n    text_settings = dict(fontsize=9, fontweight='bold', color=\"w\")\n    rects = ax.patches\n    for i, rect in enumerate(rects):\n        x_pos = rect.get_x() + rect.get_width() / 2\n        label = \"{:.1%}\".format(rect.get_height())\n        ax.text(x_pos, .05, label, ha='center', va='center', **text_settings)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"0f111740dcde489518d4270ec1a1316417cfb357","collapsed":true,"_cell_guid":"428ca036-2c13-4883-bb45-0690625c6004"},"source":"plot_gender_repartition(data, index=\"Department\", columns=\"Gender\", values=\"Attrition\", caption=\"Figure 1.4\")","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"3648dbc279541005306687aa3f451881b05a162f","_cell_guid":"6bf296e6-d1c4-4081-9b20-3be1bed50afc"},"source":"<hr>\n<h4>How are genders distributed across the hierarchy? (vertical repartition)</h4>\n<div align=\"justify\" style=\"width:95%;\">\nFigure 1.6 shows the repartition of men and women at the different job levels. We see that the proportion of women increases until almost reaching equal representation for Job Level 4. However, as discussed above, the proportion of women decreases rapidly at Level 5. This might be a sign of a glass ceiling keeping women to reach the higher spheres of hierarchy in this company.\n</div>"},{"cell_type":"code","metadata":{"_uuid":"595208a1c019c4f75a7a1e6e091c1dac764be974","scrolled":false,"collapsed":true,"_cell_guid":"c906e188-cc0e-46d2-bf23-d2f472dcb902"},"source":"plot_gender_repartition(data, index=\"JobLevel\", columns=\"Gender\", values=\"Attrition\", caption=\"Figure 1.5\")\nplt.gca().set_ylabel(\"JobLevel\");","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"d6c7570da86230c1997b60e39eb3bc2397568ca9","_cell_guid":"c3a59802-8f7a-4597-bd6e-ae5af07fbb9f"},"source":"<hr>\n<h4>What differences exist in the compensation of men and women at the same position?</h4>\n<div align=\"justify\" style=\"width:95%;\">\nAn unbalanced repartition of men and women does not necessarily mean that there is an income gap between genders. Figure 1.7 illustrates that point: for any job level, we see that the ratio between men and women wages is relatively close to 1, which means that women and men are paid equally in this company.\n</div>"},{"cell_type":"code","metadata":{"_uuid":"4d9a888d1ec48ddd2fe2c2ba070d3e5094a4ccd6","collapsed":true,"_cell_guid":"77e5ff15-4f46-46ce-8c1c-fc1249ce44ea"},"source":"plt.figure()\nsns.barplot(x=\"JobLevel\", y=\"MonthlyIncome\", hue=\"Gender\", data=data, ci=99)\nplt.title(\"Figure 1.6: Income gap between men and women\", y=-.2)\nplt.subplots_adjust(bottom=.2);","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"8e7df07577aa38ebc8159f348934e984c90f8854","_cell_guid":"596a2ebf-a705-4c0b-b066-5ebbcc1671b5"},"source":"---\n### 1.3 Turnover"},{"cell_type":"markdown","metadata":{"_uuid":"9c9186d43c3d13e25fd93d604a0079ac4d90245f","_cell_guid":"9f3bbe32-97f9-46b0-819d-c0005242833b"},"source":"<div align=\"justify\" style=\"width:95%;\">\nNow that we have a better grasp of the data, it is time to explore features variance given the turnover. That is, what can we learn from the differences between employees who stayed at the company and those who left. To do this, we will consider the <i>turnover rate</i> in a group of employees, defined as the proportion of employees who quitted in the general population regarding a given feature.\n\nIn order to get a more precise idea of which department and which field of study is at stake, we have represented the Figure 1.3, but this time with the turnover rate of the employees. We directly see that technical degrees have a higher rate of attrition, and it is maximal when they are working for the H.R. department. The Sales department sees a large share of its employees leave as well.\n</div>"},{"cell_type":"code","metadata":{"_uuid":"7ff986460ab65aaa11c0254d26252a22a54edd0e","scrolled":false,"collapsed":true,"_cell_guid":"0b0aa2f8-5ba4-4629-9c68-ef3806c5fb40"},"source":"attrition_education = data.pivot_table(index=\"Department\", values=\"Attrition\", columns=\"EducationField\", aggfunc=\"mean\")\nplt.figure(figsize=(12,6))\nsns.heatmap(attrition_education, annot=True, cmap=\"Blues\")\nax1, ax2 = plt.gcf().get_axes()\nax1.set_xlabel(\"\"); ax1.set_ylabel(\"\")\nstudy_fields = [\"H.R.\", \"Biology\", \"Marketing\", \"Medical\", \"Other\", \"Technical\"]; ax1.set_xticklabels(study_fields)\nplt.title(\"Figure 1.7: Turnover By Deparment and Education\", y=-.2)\n[el.set_text(\"{:.1%}\".format(float(el.get_text()))) for el in ax1.texts]\nax2.set_yticklabels([\"{:.0%}\".format(float(tl.get_text())) for tl in ax2.get_yticklabels()]);","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"b85a3212c3e6e0ad617f2fb2ce59d25a13468863","_cell_guid":"f410d15b-f890-4c45-9029-c3893db71206"},"source":"<div align=\"justify\" style=\"width:95%;\">\n<h4>What is the relation between employees satisfaction and attrition?</h4>\nFigure 1.8 shows that low levels of satisfaction are associated with high rates of turnover. The relation between `JobInvolvment` and `Attrition` is even more striking.\n</div>"},{"cell_type":"markdown","metadata":{"_uuid":"bde55c4c88b283aaa513557389b085cbed8cf5fb","_cell_guid":"ac25ee1c-db3b-4a13-9f75-3283167b1556"},"source":"<figure>\n<img src=\"img/Satisfaction.png\" style=\"float:center;\" width=800/>\n</figure>\n<div align=\"center\">Figure 1.8: Satisfaction vs. Turnover Rate</div>"},{"cell_type":"markdown","metadata":{"_uuid":"833ec58ecd0f4a5ec3471ac46f018488af3fd172","_cell_guid":"eed235fa-a9a9-416c-9ca8-33385d6e2130"},"source":"---\n<h4>What impact has the length of the career on the turnover?</h4>\n<div align=\"justify\" style=\"width:95%;\">\nFigure 1.9 shows a high correlation between low seniority and `Attrition`: the less time an employee has spent in the company, the more likely he is to resign. Surprisingly enough, not having been promoted in a while does not seem to play any significant role when choosing to quit or not the company.\n</div>"},{"cell_type":"markdown","metadata":{"_uuid":"064ab4c125c02f08a3aa573f11db7193a5442ede","_cell_guid":"4dc57eed-fa90-4b56-a7bb-57e4b3e35adb"},"source":"<div align=\"center\">\n<img src=\"img/WorkExperience.png\", style=\"float: left\" width=900 />\n</div>"},{"cell_type":"markdown","metadata":{"_uuid":"591ce57139c60b0d8e034f0556988577558c9e3c","_cell_guid":"e5733e73-3ea4-4d39-8bf5-78f20450ec8c"},"source":"<div align=\"center\">Figure 1.9: Satisfaction vs. Turnover Rate</div>"},{"cell_type":"markdown","metadata":{"_uuid":"a85c99c87f1faf0f80b1690ac994fe7d6ed10e74","_cell_guid":"b0c3bfee-a773-45f9-96e4-63bab464e7a6"},"source":"---\n<h4>How are the different variables correlated?</h4>\n<div align=\"justify\" style=\"width:95%;\">\nIn the predictive analysis, we will try to predict whether or not an employee is likely to quit given a certain number of explanatory variables. In order to get a better feeling of elements which are relatively strongly related with each other, we will observe the existing correlations in our dataset. But before going any further, it is important to transform our data so that categorical features are considered as such, and not as other numerical variables. We will thus create new (dummy) variables for each category. For example, the feature department will be divided into 3 features corresponding to each individual department.\n</div>"},{"cell_type":"code","metadata":{"_uuid":"e91d5a14d39eb22c9217ac43a39d8cbf742474f8","collapsed":true,"_cell_guid":"dbf35c9c-c2fa-4534-ab89-bc9d55252ca6"},"source":"data = data.replace({\"Male\": 1, \"Female\": 0, \"Non-Travel\": 0, \"Travel_Rarely\": 1, \"Travel_Frequently\": 2})\nX, y = pd.get_dummies(data.iloc[:, 1:].copy()), data.iloc[:, 0].copy()","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"ae339efb9d0171192fd175d3fa281fb5dcba14ac","_cell_guid":"f7356647-274e-4545-9751-4bd69533fe6f"},"source":"<div align=\"justify\" style=\"width:95%;\">\nThe Figure 1.10 shows a representation of the correlation matrix of our dataset. Elements that share a strong positive correlation will have a dark red intersection, while strong negative correlation will have a dark blue correlation. Weakly correlated variables have lighter tints of color.<br><br/>\nMost of the observed correlations intuitively make sense. For example, employees with a higher job level earn more money, older employees have more work experience, etc. Also note the red area in the upper left red square of the heatmap. These 8 features (`TotalWorkingYears`, `JobLevel`, `MonthlyIncome`, `Age`, `YearsAtCompany`, `YearsInCurrentRole`, `YearsSinceLastPromotion`, `YearswithCurrManager`) are all strongly correlated. These have all something to do with time or experience. Thus a positive relationship between is not surprising. `Attrition` however, does not seem to be correlated with other variables in general, with  the exception of `OverTime` and `MartitalStatus_Single`. <br><br/>\nOveral it is good to see that our dataset does not show too much signs of multicollinearity that might otherwise impact the generalisation of the predictive model.\n</div>"},{"cell_type":"code","metadata":{"_uuid":"09266a55762e5f3579176aa24240c83fbbe338a9","scrolled":false,"collapsed":true,"_cell_guid":"509db0d8-a21e-4b2e-9b09-2bfc46b48c89"},"source":"df = X.copy()\ndf[\"Attrition\"] = y\ndf = df[[\"Attrition\"] + list(X.columns)]\ncorr = df.corr()\nfig, ax = plt.subplots(figsize=(12,10))\nsns.heatmap(corr, cmap=\"RdYlBu_r\", vmin=-1, vmax=1, ax=ax)\nplt.title('Figure 1.10: How are the variables correlated?', size= 14);","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"511148420378d78c50d88760b1ebc28fcbb6f0ba","_cell_guid":"00214e9a-8877-41c3-b8a5-69f1b706dae0"},"source":"---\n<h4>How good are our features for predicting attrition?</h4>\n<div align=\"justify\" style=\"width:95%;\">\nNow we will go one step further in analysing how good our features are to predict attrition. We will look at the predictive power of our variables, by performing an analysis of the variance (ANOVA) to assess the predictive power of our features. ANOVA combined with a Chi-square test answers the question: \"Are two joint distributions independent?\". The output of the test consists of two values: <br>\n<ul>\n<li>The **Fisher statistic (F)**, which is the ratio between variations in sample means and variations within the samples. A feature that gets a large F value implies that a much of its is impacted on the target variable `Attrition`.</li> \n<li>The **p-value**, which is the probability to observe such distribution given that two variables are independent. In our case, we want to minimize the p-value.</li></ul>\nGenerally, in such analysis, we will decide of an arbitrary threshold $\\alpha$ that will dictate whether or not a feature passed the test or not. We will choose a confidence level of $1-\\alpha = 95 \\%$, meaning that we will reject every result above this probability, considering that too little of the variation of the target variable can be explained by the variation of one particular feature.<br><br/>\nAs can be seen in the correlation matrix above, we see that `OverTime` and `Attrition` are strongly correlated. From all other features, only one did not pass our test. This means that all of the 26 remaining features have less than 5% probability of being totally independent of the dependent variable `Attrition`.\n</div>"},{"cell_type":"code","metadata":{"_uuid":"9508a5fddda1b19ed798b4877387cd5f3e945285","collapsed":true,"_cell_guid":"0f17f7d9-21a1-404b-9746-63882497baff"},"source":"from sklearn.model_selection import train_test_split\n# Train / Test split (size of training set: 75 %)\nX_train, X_test, y_train, y_test = train_test_split(X, y)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"b34a9672834c4157852ca534cabe58c14a7fb591","collapsed":true,"_cell_guid":"2d6abf50-350d-4edb-8030-8a366f91624e"},"source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n#X_train[\"MonthlyIncome\"] = scaler.fit_transform(X_train[[\"MonthlyIncome\"]])[0]\n#X_test.loc[:, \"MonthlyIncome\"] = scaler.transform(X_test[[\"MonthlyIncome\"]])\n#X_train[\"MonthlyIncome\"]\nX_train = X_train.copy(); X_test = X_test.copy()\nX_train[\"MonthlyIncome\"] = scaler.fit_transform(X_train[[\"MonthlyIncome\"]])[:,0]\nX_test[\"MonthlyIncome\"] = scaler.transform(X_test[[\"MonthlyIncome\"]])[:, 0]","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"c94bc5b3d42e67bffe2e6c2518773893fbc1a903","scrolled":false,"collapsed":true,"_cell_guid":"1942abc2-33b5-4be0-a80a-6c1413377bab"},"source":"from sklearn.feature_selection import SelectKBest\n\n# Confidence Threshold\nalpha = .1\n\nnp.random.seed(42)  # To make sure our results are reproducible\nanova_filter = SelectKBest()\nanova_filter.fit(X_train, y_train)\n\n\nanova_scores = pd.DataFrame(index=X.columns)\n\nanova_scores[\"Fisher\"] = anova_filter.scores_\nanova_scores[\"p-value\"] = anova_filter.pvalues_\nanova_scores = anova_scores.sort_values(\"Fisher\", ascending=False)\nselected_features = list(anova_scores.loc[anova_scores[\"p-value\"] < 1 - alpha, :].index)\nif len(selected_features) == X.shape[1]:\n    print(\"No discarded feature\")\nX = X[selected_features]\nanova_scores.style.apply(lambda f: [\"color: red\"] * 2 if f[\"p-value\"] > 1-alpha else [\"color: black\"]*2, axis=1)","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"4e9a4aa7e52937ccf4007a0ad457c15dd50254e0","_cell_guid":"9dcd4c15-6b1e-4276-9e4e-a0b67e9e5683"},"source":"---\n## 2. Predictive Analysis"},{"cell_type":"markdown","metadata":{"_uuid":"466c5bdc3ab2ed18e0680945e23075272a426918","_cell_guid":"723caa99-8021-4d97-ad7e-5d3e5c5f102d"},"source":"<div align=\"justify\" style=\"width:95%;\">\nThe second part of this analysis is dedicated to predict the turnover rate of employees. After having converted categorical data to numerical data with LabelEncoders, we split the dataset into a training and a testing set. The model selection phase is conducted carefully by further splitting the training set into 10 folds and combining them randomly to training and testing sets (i.e. cross-validation).\n<br><br/>\nSince a large proportion of the target variable is negative, we discarded accuracy as a metric to evaluate the performance of the fitted models. After all, a baseline model (i.e. assume nobody would resign) would correctly classify 80% of our samples, hence reaching a high accuracy without performing well in terms of recall and precision. Instead, we will use a more appriopriate measure the **Receiver Operating Characteristic** (ROC) curve and its corresponding **Area Under the Curve** (AUC) score. This metric reflects True Positives versus False Positive. In our case, a dummy classifier would receive a AUC score of 50% which betters reflects its performance compared to the much higher accuracy score of 80% we obtained previously.<br><br/>\nOnce we have selected the best model to use, we tune its parameters by performing an exhaustive GridSearch analysis on its hyperparameters. Then we refit the model on the whole training set on the selected model with the optimal parameters and we calculate the final score of the model. For convenience, we have calculated both AUC scores as well as the recall and precision scores to explore what type of errors our classifier makes.</div>"},{"cell_type":"code","metadata":{"_uuid":"ab41fdb564b049bf9ecf743b8c5b737d9b1c9abb","collapsed":true,"_cell_guid":"f874c6fc-6650-4706-a403-32a3e657bd54"},"source":"from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\nfrom sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, classification_report\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"65dd8a3c2c0ea5faab138050067766dca8b5964e","collapsed":true,"_cell_guid":"3e4f1468-490c-4df7-bccf-d0a24be00240"},"source":"from collections import OrderedDict\n\nscoring = \"roc_auc\"\n\nmodels = [\n    \n    (\"Dummy\", DummyClassifier(strategy=\"most_frequent\")),\n    (\"SVM\", SVC()), \n    (\"LR\", LogisticRegression()),\n    (\"NB\", GaussianNB()),\n    (\"kNN\", KNeighborsClassifier()),\n    (\"DT\", DecisionTreeClassifier()), \n    (\"RF\", RandomForestClassifier())\n\n]\n\nresults = OrderedDict()\nfor name, model in models:\n    kfold = KFold(n_splits=3)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n    results[name] = cv_results\n    \nresults = pd.DataFrame(results)\nplt.figure(figsize=(10, 6))\nplt.bar(range(results.shape[1]), results.mean(), yerr=results.std())\nplt.gca().set_xticklabels([\"\"] + list(results.columns) + [\"\"])\nplt.title(\"Figure 2.1: Comparison of AUC score for multiple classifiers\", y=-.2)\nplt.subplots_adjust(bottom=.2)\nlabel_barchart(plt.gca())\n","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"ee420970398448d88aee0a5847e92a9c7d7a7c5b","_cell_guid":"7976ae87-f4bf-4d09-8fb1-0720803a3d2f"},"source":"<div align=\"justify\" style=\"width:95%;\">\nThe two best base models give us an AUC score of 83.3% and 76.2% for the LogisticRegression and RandomForest respectively. However, we did not perform any tuning of the hyperparameters. This is the next step: we will only select the two top performing models to search among a variety of possible values for the optimal hyperparameters.\n</div>"},{"cell_type":"code","metadata":{"_uuid":"697887ffdf7a4612d0d833e99bc9715b1fa27fc2","scrolled":false,"collapsed":true,"_cell_guid":"781c456d-0dd3-4ea7-b09b-7c706721d697"},"source":"n_features = X_train.shape[1]\n\nclassifiers = [\n    (\"LogisticRegression\", LogisticRegression()), \n    (\"RandomForest\", RandomForestClassifier(n_estimators=50)),\n    (\"GaussianNB\", GaussianNB())\n]\n\nall_params = {\n    \"LogisticRegression\": {\"penalty\": [\"l1\", \"l2\"], \"C\": np.logspace(-3, 3, 7), \"class_weight\":[\"balanced\", None]}, \n    \"RandomForest\": {\n        \"max_features\": range(5, n_features, (n_features - 5) // 3), \n        \"max_depth\": range(3, 6, 2),\n        \"min_samples_split\": range(5, 101, 25)\n    },\n    \"GaussianNB\": {\"priors\": [None, [.161, .839]]}\n}\nresults = pd.DataFrame(index=[item[0] for item in classifiers], \n                       columns=[\"name\", \"params\", \"accuracy\", \"auc_score_tr\", \"auc_score_te\", \n                                \"precision\", \"recall\", \"fscore\", \"support\", \"TP\", \"FP\", \"FN\", \"TN\"])\n\n\nbest_models, scores = [], []\nfor i, ((name, clf)) in enumerate(classifiers):\n    params = all_params[name]\n    gs = GridSearchCV(clf, params).fit(X_train, y_train)\n    best_models.append(gs.best_estimator_)\n    y_pred = gs.predict(X_test)\n    precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred)\n    auc_score_te = roc_auc_score(y_test, y_pred)\n    auc_score_tr = gs.best_score_\n    accuracy = (y_pred == y_test).mean()\n    params = gs.best_params_\n    [[TP, FN], [FP, TN]] = confusion_matrix(y_test, y_pred)\n    results.loc[name, :] = (name, params, accuracy, auc_score_tr, auc_score_te, precision, \n                            recall, fscore, support, TP, FP, FN, TN)\n    \n    scores.append(roc_auc_score(y_test, y_pred))\n    gs_results = pd.DataFrame(gs.cv_results_).drop(\"params\", axis=1).sort_values(\"rank_test_score\")\n    print(\"\\n{}:\\n\".format(name))\n    print(\"\\tAccuracy: {:.2%}\".format((y_pred == y_test).mean()))\n    print(\"\\tAUC Score (Train set): {:.2%}\".format(gs.best_score_))\n    print(\"\\tAUC Score (Test set): {:.2%}\\n\".format(scores[-1]))\n    print(classification_report(y_test, y_pred))\n    print(best_models[-1], \"\\n\")\n    if i + 1 < len(classifiers): print(\"#\" * 90)\n    \n#results\n","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"1829ed9f67a4b10bdb56499b1831ae12c48ad52d","_cell_guid":"92de541a-ab94-45da-8d17-caf73f274a2c"},"source":"<div align=\"justify\" style=\"width:95%;\">\nIt turns out the Logistic Regression model gives the best results, scoring an AUC score of 87.1% on the training set and 70.0% on the test set. Also, the logistic model performs better in terms of recall and precision. In fact, it correctly classifies 64% (vs 40%) of all employees who quitted and once it predicts an employee will resign this prediction is correct 44% (vs. 8%) of the time. Further, it seems that the RandomForest may overfit, given the difference between the score on training and testing sets (85% vs 53%).<br><br>\n\nNote that the score that we got after tuning the hyperparameters (70%) is much lower than the previous score from the cross-validation (83%). The testing methodology that was adopted required us to first split the test in 60% training set and 40% testing set. Then, the cross-validation further splitted the training set into 10 folds. The scores have been calculated on a sample of 7.5% of the total volume dataset (i.e. $\\sim110$ rows) and 25% for the testing set. We think that this explains the observed difference in performance.<br>\n</div>"},{"cell_type":"code","metadata":{"_uuid":"fa757f3914e16c716e9df150f527e5cc8664fcc3","collapsed":true,"_cell_guid":"cd664651-63fb-4bcc-97be-806f9c6d2e22"},"source":"lr_scores = best_models[0].predict_proba(X_test)[:, 1]\nrf_scores = best_models[1].predict_proba(X_test)[:, 1]\nlr_fpr, lr_tpr, _ = roc_curve(y_test.ravel(), lr_scores.ravel())\nrf_fpr, rf_tpr, _ = roc_curve(y_test.ravel(), rf_scores.ravel())\nplt.plot(lr_fpr, lr_tpr, 'b', label='LogisticRegression (AUC={:.2%})'.format(scores[0]))\nplt.plot(rf_fpr, rf_tpr, 'g', label='RandomForest (AUC={:.2%})'.format(scores[1]))\nplt.title('Figure 2.2: Receiver Operating Characteristic')\nplt.plot([0,1],[0,1],'r--', label=\"Random predictions\")\nplt.legend(loc=4)\nplt.ylabel('True Positive Rate'); plt.xlabel('False Positive Rate');","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"aa330a084713bebde3ed25277b1063017f4450de","_cell_guid":"769792a3-c3bc-40ce-9e9e-f1c79bf9ffbb"},"source":"<div align=\"justify\" style=\"width:95%;\">\nFigure 2.2 displays the ROC curve for both classifiers, that is the True Positive Rate (TPR) as a function of the False Positive Rate (FPR). We see that LogisticRegression performs always at least as well as the RandomForest. Note that the best possible classifier has a TPR of 1 and a FPR of 0 (meaning that all persons having quitted have been well classified, a no one who has remained has been misclassified).<br><br>\n\nThe table below shows the feature importances (coefficients of the logistic model). As expected, `OverTime` is the feature which affects turnover the most (see below). Features related to employee's well being like `JobInvolvement`, `WorkLifeBalance`, `EnvironmentSatisfaction` and `JobSatisfaction` are inversely proportional to the turnover rate. <br>\nThe same observation can be done for Professional Experience features like `JobLevel`, `YearsInCurrentRole` and `TotalWorkingYears`. One observation in particular is alarming: `PerformanceRating` is positively correlated to `Attrition` which can be a sign of Brain Drain. This phenomena implies that the best performing employees move to other companies because their current employer does not fulfill their expectations. \n</div>"},{"cell_type":"code","metadata":{"_uuid":"03b1b47c9a77d94e17b7515540ec61dedc172333","scrolled":false,"collapsed":true,"_cell_guid":"519b75a6-b4e6-4496-860f-6fa9d880d324"},"source":"clf = best_models[0]\ncoef = pd.DataFrame(index=X_train.columns)\ncoef[\"Coefficients\"] = clf.coef_[0]\ncoef.sort_values(\"Coefficients\", ascending=False)","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"a5d4d78d7d994530632d67597391a29bde9a0a44","_cell_guid":"6ac6da5c-6532-466f-ac58-3acb258cc9f5"},"source":"---\n## Conclusion\n<div align=\"justify\" style=\"width=95%\">\n\n\n</div>"},{"cell_type":"markdown","metadata":{"_uuid":"46032935d1b6d02144a057cc10c2c82c47ba4ead","_cell_guid":"759bf640-3869-4bca-ad70-19a23ca554e2"},"source":"<div align=\"justify\", style=\"width:95%;\">\nComing back to our starting questions, let's see if we have more elements to answer them:<br><br>\n<ul>\n    <li>**How can current and future employee attrition be minimized?**</li> \n</ul><br>\nFirst, we observed that employees working overtime are significantly more likely to resign. Our first recommendation is therefore to understand why they are working overtime. For example, if it turns out they have a too high workload, appropriate measures must be taken to reduce the workload.\n<br><br>\nSecond, it seems that once an employee has worked for a certain number of years in the company he will be less likely to resign. That's why it is important to make sure that new hires feel satisfied from the very beginning, especially because at this stage they are most likely to quit. \n<br><br>\nThird, the company must be careful not to lose too many employees with a techncial background, especially in the human resources department where the average attrition rate for these employees is 50%. The same holds for laboratory technicians who are far more likely to resign. Therefore it is recommended to further investigate why these employees have a higher attrition rate.\n<br><br>\nFourth, employees can be incentivized to stay by offering stock options. After all, the multinomial logistic regression coefficient show that stock options are negatively related to attrition. \n<br><br>\nFifth and last, the company should further look into the existence of a Brain Drain. For example, by surveying high performing employees.\n<ul><li>**What actionable insights can you give on the current composition of our workforce?**</li></ul>\nThe current composition of the workforce might benefit from some adjusments in terms of hiring policies. Although there aren't any significant income differences between men and women, there are remarkably fewer women in the company, especially for job level 1 and 5 positions and the human resources department.\n<br><br>\nFinally, in terms of what profile to hire, our last recommendation is to carefully match employee's educational background with the job role, since this can improve work performance as we saw before. \n\n<h4>Discussion and Validation</h4>\nAs indicated before the <a href=\"https://www.kaggle.com/ludobenistant/hr-analytics/data\">Human Resource Analytics</a> dataset will be compared with ours. In particular, we consider the findings by [Randy Lao](https://www.kaggle.com/randylaosat/predicting-employee-kernelover/notebook). \n<br/><br/>\nRandy concludes that both overworked (more than 10hr/day) and underworked employees (less than 6hr/day) generally left the company. The IBM dataset contains the feature `Overtime` of which positive values can be interpreted as overworked employees. Indeed, we also found that employees who work overtime are more likely to resign. In fact, it's our most predictive feature.\n<br/><br/>\nHe also states that employees with either really high or low evaluations should be taken into consideration for high turnover rate. We found a similar result for high performing employees. However, the feature `PerformanceMetric` requires attention since it is doubtful that all employees are really performming *\"Excellent\"* or *\"Outstanding\"*.\n<br/><br>\nSatisfaction was the biggest factor in determining turnover for him. Although, it was not the most significant predictor in our case, we also found a negative relationship between job satisfaction and attrition.\n<br/><br/>\nLastly, we also found contradictory results. Where Randy concluded that employees with 4 and 5 years experience at the company have the highest risk of leaving the company, we found that recent hires are most likely to quit.\n</div>"},{"cell_type":"markdown","metadata":{"_uuid":"fc8e95e74054e7e618d457c4017b6df649245a23","_cell_guid":"16e763dc-f93c-4ed2-b54f-198445b12324"},"source":"---"},{"cell_type":"markdown","metadata":{"_uuid":"356bd242412efd715b25884a481f1cdee721ffce","_cell_guid":"eecec287-e520-48bd-a1e4-c9c579d31a83"},"source":"## Authors"},{"cell_type":"markdown","metadata":{"_uuid":"82339d267a96338e1784ca51b9e30e6388afbfa9","_cell_guid":"76bf8564-24f1-434a-ae46-169cbab8901e"},"source":"<table align=\"left\">\n    <tr>\n        <th>Name</th>\n    </tr>\n    <tr>\n        <td>Roy Klaasse Bos</td>\n    </tr>\n    <tr>\n        <td>Quentin Meeus</td>\n    </tr>\n    <tr>\n        <td>Jan Seipp</td>\n    </tr>\n</table>"},{"cell_type":"markdown","metadata":{"_uuid":"1a8792023c7e1a390e2c9fb61f7d32a028ba4b26","_cell_guid":"333afb67-3c49-45cb-8cea-aae4ee927817"},"source":"## Appendix A"},{"cell_type":"markdown","metadata":{"_uuid":"763a11255cb9f340217e0b3689526ba5fa1ff257","_cell_guid":"be314140-64b2-460d-86f6-c980d5096beb"},"source":"**Categorical Features**\n\n| Feature | Categories | Definition / Comment | \n| :------- | :----------- | :-------- | \n| Attrition | {Yes, No} | Whether the employee is currently still working for the company. |\n| BusinessTravel | {Travel_Rarely, Travel_Frequently, Non-Travel} | How frequency the employee travels for work. |\n| Department | {R&D, Sales, HR} | The department the employee works for. An employee can only work for one department. |\n| Education | {1=Below College, 2=College, 3=Bachelor, 4=Master, 5=Doctor} | The highest level of education obtained by the employee. |\n| EducationField | {Life Sciences, Medical, Marketing, Technical Degree, Other} | The field of study for the most recently finished education program.| \n| EmployeeNumber | {1, 2, .., 2068} | The employee's unique identifier. |\n| Gender | {Male, Female} | The employee's gender (mutually exclusive). | \n| JobInvolvement | {1=Low, 2=Medium, 3=High, 4=Very High} | To what extent the employee is involved in his/her work. |\n| JobLevel | {1, 2, 3, 4, 5} | *Undefined* - (it is assumed a higher level implies a more senior position). | \n| JobRole | {SalesExecutive, Research Scientist, Laboratory Technician, Manufacturing Director, Healthcare Representative}} | The current position of the employee (mutually exclusive). |\n| JobSatisfaction | {1=Low, 2=Medium, 3=High, 4=Very High} | To what extent the employee is satisfied with his/her job. |\n| MaritalStatus | {Married, Single, Divorced} | The current relationship status of the employee (mutually exclusive). | \n| Over18 | {Yes} | Whether the employee's age is 18 or higher. Note all employees are older than 18 years old. |\n| OverTime | {Yes, No} | *Undefined* - It has been assumed \"Yes\" refers to an employee whose total working hours is regularly beyond normal working hours. |\n| PerformanceRating | {3=Excellent, 4=Outstanding} | At least to say remarkable that all evaluations are either \"excellent\" or \"outstanding\" (no \"low\" or \"good\" scores).  |\n| RelationshipSatisfaction | {1=Low, 2=Medium, 3=High, 4=Very High} | *Undefined* -  (probably the quality of relationships with colleagues and managers) | \n| StandardHours | {80} | *Undefined* - (note that all records in this column are equal to 80). |\n| StockOptionLevel | {0, 1} | *Undefined* - (probably whether the job offer includes stock options or not. |\n| WorkLifeBalance | {1=Bad, 2=Good, 3=Better, 4=Best} | To what extent the employee is able to combine work and personal life.  |\n\n\n<br>\n**Numerical Features**\n\n| Feature | Mean | Range | Definition / Comment | \n| :------- | :----------- | :----------- |  :----------- |\n| Age | 36.9 | 18-60 | The current age of the employee |\n| DailyRate | 802 | 102-1500 | *Undefined* | \n| DistanceFromHome | 9.19 | 1-29 | One-way distance from the employee's home to work. Kilometer has been assumed as the measurement unit.\n| HourlyRate | 65.9 | 30-100 | *Undefined* |\n| MonthlyIncome | 6.50K | 1.01K - 20.0K | The monthly earnings of the employee. Since the data originates from IBM the American US dollar has been assumed as the used currency. |\n| MonthlyRate | 1.4K | 2.09K - 27K | *Undefined* |\n| NumCompanies | 2.69 | 0-9 | The number of companies worked for previously (excluding the current company). |\n| PercentSalaryHike | 15.2 | 11-25 | *Undefined* - (definition is unclear, especially since employees who have never been promoted (JobLevel=1) still have a salary hike of  11% or higher). |\n| TotalWorkingYears | 11.3 | 0-40 | The total number of working years at the current company plus any previous experiences. | \n| TrainingTimesLastYear | 2.8 | 0-6 | The number of trainings the employee participated in last year. |\n| YearsAtCompany | 7.01 | 0-40 | The total number of working years at the current company. |\n| YearsInCurrentRole | 4.23 | 0-18 | The number of years the employee has worked in the current position. |\n| YearsSinceLastPromotion | 2.19 | 0-15 | The number of years since the last promotion. Note that an employee can be promoted without changing postions. |\n| YearsWithCurrManager | 4.12 | 0-17 | The number of years the employee has worked for his/her current manager. |"}],"nbformat_minor":1,"nbformat":4}