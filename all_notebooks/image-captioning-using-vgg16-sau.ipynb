{"cells":[{"metadata":{"trusted":true,"_uuid":"9c07a60aa33345953e1ddeb3f8bb19c5efad93fd"},"cell_type":"code","source":"import os\nimport time\nimport progressbar\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\nimport keras\nimport sys, time, os, warnings \nimport numpy as np\nimport pandas as pd \nfrom collections import Counter \nfrom keras.preprocessing.image import load_img\nfrom nltk.tokenize import word_tokenize\nwarnings.filterwarnings(\"ignore\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":true},"cell_type":"code","source":"## The location of the caption file\n#dir_Flickr_text = \"../input/flickr-image-dataset/flickr30k_images/flickr30k_images/results.csv\"\n#dir_Flickr_jpg = \"../input/flickr-image-dataset/flickr30k_images/flickr30k_images/flickr30k_images\"\ndir_Flickr_text = \"../input/flickr8k-sau/flickr8k-sau/Flickr_Data/Flickr_TextData/Flickr8k.token.txt\"\ndir_Flickr_jpg = \"../input/flickr8k-sau/flickr8k-sau/Flickr_Data/Images\"\n\njpgs = os.listdir(dir_Flickr_jpg)\nprint(\"The number of jpg flies in Flicker30k: {}\".format(len(jpgs)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dir_Flickr_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c0cf027a3a4944f4140a1c3d60e8e36e45e13ab"},"cell_type":"code","source":"file = open(dir_Flickr_text,'r')\ntext = file.read()\nfile.close()\n\n\ndatatxt = []\nfor line in text.split('\\n'):\n    col = line.split('\\t')\n    if len(col) == 1:\n        continue\n    w = col[0].split(\"#\")\n    datatxt.append(w + [col[1].lower()])\n\ndf_txt = pd.DataFrame(datatxt,columns=[\"filename\",\"index\",\"caption\"])\n\nuni_filenames = np.unique(df_txt.filename.values)\nprint(\"The number of unique file names : {}\".format(len(uni_filenames)))\nprint(\"The distribution of the number of captions for each image:\")\nCounter(Counter(df_txt.filename.values).values())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_txt.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(df_txt)\nprint(df_txt.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import load_img, img_to_array\n\nnpic = 5\nnpix = 224\ntarget_size = (npix,npix,3)\n\ncount = 1\nfig = plt.figure(figsize=(10,20))\nfor jpgfnm in uni_filenames[:npic]:\n    filename = dir_Flickr_jpg + '/' + jpgfnm\n    captions = list(df_txt[\"caption\"].loc[df_txt[\"filename\"]==jpgfnm].values)\n    image_load = load_img(filename, target_size=target_size)\n    \n    ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n    ax.imshow(image_load)\n    count += 1\n    \n    ax = fig.add_subplot(npic,2,count)\n    plt.axis('off')\n    ax.plot()\n    ax.set_xlim(0,1)\n    ax.set_ylim(0,len(captions))\n    for i, caption in enumerate(captions):\n        ax.text(0,i,caption,fontsize=20)\n    count += 1\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def df_word(df_txt):\n    vocabulary = []\n    for i in range(len(df_txt)):\n        temp=df_txt.iloc[i,2]\n        vocabulary.extend(temp.split())\n    print('Vocabulary Size: %d' % len(set(vocabulary)))\n    ct = Counter(vocabulary)\n    dfword = pd.DataFrame({\"word\":list(ct.keys()),\"count\":list(ct.values())})\n    dfword = dfword.sort_values(\"count\",ascending=False)\n    dfword = dfword.reset_index()[[\"word\",\"count\"]]\n    return(dfword)\ndfword = df_word(df_txt)\ndfword.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(dfword)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfsub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 50 most occuring word \ntopn=50 \n\ndef plthist(dfsub,title='50 most occuring words'):\n    plt.figure(figsize=(20,3))\n    plt.bar(dfsub.index,dfsub['count'])\n    plt.yticks(fontsize=20)\n    plt.xticks(dfsub.index,dfsub['word'],rotation=90,fontsize=20)\n    plt.title(title,fontsize=20)\n    plt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plthist(dfword.iloc[:topn,:],title=\"50 most occuring word\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plthist(dfword.iloc[-topn:,:],title=\"50 least occuring word\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string \ndef remove_punctutation(text_original) :\n    text_no_punctuation=text_original.translate(str.maketrans('','',string.punctuation))\n    return (text_no_punctuation)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lty='saurabh ! @#$ dksd;gs '\nremove_punctutation(text_original=lty)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_single_char(text):\n    text_len_more_1=\" \"\n    for word in text.split():\n        if len(word) > 1:\n            text_len_more_1+=\"  \"+word\n    return(text_len_more_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t='a'\nremove_single_char(text=t)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_numeric(text ,printTF=False):\n    text_no_numeric= ''\n    for word in text.split():\n        isalpha=word.isalpha()\n        if printTF:\n            print(\"    {:10} : {:}\".format(word,isalpha))\n        if isalpha:\n            text_no_numeric += ' '+word\n    return(text_no_numeric)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lko='asklsff45'\ntkr='alphaaa'\nprint(remove_numeric(lko))\nprint(remove_numeric(tkr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_clean(text_original):\n    text = remove_punctutation(text_original)\n    text = remove_single_char(text)\n    text = remove_numeric(text)\n    return(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with progressbar.ProgressBar(max_value=len(df_txt.caption.values)) as bar:\n    for i, caption in enumerate(df_txt.caption.values):\n        newcaption = text_clean(caption)\n        df_txt[\"caption\"].iloc[i] = newcaption\n        bar.update(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bar","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from copy import copy\ndef add_start_end_seq_token(captions):\n    caps = []\n    for txt in captions:\n        txt = 'startseq ' + txt + ' endseq'\n        caps.append(txt)\n    return(caps)\ndf_txt0 = copy(df_txt)\ndf_txt0[\"caption\"] = add_start_end_seq_token(df_txt[\"caption\"])\ndf_txt0.head(5)\ndel df_txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_txt0.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications import VGG16\n\nmodelvgg=VGG16(include_top=True ,weights=None)\nmodelvgg.load_weights(\"../input/vgg16-weights-image-captioning/vgg16_weights_tf_dim_ordering_tf_kernels.h5\")\nmodelvgg.summary()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import models\nmodelvgg.layers.pop()\nmodelvgg = models.Model(inputs=modelvgg.inputs, outputs=modelvgg.layers[-1].output)\n## show the deep learning model\nmodelvgg.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import load_img, img_to_array\nfrom keras.applications.vgg16 import preprocess_input\nfrom collections import OrderedDict\n\nimages = OrderedDict()\nnpix = 224\ntarget_size = (npix,npix,3)\nwith progressbar.ProgressBar(max_value=len(jpgs)) as bar:\n    for i,name in enumerate(jpgs):\n        # load an image from file\n        filename = dir_Flickr_jpg + '/' + name\n        image = load_img(filename, target_size=target_size)\n        # convert the image pixels to a numpy array\n        image = img_to_array(image)\n        nimage = preprocess_input(image)\n        y_pred = modelvgg.predict(nimage.reshape( (1,) + nimage.shape[:3]))\n        images[name] = y_pred.flatten()\n        bar.update(i)\n    #print(i,filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dimage, keepindex =[],[]\nnd=(df_txt_0['index'].values)\nb=[(int(i)==0) for i in nd]\ndf_txt0 = df_txt0.loc[df_txt0[\"index\"].values == \"0\",: ]\n\nfor i, fnm in enumerate(df_txt0.filename):\n    if fnm in images.keys():\n        dimages.append(images[fnm])\n        keepindex.append(i)\n        \nfnames = df_txt0[\"filename\"].iloc[keepindex].values\ndcaptions = df_txt0[\"caption\"].iloc[keepindex].values\ndimages = np.array(dimages)\nprint(df_txt0[\"index\"][:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n## the maximum number of words in dictionary\ncount_words=22000\n#nb_words = 31782\ntokenizer = Tokenizer(num_words=8000)\ntokenizer.fit_on_texts(dcaptions)\nvocab_size = len(tokenizer.word_index) + 1\nprint(\"vocabulary size : {}\".format(vocab_size))\ndtexts = tokenizer.texts_to_sequences(dcaptions)\nprint(dtexts[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prop_test, prop_val = 0.2, 0.2 \n\nN = len(dtexts)\nNtest, Nval = int(N*prop_test), int(N*prop_val)\n\ndef split_test_val_train(dtexts,Ntest,Nval):\n    return(dtexts[:Ntest], \n           dtexts[Ntest:Ntest+Nval],  \n           dtexts[Ntest+Nval:])\n\ndt_test,  dt_val, dt_train   = split_test_val_train(dtexts,Ntest,Nval)\ndi_test,  di_val, di_train   = split_test_val_train(dimages,Ntest,Nval)\nfnm_test,fnm_val, fnm_train  = split_test_val_train(fnames,Ntest,Nval)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maxlen = np.max([len(text) for text in dtexts])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\n\ndef preprocessing(dtexts,dimages):\n    N = len(dtexts)\n    print(\"# captions/images = {}\".format(N))\n\n    assert(N==len(dimages))\n    Xtext, Ximage, ytext = [],[],[]\n    for text,image in zip(dtexts,dimages):\n\n        for i in range(1,len(text)):\n            in_text, out_text = text[:i], text[i]\n            in_text = pad_sequences([in_text],maxlen=maxlen).flatten()\n            out_text = to_categorical(out_text,num_classes = vocab_size)\n\n            Xtext.append(in_text)\n            Ximage.append(image)\n            ytext.append(out_text)\n\n    Xtext  = np.array(Xtext)\n    Ximage = np.array(Ximage)\n    ytext  = np.array(ytext)\n    print(\" {} {} {}\".format(Xtext.shape,Ximage.shape,ytext.shape))\n    return(Xtext,Ximage,ytext)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtext_train, Ximage_train, ytext_train = preprocessing(dt_train,di_train)\nXtext_val,   Ximage_val,   ytext_val   = preprocessing(dt_val,di_val)\n# pre-processing is not necessary for testing data\n#Xtext_test,  Ximage_test,  ytext_test  = preprocessing(dt_test,di_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import layers\nprint(vocab_size)\n## image feature\n\ndim_embedding = 64\n\ninput_image = layers.Input(shape=(Ximage_train.shape[1],))\nfimage = layers.Dense(256,activation='relu',name=\"ImageFeature\")(input_image)\n## sequence model\ninput_txt = layers.Input(shape=(maxlen,))\nftxt = layers.Embedding(vocab_size,dim_embedding, mask_zero=True)(input_txt)\nftxt = layers.LSTM(256,name=\"CaptionFeature\")(ftxt)\n## combined model for decoder\ndecoder = layers.add([ftxt,fimage])\ndecoder = layers.Dense(256,activation='relu')(decoder)\noutput = layers.Dense(vocab_size,activation='softmax')(decoder)\nmodel = models.Model(inputs=[input_image, input_txt],outputs=output)\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Ximage_train.shape,Xtext_train.shape,ytext_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for label in [\"loss\",\"val_loss\"]:\n    plt.plot(hist.history[label],label=label)\nplt.legend()\nplt.xlabel(\"epochs\")\nplt.ylabel(\"loss\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index_word = dict([(index,word) for word, index in tokenizer.word_index.items()])\ndef predict_caption(image):\n    '''\n    image.shape = (1,4462)\n    '''\n\n    in_text = 'startseq'\n\n    for iword in range(maxlen):\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        sequence = pad_sequences([sequence],maxlen)\n        yhat = model.predict([image,sequence],verbose=0)\n        yhat = np.argmax(yhat)\n        newword = index_word[yhat]\n        in_text += \" \" + newword\n        if newword == \"endseq\":\n            break\n    return(in_text)\n\n\n\nnpic = 5\nnpix = 224\ntarget_size = (npix,npix,3)\n\ncount = 1\nfig = plt.figure(figsize=(10,20))\nfor jpgfnm, image_feature in zip(fnm_test[:npic],di_test[:npic]):\n    ## images \n    filename = dir_Flickr_jpg + '/' + jpgfnm\n    image_load = load_img(filename, target_size=target_size)\n    ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n    ax.imshow(image_load)\n    count += 1\n\n    ## captions\n    caption = predict_caption(image_feature.reshape(1,len(image_feature)))\n    ax = fig.add_subplot(npic,2,count)\n    plt.axis('off')\n    ax.plot()\n    ax.set_xlim(0,1)\n    ax.set_ylim(0,1)\n    ax.text(0,0.5,caption,fontsize=20)\n    count += 1\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu\nindex_word = dict([(index,word) for word, index in tokenizer.word_index.items()])\n\n\nnkeep = 5\npred_good, pred_bad, bleus = [], [], [] \ncount = 0 \nfor jpgfnm, image_feature, tokenized_text in zip(fnm_test,di_test,dt_test):\n    count += 1\n    if count % 200 == 0:\n        print(\"  {:4.2f}% is done..\".format(100*count/float(len(fnm_test))))\n    \n    caption_true = [ index_word[i] for i in tokenized_text ]     \n    caption_true = caption_true[1:-1] ## remove startreg, and endreg\n    ## captions\n    caption = predict_caption(image_feature.reshape(1,len(image_feature)))\n    caption = caption.split()\n    caption = caption[1:-1]## remove startreg, and endreg\n    \n    bleu = sentence_bleu([caption_true],caption)\n    bleus.append(bleu)\n    if bleu > 0.7 and len(pred_good) < nkeep:\n        pred_good.append((bleu,jpgfnm,caption_true,caption))\n    elif bleu < 0.3 and len(pred_bad) < nkeep:\n        pred_bad.append((bleu,jpgfnm,caption_true,caption))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_images(pred_bad):\n    def create_str(caption_true):\n        strue = \"\"\n        for s in caption_true:\n            strue += \" \" + s\n        return(strue)\n    npix = 224\n    target_size = (npix,npix,3)    \n    count = 1\n    fig = plt.figure(figsize=(10,20))\n    npic = len(pred_bad)\n    for pb in pred_bad:\n        bleu,jpgfnm,caption_true,caption = pb\n        ## images \n        filename = dir_Flickr_jpg + '/' + jpgfnm\n        image_load = load_img(filename, target_size=target_size)\n        ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n        ax.imshow(image_load)\n        count += 1\n\n        caption_true = create_str(caption_true)\n        caption = create_str(caption)\n        \n        ax = fig.add_subplot(npic,2,count)\n        plt.axis('off')\n        ax.plot()\n        ax.set_xlim(0,1)\n        ax.set_ylim(0,1)\n        ax.text(0,0.7,\"true:\" + caption_true,fontsize=20)\n        ax.text(0,0.4,\"pred:\" + caption,fontsize=20)\n        ax.text(0,0.1,\"BLEU: {}\".format(bleu),fontsize=20)\n        count += 1\n    plt.show()\n\nprint(\"Bad Caption\")\nplot_images(pred_bad)\nprint(\"Good Caption\")\nplot_images(pred_good)\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}