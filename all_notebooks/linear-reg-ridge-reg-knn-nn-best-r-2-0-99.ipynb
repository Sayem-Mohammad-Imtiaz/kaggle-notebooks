{"cells":[{"metadata":{"_uuid":"be59482c850c14bfa837d11277b092f83494ed95"},"cell_type":"markdown","source":"In this notebook, we try to solve the regression problem of atomic ground state energies using linear and non-linear methods.  These methods are trained without implementing any feature selection, as the problem of minimizing R^2 seems to be more important than interpretability. \n\nContents: 1. Data Preparation 2. Linear Models 3. Non-linear Models 4. Final Model"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca9a6ad3238557dab45a3eb7f9a80e92844e92a5"},"cell_type":"markdown","source":"**1. Preparing Data**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"data =  pd.read_csv('../input/roboBohr.csv')\ntrain_x = data.drop(['Unnamed: 0', 'pubchem_id', 'Eat'], axis = 1)\ntrain_y = data['Eat']\ndata.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01e389df33439268ce0df2d06bca6609f84b6fee","collapsed":true},"cell_type":"code","source":"# Changes the mean and standard deviation of features to 0 and 1 respectively.\ndef standardize(df):\n    for column in range(0, df.shape[1]):\n        df[str(column)] = (df[str(column)] - np.mean(df[str(column)]))/np.std(df[str(column)]) \n        \nstandardize(train_x)        ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be8a5d11d78b1287a527da34a64ddfb7741a6242"},"cell_type":"markdown","source":"Using the following code, we split train_x into the actual training set (train_x_1)  and validation set (val_x). This step is important because we cannot judge the superiority of two models of different complexities by their \nscores on the training set as the more complex model almost always out-performs the less complex model -- irrespective of their performances out-of-sample."},{"metadata":{"trusted":true,"_uuid":"1d194774f3811dac9f63f5b8797cbd244cd8c9c2","collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_x_1, val_x, train_y_1, val_y = train_test_split(train_x, train_y, test_size=0.3)\nprint(train_x_1.shape, val_x.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a923a6f9a750c17da06dc7a0d6b8ac33bac7856"},"cell_type":"markdown","source":"**2. Linear Models**"},{"metadata":{"trusted":true,"_uuid":"0bf20aa201da31fe39ac2b3c2f308d15c22ebc80","collapsed":true},"cell_type":"code","source":"# Linear Regression\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression(fit_intercept=True)\nlr.fit(train_x_1, train_y_1)\nprint(lr.score(train_x_1, train_y_1), lr.score(val_x, val_y))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f9636e1075d8223e3c0fa7dada47c74155a48a2"},"cell_type":"markdown","source":"From the above output, it is clear that the generalization for the linear model is \nhorrible, despite managing quite a good training score. It is a bit surprising that a **linear** model has overfit so dramatically -- must be a high-dimension quirk. Let us try using regularization on the linear model. **One obvious thing is that the regularized model will not result in an R^2 better than 0.97 (in and out of sample) as, by definition, the regularized model has more bias than the linear one. **"},{"metadata":{"trusted":true,"_uuid":"6005f97f34e6cebdef9a33a0e74702dd5a1cf893","collapsed":true},"cell_type":"code","source":"# Ridge Regression i.e., linear regression with L2 regularization.\nfrom sklearn.linear_model import Ridge\nridge_models = {}\nfor i in [0.001, 0.1, 0.5, 1, 5, 10, 20, 30, 50, 60, 70, 100, 110, 120, 130, 140, 150, 200]:\n    ridge_models[str(i)] = Ridge(alpha=i)\n    ridge_models[str(i)].fit(train_x_1, train_y_1)\n    print(i , ridge_models[str(i)].score(train_x_1, train_y_1), ridge_models[str(i)].score(val_x, val_y))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70daf01973d22c21503941e0104629c7d05ff399"},"cell_type":"markdown","source":"**3. Non-Linear Models**\n\nIf we are looking to obtain a score better than 0.97, we ought to try out non-linear models. We could have used polynomial regression with regularization but because there are so many parameters to start with, it is not quite sensible. Let us try nearest neigbors and neural networks. \n"},{"metadata":{"trusted":true,"_uuid":"7b65eee9c94a0571964e1571c0956e73c578d440","collapsed":true},"cell_type":"code","source":"# Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsRegressor\nknn_models = {}\nfor i in range(1, 10):\n    knn_models[str(i)] = KNeighborsRegressor(n_neighbors=i);\n    knn_models[str(i)].fit(train_x_1, train_y_1);\n    score = knn_models[str(i)].score(val_x, val_y)\n    print(\"Validation score for n_neighbors = \" + str(i) + \" is \" + str(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3be241d75e85823b7f42cf064370f0c014f94179","collapsed":true},"cell_type":"code","source":"# Neural Networks\nfrom sklearn.neural_network import MLPRegressor\nnn_models = {}\nfor i in range(4,9):\n    for j in range(1,6):\n        nn_models[str((i,j))] = MLPRegressor((i,j), activation='relu', learning_rate='adaptive')\n        nn_models[str((i,j))].fit(train_x_1, train_y_1)\n        print((i,j), nn_models[str((i,j))].score(train_x_1, train_y_1), nn_models[str((i,j))].score(val_x, val_y))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e4ec8009772ef9d648c940ac4e1101f195cbf6a"},"cell_type":"markdown","source":"**4. Final Model**\n\nBest model seems be the nearest neighbors model with n_neighbors = 3, followed closely by the neural network model. We can thus train this model on the full training data i.e., without the validation split, and have it ready for future use."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8311ec7c35e163dc743b889c59a6c96304756b8f"},"cell_type":"code","source":"final_model = KNeighborsRegressor(n_neighbors=3)\nfinal_model.fit(train_x, train_y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9e70dda98dad9a50081640fdde30a90539beab6"},"cell_type":"markdown","source":"**Any comments appreciated!**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}