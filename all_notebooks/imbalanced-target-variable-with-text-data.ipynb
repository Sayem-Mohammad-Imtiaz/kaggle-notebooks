{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Imblanced Target Variable with Text Data\n\nIn this notebook I will show 4 different techniques for handling imblanced target variable \n1. Oversampling the minority class using imblearn\n2. Undersampling the majority class using imblearn\n3. Using the `class_weight` parameter in a sklearn model\n4. Data Augmentation - by translating the text into another language and then translating it back "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# load in the data \n\nimport pandas as pd \nimport numpy as np\n\ndf = pd.read_csv('/kaggle/input/spam-text-message-classification/SPAM text message 20170820 - Data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Category'].value_counts(normalize = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"87% of my data is of class ham and 13% is of class spam \n\nFor this notebook, I am going to be focusing on different techniques for handling imbalanced classes.  For this reason I am going to be using TF-IDF and a Random Forest Classifier for all of the different techniques. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# convert all text to lowercase \ndf['Message'] = df['Message'].str.lower()\n\n# perform train test split \nX_train, X_test, y_train, y_test = train_test_split(df['Message'], df['Category'], random_state=11)\n\n# vectorize text using TFIDF\ntfidf = TfidfVectorizer()\nX_train_tfidf = tfidf.fit_transform(X_train)\nX_test_tfidf = tfidf.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To begin I am starting with a random forest model where I do not do anything to the classes even though they are imbalanced\n\n\n### Baseline Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\n\nrf = RandomForestClassifier(random_state = 11)\nrf.fit(X_train_tfidf, y_train)\nprint(classification_report(y_test, rf.predict(X_test_tfidf)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I see that I get a relatively low recall on the minority class `spam` of 0.85\n\n### Random Over Sampling\n\nNext I am going to try random over sampling "},{"metadata":{"trusted":true},"cell_type":"code","source":"# check distribution before applying over sampling \n\ndf['Category'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler\n\nros = RandomOverSampler()\nX_ros, y_ros = ros.fit_resample(X_train_tfidf, y_train)\n\n# check distribution after applying over sampling \ny_ros.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Applying the same model with the over sampled data "},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier()\nrf.fit(X_ros, y_ros)\nprint(classification_report(y_test, rf.predict(X_test_tfidf)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We get very similar results as the baseline classifier \n\n### Random Under Sampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.under_sampling import RandomUnderSampler\n\nrus = RandomUnderSampler()\nX_rus, y_rus = rus.fit_resample(X_train_tfidf, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check distribution after random under sampling \ny_rus.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier()\nrf.fit(X_rus, y_rus)\nprint(classification_report(y_test, rf.predict(X_test_tfidf)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This time we see that the precision score went down a bit for the minority class, but the recall increased.  F1-Score increased from 0.90 (baseline) to 0.92.\n\n### Class Weight "},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(class_weight = 'balanced')\nrf.fit(X_train_tfidf, y_train)\nprint(classification_report(y_test, rf.predict(X_test_tfidf)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"F1-Score for the minority class went down to 0.88"},{"metadata":{},"cell_type":"markdown","source":"### Data Augmentation \n\nNow we will try translating the Spam Messages to another language and then translate them back to English.  The idea is that we will add a little noise by performing a translation.\n\nAn example of this can be seen below "},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install googletrans==3.1.0a0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets see an example of this for a single message.  I am going to take a message, translate it to French, and then translate it back to English "},{"metadata":{"trusted":true},"cell_type":"code","source":"from googletrans import Translator\n\ntranslator = Translator()\n\n# translate to French\nfrench = translator.translate(df.loc[2, 'Message'], dest = 'fr')\n# translate back to English\ntranslator.translate(french.text, dest = 'en').text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# original message \ndf.loc[2, 'Message']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the orginal message is slightly different than the translated message.  This allows me to add new data to the dataset that is slighly different than the original messages. \n\nNow I'm going to do this for all of the Spam messages in the training set "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.concat([X_train, y_train], axis = 1)\ndf_train.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am going to take each Spam message and then randomly translate that message to either French, Spanish, or German, then will translate that back to English"},{"metadata":{"trusted":true},"cell_type":"code","source":"import time \ntranslated_text = []\n\nfor message in df_train[df_train['Category'] == 'spam']['Message']:\n    language = np.random.choice(['fr', 'es', 'de'])\n    translated_message = translator.translate(message, dest = language)\n    translated_text.append(translator.translate(translated_message.text, dest = 'en').text)\n    time.sleep(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Combine the translated and non-translated messages to one dataframe "},{"metadata":{"trusted":true},"cell_type":"code","source":"translations_df = pd.DataFrame({'Message': translated_text,'Category': 'spam'})\ndf_train_translations = pd.concat([df_train, translations_df])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_translations['Category'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you remember from earlier, we originally had 570 spam messages and we now have 1,140 spam messages after the data augmentation. \n\nPerform same TF-IDF that I did earlier "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_translations['Message'] = df_train_translations['Message'].str.lower()\n\n# perform TFIDF \nX_train_trans_tfidf = tfidf.transform(df_train_translations['Message'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use the randomforest classifier on the translated data "},{"metadata":{"trusted":true},"cell_type":"code","source":"rf.fit(X_train_trans_tfidf, df_train_translations['Category'])\nprint(classification_report(y_test, rf.predict(X_test_tfidf)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that I get a F1 score on the minority class `spam` of 0.90.\n\n## Conclusions \nWe have tried 3 different techniques for handling the unbalanced class.  Next steps, trying out more data augmentation because even after doubling the number of `spam` messages there were still a lot less `spam` messages than `ham` messages with the data augmentation technique.  "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}