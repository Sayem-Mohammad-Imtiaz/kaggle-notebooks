{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\nHello people, welcome to my kernel! Nowadays I have learnt what is Logistic Regression , and in this kernel I am going to do an exercise with Rain Prediction Data.\nIn my first Logistic Regression Kernel, I've created my own Logistic Regression function.\n\n(You can take a look at my first logical regression kernel.\n\nhttps://www.kaggle.com/mehmetlaudatekman/death-prediction-using-heart-failure-data)\n\nBut for this time,I am going to use SKLearn library because SKLearn is:\n* more efficient\n* more easy \n* and more confortable\n\nLet's take a look at the schedule\n\n# Schedule\n1. Importing Libraries and Data\n    * Importing Libraries\n    * Importing Data\n1. Examination of Data\n1. Cleaning Data\n1. Logistic Regression using SKLearn\n1. Road Map of Creating Logistic Regression Models using SKLearn\n1. Conclusion\n"},{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries and Data\n\n## Importing Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.linear_model import LogisticRegression # SKLearn\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## Importing Data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/weather-dataset-rattle-package/weatherAUS.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Examination of Data\nIn this section I am going to take a look at data, because before the cleaning we need to have an idea about the data.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cleaning Data\n\nYou know, before the machine learning we have to clean data because NaN values cause problems and non-normalized numerical values disrupt our Logistic Regression model. Let's start with dropping unnecesary features."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop([\"Date\",\"Location\",\"WindGustDir\",\"WindDir9am\",\"WindDir3pm\",\"RainTomorrow\",\"RISK_MM\"],axis=1,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, I am going to fill NaN values with his feature's median value."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.fillna(data.median(),inplace=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And now I am going to convert Rain Today feature yo to binary values ( 0 or 1 ). In order to convert this feature to binary I am going to use list comprehension."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.RainToday = [1 if i == 'Yes' else 0 for i in data.RainToday]\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And now our final step is normalization. Let's normalize our data but before the normalization we need to create x axis variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"x = data.drop(\"RainToday\",axis=1).values\nx = (x - np.min(x)) / (np.max(x) - np.min(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression using SKLearn Library\n\nAnd, finally we came our main section. In this section I am going to create a Logistic Regression model using SKLearn Library. Let's start with creating our train and test split."},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data.RainToday.values # We've created our x axis so we only need to create y axis\n\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=75)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our splits are ready. Let's take a look at our splits' shapes"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"\"\"\nx_train's shape is {x_train.shape} \nx_test's shape is {x_test.shape}\ny_train's shape is {y_train.shape}\ny_test's shape is  {y_test.shape}\n\"\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = y_train.reshape(-1,1)\ny_test = y_test.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"\"\"\nx_train's shape is {x_train.shape} \nx_test's shape is {x_test.shape}\ny_train's shape is {y_train.shape}\ny_test's shape is  {y_test.shape}\n\"\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And now we are ready to create our Logistic Regression model Let's create him\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings as wrn\nwrn.filterwarnings('ignore')\n\nlr_model = LogisticRegression()\nlr_model.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our logistic regression model is ready. Now let's learn our models score"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Our model's score is \",lr_model.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Road Map of Creating Logistic Regression Models\n\n1. Import the libraries that you will need. \n1. Import the data you will use\n1. Drop the columns that you will not use\n1. Fill NaN values if they exist\n1. If your y axis is not binary convert it to binary .\n1. Normalize your data set \n1. Create train and test split using SKLearn\n1. Create Logistic Regression model using SKLearn\n1. Fit your model using x_train and y_train\n\nYour model is ready!"},{"metadata":{},"cell_type":"markdown","source":"# Conclusion \n\nI know, this kernel is small, but I've wanted this kernell just contains how to create a logistic regression model using SKLearn, however if you want to learn the roots of logistic regression, you can take a look at this kernel.\n\nhttps://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners\n\nI am waiting for your advices, comments and upvotes. Thanks!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}