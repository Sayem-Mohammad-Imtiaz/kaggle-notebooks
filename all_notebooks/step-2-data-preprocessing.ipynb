{"cells":[{"metadata":{"id":"zVIBmsHgH3Lx"},"cell_type":"markdown","source":"# Abbreviation Disambiguation in Medical Texts - Data Preprocessing\n\nThis Notebook is in continuation of the notebook- 'Step 1- Data Wrangling and EDA' and lists down:\n\n1. Data Preprocessing done on the dataset.\n2. Exploring the Preprocessed data."},{"metadata":{"id":"Ij7bP-lvH3L7"},"cell_type":"markdown","source":"## Step# 1: Load the datasets"},{"metadata":{"executionInfo":{"elapsed":1397,"status":"ok","timestamp":1613043841120,"user":{"displayName":"Subhanshu Gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnZLmL-ndgRhZ3nkcyPMhv34RcC87fv8JUjL3sxg=s64","userId":"09099113451266627622"},"user_tz":-330},"id":"iX9lde82H3L8","trusted":false},"cell_type":"code","source":"# Lets download the spacy library\n#!pip install spacy","execution_count":null,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":4388,"status":"ok","timestamp":1613043844121,"user":{"displayName":"Subhanshu Gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnZLmL-ndgRhZ3nkcyPMhv34RcC87fv8JUjL3sxg=s64","userId":"09099113451266627622"},"user_tz":-330},"id":"LKsEQIRXH3MA","outputId":"5bacb65b-19b7-45a4-ca2a-109406f2f99a","trusted":false},"cell_type":"code","source":"#Importing the Required Python Packages\nimport os\nimport shutil\nimport string\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport spacy\nfrom sklearn.feature_extraction.text import TfidfVectorizer\npd.set_option('display.max_colwidth', -1)","execution_count":null,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":5019,"status":"ok","timestamp":1613043844758,"user":{"displayName":"Subhanshu Gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnZLmL-ndgRhZ3nkcyPMhv34RcC87fv8JUjL3sxg=s64","userId":"09099113451266627622"},"user_tz":-330},"id":"Qn8Ru2E4H3MB","trusted":false},"cell_type":"code","source":"# Lets load the default english model of spacy\nnlp = spacy.load('en_core_web_sm')","execution_count":null,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":16997,"status":"ok","timestamp":1613043856743,"user":{"displayName":"Subhanshu Gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnZLmL-ndgRhZ3nkcyPMhv34RcC87fv8JUjL3sxg=s64","userId":"09099113451266627622"},"user_tz":-330},"id":"1DNau6gbH3MC","trusted":false},"cell_type":"code","source":"# Lets load the train dataset.\ntrain = pd.read_csv('Data/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":16991,"status":"ok","timestamp":1613043856748,"user":{"displayName":"Subhanshu Gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnZLmL-ndgRhZ3nkcyPMhv34RcC87fv8JUjL3sxg=s64","userId":"09099113451266627622"},"user_tz":-330},"id":"6uLZqYbWH3MD","outputId":"c5803ba7-911b-45d4-cbd9-946d59cb3e3e","trusted":false},"cell_type":"code","source":"#Lets check the dataset\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"GPbqapZDH3ME"},"cell_type":"markdown","source":"## Step# 2a: Create a new feature 'ABV'"},{"metadata":{"executionInfo":{"elapsed":16981,"status":"ok","timestamp":1613043856749,"user":{"displayName":"Subhanshu Gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnZLmL-ndgRhZ3nkcyPMhv34RcC87fv8JUjL3sxg=s64","userId":"09099113451266627622"},"user_tz":-330},"id":"I4C4aQkOH3MF","trusted":false},"cell_type":"code","source":"# Lets create a function to create a new feature 'ABV' from dataset\ndef createFeature(df):    \n    return [x.split(' ')[y] for x,y in zip(df['TEXT'], df['LOCATION'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['ABV'] = createFeature(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets check which Abbreviations occur the most and lets take top 20 such Abbreviations for further processing due to Hardware limitations"},{"metadata":{"trusted":false},"cell_type":"code","source":"grouped = train.groupby(by=['ABV', 'LABEL'], as_index = False, sort = False).count()\ngrouped = grouped.sort_values(by='TEXT', ascending = False)\ngrouped","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Lets take top 20 Abbreviations for further processing\ntopAbv = grouped['ABV'][:20]\ntopAbv","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets extract only the above 20 Abbreviations from train set for futher processing."},{"metadata":{"trusted":false},"cell_type":"code","source":"train = train[train['ABV'].isin(topAbv)]\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"rNtNzxC-H3MD"},"cell_type":"markdown","source":"## Step# 2: Data Preprocessing: Following preprocessing steps will be performed on the Dataset:\n\n    a. Create a new feature 'ABV' for abbreviation directly deriving it from Location and Text columns.-- Already done above\n    b. Convert text to lowercase.\n    c. Remove Punctuations from the Text Column.\n    d. Tokenize the Text column.\n    e. Dropping Abstract_id, Location and Text columns.\n    f. Remove stop words from the Text Column.\n\nSo, lets start with the Data Preprocessing sub-steps."},{"metadata":{"id":"x7co5s48H3MF"},"cell_type":"markdown","source":"## Step# 2b: Convert the data to lowercase"},{"metadata":{"executionInfo":{"elapsed":16976,"status":"ok","timestamp":1613043856750,"user":{"displayName":"Subhanshu Gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnZLmL-ndgRhZ3nkcyPMhv34RcC87fv8JUjL3sxg=s64","userId":"09099113451266627622"},"user_tz":-330},"id":"eAeamEueH3MG","trusted":false},"cell_type":"code","source":"#Lets create a function to convert all the text in lowercase\ndef tolower(df):\n    return [t.lower() for t in df['TEXT']]","execution_count":null,"outputs":[]},{"metadata":{"id":"mtmJI4aTH3MG"},"cell_type":"markdown","source":"## Step# 2c: Remove Punctuations"},{"metadata":{"executionInfo":{"elapsed":16968,"status":"ok","timestamp":1613043856751,"user":{"displayName":"Subhanshu Gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnZLmL-ndgRhZ3nkcyPMhv34RcC87fv8JUjL3sxg=s64","userId":"09099113451266627622"},"user_tz":-330},"id":"jNA7-O1rH3MH","trusted":false},"cell_type":"code","source":"# Lets create a function to remove all the Punctuations from Text\ndef removePunctuation(df):\n    return [t.translate(str.maketrans('','',string.punctuation)) for t in df['TEXT']]","execution_count":null,"outputs":[]},{"metadata":{"id":"V9cdOIP_H3MH"},"cell_type":"markdown","source":"## Step# 2d: Tokenize the text column and save the tokenized data in a new column 'TOKEN'."},{"metadata":{"executionInfo":{"elapsed":16963,"status":"ok","timestamp":1613043856752,"user":{"displayName":"Subhanshu Gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnZLmL-ndgRhZ3nkcyPMhv34RcC87fv8JUjL3sxg=s64","userId":"09099113451266627622"},"user_tz":-330},"id":"WNGY6w_NH3MH","trusted":false},"cell_type":"code","source":"# Lets create a function to Tokenize the Text column of dataset\ndef createTokens(df):\n    return df['TEXT'].apply(lambda x: x.split(' '))","execution_count":null,"outputs":[]},{"metadata":{"id":"xF4x6k2FH3MI"},"cell_type":"markdown","source":"## Step# 2e: Dropping the columns not needed"},{"metadata":{"executionInfo":{"elapsed":16958,"status":"ok","timestamp":1613043856753,"user":{"displayName":"Subhanshu Gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnZLmL-ndgRhZ3nkcyPMhv34RcC87fv8JUjL3sxg=s64","userId":"09099113451266627622"},"user_tz":-330},"id":"VlxE3bB7H3MI","trusted":false},"cell_type":"code","source":"#Lets create a function to drop \"Abstract_id\", \"Location\" and \"TEXT\" columns from dataset\ndef dropCols(df):\n    return df.drop(columns=['ABSTRACT_ID', 'LOCATION', 'TEXT'])","execution_count":null,"outputs":[]},{"metadata":{"id":"RkoGF9A-H3MJ"},"cell_type":"markdown","source":"## Step 2f: Remove Stop words"},{"metadata":{"executionInfo":{"elapsed":16952,"status":"ok","timestamp":1613043856754,"user":{"displayName":"Subhanshu Gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnZLmL-ndgRhZ3nkcyPMhv34RcC87fv8JUjL3sxg=s64","userId":"09099113451266627622"},"user_tz":-330},"id":"fgV5otL3H3MJ","trusted":false},"cell_type":"code","source":"# Lets create a function to remove stop words from the Text column\ndef removeStop(df):\n    stopWords = spacy.lang.en.stop_words.STOP_WORDS\n    # Remove any stopwords which appear to be an Abbreviation\n    [stopWords.remove(t) for t in df['ABV'].str.lower() if t in stopWords]\n    return df['TOKEN'].apply(lambda x: [item for item in x if not item in stopWords])","execution_count":null,"outputs":[]},{"metadata":{"id":"cXUUjrRfH3ML"},"cell_type":"markdown","source":"## Lets Create a function to apply all the above preprocessing steps to the dataset"},{"metadata":{"executionInfo":{"elapsed":16941,"status":"ok","timestamp":1613043856756,"user":{"displayName":"Subhanshu Gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnZLmL-ndgRhZ3nkcyPMhv34RcC87fv8JUjL3sxg=s64","userId":"09099113451266627622"},"user_tz":-330},"id":"QAL00FYbH3ML","trusted":false},"cell_type":"code","source":"def preProcessData(df):   \n    df['TEXT'] = tolower(df)\n    df['TEXT'] = removePunctuation(df)\n    df['TOKEN'] = createTokens(df)\n    df = dropCols(df)\n    df['TOKEN'] = removeStop(df)\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Apply the above preProcessData function to train set"},{"metadata":{"trusted":false},"cell_type":"code","source":"preProcessData(train).to_csv('Train/train_final.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets load the Validation and Test set for Preprocessing."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Lets load the Valid and test datasets as well.\nvalid = pd.read_csv('Data/valid.csv')\ntest = pd.read_csv('Data/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"valid.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's only use the Abbreviations for which we are training the model"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create ABV feature for valid and test sets.\nvalid['ABV'] = createFeature(valid)\ntest['ABV'] = createFeature(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Filter the valid and test datasets based on the topAbv list and check their shapes\nvalid = valid[valid['ABV'].isin(topAbv)]\ntest = test[test['ABV'].isin(topAbv)]\nprint('Valid:', valid.shape)\nprint('Test:', test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Again, due to Hardware limitations, lets use 10K rows for validation and test sets."},{"metadata":{"trusted":false},"cell_type":"code","source":"valid = valid[:10000]\ntest = test[:10000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets apply the preprocessing steps to Valid and Test datasets"},{"metadata":{"trusted":false},"cell_type":"code","source":"valid = preProcessData(valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test = preProcessData(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"valid.to_csv('Validation/valid_final.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test.to_csv('Test/test_final.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}