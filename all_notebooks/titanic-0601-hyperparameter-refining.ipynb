{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport time\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn import linear_model, preprocessing, tree, model_selection\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, VotingClassifier\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/titanic/train.csv')\n\ntrain_cp = train_data.copy()\ntrain_data.head()\n#print (train_data.info())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv('/kaggle/input/titanic/test.csv')\n\ntest_cp = test_data.copy()\ntest_data.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"women = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)/len(women)\n\nprint(\"% of women who survived:\", rate_women)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"man = train_data.loc[train_data.Sex == 'male']['Survived']\nrate_man = sum(man)/len(man)\n#man2 = pd.get_dummies(train_data[['Sex','Survived']])\n#print('% of men who survived:', man2)\n#print(train_data.shape)\nprint('% of men who survived:', rate_man)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_gender(train_data):\n    train_data['hypo'] = 0\n    train_data.loc[train_data.Sex =='female','hypo'] = 1\n    \n    train_data['result'] = 0\n    train_data.loc[train_data.Survived == train_data['hypo'],'result'] = 1\n    \n    print(train_data['result'].value_counts(normalize=True))\n    \npredict_gender(train_data)\ntrain_data.sample(20)\ntype(train_cp)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cp_all = [train_cp, test_cp]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_engineering(data_all):\n    for data in data_all:\n        #create feature combining the existing ones, family size from sibsp and parch\"\n        data['Family size'] = data['SibSp'] + data['Parch'] + 1\n        #extract info from other features, title from name;\n        data['Title'] = data['Name'].str.split(',', expand=True)[1].str.split('.', expand=True)[0]\n        #outputs: [' Mr' ' Mrs' ' Miss' ' Master' ' Don' ' Rev' ' Dr' ' Mme' ' Ms' ' Major'\n        #' Lady' ' Sir' ' Mlle' ' Col' ' Capt' ' the Countess' ' Jonkheer']\n        ##feature engineering #2\n        ##For title counts < 5, we will replace it with 'misc'\n        title_count = data.Title.value_counts() < 10\n        #print(title_count)\n        data.Title = data.Title.apply(lambda x: 'Misc' if title_count.loc[x] == True else x)\n        #train_cp.loc[train_cp.Title.value_counts() < 6,'Title'] = 'misc'\n\n\n\n\ndata_engineering(data_cp_all) \n\n\nprint(train_cp.Title.value_counts())\ndata_cp_all[0].info()\ntrain_cp.sample(12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##fancy randomforestregressor to interpolate the NAN\nfrom sklearn.ensemble import RandomForestRegressor\nfor data in data_cp_all:    \n    \n    data2 = data.copy()\n    label = LabelEncoder()\n    data2['Title'] = label.fit_transform(data2['Title'])\n    \n    age_related_feat = data2[['Age', 'SibSp', 'Parch', 'Pclass', 'Title']]\n    \n    print(age_related_feat.shape)\n    \n    age_known = age_related_feat[age_related_feat.Age.notna()].values    ###extract value to form a numpy array instead of dataframe\n    age_unknown = age_related_feat[age_related_feat.Age.isna()].values\n    \n    print(len(age_known))\n    print(len(age_unknown))\n    \n    known_age_Y = age_known[:,0]\n    known_age_X = age_known[:,1:]\n    \n    rf = RandomForestRegressor(random_state = 1, n_estimators = 300)\n    rf.fit(known_age_X, known_age_Y)\n    \n    unknown_age_X = age_unknown[:,1:]\n    \n    age_pred = rf.predict(unknown_age_X)\n    data.loc[data.Age.isna(), 'Age'] = np.round_(age_pred,0)\n    \n    print(data.Age.isna().sum())\n    \n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####QC the interpolated Age\n\nprint(train_cp.sample(5))\nprint(test_cp.sample(5))\n#print(test_data.loc[test_data.PassengerId == 1005])\n#print(test_cp.loc[test_data.PassengerId == 1005])\nprint(train_cp.columns.values)\nprint(train_cp.columns.tolist())\nprint(train_cp.Survived)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_data(data):   \n    data1 = data.copy()\n\n    ### 2 methods to encode categorical data for comparison here: 1, label encoding, 2, get_dummies: one-hot encoding\n    data1['Fare'] = data1['Fare'].fillna(data1['Fare'].dropna().median())\n    data1['Age'] = data1['Age'].fillna(data1['Age'].dropna().median())\n    data1['Embarked'] = data1['Embarked'].fillna(data1['Embarked'].dropna().mode()[0])\n    \n    #Continuous variable assigned to different bins to simplify the data.\n    data1['FareBin'] = pd.qcut(data1['Fare'], 12)\n    data1['AgeBin'] = pd.cut(data1['Age'].astype(int), 8)\n    \n    all_feat = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Family size', 'Title']\n    '''\n    data.loc[data['Embarked'] == 'S', 'Embarked'] = 0\n    data.loc[data['Embarked'] == 'C', 'Embarked'] = 1\n    data.loc[data['Embarked'] == 'Q', 'Embarked'] = 2\n    '''\n    \n    \n    \n    ###1, get_dummies to get onehot encoding\n    dummy_pclass = pd.get_dummies(data1['Pclass'], prefix = 'Pclass')\n    dummy_embarked = pd.get_dummies(data1['Embarked'], prefix = 'Embarked')\n    data_dummy = pd.get_dummies(data1[all_feat])\n    #data_dummy.insert(2, 'Survived', dd, True)\n    \n    #data_dummy.drop(['Pclass'],axis=1,inplace = True)\n    #print(dummy_pclass)\n    #print(data_dummy)\n    \n    ###2, Sklearn LabelEncoder to conver 'object' to other format\n    label = LabelEncoder()\n    data1['Title'] = label.fit_transform(data1['Title'])\n    data1['Sex'] = label.fit_transform(data1['Sex'])\n    data1['Embarked'] = label.fit_transform(data1['Embarked'])\n    data1['FareBin'] = label.fit_transform(data1['FareBin'])\n    data1['AgeBin'] = label.fit_transform(data1['AgeBin'])\n    \n    data_dummy = pd.concat([data_dummy, dummy_pclass, dummy_embarked, data1['Embarked']], axis = 1)\n    return data1, data_dummy\n\ntrain_cp, test_cp = data_cp_all\n\ntrain_clean, train_dummy = clean_data(train_cp)\n#print(train_clean.info())\ntrain_dummy.insert(1, 'Survived', train_cp['Survived'], True)    \n\ntest_clean, test_dummy = clean_data(test_cp)\n#print(test_clean.info())\n\n#print(train_dummy.shape) \n\n#print(train_clean['Survived'])     #succeed\nprint(train_dummy.columns.values)\n\ntrain_dummy.sample(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for x in ['Pclass','Sex', 'Title', 'SibSp', 'Parch', 'Family size', 'Embarked']:\n    print('Survival correlation by:', x)\n    a = train_cp[[x, 'Survived']].groupby(x, as_index=True).mean()\n    g = train_cp.groupby([x, 'Survived'])\n    print(pd.DataFrame(g.count()['PassengerId'])) \n    print(a)\n    print('_'*10, '\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Age and Fare are largely distributed, try to scale them down, see if it improves the result.\nimport sklearn.preprocessing as preprocessing\nscaler = preprocessing.StandardScaler().fit(train_clean['Age'].values.reshape(-1,1))\n#age_scale_param = scaler.fit(train_clean['Age'])\n#print(scaler.transform(train_clean[['Age', 'Fare']]))\n#train_clean['Age_scl'] = scaler.transform(train_clean['Age'].values.reshape(-1,1)) #both this one and the fit_transform works, but need to convert DF to numpyarray and use reshape to convert it to a 2D array\ntrain_clean['Age_scl'] = scaler.fit_transform(train_clean['Age'].values.reshape(-1,1))\ntrain_clean['Fare_scl'] = scaler.fit_transform(train_clean['Fare'].values.reshape(-1,1))\n\nscaler = preprocessing.StandardScaler().fit(train_dummy['Age'].values.reshape(-1,1))\ntrain_dummy['Age_scl'] = scaler.fit_transform(train_dummy['Age'].values.reshape(-1,1))\ntrain_dummy['Fare_scl'] = scaler.fit_transform(train_dummy['Fare'].values.reshape(-1,1))\ntest_dummy['Age_scl'] = scaler.fit_transform(test_dummy['Age'].values.reshape(-1,1))\ntest_dummy['Fare_scl'] = scaler.fit_transform(test_dummy['Fare'].values.reshape(-1,1))\n\ntest_dummy.sample(3)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###based on the analysis from the statistic above, we chose the features that is highly correlated to the Survive rate as follows:\n#first round of features, by rough estimate: feature_names = ['Pclass', 'Age', 'Fare', 'Embarked', 'Sex', 'SibSp', 'Parch']\n\n\n\n#### feature order matters for RFC,\nfeature_names1 = ['Sex', 'Title', 'Age', 'Pclass', 'Fare',  'Family size']   ##found that Age and Fare are largely distributed, which is not good for model to converge, need to scale them\n\nfeature_names1b = ['Sex', 'Title', 'Age_scl', 'Pclass', 'Fare_scl',  'Family size']   ##the scaled input doesn't change the performance of the RF algorithm\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###a fair comparison for onehot encoded features:\nfeature_names2 = ['Sex_female', 'Sex_male', 'Title_ Master', 'Title_ Miss', 'Title_ Mrs', 'Title_ Mr', 'Title_Misc', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked','Age_scl','Fare_scl', 'SibSp', 'Parch', 'Family size']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"/kaggle/input/titanic-leaked\"))\ngroundtruth_data = pd.read_csv('/kaggle/input/titanic-leaked/titanic.csv')\ngroundtruth_data.sample(4)\ngroundtruth_data = pd.concat([groundtruth_data['Survived'], test_data], axis = 1)\ngroundtruth_data.sample(5)\nprint(test_dummy.shape)\ntest_dummy = pd.concat([groundtruth_data['Survived'], test_dummy], axis = 1)\ntest_dummy.shape\nprint(test_dummy['Survived'].shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\ndef predict_logistic(data):\n    target = data['Survived'].values\n    features = data[feature_names2].values\n    \n    lr_clf = linear_model.LogisticRegression().fit(features, target)\n    bagging_clf = BaggingClassifier(lr_clf, n_estimators=50, max_samples=0.8, max_features=0.8, bootstrap=True, bootstrap_features=False, random_state=0)\n    ensemble = bagging_clf.fit(features, target)\n    cv_scores = model_selection.cross_val_score(lr_clf, features, target, scoring = 'accuracy', cv = 30)\n    print('logistic regression:', lr_clf.score(features,target), 'ensemble:', ensemble.score(features,target), 'cv:', cv_scores.mean())\n    \n    return lr_clf\n    \n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nlr_clf = predict_logistic(train_dummy)\nend = time.time()\nprint(end - start)\nprint(lr_clf)\n\n#use the feature_names1, without scaling the 'Age', 'Fare' logistic regression: 0.7991021324354658 \n#0.034967899322509766\n#use the feature_names1b, with scaling the 'Age', 'Fare', logistic regression: 0.7991021324354658\n#0.01706242561340332    ##faster!!\n#use the feature_names2, onehot encoded features, logistic regression: 0.8316498316498316   ##better accuracy!!\n#0.07921242713928223    \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#further examine the model coefficient:\n#print(list(clf.coef_.T))\npd.DataFrame({'columns': feature_names2, 'coef': list(lr_clf.coef_.T)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##SVC test\ntarget = train_dummy['Survived'].values\nfeatures = train_dummy[feature_names2].values\nsvc_clf = SVC(probability=True).fit(features, target)\nprint(svc_clf.score(features,target))\n\ncv_scores = model_selection.cross_val_score(svc_clf, features, target, scoring = 'accuracy', cv = 20)\nprint(cv_scores.mean())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###randomforest parameter tweaking:\n#1, max_depth 4, 6, 8, 10, 12, 14\ntarget = train_dummy['Survived'].values\nfeatures = train_dummy[feature_names1b].values\nparam_grid = {'max_depth': [4, 6, 8, 10, 12, 14], 'n_estimators':[50, 100, 500, 1000], 'random_state': [10]}\n\nclf = RandomForestClassifier()\nmodel_tune = model_selection.GridSearchCV(clf, param_grid = param_grid, scoring = 'accuracy', cv = 20)\nmodel_tune.fit(features, target)\nprint(model_tune.best_params_)\n    \n\n##{'max_depth': 6, 'n_estimators': 500, 'random_state': 10} is the same as I tested it separately","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef predict_randomforest1(data):\n    target = data['Survived'].values\n    features = data[feature_names1b].values\n    \n    clf = RandomForestClassifier(max_depth = 6, n_estimators =500, random_state = 10)\n    \n    #clf = random_forest = RandomForestClassifier(criterion = \"gini\", \n    #                                   min_samples_leaf = 1, \n    #                                   min_samples_split = 10,   \n    #                                   n_estimators=100, \n    #                                   max_features='auto', \n    #                                   oob_score=True, \n    #                                   random_state=10, \n    #                                   n_jobs=-1)\n    rfc = clf.fit(features,target)\n    print('random forest:', clf.score(features,target))\n    \n    scores = model_selection.cross_val_score(clf, features, target, scoring = 'accuracy', cv = 50)\n    #print(scores)\n    print('avg random forest w/ CV50:',scores.mean())\n    return clf\n\ndef predict_randomforest2(data):\n    target = data['Survived'].values\n    features = data[feature_names2].values\n    \n    clf = RandomForestClassifier(max_depth = 6, n_estimators =500, random_state = 10)\n    \n    ###use bagging regressor to fit:\n    \n    #clf = random_forest = RandomForestClassifier(criterion = \"gini\", \n    #                                   min_samples_leaf = 1, \n    #                                   min_samples_split = 10,   \n    #                                   n_estimators=100, \n    #                                   max_features='auto', \n    #                                   oob_score=True, \n    #                                   random_state=10, \n    #                                   n_jobs=-1)\n    rfc = clf.fit(features,target)\n    print('random forest:', clf.score(features,target))\n    \n    #scores = model_selection.cross_val_score(clf, features, target, scoring = 'accuracy', cv = 50)\n    #print(scores)\n    #print('avg random forest w/ CV50:',scores.mean())\n    return rfc\n    \n#clean_data_ = clean_data(train_data)\n#predict_randomforest(clean_data_)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\n#randomforest1 = predict_randomforest1(train_clean)   ##RF: 0.9158; RF CV50: 0.8285   With Sibsp, Parch\n                                                     ##RF: 0.9068; RF CV50: 0.8341   With Family size instead\n                                                     ##RF: 0.8710; RF CV50: 0.8410   With Family size instead; remove Title Rev, Dr; n_estimator 100 > 500 \nend = time.time() \nprint('runing time:', end - start)    ##65.048 vs 65.378: n_jobs = -1 vs none\n\nrandomforest2 = predict_randomforest2(train_dummy)   ##RF: 0.9181; RF CV50: 0.8329   With Sibsp, Parch; With Sex onehot only\n                                                     ##RF: 0.8530; RF CV50: 0.8365   With Sibsp, Parch; With Sex, Title onehot\n                                                     ##RF: 0.8597; RF CV50: 0.8320   With Family size instead; With Sex, Title onehot\n                                                     ##RF: 0.8653; RF CV50: 0.8375   With Family size instead; With Sex, Title onehot, remove Rev, Dr\n                                                     ##RF: 0.8676; RF CV50: 0.8420   With Family size instead; With Sex, Title onehot, remove Rev, Dr; n_estimator 100 -> 500 \npd.DataFrame({'features': feature_names2, 'importance': list(randomforest2.feature_importances_)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##use the ground_truth for quick turnaround test:\ntest_features = test_dummy[feature_names2].values\ntest_target = test_dummy['Survived'].values\nprint(test_target.shape)\nprint('random forest:', randomforest2.score(test_features,test_target))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####model ensemble using lr_clf, svc_clf, rfc\nlr_pred = lr_clf.predict(test_features)\nsvc_pred = svc_clf.predict(test_features)\nrfc_pred = randomforest2.predict(test_features)\n\n#vote_est = [lr_pred, svc_pred, rfc_pred]\nvote_est = [('rf',randomforest2),('lr',lr_clf),('svc',svc_clf)]\n\nvote_hard = VotingClassifier(estimators = vote_est, voting = 'hard').fit(features, target)\nvote_hard_cv = model_selection.cross_validate(vote_hard, features, target, cv = 20)\nprint('hard voting:',vote_hard_cv['test_score'].mean())\n\nvote_soft = VotingClassifier(estimators = vote_est, voting = 'soft').fit(features, target)\nvote_soft_cv = model_selection.cross_validate(vote_soft, features, target, cv = 20)\nprint('soft voting:',vote_soft_cv['test_score'].mean())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##use the ground_truth for quick turnaround test:\ntest_features = test_dummy[feature_names2].values\ntest_target = test_dummy['Survived'].values\nprint(test_target.shape)\nprint('random forest:', vote_soft.score(test_features,test_target))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#test_clean = clean_data(test_cp)\nX_test_features = test_dummy[feature_names2].values\npredictions = vote_soft.predict(X_test_features)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}