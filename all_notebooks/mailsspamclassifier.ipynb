{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Welcome to my notebook\n**In this notebook, i will introduce to you some technique to preprocess text data and build model to classify spam or ham email**\n**.This notebook will cover:**\n1. Preprocessing text data\n    * Remove stopwords, punctuation, stemming, lemmazation,....\n    * Vectorize text data using Term frequency inverse document frequency TfidfVectorizer   \n2. Build model\n    * Use Naive bayes, Logistic regression and SVC to classify spam email\n    * Compare Stemming and lemmazation text in term of model performance on it \n3. Oversampling technique\n    * We will use SMOTE(Synthetic Minority Over-sampling Technique) to oversampling the minority class to see if we can increase model performance","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Import neccessary librarys","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nnltk.download('stopwords')\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import classification_report, plot_confusion_matrix\n#for oversampling minority class\nfrom imblearn.over_sampling import SMOTE\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Reading data and see some emails**","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/spam-mails-dataset/spam_ham_dataset.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"sample = data.sample(5)\nfor i in range(5):\n    print('Class: ', sample.iloc[i]['label'])\n    print('Email:')\n    print(sample.iloc[i]['text'])\n    print('\\n', '---'*45)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We see that emails contains many special character, number, http,... which will not useful to classify spam email. So we will remove them later.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop the ID column\ndata.drop('Unnamed: 0', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Plot the distribution of class, we see that spam email only contain 29%. This is not good because this dataset is skewed, when we directly feed it to our model, it will not generalize well**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data['label_num'].value_counts()/sum(data['label_num'].value_counts())*100)\nsns.countplot('label_num', data = data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing text:\n* Lower case text\n* Remove number, punctuation, leading and ending space, stopwords\n* I add the 'subject' to stopwords because it not look like useful for predict spam.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords_set = set(stopwords.words('english'))\n#Save the 'not'\n#stopwords_set.remove('not')\n#add subject to stopwords\nstopwords_set.add('subject')\nstopwords_set.add('http')\ndef preprocessing_text(x):\n    import string\n    #lower case\n    x = x.lower()\n    #remove number\n    x = re.sub(r'\\d+','',x)\n    #remove punctuation\n    x = re.sub(r'[^\\w\\s]', '',x)\n    #remove leading and ending space\n    x = x.strip()\n    #remove stopword\n    x = ' '.join([word for word in word_tokenize(x) if not word in stopwords_set])\n    return x\n#apply preprocessing text on text\ndata['text'] = data['text'].apply(lambda x: preprocessing_text(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Split dataset to train and test**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = train_test_split(data, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Draw wordclouds of Spam and Not spam emails.\n**WordCloud will show us what is the most popular word in a specific class, which will give us some insight about the data**\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#draw wordcloud\nfrom wordcloud import WordCloud\nsns.set(style = None)\ntrain_spam = train[train['label_num'] == 1]\ntrain_spam = train_spam['text']\n#turn series to string by join ' ' to it\ntrain_spam = ' '.join(train_spam)\ntrain_ham = train[train['label_num'] == 0]\ntrain_ham = train_ham['text']\ntrain_ham = ' '.join(train_ham)\nwordcloud_spam = WordCloud(background_color = 'black', width = 2500, height = 2000 ).generate(train_spam)\nplt.figure(figsize = (13,13))\nprint('Spam email wordcloud')\nplt.imshow(wordcloud_spam)\nplt.show()\nwordcloud_ham = WordCloud(background_color = 'white', width = 2500, height = 2000).generate(train_ham)\nprint('Ham email wordcloud')\nplt.figure(figsize = (13,13))\n\nplt.imshow(wordcloud_ham)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stemming word\n**stemming mean bring all the word to its original form. e.g. surveys -> survey, started -> start**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Email before stemming**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain.iloc[0]['text']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Stemming all mails**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\n#function take in a list of tokenized word and stemming it \ndef stemming_words(words):\n    stemmed_words = []\n    for word in words:\n        stemmed_words.append(stemmer.stem(word))\n    return ' '.join(stemmed_words)\nstemmer = PorterStemmer()\n#tokenize word before use stemming\ntrain['text'] = train['text'].apply(lambda x: word_tokenize(x))\ntrain['text'] = train['text'].apply(lambda x: stemming_words(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Email after stemming**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.iloc[0]['text']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build model using stemming","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Before we feed emails to our model, we have to vectorize all of it. Because we are human, we can understand text, but computer do not. It only works with numbers**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#control the max_features to vectorize to leave some unpopular word out, which is consider not important, like personal names, ...\ntfidf = TfidfVectorizer( strip_accents = 'ascii', max_df = 0.8, max_features = 27000)\ntrain_vectorized = tfidf.fit_transform(train['text'])\ntest_vectorized = tfidf.transform(test['text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Oversample the spam class. This will give us a more balance dataset**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sm = SMOTE(sampling_strategy = 1,random_state = 42)\nX_resample,y_resample = sm.fit_resample(train_vectorized, train['label_num'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_resample.value_counts()/sum(y_resample.value_counts())*100)\nsns.countplot(y_resample)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Look pretty good. We have a balance dataset now. Next, we will compare beween model train on the non-resample dataset and the resample-dataset to see if SMOTE make the model performance better**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, plot_confusion_matrix\nnb = MultinomialNB()\nnb.fit(train_vectorized, train['label_num'])\np = nb.predict(test_vectorized)\nprint('Naive Bayes on non-resample dataset\\n\\n')\nprint(classification_report(test['label_num'],p))\nplot_confusion_matrix(nb, test_vectorized, test['label_num'], cmap = 'Paired')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, plot_confusion_matrix\nnb = MultinomialNB()\nnb.fit(X_resample, y_resample)\np = nb.predict(test_vectorized)\nprint('Naive Bayes on resample dataset\\n\\n')\nprint(classification_report(test['label_num'],p))\nplot_confusion_matrix(nb, test_vectorized, test['label_num'], cmap = 'Paired')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Look pretty good. Our Naive bayes model have improve significantly on resample dataset. Next we will check on Logistic regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(train_vectorized, train['label_num'])\nlr_p = lr.predict(test_vectorized)\nprint('Logistic regression on non-resample dataset')\n\nprint(classification_report(test['label_num'], lr_p))\nplot_confusion_matrix(lr, test_vectorized, test['label_num'], cmap = 'Paired')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_resample, y_resample)\nlr_p = lr.predict(test_vectorized)\nprint('Logistic regression on resample dataset\\n\\n')\nprint(classification_report(test['label_num'], lr_p))\nplot_confusion_matrix(lr, test_vectorized, test['label_num'], cmap = 'Blues')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**This logistic regression model does not perform well on resample dataset as expected. Let check on SVC**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvc = SVC()\nsvc.fit(train_vectorized, train['label_num'])\nsvc_p = svc.predict(test_vectorized)\nprint('SVC on non-resample dataset')\nprint(classification_report(test['label_num'], svc_p))\nplot_confusion_matrix(svc, test_vectorized, test['label_num'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvc = SVC()\nsvc.fit(X_resample, y_resample)\nsvc_p = svc.predict(test_vectorized)\nprint(classification_report(test['label_num'], svc_p))\nplot_confusion_matrix(svc, test_vectorized, test['label_num'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Well, it look like SVC does not better or worser with resample dataset**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Build model using Lemmazation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Lemmazation\n**The aim of lemmatization, like stemming, is to reduce inflectional forms to a common base form. As opposed to stemming, lemmatization does not simply chop off inflections. Instead it uses lexical knowledge bases to get the correct base forms of words**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#split the data again\ntrain, test = train_test_split(data, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Email before lemmazation')\ntrain.iloc[0]['text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n#lemmatize function take in list of words, so you have to tokenize word before give it to this function\ndef lemmatize_words(words):\n    lemmatizer = WordNetLemmatizer()\n    lemmatized_words = []\n    for word in words:\n        lemmatized_words.append(lemmatizer.lemmatize(word))\n    return ' '.join(lemmatized_words)\ntrain.loc[:,'text'] = train.loc[:,'text'].apply(lambda x: word_tokenize(x))\ntrain.loc[:,'text'] = train.loc[:,'text'].apply(lambda x: lemmatize_words(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Email after lemmazation')\ntrain.iloc[0]['text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Vectorize text \ntfidf = TfidfVectorizer( strip_accents = 'ascii', max_df = 0.8, max_features = 27000)\ntrain_vectorized = tfidf.fit_transform(train['text'])\ntest_vectorized = tfidf.transform(test['text'])\n#and oversampling data \nsm = SMOTE(sampling_strategy = 1,random_state = 42)\nX_resample,y_resample = sm.fit_resample(train_vectorized, train['label_num'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we compare model train on non-resample data and resample data as we did before**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nnb = MultinomialNB()\nnb.fit(train_vectorized, train['label_num'])\np = nb.predict(test_vectorized)\nprint('Naive bayes on non-resample dataset')\nprint(classification_report(test['label_num'],p))\nplot_confusion_matrix(nb, test_vectorized, test['label_num'], cmap = 'Paired')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**This naive bayes on lemmazation data is clearly better than the one stemming data. Next, lets see naive bayes on lemmazation resample dataset**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nnb = MultinomialNB()\nnb.fit(X_resample, y_resample)\np = nb.predict(test_vectorized)\nprint('Naive bayes on resample dataset')\nprint(classification_report(test['label_num'],p))\nplot_confusion_matrix(nb, test_vectorized, test['label_num'], cmap = 'Paired')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**This model is slightly better than the one train on stemming resample dataset. Next, lets see logistic regression**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlr = LogisticRegression()\nlr.fit(train_vectorized, train['label_num'])\nlr_p = lr.predict(test_vectorized)\nprint('Logistic regression on non-resample data')\nprint(classification_report(test['label_num'], lr_p))\nplot_confusion_matrix(lr, test_vectorized, test['label_num'], cmap = 'Paired')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Slightly better than the former logistic regression. Lets see the Logistic regression train on resampling dataset**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(X_resample, y_resample)\nlr_p = lr.predict(test_vectorized)\nprint('Logistic regression on resample data')\nprint(classification_report(test['label_num'], lr_p))\nplot_confusion_matrix(lr, test_vectorized, test['label_num'], cmap = 'Blues')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Well, it look worser than the one train with non-resampling. But it is not sure, if we can test it on more sample we will know that which one is better.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsvc = SVC()\nsvc.fit(train_vectorized, train['label_num'])\nsvc_p = svc.predict(test_vectorized)\nprint('SVC on non-resample data\\n\\n')\nprint(classification_report(test['label_num'], svc_p))\nplot_confusion_matrix(svc, test_vectorized, test['label_num'], cmap = 'Blues')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Look like SVC do the best job. Let see how it perform on resampling dataset**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsvc = SVC()\nsvc.fit(X_resample, y_resample)\nsvc_p = svc.predict(test_vectorized)\nprint('SVC on resample data\\n\\n')\nprint(classification_report(test['label_num'], svc_p))\nplot_confusion_matrix(svc, test_vectorized, test['label_num'], cmap = 'Blues')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Notthing change. If we have more test data, we can then test our model and see its generalization.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n**Finally, we come to the end of this notebook. Let see what we have learn so far.**\n1. We know how to preprocessing text data, Vectorize it using TfidfVectorizer\n2. We know how to use Naive bayes, logistic regression, SVC to classify spam emails\n3. We know how to use stemming, lemmazation, and compare model performance on each type, and see that lemmazation give us better performance\n**Well, that its for this notebook. See you next time.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}