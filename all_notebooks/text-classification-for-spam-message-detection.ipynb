{"cells":[{"metadata":{"trusted":true,"_uuid":"4b75410f697ac359667b5f45317089e199c586d0"},"cell_type":"code","source":"#import useful packages\nimport numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02b1d2f0f925caf1ccbb46103fd8afa9bf7a107e","scrolled":true},"cell_type":"code","source":"#download nltk's 'stopwords' for removal in pre-process\nimport nltk\nnltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"267d2197d3d09db1c6e70a3af342bd8763d54e92"},"cell_type":"code","source":"#read data, rename the columns, and check the format\nSMS= pd.read_csv('../input/filtering-mobile-phone-spam/sms_spam.csv',  names=['label','messages'] )\nSMS.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"2e159752bf960aa4c2cb02c9754c8f448619f255"},"cell_type":"code","source":"SMS=SMS.iloc[1:]  #remove original labels\nSMS.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"3cdc4fd8e71043a40a3357bed52f869f8e207928"},"cell_type":"code","source":"SMS.groupby('label').describe() #explore the data to get some initial understanding","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ef97bad0ac7187a78af4557fcf4230044ee3c88"},"cell_type":"code","source":"#next step is to pre-process text data \n\n#firstly import useful functions\nimport string\nfrom nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"229a344eea353a9ddda1c8efe11bf083ea1f3165"},"cell_type":"code","source":"\n#    define a function to:\n#    1. Remove punctuations\n#    2. tokenize the terms in each text message\n#    3. Remove stopwords\n\ndef text_process(content):\n    removepunc=[word for word in content if word not in string.punctuation]\n    removepunc=''.join(removepunc)\n    \n    return[term for term in removepunc.split() if term.lower() not in stopwords.words('english')]\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"2d81ea6e73231d0692536c7dfec5eb2afcf02df4"},"cell_type":"code","source":"SMS['messages'].head(5).apply(text_process) #check if defined function works ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"beddbdaaea5e256c68530535429f09b6aaf6ccbc"},"cell_type":"code","source":"#Next step is to vectorize each term and weight it by tf-idf model\n\n#firstly import cpuntvectorizer to measure the frequency of each word term\n\nfrom sklearn.feature_extraction.text import CountVectorizer \nbow_process=CountVectorizer(analyzer=text_process).fit(SMS['messages'])\n\nprint (len(bow_process.vocabulary_)) #check the number of terms (vectors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b446dd64d1eed6e214032b1da021ae3d5675d4ff"},"cell_type":"code","source":"#transform vectors to term-document incidence matrix\nSMS_bow= bow_process.transform(SMS['messages'])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"253e1fefcbee23f8ec00699a1c9c08e2d891de0d"},"cell_type":"code","source":"print('Shape :', SMS_bow.shape) #check the size of term-document incidence matrix","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"34aabeee5f2e34ba0289e7719d67c92ff7796874"},"cell_type":"code","source":"#weight vectors by tf-idf model\nfrom sklearn.feature_extraction.text import TfidfTransformer\ntfidf_trans=TfidfTransformer().fit(SMS_bow)\nSMS_tfidf= tfidf_trans.transform(SMS_bow)\n\nprint(SMS_tfidf.shape) #check the size of weighted term-document incidence matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65083fb069b10bbcdd4b4823d135d4269f7a4192"},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nnaive_bayes_model=MultinomialNB().fit(SMS_tfidf, SMS['label']) #train Naive Bayes classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb2b931d5682509c5ec0cb9f0a8047b1f812067a"},"cell_type":"code","source":"from sklearn.svm import NuSVC\nSVM_model=NuSVC(nu = 0.05, class_weight = 'balanced').fit(SMS_tfidf, SMS['label']) #train SVM classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9695cc9922405633321aa59df79d45315dbe90ed"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nKNN_model=KNeighborsClassifier().fit(SMS_tfidf, SMS['label']) #train KNN classfier","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"d3d8d495f893631699ea9c6d8556688a1e0875f9"},"cell_type":"code","source":"#now we have developed three trained classification models\n#next step is to test and evaluate the models\n#first need to use the same way to pre-process test data to weighted vectors\nSMS_test= pd.read_csv('../input/spam-test-set/spam_test.csv')\nSMS_test=SMS_test.rename(columns={'v1':'label','v2':'messages'})\nSMS_test=SMS_test.iloc[:,:2]\nSMS_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d5b60ae5b299683a67764b209db9eaed44c9ca6"},"cell_type":"code","source":"SMS_test['messages'].head(5).apply(text_process)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ff4efa2ce9aa227082e0ea0da0b65b1ff58b918"},"cell_type":"code","source":"SMS_test_bow=bow_process.transform(SMS_test['messages'])\nSMS_test_tfidf=tfidf_trans.transform(SMS_test_bow) #now we have vectorized test data that can be classified by three models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb04f05467dd02c5fc65dc7afc687b5f51cbffd3"},"cell_type":"code","source":"print(SMS_test_bow.shape)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"61cf41303c89feea17b7ab15f281d3eed2287973"},"cell_type":"code","source":"from sklearn.metrics import classification_report\nNB_predict = naive_bayes_model.predict(SMS_test_tfidf)    # test the Naive Bayes model and get prediction\n\nprint(classification_report(SMS_test['label'],NB_predict))    # generate evaluation report of NB model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a0a947806e13dc6b1c84d324976a10373582bd2c"},"cell_type":"code","source":"SVM_predict=SVM_model.predict(SMS_test_tfidf)    # test the SVM model and get prediction\nprint(classification_report(SMS_test['label'],SVM_predict))     # generate evaluation report of SVM model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9686ce4ecbe4bce96b94f8b26b76d30505683014"},"cell_type":"code","source":"KNN_predict=KNN_model.predict(SMS_test_tfidf)     # test the KNN model and get prediction\nprint(classification_report(SMS_test['label'],KNN_predict))     # generate evaluation report of KNN model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba8e8fb606b950a0a83c057da8dd2c2bc1c2b3f4"},"cell_type":"code","source":"#create pipelines for three models to systematically pre-process text data based on our previous pre-processing steps\n#to store pipelines of workfolow\n#for further study use\n\nfrom sklearn.pipeline import Pipeline\nNB_classifier = Pipeline([\n    ('bow', CountVectorizer(analyzer=text_process)),   #vectorize terms within text data sets\n    ('tfidf', TfidfTransformer()),  #weight terms\n    ('classifier', MultinomialNB()),   #implement Naive Bayes classifier\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a0f64b4faac9bc8fc812fbf5a0ad3a79cb0f7b2"},"cell_type":"code","source":"NB_classifier.fit(SMS['messages'],SMS['label'])  #train the model by fitting training data sets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"324d62f1ede6635167bf43e5df5acd295283f40d"},"cell_type":"code","source":"NB_prediction=NB_classifier.predict(SMS_test['messages'])  #test the model and get prediction\nprint(classification_report(SMS_test['label'],NB_prediction))  #create evaluation report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f387cebb184ef2b1d19cdb33a3cd28b613f983f7"},"cell_type":"code","source":"### same steps as above to produce ###\n\nSVM_classifier = Pipeline([\n    ('bow', CountVectorizer(analyzer=text_process)), \n    ('tfidf', TfidfTransformer()),   \n    ('classifier', NuSVC(nu = 0.05, class_weight = 'balanced')),\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a51eed72ce00e2ebb161a73947c8670fc0b1b43"},"cell_type":"code","source":"SVM_classifier.fit(SMS['messages'],SMS['label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb936b8d8fe110b24d5fd1ab6f6b5183207639bc"},"cell_type":"code","source":"SVM_prediction=SVM_classifier.predict(SMS_test['messages'])\nprint(classification_report(SMS_test['label'],SVM_prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20bc38669fd43ece5f1df59ca9780ac3ee74653f"},"cell_type":"code","source":"KNN_classifier= Pipeline([\n    ('bow', CountVectorizer(analyzer=text_process)), \n    ('tfidf', TfidfTransformer()),   \n    ('classifier', KNeighborsClassifier()),\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c81fd4475110feaad3766ca07bb4f4997c88e047"},"cell_type":"code","source":"KNN_classifier.fit(SMS['messages'],SMS['label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d61acdd9488c4bf2f7415f6d53a525286da07ad"},"cell_type":"code","source":"KNN_prediction=KNN_classifier.predict(SMS_test['messages'])\nprint(classification_report(SMS_test['label'],KNN_prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4e5a27976ff0d74e04e260fd8d02ff56e37f6f4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":1}