{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-03T16:51:26.025694Z","iopub.execute_input":"2021-08-03T16:51:26.026108Z","iopub.status.idle":"2021-08-03T16:51:26.038296Z","shell.execute_reply.started":"2021-08-03T16:51:26.026073Z","shell.execute_reply":"2021-08-03T16:51:26.037348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#runtime\nimport time\n# Base Libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt  \nimport numpy as np\n# Transformation\nfrom sklearn.preprocessing import MinMaxScaler\n# Models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import neighbors\n# Metrics\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import explained_variance_score\nimport numpy as np \nimport pandas as pd #data processing, csv file i/o (eg pd.read csv)\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom optuna.samplers import TPESampler\n\n#from sklearn.model_selection import GridSearchCV\n#Regular Expressions\nimport re \n# from datetime import datetime\n\n#Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n# import plotly \n\nimport optuna #optimal hyperperameters used in our xgb model\nimport xgboost as xgb\n# from optuna import visualization\n\n#Metrics\nfrom sklearn.metrics import r2_score #r2\nfrom sklearn.metrics import mean_absolute_error #MAE\nfrom sklearn.metrics import mean_squared_error #RMSE\nfrom sklearn.metrics import explained_variance_score #EVS\n\nstart = time.time()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T16:51:26.039916Z","iopub.execute_input":"2021-08-03T16:51:26.040247Z","iopub.status.idle":"2021-08-03T16:51:26.055996Z","shell.execute_reply.started":"2021-08-03T16:51:26.040218Z","shell.execute_reply":"2021-08-03T16:51:26.054964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = MinMaxScaler(feature_range=(0, 1))\n\ndata = pd.read_csv('../input/hydropower-generation/Hydropower_Consumption.csv', sep=',')\ndata = data.drop(columns = [\"Country\"])\ndata = pd.DataFrame(scaler.fit_transform(data), \n                        columns=['2000','2001','2002','2003','2004','2005',\n                                 '2006','2007','2008','2009','2010','2011',\n                                 '2012','2013','2014','2015','2016','2017',\n                                 '2018','2019'])\ndata.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T16:51:26.05796Z","iopub.execute_input":"2021-08-03T16:51:26.058288Z","iopub.status.idle":"2021-08-03T16:51:26.16829Z","shell.execute_reply.started":"2021-08-03T16:51:26.058259Z","shell.execute_reply":"2021-08-03T16:51:26.167206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = data['2019'].copy()\nX = data.drop('2019',axis=1).copy()\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\npd.DataFrame(X)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T16:51:26.169983Z","iopub.execute_input":"2021-08-03T16:51:26.170285Z","iopub.status.idle":"2021-08-03T16:51:26.207195Z","shell.execute_reply.started":"2021-08-03T16:51:26.170257Z","shell.execute_reply":"2021-08-03T16:51:26.206115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.70, random_state=100)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, train_size=0.80, random_state=200)\n\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndval = xgb.DMatrix(X_val, label=y_val)\ndtest = xgb.DMatrix(X_test, label=y_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T16:51:26.208977Z","iopub.execute_input":"2021-08-03T16:51:26.209405Z","iopub.status.idle":"2021-08-03T16:51:26.226789Z","shell.execute_reply.started":"2021-08-03T16:51:26.209364Z","shell.execute_reply":"2021-08-03T16:51:26.225691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(trial):\n      \n    learning_rate = trial.suggest_loguniform('learning_rate', 0.04, 0.08)\n    max_depth = trial.suggest_int('max_depth', 7,11)\n    \n    subsample = trial.suggest_loguniform('subsample', 0.6,1.0)\n    colsample_bytree = trial.suggest_loguniform('colsample_bytree', 0.9,1.0)\n    \n    gamma = trial.suggest_loguniform('gamma', 1e-8,1.0)\n    max_leaves = trial.suggest_int('max_leaves', 10,200)\n    \n    reg_alpha = trial.suggest_loguniform ('reg_alpha', 1e-8,6e-5)\n\n    params = {\n              \n              'booster':'gbtree',\n              'objective': 'reg:squarederror',\n              'grow_policy': 'lossguide',\n              'eval_metric':'rmse',\n              'tree_method':'hist',\n               \n\n        \n              'learning_rate': learning_rate,  \n              'max_depth': max_depth,\n              'subsample': subsample,\n              'colsample_bytree': colsample_bytree, \n              'gamma': gamma,\n              'max_leaves': max_leaves,\n              \n              \n              'max_bin': 512,\n              'max_delta_step':0,\n              'num_parallel_tree':1,\n              'scale_pos_weight':1,\n              'reg_alpha': reg_alpha, \n              'reg_lambda': 1,\n              'min_child_weight':1,\n         \n              }\n    \n    \n\n    prune_error = 'eval-' + params['eval_metric']   \n    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, prune_error)   \n    model = xgb.train(params, dtrain, num_boost_round=1000, evals=[(dval, 'eval')],callbacks=[pruning_callback],\n                      early_stopping_rounds=10, verbose_eval=0,)\n    results = model.eval(dval)\n    rmse = np.float(re.search(r'[\\d.]+$', results).group(0))\n    return rmse","metadata":{"execution":{"iopub.status.busy":"2021-08-03T16:51:26.230713Z","iopub.execute_input":"2021-08-03T16:51:26.231024Z","iopub.status.idle":"2021-08-03T16:51:26.240407Z","shell.execute_reply.started":"2021-08-03T16:51:26.23098Z","shell.execute_reply":"2021-08-03T16:51:26.239186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study = optuna.create_study(sampler=(TPESampler()))\nstudy.optimize(objective, n_trials=100, show_progress_bar=True)\n\n\n\nbest_params = study.best_params\nprint(best_params)\n\ncv_output = xgb.cv(best_params, dtrain, num_boost_round=10000, nfold=8,\n                   early_stopping_rounds=10, verbose_eval=0, show_stdv=False)\ncv_output[['train-rmse-mean', 'test-rmse-mean']].plot()\nplt.xlabel('Rounds')\nplt.ylabel('rmse')\nplt.show()\n\n\ny_true = np.array(y_test, dtype=np.float)\n\nnum_boost_rounds = len(cv_output)\nprint(num_boost_rounds)\n\nmodel = xgb.train(dict(best_params), dtrain, num_boost_round= num_boost_rounds,\n                  evals=[(dval, 'eval')], early_stopping_rounds=10)\n\n\n# r2 = r2_score(y_true, y_pred)\nMAE = mean_absolute_error(y_test, model.predict(dtest))\n# MAPE = np.mean(np.abs((y_true - model.predict(dtest))/y_test)*100)\nMBE = np.mean(model.predict(dtest) - y_test)\nRMSE = np.sqrt(mean_squared_error(y_test, model.predict(dtest)))\n# rRMSE = (RMSE/np.mean(model.predict(dtest)))*100\n# n = len(y_test)\n# t_stat = np.sqrt(((n-1)*MBE*MBE)/((RMSE*RMSE)-(MBE*MBE)))\n\n\n\nprint(\"R^2 in training: %s\"  % r2_score(dtrain.get_label(), model.predict(dtrain)))\nprint(\"R^2 in testing: %s\"  % r2_score(y_test, model.predict(dtest)))\n\n\n\n\n# print(\"R^2 Score: {:.6f}\".format(r2))\nprint('MAE Valid:', MAE)\n# print('MAPE Valid:', MAPE)\nprint('MBE Valid:', MBE)\nprint('RMSE Valid:', RMSE)\n# print('rRMSE Valid:', rRMSE)\n# print('t-stat:', t_stat)\n\n\n\ny_true = np.array(y_test, dtype=np.float)\ny_pred = np.array(model.predict(dtest), dtype=np.float)\n\ndata_prediction = list(zip(y_true,y_pred))\ndata_prediction = pd.DataFrame(data_prediction, columns=['Test','Prediction'])\ndata_prediction.head(10)\n\n\ndata_prediction=data_prediction.rename({'Radiation':'Radiation_observed'}, axis=1)\ndata_prediction.plot(figsize=(20,12))\nplt.ylabel('Radiation(W/m2)')\nplt.show()\n\n\nend = time.time()\n\nprint(f\"Runtime of the program is {end - start}\")","metadata":{"execution":{"iopub.status.busy":"2021-08-03T16:51:26.282662Z","iopub.execute_input":"2021-08-03T16:51:26.282997Z","iopub.status.idle":"2021-08-03T16:51:32.213061Z","shell.execute_reply.started":"2021-08-03T16:51:26.282967Z","shell.execute_reply":"2021-08-03T16:51:32.212109Z"},"trusted":true},"execution_count":null,"outputs":[]}]}