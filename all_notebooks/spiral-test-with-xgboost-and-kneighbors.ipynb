{"cells":[{"metadata":{"_uuid":"177aee30cc8c8af8a7e7dbf1e9df874c05a1cd60"},"cell_type":"markdown","source":"# Interpolating and Extrapolating with XGBoost and KNeighbors\n\nYou might find discussions about certain machine learning optimizers being good at **interpolating** but not so good at **extrapolating**. XGBoost is a very popular optimizationg algorithm among kagglers. Here we apply XGBoost\nto this [**simple data set**](https://www.kaggle.com/pliptor/a-visual-and-intuitive-traintest-pattern). It is a classification problem with three classes of points on a plane. We first run the classification using the KNeiborsClassifier and then compare with XGBoost and draw conclusions.\n\nNote there are already a few samples of scripts for this dataset for various classifiers including neural networks written in R. This is a sample script using Python and sklearn. \n","outputs":[],"execution_count":null},{"metadata":{"_uuid":"0f1effaefb4fed8bcead010bf9d885c51576ee68"},"cell_type":"markdown","source":"# Load data and split point coordinates and target values\n","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"1d68192be1f9d7e1508458607f6fa968ddb52870"},"cell_type":"code","source":"import pandas as pd\n%matplotlib inline\ntrain = pd.read_csv(\"../input/train.csv\");\nprint(train.head())\ntrainnp = train.values\nX_train = trainnp[:,1:] \ny_train = trainnp[:,0] ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5dd2f8c50047aa013ced0c8fe8689d2d84a0c988"},"cell_type":"markdown","source":"# KNeiborsClassifier decision regions\n\nLet's see how KNeiborsClassifier does with this data set","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"_uuid":"5b2faff985b4a4f9e95608653f6dd6e2e1c48b0f"},"cell_type":"code","source":"from mlxtend.plotting import plot_decision_regions # convenient library for plotting decision regions\nfrom sklearn.neighbors import KNeighborsClassifier as KNeighborsClassifier\nknclassifier = KNeighborsClassifier(n_neighbors=3, p=2)\n\n# train the classifier\nknclassifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"14b0cb047d553abb2b08f4eeddba666649b6f4f3"},"cell_type":"code","source":"plot_decision_regions(X_train, y_train.astype(int), knclassifier)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d1466dfa1ebf5aa3da0782f5da7fa4e633e9366"},"cell_type":"markdown","source":"We see the KNeighborsClassifier does a good job drawing the boundaries for each class. We also see very good **interpolation**. We intepret interpolated point as points that are near or between points of the same class. **Extrapolation** is also good. We interpret extrapolated points as points away from any point from the trainig set. One could argue though that the extrapolation is failing if the arms were to continue the spiral outwards. However, we will se XGBoost does a terrible extrapolation job.  ","outputs":[],"execution_count":null},{"metadata":{"_uuid":"831adf2ced1a9a5a2d066ed9a5dbff1e2858df80"},"cell_type":"markdown","source":"# XGBoost\n\nNow let's try with xgboost. We'll be running with pretty much the default parameters. ","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"9e5c70976622fd0f330307cca32a4aa7374b92bc"},"cell_type":"code","source":"import xgboost as xgb\n\nparams = {\n    'booster':'gbtree',\n    'colsample_bylevel':1,\n    'colsample_bytree':1,\n    'gamma':0, \n    'learning_rate':0.1, \n    'max_delta_step':0,\n    'max_depth':3,\n    'min_child_weight':1,\n    'n_estimators':100,\n    'objective':'multi:softprob',\n    'random_state':2018,\n    'reg_alpha':0, \n    'reg_lambda':1,\n    'seed':2018,\n    'subsample':1}\n\nxgb_clf = xgb.XGBClassifier(**params)\nxgb_clf = xgb_clf.fit(X_train, y_train, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3099949705940756b925be41bf8d42bde63474bb"},"cell_type":"code","source":"xgb_clf = xgb_clf.fit(X_train, y_train)\n\nplot_decision_regions(X_train, y_train.astype(int), xgb_clf)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d504001ebaaa92c22a77443a401c2bd7e81c80dd"},"cell_type":"markdown","source":"# Conclusions\n\nIt seems XGBoost has no issues with **interpolation**. However, **extrapolation** is not nearly as smooth compared with that of KNeiborsClassifier. It might be improved by adjusting parameters but in my attempts it is not easy. This might be indication that dealing of unseen data that is not a result of **interpolation** may not be really well handled by XGBoost. I thought that's something to be kept in mind.This is not to say that XGBoost is bad. On the contrary, it does pretty well in many competitions. However it may just be because the unseen data typically don't deviate much from the train data.","outputs":[],"execution_count":null}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}