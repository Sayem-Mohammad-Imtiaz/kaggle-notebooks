{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Disclaimer :** Before, we go any further, Let me clear that You would need **GPU** and **Internet**- toggle turned **ON** (install external libraries) to succesfully run this kernel."},{"metadata":{},"cell_type":"markdown","source":"![Begin](https://pbs.twimg.com/tweet_video_thumb/ECvtpBRXoAIpyUv.jpg)\n\nImg source : https://pbs.twimg.com/tweet_video_thumb/ECvtpBRXoAIpyUv.jpg"},{"metadata":{"trusted":true},"cell_type":"code","source":"# pytorch_pretained_bert already available in kaggle conda env.\n# !pip install pytorch-nlp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note :** uncomment the code line in above cell; you are running this notebook locally, and would need pytorch-nlp library.Here, it is pre-installed."},{"metadata":{},"cell_type":"markdown","source":"### importing necessaries libraries..."},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nimport numpy as np\nimport random as rn\nimport pandas as pd\nimport torch\nfrom pytorch_pretrained_bert import BertModel\nfrom torch import nn\n# from torchnlp.datasets import imdb_dataset      # --> We are using our own uploaded dataset.\nfrom pytorch_pretrained_bert import BertTokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom torch.optim import Adam\nfrom torch.nn.utils import clip_grad_norm_\nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Initializing seed values to stabilize the outcomes."},{"metadata":{"trusted":true},"cell_type":"code","source":"rn.seed(321)\nnp.random.seed(321)\ntorch.manual_seed(321)\ntorch.cuda.manual_seed(321)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/imdb-50k-movie-reviews-test-your-bert/'\n\ntrain_data = pd.read_csv(path + 'train.csv')\ntest_data = pd.read_csv(path + 'test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# experimenting here with a sample of dataset, to avoid memory overflow.\ntrain_data = train_data[:2000]\ntest_data = test_data[:500]\n\ntrain_data = train_data.to_dict(orient='records')\ntest_data = test_data.to_dict(orient='records')\ntype(train_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Mapping sentences with their Labels..."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_texts, train_labels = list(zip(*map(lambda d: (d['text'], d['sentiment']), train_data)))\ntest_texts, test_labels = list(zip(*map(lambda d: (d['text'], d['sentiment']), test_data)))\n\nlen(train_texts), len(train_labels), len(test_texts), len(test_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### visualizing one of the sentences from train set"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_texts[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## visualizing sentences lengths"},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = [len(sent) for sent in train_texts]\n\nplt.rcParams.update({'figure.figsize':(7,5), 'figure.dpi':100})\nplt.bar(range(1,2001), sentences, color = ['red'])\nplt.gca().set(title='No. of characters in each sentence', xlabel='Number of sentence', ylabel='Number of Characters in each sentence');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can see that most of the sentences are around 700 - 1000 characters long, which is pretty obvious. HOwever, few sentences are shorter and few even long as 6000 characters. So, this is a good, very versatile Review Dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.tokenize('Hi my name is Atul')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sample of how BERT Tokenizer works and Embeddings prepared to be fed into BERT Model.\n\n![BERT TOKENS](https://miro.medium.com/max/619/1*iJqlhZz-g6ZQJ53-rE9VvA.png)"},{"metadata":{},"cell_type":"markdown","source":"## Preparing Token embeddings..."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], train_texts))\ntest_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], test_texts))\n\nlen(train_tokens), len(test_tokens)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing Token Ids...\n\n\n![token ids](https://jalammar.github.io/images/distilBERT/sst2-text-to-tokenized-ids-bert-example.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, train_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\ntest_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, test_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n\ntrain_tokens_ids.shape, test_tokens_ids.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Many a times your Kernel will Freeze but this is just OK. Let it be. This is a heavy computing task; So,it is just a common thing to happen. I have also put Monitoring code snippets to monitor your CPU/GPU usage and also Garbage Collector to free up space.\n\n\nIt is quite common to see your CPU floating above 100% and/or GPU over 100% like these screens below:\n![SNAP-1](https://i.ibb.co/3cFD5Hs/cut-1.png)\n![SNAP-2](https://i.ibb.co/G5qFRxj/cut-2.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_y = np.array(train_labels) == 'pos'\ntest_y = np.array(test_labels) == 'pos'\ntrain_y.shape, test_y.shape, np.mean(train_y), np.mean(test_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now Masking few random IDs from each sentences to remove Biasness from model."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\ntest_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Baseline"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"baseline_model = make_pipeline(CountVectorizer(ngram_range=(1,3)), LogisticRegression()).fit(train_texts, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"baseline_predicted = baseline_model.predict(test_texts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(test_labels, baseline_predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Our baseline model is working just fine and yeilding a fair enough score. Now, its time to play Dirty with the \"BERT\"."},{"metadata":{},"cell_type":"markdown","source":"# BERT Model\n\n\n### Bidirectional Encoder Representations from Transformers. Each word here has a meaning to it and we will encounter that one by one in this article. For now, the key takeaway from this line is â€“ **BERT is based on the Transformer architecture**."},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertBinaryClassifier(nn.Module):\n    def __init__(self, dropout=0.1):\n        super(BertBinaryClassifier, self).__init__()\n\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n\n        self.dropout = nn.Dropout(dropout)\n        self.linear = nn.Linear(768, 1)\n        self.sigmoid = nn.Sigmoid()\n    \n    def forward(self, tokens, masks=None):\n        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n        dropout_output = self.dropout(pooled_output)\n        linear_output = self.linear(dropout_output)\n        proba = self.sigmoid(linear_output)\n        return proba","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ensuring that the model runs on GPU, not on CPU\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_clf = BertBinaryClassifier()\nbert_clf = bert_clf.cuda()     # running BERT on CUDA_GPU","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = torch.tensor(train_tokens_ids[:3]).to(device)\ny, pooled = bert_clf.bert(x, output_all_encoded_layers=False)\nx.shape, y.shape, pooled.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = bert_clf(x)\ny.cpu().detach().numpy()        # kinda Garbage Collector to free up used and cache space","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross- checking CUDA GPU Memory to ensure GPU memory is not overflowing.\nstr(torch.cuda.memory_allocated(device)/1000000 ) + 'M'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y, x, pooled = None, None, None\ntorch.cuda.empty_cache()     # Clearing Cache space for fresh Model run\nstr(torch.cuda.memory_allocated(device)/1000000 ) + 'M'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fine Tune BERT"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting hyper-parameters\n\nBATCH_SIZE = 4\nEPOCHS = 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tokens_tensor = torch.tensor(train_tokens_ids)\ntrain_y_tensor = torch.tensor(train_y.reshape(-1, 1)).float()\n\ntest_tokens_tensor = torch.tensor(test_tokens_ids)\ntest_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()\n\ntrain_masks_tensor = torch.tensor(train_masks)\ntest_masks_tensor = torch.tensor(test_masks)\n\nstr(torch.cuda.memory_allocated(device)/1000000 ) + 'M'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\ntrain_sampler = RandomSampler(train_dataset)\ntrain_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n\ntest_dataset = TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\ntest_sampler = SequentialSampler(test_dataset)\ntest_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_optimizer = list(bert_clf.sigmoid.named_parameters()) \noptimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = Adam(bert_clf.parameters(), lr=3e-6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.empty_cache()   # Clearing Cache space for a fresh Model run","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch_num in range(EPOCHS):\n    bert_clf.train()\n    train_loss = 0\n    for step_num, batch_data in enumerate(train_dataloader):\n        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n        print(str(torch.cuda.memory_allocated(device)/1000000 ) + 'M')\n        logits = bert_clf(token_ids, masks)\n        \n        loss_func = nn.BCELoss()\n\n        batch_loss = loss_func(logits, labels)\n        train_loss += batch_loss.item()\n        \n        \n        bert_clf.zero_grad()\n        batch_loss.backward()\n        \n\n        clip_grad_norm_(parameters=bert_clf.parameters(), max_norm=1.0)\n        optimizer.step()\n        \n        clear_output(wait=True)\n        print('Epoch: ', epoch_num + 1)\n        print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(train_data) / BATCH_SIZE, train_loss / (step_num + 1)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**NOte :** This will take anything around 20 to 30 minutes on Kaggle GPU. I literally felt asleep. But,that was a **POWER NAP** for me ;-) .\n\n### You can go, and Grab some Coffee for yourself, or call someone you Love. They need your time, especially your MOM :-).\n\n\n![Coffee Break](https://media.harpersbazaar.com.sg/2019/04/coffee-feat1-700x350.jpg)"},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_clf.eval()\nbert_predicted = []\nall_logits = []\nwith torch.no_grad():\n    for step_num, batch_data in enumerate(test_dataloader):\n\n        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n\n        logits = bert_clf(token_ids, masks)\n        loss_func = nn.BCELoss()\n        loss = loss_func(logits, labels)\n        numpy_logits = logits.cpu().detach().numpy()\n        \n        bert_predicted += list(numpy_logits[:, 0] > 0.5)\n        all_logits += list(numpy_logits[:, 0])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(bert_predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(test_y, bert_predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### A much better score with BERT.\n\n**P.S.** - Since, you have come this far, I hope you Liked and got to learn something new. Its time to show your **LOVE** with an upvote. **All kind of Feedbacks are welcomed**! "},{"metadata":{},"cell_type":"markdown","source":"# Thank You! Please UPVOTE to show your support."},{"metadata":{},"cell_type":"markdown","source":"![BYE BYE](https://bloximages.newyork1.vip.townnews.com/mdjonline.com/content/tncms/assets/v3/editorial/6/7c/67cfcc90-ec48-11e9-a76f-bffb4b3ee322/5da0b4cd79df1.image.jpg?resize=400%2C266)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}