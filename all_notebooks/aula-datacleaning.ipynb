{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Treinamento de Data Cleaning & Manipulation\n\nObs: Devido a limitação da minha máquina, optei por utilizar o kaggle para apresentar esta aula. Mas não foi possível subir o dataset do enem de 2017. Deste modo, a parte de estudar um pouco sobre as falhas não deveser incluida no dataset de 2016.\n\n![Alpha](https://cdn-images-1.medium.com/max/800/1*wdXqc19MFEcIplg5Pzazuw.png)\n\nOla, meu nome é Felipe Sibuya. Atualmente sou membro do @ __[Grupo Turing](https://www.facebook.com/grupoturing.poliusp/)__, sendo responsável da área de Comunidade do grupo. Durante 1 ano dentro do grupo, tive a oportunidade de participar da organização do workshop de libras e do Hackaturing.\n\n## Introdução\n\nNa aula de hoje vou apresentar uma introdução ao Data Cleaning & Manipulation. Mas o que é Data Cleaning & Manipulation?\n\n\n### Vamos lembrar sobre o Data Science Pipeline\n  1. Problem Understanding / Scope Definition\n  2. Goal Definition and Metrics Setting\n  3. Determine the required data\n  4. Data Acquisition\n  5. Data Cleaning & Manipulation\n  6. Exploratory Data Analysis (E.D.A.)\n  7. Feature Engineering\n  8. Build and Evaluate your model\n  9. Result Interpretation & Reporting/Data Storytelling\n  10. Deployment\n  11. Monitoring & Maintenance\n  \nData cleaning & Manipulation é o 5° passo que todo DS deve realizar em seu projeto. Os objetivos de realizar tal etapa são: \n - Entender os dados coletados\n - Estruturar os dados de maneira manipulável\n - Encontrar e tratar informações incompletas, erradas e inconsistentes\n - Tratar types, palavras e categorias\n - Juntar datasets\n \n \n ![Alpha](https://thumbor.forbes.com/thumbor/960x0/https%3A%2F%2Fblogs-images.forbes.com%2Fgilpress%2Ffiles%2F2016%2F03%2FTime-1200x511.jpg)\n \n ![Alpha](https://thumbor.forbes.com/thumbor/960x0/https%3A%2F%2Fblogs-images.forbes.com%2Fgilpress%2Ffiles%2F2016%2F03%2FLeast-Enjoyable4-1200x511.jpg)\n \n  ![Alpha](https://pbs.twimg.com/media/DbzG1kPX0AEFHdt.png)"},{"metadata":{},"cell_type":"markdown","source":"## O que você precisa para fazer este notebook em casa?\n - __[Microdados Enem 2016](http://download.inep.gov.br/microdados/microdados_enem2016.zip)__\n - Um PC com pelo menos 8 Gb de memória RAM\n - Jupyter ou Jupyter Lab (Recomendo baixar o __[Anaconda](https://www.anaconda.com/distribution/)__)\n - Tempo\n\n\n## Vamos começar"},{"metadata":{},"cell_type":"markdown","source":"## Pandas\nPandas é uma biblioteca open source, de alta performace, fácil de trabalhar com estruturas de dados e análise de dados para Python.\n\n__[Documentação](https://pandas.pydata.org/)__"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\n\n# Serão importados somente para suporte\nimport numpy as np\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Carregando Dataset\n\nApós você adquirir os dados para o projeto, você deve importar para utiliza-los. A biblioteca pandas tem suporte para a leitura de diversos tipos de arquivos:\n - __[CSV](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)__\n - __[Excel](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html#pandas.read_excel)__\n - __[JSON](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html#pandas.read_json)__\n - __[SQL](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html#pandas.read_sql)__\n - __[HTML](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_html.html#pandas.read_html)__"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.listdir('/kaggle/input')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"arq=pd.read_csv(\"/kaggle/input/microdados_enem_2016_coma.csv\", engine=\"c\", error_bad_lines=False,\n                   encoding = \"latin\", sep=',')"},{"metadata":{"trusted":true},"cell_type":"code","source":"arq=pd.read_csv(\"/kaggle/input/microdados_enem_2016_coma.csv\", engine=\"python\", \n                   encoding = \"latin\", sep=',',nrows=1000000, error_bad_lines=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"   - engine: Pode ser utilizado Python ou c. Python é mais lento na leitura dos dados, mas em compensação ele é mais consegue completar de modo mais completo.\n   - encoding: Padrão de codificação dos caracteres. O mais usual nos arquivos é o UTF-8. __[Standard Encodings](https://docs.python.org/3/library/codecs.html#standard-encodings)__\n   - sep: Delimitador de texto. Por padrão é ','."},{"metadata":{},"cell_type":"markdown","source":"### Entendendo o seu dataset\nAgora que criamos o nosso Dataframe, vamos entender os nossos dados. Vamos utilizar as seguintes funções:\n - __[pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html)__\n - __[pandas.DataFrame.head](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html)__\n - __[pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html)__\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"arq.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arq.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arq.describe()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"Devido ao alto volume de colunas (137), as funções foram otimizadas para não mostrar todas elas :(\n\nMas é possível alterar o número de linhas e colunas mostradas :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"arq.info(verbose=True, null_counts=True)\n#Por padrão, ele só mostra todos os elementos se o número de colunas for menor que pandas.options.display.max_info_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows', 500) # Número máximo de linhas mostradas\npd.set_option('display.max_columns', 500) # Número máximo de colunas mostradas","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Para mais informações sobre __[pandas.set_option](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.set_option.html)__."},{"metadata":{"trusted":true},"cell_type":"code","source":"arq.head() #Por padrão n=5, ou seja, o número de linhas mostradas, mas da para se alterar nos argumentos","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arq.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arq.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arq","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Filtering & Slicing\n\nFiltering e Slicing (\"Filtragem e fatiamento\") são técnicas usadas para isolar parte do dataframe, podendo ser linhas, colunas e células. Muito comum queremos usar parte do nossos dados, dadas algumas condições específicas, para que podemos estudá-las. O Pandas apresenta ferramentas para realizar tais funções.\n\n### Diferença entre Series e Dataframe\n\nTanto Series quanto Dataframe são objetos do Pandas.\n - *Series* nada mais é que um array de 1 dimensão. Você pode considerar um Series também como uma coluna de uma tabela\n - Um *DataFrame* é simplesmente um conjunto de Series. Trata-se de uma estrutura de dados de 2 dimensões — colunas e linhas — que transforma os dados em uma bela tabela.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"arq.NU_INSCRICAO.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(arq.NU_INSCRICAO)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arq[\"NU_INSCRICAO\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(arq[\"NU_INSCRICAO\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arq[[\"NU_INSCRICAO\"]].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(arq[[\"NU_INSCRICAO\"]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Slicing\nNo Pandas há 2 maneiras de separar dados, baseando-se nos indexes e colunas. Você pode separar escolhendo as colunas que você deseja ou separar utilizando os métodos .loc e .iloc.\n\nPara começar, irei optar pelo de escolha das colunas. Vamos pensar que eu gostaria de somente saber o número de inscrição e o estado de moradia do candidato do Enem."},{"metadata":{"trusted":true},"cell_type":"code","source":"esta_candidato=arq[['NU_INSCRICAO','SG_UF_RESIDENCIA']].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"esta_candidato","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### .iloc\nO método .illoc é utilizado somente para a escolha de elementos utilizando valores numéricos nas posições X, Y ((x+1)° linha, (y+1)° coluna). Como ele funciona:\n\ndf.iloc[linhas, colunas]\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"arq.iloc[0] # Devolve todos os elementos da linha 0 na forma de Pandas.Series","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arq.iloc[[0]] # Devolve todos os elementos da linha 0 na forma de Pandas.Dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arq.iloc[0:5,5]# Devolve os elementos 0 a 4 da coluna 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arq.iloc[0:5,0:6]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arq.iloc[[0,3,4],[0,1,5]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### .loc\nO recurso .loc é mais versátil que o .iloc, pois além de números, é possível escolher as colunas por sua nomenclatura. Como ele funciona:\n\ndf.loc[linhas, colunas]"},{"metadata":{"trusted":true},"cell_type":"code","source":"arq.loc[[0]] # O que vale para o .iloc vale para o .loc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arq.loc[0:5,0] # Verificamos que as colunas não podem ser dados de forma numérica","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arq.loc[0:5,\"NU_INSCRICAO\"] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arq.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# setando um index para podermos trabalhar com o nome das linhas\narq_copy=arq.set_index(\"NU_INSCRICAO\")\n\n#verificando a alteração\narq_copy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arq.head() #verificamos que o df inicial não sofreu alterações","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arq_copy.loc[160000036736]\n# Como NU_INSCRICAO é um inteiro, não é necessário colocar entre \"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arq_copy.loc[[160000036736,160000063645,160000065051,160000003846],[\"NU_ANO\",\"TP_LINGUA\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Filtering\nAgora que sabemos isolar partes do dataset por suas localizações, vamos isolar os dados baseados em condições e caracteríticas. Uma vez que os dados carregam muitos grupos de dados, as vezes é interessante estuda-los de forma individual. No Pandas há ferramentas internas capazes de realizar tal tarefa.\n\nQue tal vermos os resultados dos alunos em Matemática?"},{"metadata":{"trusted":true},"cell_type":"code","source":"arq.NU_NOTA_MT.mean() #Verificamos a nota média de Matemática dos estudantes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arq[arq[\"NU_NOTA_MT\"]>900].NU_NOTA_MT.mean() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# é o mesmo que realizar\narq.loc[(arq['NU_NOTA_MT']) > 900].NU_NOTA_MT.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arq.loc[(arq['NU_NOTA_MT']) >= 900].head()\n#Trocar por .shpae para ver o número de pessoas que tiraram mais de 900","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n#### Vamos realizar um estudo das notas de Matemática por algumas UF's"},{"metadata":{"trusted":true},"cell_type":"code","source":"# São Paulo\narq[arq[\"SG_UF_RESIDENCIA\"]==\"SP\"].NU_NOTA_MT.mean() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rio de Janeiro\narq[arq[\"SG_UF_RESIDENCIA\"]==\"RJ\"].NU_NOTA_MT.mean() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Alagoas\narq[arq[\"SG_UF_RESIDENCIA\"]==\"AL\"].NU_NOTA_MT.mean() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Amazonas\narq[arq[\"SG_UF_RESIDENCIA\"]==\"AM\"].NU_NOTA_MT.mean() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A média nunca é um bom parâmetro para ser analisada únicamente. Vamos adicionar os quartis e o desvio padrão. Aproveitando, por que não avaliamos todas as notas?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# São Paulo\narq[arq[\"SG_UF_RESIDENCIA\"]==\"SP\"][[\"NU_NOTA_CN\",\"NU_NOTA_CH\",\"NU_NOTA_LC\",\n                                    \"NU_NOTA_MT\",\"NU_NOTA_REDACAO\"\n                                   ]].describe() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Se vamos fazer uma análise de sempre os mesmos dados, ou seja, das notas, por que não fazemos um dataframe com elas?"},{"metadata":{"trusted":true},"cell_type":"code","source":"arq_notas=arq[[\"SG_UF_RESIDENCIA\",\"NU_NOTA_CN\",\"NU_NOTA_CH\",\"NU_NOTA_LC\",\n                \"NU_NOTA_MT\",\"NU_NOTA_REDACAO\"]]\narq_notas.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#São Paulo\narq_notas[arq_notas[\"SG_UF_RESIDENCIA\"]==\"SP\"].describe() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#São Paulo\narq_notas[arq[\"SG_UF_RESIDENCIA\"]==\"SP\"].describe() \n\n# Como os dois dataframes compartilham o mesmo número de linhas, é possível referenciar\n# no primeiro dataframe a condicionaç","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Amazonas\narq_notas[arq_notas[\"SG_UF_RESIDENCIA\"]==\"AM\"].describe() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\nAnalisando os dados, verificamos que temos algumas notas 0. Podemos ter 2 dúvidas iniciais sobre esses dados:\n\n1. Será que apresenta muitos 0's no dataset?\n2. Será que esses 0's pressionam significantemente a média para baixo?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Para responder a pergunta 1\narq_elementos_nulos=arq[(arq.NU_NOTA_CN==0)|(arq.NU_NOTA_CH==0)|\n                       (arq.NU_NOTA_LC==0)|(arq.NU_NOTA_MT==0)|\n                       (arq.NU_NOTA_REDACAO==0)]\n\narq_elementos_nulos2=arq[(arq.NU_NOTA_CN==0)&(arq.NU_NOTA_CH==0)&\n                       (arq.NU_NOTA_LC==0)&(arq.NU_NOTA_MT==0)&\n                       (arq.NU_NOTA_REDACAO==0)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"apresenta 1 termo nulo : \",len(arq_elementos_nulos), ', todos nulos: ',len(arq_elementos_nulos2), \", total de linhas: \",len(arq))\n#print(\"apresenta 1 termo nulo, todos nulos, total de linhas\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Opa, somente 3 pessoas tem 0's em todas as matérias? Só 36367 tem nota nula em alguma matéria?  O que aconteceu?\n\nPesquisando rápido na internet, cheguei a seguinte noticia:\n***\n\"O segundo dia de prova do Exame Nacional do Ensino Médio (Enem) 2017, ocorrido neste domingo (12), teve 32% de abstenção – foram 2,1 milhões de candidatos ausentes, com 6,7 milhões de inscrições confirmadas.Esse é o maior índice de abstenção desde 2009, quando foram registradas 37,7% de ausência.\n\nNo total, 580 pessoas foram eliminadas no segundo dia, sendo que 578 foram por descumprimento das regras gerais do edital e duas por recusa da coleta do dado biométrico. No primeiro dia, foram 273 eliminações, somando 853 no total. Em 2016, o exame teve 3.942 eliminações ao final do primeiro dia e 4.780 no segundo.\"\n\nFonte: __[Segundo dia do Enem 2017 tem 32% de abstenção e 580 eliminados](https://guiadoestudante.abril.com.br/enem/segundo-dia-do-enem-2017-tem-32-de-abstencao-e-580-eliminados/)__\n***\n\nPodemos verificar que tem inconsistências nos dados, vamos voltar aos dados do inicio para estudar este problema antes de prosseguir na análise.\n\n## Estudo de inconsistência de valores\n"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"arq[[\"SG_UF_RESIDENCIA\",\"NU_NOTA_CN\",\"NU_NOTA_CH\",\"NU_NOTA_LC\",\n                \"NU_NOTA_MT\",\"NU_NOTA_REDACAO\"]].info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Olha aqui... Podemos ver que apesar de termos 1000000 na nossa base, não temos 1000000 de notas não nulas nas matéria. Conclusões?\n\nSerá que é mera conincidência termos o mesmo número de dados faltantes em NU_NOTA_CN com NU_NOTA_MT e termos também em NU_NOTA_CH com NU_NOTA_LC e NU_NOTA_REDACAO?"},{"metadata":{},"cell_type":"markdown","source":"***\n\nNa verdade não, tem uma razão. O Enem 2017 foi o primeiro a ter LC, CH e Redação no primeiro dia, enquanto CN e MT ficaram no segundo dia. É possível ter como hipótese que os dados faltantes são das pessoas que não fizeram a prova, já que os dados com 0 podem ser de pessoas que foram eliminadas do processo. É preciso investigar.\n\nVamos começar com os dados nulos."},{"metadata":{"trusted":false},"cell_type":"code","source":"arq_d2=arq[(arq.TP_PRESENCA_MT==0)&(arq.TP_PRESENCA_CN==0)]\narq_d22=arq[(arq.TP_PRESENCA_MT==2)|(arq.TP_PRESENCA_CN==2)]\narq_d1=arq[(arq.TP_PRESENCA_CH==0)&(arq.TP_PRESENCA_LC==0)]\narq_d11=arq[(arq.TP_PRESENCA_CH==2)|(arq.TP_PRESENCA_LC==2)]\n\nprint(len(arq_d2)+len(arq_d22), len(arq_d1))\nprint(1000000-len(arq_d2)-len(arq_d22), 1000000-len(arq_d1)-len(arq_d11))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Podemos ver que mesmo tentando tirar os alunos que foram eliminados e os que não realizaram a prova, não conseguimos chegar ao valor de notas não nulas. O que fazer?\n\nVamos avaliar quais são esses elementos:"},{"metadata":{"trusted":false},"cell_type":"code","source":"arq_ver=arq[(((arq.TP_PRESENCA_MT==1)&(arq.TP_PRESENCA_CN==1)))]\narq_d23=arq_ver[arq_ver.NU_NOTA_CN.isnull()]\narq_d23.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pera, não há nenhum candidato que fez a prova e não foi eliminado que apresenta nota nula.... Hm... O que acontece?\n\nVamos investigar outro ponto. Vamos ver qual a diferença entre o número de pessoas que fizeram a prova e o número de notas não nulas:\n\n1. 740413-740391=22\n2. 774372-774350=22\n\nPodemos verificar que a diferença é igual para ambos os dias. Agora falta o porque."},{"metadata":{"trusted":false},"cell_type":"code","source":"arq[[\"TP_PRESENCA_CN\",\n\"TP_PRESENCA_CH\",\n\"TP_PRESENCA_LC\",\n\"TP_PRESENCA_MT\"]].info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aqui está. Podemos verificar que temos 22 dados faltando na presença de alunos. Estes 22 são os candidatos que estavam faltando na nossa avaliação anterior. \n\nComo não sabemos como foram realizadas tais medições, não é possível saber o porque destes erros nesta base de dados :(\n\nAlgumas análises que podemos realizar para verificar possíveis motivos deste erro são:\n - Verificar em qual aplicação apresenta estes erros\n - Verificar a localidade do erro\n - Verificar se são candidatos com condições especiais\n - ...\n "},{"metadata":{},"cell_type":"markdown","source":"---\n## Trabalhando com valores faltantes\n\nAgora que entendemos um pouco sobrea complexidade dos valores nulos, vamos aprender como trata-los.\n\nExiste 2 funções muito usadas para esta situação, .dropna() e .fillna().\n - __[Fillna](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html)__:: neste método os valores faltantes são completados com algum valor. Podem ser utilizados a média, a mediana ou um valor fixo.\n - __[Dropna](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html)__: Neste método as linhas dos valores faltantes são retiradas. Há a opção de retirar somente valores faltantes que estejam em colunas específicas.\n \nA escolha entre estas 2 ferramentas varia muito em relação ao tamanho do seu dataset, as caractrísticas das colunas e o type dos elementos. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"arq_limpeza=arq #estou clonando o dataframe arq (inicial sem mudança) para arq_limpeza","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dropna\nVamos retirar os valores faltantes das notas. Como sabemos os motivos da maioria e aqueles 22, que não estão completos, são poucos perto do restante, podemos retirar nesta situação. Assim podemos avaliar quais estudantes realizaram os dois dias de provas e podemos tirar algumas conclusões sobre eles."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(arq_limpeza.shape)\narq_limpeza=arq_limpeza.dropna(subset=[\"NU_NOTA_CN\",\"NU_NOTA_CH\",\"NU_NOTA_LC\",\n                \"NU_NOTA_MT\",\"NU_NOTA_REDACAO\"])\nprint(arq_limpeza.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lembra da analise de notas? Vamos realizar novamente para reficiar se as médias mudaram."},{"metadata":{"trusted":true},"cell_type":"code","source":"arq_notas2=arq_limpeza[[\"SG_UF_RESIDENCIA\",\"NU_NOTA_CN\",\"NU_NOTA_CH\",\"NU_NOTA_LC\",\n                \"NU_NOTA_MT\",\"NU_NOTA_REDACAO\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# São Paulo Original\narq_notas[arq_notas[\"SG_UF_RESIDENCIA\"]==\"SP\"].describe() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#São Paulo com Dropna\narq_notas2[arq_notas2[\"SG_UF_RESIDENCIA\"]==\"SP\"].describe() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Amazonas Original\narq_notas[arq_notas[\"SG_UF_RESIDENCIA\"]==\"AM\"].describe() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Amazonas com Dropna\narq_notas2[arq_notas2[\"SG_UF_RESIDENCIA\"]==\"AM\"].describe() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fillna\nAgora vamos completar valores. Queremos avaliar o rendimento por escola. No CO_ESCOLA os alunos do 3° escolhem por qual escola estão prestando o Enem, sendo este valor um identificador da escola no Censo Escolar . O restante dos candidatos, sejam eles treineiros ou fora do terceiro ano do ensino médio, tem nesta coluna o valor faltando.\n\nPara realizar esta avalição, tenho o objetivo de também avaliar os alunos que estão fora do 3° ano do ensino médio. Vamos supor que estes candidatos recebam o código -100 de identificação da escola (avaliando é possível perceber que todos os códigos já registrados são números positivos, logo não há o risco de conflito).\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(arq[arq[\"CO_ESCOLA\"]<0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arq_limpeza=arq.copy() #estou clonando o dataframe arq (inicial sem mudança) para arq_limpeza","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arq_limpeza[\"CO_ESCOLA\"]=arq_limpeza[\"CO_ESCOLA\"].fillna(-100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"co_escolas=arq_limpeza[\"CO_ESCOLA\"].unique()\nprint(type(co_escolas))\nprint(\"número de escolas\",len(co_escolas))\nprint(\"número de alunos sem fillna\",len(arq_limpeza[arq_limpeza[\"CO_ESCOLA\"]>0][\"CO_ESCOLA\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arq_limpeza[arq_limpeza[\"CO_ESCOLA\"]==-100]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Completada as escolas, vamos ver qual é a média dos candidatos por escola.\n\n\n## Group By\nUtilizando o Pandas, nós podemos fazer operações baseadas em agrupamentos de características. __[Groupby](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html)__\n\nUtilizando o dataset arq_limpeza, podemos verificar a média das notas dos candidatos por escola."},{"metadata":{"trusted":true},"cell_type":"code","source":"arq_agrupamento=arq_limpeza.groupby('CO_ESCOLA')[[\"NU_NOTA_CN\",\"NU_NOTA_CH\",\"NU_NOTA_LC\",\n                \"NU_NOTA_MT\",\"NU_NOTA_REDACAO\"]].mean()\narq_agrupamento","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Criação de novas colunas\nFrequentemente após algumas mudanças nos datasets é verificado a falta de alguns dados para resolver os problemas centrais. Os dados que buscamos, caso possam ser retirados de outras features, podem ser tranquilamente gerados utilizando os ferramentais do Pandas.\n\nAgora que criamos um dataframe que apresenta a média das notas por escola, podemos também avaliar o desempenho médio da escola nessas notas.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"arq_agrupamento[\"MEDIA_NOTAS_MULT\"]=(arq_agrupamento.iloc[:,0:4].sum(axis=1))/4\narq_agrupamento[\"MEDIA_NOTAS_TOT\"]=(arq_agrupamento.iloc[:,0:5].sum(axis=1))/5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arq_agrupamento","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Mudança no nome da coluna\nQuando realizamos a extração de dados é comum que o nome das colunas estejam erradas ou com nomes estranhos. Para melhorar o seu trabalho, é possível e recomendado a alteração de nomes de colunas. As vezes esta alteração é necessária para evitar o conflito de nome de colunas quando realizamos merge com outros datasets.\n\nPara realizar tal operação nós usaremos a função __[Rename](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html)__.\n\nDentro do dataset é possível verificar que as últimas colunas são Qxxx, sendo x o número da questão feita no formulário de inscrição para todos os candidatos (felizmente não há dados faltandos nesta coluna). Vamos supor que queremos avaliar a relação entre nota média dos estudantes e a renda familiar média. Sabemos que a coluna Q006 é registrada as respostas para esta pergunta.\n\nComo queremos compreender melhor os dados, vamos alterar o nome desta coluna dentro do dataframe arq."},{"metadata":{"trusted":true},"cell_type":"code","source":"arq[[\"NU_INSCRICAO\",\"Q006\"]].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arq_nome=arq.copy()\narq_nome=arq_nome.rename(columns={\"Q006\": \"RENDA_FAMILAR\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arq_nome.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arq_rend_notas=arq_nome.groupby('RENDA_FAMILAR')[[\"NU_NOTA_CN\",\"NU_NOTA_CH\",\"NU_NOTA_LC\",\n                \"NU_NOTA_MT\",\"NU_NOTA_REDACAO\"]].mean()\narq_rend_notas","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Alteração de valores da coluna\nComo verificamos no dataframe anterior, as letras de A a Q referem-se aos valores das rendas familiares dos candidatos. Apesar de sua interpretabilidade, alguns algoritmos de ML tem dificuldade de trabalhar com strings e valores não numéricos. No tratamento de dados é importante transformar esses valores para a máxima eficiência dos algoritmos no momento de modelagem.\n\nExistem 3 boas opções para você tratar dados categoricos. As opções são:\n 1.  Alterar os valores da categoria para valores numéricos, seguindo uma ordem escalar de crescimento igual a 1\n     - Este método não é recomendado quando a relação de categorias não segue a mesma ordem de crescimento/decrescimento de valores discretos.\n     - Caso a taxa de crescimento seja linear, tal método talvez possa ser usado sem grandes problemas.\n     - Exemplo: A = Minha casa tem 0 Banheiro, B= Minha casa tem 1 banheiro ---> A=0, B=1.\n 2. Alterar os valores e substitui-los por números, de modo que representem um valor com relação lógica.\n     - Este método não é recomendado quando a relação de categorias não segue a mesma ordem de crescimento/decrescimento\n     - Exemplo: A = 'BRL 0,00', B= 'BRL 0,00 - BRL 10,00' ---> A=0, B=5\n 3. Utilizar __[Get_dummies](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html)__\n     - As categorias serão divididas em um maior numero de colunas, em que cada coluna será um valor binário se aquela linha aplica-se a tal categoria da coluna.\n     - Recomendada quando os dados categóricos não tem qualquer relação polinomial.\n     - Exemplo:\n     \n| ID | Comida   |\n|------|------|\n|  1  |Banana|\n| 2 | Pudim|\n\n| ID | Banana   | Pudim |\n|------|------|------|\n|  1  |1|0|\n| 2 | 0|1|"},{"metadata":{},"cell_type":"markdown","source":"***\n### Alteração de valores categóricos para numéricos\n\nVerificando a tabela da renda familiar e as notas por critério, verificamos que a renda familiar está dividida por categorias. A categoria \"A\" é referente as pessoas com menor renda familiar, enquanto a categoria \"Q\" é referente aos com maior renda familiar. Uma vez que temos os valores das rendas por categoria, iremos substitui-las pelos valores numéricos nos quais estas se referem.\n\n| Categoria | Renda   |\n|------|------|\n| A |Nenhuma renda.|\n| B | Até 937,00.|\n| C | De 937,01 até 1.405,50.|\n| D | De 1.405,51 até 1.874,00.|\n| E | De 1.874,01 até 2.342,50.|\n| F | De 2.342,51 até 2.811,00.|\n| G | De 2.811,01 até 3.748,00.|\n| H | De 3.748,01 até 4.685,00.|\n| I | De 4.685,01 até 5.622,00.|\n| J | De 5.622,01 até 6.559,00.|\n| K | De 6.559,01 até 7.496,00.|\n| L | De 7.496,01 até 8.433,00.|\n| M | De 8.433,01 até 9.370,00.|\n| N | De 9.370,01 até 11.244,00.|\n| O | De 11.244,01 até 14.055,00.|\n| P | De 14.055,01 até 18.740,00.|\n| Q | Mais de 18.740,00.|\n\nA princípio será optado utilizar o limite inferior para substituir os valores. Desta maneira surge um problema, como fazer com a categoria A e B?\n\nPor opção para demonstração, será utilizado o valor inferior. No caso de B será substituido pelo valor de 0, enquanto A será -100.\n\nPara esta função será utilizado __[replace](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html)__."},{"metadata":{"trusted":true},"cell_type":"code","source":"arq_nome=arq.copy()\narq_nome=arq_nome.rename(columns={\"Q006\": \"RENDA_FAMILAR\"})\narq_nome=arq_nome.replace({\"RENDA_FAMILAR\":{\n    \"A\": -100,\n    \"B\":0,\n    \"C\":937,\n    \"D\":1405.5,\n    \"E\":1874,\n    \"F\":2342.5,\n    \"G\":2811,\n    \"H\":3748,\n    \"I\":4685,\n    \"J\":5622,\n    \"K\":6559,\n    \"L\":7496,\n    \"M\":8433,\n    \"N\":9370,\n    \"O\":11244,\n    \"P\":14055,\n    \"Q\":18740\n}})\narq_graph=arq_nome.groupby('RENDA_FAMILAR')[[\"NU_NOTA_CN\",\"NU_NOTA_CH\",\"NU_NOTA_LC\",\n                \"NU_NOTA_MT\",\"NU_NOTA_REDACAO\"]].mean()\narq_graph","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nx=[-100,\n    0,\n    937,\n    1405.5,\n    1874,\n    2342.5,\n    2811,\n    3748,\n    4685,\n    5622,\n    6559,\n    7496,\n    8433,\n    9370,\n    11244,\n    14055,\n    18740]\ny1=arq_graph[\"NU_NOTA_CN\"]\ny2=arq_graph[\"NU_NOTA_CH\"]\ny3=arq_graph[\"NU_NOTA_LC\"]\ny4=arq_graph[\"NU_NOTA_MT\"]\ny5=arq_graph[\"NU_NOTA_REDACAO\"]\n\nplt.plot(x, y1, '.')\nplt.plot(x, y2, '.')\nplt.plot(x, y3, '.')\nplt.plot(x, y4, '.')\nplt.plot(x, y5, '.')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get_Dummies\n A pergunta Q003 é :\n \n \"A partir da apresentação de algumas ocupações divididas em grupos ordenados, indique o grupo que contempla a ocupação mais próxima da ocupação do seu pai ou do homem responsável por você. (Se ele não estiver trabalhando, escolha uma ocupação pensando no último trabalho dele).\"\n \n Sendo as seguintes respostas possíveis\n 1. A\tGrupo 1: Lavrador, agricultor sem empregados, boia-fria, criador de animais (gado, porcos, galinhas, ovelhas, cavalos etc.), apicultor, pescador, lenhador, seringueiro, extrativista.\n 2. B\tGrupo 2: Diarista, empregado doméstico, cuidador de idosos, babá, cozinheiro (em casas particulares), motorista particular, jardineiro, faxineiro de empresas e prédios, vigilante, porteiro, carteiro, office-boy, vendedor, caixa, atendente de loja, auxiliar administrativo, recepcionista, servente de pedreiro, repositor de mercadoria.\n 3. C\tGrupo 3: Padeiro, cozinheiro industrial ou em restaurantes, sapateiro, costureiro, joalheiro, torneiro mecânico, operador de máquinas, soldador, operário de fábrica, trabalhador da mineração, pedreiro, pintor, eletricista, encanador, motorista, caminhoneiro, taxista.\n 4. D\tGrupo 4: Professor (de ensino fundamental ou médio, idioma, música, artes etc.), técnico (de enfermagem, contabilidade, eletrônica etc.), policial, militar de baixa patente (soldado, cabo, sargento), corretor de imóveis, supervisor, gerente, mestre de obras, pastor, microempresário (proprietário de empresa com menos de 10 empregados), pequeno comerciante, pequeno proprietário de terras, trabalhador autônomo ou por conta própria.\n 5. E\tGrupo 5: Médico, engenheiro, dentista, psicólogo, economista, advogado, juiz, promotor, defensor, delegado, tenente, capitão, coronel, professor universitário, diretor em empresas públicas ou privadas, político, proprietário de empresas com mais de 10 empregados.\n 6. F\tNão sei.\n\nPodemos avaliar que não há uma relação numérica/polinomial entre estas categorias. Para transformar de modo numérico, que sejá possível utilizar nos algoritmos, iremos utilizar a função __[get_dummies](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html)__."},{"metadata":{"trusted":true},"cell_type":"code","source":"arq_getdummies=pd.get_dummies(arq, columns=['Q003'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arq_getdummies.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note que a coluna Q003 desapareceu, surgindo no final do datafrema as colunas Q003_A\tQ003_B\tQ003_C\tQ003_D\tQ003_E\tQ003_F"},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"## Drop Columns\nDurante esse tutorial você percebeu que muita das 157 colunas não foram utilizadas? Muito dos dados coletados não são necessários após as modificaçõese limpezas realizadas. Então o que fazer com estes dados?\n\nUma solução muito simples é tirar eles do seu dataframe.\n\nPara esta função será utilizado __[drop](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html)__."},{"metadata":{"trusted":true},"cell_type":"code","source":"lista=arq.columns\nprint(lista[:50])\nprint(lista[50:100])\nprint(lista[100:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arq = arq.drop(['IN_BAIXA_VISAO',\n       'IN_CEGUEIRA', 'IN_SURDEZ', 'IN_DEFICIENCIA_AUDITIVA',\n       'IN_SURDO_CEGUEIRA', 'IN_DEFICIENCIA_FISICA', 'IN_DEFICIENCIA_MENTAL',\n       'IN_DEFICIT_ATENCAO', 'IN_DISLEXIA', 'IN_DISCALCULIA', 'IN_AUTISMO',\n       'IN_VISAO_MONOCULAR', 'IN_OUTRA_DEF', 'IN_GESTANTE', 'IN_LACTANTE',\n       'IN_IDOSO', 'IN_ESTUDA_CLASSE_HOSPITALAR', 'IN_SEM_RECURSO',\n       'IN_BRAILLE', 'IN_AMPLIADA_24', 'IN_AMPLIADA_18', 'IN_LEDOR','IN_ACESSO', 'IN_TRANSCRICAO', 'IN_LIBRAS', 'IN_LEITURA_LABIAL',\n       'IN_MESA_CADEIRA_RODAS', 'IN_MESA_CADEIRA_SEPARADA', 'IN_APOIO_PERNA',\n       'IN_GUIA_INTERPRETE', 'IN_COMPUTADOR', 'IN_CADEIRA_ESPECIAL',\n       'IN_CADEIRA_CANHOTO', 'IN_CADEIRA_ACOLCHOADA', 'IN_PROVA_DEITADO',\n       'IN_MOBILIARIO_OBESO', 'IN_LAMINA_OVERLAY', 'IN_PROTETOR_AURICULAR',\n       'IN_MEDIDOR_GLICOSE', 'IN_MAQUINA_BRAILE', 'IN_SOROBAN',\n       'IN_MARCA_PASSO', 'IN_SONDA', 'IN_MEDICAMENTOS', 'IN_SALA_INDIVIDUAL',\n       'IN_SALA_ESPECIAL', 'IN_SALA_ACOMPANHANTE', 'IN_MOBILIARIO_ESPECIFICO',\n       'IN_MATERIAL_ESPECIFICO', 'IN_NOME_SOCIAL'], axis=1)\narq.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arq.info() # Verificamos uma redução no uso de memória","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# .apply()\nUtilizado para aplicar funções nos termos"},{"metadata":{"trusted":true},"cell_type":"code","source":"arq[\"coluna_nova\"]=arq['Q003'].apply(lambda x: x.lower())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arq.coluna_nova","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lowr(x):\n    return x.lower()\narq[\"coluna_nova2\"]=arq['Q003'].apply(lowr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arq.coluna_nova2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Merge"},{"metadata":{"trusted":true},"cell_type":"code","source":"escola_UF= arq[[\"CO_ESCOLA\",\"CO_UF_ESC\"]]\nescola_UF=escola_UF.drop_duplicates()\nescola_UF=escola_UF.dropna()\nescola_UF.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"escola_MU= arq[[\"CO_ESCOLA\",\"CO_MUNICIPIO_ESC\"]]\nescola_MU=escola_MU.drop_duplicates()\nescola_MU=escola_MU.dropna()\nescola_MU.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"escola_MU.merge(escola_UF, on=\"CO_ESCOLA\", how=\"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"!pip install missingno","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import missingno as msno\n%matplotlib inline\nmsno.matrix(arq[['NU_INSCRICAO', 'NU_ANO', 'CO_MUNICIPIO_RESIDENCIA',\n       'NO_MUNICIPIO_RESIDENCIA', 'CO_UF_RESIDENCIA', 'SG_UF_RESIDENCIA',\n       'NU_IDADE', 'TP_SEXO', 'TP_ESTADO_CIVIL', 'TP_COR_RACA',\n       'TP_NACIONALIDADE', 'CO_MUNICIPIO_NASCIMENTO']].sample(1000000))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"https://medium.com/@rrfd/cleaning-and-prepping-data-with-python-for-data-science-best-practices-and-helpful-packages-af1edfbe2a3","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":1}