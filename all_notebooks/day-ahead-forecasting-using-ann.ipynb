{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(r'/kaggle/input/russian-wholesale-electricity-market/RU_Electricity_Market_PZ_dayahead_price_volume.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a total of five columns that includes : The timestamp of every observation, the consumption of European and Siberian zones, and the prices of European and Siberian Zones.\n\nThe data has hourly resolution, which means a total of 24 slots any given day. Thus, in order to make day ahead predictions, we would need to predict for all the 24 slots.\n\nWe will design 4 separate neural networks, that can be used for predicting each of the four attributes. \n\nThe most popular statictical tool used for time-series forecasting is the ARIMA model, which is based on auto-regression. Our ANNs will also use the autoregression concept in order to make future predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thankfully, no NaN values. \n\nThe power consumptions are already of float type (numerical).\n\nHowever, the prices are of object type (maybe because they have \",\" in them alongside numbers). Let's deal with that first.\n\nAlso, I like my dataframe to have a \"datetime\" type index."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['price_eur'] = df['price_eur'].apply(lambda x : float(x.replace(',', '')))\n\ndf['price_sib'] = df['price_sib'].apply(lambda x : float(x.replace(',', '')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['datetime'] = pd.to_datetime(df['timestep'])\n\ndf.drop(['timestep'], axis=1, inplace=True)\n\ndf.set_index(['datetime'], drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay, great. Everything's perfect now. Let's plot and see each of the four time-series."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (9,9))\n\nplt.subplot(4, 1, 1)\ndf['consumption_eur'].plot()\n\nplt.subplot(4, 1, 2)\ndf['consumption_sib'].plot()\n\nplt.subplot(4, 1, 3)\ndf['price_eur'].plot()\n\nplt.subplot(4, 1, 4)\ndf['price_sib'].plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, the power consumptions seem to have a nice symmetrical cyclic pattern.\nThe prices though seem to be quite volatile, but not unexpeted certainly.\nThe prices also seem to have a lot of values that are zero before the year 2010.\n\nSince we are going to use a form of auto-regression, we are only going to use historical data of any particular attribute to make predictions for that attribute.\n\nLet's do some autocorrelation analysis for each of the four attributes."},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa import stattools\n\nacf_cons_eur = stattools.acf(df['consumption_eur'], unbiased=True, nlags=100)\nacf_cons_sib = stattools.acf(df['consumption_sib'], unbiased=True, nlags=100)\nacf_price_eur = stattools.acf(df['price_eur'], unbiased=True, nlags=100)\nacf_price_sib = stattools.acf(df['price_sib'], unbiased=True, nlags=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (9,11))\n\nplt.subplot(4, 1, 1)\npd.Series(acf_cons_eur).plot()\nplt.grid()\n\nplt.subplot(4, 1, 2)\npd.Series(acf_cons_sib).plot()\nplt.grid()\n\nplt.subplot(4, 1, 3)\npd.Series(acf_price_eur).plot()\nplt.grid()\n\nplt.subplot(4, 1, 4)\npd.Series(acf_price_sib).plot()\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On the x-axis we have the lag, and on the y-axis we have the correlation value.\n\nBy a lag of one, we mean the past hour time instant. A lag of 2 implies the instant two hours ago, and so on...\n\nBy correlation, we mean the strength of associability between two variables. In auto-correlation, the strength is measured on the same variable between it's current value and it's historical values.\n\nA correlation value of zero means no asscoiabilty whatsoever, and as it approcahes one, the associabilty increases. As one can guess, correlation of any instant with itself will be one.\n\nThe objective of plotting all the auto-correlations with the past time instants (lags) is to see which ones are highly related (or have highly similar traits). We select those lags which give the highest correlation valuesand use them as inputs for our prediction model.\n\nFor the consumption attributes, we have a lot of lags that give more than 0.9 correlation. For the prices however, not so many. We will get the list of all the lags we need and create four seperate datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"autocorrs = pd.Series(acf_cons_eur)\n\nlist_of_regressors_cons_eur = autocorrs.loc[autocorrs > 0.9].index\n\nlist_of_regressors_cons_eur = list_of_regressors_cons_eur[1:11]\n\nlist_of_regressors_cons_eur","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We want to have atleast 10 lags as regressor input variables for each model, provided they have a correlation of 0.9 at least. \n\nNote that we don't have it as [0:10] since the first element in that list is the lag '0', i.e the curent time-instant itself (target variable for our models) which obviously can't be used as an input variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"autocorrs = pd.Series(acf_cons_sib)\n\nlist_of_regressors_cons_sib = autocorrs.loc[autocorrs > 0.9].index\n\nlist_of_regressors_cons_sib = list_of_regressors_cons_sib[1:11]\n\nlist_of_regressors_cons_sib","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"autocorrs = pd.Series(acf_price_eur)\n\nlist_of_regressors_price_eur = autocorrs.loc[autocorrs > 0.8].index\n\nlist_of_regressors_price_eur = list_of_regressors_price_eur[1:11]\n\nlist_of_regressors_price_eur","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"autocorrs = pd.Series(acf_price_sib)\n\nlist_of_regressors_price_sib = autocorrs.loc[autocorrs > 0.85].index\n\nlist_of_regressors_price_sib = list_of_regressors_price_sib[1:11]\n\nlist_of_regressors_price_sib","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create a function that when given a time-series can add the regressor variables as separate columns. Such a dataframe can be easily used for training a deep neural network."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_regressor_attributes(df, attribute, list_of_prev_t_instants) :\n    \n    \"\"\"\n    Ensure that the index is of datetime type\n    Creates features with previous time instant values\n    \"\"\"\n        \n    list_of_prev_t_instants.sort_values\n    start = list_of_prev_t_instants[-1] \n    end = len(df)\n    df['datetime'] = df.index\n    df.reset_index(drop=True)\n\n    df_copy = df[start:end]\n    df_copy.reset_index(inplace=True, drop=True)\n\n    for attribute in attribute :\n            foobar = pd.DataFrame()\n\n            for prev_t in list_of_prev_t_instants :\n                new_col = pd.DataFrame(df[attribute].iloc[(start - prev_t) : (end - prev_t)])\n                new_col.reset_index(drop=True, inplace=True)\n                new_col.rename(columns={attribute : '{}_(t-{})'.format(attribute, prev_t)}, inplace=True)\n                foobar = pd.concat([foobar, new_col], sort=False, axis=1)\n\n            df_copy = pd.concat([df_copy, foobar], sort=False, axis=1)\n            \n    df_copy.set_index(['datetime'], drop=True, inplace=True)\n    return df_copy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cons_eur = df.loc[:, [ 'consumption_eur']]\ndf_consum_eur = create_regressor_attributes(cons_eur, ['consumption_eur'], list_of_regressors_cons_eur)\ndf_consum_eur.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cons_sib = df.loc[:, [ 'consumption_sib']]\ndf_consum_sib = create_regressor_attributes(cons_sib, ['consumption_sib'], list_of_regressors_cons_sib)\ndf_consum_sib.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For creating the datasets for price attributes, we will only the data after 2010 from the original dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_after_2010 = df.loc[df.index.year >= 2010]\ndf_after_2010.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"price_eur = df_after_2010.loc[:, [ 'price_eur']]\ndf_price_eur = create_regressor_attributes(price_eur, ['price_eur'], list_of_regressors_price_eur)\ndf_price_eur.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"price_sib = df_after_2010.loc[:, [ 'price_sib']]\ndf_price_sib = create_regressor_attributes(price_sib, ['price_sib'], list_of_regressors_price_sib)\ndf_price_sib.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Okay, we have our four datasets. We now need to build our four ANN models. \nBut before we need to split the data into test, train, and validation sets. \nWe also need to normalize out data.\nLet's create a function that does both."},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_test_valid_split_plus_scaling(df, valid_set_size, test_set_size):\n    \n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import MinMaxScaler\n    \n    df_copy = df.reset_index(drop=True)\n    \n    df_test = df_copy.iloc[ int(np.floor(len(df_copy)*(1-test_set_size))) : ]\n    df_train_plus_valid = df_copy.iloc[ : int(np.floor(len(df_copy)*(1-test_set_size))) ]\n\n    df_train = df_train_plus_valid.iloc[ : int(np.floor(len(df_train_plus_valid)*(1-valid_set_size))) ]\n    df_valid = df_train_plus_valid.iloc[ int(np.floor(len(df_train_plus_valid)*(1-valid_set_size))) : ]\n\n\n    X_train, y_train = df_train.iloc[:, 1:], df_train.iloc[:, 0]\n    X_valid, y_valid = df_valid.iloc[:, 1:], df_valid.iloc[:, 0]\n    X_test, y_test = df_test.iloc[:, 1:], df_test.iloc[:, 0]\n    \n    global Target_scaler\n    \n    Target_scaler = MinMaxScaler(feature_range=(0.01, 0.99))\n    Feature_scaler = MinMaxScaler(feature_range=(0.01, 0.99))\n    \n    X_train_scaled = Feature_scaler.fit_transform(np.array(X_train))\n    X_valid_scaled = Feature_scaler.fit_transform(np.array(X_valid))\n    X_test_scaled = Feature_scaler.fit_transform(np.array(X_test))\n    \n    y_train_scaled = Target_scaler.fit_transform(np.array(y_train).reshape(-1,1))\n    y_valid_scaled = Target_scaler.fit_transform(np.array(y_valid).reshape(-1,1))\n    y_test_scaled = Target_scaler.fit_transform(np.array(y_test).reshape(-1,1))\n    \n    print('Shape of training inputs, training target:', X_train_scaled.shape, y_train_scaled.shape)\n    print('Shape of validation inputs, validation target:', X_valid_scaled.shape, y_valid_scaled.shape)\n    print('Shape of test inputs, test targets:', X_test_scaled.shape, y_test_scaled.shape)\n\n    return X_train_scaled, X_valid_scaled, X_test_scaled, y_train_scaled, y_valid_scaled, y_test_scaled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_set_size = 0.1\ntest_set_size = 0.1\nX_train, X_valid, X_test, y_train, y_valid, y_test = train_test_valid_split_plus_scaling(df_consum_eur, \n                                                                                         valid_set_size, \n                                                                                        test_set_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's import all the stuff required for building our Neural Networks"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Input, Dense, Dropout\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.models import Model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will create an ANN model for predicting the consumer_eur part"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_layer = Input(shape=9, dtype='float32')\ndense1 = Dense(27, activation='linear')(input_layer)\ndense2 = Dense(18, activation='linear')(dense1)\ndense3 = Dense(18, activation='linear')(dense2)\ndropout_layer = Dropout(0.2)(dense2)\noutput_layer = Dense(1, activation='linear')(dropout_layer)\n\nconsum_eur_model = Model(inputs=input_layer, outputs=output_layer)\nconsum_eur_model.compile(loss='mean_squared_error', optimizer='adam')\nconsum_eur_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"consum_eur_model.fit(x=X_train, y=y_train, batch_size=100, epochs=25, verbose=1, validation_data=(X_valid, y_valid), shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we will predict our trained model on the test set that we separated earlier. \n\nTo judge the model's performane, we will evaluate the r2 score of the model's predicted values, and also plot them along with the true values from our test set.\n\nAs we will have to do this for the other three attributes as well, let's create a function which does all that."},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_model_on_test_set(model, df, X_test, y_test, Target_scaler):\n    \n    #Recall that we have already defined \"Target_scaler\" as global in the 'split n scale' function earlier \n    \n    y_pred = model.predict(X_test)\n    y_pred_rescaled = Target_scaler.inverse_transform(y_pred)\n    \n    y_test_rescaled =  Target_scaler.inverse_transform(y_test)\n    \n    from sklearn.metrics import r2_score\n    score = r2_score(y_test_rescaled, y_pred_rescaled)\n    print('R-squared score for the test set:', round(score,4))\n    \n    y_actual = pd.DataFrame(y_test_rescaled, columns=['Actual'])\n    y_actual.set_index(df.index[ int(np.floor((1- test_set_size)*len(df))) : ], inplace=True)\n\n    y_hat = pd.DataFrame(y_pred_rescaled, columns=['Predicted'])\n    y_hat.set_index(df.index[ int(np.floor((1- test_set_size)*len(df))) : ], inplace=True)\n    \n    plt.figure(figsize=(7, 5))\n    plt.plot(y_actual[500:600], linestyle='solid', color='r')   #plotting only a few values for better visibility\n    plt.plot(y_hat[500:600], linestyle='dashed', color='b')\n    plt.legend(['Actual','Predicted'], loc='best', prop={'size': 14})\n    plt.title('{}'.format(df.columns[0]), weight='bold', fontsize=16)\n    plt.ylabel('Value', weight='bold', fontsize=14)\n    plt.xlabel('Date', weight='bold', fontsize=14)\n    plt.xticks(weight='bold', fontsize=12, rotation=45)\n    plt.yticks(weight='bold', fontsize=12)\n    plt.grid(color = 'y', linewidth='0.5')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run_model_on_test_set(consum_eur_model, df_consum_eur, X_test, y_test, Target_scaler)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay, now let's repeat the same for other three attributes."},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_set_size = 0.1\ntest_set_size = 0.1\nX_train, X_valid, X_test, y_train, y_valid, y_test = train_test_valid_split_plus_scaling(df_consum_sib, \n                                                                                         valid_set_size, \n                                                                                        test_set_size)\n\ninput_layer = Input(shape=10, dtype='float32')        #Refer again the number of regressors that we had fixed for this particular attribute\ndense1 = Dense(30, activation='linear')(input_layer)\ndense2 = Dense(20, activation='linear')(dense1)\ndense3 = Dense(20, activation='linear')(dense2)\ndropout_layer = Dropout(0.2)(dense2)\noutput_layer = Dense(1, activation='linear')(dropout_layer)\n\nconsum_sib_model = Model(inputs=input_layer, outputs=output_layer)\nconsum_sib_model.compile(loss='mean_squared_error', optimizer='adam')\nconsum_sib_model.summary()\n\nconsum_sib_model.fit(x=X_train, y=y_train, batch_size=100, epochs=25, verbose=1, validation_data=(X_valid, y_valid), shuffle=True)\n\nrun_model_on_test_set(consum_sib_model, df_consum_sib, X_test, y_test, Target_scaler)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_set_size = 0.1\ntest_set_size = 0.1\nX_train, X_valid, X_test, y_train, y_valid, y_test = train_test_valid_split_plus_scaling(df_price_eur, \n                                                                                         valid_set_size, \n                                                                                        test_set_size)\n\ninput_layer = Input(shape=10, dtype='float32')        \ndense1 = Dense(30, activation='linear')(input_layer)\ndense2 = Dense(20, activation='linear')(dense1)\ndense3 = Dense(20, activation='linear')(dense2)\ndropout_layer = Dropout(0.2)(dense2)\noutput_layer = Dense(1, activation='linear')(dropout_layer)\n\nprice_eur_model = Model(inputs=input_layer, outputs=output_layer)\nprice_eur_model.compile(loss='mean_squared_error', optimizer='adam')\nprice_eur_model.summary()\n\nprice_eur_model.fit(x=X_train, y=y_train, batch_size=50, epochs=25, verbose=1, validation_data=(X_valid, y_valid), shuffle=True)\n\nrun_model_on_test_set(price_eur_model, df_price_eur, X_test, y_test, Target_scaler)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_set_size = 0.1\ntest_set_size = 0.1\nX_train, X_valid, X_test, y_train, y_valid, y_test = train_test_valid_split_plus_scaling(df_price_sib, \n                                                                                         valid_set_size, \n                                                                                        test_set_size)\n\ninput_layer = Input(shape=10, dtype='float32')        \ndense1 = Dense(30, activation='linear')(input_layer)\ndense2 = Dense(20, activation='linear')(dense1)\ndense3 = Dense(20, activation='linear')(dense2)\ndropout_layer = Dropout(0.2)(dense2)\noutput_layer = Dense(1, activation='linear')(dropout_layer)\n\nprice_sib_model = Model(inputs=input_layer, outputs=output_layer)\nprice_sib_model.compile(loss='mean_squared_error', optimizer='adam')\nprice_sib_model.summary()\n\nprice_sib_model.fit(x=X_train, y=y_train, batch_size=50, epochs=25, verbose=1, validation_data=(X_valid, y_valid), shuffle=True)\n\nrun_model_on_test_set(price_sib_model, df_price_sib, X_test, y_test, Target_scaler)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, we finally have out four trained models that have given satisfactory results on the test sets:\n\n* consum_eur_model for predicting day ahead consumption_eur values\n* consum_sib_model for predicting day ahead consumption_sib values\n* price_eur_model for predicting day ahead price_eur values\n* price_sib_model for predicting day ahead price_sib values\n\nPredictions can be made for the coming day by scaling a very miniscule part of true historical data gathered till now (keeping in mind the lags (past time-instants) that are to be used as input regressors for these models).\n\nAll one needs to do is create a small dataset of those past values, normalize it using the MinMax scaler and then predict using the corresponding model. Then rescale the predictions back into their original form.\n\nOne can very easily use a different set of regresorrs and train a different models as well."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}