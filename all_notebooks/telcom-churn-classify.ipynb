{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 align=\"center\"> Telcom Churn: Classify </h1>\n\n<img src=\"https://austindatascience.files.wordpress.com/2017/11/screen-shot-2017-11-19-at-4-00-11-pm.png\" width=\"50%\" />\n\nCreated: 2020-09-18\n\nLast updated: 2020-09-19\n\nKaggle Kernel made by ðŸš€ <a href=\"https://www.kaggle.com/rafanthx13\"> Rafael Morais de Assis</a>"},{"metadata":{},"cell_type":"markdown","source":"## Problem Description\n\nhttps://www.kaggle.com/blastchar/telco-customer-churn"},{"metadata":{},"cell_type":"markdown","source":"## Table Of Content (TOC) <a id=\"top\"></a>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Configs\npd.options.display.float_format = '{:,.4f}'.format\nsns.set(style=\"whitegrid\")\nplt.style.use('seaborn')\nseed = 42\nnp.random.seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"file_path = '/kaggle/input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv'\ndf = pd.read_csv(file_path)\nprint(\"DataSet = {:,d} rows and {} columns\".format(df.shape[0], df.shape[1]))\n\nprint(\"\\nAll Columns:\\n=>\", df.columns.tolist())\n\nquantitative = [f for f in df.columns if df.dtypes[f] != 'object']\nqualitative = [f for f in df.columns if df.dtypes[f] == 'object']\n\nprint(\"\\nStrings Variables:\\n=>\", qualitative,\n      \"\\n\\nNumerics Variables:\\n=>\", quantitative)\n\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Snippets"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import accuracy_score, f1_score\n\nthis_labels = ['No Churn','Churn']\nscoress = {}\n\ndef class_report(y_real, y_my_preds, name=\"\", labels=this_labels):\n    if(name != ''):\n        print(name,\"\\n\")\n    print(confusion_matrix(y_real, y_my_preds), '\\n')\n    print(classification_report(y_real, y_my_preds, target_names=labels))\n    scoress[name] = [accuracy_score(y_test, y_pred), f1_score(y_test, y_pred, average='macro')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\n\ndef time_spent(time0):\n    t = time.time() - time0\n    t_int = int(t) // 60\n    t_min = t % 60\n    if(t_int != 0):\n        return '{} min {:.3f} s'.format(t_int, t_min)\n    else:\n        return '{:.3f} s'.format(t_min)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# statistics\nfrom scipy import stats\nfrom scipy.stats import norm, skew, boxcox_normmax #for some statistics\nfrom scipy.special import boxcox1p\n\ndef test_normal_distribution(serie, series_name='series', thershold=0.4):\n    f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18, 6), sharex=False)\n    f.suptitle('{} is a Normal Distribution?'.format(series_name), fontsize=18)\n    ax1.set_title(\"Histogram to \" + series_name)\n    ax2.set_title(\"Q-Q-Plot to \"+ series_name)\n    mu, sigma = norm.fit(serie)\n    print('Normal dist. (mu= {:,.2f} and sigma= {:,.2f} )'.format(mu, sigma))\n    skewness = serie.skew()\n    kurtoise = serie.kurt()\n    print(\"Skewness: {:,.2f} | Kurtosis: {:,.2f}\".format(skewness, kurtoise))\n    pre_text = '\\t=> '\n    if(skewness < 0):\n        text = pre_text + 'negatively skewed or left-skewed'\n    else:\n        text =  pre_text + 'positively skewed or right-skewed\\n'\n        text += pre_text + 'in case of positive skewness, log transformations usually works well.\\n'\n        text += pre_text + 'np.log(), np.log1(), boxcox1p()'\n    if(skewness < -1 or skewness > 1):\n        print(\"Evaluate skewness: highly skewed\")\n        print(text)\n    if( (skewness <= -0.5 and skewness > -1) or (skewness >= 0.5 and skewness < 1)):\n        print(\"Evaluate skewness: moderately skewed\")\n        print(text)\n    if(skewness >= -0.5 and skewness <= 0.5):\n        print('Evaluate skewness: approximately symmetric')\n    print('evaluate kurtoise')\n    if(kurtoise > 3 + thershold):\n        print(pre_text + 'Leptokurtic: anormal: Peak is higher')\n    elif(kurtoise < 3 - thershold):\n        print(pre_text + 'Platykurtic: anormal: The peak is lower')\n    else:\n        print(pre_text + 'Mesokurtic: normal: the peack is normal')\n    sns.distplot(serie , fit=norm, ax=ax1)\n    ax1.legend(['Normal dist. ($\\mu=$ {:,.2f} and $\\sigma=$ {:,.2f} )'.format(mu, sigma)],\n            loc='best')\n    ax1.set_ylabel('Frequency')\n    stats.probplot(serie, plot=ax2)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_top_bottom_rank_correlation(my_df, column_target, top_rank=5, title=''):\n    corr_matrix = my_df.corr()\n    f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(20, 7), sharex=False)\n    if(title):\n        f.suptitle(title)\n\n    ax1.set_title('Top {} Positive Corr to {}'.format(top_rank, column_target))\n    ax2.set_title('Top {} Negative Corr to {}'.format(top_rank, column_target))\n    \n    cols_top = corr_matrix.nlargest(top_rank+1, column_target)[column_target].index\n    cm = np.corrcoef(my_df[cols_top].values.T)\n    mask = np.zeros_like(cm)\n    mask[np.triu_indices_from(mask)] = True\n    hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f',\n                     annot_kws={'size': 11}, yticklabels=cols_top.values,\n                     xticklabels=cols_top.values, mask=mask, ax=ax1)\n    \n    cols_bot = corr_matrix.nsmallest(top_rank, column_target)[column_target].index\n    cols_bot  = cols_bot.insert(0, column_target)\n    cm = np.corrcoef(my_df[cols_bot].values.T)\n    mask = np.zeros_like(cm)\n    mask[np.triu_indices_from(mask)] = True\n    hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f',\n                     annot_kws={'size': 11}, yticklabels=cols_bot.values,\n                     xticklabels=cols_bot.values, mask=mask, ax=ax2)\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_balanced_train_test_binary(x_train, y_train, x_test, y_test, original_size, labels):\n    \"\"\" To binary classification\n    each paramethes is pandas.core.frame.DataFrame\n    @total_size = len(X) before split\n    @labels = labels in ordem [0,1 ...]\n    \"\"\"\n    train_unique_label, train_counts_label = np.unique(y_train, return_counts=True)\n    test_unique_label, test_counts_label = np.unique(y_test, return_counts=True)\n\n    prop_train = train_counts_label/ len(y_train)\n    prop_test = test_counts_label/ len(y_test)\n\n    print(\"Original Size:\", '{:,d}'.format(original_size))\n    print(\"\\nTrain: must be 80% of dataset:\\n\", \n          \"the train dataset has {:,d} rows\".format(len(x_train)),\n          'this is ({:.2%}) of original dataset'.format(len(x_train)/original_size),\n                \"\\n => Classe 0 ({}):\".format(labels[0]), train_counts_label[0], '({:.2%})'.format(prop_train[0]), \n                \"\\n => Classe 1 ({}):\".format(labels[1]), train_counts_label[1], '({:.2%})'.format(prop_train[1]),\n          \"\\n\\nTest: must be 20% of dataset:\\n\",\n          \"the test dataset has {:,d} rows\".format(len(x_test)),\n          'this is ({:.2%}) of original dataset'.format(len(x_test)/original_size),\n                  \"\\n => Classe 0 ({}):\".format(labels[0]), test_counts_label[0], '({:.2%})'.format(prop_test[0]),\n                  \"\\n => Classe 1 ({}):\".format(labels[1]),test_counts_label[1], '({:.2%})'.format(prop_test[1])\n         )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eda_categ_feat_desc_plot(series_categorical, title = \"\", fix_labels=False):\n    \"\"\"Generate 2 plots: barplot with quantity and pieplot with percentage. \n       @series_categorical: categorical series\n       @title: optional\n       @fix_labels: The labes plot in barplot in sorted by values, some times its bugs cuz axis ticks is alphabethic\n           if this happens, pass True in fix_labels\n       @bar_format: pass {:,.0f} to int\n    \"\"\"\n    series_name = series_categorical.name\n    val_counts = series_categorical.value_counts()\n    val_counts.name = 'quantity'\n    val_percentage = series_categorical.value_counts(normalize=True)\n    val_percentage.name = \"percentage\"\n    val_concat = pd.concat([val_counts, val_percentage], axis = 1)\n    val_concat.reset_index(level=0, inplace=True)\n    val_concat = val_concat.rename( columns = {'index': series_name} )\n    \n    fig, ax = plt.subplots(figsize = (12,4), ncols=2, nrows=1) # figsize = (width, height)\n    if(title != \"\"):\n        fig.suptitle(title, fontsize=18)\n        fig.subplots_adjust(top=0.8)\n\n    s = sns.barplot(x=series_name, y='quantity', data=val_concat, ax=ax[0])\n    if(fix_labels):\n        val_concat = val_concat.sort_values(series_name).reset_index()\n    \n    for index, row in val_concat.iterrows():\n        s.text(row.name, row['quantity'], '{:,d}'.format(int(row['quantity'])), color='black', ha=\"center\")\n\n    s2 = val_concat.plot.pie(y='percentage', autopct=lambda value: '{:.2f}%'.format(value),\n                             labels=val_concat[series_name].tolist(), legend=None, ax=ax[1],\n                             title=\"Percentage Plot\")\n\n    ax[1].set_ylabel('')\n    ax[0].set_title('Quantity Plot')\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eda_numerical_feat(series, title=\"\", with_label=True, number_format=\"\", show_describe=False, size_labels=10):\n    # Use 'series_remove_outiliers' to filter outiliers\n    \"\"\" Generate series.describe(), bosplot and displot to a series\n    @with_label: show labels in boxplot\n    @number_format: \n        integer: \n            '{:d}'.format(42) => '42'\n            '{:,d}'.format(12855787591251) => '12,855,787,591,251'\n        float:\n            '{:.0f}'.format(91.00000) => '91' # no decimal places\n            '{:.2f}'.format(42.7668)  => '42.77' # two decimal places and round\n            '{:,.4f}'.format(1285591251.78) => '1,285,591,251.7800'\n            '{:.2%}'.format(0.09) => '9.00%' # Percentage Format\n        string:\n            ab = '$ {:,.4f}'.format(651.78) => '$ 651.7800'\n    def swap(string, v1, v2):\n        return string.replace(v1, \"!\").replace(v2, v1).replace('!',v2)\n    # Using\n        swap(ab, ',', '.')\n    \"\"\"\n    f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18, 5), sharex=False)\n    if(show_describe):\n        print(series.describe())\n    if(title != \"\"):\n        f.suptitle(title, fontsize=18)\n    sns.distplot(series, ax=ax1)\n    sns.boxplot(series, ax=ax2)\n    if(with_label):\n        describe = series.describe()\n        labels = { 'min': describe.loc['min'], 'max': describe.loc['max'], \n              'Q1': describe.loc['25%'], 'Q2': describe.loc['50%'],\n              'Q3': describe.loc['75%']}\n        if(number_format != \"\"):\n            for k, v in labels.items():\n                ax2.text(v, 0.3, k + \"\\n\" + number_format.format(v), ha='center', va='center', fontweight='bold',\n                         size=size_labels, color='white', bbox=dict(facecolor='#445A64'))\n        else:\n            for k, v in labels.items():\n                ax2.text(v, 0.3, k + \"\\n\" + str(v), ha='center', va='center', fontweight='bold',\n                     size=size_labels, color='white', bbox=dict(facecolor='#445A64'))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df.isnull(), cbar=False, yticklabels=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.duplicated().sum()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['TotalCharges'] = df['TotalCharges'].replace(\" \", 0).astype('float32')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA\n\n### Each feature individually"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eda_categ_feat_desc_plot(df['gender'], title = \"gender distribution\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eda_categ_feat_desc_plot(df['SeniorCitizen'], title = \"SeniorCitizen distribution\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eda_categ_feat_desc_plot(df['Partner'], title = \"Partner distribution\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PhoneService\neda_categ_feat_desc_plot(df['PhoneService'], title = \"PhoneService distribution\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PhoneService\neda_categ_feat_desc_plot(df['MultipleLines'], title = \"MultipleLines distribution\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PhoneService\neda_categ_feat_desc_plot(df['InternetService'], title = \"InternetService distribution\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PhoneService\neda_categ_feat_desc_plot(df['OnlineSecurity'], title = \"OnlineSecurity distribution\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# InternetService\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# InternetService\neda_categ_feat_desc_plot(df['InternetService'], title = \"InternetService distribution\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eda_numerical_feat(df['MonthlyCharges'], title=\"MonthlyCharges distribution\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eda_categ_feat_desc_plot(df['Churn'], title = \"Churn distribution\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Churn by other features"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"## Correlations"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['StreamingMovies'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yes_no = {'No':0, 'Yes': 1}\ngender = {'Female':0, 'Male':1}\n\ndf1 = df.copy().drop(['customerID'], axis=1)\n\ndf1['Churn'] = df1['Churn'].replace(yes_no)\ndf1['PaperlessBilling'] = df1['PaperlessBilling'].replace(yes_no)\ndf1['Partner'] = df1['Partner'].replace(yes_no)\ndf1['Dependents'] = df1['Dependents'].replace(yes_no)\ndf1['PhoneService'] = df1['PhoneService'].replace(yes_no)\n\ndf1['gender'] = df1['gender'].replace(gender)\n\nmultiple_lines = pd.get_dummies(df1['MultipleLines'], prefix='ML')\ninternet_service = pd.get_dummies(df1['InternetService'], prefix='IS')\nonline_security = pd.get_dummies(df1['OnlineSecurity'], prefix='OS')\nonline_backup = pd.get_dummies(df1['OnlineBackup'], prefix='OB')\n\ndevice_protection = pd.get_dummies(df1['DeviceProtection'], prefix='DP')\ntech_support = pd.get_dummies(df1['TechSupport'], prefix='TS')\nstreaming_tv = pd.get_dummies(df1['StreamingTV'], prefix='ST')\nstreaming_movies = pd.get_dummies(df1['StreamingMovies'], prefix='SM')\n\ncontract = pd.get_dummies(df1['Contract'], prefix='Contr')\npayment_method = pd.get_dummies(df1['PaymentMethod'], prefix='PM')\n\ndummies_columns = [multiple_lines, internet_service, online_security, online_backup,\n                  device_protection, tech_support, streaming_tv, streaming_movies,\n                  contract, payment_method]\n\ndf1['TotalCharges'] = df1['TotalCharges'].replace(\" \", 0).astype('float32')\n\ndf1 = pd.concat([df1, *dummies_columns], axis=1)\n\ndf1 = df1.drop(['MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup',\n               'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',\n               'Contract', 'PaymentMethod'], axis=1)\n\ndf1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abc = plot_top_bottom_rank_correlation(df1, 'Churn', top_rank=12, title='Top Cors')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split Train and Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = df1.drop(['Churn'], axis=1)\n\ny = df1['Churn']\n\nx_train, x_test, y_train, y_test = train_test_split(X, y.values, test_size=0.20, random_state=42)\n\ncheck_balanced_train_test_binary(x_train, y_train, x_test, y_test, len(df), ['Response 0', 'Response 1'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Handle Unbalanced DataSet"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN, SVMSMOTE, BorderlineSMOTE\nfrom imblearn.under_sampling import RandomUnderSampler, TomekLinks, NearMiss\nfrom imblearn.combine import SMOTEENN, SMOTETomek # over and under sampling\nfrom imblearn.metrics import classification_report_imbalanced\n\nimb_models = {\n    'ADASYN': ADASYN(random_state=42),\n    'SMOTE': SMOTE(random_state=42),\n    'SMOTEENN': SMOTEENN(\"minority\", random_state=42),\n    'SMOTETomek': SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'), random_state=42),\n    'RandomUnderSampler': RandomUnderSampler(random_state=42)\n}\n\nimb_strategy = \"None\"\n\nif(imb_strategy != \"None\"):\n    before = x_train.shape[0]\n    imb_tranformer = imb_models[imb_strategy]\n    x_train, y_train = imb_tranformer.fit_sample(x_train, y_train)\n    print(\"train dataset before: {:,d}\\nimbalanced_strategy: {}\".format(before, imb_strategy),\n          \"\\ntrain dataset after: {:,d}\\ngenerate: {:,d}\".format(x_train.shape[0], x_train.shape[0] - before))\nelse:\n    print(\"Dont correct unbalanced dataset\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classifier Libraries\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\n\n# Ensemble Classifiers\nfrom sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n\n# Others Linear Classifiers\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier\nfrom sklearn.linear_model import Perceptron, PassiveAggressiveClassifier\n\n# xboost\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\n# scores\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n\n# neural net of sklearn\nfrom sklearn.neural_network import MLPClassifier\n\n# others\nimport time\nimport operator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_classifiers = {\n    \"NaiveBayes\": GaussianNB(),\n    \"Ridge\": RidgeClassifier(),\n    \"Perceptron\": Perceptron(),\n    \"PassiveAggr\": PassiveAggressiveClassifier(),\n    \"XGBoost\": XGBClassifier(),\n    \"LightGB\": LGBMClassifier(boosting_type='gbdt',n_estimators=500,depth=10,learning_rate=0.04,objective='binary',\n                 colsample_bytree=0.5,reg_lambda=2,reg_alpha=2,random_state=294,n_jobs=-1),\n    \"SVM\": SVC(),\n    \"LogisiticR\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n#     \"DecisionTree\": DecisionTreeClassifier(),\n    \"AdaBoost\": AdaBoostClassifier(), # All 100 features: 48min\n    # \"SGDC\": SGDClassifier(),\n    \"GBoost\": GradientBoostingClassifier(),\n#     \"Bagging\": BaggingClassifier(),\n    \"RandomForest\": RandomForestClassifier(),\n    \"ExtraTree\": ExtraTreesClassifier()\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics = { 'cv_acc': {}, 'acc_test': {}, 'f1_test': {} }\nm = list(metrics.keys())\ntime_start = time.time()\nprint('CrossValidation, Fitting and Testing')\n\n# Cross Validation, Fit and Test\nfor name, model in all_classifiers.items():\n    print('{:15}'.format(name), end='')\n    t0 = time.time()\n    # Cross Validation\n    training_score = cross_val_score(model, x_train, y_train, scoring=\"accuracy\", cv=4)\n    # Fitting\n    all_classifiers[name] = model.fit(x_train, y_train) \n    # Testing\n    y_pred = all_classifiers[name].predict(x_test)\n    t1 = time.time()\n    # Save metrics\n    metrics[m[0]][name] = training_score.mean()\n    metrics[m[1]][name] = accuracy_score(y_test, y_pred)\n    metrics[m[2]][name] = f1_score(y_test, y_pred, average=\"macro\") \n    # Show metrics\n    print('| {}: {:6,.4f} | {}: {:6,.4f} | {}: {:6.4f} | took: {:>15} |'.format(\n        m[0], metrics[m[0]][name], m[1], metrics[m[1]][name],\n        m[2], metrics[m[2]][name], time_spent(t0) ))\n        \nprint(\"\\nDone in {}\".format(time_spent(time_start)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"````\nAll Dumies\nCrossValidation, Fitting and Testing :: Done in 29.003 s\nNaiveBayes     | cv_acc: 0.6876 | acc_test: 0.6977 | f1_test: 0.6798 | took:         0.061 s |\nRidge          | cv_acc: 0.7993 | acc_test: 0.8233 | f1_test: 0.7575 | took:         0.068 s |\nPerceptron     | cv_acc: 0.7501 | acc_test: 0.4627 | f1_test: 0.4612 | took:         0.091 s |\nPassiveAggr    | cv_acc: 0.6734 | acc_test: 0.7473 | f1_test: 0.7162 | took:         0.110 s |\nXGBoost        | cv_acc: 0.7806 | acc_test: 0.7956 | f1_test: 0.7203 | took:         2.424 s |\nLightGB        | cv_acc: 0.7922 | acc_test: 0.8084 | f1_test: 0.7357 | took:         2.910 s |\nSVM            | cv_acc: 0.7345 | acc_test: 0.7353 | f1_test: 0.4237 | took:         7.705 s |\nLogisiticR     | cv_acc: 0.8023 | acc_test: 0.8226 | f1_test: 0.7620 | took:         0.503 s |\nKNearest       | cv_acc: 0.7597 | acc_test: 0.7771 | f1_test: 0.6945 | took:         0.515 s |\nAdaBoost       | cv_acc: 0.8000 | acc_test: 0.8141 | f1_test: 0.7476 | took:         1.960 s |\nGBoost         | cv_acc: 0.8008 | acc_test: 0.8091 | f1_test: 0.7380 | took:         4.865 s |\nRandomForest   | cv_acc: 0.7840 | acc_test: 0.7999 | f1_test: 0.7180 | took:         3.814 s |\nExtraTree      | cv_acc: 0.7758 | acc_test: 0.7871 | f1_test: 0.7013 | took:         3.974 s |\n\n\n````"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best cv acc  :\", max( metrics[m[0]].items(), key=operator.itemgetter(1) ))\nprint(\"Best acc test:\", max( metrics[m[1]].items(), key=operator.itemgetter(1) ))\nprint(\"Best f1 test :\", max( metrics[m[2]].items(), key=operator.itemgetter(1) ))\n\ndf_metrics = pd.DataFrame(data = [list(metrics[m[0]].values()),\n                                  list(metrics[m[1]].values()),\n                                  list(metrics[m[2]].values())],\n                          index = ['cv_acc', 'acc_test', 'f1_test' ],\n                          columns = metrics[m[0]].keys() ).T.sort_values(by=m[2], ascending=False)\ndf_metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostClassifier\nfrom sklearn.metrics import accuracy_score\n\nname = 'CatBoost'\ncatb = CatBoostClassifier()\n\nt0 = time.time()\n# Fitting\ncatb = catb.fit(x_train, y_train, eval_set=(x_test, y_test), plot=False, early_stopping_rounds=30,verbose=0)\n# catb = catb.fit(x_train, y_train, cat_features=cat_col, eval_set=(x_test, y_test), plot=False, early_stopping_rounds=30,verbose=0) \n# Testing\ny_pred = catb.predict(x_test)\nt1 = time.time()\n# Save metrics\nmetrics[m[0]][name] = 0.0\nmetrics[m[1]][name] = accuracy_score(y_test, y_pred)\nmetrics[m[2]][name] = f1_score(y_test, y_pred, average=\"macro\") \n\n# Show metrics\nprint('{:15} | {}: {:6,.4f} | {}: {:6.4f} | took: {:>15} |'.format(\n    name, m[1], metrics[m[1]][name],\n    m[2], metrics[m[2]][name], time_spent(t0) ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_importances = pd.Series(catb.feature_importances_, index=X.columns)\nfeat_importances.nlargest(25).plot(kind='barh')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyper Tuning Logistic R"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\ndef optimize_logistic_r(mx_train, my_train, my_hyper_params, hyper_to_search, hyper_search_name, cv=4, scoring='accuracy'):\n    \"\"\"search best param to unic one hyper param\n    @mx_train, @my_train = x_train, y_train of dataset\n    @my_hyper_params: dict with actuals best_params: start like: {}\n      => will be accumulated and modified with each optimization iteration\n      => example stater: best_hyper_params = {'random_state': 42, 'n_jobs': -1}\n    @hyper_to_search: dict with key @hyper_search_name and list of values to gridSearch:\n    @hyper_search_name: name of hyperparam\n    \"\"\"\n    if(hyper_search_name in my_hyper_params.keys()):\n        del my_hyper_params[hyper_search_name]\n    if(hyper_search_name not in hyper_to_search.keys()):\n        raise Exception('\"hyper_to_search\" dont have {} in dict'.format(hyper_search_name))\n        \n    t0 = time.time()\n        \n    rf = LogisticRegression(**my_hyper_params)\n    \n    grid_search = GridSearchCV(estimator = rf, param_grid = hyper_to_search, \n      scoring = scoring, n_jobs = -1, cv = cv)\n    grid_search.fit(mx_train, my_train)\n    \n    print('took', time_spent(t0))\n    \n    data_frame_results = pd.DataFrame(\n        data={'mean_fit_time': grid_search.cv_results_['mean_fit_time'],\n        'mean_test_score_'+scoring: grid_search.cv_results_['mean_test_score'],\n        'ranking': grid_search.cv_results_['rank_test_score']\n         },\n        index=grid_search.cv_results_['params']).sort_values(by='ranking')\n    \n    print('The Best HyperParam to \"{}\" is {} with {} in {}'.format(\n        hyper_search_name, grid_search.best_params_[hyper_search_name], grid_search.best_score_, scoring))\n    \n    my_hyper_params[hyper_search_name] = grid_search.best_params_[hyper_search_name]\n    \n    \"\"\"\n    @@my_hyper_params: my_hyper_params appends best param find to @hyper_search_name\n    @@data_frame_results: dataframe with statistics of gridsearch: time, score and ranking\n    @@grid_search: grid serach object if it's necessary\n    \"\"\"\n    return my_hyper_params, data_frame_results, grid_search","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_hyper_params = {'random_state': 42, 'n_jobs': -1} # Stater Hyper Params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search_hyper = {'penalty': ['l1', 'l2', 'elasticnet', 'none']}\n\nbest_hyper_params, results, last_grid_search = optimize_logistic_r(\n    x_train, y_train, best_hyper_params, search_hyper, 'penalty')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search_hyper = {'C': [0.0001, 0.001, 0.01, 0.1, 1.0, 2.0 , 4.0, 8.0, 16.0, 32.0, 64.0]}\n\nbest_hyper_params, results, last_grid_search = optimize_logistic_r(\n    x_train, y_train, best_hyper_params, search_hyper, 'C')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search_hyper = {'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n\nbest_hyper_params, results, last_grid_search = optimize_logistic_r(\n    x_train, y_train, best_hyper_params, search_hyper, 'solver')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# last_grid_search\n\ny_pred = all_classifiers['LogisiticR'].predict(x_test)\nprint(accuracy_score(y_test, y_pred))\nclass_report(y_test, y_pred, name=\"LogisiticR\")\n\ny_pred = last_grid_search.predict(x_test)\nprint(accuracy_score(y_test, y_pred))\nclass_report(y_test, y_pred, name=\"LogisiticR0\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# all_classifiers['LogisiticR'].get_params()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Super Leaner"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom mlens.ensemble import SuperLearner\n \n# create a list of base-models\ndef get_models():\n    models = list()\n    models.append(LogisticRegression(**best_hyper_params))\n    models.append(DecisionTreeClassifier())\n    models.append(XGBClassifier())\n    models.append(AdaBoostClassifier())\n    models.append(CatBoostClassifier(verbose=0))\n    models.append(RandomForestClassifier())\n    models.append(LGBMClassifier())\n    return models\n \n# create the super learner\ndef get_super_learner(X):\n    ensemble = SuperLearner(scorer=accuracy_score, folds=5, shuffle=True, sample_size=len(X), verbose=0)\n    # add base models\n    models = get_models()\n    ensemble.add(models)\n    # add the meta model\n    ensemble.add_meta(LogisticRegression(**best_hyper_params))\n    return ensemble","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nt0 = time.time()\n\n# create the super learner\nensemble = get_super_learner(x_train.values)\n\n# fit the super learner\nensemble.fit(x_train.values, y_train)\n\n# summarize base learners\nprint(ensemble.data)\n\n# make predictions on hold out set\ny_pred = ensemble.predict(x_test.values)\n\nprint(\"took \", time_spent(t0))\nclass_report(y_test, y_pred, name=\"SuperLeaner\")\n\n# y_probs = ensemble.predict_proba(x_test.values)\n\n# roc_auc_score(y_test, y_probs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Best Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = all_classifiers['LogisiticR'].predict(x_test)\nprint(accuracy_score(y_test, y_pred))\nclass_report(y_test, y_pred, name=\"LogisiticR\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## pyCaret\n\nhttps://www.kaggle.com/frtgnn/pycaret-introduction-classification-regression"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install pycaret","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pycaret.classification import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pycaret = df.copy().drop(['customerID'],axis=1)\n\ndf_pycaret['Churn'] = df_pycaret['Churn'].replace(yes_no)\ndf_pycaret['SeniorCitizen'] = df_pycaret['SeniorCitizen'].replace({0: 'No', 1: 'Yes'})\ndf_pycaret.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features = [f for f in df_pycaret.columns if df_pycaret.dtypes[f] == 'object']\n# categorical_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.model_selection import train_test_split\n\n# X = df1.drop(['Churn'], axis=1)\n\n# y = df1['Churn']\n\n# x_train, x_test, y_train, y_test = train_test_split(X, y.values, test_size=0.20, random_state=42)\n\n# # https://pycaret.org/classification/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pycaret_setup = setup(data = df_pycaret,\n                         target = 'Churn',\n                         numeric_imputation = 'mean',\n                         categorical_features = categorical_features, \n                         train_size = 0.80,\n                         session_id = 42,\n                         silent = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compare_models()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_pycaret  = create_model('lr')     ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(estimator = lr_pycaret, plot = 'learning')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(estimator = lr_pycaret, plot = 'auc')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(estimator = lr_pycaret, plot = 'confusion_matrix')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(estimator = lr_pycaret, plot = 'feature')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n\nThis kernel is not finished. If you think that it's useful, votes up the kernel"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}