{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data Understanding and Cleaning**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/italian-sarcasm-detection/News_Dataset.csv\")\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport string\n# convert all words in headline into lower cas\ndf['titolo'] = df.titolo.apply(lambda x:x.lower()) \n\n# remove all punctuations in headline  \ndf['titolo'] = df.titolo.apply(lambda x: ' '.join(word.strip(string.punctuation) for word in x.split())) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pie chart \ndf['sarcastic'].value_counts().plot(kind='pie', \n                                   title='1:Sarcastic / 0:Non sarcastic ITA',\n                                   autopct='%1.1f%%',\n                                   explode= (0, 0.1))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.sarcastic.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The data was taken from the following sites\ndf.link.apply(lambda x: x.split('/')[2]).value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Linguistic Analysis**","metadata":{}},{"cell_type":"code","source":"#REGULAR EXPRESSION\nimport re\n\nimport nltk\nnltk.download('stopwords')\n\nfrom nltk.corpus import stopwords","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stopITA = set(stopwords.words('italian'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cleaned_titleITA = []\n\nfor sentance in df['titolo'].values:\n    sentance = str(sentance)\n    sentance = re.sub(r'[?|!|\\'|\"|#|+|$]', r'', sentance) #sostituisce con lo spazio vuoto\n    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip() #numeri\n    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopITA)\n    cleaned_titleITA.append(sentance.strip())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Titolo_cleaned_re'] = cleaned_titleITA","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Tokenization with NLTK**","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['titolo_tokenize'] = df.titolo.apply(nltk.word_tokenize)\ndf.titolo_tokenize.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_tokensITA = []\nfor t in df['titolo_tokenize']:\n  for i in t:\n    x = i.lower()\n    list_tokensITA.append(x)\nprint(list_tokensITA[:20])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('averaged_perceptron_tagger')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nstopWords = set(stopwords.words('italian'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filtered_tokensITA = []\nfor w in list_tokensITA:\n  if w not in stopWords:\n    filtered_tokensITA.append(w)\n#print(\"Tokenized Sentence:\",list_tokens)\nprint(\"Filtered Sentence:\",filtered_tokensITA[:10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"punkt= [',', '.', '!', '?', '...', '-', '…', \"'\", \"’\", ':', '\"', '$', '\\'s', '%', '“','”','«', '»', '``', \"''\"]\n\nfiltered_tokens_nITA= []\nfor w in filtered_tokensITA:\n  if w not in punkt:\n    filtered_tokens_nITA.append(w)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#POS Tagging\nlistPos = nltk.pos_tag(filtered_tokens_nITA)\nlistPos[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk import Counter\nCounter(listPos).most_common(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"freq_dist_Nostop = nltk.FreqDist(filtered_tokens_nITA)\nfreq_dist_Nostop.plot(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk_text = nltk.Text(list_tokensITA)\nnltk_text.concordance('coronavirus')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**LEMMATIZATION**","metadata":{}},{"cell_type":"code","source":"nltk.download('wordnet')\nfrom nltk.stem.wordnet import WordNetLemmatizer\nlmtzr = WordNetLemmatizer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_lem = []\nfor t in listPos:\n  lem = lmtzr.lemmatize(t[0])  #(t[0], pos = t[1]) \n  list_lem.append(lem)\nprint(list_lem[:20])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Latent Dirichlet Allocation (*LDA*)***","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport string\nimport re\nimport nltk\nimport spacy\nimport sys\nfrom spacy.lang.en import English\nimport en_core_web_sm\nfrom nltk.corpus import wordnet as wn\nfrom nltk.stem.wordnet import WordNetLemmatize","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp = en_core_web_sm.load()\nnltk.download('stopwords')\nparser = English()\nen_stop = set(nltk.corpus.stopwords.words('english'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize(text):\n    \"\"\"this function is to tokenize the headline into a list of individual words\"\"\"\n    lda_tokens = []\n    tokens = parser(text)  # need to use parser for python to treat the list as words\n    for token in tokens:\n        if token.orth_.isspace():  # to ignore any whitespaces in the headline, so that token list does not contain whitespaces \n            continue\n        elif token.like_url:\n            lda_tokens.append('URL')\n        elif token.orth_.startswith('@'):\n            lda_tokens.append('SCREEN_NAME')\n        else:\n            lda_tokens.append(token.lower_)   # tokens (headlines) are already in lowercase\n    return lda_tokens","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_lemma(word):\n    \"\"\"this function is to lemmatize the words in a headline into its root form\"\"\"\n    lemma = wn.morphy(word)  # converts the word into root form from wordnet\n    if lemma is None:\n        return word\n    else:\n        return lemma","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_text_for_lda(text):\n    tokens = tokenize(text)  # parse and tokenize the headline into a list of words\n    tokens = [token for token in tokens if len(token) > 4]  # remove headlines with only length of 4 words or less\n    tokens = [token for token in tokens if token not in en_stop]  # remove stopwords in the headline\n    tokens = [get_lemma(token) for token in tokens]  # lemmatize the words in the headline\n    return tokens","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NS=df[df.sarcastic==0]\nS=df[df.sarcastic==1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('wordnet')\ntext_data = []\nfor headline in df.titolo:\n    tokens = prepare_text_for_lda(headline)\n    text_data.append(tokens)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Sarcasm_data = []\nfor headline in S.titolo:\n    tokens = prepare_text_for_lda(headline)\n    Sarcasm_data.append(tokens)\nNot_Sar = []\nfor headline in NS.titolo:\n    tokens = prepare_text_for_lda(headline)\n    Not_Sar.append(tokens)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim import corpora\nimport pickle\n\ndictionary = corpora.Dictionary(text_data)\nS_dictionary = corpora.Dictionary(Sarcasm_data)\nNS_dictionary = corpora.Dictionary(Not_Sar)# Convert all headlines into a corpus of words, with each word as a token\ncorpus = [dictionary.doc2bow(text) for text in text_data]\nS_corpus = [S_dictionary.doc2bow(text) for text in Sarcasm_data]\nNS_corpus = [NS_dictionary.doc2bow(text) for text in Not_Sar]# Convert each headline (a list of words) into the bag-of-words format","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(text_data), len(Sarcasm_data), len(Not_Sar))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gensim\n\nNUM_TOPICS = [3, 5, 10]\n# passes: Number of passes through the corpus during training\n# alpha: priori on the distribution of the topics in each document.\n# The higher the alpha, the higher the likelihood that document contains a wide range of topics, vice versa. \n# beta: priori on the distribution of the words in each topic.\n# The higher the beta, the higher the likelihood that topic contains a wide range of words, vice versa.\n# we do not alter / fine tune the default values of alpha and beta\nldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS[1], id2word=dictionary, passes=15)\nldamodel.save('model5.gensim')\ntopics = ldamodel.print_topics(num_words=5)\ntopics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#LDA on most 5 non sacrcastic topics\nldamodel = gensim.models.ldamodel.LdaModel(NS_corpus, num_topics = NUM_TOPICS[1], id2word=NS_dictionary, passes=15)\nldamodel.save('model5.gensim')\ntopics = ldamodel.print_topics(num_words=5)\ntopics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#LDA su most 5 topic news sarcastiche \nldamodel = gensim.models.ldamodel.LdaModel(S_corpus, num_topics = NUM_TOPICS[1], id2word=S_dictionary, passes=15)\nldamodel.save('model10.gensim')\ntopics = ldamodel.print_topics(num_words=5)\ntopics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**CLASSIFICATION**","metadata":{}},{"cell_type":"markdown","source":"*BERT*","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport sklearn\nimport seaborn as sbs\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom transformers import TFBertModel, BertTokenizer\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = df.sarcastic.values\nsentences = df.titolo.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME,do_lower_case = True)\n\ndef encoder(sentences):\n  ids = []\n  for sentence in sentences:\n    encoding = tokenizer.encode_plus(\n    sentence,\n    max_length=16,\n    truncation = True,\n    add_special_tokens=True,\n    return_token_type_ids=False,\n    pad_to_max_length=True,\n    return_attention_mask=False)\n    ids.append(encoding['input_ids'])\n  return ids\n\n#Train test split\ntrain_sents,test_sents, train_labels, test_labels  = train_test_split(sentences, labels ,test_size=0.15)\n\ntrain_ids = encoder(train_sents)\ntest_ids = encoder(test_sents)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train test split\ntrain_sents1,test_sents1, train_labels1, test_labels1  = train_test_split(sentences, labels ,test_size=0.15)\n\ntrain_ids = encoder(train_sents1)\ntest_ids = encoder(test_sents1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ids = tf.convert_to_tensor(train_ids)\ntest_ids = tf.convert_to_tensor(test_ids)\ntest_labels = tf.convert_to_tensor(test_labels)\ntrain_labels = tf.convert_to_tensor(train_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_encoder = TFBertModel.from_pretrained('bert-base-uncased')\ninput_word_ids = tf.keras.Input(shape=(16,), dtype=tf.int32, name=\"input_word_ids\")  \nembedding = bert_encoder([input_word_ids])\ndense = tf.keras.layers.Lambda(lambda seq: seq[:, 0, :])(embedding[0])\ndense = tf.keras.layers.Dense(128, activation='relu')(dense)\ndense = tf.keras.layers.Dropout(0.2)(dense)   \noutput = tf.keras.layers.Dense(1, activation='sigmoid')(dense)    \n\nmodel = tf.keras.Model(inputs=[input_word_ids], outputs=output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(tf.keras.optimizers.Adam(1e-5), loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(x = train_ids, y = train_labels, epochs = 8, verbose = 1, batch_size = 32, validation_data = (test_ids, test_labels))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*The model is clearly overfitting due to the small dimension*","metadata":{}},{"cell_type":"code","source":"def plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs/Iterations\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n\nplot_graphs(history, 'accuracy')\nplot_graphs(history, 'loss')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report,confusion_matrix,accuracy_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Evaluate on test data\")\nresults = model.evaluate(test_ids, test_labels, batch_size=128)\nprint(\"test loss, test acc:\", results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Thanks for watching*","metadata":{}}]}