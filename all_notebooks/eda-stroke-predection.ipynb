{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = pd.read_csv('/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Initial observations from the data and ToDo list:\n\n* 5110 Items 12 labes (Stroke-Dependent Variable)\n* We need to handle categorical variables\n* Check for imbalance data\n* Handle Null values\n* Scaling (Maybe-Maybe not)\n* Feature selection\n\n","metadata":{}},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"dataset.describe()\n\ndataset = dataset.drop(['id'],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Missing values","metadata":{}},{"cell_type":"code","source":"features_with_na = [features for features in dataset.columns if dataset[features].isnull().sum()>1]\n\nfor feature in features_with_na:\n  print(f\"{feature} has {100*(np.round(dataset[feature].isnull().mean(),4))} % missing values\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlation_with_features_with_na = dataset.corr()\nprint(correlation_with_features_with_na)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see the correlation between Stroke and Bmi is very less. I can not use it but we'll see further.","metadata":{}},{"cell_type":"markdown","source":"### Numerical Values","metadata":{}},{"cell_type":"code","source":"numerical_features = [features for features in dataset.columns if dataset[features].dtype != 'O']\n\nprint(f\"Numerical features {len(numerical_features)}\")\ndataset[numerical_features].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"discrete_features = [features for features in numerical_features if dataset[features].dtype == 'int']\n\nprint(f'There are {len(discrete_features)} discrete variables present ')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Continous variables","metadata":{}},{"cell_type":"code","source":"continous_features = [features for features in numerical_features if dataset[features].dtype == 'float' ]\n\nprint(continous_features)\nprint(f'There are {len(continous_features)} features present')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for features in continous_features:\n  data = dataset.copy()\n  data[features].hist(bins=25)\n  plt.xlabel(features)\n  plt.ylabel('Stroke')\n  plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Performing Log transformation on bmi and avg_gucose_level","metadata":{}},{"cell_type":"code","source":"for features in ['bmi','avg_glucose_level']:\n  data = dataset.copy()\n  data[features]=np.log(data[features])\n  data[features].hist(bins=25)\n  plt.xlabel(features)\n  plt.ylabel('Stroke')\n  plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finding outliers in the data","metadata":{}},{"cell_type":"code","source":"for feature in continous_features:\n  data = dataset.copy()\n  if 0 in data[features].unique():\n    pass\n  else:\n    data[feature] = np.log(data[feature])\n    data.boxplot(column=feature)\n    plt.ylabel(feature)\n    plt.title(feature)\n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Categorical features","metadata":{}},{"cell_type":"code","source":"categorical_features = [features for features in dataset.columns if data[features].dtypes == 'O']\nprint(categorical_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cardinality of categorical variables","metadata":{}},{"cell_type":"code","source":"for features in categorical_features:\n  print(f\"Feature {features} number of categories {len(dataset[features].unique())}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can one hot encode them all but before that we should find the relation between them and stroke","metadata":{}},{"cell_type":"code","source":"for features in categorical_features:\n  data = dataset.copy()\n  data.groupby(features)['stroke'].count().plot.bar()\n  plt.xlabel(features)\n  plt.ylabel('Stroke')\n  plt.title(features)\n  plt.show()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"### Creating a train test split before any preprocessing to prevent any data leakage","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(dataset,dataset['stroke'],test_size = 0.1, random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Handling missing values","metadata":{}},{"cell_type":"code","source":"features_nan = [feature for feature in dataset.columns if dataset[feature].isnull().sum()>1]\n\nprint(features_nan)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Replacing missing values with median","metadata":{}},{"cell_type":"code","source":"for features in ['bmi']:\n  median_value = dataset[features].median()\n\n  #dataset[features+'nan']=np.where(dataset[features].isnull(),1,0)\n  dataset[features].fillna(median_value,inplace=True)\n\ndataset['bmi'].isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"median_value_train = X_train['bmi'].median()\nmedian_value_test = X_test['bmi'].median()\n\nX_train['bmi'].fillna(median_value_train, inplace=True)\nX_test['bmi'].fillna(median_value_test, inplace=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train.drop(['stroke'],axis=1)\nX_test = X_test.drop(['stroke'],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Handling skewed data","metadata":{}},{"cell_type":"code","source":"for features in ['bmi','avg_glucose_level']:\n  X_train[features] = np.log(X_train[features])\n  X_test[features] = np.log(X_test[features])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Label encoder","metadata":{}},{"cell_type":"code","source":"categorical_variables_to = [feature for feature in dataset.columns if dataset[feature].dtype == 'O']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_X_train = X_train.copy()\nlabel_X_test = X_test.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nlabel_encoder= LabelEncoder()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for cols in categorical_variables_to:\n  label_X_train[cols] = label_encoder.fit_transform(X_train[cols])\n  label_X_test[cols] = label_encoder.transform(X_test[cols])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## One Hot encoder second","metadata":{}},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ct = ColumnTransformer(transformers = [('encoder', OneHotEncoder(), [0,4,5,6,9])], remainder='passthrough')\noh2_x_train = pd.DataFrame(ct.fit_transform(X_train))\noh2_x_test = pd.DataFrame(ct.transform(X_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oh2_x_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oh2_x_train.index = X_train.index\noh2_x_test.index = X_test.index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling Decision tree","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accur = []\npred_as_1 =[]\nfor i in range(2,51):\n    model = DecisionTreeClassifier(max_depth= i).fit(oh2_x_train, y_train)\n    pred = model.predict(oh2_x_test)\n    accur.append(accuracy_score(y_test, pred))\n    pred_as_1.append(confusion_matrix(y_test, pred)[1][1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,4))\nx=list(range(2,51))\naxes[0].plot(x,accur,'r')\naxes[1].plot(x,pred_as_1,'b')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = DecisionTreeClassifier(max_depth= 30).fit(oh2_x_train, y_train)\npred = model.predict(oh2_x_test)\n\nconfusion_matrix(y_test, pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test,pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}