{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66fab5b194e718848490953a27a1e33db33b531c","collapsed":true},"cell_type":"code","source":"comments_data = pd.read_csv(r'../input/airline-sentiment/Tweets.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"314b63bf5806303214383b4f2d788bcaf97bedbc","collapsed":true},"cell_type":"code","source":"comments_data.head(n=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1b62381b192a1c5ab4fea94b7df20008cc44c7f","scrolled":true,"collapsed":true},"cell_type":"code","source":"import re\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, Dropout, LSTM, Activation\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\nfrom keras.initializers import glorot_uniform","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1bffcfe1ffd21d4dbd2166a9005e826b7a2e9df","collapsed":true},"cell_type":"code","source":"# extract sentiment(positive/negative/neutral) into labels\nlabels = comments_data[\"airline_sentiment\"].map({'neutral':0,'negative':1,'positive':2})\nlabels = np.asarray(labels, dtype=int)\n\n# extract text to be analysed into comments\ncomments = comments_data[\"text\"]\n#remove words which are starts with @ symbols\ncomments = comments.map(lambda x: \" \".join(str(x).split()))\ncomments = comments.map(lambda x:re.sub('@\\w*','',str(x)))\n#remove special characters except [a-zA-Z]\ncomments = comments.map(lambda x:re.sub('[^a-zA-Z]',' ',str(x)))\n#remove link starts with https\ncomments = comments.map(lambda x:re.sub('http.*','',str(x)))\n#convert to ndarray\ncomments = np.asarray(comments)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"b3e1f6d53c8b8cde594eacce91ea06272532cebe","collapsed":true},"cell_type":"code","source":"# Change index value to see comments and corresponding sentiment type\nindex = 0\nprint(comments[index], labels[index])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ff1e5fff8a41908697abf38509575602241d09a","collapsed":true},"cell_type":"code","source":"# Find number of unique categories\nnum_classes = comments_data[\"airline_sentiment\"].unique().shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5409584c9bd499491f85070cbddea6c1dca0c29","collapsed":true},"cell_type":"code","source":"# Convert to one-hot vector\ndef convert_to_one_hot(Y, C):\n    Y = np.eye(C)[Y.reshape(-1)]\n    return Y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c2093dfa256764aad43f7aa37346935d81b5387","collapsed":true},"cell_type":"code","source":"Y_one_hot = convert_to_one_hot(labels, C = num_classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e05ec4d0bc629a62500d0872c4f750d6992095fc","collapsed":true},"cell_type":"code","source":"# Split into training and test data\nX_train,X_test,y_train,y_test = train_test_split(comments,Y_one_hot,test_size=0.2,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72c9e7005ae0cebedadda9e449a3693dbcb45209","collapsed":true},"cell_type":"code","source":"print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12c1b32c053674af9475dcda8107a6176c0bb801","collapsed":true},"cell_type":"code","source":"maxLen = 40","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"667b00491d27f655bbfcbd56aed07b6e304043e4","collapsed":true},"cell_type":"code","source":"#Load Glove Vectors\nGLOVE_DIR='../input/glove-global-vectors-for-word-representation/'\n\nembeddings_index = {}\nf = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\nwords = set()\nword_to_vec_map = {}\nfor line in f:\n    values = line.strip().split()\n    word = values[0]\n    words.add(word)\n    word_to_vec_map[word] = np.array(values[1:], dtype=np.float64)\n    \ni = 1\nwords_to_index = {}\nindex_to_words = {}\nfor w in sorted(words):\n    words_to_index[w] = i\n    index_to_words[i] = w\n    i = i + 1\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fdd339f232abaae70834cf258a35ef64a5f1515e","collapsed":true},"cell_type":"code","source":"#look at a word embedding\nword_to_vec_map[\"hello\"].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b780b450d58b25190d0e2c4d8ca97b082296339","collapsed":true},"cell_type":"code","source":"def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n    \"\"\"\n    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n    \n    Arguments:\n    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n\n    Returns:\n    embedding_layer -- pretrained layer Keras instance\n    \"\"\"\n    \n    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n    emb_dim = word_to_vec_map[\"hello\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n    \n    ### START CODE HERE ###\n    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n    emb_matrix = np.zeros((vocab_len, emb_dim))\n    \n    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n    for word, index in word_to_index.items():\n        emb_matrix[index, :] = word_to_vec_map[word]\n\n    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. \n    embedding_layer = Embedding(input_dim=vocab_len, output_dim=emb_dim,trainable=False)\n    ### END CODE HERE ###\n\n    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n    embedding_layer.build((None,))\n    \n    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n    embedding_layer.set_weights([emb_matrix])\n    \n    return embedding_layer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae9c3528e662e0eb33944205e1268c939f740fa0","collapsed":true},"cell_type":"code","source":"embedding_layer = pretrained_embedding_layer(word_to_vec_map, words_to_index)\nprint(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4fe08502d0502368c7d21ab0aa060b9f7441b062","collapsed":true},"cell_type":"code","source":"def sentences_to_indices(X, word_to_index, max_len):\n    \n    m = X.shape[0]\n    X_indices = np.zeros((m, max_len))\n    \n    for i in range(m):\n        \n        sentence_words = X[i].strip().lower().split(\" \")\n        j = 0\n        \n        for w in sentence_words:\n            if w and w in word_to_index:\n                X_indices[i, j] = word_to_index[w]\n                j = j + 1\n    \n    return X_indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7727659a85fc2f2b941cfddc7d079dfae38b23e2","collapsed":true},"cell_type":"code","source":"input_shape = (maxLen,)\nsentence_indices = Input(shape=input_shape,dtype='int32')\n    \n# Create the embedding layer pretrained with GloVe Vectors\nembedding_layer = pretrained_embedding_layer(word_to_vec_map, words_to_index)\n    \n# Embedding layer, you get back the embeddings\nembeddings = embedding_layer(sentence_indices)   \n    \n# LSTM layer with 128-dimensional hidden state\nX = LSTM(128,return_sequences=True)(embeddings)\nX = Dropout(rate=0.5)(X)\n# LSTM layer with 128-dimensional hidden state\nX = LSTM(128)(X)\nX = Dropout(rate=0.5)(X)\n# Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\nX = Dense(num_classes)(X)\n# Add a softmax activation\nX = Activation(\"softmax\")(X)\n    \n# Create Model instance which converts sentence_indices into X.\nmodel = Model(inputs=sentence_indices, outputs=X)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"b59d24a1f38d92a2c9e32fb4ab2f3d2ea1064872","collapsed":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae3e361b9d235c48f225e94b59fbcf0474480064","collapsed":true},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93535a247c7d6ed397b452203db4449cbd645396","collapsed":true},"cell_type":"code","source":"X_train_indices = sentences_to_indices(X_train, words_to_index, maxLen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f30f2d5020461f8f568990c88992d09127172614","scrolled":true,"collapsed":true},"cell_type":"code","source":"model.fit(X_train_indices, y_train, epochs = 20, batch_size = 32, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0634218502386974d14f88ec7a4ab939f2bab8f7","scrolled":true,"collapsed":true},"cell_type":"code","source":"X_test_indices = sentences_to_indices(X_test, words_to_index, maxLen)\nloss, acc = model.evaluate(X_test_indices, y_test)\nprint()\nprint(\"Test accuracy = \", acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b607d4d84460e9abeb1173fdc215f0e0990ec117","collapsed":true},"cell_type":"code","source":"#Show mis-classified comments\ntest_data = np.asarray([\"worst experience ever\",\"it was a wonderful experience\",\"it was just fine\", \"hello how are you doing\", \"not a very good airline\"])\ntest_data_y = np.asarray([1,2,2,0,1])\ntest_indices = sentences_to_indices(test_data, words_to_index, maxLen)\npred = model.predict(test_indices)\nfor i in range(len(test_data)):\n    x = test_indices\n    num = np.argmax(pred[i])\n    act = test_data_y[i]\n    print(test_data[i] + '::::Expected='+ str(act) + ', Prediction=' + str(num))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ed54ecb241475b36f99ecdc049b5ef1f26040793"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fe205be882d15b19adc6d0ab5ce88d7d2cb212c2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}