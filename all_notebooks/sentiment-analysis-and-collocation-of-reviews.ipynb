{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"448cca54-45e0-591e-803d-6eec3b8bc398"},"source":"# Sentiment Analysis and Collocation of Reviews\n\nIn this notebook we apply two techniques to the reviews for the Boston-area AirBnBs in our dataset: sentiment analysis and collocation."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1b80a140-47df-7d65-7bbd-e28c794c0761"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\npd.set_option(\"max_columns\", None)\n\nreviews = pd.read_csv(\"../input/reviews.csv\")\nlistings = pd.read_csv(\"../input/listings.csv\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"665b3709-daa4-ce49-d979-961b9b072ab4"},"outputs":[],"source":"listings.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a7ab1492-204e-f6c3-6b95-bf323f311024"},"outputs":[],"source":"reviews.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"4d8a0a29-cc15-0fd9-aee5-53b16b149422"},"source":"## Sentiment Analysis\n\nFirst we note the highly skewed distribution of reviews on the Internet: many many positives, not many negatives. This holds just as true on AirBnB as everywhere else."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ca0a35f5-747f-9d34-0fe1-8a4c5702db21"},"outputs":[],"source":"listings['review_scores_rating'].sort_values().reset_index(drop=True).dropna().plot()"},{"cell_type":"markdown","metadata":{"_cell_guid":"5aa69416-56fd-c5e1-f5bf-c0b5575e107d"},"source":"There's an XKCD for this."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"72022f75-fcf2-86e7-6a97-3fde69eb4502"},"outputs":[],"source":"from IPython.display import Image\n\nImage(\"https://imgs.xkcd.com/comics/star_ratings.png\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"4bf30d57-b8d6-4fe0-2693-03b4f5a340a1"},"source":"Ok, let's try out sentiment analysis.\n\nSentiment analysis is a technique in natural language processing which aims to retrieve the \"sentiment\" of a piece of text&mdash;positive, negative, or neutral. This is an easy way of summarizing the contents of a piece of text, and one that is easily understood.\n\nNote, however, that sentiment analysis is a difficult problem. Humans agree on the sentiment of sentences only 80% of the time, and the best classifiers can get around that level of accuracy, but we're going to just use a built-in analyzer in the `nltk` (natural language toolkit) Python library.\n\nSo I don't expect our results to be astonishingly good, but let's see what we get..."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"08b49520-905c-3ada-4f59-69984047f04f"},"outputs":[],"source":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\nsid = SentimentIntensityAnalyzer()\nfor sentence in reviews['comments'].values[:5]:\n    print(sentence)\n    ss = sid.polarity_scores(sentence)\n    for k in sorted(ss):\n        print('{0}: {1}, '.format(k, ss[k]), end='')\n    print()"},{"cell_type":"markdown","metadata":{"_cell_guid":"d3732af9-7580-8e85-ab01-5850e387c7b1"},"source":"Our reviews contain both null reviews and reviews in other languages. [langdetect](https://github.com/Mimino666/langdetect) makes this trivially easy, but it doesn't install on Kaggle for whatever reason. `nltk` can do this too, but for whatever reason it doesn't have a built-in for it. We'll use the following bit of code to filter out non English-language reviews, borrowed from elsewhere:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0170718f-c10a-9597-4080-03fc8a2062cc"},"outputs":[],"source":"# Snippet from:\n# http://h6o6.com/2012/12/detecting-language-with-python-and-the-natural-language-toolkit-nltk/\n\nfrom nltk.corpus import stopwords   # stopwords to detect language\nfrom nltk import wordpunct_tokenize # function to split up our words\n\ndef get_language_likelihood(input_text):\n    \"\"\"Return a dictionary of languages and their likelihood of being the \n    natural language of the input text\n    \"\"\"\n \n    input_text = input_text.lower()\n    input_words = wordpunct_tokenize(input_text)\n \n    language_likelihood = {}\n    total_matches = 0\n    for language in stopwords._fileids:\n        language_likelihood[language] = len(set(input_words) &\n                set(stopwords.words(language)))\n \n    return language_likelihood\n \ndef get_language(input_text):\n    \"\"\"Return the most likely language of the given text\n    \"\"\" \n    likelihoods = get_language_likelihood(input_text)\n    return sorted(likelihoods, key=likelihoods.get, reverse=True)[0]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f3e5adc8-a02d-1dbb-491f-d11553877c92"},"outputs":[],"source":"reviews_f = [r for r in reviews['comments'] if pd.notnull(r) and get_language(r) == 'english']"},{"cell_type":"markdown","metadata":{"_cell_guid":"de65573d-d16b-8e06-c07d-0a735ca390b1"},"source":"Generate our scores."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"430ff746-2d6a-7470-bc72-40df38b43dd8"},"outputs":[],"source":"pscores = [sid.polarity_scores(comment) for comment in reviews_f]"},{"cell_type":"markdown","metadata":{"_cell_guid":"75026058-255a-e0f0-4a90-bac37166dba8"},"source":"How do we score on...\n\n**Neutrality**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c4b64b2b-0d92-b5e9-072c-c6495e7fdcd8"},"outputs":[],"source":"pd.Series([score['neu'] for score in pscores]).plot(kind='hist')"},{"cell_type":"markdown","metadata":{"_cell_guid":"3b69ad15-bf88-0072-6b51-2f6255b0da0d"},"source":"**Positivity**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b2646d83-a6b4-c252-3cdd-7896eff0e7e8"},"outputs":[],"source":"pd.Series([score['pos'] for score in pscores]).plot(kind='hist')"},{"cell_type":"markdown","metadata":{"_cell_guid":"15c5f430-fc35-0d30-dc74-a0de5fc29d88"},"source":"**Negativity**.\n\nAlmost none of the texts are classified as having significant amounts of negativity! In fact, a significant amount of them are given exactly 0.0 negativity."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"87e3dcda-5dd2-2a8e-1ba2-8da0019d67be"},"outputs":[],"source":"pd.Series([score['neg'] for score in pscores]).plot(kind='hist', bins=100)"},{"cell_type":"markdown","metadata":{"_cell_guid":"05b5d33a-e9e5-d3ca-24bc-13f3255062da"},"source":"These charts tell us about the characteristics of the off-the-shelf sentiment classifier that we are used and its performance on our dataset. Although the compound score is supposed to be the best estimate of overall sentiment (not shown in the charts above), the fact that negativities are ranked so lowly hints that we're doing a not so great job with this."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"555ac2da-ff20-2ae0-6a8f-ae71082fa3e3"},"outputs":[],"source":"scored_reviews = pd.DataFrame()\nscored_reviews['review'] = [r for r in reviews_f if get_language(r) == 'english']\nscored_reviews['compound'] = [score['compound'] for score in pscores]\nscored_reviews['negativity'] = [score['neg'] for score in pscores]\nscored_reviews['neutrality'] = [score['neu'] for score in pscores]\nscored_reviews['positivity'] = [score['pos'] for score in pscores]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"59f41a9d-17a4-e9a5-0a9b-9e191521d9a0"},"outputs":[],"source":"scored_reviews.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"56371844-ac35-c85c-0d03-751c1ef45c86"},"source":"Let's look at our positive-negativity reviews. A lot of these aren't negative at all."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0f772edf-8031-c08b-6039-e0dbc67f285b"},"outputs":[],"source":"scored_reviews.query('negativity > 0')"},{"cell_type":"markdown","metadata":{"_cell_guid":"6188b3b6-ecaa-5364-06b0-55856bae3508"},"source":"Here are two that are:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c334ab23-f194-3e9a-221d-368eeeb7279f"},"outputs":[],"source":"scored_reviews.iloc[23]['review']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5e202b6f-0de4-5edc-0177-f877a624c88a"},"outputs":[],"source":"scored_reviews.iloc[28]['review']"},{"cell_type":"markdown","metadata":{"_cell_guid":"1c5a927b-f3fb-a84f-be8b-21d9a44b18b4"},"source":"Some more fiddling with queries..."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"502849e8-ae5a-3773-bce6-f0257ee412e0"},"outputs":[],"source":"scored_reviews.query('negativity > positivity').query('negativity > 0.1')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"07fcb6d1-f446-1547-67a8-0d93aece1bae"},"outputs":[],"source":"scored_reviews.query('negativity > positivity').query('compound < -0.2')"},{"cell_type":"markdown","metadata":{"_cell_guid":"daec95ac-b9e2-2eb7-e2e0-d79c62a37e3d"},"source":"Here's an example of the kind of (funny, sarcastic) review that seriously trips our classifier up:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c8e78cd9-7064-6136-1fc9-f4fba26ab493"},"outputs":[],"source":"scored_reviews.iloc[1181]['review']"},{"cell_type":"markdown","metadata":{"_cell_guid":"efdc9ef7-1c70-8e89-677b-d2ac360c7ea0"},"source":"Here's another one. In this text's case even though we would say the sentiment with regards to the *lister* is positive, the sentiment of the overall paragraph is *negative* because of the renter's unfortunate experience with food poisoning, being \"horrendously sick\", etc.\n\nThis is a limitation inherent in all sentiment classification tasks. The best way to get around this is to use a technique called chunking to extract what sentiment is attached to what thing in the text, but that gets complicated very quickly."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8900f2ea-977e-af5c-1be5-163cc6053706"},"outputs":[],"source":"scored_reviews.iloc[63836]['review']"},{"cell_type":"markdown","metadata":{"_cell_guid":"c9427258-0b6d-e8fd-21d7-918a29461a2a"},"source":"Here are two more bad reviews because why not:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"840d06fa-7d27-7234-3641-7363c8a7f1b9"},"outputs":[],"source":"scored_reviews.iloc[62984]['review']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f84afc08-35e7-9a1b-1858-ac60623ac37d"},"outputs":[],"source":"scored_reviews.iloc[198]['review']"},{"cell_type":"markdown","metadata":{"_cell_guid":"432438a6-d77c-750c-ee36-9dcff20db5ba"},"source":"We'll actually stop here. It's pretty clear that our sentiment analyzer is not doing a good enough job separating the wheat from the chaff to use our results for anything! That's unfortunate, but understandable.\n\nThere's a number of pre-processing techniques that we could apply to our dataset to make our sentiment analyzer work better (Google it!). We could also try a different sentiment analyzer (like the IBM or HP ones, available via API), particularly one perhaps better suited for the \"Internet reviews\" domain, and see if that would get better results."},{"cell_type":"markdown","metadata":{"_cell_guid":"278b8347-e3a8-7c31-03c4-8c1790ef214e"},"source":"## Collocation\n\nAccording to Wikipedia \"a collocation is a sequence of words or terms that co-occur more often than would be expected by chance.\" What we want to attempt now is to use `nltk` to find collocations which have a high amount of importance in the text, and we'd like to take and display them as \"summaries\" of our texts.\n\nHow do we tell when a particular combination of words is important? One way of doing it is look at those word's [pointwise mutual information](https://en.wikipedia.org/wiki/Pointwise_mutual_information).  This is a metric which attached significance to words which appear next to one another in the text, for whom such co-occurrences are far-above-averagely-common, and which are otherwise rarely used in the language. According to this metric, for example, the words \"puerto\" and \"rico\" have a very high PMI, while the words \"to\" and \"in\" have a very low one.\n\nIf you use Yelp! a lot you are probably familiar with Yelp's so-called [review highlights](https://www.yelp-support.com/article/What-are-Review-Highlights?l=en_US). These kick in after a location has had a certain reasonably large amount of reviews written, and show, by default, snippets of three reviews mentioning a combination of words which appears especially often in reviews for the location. [Here's an example](https://www.yelp.com/biz/chelsea-market-new-york) of these highlights in action.\n\nAn answer on StackOverflow says that these highlights are probably implemented using [precisely the techniques spoken about above](http://stackoverflow.com/questions/2452982/how-to-extract-common-significant-phrases-from-a-series-of-text-entries). What we're going to now try and do is replicate Yelp! review highlights with AirBnB review highlights!"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f58f8046-0e30-a653-d691-a9ee8a86d759"},"outputs":[],"source":"reviews_df = reviews[reviews.apply(lambda srs: pd.notnull(srs['comments']) and (get_language(srs['comments']) == 'english'), axis='columns')]"},{"cell_type":"markdown","metadata":{"_cell_guid":"fa98cb44-d87a-271d-09e5-e7d2fbbb11b5"},"source":"Let's try and find interesting word combinations for an example listing, just to see if it's possible. In this case we're picking an ID with 200 reviews to it, a substantial number which should hopefully let us mine good subject commonalities between them.\n\nNote that in this case our \"combinations of words\" means bigrams: pairs of two words which appear right next to each other in the text. This can be extended to n-grams of arbitrary size, if you're so inclined, and Yelp! uses n-gram sizes between 1 and 3, but for simplicity's sake we're going to stick to bigrams (2-grams) here."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d57ae7bb-087b-0311-d0a7-b49710851f96"},"outputs":[],"source":"example_listing_reviews = reviews_df.query('listing_id == 1178162')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8d27e0e9-77fd-bb0c-1981-9a5554bdf530"},"outputs":[],"source":"len(example_listing_reviews)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"727d3c15-3191-5e46-2db4-6cd9d2b36d54"},"outputs":[],"source":"from nltk import word_tokenize"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cf1470a1-41ef-d1c6-e49e-1e44fb7b348e"},"outputs":[],"source":"words = np.concatenate(np.array([word_tokenize(r) for r in example_listing_reviews['comments'].values]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2365a96d-2c55-b61d-4a71-41b091e6c89e"},"outputs":[],"source":"words"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"80b489f0-d02f-74e7-ccc1-c5761629a2f7"},"outputs":[],"source":"from nltk.collocations import BigramAssocMeasures, TrigramAssocMeasures, BigramCollocationFinder\n\nbigram_measures = BigramAssocMeasures()\nfinder = BigramCollocationFinder.from_words(words)\n\nfinder.apply_freq_filter(3) \nfinder.nbest(bigram_measures.pmi, 10)  "},{"cell_type":"markdown","metadata":{"_cell_guid":"cc0530d7-e77a-3496-931a-6f586001bef8"},"source":"Ok great. How many reviews do we have to work with per location?"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e554fa3b-bba2-3408-e7fa-29cc3876f979"},"outputs":[],"source":"reviews_df.groupby('listing_id')['comments'].count().plot(kind='hist', bins=20)"},{"cell_type":"markdown","metadata":{"_cell_guid":"cd6aaff1-c18b-e0ea-b4b1-54494df65d7e"},"source":"To process the words we're going to use a `BigramCollocationFinder`, which expects all of the text from our reviews, tokenized into individual words, as input. To do that we're going to use the `nltk` `word_tokenize` method on the words, then run a couple of maps on the result to tweak a couple of things: remove punctionation marks and recombine contractions that `word_tokenize` splits up (`word_tokenize` will render `didn't` as `[\"did\", \"n't\"]` for example, which we don't want."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5c9314dd-a68c-c7fa-d392-4c091295f879"},"outputs":[],"source":"review_words = reviews_df.groupby('listing_id').apply(\n    lambda df: np.concatenate(np.array([word_tokenize(r) for r in df['comments'].values]))\n)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8f05bb1f-4f73-350f-3730-b864c5ec6c2c"},"outputs":[],"source":"import string\n\nex = ['Hi', 'there', '.', '?', '!', ',']\n[w for w in ex if w not in string.punctuation]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e2b26273-2a4c-1820-0ed5-204391012e07"},"outputs":[],"source":"review_words_f = review_words.map(lambda arr: np.array([w for w in arr if w not in string.punctuation]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e32a435c-09c3-3925-31a8-cd088373ce75"},"outputs":[],"source":"review_words_f.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4a727854-a900-2dbf-bcbd-2fe68433d05e"},"outputs":[],"source":"def reattach_contractions(wordlist):\n    words = []\n    for i, word in enumerate(wordlist):\n        if word[0] == \"'\" or word == \"n't\":\n            words[-1] = words[-1] + word\n        else:\n            words.append(word)\n    return words"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1ceb6f3e-9a8d-0322-5a0b-c6d6c5b8431d"},"outputs":[],"source":"review_words_f = review_words_f.map(reattach_contractions)"},{"cell_type":"markdown","metadata":{"_cell_guid":"2683f19e-564e-35ed-4d1e-1bd66b081e8a"},"source":"Ok great! Let's see how many words we're working with for each of our reviews."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f38b3cc6-b3f7-8546-4853-57a5f416eaf8"},"outputs":[],"source":"review_words_f.map(len).plot(kind='hist', bins=20)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d29e1f36-4b0d-3efc-065e-a20fe2a9877f"},"source":"There's going to be some sort of cut-off in terms of the number of words that, were we to use this result in production, we would need to find. Yelp! seems to put that cutoff at 20 or so reviews; below that there's not enough information for highlighting to work.\n\nNot knowing any more about how they do things, we're just going to apply our collocation finder to all of the review texts. First we're going to retrieve a list of bigrams that appear in the review text at least three times. Then we'll pick the three \"best\" bigrams, where \"best\" means the large PMI.\n\nThat's what the function below does."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7ac337f8-6f92-57d7-8adc-28b03ba36d5f"},"outputs":[],"source":"# from nltk.collocations import BigramAssocMeasures, TrigramAssocMeasures, BigramCollocationFinder\n\ndef bigramify(words):\n    finder = BigramCollocationFinder.from_words(words)\n    finder.apply_freq_filter(3) \n    return finder.nbest(bigram_measures.pmi, 3)\n\nreview_bigrams = review_words_f.map(bigramify)"},{"cell_type":"markdown","metadata":{"_cell_guid":"6bee497f-cb79-1631-b941-c135e0dca3d3"},"source":"Let's see what our results look like!"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1ac64047-0e30-2170-2670-237ba6e6b846"},"outputs":[],"source":"review_bigrams.head(20)"},{"cell_type":"markdown","metadata":{"_cell_guid":"cd629ec9-f281-687e-d6cc-da66e954ffb3"},"source":"Not bad! It could definitely use improvement, but we're already seeing some interesting topics recur here.\n\nLet's generate \"Yelp! style\" top-level highlights and print them to see what we get."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"58484f48-d077-aa1c-b91a-46868d65a402"},"outputs":[],"source":"def sample_reviews(listing_id):\n    bigrams = review_bigrams[listing_id]\n    review_texts = reviews[reviews['listing_id'] == listing_id]['comments'].values\n    sample_reviews = []\n    for bigram in bigrams:\n        sample_review_list = list(filter(lambda txt: \" \".join(bigram) in txt, review_texts))\n        num_reviews = len(sample_review_list)\n        sample_review = sample_review_list[0]\n        sample_review = sample_review.replace(\" \".join(bigram), \"****\" + \" \".join(bigram) + \"****\")\n        start_index = sample_review.index(\"****\")\n        sample_text = \"...\" + sample_review[start_index - 47: start_index + 47] + \"...\"\n        sample_reviews.append(sample_text)\n    return sample_reviews"},{"cell_type":"markdown","metadata":{"_cell_guid":"0f6a726a-13bb-b6ed-3c6f-f15799e43e87"},"source":"For reference I'll also provide listing URLs."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"325a6abf-5b66-aac9-248d-e75c5b82ba8c"},"outputs":[],"source":"listings.query('id == 3353')['listing_url']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3bf7c8a1-cd04-dd6f-dd7c-7753e7a1028e"},"outputs":[],"source":"for review in sample_reviews(3353):\n    print(review)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b82c3498-dd09-2b0f-28d7-267e2e033535"},"outputs":[],"source":"listings.query('id == 1497879')['listing_url']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"64da336c-16f8-0577-9b95-5b44fd7ba72a"},"outputs":[],"source":"for review in sample_reviews(1497879):\n    print(review)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7cb819a5-3914-3805-500f-57ab10760cee"},"outputs":[],"source":"listings.query('id == 414419')['listing_url']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5aa4ccb8-4662-8a6a-0d24-1639b0745870"},"outputs":[],"source":"for review in sample_reviews(414419):\n    print(review)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d11841ac-5bdc-6e4a-5b55-70f1da06c590"},"outputs":[],"source":"listings.query('id == 1136972')['listing_url']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c25202df-f655-475b-3095-497792bcf78b"},"outputs":[],"source":"for review in sample_reviews(1136972):\n    print(review)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"54625642-c7f9-c9b9-33b1-e799b1978ab1"},"outputs":[],"source":"for review in sample_reviews(3353):\n    print(review)"},{"cell_type":"markdown","metadata":{"_cell_guid":"bf689ca4-591a-e76b-d642-81afd458b577"},"source":"## Conclusion\n\nThe basic `nltk` sentiment analysis built-in did not do a good job analyzing the sentiments in our sample of AirBnB reviews. Without knowing more details about how the classifier was trained (there is a paper you can read FYI) I can't say for sure why that is, exactly, but it's nevertheless an interesting limitation to keep in mind, as most Internet review texts are going to be pretty similar to the AirBnB one. Perhaps other analyzers would do a better job.\n\nCollocation with `nltk`, on the other hand, worked brilliantly! It turns out to be something that's pretty easy to do but which generates reasonably good results with just a little bit of elbow grease. You can apply this technique to just about about any reservoir of review texts out there, so keep it in mind because it's a useful tool to have under your belt!"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}