{"cells":[{"metadata":{"papermill":{"duration":0.039638,"end_time":"2020-12-22T16:06:48.659412","exception":false,"start_time":"2020-12-22T16:06:48.619774","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### **Problem Understanding**"},{"metadata":{"papermill":{"duration":0.040015,"end_time":"2020-12-22T16:06:48.739455","exception":false,"start_time":"2020-12-22T16:06:48.69944","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Geomagnetic storms are caused by the interaction of solar wind with Earth's magnetic field. The resulting disturbances in the geomagnetic field can wreak havoc on GPS systems, satellite communication, electric power transmission, and more. These disturbances are measured by the Disturbance Storm-Time Index, or [Dst](https://www.ngdc.noaa.gov/stp/GEOMAG/dst.html).\n\nThe task is to forecast Dst in real-time to help satellite operators, power grid operators, and users of magnetic navigation systems prepare for magnetic disturbances.\n\nThe primary input data  is provided by sensor data from two satellites, NASA's [ACE](https://www.swpc.noaa.gov/products/ace-real-time-solar-wind) and NOAA's DSCOVR. This space weather data includes sensor readings related to both the interplanetary magnetic field and plasma from solar wind.\n\n\n> **The Interplanetary Magnetic Field (IMF)**\n>\nThe interplanetary magnetic field (IMF) plays a huge rule in how the solar wind interacts with Earth’s magnetosphere. In this article we will learn what the interplanetary magnetic field is and how it affects auroral activity here on Earth.\n\n> **The Sun’s magnetic field**\n>\nDuring solar minimum, the magnetic field of the Sun looks similar to Earth’s magnetic field. It looks a bit like an ordinary bar magnet with closed lines close to the equator and open field lines near the poles. Scientist call those areas a dipole. The dipole field of the Sun is about as strong as a magnet on a refrigerator (around 50 gauss). The magnetic field of the Earth is about 100 times weaker.\nAround solar maximum, when the Sun reaches her maximum activity, many sunspots are visible on the visible solar disk. These sunspots are filled with magnetism and large magnetic field lines which run material along them. These field lines are often hundreds of times stronger than the surrounding dipole. This causes the magnetic field around the Sun to be a very complex magnetic field with many disturbed field lines.\nThe magnetic field of our Sun doesn’t stay around the Sun itself. The solar wind carries it through the Solar System until it reaches the heliopause. The heliopause is the place where the solar wind comes to a stop and where it collides with the interstellar medium. Because the Sun turns around her axis (once in about 25 days) the interplanetary magnetic field has a spiral shape which is called the Parker Spiral.\n\n> **Bt value**\n>\nThe Bt value of the interplanetary magnetic field indicates the total strength of the interplanetary magnetic field. It is a combined measure of the magnetic field strength in the north-south, east-west, and towards-Sun vs. away-from-Sun directions. The higher this value, the better it is for enhanced geomagnetic conditions. We speak of a moderately strong total interplanetary magnetic field when the Bt exceeds 10nT. Strong values start at 20nT and we speak of a very strong total interplanetary magnetic field when values exceed 30nT. The units are in nano-Tesla (nT) — named after Nikola Tesla, the famous physicist, engineer and inventor.\n\n> **Bx, By and Bz**\n>\nThe interplanetary magnetic field is a vector quantity with a three axis component, two of which (Bx and By) are orientated parallel to the ecliptic. The Bx and By components are not important for auroral activity and are therefor not featured on our website. The third component, the Bz value is perpendicular to the ecliptic and is created by waves and other disturbances in the solar wind."},{"metadata":{"papermill":{"duration":0.037096,"end_time":"2020-12-22T16:06:48.817018","exception":false,"start_time":"2020-12-22T16:06:48.779922","status":"completed"},"tags":[]},"cell_type":"markdown","source":"![im1](https://www.spaceweatherlive.com/images/help/IMF/BxByBz.gif)"},{"metadata":{"papermill":{"duration":0.037952,"end_time":"2020-12-22T16:06:48.893931","exception":false,"start_time":"2020-12-22T16:06:48.855979","status":"completed"},"tags":[]},"cell_type":"markdown","source":"> **Interaction with Earth’s magnetosphere**\n>\nThe north-south direction of the interplanetary magnetic field (Bz) is the most important ingredient for auroral activity. When the north-south direction (Bz) of the the interplanetary magnetic field is orientated southward, it will connect with Earth’s magnetosphere which points northward. Think of the ordinary bar magnets that you have at home. Two opposite poles attract each other! A (strong) southward Bz can create havoc with Earth’s magnetic field, disrupting the magnetosphere and allowing particles to rain down into our atmosphere along Earth’s magnetic field lines. When these particles collide with the oxygen and nitrogen atoms that make up our atmosphere, it causes them to glow and emit light which we see as aurora.\n>\nFor a geomagnetic storm to develop it is vital that the direction of the interplanetary magnetic field (Bz) turns southward. Continues values of -10nT and lower are good indicators that a geomagnetic storm could develop but the lower this value goes the better it is for auroral activity. Only during extreme events with high solar wind speeds it is possible for a geomagnetic storm (Kp5 or higher) to develop with a northward Bz."},{"metadata":{"papermill":{"duration":0.039704,"end_time":"2020-12-22T16:06:48.971391","exception":false,"start_time":"2020-12-22T16:06:48.931687","status":"completed"},"tags":[]},"cell_type":"markdown","source":"\n![im2](https://www.spaceweatherlive.com/images/help/IMF/magnetosphere.jpg)\n A schematic diagram showing the interaction between the IMF with a southward Bz and Earth’s magnetosphere."},{"metadata":{"papermill":{"duration":0.039934,"end_time":"2020-12-22T16:06:49.048864","exception":false,"start_time":"2020-12-22T16:06:49.00893","status":"completed"},"tags":[]},"cell_type":"markdown","source":"> **Measuring the interplanetary magnetic field**\n>\n>The real-time solar wind and interplanetary magnetic field data that you can find on this website come from the Deep Space Climate Observatory (DSCOVR) satellite which is stationed in an orbit around the Sun-Earth Lagrange Point 1. This is a point in space which is always located between the Sun and Earth where the gravity of the Sun and Earth have an equal pull on satellites meaning they can remain in a stable orbit around this point. This point is ideal for solar missions like DSCOVR, as this gives DSCOVR the opportunity to measure the parameters of the solar wind and the interplanetary magnetic field before it arrives at Earth. This gives us a 15 to 60 minute warning time (depending on the solar wind speed) as to what kind of solar wind structures are on their way to Earth.\n>\n>The Deep Space Climate Observatory (DSCOVR) mission is now the primary source for real-time solar wind and interplanetary magnetic field data but there is one more satellite at the Sun-Earth L1 point that measures the incoming solar wind and and that is the Advanced Composition Explorer. This satellite used to be the primary real-time space weather data source up until July 2016 when DSCOVR become fully operational. The Advanced Composition Explorer (ACE) satellite is still collecting data and now operates mostly as a backup to DSCOVR."},{"metadata":{"papermill":{"duration":0.039273,"end_time":"2020-12-22T16:06:49.127593","exception":false,"start_time":"2020-12-22T16:06:49.08832","status":"completed"},"tags":[]},"cell_type":"markdown","source":"*The location of a satellite at the Sun-Earth L1 point.*\n![im3](https://www.spaceweatherlive.com/images/help/zonnewind/L1_animation.gif)\n"},{"metadata":{"papermill":{"duration":0.041993,"end_time":"2020-12-22T16:06:49.206756","exception":false,"start_time":"2020-12-22T16:06:49.164763","status":"completed"},"tags":[]},"cell_type":"markdown","source":"In this notebook  we'll cover how to:\n\n- load the data\n- create features using the timedelta index\n- generate batches of 32-length sequences for training\n- train an LSTM model in Keras"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-22T16:06:49.275947Z","iopub.status.busy":"2020-12-22T16:06:49.275346Z","iopub.status.idle":"2020-12-22T16:06:50.250441Z","shell.execute_reply":"2020-12-22T16:06:50.248906Z"},"papermill":{"duration":1.015684,"end_time":"2020-12-22T16:06:50.250572","exception":false,"start_time":"2020-12-22T16:06:49.234888","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\n\nfrom sklearn.model_selection import train_test_split, KFold, GroupKFold, GridSearchCV, StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\n\nfrom sklearn.metrics import *\n\nimport sys, os\nimport random \n\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")\n    \nfrom IPython import display, utils\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-22T16:06:50.321879Z","iopub.status.busy":"2020-12-22T16:06:50.321284Z","iopub.status.idle":"2020-12-22T16:08:05.160577Z","shell.execute_reply":"2020-12-22T16:08:05.159795Z"},"papermill":{"duration":74.880907,"end_time":"2020-12-22T16:08:05.160706","exception":false,"start_time":"2020-12-22T16:06:50.279799","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"\n\nsolar_wind = pd.read_csv(\"../input/soalr-wind/solar_wind.csv\")\nsolar_wind.timedelta = pd.to_timedelta(solar_wind.timedelta)\nsolar_wind.set_index([\"period\", \"timedelta\"], inplace=True)\n\ndst = pd.read_csv(\"../input/soalr-wind/labels.csv\")\ndst.timedelta = pd.to_timedelta(dst.timedelta)\ndst.set_index([\"period\", \"timedelta\"], inplace=True)\n\nsunspots = pd.read_csv(\"../input/soalr-wind/sunspots.csv\")\nsunspots.timedelta = pd.to_timedelta(sunspots.timedelta)\nsunspots.set_index([\"period\", \"timedelta\"], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-22T16:08:05.22088Z","iopub.status.busy":"2020-12-22T16:08:05.220065Z","iopub.status.idle":"2020-12-22T16:08:05.275253Z","shell.execute_reply":"2020-12-22T16:08:05.275716Z"},"papermill":{"duration":0.086827,"end_time":"2020-12-22T16:08:05.275843","exception":false,"start_time":"2020-12-22T16:08:05.189016","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"dst.groupby(\"period\").describe()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.027215,"end_time":"2020-12-22T16:08:05.330482","exception":false,"start_time":"2020-12-22T16:08:05.303267","status":"completed"},"tags":[]},"cell_type":"markdown","source":"We have nearly 140,000 observations of hourly dst data, representing over 15 years. There are almost twice as many observations in either the train_b or train_c periods than there are train_a. It also seems train_a represents a more intense period, given that it has a lower mean and higher standard deviation. Also note that most of the values are negative.\n\nA very strong magnetic field disturbance has a large Dst value, measured in nano-Teslas (nT). Because these disturbances are usually flowing towards the Earth, the values are negative. Sometimes Dst can be highly positive. During calm conditions, Dst values are situated at or just below 0.\n\n"},{"metadata":{"papermill":{"duration":0.027591,"end_time":"2020-12-22T16:08:05.386121","exception":false,"start_time":"2020-12-22T16:08:05.35853","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### EDA"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-22T16:08:05.460773Z","iopub.status.busy":"2020-12-22T16:08:05.459657Z","iopub.status.idle":"2020-12-22T16:08:05.470602Z","shell.execute_reply":"2020-12-22T16:08:05.471159Z"},"papermill":{"duration":0.056628,"end_time":"2020-12-22T16:08:05.471278","exception":false,"start_time":"2020-12-22T16:08:05.41465","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"print(\"Solar wind shape: \", solar_wind.shape)\nsolar_wind.head()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-22T16:08:05.541671Z","iopub.status.busy":"2020-12-22T16:08:05.540714Z","iopub.status.idle":"2020-12-22T16:08:05.547928Z","shell.execute_reply":"2020-12-22T16:08:05.547267Z"},"papermill":{"duration":0.047744,"end_time":"2020-12-22T16:08:05.548096","exception":false,"start_time":"2020-12-22T16:08:05.500352","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"print(\"Sunspot shape: \", sunspots.shape)\nsunspots.head()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.029333,"end_time":"2020-12-22T16:08:05.607171","exception":false,"start_time":"2020-12-22T16:08:05.577838","status":"completed"},"tags":[]},"cell_type":"markdown","source":"We can see that we're mostly working with floats, except for the solar wind source column which tells us which of the two satellites recorded the data. We also see that the size of the solar_wind data is fairly large, close to 8.4 million rows. That makes sense, given we're working with minutely values.\n\nOn the other hand, we only have 192 observations of monthly sunspot data. When it comes to feature generation, we should think about the best ways to combine these features given their different frequencies.\n\n"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-22T16:08:05.672515Z","iopub.status.busy":"2020-12-22T16:08:05.671591Z","iopub.status.idle":"2020-12-22T16:08:10.422319Z","shell.execute_reply":"2020-12-22T16:08:10.421427Z"},"papermill":{"duration":4.785632,"end_time":"2020-12-22T16:08:10.422441","exception":false,"start_time":"2020-12-22T16:08:05.636809","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"solar_wind.groupby(\"period\").describe().T","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-22T16:08:10.552783Z","iopub.status.busy":"2020-12-22T16:08:10.551852Z","iopub.status.idle":"2020-12-22T16:08:10.596394Z","shell.execute_reply":"2020-12-22T16:08:10.598348Z"},"papermill":{"duration":0.126576,"end_time":"2020-12-22T16:08:10.598536","exception":false,"start_time":"2020-12-22T16:08:10.47196","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"sunspots.groupby(\"period\").describe().T","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.045324,"end_time":"2020-12-22T16:08:10.688626","exception":false,"start_time":"2020-12-22T16:08:10.643302","status":"completed"},"tags":[]},"cell_type":"markdown","source":"The mean and standard deviation of values in train_a is generally more intense than in the other two periods.\n\nAlso our values exist across very different scales. For instance, temperature values are quite high - reaching into the hundreds of thousands Kelvin. Meanwhile, IMF readings are fairly small values, and usually negative. One of the best practices when training deep learning models is scaling your features; we'll definitely want to think about that later on.\n\nLet's do a few visualizations before we move onto feature generation. First, we'll just plot the first 1,000 rows for some of our time series data to get a sense of its shape. Instead of plotting all of our features, let's just choose a few IMF features and a few plasma features."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-22T16:08:10.789223Z","iopub.status.busy":"2020-12-22T16:08:10.788391Z","iopub.status.idle":"2020-12-22T16:08:12.202121Z","shell.execute_reply":"2020-12-22T16:08:12.201561Z"},"papermill":{"duration":1.469206,"end_time":"2020-12-22T16:08:12.202222","exception":false,"start_time":"2020-12-22T16:08:10.733016","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"plt.style.use('fivethirtyeight')\ndef show_raw_visualization(data):\n    fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 15), dpi=80)\n    for i, key in enumerate(data.columns):\n        t_data = data[key]\n        ax = t_data.plot(\n            ax=axes[i // 2, i % 2],\n            title=f\"{key.capitalize()}\",\n            rot=25,color='teal', lw=1.2\n        )\n\n    fig.subplots_adjust(hspace=0.8)\n    plt.tight_layout()\n\n\ncols_to_plot = [\"bx_gse\", \"bx_gsm\", \"bt\", \"density\", \"speed\", \"temperature\"]\nshow_raw_visualization(solar_wind[cols_to_plot].iloc[:1000])","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-22T16:08:12.28085Z","iopub.status.busy":"2020-12-22T16:08:12.279753Z","iopub.status.idle":"2020-12-22T16:08:13.34589Z","shell.execute_reply":"2020-12-22T16:08:13.346388Z"},"papermill":{"duration":1.107492,"end_time":"2020-12-22T16:08:13.346518","exception":false,"start_time":"2020-12-22T16:08:12.239026","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"solar_wind.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.036178,"end_time":"2020-12-22T16:08:13.419607","exception":false,"start_time":"2020-12-22T16:08:13.383429","status":"completed"},"tags":[]},"cell_type":"markdown","source":"The proportion varies by feature, but every one will require some kind of imputation. Turns out sensor readings from space aren't always reliable - instruments are subject to all kinds of nasty space weather. It's up to us to figure out a sensible way of dealing with these missing values.\n\nAnother observation is that the IMF features bx_gsm and bx_gse are closely related. This could present multicollinearity issues if both are present in our model. As our last exploratory step, let's plot a correlation matrix to find other relationships between our features."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-22T16:08:13.499393Z","iopub.status.busy":"2020-12-22T16:08:13.498397Z","iopub.status.idle":"2020-12-22T16:10:34.118029Z","shell.execute_reply":"2020-12-22T16:10:34.11879Z"},"papermill":{"duration":140.663094,"end_time":"2020-12-22T16:10:34.118985","exception":false,"start_time":"2020-12-22T16:08:13.455891","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"joined = solar_wind.join(sunspots).join(dst).fillna(method=\"ffill\")\n\nplt.figure(figsize=(20, 15))\nsns.clustermap(joined.corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.061347,"end_time":"2020-12-22T16:10:34.253436","exception":false,"start_time":"2020-12-22T16:10:34.192089","status":"completed"},"tags":[]},"cell_type":"markdown","source":"- The plasma related features like speed and temperature look to be strongly anti-correlated with Dst. The IMF feature bt also exhibits strong anti-correlation.\n\n- the gsm and gse variables are strongly correlated. We probably want to leave some of those out.\n\n"},{"metadata":{"papermill":{"duration":0.063381,"end_time":"2020-12-22T16:10:34.383258","exception":false,"start_time":"2020-12-22T16:10:34.319877","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Feature Engineering"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-22T16:10:34.501408Z","iopub.status.busy":"2020-12-22T16:10:34.50062Z","iopub.status.idle":"2020-12-22T16:10:38.468256Z","shell.execute_reply":"2020-12-22T16:10:38.467523Z"},"papermill":{"duration":4.036725,"end_time":"2020-12-22T16:10:38.468378","exception":false,"start_time":"2020-12-22T16:10:34.431653","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"from numpy.random import seed\nfrom tensorflow.random import set_seed\n\nseed(2020)\nset_seed(2021)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.048749,"end_time":"2020-12-22T16:10:38.5637","exception":false,"start_time":"2020-12-22T16:10:38.514951","status":"completed"},"tags":[]},"cell_type":"markdown","source":"**We've learned from data exploration:**\n\n\n*1. Features exist across different scales*\n\nWe'll fix this using the StandardScaler from scikit-learn. It's a pretty standard affair - it'll help us subtract the mean and divide by the standard deviation for each feature. What's nice about it is that it'll save the parameters used for scaling so that we can re-use it later during prediction. You could also experiment with using the MinMaxScaler or other scaling methods instead.\n\n*2. Features are provided at different frequencies*\n\nOne easy way to fix this is to aggregate values to the same frequency. Since our dst values are provided hourly, we'll aggregate solar_wind data to the hour. The timedelta API will make that really easy for us to group by the hour. We'll take both the mean and std of each value for each hour. This would be a great place to experiment with different frequencies and aggregations to try and get the best performance out of your model.\n\n*3. Certain IMF features are highly correlated*\n\nWe'll solve this by taking a subset of the data. Let's use the plasma variables temperature, speed, and density, as well as the IMF feature bt. Since we saw that gse and gsm variables were highly correlated, let's only take the gse variables. This is a first pass at subsetting the variables - you may want to take a more principled approach in your implementation.\n\n*4. There are many missing values*\n\nWe've already decided to aggregate our minutely solar wind data to an hourly frequency. That'll help with some of the missing values. For the remaining, there are a few different methods we could try. We want something that will be effective that will also work in a real-time environment. For simplicity, we're going to interpolate between missing values using df.interpolate(). Again, this is another great place for experimentation! You could impute using the mean or median, or you might even think about developing another model to estimate the missing values.\n\nFinally, we have our monthly sunspot numbers. These aren't really \"missing\" - they're just provided at a coarser frequency. We'll fix that by using the forward fill or ffill method of imputation so that the correct monthly number is assigned to each row."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-22T16:10:38.669899Z","iopub.status.busy":"2020-12-22T16:10:38.667984Z","iopub.status.idle":"2020-12-22T16:10:38.670565Z","shell.execute_reply":"2020-12-22T16:10:38.671024Z"},"papermill":{"duration":0.062015,"end_time":"2020-12-22T16:10:38.671157","exception":false,"start_time":"2020-12-22T16:10:38.609142","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# subset of solar wind features to use for modeling\nSOLAR_WIND_FEATURES = [\n    \"bt\",\n    \"temperature\",\n    \"bx_gse\",\n    \"by_gse\",\n    \"bz_gse\",\n    \"speed\",\n    \"density\",\n]\n\n# all of the features we'll use, including sunspot numbers\nXCOLS = (\n    [col + \"_mean\" for col in SOLAR_WIND_FEATURES]\n    + [col + \"_std\" for col in SOLAR_WIND_FEATURES]\n    + [\"smoothed_ssn\"]\n)\n\n\ndef impute_features(feature_df):\n    \"\"\"Imputes data using the following methods:\n    - `smoothed_ssn`: forward fill\n    - `solar_wind`: interpolation\n    \"\"\"\n    # forward fill sunspot data for the rest of the month\n    feature_df.smoothed_ssn = feature_df.smoothed_ssn.fillna(method=\"ffill\")\n    # interpolate between missing solar wind values\n    feature_df = feature_df.interpolate()\n    return feature_df\n\n\ndef aggregate_hourly(feature_df, aggs=[\"mean\", \"std\"]):\n    \"\"\"Aggregates features to the floor of each hour using mean and standard deviation.\n    e.g. All values from \"11:00:00\" to \"11:59:00\" will be aggregated to \"11:00:00\".\n    \"\"\"\n    # group by the floor of each hour use timedelta index\n    agged = feature_df.groupby(\n        [\"period\", feature_df.index.get_level_values(1).floor(\"H\")]\n    ).agg(aggs)\n    # flatten hierachical column index\n    agged.columns = [\"_\".join(x) for x in agged.columns]\n    return agged\n\n\ndef preprocess_features(solar_wind, sunspots, scaler=None, subset=None):\n    \"\"\"\n    Preprocessing steps:\n        - Subset the data\n        - Aggregate hourly\n        - Join solar wind and sunspot data\n        - Scale using standard scaler\n        - Impute missing values\n    \"\"\"\n    # select features we want to use\n    if subset:\n        solar_wind = solar_wind[subset]\n\n    # aggregate solar wind data and join with sunspots\n    hourly_features = aggregate_hourly(solar_wind).join(sunspots)\n\n    # subtract mean and divide by standard deviation\n    if scaler is None:\n        scaler = StandardScaler()\n        scaler.fit(hourly_features)\n\n    normalized = pd.DataFrame(\n        scaler.transform(hourly_features),\n        index=hourly_features.index,\n        columns=hourly_features.columns,\n    )\n\n    # impute missing values\n    imputed = impute_features(normalized)\n\n    # we want to return the scaler object as well to use later during prediction\n    return imputed, scaler\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-22T16:10:38.912453Z","iopub.status.busy":"2020-12-22T16:10:38.910948Z","iopub.status.idle":"2020-12-22T16:10:44.063599Z","shell.execute_reply":"2020-12-22T16:10:44.062986Z"},"papermill":{"duration":5.347921,"end_time":"2020-12-22T16:10:44.063721","exception":false,"start_time":"2020-12-22T16:10:38.7158","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"features, scaler = preprocess_features(solar_wind, sunspots, subset=SOLAR_WIND_FEATURES)\nprint(features.shape)\nfeatures.head()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-22T16:10:44.171485Z","iopub.status.busy":"2020-12-22T16:10:44.170528Z","iopub.status.idle":"2020-12-22T16:10:44.178162Z","shell.execute_reply":"2020-12-22T16:10:44.177672Z"},"papermill":{"duration":0.062002,"end_time":"2020-12-22T16:10:44.178262","exception":false,"start_time":"2020-12-22T16:10:44.11626","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"assert (features.isna().sum() == 0).all()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.042965,"end_time":"2020-12-22T16:10:44.265603","exception":false,"start_time":"2020-12-22T16:10:44.222638","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Our final feature set is composed of 15 features - the mean and standard deviation of seven solar_wind features, along with smoothed_ssn. We've also saved our scaler object. We'll serialize this later along with our model so that it can be used to preprocess features during prediction.\n\nBefore we start modeling, we also have to reshape our label, dst. Remember that we have to predict both t0, the current timestep, and t+1, an hour ahead. We'll train our LSTM to do multi-step prediction by providing it both steps. To do that, we just add another column called t1 that is dst shifted by 1. We'll also renamed our dst column to t0 for consistency."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-22T16:10:44.361515Z","iopub.status.busy":"2020-12-22T16:10:44.360664Z","iopub.status.idle":"2020-12-22T16:10:44.380215Z","shell.execute_reply":"2020-12-22T16:10:44.379616Z"},"papermill":{"duration":0.070148,"end_time":"2020-12-22T16:10:44.380311","exception":false,"start_time":"2020-12-22T16:10:44.310163","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"YCOLS = [\"t0\", \"t1\"]\n\n\ndef process_labels(dst):\n    y = dst.copy()\n    y[\"t1\"] = y.groupby(\"period\").dst.shift(-1)\n    y.columns = YCOLS\n    return y\n\n\nlabels = process_labels(dst)\nlabels.head()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.044644,"end_time":"2020-12-22T16:10:44.469611","exception":false,"start_time":"2020-12-22T16:10:44.424967","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Et voilà! Now we have our features and our labels. Let's join them together into one data df so that it's easier to keep the appropriate rows together when splitting into our train, test, and validation sets."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-22T16:10:44.565671Z","iopub.status.busy":"2020-12-22T16:10:44.564827Z","iopub.status.idle":"2020-12-22T16:10:44.612029Z","shell.execute_reply":"2020-12-22T16:10:44.612542Z"},"papermill":{"duration":0.097597,"end_time":"2020-12-22T16:10:44.612666","exception":false,"start_time":"2020-12-22T16:10:44.515069","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"data = labels.join(features)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.045677,"end_time":"2020-12-22T16:10:44.704378","exception":false,"start_time":"2020-12-22T16:10:44.658701","status":"completed"},"tags":[]},"cell_type":"markdown","source":"We want to split our data into three datasets. As you might have guessed, the train set will be used for training. We'll also pass a validation set to keras as it's training to monitor the modeling fitting over epochs. Finally, we'll use a test set to evaluate our model for over or under fitting before we submit to the competition.\n\nWe have two atypical concerns to consider when splitting this dataset:\n\n1. We're dealing with timeseries data. Observations in a time series are not indepedent, so we cannot randomly assign observations across our datasets. We also don't want to \"cheat\" by leaking future information into our training data. In the real-world, we will never be able to train on data from the future, so we should emulate those same contraints here.\n\n2. We have three non-contiguous periods, meaning we have gaps in our data. We don't know how long each gap is or in what order each period occured. We also know that the three periods are differently distributed. That suggests that observations from each period should be included in our train set, instead of reserving one wholesale as our test or validation set.\n\n*To solve these problems, we'll hold out the last 6,000 rows from each period for our test set, and reserve last 3,000 before that for our validation set. The remaining rows will be used in the training set.*"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-22T16:10:44.806414Z","iopub.status.busy":"2020-12-22T16:10:44.805531Z","iopub.status.idle":"2020-12-22T16:10:49.6528Z","shell.execute_reply":"2020-12-22T16:10:49.651746Z"},"papermill":{"duration":4.902982,"end_time":"2020-12-22T16:10:49.652919","exception":false,"start_time":"2020-12-22T16:10:44.749937","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"def get_train_test_val(data, test_per_period, val_per_period):\n    \"\"\"Splits data across periods into train, test, and validation\"\"\"\n    # assign the last `test_per_period` rows from each period to test\n    test = data.groupby(\"period\").tail(test_per_period)\n    interim = data[~data.index.isin(test.index)]\n    # assign the last `val_per_period` from the remaining rows to validation\n    val = data.groupby(\"period\").tail(val_per_period)\n    # the remaining rows are assigned to train\n    train = interim[~interim.index.isin(val.index)]\n    return train, test, val\n\n\ntrain, test, val = get_train_test_val(data, test_per_period=6_000, val_per_period=3_000)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-22T16:10:49.755883Z","iopub.status.busy":"2020-12-22T16:10:49.755002Z","iopub.status.idle":"2020-12-22T16:10:49.75838Z","shell.execute_reply":"2020-12-22T16:10:49.757769Z"},"papermill":{"duration":0.059017,"end_time":"2020-12-22T16:10:49.758491","exception":false,"start_time":"2020-12-22T16:10:49.699474","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"### Modelingm","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.04488,"end_time":"2020-12-22T16:10:49.852573","exception":false,"start_time":"2020-12-22T16:10:49.807693","status":"completed"},"tags":[]},"cell_type":"markdown","source":"\nThe first thing we have to do is separate our data into sequences and batches for modeling. We have to decide on:\n\ntimesteps: this determines the sequence length, ie. how many timesteps in the past to use to predict each step at t0 and t1. Our data is aggregated hourly, so timesteps is equal to the number of hours we want to use for each prediction.\nbatch_size: this determines the number of samples to work through before a model's parameters are updated.\nFor this tutorial, we'll choose fairly standard numbers of 32 timesteps per sequence and 32 sequences per batch. These numbers will likely have a large impact on your model, so feel free to experiment.\n\nNow we need to use these numbers to separate our training data and labels into batches of sequences that will be fed into the model. Luckily, keras has just the tool with their timeseries_dataset_from_array function. According to the documentation:\n\nIf targets was passed, the dataset yields tuple (batch_of_sequences, batch_of_targets). If not, the dataset yields only batch_of_sequences.\n\nSo We can easily specify timesteps (referred to as sequence_length in the documentation) and batchsize to get a feature generator and a target generator that we can pass to model.fit(). For your implementation, you can experiment with different sequence_lengths along with sequence_stride (how many observations to skip between sequences) and sampling_rate (how many observations to sample per sequence).\n\nThere's one hiccup - we need to make sure that our sequences don't span across periods. To get around that, we'll iterate through our periods and generate a timeseries dataset for each one. Then we'll concatenate them at the end to rejoin our training set and validation set. And let's not forget - since we're only allowed to use feature data up until t-1, we'll need to realign our features and labels. We'll do that during our loop as well."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-22T16:10:49.953773Z","iopub.status.busy":"2020-12-22T16:10:49.953144Z","iopub.status.idle":"2020-12-22T16:10:52.730605Z","shell.execute_reply":"2020-12-22T16:10:52.729813Z"},"papermill":{"duration":2.833258,"end_time":"2020-12-22T16:10:52.73075","exception":false,"start_time":"2020-12-22T16:10:49.897492","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"import tensorflow as tf\nfrom keras import preprocessing\n\n\ndata_config = {\n    \"timesteps\": 32,\n    \"batch_size\": 32,\n}\n\n\ndef timeseries_dataset_from_df(df, batch_size):\n    dataset = None\n    timesteps = data_config[\"timesteps\"]\n\n    # iterate through periods\n    for _, period_df in df.groupby(\"period\"):\n        # realign features and labels so that first sequence of 32 is aligned with the 33rd target\n        inputs = period_df[XCOLS][:-timesteps]\n        outputs = period_df[YCOLS][timesteps:]\n\n        period_ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n            inputs,\n            outputs,\n            timesteps,\n            batch_size=batch_size,\n        )\n\n        if dataset is None:\n            dataset = period_ds\n        else:\n            dataset = dataset.concatenate(period_ds)\n\n    return dataset\n\n\ntrain_ds = timeseries_dataset_from_df(train, data_config[\"batch_size\"])\nval_ds = timeseries_dataset_from_df(val, data_config[\"batch_size\"])\n\nprint(f\"Number of train batches: {len(train_ds)}\")\nprint(f\"Number of val batches: {len(val_ds)}\")\n","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.045905,"end_time":"2020-12-22T16:10:52.826448","exception":false,"start_time":"2020-12-22T16:10:52.780543","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Finally, we need to design our LSTM network. We're going to build a simple sequential model with one hidden LSTM layer and one output later with 2 output values (t0 and t1). You can experiment by making your model deep (many layers) or wide (adding more neurons).\n\nThere are many hyperparameters that we can tune. We'll concentrate on a few here:\n\nn_epochs: This determines the number of complete passes your model takes through the training data. For this tutorial, we'll choose 20 epochs. You'll want to monitor your rate of convergence to tune this number.\nn_neurons: The number of hidden nodes. Usually increases by powers of 2. We'll use 512.\ndropout: This regularizes by randomly \"ignoring\" a dropout fraction of a layer's neurons during each pass through the network during training, so that no particular neuron overfits its input. We'll start with a value of 0.4.\nstateful: This determines whether the model keeps track of the historical data that its seen within each batch. Since each sample within a batch encodes the entire sequence we care about, we can set this to False.\nTuning these values will impact how fast your model learns, whether it converges, and affect over/under fitting. Play around to see what works best.\n\nAfter instantiating the model with these hyperparameters, we'll compile it with mean_squared_error as our loss function and adam as our optimizer. We'll have to remember to take the square root of our loss to get our competition metric, root_mean_squared_error."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-22T16:10:52.928509Z","iopub.status.busy":"2020-12-22T16:10:52.927792Z","iopub.status.idle":"2020-12-22T16:10:53.701079Z","shell.execute_reply":"2020-12-22T16:10:53.699826Z"},"papermill":{"duration":0.82887,"end_time":"2020-12-22T16:10:53.701193","exception":false,"start_time":"2020-12-22T16:10:52.872323","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"\n\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Conv2D, Flatten, Dense,LSTM\n\n# define our model\nmodel_config = {\"n_epochs\": 20, \"n_neurons\": 512, \"dropout\": 0.4, \"stateful\": False}\n\nmodel = Sequential()\nmodel.add(\n    LSTM(\n        model_config[\"n_neurons\"],\n        # usually set to (`batch_size`, `sequence_length`, `n_features`)\n        # setting the batch size to None allows for variable length batches\n        batch_input_shape=(None, data_config[\"timesteps\"], len(XCOLS)),\n        stateful=model_config[\"stateful\"],\n        dropout=model_config[\"dropout\"],\n    )\n)\nmodel.add(Dense(len(YCOLS)))\nmodel.compile(\n    loss=\"mean_squared_error\",\n    optimizer=\"adam\",\n)\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-22T16:10:53.806114Z","iopub.status.busy":"2020-12-22T16:10:53.805506Z","iopub.status.idle":"2020-12-22T16:25:39.467106Z","shell.execute_reply":"2020-12-22T16:25:39.466075Z"},"papermill":{"duration":885.71851,"end_time":"2020-12-22T16:25:39.467229","exception":false,"start_time":"2020-12-22T16:10:53.748719","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"history = model.fit(\n    train_ds,\n    batch_size=data_config[\"batch_size\"],\n    epochs=model_config[\"n_epochs\"],\n    verbose=1,\n    shuffle=False,\n    validation_data=val_ds,\n)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-22T16:25:50.947364Z","iopub.status.busy":"2020-12-22T16:25:50.944543Z","iopub.status.idle":"2020-12-22T16:25:51.08013Z","shell.execute_reply":"2020-12-22T16:25:51.080656Z"},"papermill":{"duration":5.820673,"end_time":"2020-12-22T16:25:51.080807","exception":false,"start_time":"2020-12-22T16:25:45.260134","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"for name, values in history.history.items():\n    plt.plot(values)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-22T16:26:01.965492Z","iopub.status.busy":"2020-12-22T16:26:01.964777Z","iopub.status.idle":"2020-12-22T16:26:05.934657Z","shell.execute_reply":"2020-12-22T16:26:05.934091Z"},"papermill":{"duration":9.474338,"end_time":"2020-12-22T16:26:05.934791","exception":false,"start_time":"2020-12-22T16:25:56.460453","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"test_ds = timeseries_dataset_from_df(test, data_config[\"batch_size\"])\nmse = model.evaluate(test_ds)\nprint(f\"Test RMSE: {mse**.5:.2f}\")","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-22T16:26:17.495453Z","iopub.status.busy":"2020-12-22T16:26:17.494138Z","iopub.status.idle":"2020-12-22T16:26:21.75283Z","shell.execute_reply":"2020-12-22T16:26:21.75193Z"},"papermill":{"duration":9.662997,"end_time":"2020-12-22T16:26:21.752943","exception":false,"start_time":"2020-12-22T16:26:12.089946","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"import json\nimport pickle\n\nmodel.save(\"model\")\n\nwith open(\"scaler.pck\", \"wb\") as f:\n    pickle.dump(scaler, f)\n\ndata_config[\"solar_wind_subset\"] = SOLAR_WIND_FEATURES\nprint(data_config)\nwith open(\"config.json\", \"w\") as f:\n    json.dump(data_config, f)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":5.68582,"end_time":"2020-12-22T16:26:32.812315","exception":false,"start_time":"2020-12-22T16:26:27.126495","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}