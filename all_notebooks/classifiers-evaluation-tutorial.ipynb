{"cells":[{"metadata":{},"cell_type":"markdown","source":"Sample Data to Simulate Cancer Prediction \n================================================","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Overview\n\nThis sample data is for requirements purposes only. Data is used to classify if the patient has breast cancer or not.","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Load csv file\n\ndf = pd.read_csv('/kaggle/input/bcancer/Breastcancer_data.csv') \n\n# Shape shows you the # of samples and the number of features / cols you have in your data set.\nprint(f'Data Frame Shape (rows, columns): {df.shape}') \n\n# This method is one of your basic tool to quickly view data. It shows first 5 rows of a frame\ndf.head() \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data Analysis and Exploration\n=============================","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"sns.pairplot(df, hue=\"diagnosis\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Visualize outcome of classes where the zero value means that its negative with cancer and one is positive with cancer\nsns.countplot(data=df, x=\"diagnosis\").set_title(\"Result\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The classes were plotted based on their no. of instances in the data set.","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"sns.relplot(data=df, x=\"mean_radius\", y=\"mean_area\", hue=\"diagnosis\", palette=\"bright\", height=6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Correlation between mass area and radius**\n\nThe graph shows correlation between the mean area and mean radius in which it can be observed that as the area of coverage gets bigger the radius also increases.\n\nIt's also observed that the diagnosis on datasets having lower mean area and mean radius would likely result to a test being positive with cancer.\n","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"sns.relplot(data=df, x=\"mean_texture\", y=\"mean_smoothness\", hue=\"diagnosis\", palette=\"bright\", height=6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Correlation between smoothness and texture**\n\nThe 2nd graph shows the correlation between the mean smoothness and mean texture. Based on the result, the positive result are from the datasets with lower values on the smoothness and the texture.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Data Preparation, Balancing and Cleanup\n===========================================","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Check if there are any null values\ndf.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Remove null values\ndf = df.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Check if there are any null values\ndf.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Classifier Setups and Build Model\n=================================","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Import Necessary Libraries for Scoring and Evaluation**","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Import required libraries for performance metrics\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This function returns an ordered list of the performance measures.\n\nTP is True Positives\nFP is False Positives\nTN is True Negative\nFN is False Negative","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def get_performance_measures(actual, prediction):\n    matrix = confusion_matrix(actual, prediction)\n    FP = matrix.sum(axis=0) - np.diag(matrix)  \n    FN = matrix.sum(axis=1) - np.diag(matrix)\n    TP = np.diag(matrix)\n    TN = matrix.sum() - (FP + FN + TP)\n\n    return(TP, FP, TN, FN)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Below we create our custom scorers. A scorer is basically a benchmark of how well your model performs given an actual result and a predicted result.\n","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"#Custom Scorers\n\n# # Sensitivity, hit rate, recall, or true positive rate\n# TPR = TP/(TP+FN)\n# # Specificity or true negative rate\n# TNR = TN/(TN+FP) \n# # Precision or positive predictive value\n# PPV = TP/(TP+FP)\n# # Negative predictive value\n# NPV = TN/(TN+FN)\n# # Fall out or false positive rate\n# FPR = FP/(FP+TN)\n# # False negative rate\n# FNR = FN/(TP+FN)\n# # False discovery rate\n# FDR = FP/(TP+FP)\n\n# # Overall accuracy\n# ACC = (TP+TN)/(TP+FP+FN+TN)\n\n# Also remember:\n# specificity = true negative rate\n# sensitivity = true positive rate\n\ndef sensitivity_score(y_true, y_pred, mode=\"multiclass\"):\n    if mode == \"multiclass\":\n        TP, FP, TN, FN = get_performance_measures(y_true, y_pred)\n        TPR = (TP/(TP+FN)).mean()\n    elif mode == \"binary\":\n        TP, FP, TN, FN = get_performance_measures(y_true, y_pred)\n        TPR = (TP/(TP+FN))[1] # Since the [0] part is the index\n    else:\n        raise Exception(\"Mode not recognized!\")\n    \n    return TPR\n\ndef specificity_score(y_true, y_pred, mode=\"multiclass\"):\n    if mode == \"multiclass\":\n        TP, FP, TN, FN = get_performance_measures(y_true, y_pred)\n        TNR = (TN/(TN+FP)).mean()\n    elif mode == \"binary\":\n        TP, FP, TN, FN = get_performance_measures(y_true, y_pred)\n        TNR = (TN/(TN+FP))[1]\n    else:\n        raise Exception(\"Mode not recognized!\")\n    \n    return TNR","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Setup Our Scorers**","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Define dictionary with performance metrics\n# To know what everaging to use: https://stats.stackexchange.com/questions/156923/should-i-make-decisions-based-on-micro-averaged-or-macro-averaged-evaluation-mea#:~:text=So%2C%20micro%2Daveraged%20measures%20add,is%20more%20like%20an%20average.\n\n\nscoring = {\n            'accuracy':make_scorer(accuracy_score), \n            'precision':make_scorer(precision_score, average='weighted'),\n            'f1_score':make_scorer(f1_score, average='weighted'),\n            'recall':make_scorer(recall_score, average='weighted'), \n            'sensitvity':make_scorer(sensitivity_score, mode=\"multiclass\"), \n            'specificity':make_scorer(specificity_score, mode=\"multiclass\"), \n           }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Setting up our Classifiers**","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# Import required libraries for machine learning classifiers\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.naive_bayes import GaussianNB #Naive Bayes\nfrom sklearn.linear_model import LogisticRegression #Logistic Regression\nfrom sklearn.svm import LinearSVC # Support Vector Machine\nfrom sklearn.neighbors import KNeighborsClassifier #K-nearest Neighbors\nfrom sklearn.cluster import KMeans #K-means\nfrom sklearn.ensemble import RandomForestClassifier #random forest\n\n# Instantiate the machine learning classifiers\ndecisionTreeClassifier_model = DecisionTreeClassifier()\ngaussianNB_model = GaussianNB()\nlogisticRegression_model = LogisticRegression(max_iter=10000)\nlinearSVC_model = LinearSVC(dual=False)\nkNeighbors_model = KNeighborsClassifier()\nrandom_forest_model = RandomForestClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# features = data frame set that contain your features that will be used as input to see if prediction is equal to actual result\n# target = data frame set (1 column usually) that will contain your target or actual results.\n# folds = this is added so we can easily change the number of folds we want to do with our data set.\n# folding is a technique to minimise overfitting and therefore make our model more accurate.\ndef models_evaluation(features, target, folds):    \n    # Perform cross-validation to each machine learning classifier\n    decisionTreeClassifier_result = cross_validate(decisionTreeClassifier_model, features, target, cv=folds, scoring=scoring)\n    gaussianNB_result = cross_validate(gaussianNB_model, features, target, cv=folds, scoring=scoring)\n    logisticRegression_result = cross_validate(logisticRegression_model, features, target, cv=folds, scoring=scoring)\n    linearSVC_result = cross_validate(linearSVC_model, features, target, cv=folds, scoring=scoring)\n    kNeighbors_result = cross_validate(kNeighbors_model, features, target, cv=folds, scoring=scoring)\n    randomforest_result = cross_validate(random_forest_model, features, target, cv=folds, scoring=scoring)\n    # kMeans_result = cross_validate(kMeans_model, features, target, cv=folds, scoring=scoring)\n\n    # Create a data frame with the models perfoamnce metrics scores\n    models_scores_table = pd.DataFrame({\n      'Decision Tree':[\n                        decisionTreeClassifier_result['test_accuracy'].mean(),\n                        decisionTreeClassifier_result['test_precision'].mean(),\n                        decisionTreeClassifier_result['test_recall'].mean(),\n                        decisionTreeClassifier_result['test_sensitvity'].mean(),\n                        decisionTreeClassifier_result['test_specificity'].mean(),\n                        decisionTreeClassifier_result['test_f1_score'].mean()\n                       ],\n\n      'Gaussian Naive Bayes':[\n                                gaussianNB_result['test_accuracy'].mean(),\n                                gaussianNB_result['test_precision'].mean(),\n                                gaussianNB_result['test_recall'].mean(),\n                                gaussianNB_result['test_sensitvity'].mean(),\n                                gaussianNB_result['test_specificity'].mean(),\n                                gaussianNB_result['test_f1_score'].mean()\n                              ],\n\n      'Logistic Regression':[\n                                logisticRegression_result['test_accuracy'].mean(),\n                                logisticRegression_result['test_precision'].mean(),\n                                logisticRegression_result['test_recall'].mean(),\n                                logisticRegression_result['test_sensitvity'].mean(),\n                                logisticRegression_result['test_specificity'].mean(),\n                                logisticRegression_result['test_f1_score'].mean()\n                            ],\n\n      'Support Vector Classifier':[\n                                    linearSVC_result['test_accuracy'].mean(),\n                                    linearSVC_result['test_precision'].mean(),\n                                    linearSVC_result['test_recall'].mean(),\n                                    linearSVC_result['test_sensitvity'].mean(),\n                                    linearSVC_result['test_specificity'].mean(),\n                                    linearSVC_result['test_f1_score'].mean()\n                                   ],\n\n       'K-nearest Neighbors':[\n                        kNeighbors_result['test_accuracy'].mean(),\n                        kNeighbors_result['test_precision'].mean(),\n                        kNeighbors_result['test_recall'].mean(),\n                        kNeighbors_result['test_sensitvity'].mean(),\n                        kNeighbors_result['test_specificity'].mean(),\n                        kNeighbors_result['test_f1_score'].mean()\n                       ],\n        'Random Forest':[\n                        randomforest_result['test_accuracy'].mean(),\n                        randomforest_result['test_precision'].mean(),\n                        randomforest_result['test_recall'].mean(),\n                        randomforest_result['test_sensitvity'].mean(),\n                        randomforest_result['test_specificity'].mean(),\n                        randomforest_result['test_f1_score'].mean()\n                        ],\n      },\n\n      index=['Accuracy', 'Precision', 'Recall', 'Sensitivity', 'Specificity', 'F1 Score', ])\n    \n    # Return models performance metrics scores data frame\n    return(models_scores_table)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Preparing Features and Targets**","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"features = df.drop(columns=\"diagnosis\", axis=0)\n\nfeatures.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"\ntarget = df[\"diagnosis\"]\n\ntarget","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Running our Evaluation**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluationResult = models_evaluation(features, target, 6)\nview = evaluationResult\nview = view.rename_axis('Test Type').reset_index() #Add the index names to the column. This will be used for our presentation\n\n# https://pandas.pydata.org/docs/reference/api/pandas.melt.html\n# Re-Organizing our dataframe to fit our view need\nview = view.melt(var_name='Classifier', value_name='Value', id_vars='Test Type')\n# result\nsns.catplot(data=view, x=\"Test Type\", y=\"Value\", hue=\"Classifier\", kind='bar', palette=\"bright\", alpha=0.8, legend=True, height=5, margin_titles=True, aspect=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# In here we just add a new column to our raw data frame, that gets the result for the highest\n# scoring classifier in every score test.\nevaluationResult['Best Score'] = evaluationResult.idxmax(axis=1)\nevaluationResult","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\nSo in our findings, for this particular data set and classifiers used,** random forest** is by far the most performing and accurate classifier. \n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}