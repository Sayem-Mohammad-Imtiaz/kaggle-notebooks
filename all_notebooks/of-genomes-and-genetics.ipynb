{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-28T19:21:31.535766Z","iopub.execute_input":"2021-07-28T19:21:31.536123Z","iopub.status.idle":"2021-07-28T19:21:31.540491Z","shell.execute_reply.started":"2021-07-28T19:21:31.536086Z","shell.execute_reply":"2021-07-28T19:21:31.539409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = pd.read_csv('../input/of-genomes-and-genetics-hackerearth-ml-challenge/train.csv')\ndf2 = pd.read_csv('../input/of-genomes-and-genetics-hackerearth-ml-challenge/test.csv')\ndf = pd.concat([df1,df2],axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:21:35.355608Z","iopub.execute_input":"2021-07-28T19:21:35.356125Z","iopub.status.idle":"2021-07-28T19:21:35.817949Z","shell.execute_reply.started":"2021-07-28T19:21:35.356079Z","shell.execute_reply":"2021-07-28T19:21:35.817087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dropping irrelevant features \ndf = df.drop(['Patient Id','Patient First Name','Family Name',\"Father's name\",],axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:21:35.819428Z","iopub.execute_input":"2021-07-28T19:21:35.819745Z","iopub.status.idle":"2021-07-28T19:21:35.860149Z","shell.execute_reply.started":"2021-07-28T19:21:35.819709Z","shell.execute_reply":"2021-07-28T19:21:35.859313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:21:36.140441Z","iopub.execute_input":"2021-07-28T19:21:36.140778Z","iopub.status.idle":"2021-07-28T19:21:36.149095Z","shell.execute_reply.started":"2021-07-28T19:21:36.140747Z","shell.execute_reply":"2021-07-28T19:21:36.14797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:21:36.450498Z","iopub.execute_input":"2021-07-28T19:21:36.450814Z","iopub.status.idle":"2021-07-28T19:21:36.535129Z","shell.execute_reply.started":"2021-07-28T19:21:36.450782Z","shell.execute_reply":"2021-07-28T19:21:36.534295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:21:36.715041Z","iopub.execute_input":"2021-07-28T19:21:36.715302Z","iopub.status.idle":"2021-07-28T19:21:36.785356Z","shell.execute_reply.started":"2021-07-28T19:21:36.715276Z","shell.execute_reply":"2021-07-28T19:21:36.784299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# droping na values\ndf.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:21:36.975355Z","iopub.execute_input":"2021-07-28T19:21:36.975607Z","iopub.status.idle":"2021-07-28T19:21:37.064244Z","shell.execute_reply.started":"2021-07-28T19:21:36.975582Z","shell.execute_reply":"2021-07-28T19:21:37.063462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# seprating categorical and numerical data\ndef sep(df):\n    cat_df = []\n    num_df = []\n    for i in df.columns:\n        if df[i].dtypes == 'object':\n            cat_df.append(i)\n        else:\n            num_df.append(i)\n    cat_df = df[cat_df]\n    num_df = df[num_df]\n    return cat_df, num_df\ncat_df,num_df = sep(df)\nprint(cat_df.columns)\nprint('@@@@')\nprint(num_df.columns)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:21:37.295246Z","iopub.execute_input":"2021-07-28T19:21:37.295513Z","iopub.status.idle":"2021-07-28T19:21:37.308835Z","shell.execute_reply.started":"2021-07-28T19:21:37.295487Z","shell.execute_reply":"2021-07-28T19:21:37.307937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# label encoding categorical value and getting name of numerical and categorical label\ndef leb(df):\n    from sklearn.preprocessing import LabelEncoder\n    label = LabelEncoder()\n    for i in df.columns:\n        if df[i].dtypes == 'object':\n            df[i] = label.fit_transform(df[i])\n    return df\ncat_df = leb(cat_df)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:21:37.780488Z","iopub.execute_input":"2021-07-28T19:21:37.780809Z","iopub.status.idle":"2021-07-28T19:21:37.975955Z","shell.execute_reply.started":"2021-07-28T19:21:37.780777Z","shell.execute_reply":"2021-07-28T19:21:37.974416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking no of unique value , if unique value is 1 it will remove that label\ndef check_value(df):\n    for i in df.columns:\n        if len(df[i].unique())<2:\n            print(f'{i} -> {len(df[i].unique())}')\n            df = df.drop(i,axis=1)\n    return df\n\nnum_df = check_value(num_df)\ncat_df = check_value(cat_df)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:21:38.350286Z","iopub.execute_input":"2021-07-28T19:21:38.350567Z","iopub.status.idle":"2021-07-28T19:21:38.372129Z","shell.execute_reply.started":"2021-07-28T19:21:38.350539Z","shell.execute_reply":"2021-07-28T19:21:38.371165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(50,50))\nsns.heatmap((pd.concat([num_df,cat_df],axis = 1)).corr(),linewidths=0.5,annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:21:39.325446Z","iopub.execute_input":"2021-07-28T19:21:39.325763Z","iopub.status.idle":"2021-07-28T19:21:46.42515Z","shell.execute_reply.started":"2021-07-28T19:21:39.325734Z","shell.execute_reply":"2021-07-28T19:21:46.421988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from heatmap we observe that the feature named \"Location of Institute\" highly correlate with \"Place of birth\" \n# so we can drop any one of them which has more number of unique value and that is \"Location of Institute\" \n# and it is categorical data\ncat_df = cat_df.drop([\"Location of Institute\"],axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:21:46.426479Z","iopub.execute_input":"2021-07-28T19:21:46.426813Z","iopub.status.idle":"2021-07-28T19:21:46.437935Z","shell.execute_reply.started":"2021-07-28T19:21:46.426776Z","shell.execute_reply":"2021-07-28T19:21:46.436849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feature engnieering\n# feature selection in numerical df with numerical filter low variance\n# below function will remove the features with low variance (lower the variance lower impact on the targeted variable)\ndef num_feature_eng(df):\n    from sklearn.preprocessing import normalize\n    norm = normalize(df)\n    scl = pd.DataFrame(norm).var()\n    l = []\n    for i in range(len(scl)):\n        if scl[i]<0.005:\n            l.append(df.columns[i])\n    return l\nlst = num_feature_eng(num_df)\nnum_df = num_df.drop(lst,axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:21:46.43957Z","iopub.execute_input":"2021-07-28T19:21:46.440126Z","iopub.status.idle":"2021-07-28T19:21:46.462614Z","shell.execute_reply.started":"2021-07-28T19:21:46.440088Z","shell.execute_reply":"2021-07-28T19:21:46.461799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_df","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:21:46.805269Z","iopub.execute_input":"2021-07-28T19:21:46.805577Z","iopub.status.idle":"2021-07-28T19:21:46.825603Z","shell.execute_reply.started":"2021-07-28T19:21:46.805547Z","shell.execute_reply":"2021-07-28T19:21:46.824909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# categorical feature selection with help of chi square test\n# there are two targeted variables so we have two find relev\n\nx1 = cat_df.drop(['Genetic Disorder', 'Disorder Subclass'],axis = 1)\ny1 = cat_df['Genetic Disorder']\ny2 = cat_df['Disorder Subclass']\n\n# this funtion will return the list of relevant features, who have p_value less than 00.5 \ndef cat_feat_sel(x,y):\n    from sklearn.feature_selection import chi2\n    l = []\n    f_score = chi2(x,y)\n    for i in range(len(f_score[1])):\n        if f_score[1][i]<0.6:\n            l.append(list(x.columns.values.tolist())[i])\n    return l\n\nl1 = cat_feat_sel(x1,y1)\nX1 = pd.concat([num_df,cat_df[l1]],axis=1)\n\nl2 = cat_feat_sel(x1,y2)\nX2 = pd.concat([num_df,cat_df[l2]],axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T20:13:33.665789Z","iopub.execute_input":"2021-07-28T20:13:33.666157Z","iopub.status.idle":"2021-07-28T20:13:33.690143Z","shell.execute_reply.started":"2021-07-28T20:13:33.666125Z","shell.execute_reply":"2021-07-28T20:13:33.689365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now we have made two sets of feature X1 for \"Genetic Disoder\" and X2 for 'Disorder Subclass'\nX1.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-28T20:18:48.031321Z","iopub.execute_input":"2021-07-28T20:18:48.031657Z","iopub.status.idle":"2021-07-28T20:18:48.046233Z","shell.execute_reply.started":"2021-07-28T20:18:48.031626Z","shell.execute_reply":"2021-07-28T20:18:48.045462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X2.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-28T20:18:54.67741Z","iopub.execute_input":"2021-07-28T20:18:54.677742Z","iopub.status.idle":"2021-07-28T20:18:54.692665Z","shell.execute_reply.started":"2021-07-28T20:18:54.67771Z","shell.execute_reply":"2021-07-28T20:18:54.691919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now its time slice data into test and train\nfrom sklearn.model_selection import train_test_split\nX1_train, X1_test, y1_train, y1_test = train_test_split(X1,y1,test_size = 0.3)\nX2_train, X2_test, y2_train, y2_test = train_test_split(X2,y2,test_size = 0.3)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T20:23:26.021363Z","iopub.execute_input":"2021-07-28T20:23:26.021682Z","iopub.status.idle":"2021-07-28T20:23:26.032403Z","shell.execute_reply.started":"2021-07-28T20:23:26.021651Z","shell.execute_reply":"2021-07-28T20:23:26.031539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this function will return best classifier model on the basis of accuracy score\ndef best_model(X_train, X_test, y_train, y_test):\n    from sklearn.linear_model import LogisticRegression,SGDClassifier\n    from sklearn.naive_bayes import GaussianNB\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.svm import SVC\n    from sklearn.metrics import accuracy_score\n    l = []\n    \n    m1 = LogisticRegression()\n    m1.fit(X_train,y_train)\n    y_p1 = m1.predict([X_test])\n    a1 = accuracy_score(y_test,y_p1)\n    l.append([a1,'Logistic regression'])\n    \n    m2 = SGDClassifier()\n    m2.fit(X_train,y_train)\n    y_p2 = m2.predict([X_test])\n    a2 = accuracy_score(y_test,y_p2)\n    l.append([a2,\"SGD\"])\n    \n    m3 = GaussianNB()\n    m3.fit(X_train,y_train)\n    y_p3 = m3.predict([X_test])\n    a3 = accuracy_score(y_test,y_p3)\n    l.append([a3,'Gaussion NB'])\n    \n    m4 = KNeighborsClassifier()\n    m4.fit(X_train,y_train)\n    y_p4 = m1.predict([X_test])\n    a4 = accuracy_score(y_test,y_p4)\n    l.append([a4,'KNN'])\n    \n    m5 = DecisionTreeClassifier()\n    m5.fit(X_train,y_train)\n    y_p5 = m1.predict([X_test])\n    a5 = accuracy_score(y_test,y_p5)\n    l.append([a5,'Decession tree'])\n    \n    m6 = RandomForestClassifier()\n    m6.fit(X_train,y_train)\n    y_p6 = m1.predict([X_test])\n    a6 = accuracy_score(y_test,y_p6)\n    l.append([a6,'Random forest'])\n    \n    m7 = SVC()\n    m7.fit(X_train,y_train)\n    y_p7 = m1.predict([X_test])\n    a7 = accuracy_score(y_test,y_p7)\n    l.append([a7,'SVC'])\n    return l\nbest_model(X1_train, X1_test, y1_train, y1_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T20:44:59.751955Z","iopub.execute_input":"2021-07-28T20:44:59.752283Z","iopub.status.idle":"2021-07-28T20:45:00.163343Z","shell.execute_reply.started":"2021-07-28T20:44:59.75225Z","shell.execute_reply":"2021-07-28T20:45:00.16118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nm1 = LogisticRegression()\nm1.fit(X1_train,y1_train)\ny_p1 = m1.predict([X1_test])\na1 = accuracy_score(y1_test,y_p1)\na1","metadata":{"execution":{"iopub.status.busy":"2021-07-28T20:46:54.39646Z","iopub.execute_input":"2021-07-28T20:46:54.396784Z","iopub.status.idle":"2021-07-28T20:46:54.604829Z","shell.execute_reply.started":"2021-07-28T20:46:54.396752Z","shell.execute_reply":"2021-07-28T20:46:54.602473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y1_train","metadata":{"execution":{"iopub.status.busy":"2021-07-28T20:47:26.55643Z","iopub.execute_input":"2021-07-28T20:47:26.556753Z","iopub.status.idle":"2021-07-28T20:47:26.562781Z","shell.execute_reply.started":"2021-07-28T20:47:26.556722Z","shell.execute_reply":"2021-07-28T20:47:26.562027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}