{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Intro","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-21T13:46:39.939987Z","iopub.execute_input":"2021-06-21T13:46:39.940464Z","iopub.status.idle":"2021-06-21T13:46:42.466352Z","shell.execute_reply.started":"2021-06-21T13:46:39.940393Z","shell.execute_reply":"2021-06-21T13:46:42.465193Z"}}},{"cell_type":"markdown","source":"Recommender systems can be used to evaluate cross selling opportunities on the domain of retail marketing. Further reading about the key concepts of \"recommender systems\",  \"cross selling\", \"collaborative filtering\" and \"deep learning\" and their applications, one can look related papers such as:\n\n1. Kamakura, W. A., Wedel, M., de Rosa, F., Mazzon, J. A. (2003), Cross-selling through database marketing: A mixed data factor analyzer for data augmentation and prediction. International Journal of Research in Marketing, 20, 45–65.\n\n2. Knott, A., Hayes, A., & Neslin, S. A. (2002), Next-product-to-buy models for cross-selling applications. Journal of Interactive Marketing, 16(3), 59–75.\n\n3. Thuring F., Nielsen J.P., Guillén M., Bolancé C.,(2012), Selecting prospects for cross-selling ﬁnancial products using multivariate credibility, Expert Systems with Applications 39, 8809–8816.\n\n4. Zhang S., Yao L., Sun A., Tay Y., (2018), Deep Learning based Recommender System: A Survey and New Perspectives. ACM Comput. Surv. 1(1), 1-35.\n\n5. Shi Y., Larson M., Hanjalic A., (2014), Collaborative filtering beyond the user-item matrix: A survey of the state of the art and future challenges. ACM Comput. Surv. 47(1) 45. DOI: http://dx.doi.org/10.1145/2556270\n\n6. Hidasi B., Karatzoglou A., (2018), Recurrent Neural Networks with Top-k Gains for Session-based Recommendations. In The 27th ACM International Conference on Information and Knowledge Management (CIKM ’18), October 22–26, 2018, Torino, Italy. ACM, New York, NY, USA, 10 pages. DOI: https://doi.org/10.1145/3269206.3271761\n\n.\n.","metadata":{}},{"cell_type":"markdown","source":"We try to implement a collaborative filtering system using embedding layers for user-item instances. We, mainly aimed at prediction of customers next products to buy using implicit feedback from purchase preferences. Python code mostly adapted from the notebooks of:   \n\n* [colinmorris-1](https://www.kaggle.com/colinmorris/embedding-layers)\n* [colinmorris-2](https://www.kaggle.com/colinmorris/matrix-factorization)\n* [rajmehra03-1](https://www.kaggle.com/rajmehra03/a-detailed-explanation-of-keras-embedding-layer)\n* [rajmehra03-2](https://www.kaggle.com/rajmehra03/cf-based-recsys-by-low-rank-matrix-factorization)\n* [rounakbanik](https://www.kaggle.com/rounakbanik/movie-recommender-systems)\n* [keras.io examples](https://keras.io/examples/structured_data/collaborative_filtering_movielens/)\n\nand we try to evaluate the system based on the instructions from: \n\n[jamesloy](https://www.kaggle.com/jamesloy/deep-learning-based-recommender-systems)\n\nDataset choosen, famous, Online Retail II. For detailed information please visit:\n\n[https://www.kaggle.com/mashlyn/online-retail-ii-uci](https://www.kaggle.com/mashlyn/online-retail-ii-uci)\n\nand for the original source:\n\n[UCI Repository](https://archive.ics.uci.edu/ml/datasets/Online+Retail+II)","metadata":{}},{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","metadata":{"execution":{"iopub.status.busy":"2021-07-18T12:30:31.493217Z","iopub.execute_input":"2021-07-18T12:30:31.493568Z","iopub.status.idle":"2021-07-18T12:30:38.734121Z","shell.execute_reply.started":"2021-07-18T12:30:31.493483Z","shell.execute_reply":"2021-07-18T12:30:38.733089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data & Preparation","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"../input/online-retail-ii-uci/online_retail_II.csv\",\n                   parse_dates=[\"InvoiceDate\"],\n                   dtype={\"Customer ID\":\"object\"})","metadata":{"execution":{"iopub.status.busy":"2021-07-18T12:30:45.2153Z","iopub.execute_input":"2021-07-18T12:30:45.215651Z","iopub.status.idle":"2021-07-18T12:30:47.923412Z","shell.execute_reply.started":"2021-07-18T12:30:45.215623Z","shell.execute_reply":"2021-07-18T12:30:47.922141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = data.copy()\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-18T12:30:51.188063Z","iopub.execute_input":"2021-07-18T12:30:51.188341Z","iopub.status.idle":"2021-07-18T12:30:51.249103Z","shell.execute_reply.started":"2021-07-18T12:30:51.188318Z","shell.execute_reply":"2021-07-18T12:30:51.247613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## A brief of data cleaning\n\nDroping rows with missing values and irrelevant labels","metadata":{}},{"cell_type":"code","source":"df = df.dropna()\ndf = df.drop(df[df[\"Quantity\"]<0].index)\ndf = df.drop(df[df[\"StockCode\"].str.contains(\"TEST\")].index)\ndf = df.drop(df[df[\"StockCode\"]==\"POST\"].index)\n\ndf = df.sort_values(\"InvoiceDate\")","metadata":{"execution":{"iopub.status.busy":"2021-07-18T12:30:55.999875Z","iopub.execute_input":"2021-07-18T12:30:56.00016Z","iopub.status.idle":"2021-07-18T12:30:57.007941Z","shell.execute_reply.started":"2021-07-18T12:30:56.000136Z","shell.execute_reply":"2021-07-18T12:30:57.006831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Common Functions\n\nFirst function can be used to obtain lists having unique elements. Second, for generating product purchase sequences and a target sequence having \"n_target\" length occuring after a sequence of product purchased. The last one for generating a negative sample.  ","metadata":{}},{"cell_type":"code","source":"def unique(list1):\n    list_set = set(list1)\n    unique_list = (list(list_set))\n    return unique_list\n\ndef generate_sequence(serie, n_target):\n    input_sequence = []\n    output_sequence = []\n    for x in serie:\n        x = unique(x)\n        if len(x)>n_target:\n            input_sequence.append(x[:-n_target])\n            output_sequence.append(x[-n_target:])\n    return input_sequence, output_sequence\n\ndef agg(x, corp, sample_size=1):\n    diff = np.setdiff1d(corp, list(x))\n    ind = np.random.permutation(len(diff))\n    return diff[ind[:int(sample_size*len(x))]]","metadata":{"execution":{"iopub.status.busy":"2021-07-18T12:31:03.861566Z","iopub.execute_input":"2021-07-18T12:31:03.861846Z","iopub.status.idle":"2021-07-18T12:31:03.882108Z","shell.execute_reply.started":"2021-07-18T12:31:03.861822Z","shell.execute_reply":"2021-07-18T12:31:03.881076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By following cells, we try to generate customers' purchase sequences of distinct products. ","metadata":{}},{"cell_type":"code","source":"by_customer = df.groupby(\"Customer ID\", as_index=False).agg(\n    {\"StockCode\": [lambda x: list(x)]}\n)\nsequential_df = by_customer[\"StockCode\"].rename(\n    columns={\"<lambda>\":\"purchase_sequence\"}\n)\nsequential_df[\"CustomerID\"] = by_customer[\"Customer ID\"]\nsequential_df[\"product_count\"] = sequential_df[\"purchase_sequence\"].apply(\n    lambda x: len(unique(list(x)))\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T12:31:08.022177Z","iopub.execute_input":"2021-07-18T12:31:08.022452Z","iopub.status.idle":"2021-07-18T12:31:08.507277Z","shell.execute_reply.started":"2021-07-18T12:31:08.022428Z","shell.execute_reply":"2021-07-18T12:31:08.505985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We choose some hyperparameter values arbitrarily but it can be a good practice to look at some statistics like below: number of distinct products purchased.  ","metadata":{}},{"cell_type":"code","source":"sequential_df[\"product_count\"].describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-18T12:31:16.57561Z","iopub.execute_input":"2021-07-18T12:31:16.575885Z","iopub.status.idle":"2021-07-18T12:31:16.59018Z","shell.execute_reply.started":"2021-07-18T12:31:16.575863Z","shell.execute_reply":"2021-07-18T12:31:16.58892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_target = 1\nn_embedding = 16\nn_frequency = 3\ncorp = sequential_df.explode(\"purchase_sequence\")[\"purchase_sequence\"].unique()\nfrequent_df = sequential_df[(sequential_df[\"product_count\"]>n_frequency)]\n\ninput_seq, output_seq = generate_sequence(\n    frequent_df[\"purchase_sequence\"],\n    n_target\n    )\n\nfrequent_df[\"input_sequence\"] = input_seq\nfrequent_df[\"output_sequence\"] = output_seq\nfrequent_df = frequent_df[[\"CustomerID\", \"input_sequence\", \"output_sequence\"]]\nfrequent_df = frequent_df.explode(\"input_sequence\")\nfrequent_df[\"purchase\"] = 1\nfrequent_df = frequent_df.set_index(\"CustomerID\", drop=True)\nfrequent_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T12:32:21.431854Z","iopub.execute_input":"2021-07-18T12:32:21.432154Z","iopub.status.idle":"2021-07-18T12:32:22.092581Z","shell.execute_reply.started":"2021-07-18T12:32:21.432127Z","shell.execute_reply":"2021-07-18T12:32:22.091084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Negative Sampling & Merging\n\nSince all instances prepared so far represent positive-only feedback, we try to supply some negative information to the model. Negative instances are chosen from products not purchased for a particular customer.\n> sample_size=1 \n\nmeans there is 1 non-purchased product to be selected randomly.","metadata":{}},{"cell_type":"code","source":"new_df = frequent_df.reset_index().groupby(\"CustomerID\").agg({\"input_sequence\": (lambda x: list(x))})\nnew_df[\"agg\"] = new_df[\"input_sequence\"].apply(lambda y: agg(y, corp, 1))\nndf = new_df.explode(\"agg\")[[\"agg\"]]\nndf[\"purchase\"] = 0\nndf = ndf.rename(columns={\"agg\":\"input_sequence\"})\n\npdf = frequent_df[[\"input_sequence\", \"purchase\"]]\n\nsample_df = pdf.append(ndf)\nsample_df = sample_df.reset_index()\nsample_df = sample_df.sort_values(\"CustomerID\", ignore_index=True)\n\ndisplay(sample_df.info())\ndisplay(sample_df.head(50))","metadata":{"execution":{"iopub.status.busy":"2021-07-18T12:32:30.544896Z","iopub.execute_input":"2021-07-18T12:32:30.545182Z","iopub.status.idle":"2021-07-18T12:33:35.135705Z","shell.execute_reply.started":"2021-07-18T12:32:30.545157Z","shell.execute_reply":"2021-07-18T12:33:35.134122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encoding & Splitting\n\nAs a last step we try to encode user and product features. Method taken from [keras.io](https://keras.io/examples/structured_data/collaborative_filtering_movielens/) examples. We take the data as train & validation, but the better practice is holding out some samples in advance as test data.    ","metadata":{}},{"cell_type":"code","source":"cust_ids = sample_df[\"CustomerID\"].unique().tolist()\ncust2cust_encoded = {x: i for i, x in enumerate(cust_ids)}\ncust_encoded2cust = {i: x for i, x in enumerate(cust_ids)}\nprod_ids = corp\nprod2prod_encoded = {x: i for i, x in enumerate(prod_ids)}\nprod_encoded2prod = {i: x for i, x in enumerate(prod_ids)}\nsample_df[\"cust\"] = sample_df[\"CustomerID\"].map(cust2cust_encoded)\nsample_df[\"prod\"] = sample_df[\"input_sequence\"].map(prod2prod_encoded)\n\nnum_custs = len(cust2cust_encoded)\nnum_prods = len(prod2prod_encoded)\nsample_df[\"purchase\"] = sample_df[\"purchase\"].values.astype(np.float32)\n\nprint(\n    \"Number of Customers: {}, Number of Products: {}, Purchase: {}, Not Purchase: {}\".format(\n        num_custs, num_prods, 1, 0\n    )\n)\n\nsample_df = sample_df.sample(frac=1, random_state=52)\nX = sample_df[[\"cust\", \"prod\"]].values\ny = sample_df[\"purchase\"].values\n\ntrain_indices = int(0.8 * sample_df.shape[0])\nX_train, X_val, y_train, y_val = (X[:train_indices],\n                                  X[train_indices:],\n                                  y[:train_indices],\n                                  y[train_indices:])","metadata":{"execution":{"iopub.status.busy":"2021-07-18T12:33:47.805964Z","iopub.execute_input":"2021-07-18T12:33:47.806244Z","iopub.status.idle":"2021-07-18T12:33:48.341014Z","shell.execute_reply.started":"2021-07-18T12:33:47.80622Z","shell.execute_reply":"2021-07-18T12:33:48.339259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Baseline Model & Training","metadata":{}},{"cell_type":"code","source":"class RecommenderNet(keras.Model):\n    def __init__(self, num_custs, num_prods, embedding_size, **kwargs):\n        super(RecommenderNet, self).__init__(**kwargs)\n        self.num_custs = num_custs\n        self.num_prods = num_prods\n        self.embedding_size = embedding_size\n        self.cust_embedding = layers.Embedding(\n            num_custs,\n            embedding_size,\n            embeddings_initializer=\"he_normal\",\n            embeddings_regularizer=keras.regularizers.l2(1e-7),\n        )\n        self.cust_bias = layers.Embedding(num_custs, 1)\n        self.prod_embedding = layers.Embedding(\n            num_prods,\n            embedding_size,\n            embeddings_initializer=\"he_normal\",\n            embeddings_regularizer=keras.regularizers.l2(1e-6),\n        )\n        self.prod_bias = layers.Embedding(num_prods, 1)\n\n    def call(self, inputs):\n        cust_vector = self.cust_embedding(inputs[:, 0])\n        cust_bias = self.cust_bias(inputs[:, 0])\n        prod_vector = self.prod_embedding(inputs[:, 1])\n        prod_bias = self.prod_bias(inputs[:, 1])\n        dot_cust_product = tf.tensordot(cust_vector, prod_vector, 2)\n        x = dot_cust_product + cust_bias + prod_bias\n        return tf.nn.sigmoid(x)\n\nmodel = RecommenderNet(num_custs, num_prods, n_embedding)\n\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n              optimizer=keras.optimizers.Adam(learning_rate=0.001))","metadata":{"execution":{"iopub.status.busy":"2021-07-18T12:34:25.828736Z","iopub.execute_input":"2021-07-18T12:34:25.829077Z","iopub.status.idle":"2021-07-18T12:34:25.896517Z","shell.execute_reply.started":"2021-07-18T12:34:25.829046Z","shell.execute_reply":"2021-07-18T12:34:25.895648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"es = keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n                                   mode=\"min\",\n                                   verbose=1,\n                                   patience=5)\n\nhistory = model.fit(X_train, y_train,\n                    batch_size=256,\n                    epochs=20,\n                    verbose=1,\n                    validation_data=(X_val, y_val))","metadata":{"execution":{"iopub.status.busy":"2021-07-18T12:34:29.896354Z","iopub.execute_input":"2021-07-18T12:34:29.896674Z","iopub.status.idle":"2021-07-18T12:36:27.363968Z","shell.execute_reply.started":"2021-07-18T12:34:29.896651Z","shell.execute_reply":"2021-07-18T12:36:27.362598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\nplt.title(\"embedding loss\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"epoch\")\nplt.legend([\"train\", \"val\"], loc=\"upper left\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-18T12:36:33.508437Z","iopub.execute_input":"2021-07-18T12:36:33.508771Z","iopub.status.idle":"2021-07-18T12:36:33.668908Z","shell.execute_reply.started":"2021-07-18T12:36:33.508747Z","shell.execute_reply":"2021-07-18T12:36:33.668137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation\n\nWe try to measure the model performance by providing candidate products to the model and evaluating the outputs. Candidate products are merged with 49 products selected from non-purchased products and a target product which respresented in output_sequence variable. If target product occures in the top 20 of the model outputs, we count this event as a hit.\n\nOn the other hand; Hidasi and Karatzoglou (2018) define \"recall@20\" as an evaluatinon metric as \"the proportion of cases having the desired item amongst the top-20 items in all test cases.\"","metadata":{}},{"cell_type":"markdown","source":"## Output Examples","metadata":{}},{"cell_type":"code","source":"cust_id = sample_df[\"CustomerID\"].sample(1).iloc[0]\ncust_encoder = cust2cust_encoded.get(cust_id)\npurchased = frequent_df[(frequent_df.index==cust_id) & (frequent_df[\"purchase\"]==1)]\n\ncandidates = frequent_df[~frequent_df[\"input_sequence\"].isin(purchased[\"input_sequence\"].values)][\"input_sequence\"][:49]\ncandidates = set(candidates).intersection(set(prod2prod_encoded.keys()))\ncandidates = candidates.union(set(frequent_df[frequent_df.index==cust_id][\"output_sequence\"].values[0]))\ncandidates = [[prod2prod_encoded.get(x)] for x in list(candidates)]\n\ncust_prod_array = np.hstack(([[cust_encoder]] * len(candidates), candidates))\n\nvals = model.predict(cust_prod_array).flatten()\ntop_ratings_indices = vals.argsort()[-20:][::-1]\n\nrecommended_prod_ids = [prod_encoded2prod.get(candidates[x][0]) for x in top_ratings_indices]\n\nprint(\"Showing recommendations for user: {}\".format(cust_id))\nprint(\"====\" * 12)\nprint(\"Products purchased from customer\")\nprint(\"----\" * 8)\nprint(frequent_df[frequent_df.index==cust_id])\n\nprint(\"----\" * 8)\nprint(\"Top 10 product recommendations\")\nprint(\"----\" * 8)\nprint(recommended_prod_ids)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T12:41:10.692995Z","iopub.execute_input":"2021-07-18T12:41:10.693291Z","iopub.status.idle":"2021-07-18T12:41:10.986103Z","shell.execute_reply.started":"2021-07-18T12:41:10.693268Z","shell.execute_reply":"2021-07-18T12:41:10.984701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"counter = 0\nsize = 100\nfor s in range(size):\n    cust_id = sample_df[\"CustomerID\"].unique()[s]\n    cust_encoder = cust2cust_encoded.get(cust_id)\n    purchased = frequent_df[(frequent_df.index==cust_id) & (frequent_df[\"purchase\"]==1)]\n\n    candidates = frequent_df[~frequent_df[\"input_sequence\"].isin(purchased[\"input_sequence\"].values)][\"input_sequence\"][:49]\n    candidates = set(candidates).intersection(set(prod2prod_encoded.keys()))\n    candidates = candidates.union(set(frequent_df[frequent_df.index==cust_id][\"output_sequence\"].values[0]))\n    candidates = [[prod2prod_encoded.get(x)] for x in list(candidates)]\n\n    cust_prod_array = np.hstack(([[cust_encoder]] * len(candidates), candidates))\n\n    vals = model.predict(cust_prod_array).flatten()\n    top_ratings_indices = vals.argsort()[-20:][::-1]\n\n    recommended_prod_ids = [prod_encoded2prod.get(candidates[x][0]) for x in top_ratings_indices]\n    target_prod_ids = frequent_df.loc[(frequent_df.index==cust_id), \"output_sequence\"].values[0]\n    if len(np.setdiff1d(target_prod_ids, recommended_prod_ids)) < n_target:\n        counter = counter + 1\n        \nprint(\"recall@20 for first\", size, \" input: \", counter/size)","metadata":{"execution":{"iopub.status.busy":"2021-07-18T12:41:40.469258Z","iopub.execute_input":"2021-07-18T12:41:40.46959Z","iopub.status.idle":"2021-07-18T12:42:09.926502Z","shell.execute_reply.started":"2021-07-18T12:41:40.469565Z","shell.execute_reply":"2021-07-18T12:42:09.925274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Critics\n\nPlease criticise this study and faulty issues other than hyperparameter tuning. Any comment is more precious than upvotes for this fresh notebook. To compare metrics please see: https://medium.com/decathlondevelopers/building-a-rnn-recommendation-engine-with-tensorflow-505644aa9ff3. They developed a model for more than 10,000 different products. \n\nSome topics which are ambiguous:\n\n* Can prediction performance be upgraded for this model? \n* Is there a need for negative sampling? Is there a room for improvement by adjusting negative sample size?\n* Are there more suitable or effective techniques to measure the performance of the model?\n* Can execution time be shortened?\n* Any other effective ways to predict next-product-to-buy using deep learning?\n.\n.\n\nSorry for language...\nThanks in advance","metadata":{}}]}