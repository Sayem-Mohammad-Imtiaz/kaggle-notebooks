{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sb\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nimport sklearn.metrics\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import roc_auc_score\nfrom scipy import interp\nfrom itertools import cycle\nfrom yellowbrick.classifier import ConfusionMatrix\nfrom yellowbrick.classifier import ClassificationReport\nfrom yellowbrick.classifier import ROCAUC\nfrom yellowbrick.classifier import DiscriminationThreshold\nfrom yellowbrick.classifier import PrecisionRecallCurve\nimport warnings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"warnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=DeprecationWarning)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/fetal-health-classification/fetal_health.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For better understanding of variables please refer to [this link](http://oacapps.med.jhmi.edu/OBGYN-101/Text/Labor%20and%20Delivery/electronic_fetal_heart_monitorin.htm)\n\n* LB - FHR baseline (beats per minute)\n* AC - # of accelerations per second\n* FM - # of fetal movements per second\n* UC - # of uterine contractions per second\n* DL - # of light decelerations per second\n* DS - # of severe decelerations per second\n* DP - # of prolongued decelerations per second\n* ASTV - percentage of time with abnormal short term variability\n* MSTV - mean value of short term variability\n* ALTV - percentage of time with abnormal long term variability\n* MLTV - mean value of long term variability\n* Width - width of FHR histogram\n* Min - minimum of FHR histogram\n* Max - Maximum of FHR histogram\n* Nmax - # of histogram peaks\n* Nzeros - # of histogram zeros\n* Mode - histogram mode\n* Mean - histogram mean\n* Median - histogram median\n* Variance - histogram variance\n* Tendency - histogram tendency\n\n* Class - fetal state class code (N= Normal ; S= Suspect ; P= Pathologic )"},{"metadata":{"trusted":true},"cell_type":"code","source":"#rename columns\ndata.columns = ['FHR', 'AC', 'FM', 'UC', 'DL', 'DS', 'DP', 'ASTV', 'MSTV',\n               'ALTV', 'MLTV', 'Width', 'Min', 'Max', 'NMax', 'Nzeros', \n                'Mode', 'Mean', 'Median', 'Variance', 'Tendency', 'Class']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert target class to Int\n\ndata.Class = data.Class.astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_encoder = preprocessing.LabelEncoder()\n\ny_en = label_encoder.fit_transform(data.Class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(['Class'], axis = 1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#0-Normal\n#1-Suspect\n#2-Pathologic\n\ny_en","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#AC - # of accelerations per second\n#FM - # of fetal movements per second\n#UC - # of uterine contractions per second\n#DL - # of light decelerations per second\n#DS - # of severe decelerations per second\n#DP - # of prolongued decelerations per second\n\n#All the variables are measured per second and has very low values, this cause scaling issues in our models. \n#So we convert them into per min, as FHR is also  measured per min.\n\nclms = ['AC', 'FM', 'UC', 'DL', 'DS', 'DP']\n\nfor column in clms:\n    data[column] = data[column]*60\n    \ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[15,10])\nx=data.corr()\nsb.heatmap(x,annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Following variables has maximum correlations.**\n* Min and width has -0.9 \n* Min and NMax has -0.67 \n* Min and MSTV has -0.62  \n* Mode and Mean has 0.89 \n* Mode and Median has 0.93 \n* Mean and Median has 0.95"},{"metadata":{},"cell_type":"markdown","source":"# **Visualization**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = [10, 8])\nsb.distplot(data['FHR'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* distplots are great to know about the distribution of variable. \n* But if we have a classification prob with multiple classes and we would like to know about the distribution of single variable for different class violinplots helps alot."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[15,8])\nsb.violinplot(x=y_en, y=data.FHR, palette=\"deep\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sb.pairplot(data[['AC', 'UC', 'DL', 'FM', 'DS', 'DP']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Few graphs in plot are very sparse, becouse they have more Zero's. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def zero_table(df):\n    for column in df.columns:\n        zero_count = (df[column] == 0).sum()\n        if zero_count != 0:\n            zero_percentage = 100*zero_count/len(df[column])\n            if zero_percentage > 60:\n                print(\"%s has %s Zeros\" % (column, zero_count))\n                print(\"Percentage of Zeros %0.1f%%\" % (zero_percentage))\n                print(\"-\"*25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"zero_table(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sb.pairplot(data[['AC', 'UC', 'DL']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[15,8])\nsb.scatterplot(x = data['ASTV'], y = data['MSTV'],\n              hue = y_en, palette=\"deep\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We can observe that Suspect and Pathologic cases are increasing as ASTV increase.\n* Suspect and Pathologic cases are increasing as MSTV increasing by keeping ASTV constant."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[15,8])\nsb.scatterplot(x = data['ALTV'], y = data['MLTV'],\n              hue = y_en, palette=\"deep\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We can observe that Suspect and Pathologic cases are increasing as ALTV increase."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = [10, 8])\nsb.distplot(data['Variance'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[15,8])\nsb.violinplot(x=y_en, y=data.Variance, palette=\"deep\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We can see that Normal clases have variance values concentrated more at Zero and slightly distrubuted for low values\n* For Suspect cases the variance concentrated more at Zero and very low values at small varince values.\n* For Pathologic case, the variance values are almost equally distributed among all the values."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = [10, 8])\nsb.distplot(data['Width'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[15,8])\nsb.violinplot(x=y_en, y=data.Width, palette=\"deep\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = [10, 8])\nsb.distplot(data['Max'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[15,8])\nsb.violinplot(x=y_en, y=data.Max, palette=\"deep\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[15,8])\nsb.violinplot(x=y_en, y=data.NMax, palette=\"deep\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[15,8])\nsb.scatterplot(x=data.FHR, y=data.Variance, hue=y_en, palette=\"deep\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We can see that the most Pathologic cases are found at mean FHR and medium to high variance values."},{"metadata":{},"cell_type":"markdown","source":"# Model Fitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"def classifier_results(x, y):\n    \n    x_train,x_test,y_train,y_test = train_test_split(x, y, test_size=0.3)\n\n    classifiers = {\n        'L1 logistic': LogisticRegression(penalty = 'l2', solver = 'saga', \n                            multi_class = 'multinomial', max_iter = 10000),\n        'L2 logistic (Multinomial)': LogisticRegression(penalty = 'l1', solver = 'saga', \n                             multi_class = 'multinomial', max_iter = 10000),\n        'L2 logistic (OvR)': LogisticRegression(penalty='l2', solver='saga',\n                       multi_class='ovr', max_iter=10000),\n        'Linear SVC': SVC(kernel='linear', probability=True),\n\n    }\n    \n    class_names = ['Normal', 'Suspect', 'Pathologic']\n\n    for index, (name, classifier) in enumerate(classifiers.items()):\n        classifier.fit(x_train, y_train)\n        y_pred = classifier.predict(x_test)\n        accuracy = accuracy_score(y_test, y_pred)\n        print(\"Accuracy (test) for %s: %0.1f%% \" % (name, accuracy * 100))\n        print('-'*40)\n        \n        fig, ax = plt.subplots(figsize=(12, 7))\n        visualizer = ClassificationReport(classifier, classes=class_names, support=True, ax=ax)\n        visualizer.fit(x_train, y_train)       \n        visualizer.score(x_test, y_test)       \n        visualizer.show()\n        \n        fig, ax = plt.subplots(figsize=(12, 7))\n        cm = ConfusionMatrix(classifier, classes = class_names, ax=ax)\n        cm.fit(x_train, y_train)\n        cm.score(x_test, y_test)\n        cm.show()\n        \n        y_lb = label_binarize(y_en, classes=[0, 1, 2])\n        n_classes = y_lb.shape[1]\n        \n        x2_train,x2_test,y2_train,y2_test = train_test_split(data, y_lb, test_size=0.3)\n\n        estimator = OneVsRestClassifier(classifier)\n        y2_dist = estimator.fit(x2_train, y2_train).decision_function(x2_test)\n        y2_pred = estimator.predict(x2_test)\n        \n        fpr = dict()\n        tpr = dict()\n        roc_auc = dict()\n\n        # Compute ROC curve and ROC area for each class\n        for i in range(n_classes):\n            fpr[i], tpr[i], _ = roc_curve(y2_test[:, i], y2_dist[:, i])\n            roc_auc[i] = auc(fpr[i], tpr[i])\n\n        # Compute micro-average ROC curve and ROC area\n        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y2_test.ravel(), y2_dist.ravel())\n        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n        \n        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n        # Then interpolate all ROC curves at this points\n        mean_tpr = np.zeros_like(all_fpr)\n        for i in range(n_classes):\n            mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n\n        # Finally average it and compute AUC\n        mean_tpr /= n_classes\n\n        fpr[\"macro\"] = all_fpr\n        tpr[\"macro\"] = mean_tpr\n        roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n        # Plot all ROC curves\n        plt.figure(figsize=[15,7])\n        plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n                 label='micro-average ROC curve (area = {0:0.2f})'\n                       ''.format(roc_auc[\"micro\"]),\n                 color='deeppink', linestyle=':', linewidth=4)\n\n        plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n                 label='macro-average ROC curve (area = {0:0.2f})'\n                       ''.format(roc_auc[\"macro\"]),\n                 color='navy', linestyle=':', linewidth=4)\n\n        colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n        for i, color in zip(range(n_classes), colors):\n            plt.plot(fpr[i], tpr[i], color=color, lw=2,\n                     label='ROC curve of class {0} (area = {1:0.2f})'\n                     ''.format(i+1, roc_auc[i]))\n\n        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('Some extension of Receiver operating characteristic to multi-class')\n        plt.legend(loc=\"lower right\")\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier_results(data, y_en)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def RF_AdB_GNB_classifier_results(x, y):\n    \n    x_train,x_test,y_train,y_test = train_test_split(x, y, test_size=0.3)\n\n    classifiers = {\n        'RandomForest': RandomForestClassifier(),\n        'AdaBoost': AdaBoostClassifier(),\n        'GaussianNB': GaussianNB()\n\n    }\n    \n    class_names = ['Normal', 'Suspect', 'Pathologic']\n\n    for index, (name, classifier) in enumerate(classifiers.items()):\n        classifier.fit(x_train, y_train)\n        y_pred = classifier.predict(x_test)\n        accuracy = accuracy_score(y_test, y_pred)\n        print(\"Accuracy (test) for %s: %0.1f%% \" % (name, accuracy * 100))\n        print('-'*40)\n        \n        fig, ax = plt.subplots(figsize=(12, 7))\n        visualizer = ClassificationReport(classifier, classes=class_names, support=True, ax=ax)\n        visualizer.fit(x_train, y_train)        # Fit the visualizer and the model\n        visualizer.score(x_test, y_test)        # Evaluate the model on the test data\n        visualizer.show()\n        \n        fig, ax = plt.subplots(figsize=(12, 7))\n        cm = ConfusionMatrix(classifier, classes = class_names, ax=ax)\n        cm.fit(x_train, y_train)\n        cm.score(x_test, y_test)\n        cm.show()\n\n        fig, ax = plt.subplots(figsize=(12, 7))\n        roc = ROCAUC(classifier, classes=class_names, ax=ax)\n        roc.fit(x_train, y_train)        # Fit the training data to the visualizer\n        roc.score(x_test, y_test)        # Evaluate the model on the test data\n        roc.show()\n        \n        fig, ax = plt.subplots(figsize=(12, 7))\n        prc = PrecisionRecallCurve(classifier,\n                                   classes=class_names,\n                                   colors=[\"purple\", \"cyan\", \"blue\"],\n                                   iso_f1_curves=True,\n                                   per_class=True,\n                                   micro=False, ax=ax)\n        prc.fit(x_train, y_train)\n        prc.score(x_test, y_test)\n        prc.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RF_AdB_GNB_classifier_results(data, y_en)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ***Observed that Random Forest Classifer did best of all the classifiers.***"},{"metadata":{},"cell_type":"markdown","source":"## **Please Upvote if you find this notebook useful**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}