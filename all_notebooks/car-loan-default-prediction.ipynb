{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Vehicle Loan Default Prediction"},{"metadata":{},"cell_type":"markdown","source":"-------------------------------------------------------------------------\nIn this kernel we will analyse and make predictions of this dataset\n\n--------------------------------------------------------------------------"},{"metadata":{},"cell_type":"markdown","source":"We begin by importing the different librairies that will be needed for our analysis"},{"metadata":{"trusted":false},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Scikit learn librairies\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import scale\n\nimport pandas as pd\nimport numpy as np\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We charge the dataset and transform it into a pandas DataFrames"},{"metadata":{"trusted":false},"cell_type":"code","source":"df= pd.read_csv('../input/train.csv')\ndftmp= pd.read_csv('../input/train.csv')\ndftmp2= pd.read_csv('../input/train.csv')\n#dfe= pd.read_csv('test.csv')\n#dftest= pd.read_csv('loan_car_short.csv')\n#df=dftest\n#print(dftest.shape,df.shape,df.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{"trusted":false},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We plot the distribution of the target"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"fig11=plt.figure()\nax11=plt.axes()\nthe_target = dftmp['loan_default']\nthe_target.replace(to_replace=[1,0], value= ['YES','NO'], inplace = True)\nplt.title('Target repartition')\nax11 = ax11.set(xlabel='Default proportion', ylabel='Number of people')\nthe_target.value_counts().plot.bar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We want to plot the correlation matrix of the data"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Correlation Matrix calculation\ncorr_mat = df.corr()\n\nfig2=plt.figure()\nsns.set(rc={'figure.figsize':(20,15)})\nk = 10\ncols = corr_mat.nlargest(k, 'loan_default')['loan_default'].index\ncm = np.corrcoef(df[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.title('Correlation Matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"An other good way to understant the data is to print the sorted correlation coefficients of the matrix"},{"metadata":{"trusted":false},"cell_type":"code","source":"corr_mat['loan_default'].sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To have a clearer view of the data we created a function to print every value for every features in function of the id of the custumer to have a better idea of the shape of the data"},{"metadata":{"trusted":false},"cell_type":"code","source":"#A function to print every graph with the ID as \ndef print_all_values():\n    df1=df.drop('UniqueID',axis=1)\n    cols=df1.columns\n    for col in cols:\n        if (df[col].dtypes !='object'):\n\n            fig1=plt.figure()\n            ax1=plt.axes()\n            plt.scatter(df[[col]],df.UniqueID,alpha=1,)\n            plt.title(col)\n            ax1 = ax1.set(xlabel=col, ylabel='ID')\n            plt.show()\n            \n            \nprint_all_values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some features have to high or to low values, we want to delete them to help our model on regular values.\n\nWe decided to create a function that takes as parameters the quatile to compare the value to. We multiplie this value with a max value to fix a value limite\n\nWe are targeting columns that have disproportionated values. We are dropping to high value to concentrate more on mid range value.\nCustumers with extreme values (good or bad) are easy to predict, we want to improve our model on average custumer"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Delete to high or to low values\ndef delete_absurd_values(df_transformed,cols,max_value,percentage):\n        \n        \n        for col in cols:\n            if (df_transformed[col].dtypes !='object'):\n                       \n                q99=df_transformed[col].quantile(q=percentage)\n                q01=df_transformed[col].quantile(q=(1-percentage))\n                for i in df_transformed.index:\n                    \n                    if (df_transformed.loc[i,col]> max_value*q99 or df_transformed.loc[i,col]< q01/max_value):\n                        df_transformed=df_transformed.drop(index=i)\n        \n        return df_transformed\n\ncols=['disbursed_amount', 'asset_cost', 'PRI.NO.OF.ACCTS', 'PRI.ACTIVE.ACCTS','PRI.OVERDUE.ACCTS','PRI.CURRENT.BALANCE', 'PRI.SANCTIONED.AMOUNT','PRI.SANCTIONED.AMOUNT',\n       'PRI.DISBURSED.AMOUNT', 'SEC.NO.OF.ACCTS', 'SEC.ACTIVE.ACCTS','SEC.CURRENT.BALANCE', 'SEC.SANCTIONED.AMOUNT',\n       'SEC.DISBURSED.AMOUNT', 'PRIMARY.INSTAL.AMT', 'SEC.INSTAL.AMT',\n       'NEW.ACCTS.IN.LAST.SIX.MONTHS', 'DELINQUENT.ACCTS.IN.LAST.SIX.MONTHS',\n       'AVERAGE.ACCT.AGE', 'CREDIT.HISTORY.LENGTH', 'NO.OF_INQUIRIES']\ndf=delete_absurd_values(df,cols,5,0.999)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's plot the repartition of the target"},{"metadata":{"trusted":false},"cell_type":"code","source":"#The repartition of the target\nfig7=plt.figure()\nax7=plt.axes()\nthe_target = dftmp2['loan_default']\nthe_target.replace(to_replace=[1,0], value= ['YES','NO'], inplace = True)\nplt.title('Target repartition')\nax7 = ax7.set(xlabel='Default proportion')\nthe_target.value_counts().plot.pie()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Printing the types of the features\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the percentage of missing value for each column"},{"metadata":{"trusted":false},"cell_type":"code","source":"def nan_count_df(df_to_print):\n    \n    nan_count = df_to_print.isnull().sum()\n\n    nan_percentage = (nan_count / len(df))*100\n\n    nan_df=pd.concat([nan_percentage], axis=1)\n    nan_df=nan_df.rename(columns={0:'Percentage'})\n    nan_df=nan_df[nan_df.Percentage != 0]\n    nan_df = nan_df.sort_values(by='Percentage',ascending=False)\n    return nan_df\n\nnan_count_df(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So there is only one column with missing values: \"Employment Type\""},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"#Number of unique values\ndf.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"trusted":false},"cell_type":"code","source":"df=df.rename(columns={'Date.of.Birth': 'Date_of_Birth','Employment.Type': 'Employment_Type', 'PERFORM_CNS.SCORE.DESCRIPTION': 'PERFORM_CNS_SCORE_DESCRIPTION'})\n\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We convert the column 'Date_of_Birth' into the age of the custumer for more clearancy"},{"metadata":{"trusted":false},"cell_type":"code","source":"now = pd.Timestamp('now')\ndf['Date_of_Birth'] = pd.to_datetime(df['Date_of_Birth'], format='%d-%m-%y')\ndf['Date_of_Birth'] = df['Date_of_Birth'].where(df['Date_of_Birth'] < now, df['Date_of_Birth'] -  np.timedelta64(100, 'Y'))\ndf['Age'] = (now - df['Date_of_Birth']).astype('<m8[Y]')\ndf=df.drop('Date_of_Birth',axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Label Encoding and One-Hot Encoding"},{"metadata":{},"cell_type":"markdown","source":"We want to create a function for encoding the two categories variables"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Creating a function for encoding 2 categories features\ndef two_cat_encoding(df_to_transf):\n    le = LabelEncoder()\n\n    for cols in df_to_transf:\n        if df_to_transf[cols].dtype == 'object':\n            if len(list(df_to_transf[cols].unique())) == 2:\n                le.fit(df_to_transf[cols])\n                df_to_transf[cols] = le.transform(df_to_transf[cols])\n    return df_to_transf\ndf=two_cat_encoding(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are two many different value type possible for a missing value so we will merge them into one column to reduce the dimension of the probleme when applying oneHotEncoding (dummies)"},{"metadata":{"trusted":false},"cell_type":"code","source":"df['PERFORM_CNS_SCORE_DESCRIPTION'].replace(to_replace=['Not Scored: More than 50 active Accounts found', 'Not Scored: No Activity seen on the customer (Inactive)','Not Scored: No Updates available in last 36 months','Not Enough Info available on the customer','Not Scored: Only a Guarantor','Not Scored: Sufficient History Not Available','Not Scored: Not Enough Info available on the customer'], value= 'Not Scored', inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will now drop the features that does not seem to improve our model"},{"metadata":{"trusted":false},"cell_type":"code","source":"columns_to_drop = ['UniqueID','MobileNo_Avl_Flag','DisbursalDate','AVERAGE.ACCT.AGE','CREDIT.HISTORY.LENGTH','SEC.OVERDUE.ACCTS']\ndf=df.drop(columns=columns_to_drop)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We finally apply the oneHotEncoding methode to the features that hasn't been encodeed yet"},{"metadata":{"trusted":false},"cell_type":"code","source":"df = pd.get_dummies(df)\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We want to know the new shape of the data after the encoding"},{"metadata":{"trusted":false},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And we also want to make sure that there are no 'object' type that hasn't been encodeed left:"},{"metadata":{"trusted":false},"cell_type":"code","source":"df.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---------------------------------------------------------------------------------------------\n#                   Machine Learning Algorithms\n----------------------------------------------------------------------------------------------"},{"metadata":{},"cell_type":"markdown","source":"#### Spliting the data between training and testing"},{"metadata":{"trusted":false},"cell_type":"code","source":"X =df.drop('loan_default',axis=1)\ny = df['loan_default']  \n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"  # Logistic Regression Implementation"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogisticRegr = LogisticRegression(C=1.0, class_weight=None,fit_intercept=True,max_iter=100)\nlogisticRegr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#ERROR\nerror = (1 - logisticRegr.score(X_test, y_test))*100\nprint('Score  = ',logisticRegr.score(X_test, y_test)*100, '%','\\nErreur = %f' % error, '%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Classifier Implementation"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=300, oob_score=True, random_state=0)\nrf.fit(X_train,y_train)\nerror = (1 - rf.score(X_test, y_test))*100\nprint('Score  = ',rf.score(X_test, y_test)*100, '%','\\nErreur = %f' % error, '%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Linear Discriminant Analysis Implementation"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n\nlda = LDA(n_components=1)  \nX_lda_sklearn = lda.fit_transform(X_train, y_train)\nerror = (1 - lda.score(X_test, y_test))*100\nprint('Erreur: %f' % error, '%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree Classifier Implementation"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn import tree\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X_train,y_train)\nerror = (1 - clf.score(X_test, y_test))*100\nprint('Erreur: %f' % error, '%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cross Validation"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\npredictions = logisticRegr.predict(X_test)\n\nprint(classification_report(y_test,predictions))\nprint('\\n')\nprint(confusion_matrix(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(estimator = logisticRegr , \n                         X=X_train, \n                         y=y_train, \n                         cv=3)\nprint('Cross-validation accuracy scores: %s' %(scores))\nprint('CV accuracy: %.3f +/- %.3f' %(np.mean(scores), np.std(scores)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Success rate by model:\\n\\nLogistic Regression:',logisticRegr.score(X_test, y_test)*100,'%','\\n\\nLDA:',lda.score(X_test, y_test)*100,'%','\\n\\nRandom Forest Classifier:',rf.score(X_test, y_test)*100,'%','\\n\\nDecision Tree Classifier:',clf.score(X_test, y_test)*100,'%')","execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'logisticRegr' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-7c8eb37a40df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Success rate by model:\\n\\nLogistic Regression:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogisticRegr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'%'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'\\n\\nLDA:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'%'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'\\n\\nRandom Forest Classifier:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'%'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'\\n\\nDecision Tree Classifier:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'logisticRegr' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":1}