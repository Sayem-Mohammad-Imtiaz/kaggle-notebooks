{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#we shall import dependencies\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nimport seaborn as sns\nmatplotlib.rcParams['figure.figsize']=(20,10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing the dataset\ndf1=pd.read_csv('/kaggle/input/bengaluru-house-price-data/Bengaluru_House_Data.csv')\ndf1.head()                ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets see the distribution of NAN values in the DataFrame","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df1.isnull(),yticklabels=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we will check what is in dataset\ndf1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets see a distribution of area types\nsns.countplot(x=df1.area_type)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will drop certain column from the datafreme, like availability, society,balcony,availability \n#as they are not important in predicting the price\ndf2=df1.drop(['availability',\"society\"],axis=1)\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we will find the number of null values in the dataframe\ndf2.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2[\"balcony\"].fillna(df2[\"balcony\"].median(),inplace=True)\ndf2[\"bath\"].fillna(df2[\"bath\"].median(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we will drop the null values in the dataframe as they are very less\ndf3=df2.dropna()\ndf3.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#the size colums has both categorical as well as numerical data, we will see unique values\ndf3['size'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we will create a separate column BHK containing number of rooms\ndf3['bhk']=df3['size'].apply(lambda x:int(x.split(' ')[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf3['bhk'].unique()\n#here rooms like 27,43 and others seems astronomical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we will find the outliers\ndf3[df3.bhk>20]\n#the outplut seems like an error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we will find the unique values in the total_sqft column\ndf3.total_sqft.unique()\n\n#the total_sqft has range values, this is a hindrance, we will try to remove that in future codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def is_float(x):\n    try:\n        float(x)\n    except:\n        return False\n    return True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we will see slice the total_sqft column based on the float\ndf3[~df3['total_sqft'].apply(is_float)].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we will convert float into range by averaging the sqft ranges\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_sqft_to_num(x):\n    tokens=x.split('-')\n    if len(tokens)==2:\n        return(float(tokens[0])+float(tokens[1]))/2\n    try:\n        return float(x)\n    except:\n        return None\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we will taste the function\nconvert_sqft_to_num('2166')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"convert_sqft_to_num('2100-3000')\n#here the output is average of the range","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we will apply this function to the sqft column and creat a new dataframe\ndf4=df3.copy()\ndf4['total_sqft']=df4['total_sqft'].apply(convert_sqft_to_num)\ndf4.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we will start with feature engineering techniques and dimensionality reduction techniques\ndf5=df4.copy()\n#now we will create price per sqft\ndf5['price_per_sqft']=df5['price']*100000/df5['total_sqft']\ndf5.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we will explore location column\nlen(df5.location.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# there are 1304 features, means this is too much feature\n#this is called dimensionality curse\n#we shall reduce this dimensionality as low as possible\ndf5.location=df5.location.apply(lambda x:x.strip())\nlocation_stats=df5.location.value_counts().sort_values(ascending=False)\nlocation_stats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we will find the location with less than 10 datapoints\nlen(location_stats[location_stats<=10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"location_stats_less_than_10=location_stats[location_stats<=10]\nlocation_stats_less_than_10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df5.location.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df5.location=df5.location.apply(lambda x: 'other'if x in location_stats_less_than_10 else x )\nlen(df5.location.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#all the locations with less than 10 properties are converted to other location \ndf5.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we will start outlier detections and removal\ndf5[df5.total_sqft/df5.bhk<300].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df5.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we will negate the criteria from above dataframe\ndf6=df5[~(df5.total_sqft/df5.bhk<300)]\ndf6.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we will chaek the outliers in price per sqft\ndf6.price_per_sqft.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will filter the data beyond (mean +1std deviation).\ndef remove_pps_outliers(df):\n    df_out=pd.DataFrame()\n    for key,subdf in df.groupby('location'):\n        m=np.mean(subdf.price_per_sqft)\n        st=np.std(subdf.price_per_sqft)\n        reduced_df=subdf[(subdf.price_per_sqft>(m-st))&(subdf.price_per_sqft<=(m+st))]\n        df_out=pd.concat([df_out,reduced_df],ignore_index=True)\n    return df_out\ndf7=remove_pps_outliers(df6)\ndf7.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_scatter_chart(df,location):\n    bhk2=df[(df.location==location)&(df.bhk==2)]\n    bhk3=df[(df.location==location)&(df.bhk==3)]\n    matplotlib.rcParams['figure.figsize']=(15,10)\n    plt.scatter(bhk2.total_sqft,bhk2.price,color='blue',label='2 BHK',s=50)\n    plt.scatter(bhk3.total_sqft,bhk3.price,marker='+',color='green',label='3 BHK',s=50)\n    plt.xlabel('total square feet area')\n    plt.ylabel('price')\n    plt.title(location)\n    plt.legend()\n    \nplot_scatter_chart(df7,'Hebbal')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_bhk_outliers(df):\n    exclude_indices = np.array([])\n    for location, location_df in df.groupby('location'):\n        bhk_stats = {}\n        for bhk, bhk_df in location_df.groupby('bhk'):\n            bhk_stats[bhk] = {\n                'mean': np.mean(bhk_df.price_per_sqft),\n                'std': np.std(bhk_df.price_per_sqft),\n                'count': bhk_df.shape[0]\n            }\n        for bhk, bhk_df in location_df.groupby('bhk'):\n            stats = bhk_stats.get(bhk-1)\n            if stats and stats['count']>5:\n                exclude_indices = np.append(exclude_indices, bhk_df[bhk_df.price_per_sqft<(stats['mean'])].index.values)\n    return df.drop(exclude_indices,axis='index')\ndf8 = remove_bhk_outliers(df7)\n# df8 = df7.copy()\ndf8.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_scatter_chart(df8,'Hebbal')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we will pot a histogram with price per squarefeet\nimport matplotlib\nmatplotlib.rcParams['figure.figsize']=(20,10)\nplt.hist(df8.price_per_sqft,rwidth=0.8)\nplt.xlabel('price per squarefoot')\nplt.ylabel('count')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we will explore bathroom feature\ndf8.bath.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df8[df8.bath>10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df8.bath,rwidth=0.9)\nplt.xlabel('number of bathrooms')\nplt.ylabel('count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we will remove any time we have bathrooms greater than number of bedrooms +2 is an outlier\ndf8[df8.bath>df8.bhk+2]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df9=df8[df8.bath<df8.bhk+2]\ndf9.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we will prepare our dataframe for ML training\n#we shall remove price per squarefoot and size\ndf10=df9.drop(['size','price_per_sqft'],axis='columns')\ndf10.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we will perform one hot encoding for location column\ndummies=pd.get_dummies(df10,columns=[\"area_type\",\"location\"],drop_first=True)\ndummies.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df11=dummies\ndf11.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df11.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#will define dependent variable for training\nX=df11.drop('price',axis='columns')\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#will define independent variable for training\ny=df11.price\ny.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since all the given inputs are of different types we need to apply Standard scalar to make mu=0 and Sigma=1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"s = preprocessing.StandardScaler()\nX1 = s.fit_transform(X)\nX1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx1_train,x1_test,y_train,y_test=train_test_split(X1,y,test_size=0.25,random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlr_clf=LinearRegression()\nlr_clf.fit(x1_train,y_train)\nlr_clf.score(x1_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we will perform k fold cross validation\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import cross_val_score\ncv=ShuffleSplit(n_splits=5,test_size=0.20,random_state=42)\ncross_val_score(LinearRegression(),X1,y,cv=cv)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#now we will perform grid search cv, here we will try multiple model and select best one with good score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeRegressor\n\ndef find_best_model_using_gridsearchcv(X,y):\n    algos = {\n        'linear_regression' : {\n            'model': LinearRegression(),\n            'params': {\n                'normalize': [True, False]\n            }\n        },\n        'lasso': {\n            'model': Lasso(),\n            'params': {\n                'alpha': [1,2],\n                'selection': ['random', 'cyclic']\n            }\n        },\n        'decision_tree': {\n            'model': DecisionTreeRegressor(),\n            'params': {\n                'criterion' : ['mse','friedman_mse'],\n                'splitter': ['best','random']\n            }\n        }\n    }\n    scores = []\n    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n    for algo_name, config in algos.items():\n        gs =  GridSearchCV(config['model'], config['params'], cv=cv, return_train_score=False)\n        gs.fit(X,y)\n        scores.append({\n            'model': algo_name,\n            'best_score': gs.best_score_,\n            'best_params': gs.best_params_\n        })\n\n    return pd.DataFrame(scores,columns=['model','best_score','best_params'])\n\nfind_best_model_using_gridsearchcv(X1,y)\n    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like we got an average score between 83% with Linear_regression which is quite good with our simple model.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}