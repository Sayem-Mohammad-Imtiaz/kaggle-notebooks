{"cells":[{"metadata":{},"cell_type":"markdown","source":"# ABSTRACT"},{"metadata":{},"cell_type":"markdown","source":"### The journal \"Housing Price Prediction with Improved Machine Learning Techniques\" has worked on house prices in Beijing city, China. That paper has predicted house prices with not only ensemble methods but also other modern techniques like Hybrid and Stacked Generalization. We will replicate their work to make a case for future work like predicting UK housing prices. "},{"metadata":{},"cell_type":"markdown","source":"# 1. INTRODUCTION"},{"metadata":{},"cell_type":"markdown","source":"### In recent years, due to the growing trend towards Big Data, machine learning has become a vital prediction approach because it can predict house prices more accurately based on their attributes, regardless of the data from previous years. Several studies explored this problem and proved the capability of the machine learning approach. However, most of them compared the models’ performance but did not consider the combination of different machine learning models such as Hybrid regression and Stacked Generalizaiton."},{"metadata":{},"cell_type":"markdown","source":"# 2. METHODOLOGY"},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Data Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"### We will use the Beijing dataset (taken from Kaggle) comprised of ten years from 2008 to 2018. It has 318851 rows (data) with 26 features. Each feature represents specific information related to housing.\n\n### To work on the data, we have selected Python Language and fetch all the relevant libraries or modules, related to reading the data, finding missing values, exploratory analysis, model designing, and model evaluation."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n\n# Data Processing \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.width', None)\n\n# ignore all future warnings\nfrom warnings import simplefilter\nsimplefilter(action='ignore', category=FutureWarning) \nsimplefilter(action='ignore', category=UserWarning) \n\n# Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Satistical Testing\nfrom scipy import stats\n\n# Converting Categorical to Numerical Finite Values\nfrom sklearn.preprocessing import LabelEncoder\n\n# Model Designing and Evaluation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.ensemble import RandomForestRegressor\nimport lightgbm as lgbm\nimport xgboost as xgb\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.ensemble import StackingRegressor\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading the Data\ndf =  pd.read_csv('/kaggle/input/lianjia/new.csv',encoding='gbk',low_memory=False)#, errors='ignore')\n# Shape of the Data\nprint ('DATA',df.shape)\ndf.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Columns or Features or Variables\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### After reading the data, the first step is to find the missing data and remove it. Eliminating any feature having more than 50% missing data or any row with missing values.\n\n### The second step is to remove unnecessary features in the dataset which has no relevance like a number of \"kitchens\", \"bathrooms\", \"drawing rooms\". Similarly, the data fetched information like \"URL\", their respective identifications, \"Cid\" and \"id\". In the dataset, there are two price features, one is showing the irrelevant price or false data and the other one is the housing price. Therefore, we will remove the \"totalPrice\" feature too. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding missing Values in each column or feature\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### After finding the missing values of each column, it is clear that the feature \"DOM\"(Day on Market) has missing data almost 50%. We will drop this feature along with missing rows.\n\n### Also removing the unnecessary features as said above. Their names are \"url\", \"kitchen\", \"drawingRoom\", \"bathRoom\", \"Cid\", \"id\", and \"totalPrice\". "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping 50% less values feature and no information features\ndf.drop(['DOM','url','kitchen','drawingRoom','bathRoom','Cid','id','totalPrice'],axis=1,inplace=True)\n# Dropping missing data rows\ndf.dropna(inplace=True)\ndf = df[df['constructionTime']!='未知']\n# Now the remaining data\nprint (\"DATA\", df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Data Aanalysis"},{"metadata":{},"cell_type":"markdown","source":"### Data Analysis is the main part before selecting the algorithm on the basis of exploratory analysis. It consists of feature engineering, feature selection, finding outliers and trends through Data visualization. \n\n### For Feature Engineering, we will create new features according to our requirements. \"distance\" from latitude and longitude values around Beijing city. This will help to see the prices of houses which are near or far away from the center of Beijing. Feature \"Age\" showing the age of a house after construction. The year 2019 is set as a threshold. By creating this feature, we don't have to use the feature's 'construction time'. \"timeTrade\" feature is changed to only year base. \n\n### Splitting the feature \"floor\" into two features \"floor type\" and \"floor height\" with some data operations like replacing Chinese language words with English words. It will also provide a reflection on the housing price. \n\n### Replacing numeric values with appropriate house type title in \"buildingType\" feature. This will help to see the importance of each building aganist price value. Later we will change the values to numeric for model simulation."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating 'distance' feature\n# To calculate Distance Between Two Points on Earth \nfrom math import radians, cos, sin, asin, sqrt\n# We will find distance agnaist each lat and lng from Beijing (lat:39.916668,lon:116.383331)\ndef distance(lat2, lon2,lat1=39.916668,lon1=116.383331): \n      \n    # The math module contains a function named \n    # radians which converts from degrees to radians. \n    lon1 = radians(lon1) \n    lon2 = radians(lon2) \n    lat1 = radians(lat1) \n    lat2 = radians(lat2) \n       \n    # Haversine formula  \n    dlon = lon2 - lon1  \n    dlat = lat2 - lat1 \n    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n  \n    c = 2 * asin(sqrt(a))  \n     \n    # Radius of earth in kilometers. Use 3956 for miles \n    r = 6371\n       \n    # calculate the result \n    return(c * r) \ndf['distance'] = df.apply(lambda x: distance (x['Lat'],x['Lng']),axis=1)\n\n# Creating \"Age\" feature by substrating from the threshold value \"2019\"\ndf['constructionTime'] = df['constructionTime'].astype(int)\ndf['Age'] = 2019 - df['constructionTime']\n\n# 'timeTrade' feature to year base only.\ndf['tradeTime'] = pd.DatetimeIndex(df['tradeTime']).year\n\n# Creating \"floor type\" and \"floor height\" features\n# Fisrt split the string (floor type) and numeric part (the height)\nlst_numeric = []\nlst_str = []\nfor value in df['floor'].values:\n    value = value.split()\n    numeric = (value[1])\n    string  = value[0]\n    lst_numeric.append(numeric)\n    lst_str.append(string)\n\n# Replacing Chinese language words with English words.    \nlst_str_eng=[]\nfor string in lst_str:\n    #print(string)\n    if string == '中':\n        lst_str_eng.append(string.replace('中','middle'))\n    elif string == '高':\n        lst_str_eng.append(string.replace('高','high'))\n    elif string == '底':\n        lst_str_eng.append(string.replace('底','bottom'))\n    elif string == '低':\n        lst_str_eng.append(string.replace('低','low'))\n    elif string == '未知':\n        lst_str_eng.append(string.replace('未知','unknown'))\n    elif string == '顶':\n        lst_str_eng.append(string.replace('顶','top'))\n\n#print (len(lst_str_eng))\n# Converting intto Data Frame or in one shape dataset\ndf1 = pd.DataFrame(lst_str_eng,columns=['floorType'])\ndf2 = pd.DataFrame(lst_numeric,columns=['floorHeight'])\ndf = pd.concat([df,df1,df2],axis=1)\n# Deleting unknown values\ndf = df[df['floorType']!='unknown']\n\n# Dropping missing data which can't be converted into real data\ndf.dropna(inplace=True)\n\n# Dropping features which are not much relevant now.\ndf.drop(['floor','constructionTime'],axis=1,inplace=True)\n\n# Converting 'buildingType' feature to object or string type\ndf['buildingType'].replace(1,'Tower',inplace=True)\ndf['buildingType'].replace(2,'Bungalow',inplace=True)\ndf['buildingType'].replace(3,'Tower and Plate',inplace=True)\ndf['buildingType'].replace(4,'Plate',inplace=True)\n\n\n# Converting features datatype to see outliers\ndf['floorHeight'] = df['floorHeight'].astype(int)\ndf['livingRoom'] = df['livingRoom'].astype(int)\ndf['district'] = df['district'].astype(int)\ndf['tradeTime'] = df['tradeTime'].astype(int)\ndf['Age'] = df['Age'].astype(int)\ndf['renovationCondition'] = df['renovationCondition'].astype(int)\ndf['buildingStructure'] = df['buildingStructure'].astype(int)\ndf['elevator'] = df['elevator'].astype(int)\ndf['fiveYearsProperty'] = df['fiveYearsProperty'].astype(int)\ndf['subway'] = df['subway'].astype(int)\ndf['followers']  = df['followers'].astype(int)\n# Reseting the index\ndf.reset_index(inplace=True)\ndf.drop(['index'],axis=1,inplace=True)\n# Now the remaining data\nprint (\"DATA\", df.shape)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### After creating new features, their relationship with housing prices along with related features can easily see through visualizations.\n\n### Visualization is a great technique to find correlation and outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distance Impact on the Price\n\nsns.set(rc={'figure.figsize':(12.7,8.27)})\nax = sns.scatterplot(data=df, x=\"Lat\", y=\"Lng\", hue=\"price\",size = 'distance',\n                     legend='brief')\nax.set(xlabel=\"Latitude\", ylabel = \"Longitude\",title = \"Price Distribution Distance Wise\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# District wise Price\n\nsns.set(rc={'figure.figsize':(12.7,8.27)})\nax = sns.scatterplot(data=df, x=\"Lat\", y=\"Lng\", hue=\"price\",size = 'district',\n                     legend='brief')\nax.set(xlabel=\"Latitude\", ylabel = \"Longitude\",title = \"Price Distribution District Wise\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Age wise Distribution\n\nsns.set(rc={'figure.figsize':(12.7,8.27)})\nax = sns.scatterplot(data=df, x=\"Lat\", y=\"Lng\", hue=\"Age\",size = 'district',\n                     legend='brief')\nax.set(xlabel=\"Latitude\", ylabel = \"Longitude\",title = \"Age Distributio District Wise\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12.7,8.27)})\nax = sns.boxplot(data=df, x=\"district\", y=\"price\")\nax.set(xlabel=\"Districts\", ylabel = \"Price\",title = \"Correlation between District and Price\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12.7,8.27)})\nax = sns.boxplot(data=df, x=\"buildingType\", y=\"price\")\nax.set(xlabel=\"Building Type\", ylabel = \"Price\",title = \"Correlation between Building Type and Price\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12.7,8.27)})\nax = sns.boxplot(data=df, x=\"buildingType\", y=\"square\")\nax.set(xlabel=\"Building Type\", ylabel = \"Square\",title = \"Correlation between Building Type and Square\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### After creating visualizations, the impact on the prices is visible.\n\n### In figures \"Price Distribution Distance and District wise\", the most expensive houses centralizes close to the center of Beijing, while the cheapest ones spread in the suburban periphery\n\n### In figure \"Age Distribution District Wise\", the oldest houses are concentrated densely in the center of Beijing, while the newest ones are spread sparsely in the suburban areas.\n\n### Since the patterns in these figures are similar, a strong correlation between location, age, and price can be observed. \n\n### There are also noticeable differences in housing prices across 13 districts, which are summarized in Fig. \"Correlation between District and Price\" with outliers are clearly visible\n\n### Besides the location features, other features of the house also significantly contribute to the models’ performance. In the Correlation between BuildingType and Price figure, the difference in price among several building types can be clearly illustrated. Since bungalow is an old building type and is more likely to be built close to the center of Beijing, its price is costly regardless of the small area of a house ( correlation between BuildingType and Square figure)"},{"metadata":{},"cell_type":"markdown","source":"### After feature engineering, now the dataset is checked for outliers. \n\n### There are 10 categorical features, and 9 are numerical ones.\n\n### We have to visualize each numerical feature through \"Box Plot\" graph to see outliers first. Then apply the equation (1) to exclude them \n\n### Through Inter-Quartile Range (IQR), an outlier x can be detected if:\n###          x < Q1 − 1.5 · IQR OR Q3 + 1.5 · IQR < x        (1)\n### where:\n###       Q1 = 25th percentiles Q3 = 75th percentiles IQR = Q3 − Q1\n\n### To find percentile values, we will find the stats summary through pandas function, applied on each feature.\n\n### Then we will apply equation (1) to every numerical feature of the dataset except the target feature.\n\n"},{"metadata":{},"cell_type":"markdown","source":"### 1. \"Lng\" Longitude of the House"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.boxplot(['Lng'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Lng'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For 'Lng' Outliers\noutliers_Lng = []\nQ1 = 116.344557\nQ3 = 116.481385\nIQR = Q3 - Q1\nfor x in df['Lng'].values:\n    if (x < (Q1 - 1.5 * IQR)) or ((Q3 + 1.5 * IQR) < x):\n        if x not in outliers_Lng:\n            outliers_Lng.append(x)\n\n#print (sorted(outliers_Lng))\nfor outlier in outliers_Lng:\n    df = df[df['Lng']!=outlier]\nprint (\"DATA\", df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. \"Lat\" Latitude of the House"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.boxplot('Lat')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Lat'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For 'Lat' Outliers\noutliers_Lat = []\nQ1 = 39.894045\nQ3 = 40.012518\nIQR = Q3 - Q1\nfor x in df['Lat'].values:\n    if (x < (Q1 - 1.5 * IQR)) or ((Q3 + 1.5 * IQR) < x):\n        if x not in outliers_Lat:\n            outliers_Lat.append(x)\n\n#print (sorted(outliers_Lat))\nfor outlier in outliers_Lat:\n    df = df[df['Lat']!=outlier]\nprint ('DATA', df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. \"distance\" Distance to the Center of Beijing"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.boxplot(['distance'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['distance'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For 'distance' Outliers\noutliers_dist = []\nQ1 =  7.821041\nQ3 = 17.444622\nIQR = Q3 - Q1\nfor x in df['distance'].values:\n    if (x < (Q1 - 1.5 * IQR)) or ((Q3 + 1.5 * IQR) < x):\n        if x not in outliers_dist:\n            outliers_dist.append(x)\n\n#print (sorted(outliers_dist))\nfor outlier in outliers_dist:\n    df = df[df['distance']!=outlier]\nprint ('DATA' ,df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. \"Age\" Age of the House"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.boxplot('Age')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Age'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For 'Age' Outliers\noutliers_age = []\nQ1 = 13\nQ3 = 25\nIQR = Q3 - Q1\nfor x in df['Age'].values:\n    if (x < (Q1 - 1.5 * IQR)) or ((Q3 + 1.5 * IQR) < x):\n        if x not in outliers_age:\n            outliers_age.append(x)\n\n#print (sorted(outliers_age))\nfor outlier in outliers_age:\n    df = df[df['Age']!=outlier]\nprint (\"DATA\" ,df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. \"square\" Area of the House"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.boxplot('square')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['square'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For 'square' Outliers\noutliers_square = []\nQ1 = 58.280000\nQ3 = 99.330000\nIQR = Q3 - Q1\nfor x in df['square'].values:\n    if (x < (Q1 - 1.5 * IQR)) or ((Q3 + 1.5 * IQR) < x):\n        if x not in outliers_square:\n            outliers_square.append(x)\n\n#print (sorted(outliers_square))\nfor outlier in outliers_square:\n    df = df[df['square']!=outlier]\nprint ('DATA', df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6. \"Community Average\" Average Housing Price of the Community"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.boxplot('communityAverage')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['communityAverage'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For 'communityAverage' Outliers\noutliers_cA = []\nQ1 = 47402.000000\nQ3 = 72877.000000\nIQR = Q3 - Q1\nfor x in df['communityAverage'].values:\n    if (x < (Q1 - 1.5 * IQR)) or ((Q3 + 1.5 * IQR) < x):\n        if x not in outliers_cA:\n            outliers_cA.append(x)\n\n#print (sorted(outliers_cA))\nfor outlier in outliers_cA:\n    df = df[df['communityAverage']!=outlier]\nprint ('DATA', df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7. \"followers\" Number of Followers"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.boxplot(['followers'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['followers'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For 'followers' Outliers\noutliers_followers = []\nQ1 = 0\nQ3 = 20\nIQR = Q3 - Q1\nfor x in df['followers'].values:\n    if (x < (Q1 - 1.5 * IQR)) or ((Q3 + 1.5 * IQR) < x):\n        if x not in outliers_followers:\n            outliers_followers.append(x)\n\n#print (sorted(outliers_followers))\nfor outlier in  outliers_followers:\n    df = df[df['followers']!=outlier]\nprint ('DATA', df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8. \"tradeTime\" Trade Time (2002-2018)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.boxplot(['tradeTime'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['tradeTime'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For 'followers' Outliers\noutliers_tradeTime = []\nQ1 = 2013\nQ3 = 2016\nIQR = Q3 - Q1\nfor x in df['tradeTime'].values:\n    if (x < (Q1 - 1.5 * IQR)) or ((Q3 + 1.5 * IQR) < x):\n        if x not in outliers_tradeTime:\n            outliers_tradeTime.append(x)\n\n#print (sorted(outliers_tradeTime))\ndf = df[df['tradeTime'] != 2002]\nprint ('DATA',df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 9. ladderRatio\" Ratio b/w population and # of elevators of floor"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.boxplot(['ladderRatio'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['ladderRatio'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For 'ladderRatio' Outliers\noutliers_ladderRatio = []\nQ1 = 2.500000e-01\nQ3 = 5.000000e-01\nIQR = Q3 - Q1\nfor x in df['ladderRatio'].values:\n    if (x < (Q1 - 1.5 * IQR)) or ((Q3 + 1.5 * IQR) < x):\n        if x not in outliers_ladderRatio:\n            outliers_ladderRatio.append(x)\n\n#print (sorted(outliers_ladderRatio))\nfor outlier in outliers_ladderRatio:\n    df = df[df['ladderRatio']!=outlier]\nprint ('DATA', df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### As we know that there are 10 categorical features in the dataset. These should be converted to numerical finite values. Machine Learning algorithms only deal with numerical values. Giving finite values to these features will act in the same manner as a categorical feature. There will be no difference. The categorical features are\n\n### 1. \"livingRoom\" Number of bedrooms\n### 2. \"subway\" Whether the house is near any subways\n### 3. \"fiveYearsProperty\" Whether the house is a five-year property\n### 4. \"elevator\" Whether the house has any elevator\n### 5. \"floorHeight\" The Floor Height\"\n### 6. \"buildingStructure\" Building Structure\n### 7. \"renovationCondition\" Renovation Condition\n### 8. \"buildingType\" Building Type\n### 9. \"floorType\" Floor height relative to the building\n### 10. \"district\" District (District 1- District 13)\n\n### Some are already in numeric form while others have to be transformed."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label Encoding of Categorical Features\nlbl = LabelEncoder()\n# 'floorType'\nlbl.fit(list(df['floorType'].values) + list(df['floorType'].values))\ndf['floorType'] = lbl.transform(list(df['floorType'].values))\n# 'buildingType'\nlbl.fit(list(df['buildingType'].values) + list(df['buildingType'].values))\ndf['buildingType'] = lbl.transform(list(df['buildingType'].values))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now all are converted. Lets check data type of each feature for algorithm and the remaining data (219271) after excluding all the outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"print ( \"The DATA\" , df.shape)\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The Final Data"},{"metadata":{},"cell_type":"markdown","source":"### According to the paper, their data after omitting outliers contained 231962 data of rows with 19 features. Though we collected 219271 data of rows after performing all the operations. The error is 5% which is acceptable due to any reason like the different approach of the researcher towards data handling in some parts of the data which we considered differently.   "},{"metadata":{},"cell_type":"markdown","source":"### Features Selection"},{"metadata":{},"cell_type":"markdown","source":"### Selecting features is always based on statistical information. That's why we have to check the correlation as well as the p-value between the target feature \"price\" and all the other features.\n\n### The statistical Analysis provides the statistical significance of any feature (the independent variables) for the target or dependent variable. \n\n### In statistics, \"the p-value is the probability of obtaining results at least as extreme as the observed results of a statistical hypothesis test, assuming that the null hypothesis is correct. A smaller p-value means that there is stronger evidence in favor of the alternative hypothesis\".\n\n\n### P-Value is the threshold to check the significance. If P-value < 0.01 or 0.05 or 0.1 (99% or 95% or 90% Significance Levels ) of any independent variable, it is become statistically significant to affect the outcome. In our case, any statistically significant variable will affect the Housing Price. \n\n### There are mathematical (Stats library SciPy) and visualization (Heat Map) ways to determine the p-value and the correlation like strong, modest, or weak relation either positive or negative relationship with the predicted variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"### View of Correlation Matrix Through head map\ncorr_matrix=df._get_numeric_data().corr()\nfig, ax = plt.subplots(figsize=(10,5))         # Sample figsize in inches\nsns.heatmap(corr_matrix, annot=False, linewidths=5, ax=ax, xticklabels=corr_matrix.columns.values,yticklabels=corr_matrix.columns.values)\n#sns.heatmap(corr, annot=True, fmt=\".1f\",linewidth=0.5 xticklabels=corr.columns.values,yticklabels=corr.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# finding correlation and p_value\npearson_coef, p_value = stats.pearsonr(df['Lng'], df['price'])\nprint(\" The Pearson Correlation Coefficient 'Lng' is\", pearson_coef, \" with a P-value of P =\", p_value)\npearson_coef, p_value = stats.pearsonr(df['Lat'], df['price'])\nprint(\" The Pearson Correlation Coefficient 'Lat' is\", pearson_coef, \" with a P-value of P =\", p_value)\npearson_coef, p_value = stats.pearsonr(df['tradeTime'], df['price'])\nprint(\" The Pearson Correlation Coefficient 'tradeTime' is\", pearson_coef, \" with a P-value of P =\", p_value)\npearson_coef, p_value = stats.pearsonr(df['followers'], df['price'])\nprint(\" The Pearson Correlation Coefficient 'followers' is\", pearson_coef, \" with a P-value of P =\", p_value)\npearson_coef, p_value = stats.pearsonr(df['square'], df['price'])\nprint(\" The Pearson Correlation Coefficient 'square' is\", pearson_coef, \" with a P-value of P =\", p_value)\npearson_coef, p_value = stats.pearsonr(df['livingRoom'], df['price'])\nprint(\" The Pearson Correlation Coefficient 'livingRoom' is\", pearson_coef, \" with a P-value of P =\", p_value)\npearson_coef, p_value = stats.pearsonr(df['buildingType'], df['price'])\nprint(\" The Pearson Correlation Coefficient 'buildingType' is\", pearson_coef, \" with a P-value of P =\", p_value)\npearson_coef, p_value = stats.pearsonr(df['renovationCondition'], df['price'])\nprint(\" The Pearson Correlation Coefficient 'renovationCondition' is\", pearson_coef, \" with a P-value of P =\", p_value)\npearson_coef, p_value = stats.pearsonr(df['buildingStructure'], df['price'])\nprint(\" The Pearson Correlation Coefficient 'buildingStructure' is\", pearson_coef, \" with a P-value of P =\", p_value)\npearson_coef, p_value = stats.pearsonr(df['ladderRatio'], df['price'])\nprint(\" The Pearson Correlation Coefficient 'ladderRatio' is\", pearson_coef, \" with a P-value of P =\", p_value)\npearson_coef, p_value = stats.pearsonr(df['elevator'], df['price'])\nprint(\" The Pearson Correlation Coefficient 'elevator' is\", pearson_coef, \" with a P-value of P =\", p_value)\npearson_coef, p_value = stats.pearsonr(df['fiveYearsProperty'], df['price'])\nprint(\" The Pearson Correlation Coefficient 'fiveYearsProperty' is\", pearson_coef, \" with a P-value of P =\", p_value)\npearson_coef, p_value = stats.pearsonr(df['communityAverage'], df['price'])\nprint(\" The Pearson Correlation Coefficient 'communityAverage' is\", pearson_coef, \" with a P-value of P =\", p_value)\npearson_coef, p_value = stats.pearsonr(df[ 'distance'], df['price'])\nprint(\" The Pearson Correlation Coefficient  'distance' is\", pearson_coef, \" with a P-value of P =\", p_value)\npearson_coef, p_value = stats.pearsonr(df['Age'], df['price'])\nprint(\" The Pearson Correlation Coefficient 'Age' is\", pearson_coef, \" with a P-value of P =\", p_value)\npearson_coef, p_value = stats.pearsonr(df['subway'], df['price'])\nprint(\" The Pearson Correlation Coefficient 'subway' is\", pearson_coef, \" with a P-value of P =\", p_value)\npearson_coef, p_value = stats.pearsonr(df['district'], df['price'])\nprint(\" The Pearson Correlation Coefficient 'district'is\", pearson_coef, \" with a P-value of P =\", p_value)\npearson_coef, p_value = stats.pearsonr(df['floorHeight'], df['price'])\nprint(\" The Pearson Correlation Coefficient 'floorHeight' is\", pearson_coef, \" with a P-value of P =\", p_value)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Through visualization and statistical findings; most features have a very small p-value which indicates the null hypothesis is true. Or in other words, they are highly significant.\n\n### Though the correlation is weak for most of the features, but due to dealing with a large sample and keep in mind the p-values, they can be statistically significant.\n\n### Therefore we will pick all the 19 independent features in the dataset to predict the housing prices as did by the researcher."},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Model Selection & Evaluation"},{"metadata":{},"cell_type":"markdown","source":"### The algorithms are dealing with two types of methods, Regression and Classification. As we are predicting prices, it means we will go for regression models.  We will select five different algorithms such as \"Random Forest\", \"Extreme Gradient Boosting (XGBoost)\", \"Light Gradient Boosting Machine (LightGBM)\", \"Hybrid Regression\" and \"Stacked Generalization\". The purpose of selecting more than one algorithm is to verify the accuracy across each model to ensure good predicting results.\n\n### After predicting results, the evaluation is a must to put a stamp on your work. We will use the \"Root Mean Squared Logarithmic Error (RMSLE)\" evaluation technique to check the results. \n\n### RMSLE is robust to outliers. It only considers the relative error between the Predicted and the actual value, and the scale of the error is not significant. RMSLE incurs a larger penalty for the underestimation of the Actual variable than the Overestimation. In simple words, more penalty is incurred when the predicted value is less than the Actual Value. On the other hand, a Less penalty is incurred when the predicted value is more than the actual value."},{"metadata":{},"cell_type":"markdown","source":"## 2.3.1 Random Forest Classifier"},{"metadata":{},"cell_type":"markdown","source":"### A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree. \n\n### The advantages of random forest are\n\n### 1. Decorrelate trees\n### 2. Error reducer\n### 3. Good performer on imbalanced dataset\n### 4. Good in Handling the huge amount of data\n### 5. Little impact of outliers\n### 6. Robust to overfitting\n\n### Similarly the disadvantages are\n\n### 1. Features need to have some predictive power else they won’t work.\n### 2. Predictions of the trees need to be uncorrelated.\n### 3. Appears as Black Box: It is tough to know what is happening. You can at best try different parameters and random seeds to change the outcomes and performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"# All the columns\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Designing the Base For Each algorithm\n\n# Feature Selection\n\nX = df[['Lng', 'Lat', 'tradeTime', 'followers', 'square', 'livingRoom',\n       'buildingType', 'renovationCondition', 'buildingStructure',\n       'ladderRatio', 'elevator', 'fiveYearsProperty', 'subway', 'district',\n       'communityAverage', 'distance', 'Age', 'floorType', 'floorHeight']]\n\n# Standardized the Features\nX = np.asarray(X)\nX = preprocessing.StandardScaler().fit(X).transform(X)  \n\n# Selecting Target Feature\ny =  np.asarray(df['price'])#.reshape(-1, 1)\n\n# Split the data into training and test data to see results on the test data\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n# Further spliting the Traning set into sub Train and Test Data to see results on training data\ntrain_X,test_X,train_y,test_y = train_test_split(X_train,y_train,random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Model Trainng For Training Set\n# Specify Model\nrf = RandomForestRegressor(random_state=42,n_estimators=900,max_depth=20,\n                                              n_jobs=-1,min_samples_split=10,) #110,325\n# Fit Model\n\nrf.fit(train_X, train_y.ravel())\n\n# Make validation predictions\n\npred_y = rf.predict(test_X)\n\n# Model Evaluation\n\nprint (\"RMSLE {:,.5f}\".format(np.sqrt(mean_squared_log_error( pred_y, test_y))))\n#print (\"Model Accuracy {:,.5f}\".format(rf.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The error is low (0.292), it means the algorithm is not working badly. We can ensure the results through visualization too."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12.7,8.27)})\nax = sns.regplot(pred_y,test_y,color = 'blue',line_kws={'color':'orange'})#,hue='y_test')\n\nax.set(xlabel=\"Random Forest Prediction\", ylabel = \"Actual Price\",title = \"RANDOM FOREST RESULTS\")\nax.legend(['Line of Best Fit', 'Housing Price'], loc='lower right')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### It is clear from the above figure that both prices are directly proportional. So the predicted results are good for the training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Model Trainng For Test Data\n# Specify Model\nrf = RandomForestRegressor(random_state=42,n_estimators=900,max_depth=20,\n                                              n_jobs=-1,min_samples_split=10,) #110,325\n# Fit Model\n\nrf.fit(X_train, y_train.ravel())\n\n# Make validation predictions\n\ny_pred = rf.predict(X_test)\n\n# Model Evaluation\n\nprint (\"RMSLE {:,.5f}\".format(np.sqrt(mean_squared_log_error( y_pred, y_test))))\n#print (\"Model Accuracy {:,.5f}\".format(rf.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The error on the test data is also low (0.302), it means the algorithm is not working badly. We can ensure the results through visualization "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12.7,8.27)})\nax = sns.regplot(y_pred,y_test,color = 'blue',line_kws={'color':'orange'})#,hue='y_test')\n\nax.set(xlabel=\"Random Forest Prediction\", ylabel = \"Actual Price\",title = \"RANDOM FOREST RESULTS\")\nax.legend(['Line of Best Fit', 'Housing Price'], loc='lower right')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### It is clear from the above figure that both prices are directly proportional. So the predicted results are good for the test data."},{"metadata":{},"cell_type":"markdown","source":"## 2.3.2 Light Gradient Boosting Machine (LightGBM)"},{"metadata":{},"cell_type":"markdown","source":"### LightGBM uses a novel technique of Gradient-based One-Side Sampling (GOSS) to filter out the data instances for finding a split value\n\n### Advantages of Light GBM\n\n### Faster training speed and higher efficiency: Light GBM use histogram based algorithm i.e it buckets continuous feature values into discrete bins which fasten the training procedure.\n\n### Lower memory usage: Replaces continuous values to discrete bins which result in lower memory usage.\n\n### Better accuracy than any other boosting algorithm: It produces much more complex trees by following leaf wise split approach rather than a level-wise approach which is the main factor in achieving higher accuracy. However, it can sometimes lead to overfitting which can be avoided by setting the max_depth parameter.\n\n### Compatibility with Large Datasets: It is capable of performing equally good with large datasets with a significant reduction in training time as compared to XGBOOST.\n\n### Disadvantages of Light GBM\n\n### Boosting models are more prone to over-fitting , bagging models on other hand is simple to use and doesn't have lot of hyper parameter.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Light GBM Model Training For Training Data\n\nimport lightgbm as lgb\ngbm = lgb.LGBMRegressor(objective='regression',num_leaves=36,learning_rate=0.15,\n                        n_estimators=64,min_child_weight = 2, colsample_bytree = 0.8,\n                        reg_lambda = 0.45)\ngbm.fit(train_X, train_y,\n    eval_set=[(test_X, test_y)],\n    eval_metric='l2_root',\n    early_stopping_rounds=50)\npred_y = gbm.predict(test_X, num_iteration=gbm.best_iteration_)\n#accuracy = round(gbm.score(X_train, y_train)*100,2)\n#mse = mean_squared_error(y_test,y_pred)\n#rmse = np.sqrt(mse)\nprint (\"RMSLE {:,.5f}\".format(np.sqrt(mean_squared_log_error( pred_y, test_y))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The error is low (0.297), it means the algorithm is not working badly. We can ensure the results through visualization "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12.7,8.27)})\nax = sns.regplot(pred_y,test_y,color = 'blue',line_kws={'color':'orange'})#,hue='y_test')\n\nax.set(xlabel=\"Light GBM Prediction\", ylabel = \"Actual Price\",title = \"Light GBM RESULTS\")\nax.legend(['Line of Best Fit', 'Housing Price'], loc='lower right')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### It is clear from the above figure that both prices are directly proportional. So the predicted results are good for the training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Light GBM Model Training For Test Data\n\nimport lightgbm as lgb\ngbm = lgb.LGBMRegressor(objective='regression',num_leaves=36,learning_rate=0.15,\n                        n_estimators=64,min_child_weight = 2, colsample_bytree = 0.8,\n                        reg_lambda = 0.45)\ngbm.fit(X_train, y_train,\n    eval_set=[(X_test, y_test)],\n    eval_metric='l2_root',\n    early_stopping_rounds=50)\ny_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration_)\n#accuracy = round(gbm.score(X_train, y_train)*100,2)\n#mse = mean_squared_error(y_test,y_pred)\n#rmse = np.sqrt(mse)\nprint (\"RMSLE {:,.5f}\".format(np.sqrt(mean_squared_log_error( y_pred, y_test))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The error on the test data is also low (0.308), it means the algorithm is not working badly. We can ensure the results through visualization. "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12.7,8.27)})\nax = sns.regplot(y_pred,y_test,color = 'blue',line_kws={'color':'orange'})#,hue='y_test')\n\nax.set(xlabel=\"Light GBM Prediction\", ylabel = \"Actual Price\",title = \"Light GBM RESULTS\")\nax.legend(['Line of Best Fit', 'Housing Price'], loc='lower right')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### It is clear from the above figure that both prices are directly proportional. So the predicted results are good for the test data."},{"metadata":{},"cell_type":"markdown","source":"## 2.3.3 Extreme Gradient Boosting (XGBoost)"},{"metadata":{},"cell_type":"markdown","source":"### XGBoost is a scalable machine learning system for tree boosting. The system is available as an open-source package.The system has generated a significant impact and been widely recognized in various machine learning and data mining challenges.\n\n# PROS\n\n### Less feature engineering required (No need for scaling, normalizing data, can also handle missing values well)\n\n### Feature importance can be found out(it output importance of each feature, can be used for feature selection)\n\n### Fast to interpret\n\n### Outliers have minimal impact.\n\n### Handles large sized datasets well.\n\n### Good Execution speed\n\n### Good model performance (wins most of the Kaggle competitions)\n\n### Less prone to overfitting\n\n# CONS\n\n### Difficult interpretation , visualization tough\n### Overfitting possible if parameters not tuned properly.\n### Harder to tune as there are too many hyperparameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# XGBoost Model Training For Training Data\n# Traing the Model\nxg_reg = xgb.XGBRegressor(objective ='reg:squarederror', min_child_weight = 2, subsample = 1,\n                          colsample_bytree = 0.8,\n                          learning_rate = 0.2, n_estimators = 500,\n                         reg_lambda = 0.45, reg_alpha = 0, gamma = 0.5)\n\n# Fitting the Model\nxg_reg.fit(train_X,train_y)\n\n# Predicting\npred_y = xg_reg.predict(test_X)\n\n# Evaluation\nprint (\"RMSLE {:,.5f}\".format(np.sqrt(mean_squared_log_error( pred_y, test_y))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The error is low (0.291), it means the algorithm is not working badly. We can ensure the results through visualization "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12.7,8.27)})\nax = sns.regplot(pred_y,test_y,color = 'blue',line_kws={'color':'orange'})#,hue='y_test')\n\nax.set(xlabel=\"XGBoost Prediction\", ylabel = \"Actual Price\",title = \"XGBoost RESULTS\")\nax.legend(['Line of Best Fit', 'Housing Price'], loc='lower right')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### It is clear from the above figure that both prices are directly proportional. So the predicted results are good for the training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# XGBoost Model Training For Test Data\n# Traing the Model\nxg_reg = xgb.XGBRegressor(objective ='reg:squarederror', min_child_weight = 2, subsample = 1,\n                          colsample_bytree = 0.8,\n                          learning_rate = 0.2, n_estimators = 500,\n                         reg_lambda = 0.45, reg_alpha = 0, gamma = 0.5)\n\n# Fitting the Model\nxg_reg.fit(X_train,y_train)\n\n# Predicting\ny_pred = xg_reg.predict(X_test)\n\n# Evaluation\nprint (\"RMSLE {:,.5f}\".format(np.sqrt(mean_squared_log_error( y_pred, y_test))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The error on the test data is very low (0.302), it means the algorithm is not working badly. We can ensure the results through visualization "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12.7,8.27)})\nax = sns.regplot(y_pred,y_test,color = 'blue',line_kws={'color':'orange'})#,hue='y_test')\n\nax.set(xlabel=\"XGBoost Prediction\", ylabel = \"Actual Price\",title = \"XGBoost RESULTS\")\nax.legend(['Line of Best Fit', 'Housing Price'], loc='lower right')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### It is clear from the above figure that both prices are directly proportional. So the predicted results are good for the test data."},{"metadata":{},"cell_type":"markdown","source":"## 2.3.4 Hybrid Regression"},{"metadata":{},"cell_type":"markdown","source":"### Hybrid Regression is a model that includes two or more different regression models. We will combine all the above three models to make a Hybrid model. \n\n### This method averages the individual predictions to form a final prediction. It is a fair approach to balance between bias and variance in the composite models. This technique also supports weight assignment to each component model, but it may lead to bias over one model, losing the benefits of generalization.\n\n###  Hybrid Regression is actually a VotingRegressor algorithm in sklearn module. A voting regressor is an ensemble meta-estimator that fits several base regressors, each on the whole dataset. Then it averages the individual predictions to form a final prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Hybrid MOdel Training For Training Data\n# Model 1\nr1 = xgb.XGBRegressor(objective ='reg:squarederror', min_child_weight = 2, subsample = 1,\n                          colsample_bytree = 0.8,\n                          learning_rate = 0.2, n_estimators = 500,\n                         reg_lambda = 0.45, reg_alpha = 0, gamma = 0.5)\n# Model 2\nr2 = lgb.LGBMRegressor(objective='regression',num_leaves=36,learning_rate=0.15,\n                        n_estimators=64,min_child_weight = 2, colsample_bytree = 0.8,\n                        reg_lambda = 0.45)\n# Model 3\nr3 = RandomForestRegressor(random_state=42,n_estimators=900,max_depth=20,\n                                              n_jobs=-1,min_samples_split=10,)\n\n# Making Hybrid Model\nvreg = VotingRegressor([('gb', r1), ('lgb', r2), ('rf', r3)])\n\n# Fitting the Model\nvreg.fit(train_X, train_y)\n\n# Predicting the Model\npred_y = vreg.predict(test_X)\n\n# Evaluation\nprint (\"RMSLE {:,.5f}\".format(np.sqrt(mean_squared_log_error( pred_y, test_y))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The error is low (0.291), it means the algorithm is not working badly. We can ensure the results through visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12.7,8.27)})\nax = sns.regplot(pred_y,test_y,color = 'blue',line_kws={'color':'orange'})#,hue='y_test')\n\nax.set(xlabel=\"Hybrid Prediction\", ylabel = \"Actual Price\",title = \"Hybrid Results\")\nax.legend(['Line of Best Fit', 'Housing Price'], loc='lower right')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### It is clear from the above figure that both prices are directly proportional. So the predicted results are good for the training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Hybrid Model Training For Test Data\n# Model 1\nr1 = xgb.XGBRegressor(objective ='reg:squarederror', min_child_weight = 2, subsample = 1,\n                          colsample_bytree = 0.8,\n                          learning_rate = 0.2, n_estimators = 500,\n                         reg_lambda = 0.45, reg_alpha = 0, gamma = 0.5)\n# Model 2\nr2 = lgb.LGBMRegressor(objective='regression',num_leaves=36,learning_rate=0.15,\n                        n_estimators=64,min_child_weight = 2, colsample_bytree = 0.8,\n                        reg_lambda = 0.45)\n# Model 3\nr3 = RandomForestRegressor(random_state=42,n_estimators=900,max_depth=20,\n                                              n_jobs=-1,min_samples_split=10,)\n\n# Making Hybrid Model\nvreg = VotingRegressor([('gb', r1), ('lgb', r2), ('rf', r3)])\n\n# Fitting the Model\nvreg.fit(X_train, y_train)\n\n# Predicting the Model\ny_pred = vreg.predict(X_test)\n\n# Evaluation\nprint (\"RMSLE {:,.5f}\".format(np.sqrt(mean_squared_log_error( y_pred, y_test))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The error is low (0.302), it means the algorithm is not working badly. We can ensure the results through visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12.7,8.27)})\nax = sns.regplot(y_pred,y_test,color = 'blue',line_kws={'color':'orange'})#,hue='y_test')\n\nax.set(xlabel=\"Hybrid Prediction\", ylabel = \"Actual Price\",title = \"Hybrid Results\")\nax.legend(['Line of Best Fit', 'Housing Price'], loc='lower right')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### It is clear from the above figure that both prices are directly proportional. So the predicted results are good for the test data."},{"metadata":{},"cell_type":"markdown","source":"## 2.3.5 Stacked Generalization"},{"metadata":{},"cell_type":"markdown","source":"### Stacked generalization consists in stacking the output of individual estimator and use a regressor to compute the final prediction. Stacking allows to use the strength of each individual estimator by using their output as input of a final estimator.\n\n### The main idea of this method is to use the predictions of previous models as features for another model. This approach also utilizes the k-fold cross-validation technique to avoid overfitting.\n\n### sklearn module StackingRegressor is based on stacked generalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Stacked Model Training For Training Data\n\n# Model 1\nr1 = xgb.XGBRegressor(objective ='reg:squarederror', min_child_weight = 2, subsample = 1,\n                          colsample_bytree = 0.8,\n                          learning_rate = 0.2, n_estimators = 500,\n                         reg_lambda = 0.45, reg_alpha = 0, gamma = 0.5)\n# Model 2\nr2 = lgb.LGBMRegressor(objective='regression',num_leaves=36,learning_rate=0.15,\n                        n_estimators=64,min_child_weight = 2, colsample_bytree = 0.8,\n                        reg_lambda = 0.45)\n# Model 3\nr3 = RandomForestRegressor(random_state=42,n_estimators=900,max_depth=20,\n                                              n_jobs=-1,min_samples_split=10,)\n\n# Making Stacked Model\nestimators = [('xgb', r1),('lgb', r2)]\nsreg = StackingRegressor(estimators=estimators,final_estimator = r3)\n                         \n\n# Fitting the Model\nsreg.fit(train_X, train_y)\n\n# Predicting the Model\npred_y = sreg.predict(test_X)\n\n# Evaluation\nprint (\"RMSLE {:,.5f}\".format(np.sqrt(mean_squared_log_error( pred_y, test_y))))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The error is low (0.293), it means the algorithm is not working badly. We can ensure the results through visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12.7,8.27)})\nax = sns.regplot(pred_y,test_y,color = 'blue',line_kws={'color':'orange'})#,hue='y_test')\n\nax.set(xlabel=\"Stacked Prediction\", ylabel = \"Actual Price\",title = \"Stacked Results\")\nax.legend(['Line of Best Fit', 'Housing Price'], loc='lower right')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is clear from the above figure that both prices are directly proportional. So the predicted results are good for the training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Stacked Model Training For Test Data\n\n# Model 1\nr1 = xgb.XGBRegressor(objective ='reg:squarederror', min_child_weight = 2, subsample = 1,\n                          colsample_bytree = 0.8,\n                          learning_rate = 0.2, n_estimators = 500,\n                         reg_lambda = 0.45, reg_alpha = 0, gamma = 0.5)\n# Model 2\nr2 = lgb.LGBMRegressor(objective='regression',num_leaves=36,learning_rate=0.15,\n                        n_estimators=64,min_child_weight = 2, colsample_bytree = 0.8,\n                        reg_lambda = 0.45)\n# Model 3\nr3 = RandomForestRegressor(random_state=42,n_estimators=900,max_depth=20,\n                                              n_jobs=-1,min_samples_split=10,)\n\n# Making Stacked Model\nestimators = [('xgb', r1),('lgb', r2)]\nsreg = StackingRegressor(estimators=estimators,final_estimator = r3)\n                         \n\n# Fitting the Model\nsreg.fit(X_train, y_train)\n\n# Predicting the Model\ny_pred = sreg.predict(X_test)\n\n# Evaluation\nprint (\"RMSLE {:,.5f}\".format(np.sqrt(mean_squared_log_error( y_pred, y_test))))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The error is very low (0.303), it means the algorithm is not working badly. We can ensure the results through visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12.7,8.27)})\nax = sns.regplot(y_pred,y_test,color = 'blue',line_kws={'color':'orange'})#,hue='y_test')\n\nax.set(xlabel=\"Stacked Prediction\", ylabel = \"Actual Price\",title = \"Stacked Results\")\nax.legend(['Line of Best Fit', 'Housing Price'], loc='lower right')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### It is clear from the above figure that both prices are directly proportional. So the predicted results are good for the test data."},{"metadata":{},"cell_type":"markdown","source":"# 3. Results"},{"metadata":{},"cell_type":"markdown","source":"## RMSLE"},{"metadata":{},"cell_type":"markdown","source":"| Model | Train Set | Test Set |\n| --- | --- | --- |\n| Random Forest | 0.292 | 0.302 |\n| Light GBM | 0.297 | 0.308 |\n| XG Boost  | 0.291 | 0.302 |\n| Hybrid Regression   | 0.291 | 0.302 |\n| Stacked Generalization | 0.293 | 0.303 |"},{"metadata":{},"cell_type":"markdown","source":"## 3.1 Results Explanation"},{"metadata":{},"cell_type":"markdown","source":"### The difference among each algorithm is very minor. Results are more or less the same.\n\n### XG Boost and Hybrid Regression results are the better for both Train as well as Test Sets.\n\n### Train Set is showing less error as compared to test set, which we can take as some overfitting to some extent."},{"metadata":{},"cell_type":"markdown","source":"## 3.2 Results Comparison witht the Researcher Work"},{"metadata":{},"cell_type":"markdown","source":"### The researcher models are showing almost the same results among them but with less error as compared to our results\n\n### Their minimum Train Set error is 0.13 while we reach at 0.29\n\n### Similarly for Test Set error is 0.16 while our is showing 0.30 "},{"metadata":{},"cell_type":"markdown","source":"## 3.3 Reasons For Different Results"},{"metadata":{},"cell_type":"markdown","source":"### We fetched the data from Kaggle source which has more bugs.\n### We deleted all the missing values as did by the researcher. But if we replace with appropriate values, then no data will be lost and there is more chance to acquire better results."},{"metadata":{},"cell_type":"markdown","source":"# 4. Recommendations"},{"metadata":{},"cell_type":"markdown","source":"### After replicating the paper, we can improve our results by doing more exploratory analysis.\n\n### We can also apply for estimating any housing price of different cities by keeping in mind the relevant data and the methodology applied in the paper."},{"metadata":{},"cell_type":"markdown","source":"### Rough Work\ndef score_func(y_true, y_pred, **kwargs):\n    y_true = np.abs(y_true)\n    y_pred = np.abs(y_pred)\n\n    return np.sqrt(mean_squared_log_error(y_true, y_pred))\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\n\nparameters = {'n_estimators':[i for i in range(100,300,100)],'max_depth':[i for i in range(0,4,2)],\n              'min_samples_split':[i for i in range(2, 4)],'random_state': [42]}\nscorer = make_scorer(score_func)\nrf = RandomForestRegressor()\nsearch = GridSearchCV(rf, parameters)#,scoring=scorer)\nresult = search.fit(X_train, y_train.ravel())\nprint('Best Score: %s' % result.best_score_)\nprint('Best Hyperparameters: %s' % result.best_params_)\nsorted(clf.cv_results_.keys())\nscorer = make_scorer(rmsle, greater_is_better=False, size=10)\ngrid = GridSearchCV(est, param_grid, scoring=scorer)"},{"metadata":{},"cell_type":"markdown","source":"from sklearn.model_selection import GridSearchCV\nparameters = {'n_estimators':[i for i in range(100,1100,100)],'max_depth':[i for i in range(1,21,2)],\n              'min_samples_split':[i for i in range(2, 12)],'random_state': [42]}\n\nrf = RandomForestRegressor()\n\n#search = GridSearchCV(rf, parameter, scoring='accuracy', n_jobs=-1, cv=cv)\nclf = GridSearchCV(rf, parameters)\nclf.fit(X_train, y_train.ravel())\nsorted(clf.cv_results_.keys())"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}