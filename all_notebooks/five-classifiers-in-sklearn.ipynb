{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport re\nimport string\nimport tqdm\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom wordcloud import WordCloud\nfrom wordcloud import STOPWORDS\nfrom nltk.corpus import stopwords\nimport xgboost\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer,HashingVectorizer\nfrom sklearn import decomposition, ensemble\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import accuracy_score,f1_score\n\nimport dask\nimport multiprocessing\nfrom joblib import delayed, Parallel, parallel_backend\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading data"},{"metadata":{"trusted":true},"cell_type":"code","source":"cores = multiprocessing.cpu_count()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"../input/tweets-with-sarcasm-and-irony/train.csv\")\ntest_data = pd.read_csv(\"../input/tweets-with-sarcasm-and-irony/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remove recurring tweets to prevent ambiguity"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tweets=train_data['tweets'].tolist()\ntest_tweets=test_data['tweets'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def keep_uniques(array, df):\n    dels=[]\n    for i in array:\n        if array.count(i)>1:\n            dels.append(i)\n    dels=list(set(dels))\n    for i in dels:\n        df.drop( df[ df['tweets'] == i ].index, inplace=True)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data=keep_uniques(train_tweets, train_data)\ntest_data=keep_uniques(test_tweets, test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_data['tweets'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(test_data['tweets'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploring Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_data.sample(frac = 1)\ntest_data = test_data.sample(frac = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we see that the `regular` class has 18k tweets, which causes our dataset to be imbalanced. So we shall delete some tweets from this class"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp=train_data.loc[train_data['class'] == 'regular']\nlis=temp['tweets'].tolist()\n\nimport random\nreg_del=[]\nvisited=set()\nfor _ in range(3600):\n    n=random.randint(0,18556)\n    if n not in visited:\n        reg_del.append(lis[n])\n        \n        \nfor i in reg_del:\n    train_data.drop( train_data[ train_data['tweets'] == i ].index, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data['class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = test_data.dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning & Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean(tweet): \n    \n\n    # Special characters\n    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\n    tweet = re.sub(r\"let\\x89Ûªs\", \"let's\", tweet)\n    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n    tweet = re.sub(r\"å_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n    tweet = re.sub(r\"åÊ\", \"\", tweet)\n    tweet = re.sub(r\"åÈ\", \"\", tweet)\n    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)    \n    tweet = re.sub(r\"Ì©\", \"e\", tweet)\n    tweet = re.sub(r\"å¨\", \"\", tweet)\n    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n    tweet = re.sub(r\"åÇ\", \"\", tweet)\n    tweet = re.sub(r\"å£3million\", \"3 million\", tweet)\n    tweet = re.sub(r\"åÀ\", \"\", tweet)\n    \n    #emojis\n    emoji_pattern = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE)\n    tweet =  emoji_pattern.sub(r'', tweet)\n    \n    # usernames mentions like \"@abc123\"\n    ment = re.compile(r\"(@[A-Za-z0-9]+)\")\n    tweet =  ment.sub(r'', tweet)\n    \n    # Contractions\n    tweet = re.sub(r\"he's\", \"he is\", tweet)\n    tweet = re.sub(r\"there's\", \"there is\", tweet)\n    tweet = re.sub(r\"We're\", \"We are\", tweet)\n    tweet = re.sub(r\"That's\", \"That is\", tweet)\n    tweet = re.sub(r\"won't\", \"will not\", tweet)\n    tweet = re.sub(r\"they're\", \"they are\", tweet)\n    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"What's\", \"What is\", tweet)\n    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n    tweet = re.sub(r\"There's\", \"There is\", tweet)\n    tweet = re.sub(r\"He's\", \"He is\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"You're\", \"You are\", tweet)\n    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n    tweet = re.sub(r\"you've\", \"you have\", tweet)\n    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n    tweet = re.sub(r\"we're\", \"we are\", tweet)\n    tweet = re.sub(r\"what's\", \"what is\", tweet)\n    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n    tweet = re.sub(r\"we've\", \"we have\", tweet)\n    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n    tweet = re.sub(r\"who's\", \"who is\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n    tweet = re.sub(r\"would've\", \"would have\", tweet)\n    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n    tweet = re.sub(r\"We've\", \"We have\", tweet)\n    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n    tweet = re.sub(r\"they've\", \"they have\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"should've\", \"should have\", tweet)\n    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n    tweet = re.sub(r\"where's\", \"where is\", tweet)\n    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n    tweet = re.sub(r\"They're\", \"They are\", tweet)\n    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n    tweet = re.sub(r\"let's\", \"let us\", tweet)\n    tweet = re.sub(r\"it's\", \"it is\", tweet)\n    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n    tweet = re.sub(r\"don't\", \"do not\", tweet)\n    tweet = re.sub(r\"you're\", \"you are\", tweet)\n    tweet = re.sub(r\"i've\", \"I have\", tweet)\n    tweet = re.sub(r\"that's\", \"that is\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n    tweet = re.sub(r\"I've\", \"I have\", tweet)\n    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n    tweet = re.sub(r\"donå«t\", \"do not\", tweet)   \n            \n    # Character entity references\n    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n    \n    # html tags\n    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n    tweet = re.sub(html, '', tweet)\n    \n    # Urls\n    tweet = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", tweet)\n    tweet = re.sub(r'https?://\\S+|www\\.\\S+','', tweet)\n        \n    #Punctuations and special characters\n    \n    tweet = re.sub('[%s]' % re.escape(string.punctuation),'',tweet)\n    \n    tweet = tweet.lower()\n    \n    splits = tweet.split()\n    splits = [word for word in splits if word not in set(nltk.corpus.stopwords.words('english'))]\n    tweet = ' '.join(splits)\n    \n    \n    return tweet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['cleaned_text']= train_data['tweets'].apply((lambda x: clean(x))) \ntest_data['cleaned_text'] = test_data['tweets'].apply((lambda x: clean(x)))\nprint(\"Cleaned\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA and Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(10,10)})\nsns.countplot(train_data['class'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nstopwords = nltk.corpus.stopwords.words('english')\n\nplt.figure(figsize=(12,6))\ntext = ' '.join(train_data.cleaned_text[train_data['class']=='regular'])\nwc = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc)\nplt.axis('off')\nplt.title('Regular Tweets',fontsize=25)\n\nplt.figure(figsize=(12,6))\ntext = ' '.join(train_data.cleaned_text[train_data['class']=='irony'])\nwc1 = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc1)\nplt.axis('off')\nplt.title('Irony Tweets',fontsize=25)\n\nplt.figure(figsize=(12,6))\ntext = ' '.join(train_data.cleaned_text[train_data['class']=='sarcasm'])\nwc2 = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc2)\nplt.axis('off')\nplt.title('Sarcasm Tweets',fontsize=25)\n\nplt.figure(figsize=(12,6))\ntext = ' '.join(train_data.cleaned_text[train_data['class']=='figurative'])\nwc3 = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc3)\nplt.axis('off')\nplt.title('Figurative Tweets',fontsize=25)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encode our text classes"},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_target(t_class):\n    t_class=str(t_class)\n    class_dict = {\n        'irony':0,\n        'sarcasm':1,\n        'regular':2,\n        'figurative':3\n    }\n    return class_dict[t_class]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[\"target\"] = train_data['class'].apply(lambda x: encode_target(x))\ntest_data[\"target\"] = test_data['class'].apply(lambda x: encode_target(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preparing our train and test sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_data[['cleaned_text','target']]\ntrain.columns = ['text','labels']\n\ntest = test_data[['cleaned_text','target']]\ntest.columns = ['text','labels']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traintexts=train['text'].tolist()\ntesttexts=test['text'].tolist()\n\nall_texts = traintexts + testtexts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Extraction"},{"metadata":{},"cell_type":"markdown","source":"### 1. Count Vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\ncount_vect.fit(all_texts)\n\n# transform the training and test data using count vectorizer object\nxtrain_count =  count_vect.transform(train['text'])\nxtest_count =  count_vect.transform(test['text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Word level tf-idf"},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=55000)\ntfidf_vect.fit(all_texts)\nxtrain_tfidf =  tfidf_vect.transform(train['text'])\nxtest_tfidf =  tfidf_vect.transform(test['text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Hashing Vectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"hash_vectorizer = HashingVectorizer(n_features=55000)\nhash_vectorizer.fit(all_texts)\nxtrain_hash_vectorizer =  hash_vectorizer.transform(train['text']) \nxtest_hash_vectorizer =  hash_vectorizer.transform(test['text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(classifier, feature_vector_train, label, feature_vector_test, test_y):\n    # fit the training dataset on the classifier\n    classifier.fit(feature_vector_train, label)\n    \n    # predict the labels on validation dataset\n    predictions = classifier.predict(feature_vector_test)\n    \n    return metrics.accuracy_score(predictions, test_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Naive Bayes Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"NAIVE BAYES CLASSIFIER\")\nprint(\"========================================================\")\n# Naive Bayes on Count Vectors\naccuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train['labels'], xtest_count, test['labels'])\nprint(\"-> Count Vectors Accuracy: {}% \".format(round(accuracy,4)*100))\n\n# Naive Bayes on Word Level TF IDF Vectors\naccuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train['labels'], xtest_tfidf, test['labels'])\nprint(\"-> WordLevel TF-IDF Accuracy: {}%\".format(round(accuracy,4)*100))\n\n# Naive Bayes on Hash Vectors\n# accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_hash_vectorizer, train['labels'], xtest_hash_vectorizer, test['labels'])\n# print(\"-> Hash Vectors Accuracy: \", round(accuracy,2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"LINEAR CLASSIFIER\")\nprint(\"========================================================\")\n# Linear Classifier on Count Vectors\naccuracy = train_model(linear_model.LogisticRegression(solver=\"lbfgs\",multi_class=\"auto\",max_iter=13000, n_jobs=2), xtrain_count, train['labels'], xtest_count, test['labels'])\nprint(\"-> Count Vectors Accuracy: {}% \".format(round(accuracy,4)*100))\n\n# Linear Classifier on Word Level TF IDF Vectors\naccuracy = train_model(linear_model.LogisticRegression(solver=\"lbfgs\",multi_class=\"auto\",max_iter=13000, n_jobs=2), xtrain_tfidf, train['labels'], xtest_tfidf, test['labels'])\nprint(\"-> WordLevel TF-IDF Accuracy: {}%\".format(round(accuracy,4)*100))\n\n# Linear Classifier on Hash Vectors\naccuracy = train_model(linear_model.LogisticRegression(solver=\"lbfgs\",multi_class=\"auto\",max_iter=13000, n_jobs=2), xtrain_hash_vectorizer, train['labels'], xtest_hash_vectorizer, test['labels'])\nprint(\"-> Hash Vectors Accuracy: {}%\".format(round(accuracy,4)*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(\"XGBOOST CLASSIFIER\")\nprint(\"========================================================\")\n# Linear Classifier on Count Vectors\naccuracy = train_model(xgboost.XGBClassifier(), xtrain_count, train['labels'], xtest_count, test['labels'])\nprint(\"-> Count Vectors Accuracy: {}% \".format(round(accuracy,4)*100))\n\n# Linear Classifier on Word Level TF IDF Vectors\naccuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf, train['labels'], xtest_tfidf, test['labels'])\nprint(\"-> WordLevel TF-IDF Accuracy: {}%\".format(round(accuracy,4)*100))\n\n# Linear Classifier on Hash Vectors\naccuracy = train_model(xgboost.XGBClassifier(), xtrain_hash_vectorizer, train['labels'], xtest_hash_vectorizer, test['labels'])\nprint(\"-> Hash Vectors Accuracy: {}%\".format(round(accuracy,4)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"KNN CLASSIFIER\")\nprint(\"========================================================\")\n# Linear Classifier on Count Vectors\naccuracy = train_model(KNeighborsClassifier(n_neighbors = 80, n_jobs=4) , xtrain_count, train['labels'], xtest_count, test['labels'])\nprint(\"-> Count Vectors Accuracy: {}% \".format(round(accuracy,4)*100))\n\n# Linear Classifier on Word Level TF IDF Vectors\naccuracy = train_model(KNeighborsClassifier(n_neighbors = 80, n_jobs=4) , xtrain_tfidf, train['labels'], xtest_tfidf, test['labels'])\nprint(\"-> WordLevel TF-IDF Accuracy: {}%\".format(round(accuracy,4)*100))\n\n# Linear Classifier on Hash Vectors\naccuracy = train_model(KNeighborsClassifier(n_neighbors = 80, n_jobs=4) , xtrain_hash_vectorizer, train['labels'], xtest_hash_vectorizer, test['labels'])\nprint(\"-> Hash Vectors Accuracy: {}%\".format(round(accuracy,4)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"SGD CLASSIFIER\")\nprint(\"========================================================\")\n# Linear Classifier on Count Vectors\naccuracy = train_model(SGDClassifier(max_iter=500, n_jobs=4) , xtrain_count, train['labels'], xtest_count, test['labels'])\nprint(\"-> Count Vectors Accuracy: {}% \".format(round(accuracy,4)*100))\n\n# Linear Classifier on Word Level TF IDF Vectors\naccuracy = train_model(SGDClassifier(max_iter=500, n_jobs=4) , xtrain_tfidf, train['labels'], xtest_tfidf, test['labels'])\nprint(\"-> WordLevel TF-IDF Accuracy: {}%\".format(round(accuracy,4)*100))\n\n# Linear Classifier on Hash Vectors\naccuracy = train_model(SGDClassifier(max_iter=500, n_jobs=4) , xtrain_hash_vectorizer, train['labels'], xtest_hash_vectorizer, test['labels'])\nprint(\"-> Hash Vectors Accuracy: {}%\".format(round(accuracy,4)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"LINEAR SVC CLASSIFIER\")\nprint(\"========================================================\")\n# Linear Classifier on Count Vectors\naccuracy = train_model(LinearSVC(max_iter=3500) , xtrain_count, train['labels'], xtest_count, test['labels'])\nprint(\"-> Count Vectors Accuracy: {}% \".format(round(accuracy,4)*100))\n\n# Linear Classifier on Word Level TF IDF Vectors\naccuracy = train_model(LinearSVC(max_iter=3500) , xtrain_tfidf, train['labels'], xtest_tfidf, test['labels'])\nprint(\"-> WordLevel TF-IDF Accuracy: {}%\".format(round(accuracy,4)*100))\n\n# Linear Classifier on Hash Vectors\naccuracy = train_model(LinearSVC(max_iter=3500) , xtrain_hash_vectorizer, train['labels'], xtest_hash_vectorizer, test['labels'])\nprint(\"-> Hash Vectors Accuracy: {}%\".format(round(accuracy,4)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}