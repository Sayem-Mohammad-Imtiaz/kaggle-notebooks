{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns  # visualization tool\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"happy = pd.read_csv('../input/world-happiness-report-2019.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"happy.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"happy.corr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#correlation map\nf,ax = plt.subplots(figsize=(12, 12))\nsns.heatmap(happy.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"happy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"happy.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"happy.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"happy.columns = [each.split()[0] + \"_\" + each.split()[1] if (len(each.split())>1) else each for each in happy.columns]\nhappy.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Line Plot\n# color = color, label = label, linewidth = width of line, alpha = opacity, grid = grid, linestyle = sytle of line\nhappy.Freedom.plot(kind = 'line', color = 'g',label = 'Speed',linewidth=1,alpha = 0.5,grid = True,linestyle = ':')\nhappy.Generosity.plot(color = 'r',label = 'Defense',linewidth=1, alpha = 0.5,grid = True,linestyle = '-.')\nplt.legend(loc='upper right')     # legend = puts label into plot\nplt.xlabel('x axis')              # label = name of label\nplt.ylabel('y axis')\nplt.title('Line Plot')            # title = title of plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scatter Plot \n# x = attack, y = defense\nhappy.plot(kind='scatter', x='Ladder', y='Log_of',alpha = 0.5,color = 'red')\n#plt.scatter(happy.Ladder, happy.Log_of, color = 'red')\nplt.xlabel('Ladder')              # label = name of label\nplt.ylabel('Log_of_GDP\\nper_capita')\nplt.title('Ladder & Log_of_GDP\\nper_capita Scatter Plot')            # title = title of plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Histogram\n# bins = number of bar in figure\nhappy.Corruption.plot(kind = 'hist',bins = 50,figsize = (12,12))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clf() = cleans it up again you can start a fresh\nhappy.Corruption.plot(kind = 'hist',bins = 50)\nplt.clf()\n# We cannot see plot due to clf()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create dictionary and look its keys and values\ndictionary = {'spain' : 'madrid','usa' : 'vegas'}\nprint(dictionary.keys())\nprint(dictionary.values())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Keys have to be immutable objects like string, boolean, float, integer or tubles\n# List is not immutable\n# Keys are unique\ndictionary['spain'] = \"barcelona\"    # update existing entry\nprint(dictionary)\ndictionary['france'] = \"paris\"       # Add new entry\nprint(dictionary)\ndel dictionary['spain']              # remove entry with key 'spain'\nprint(dictionary)\nprint('france' in dictionary, 'paris' in dictionary)        # check include or not\ndictionary.clear()                   # remove all entries in dict\nprint(dictionary)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# In order to run all code you need to take comment this line\ndel dictionary         # delete entire dictionary     \n#print(dictionary)       # it gives error because dictionary is deleted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"happy = pd.read_csv('../input/world-happiness-report-2019.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"series = happy['Corruption']        # happy['Corruption'] = series\nprint(type(series))\n#seriler vektör şeklinde uzanan tek boyutlu yapılardır\ndataFrame = happy[['Corruption']]  # happy[['Corruption']] = data frame\nprint(type(dataFrame))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Comparison operator\nprint(3 > 2)\nprint(3!=2)\n# Boolean operators\nprint(True and False)\nprint(True or False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1 - Filtering Pandas data frame\nx = happy['Corruption']> 140     # There are only 8 countries who have higher Corruption value than 140\nhappy[x] # sadece x değeri (> 140) True olanlarını yazdırır","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2 - Filtering pandas with logical_and\n# There are only 5 countries who have higher Corruption value than 14o and higher Freedom value than 100\nhappy[np.logical_and(happy['Corruption']>140, happy['Freedom']>100)]\n#happy[(happy['Corruption'] > 140) & (happy['Freedom'] > 100)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stay in loop if condition( i is not equal 5) is true\ni = 0\nwhile i != 5 :\n    print('i is: ',i)\n    i +=1 \nprint(i,' is equal to 5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stay in loop if condition( i is not equal 5) is true\nlis = [1,2,3,4,5]\nfor i in lis:\n    print('i is: ',i)\nprint('')\n\n# Enumerate index and value of list\n# index : value = 0:1, 1:2, 2:3, 3:4, 4:5\nfor index, value in enumerate(lis):\n    print(index,\" : \",value)\nprint('')   \n\n# For dictionaries\n# We can use for loop to achive key and value of dictionary. We learnt key and value at dictionary part.\ndictionary = {'spain':'madrid','france':'paris'}\nfor key,value in dictionary.items():\n    print(key,\" : \",value)\nprint('')\n\n# For pandas we can achieve index and value\nfor index,value in happy[['Corruption']][0:1].iterrows():\n    print(index,\" : \",value)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tuble_ex():\n    \"\"\" return defined t tuble\"\"\"\n    t = (1,2,3) #this lines PACKS values into variable t\n    return t\na,b,c = tuble_ex() # this lines UNPACKS values of variable t\nprint(a,b,c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scope \n# guess print what\nx = 2\ndef f():\n    x = 3\n    return x\nprint(x)      # x = 2 global scope\nprint(f())    # x = 3 local scope\n\n# What if there is no local scope\nx = 5\ndef f():\n    y = 2*x        # there is no local scope x\n    return y\nprint(f())         # it uses global scope x\n# First local scopesearched, then global scope searched, \n# if two of them cannot be found lastly built in scope searched.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#nested function\ndef square():\n    \"\"\" return square of value \"\"\"\n    def add():\n        \"\"\" add two local variable \"\"\"\n        x = 2\n        y = 3\n        z = x + y\n        return z\n    return add()**2\nprint(square())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# default arguments\ndef f(a, b = 1, c = 2):\n    y = a + b + c\n    return y\nprint(\"with def arg: \", f(5))\n# what if we want to change default arguments\nprint(\"ovverriding def arg: \",f(5,4,3))\n\n# flexible arguments *args\ndef f(*args):\n    for i in args:\n        print(i)\nprint(\"\")\nf(\"one argument with *args: \", 1)\nprint(\"\")\nf(\"multiple arguments with *args: \", 1,2,3,4)\n\n# flexible arguments **kwargs that is dictionary\n\ndef f(**kwargs):\n    \"\"\" print key and value of dictionary\"\"\"\n    for i, j in kwargs.items():               # If you do not understand this part turn for loop part and look at dictionary in for loop\n        print(i, \" \", j)\nprint(\"dictionary arguments with **kwargs: \")\nf(country = 'spain', capital = 'madrid', population = 123456)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lambda function / Faster way of writing function\nsquare = lambda x: x**2     # where x is name of argument\nprint(square(4))\ntot = lambda x,y,z: x+y+z   # where x,y,z are names of arguments\nprint(tot(1,2,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ANONYMOUS FUNCTION\n#Like lambda function but it can take more than one arguments.\n# map(func,seq) : applies a function to all the items in a list\n\nnumber_list = [1,2,3]\ny = map(lambda x:x**2,number_list)\nprint(list(y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# iteration example\nname = \"ronaldo\"\nit = iter(name)\nprint(next(it))    # print next iteration\nprint(*it)         # print remaining iteration","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# zip example\nlist1 = [1,2,3,4]\nlist2 = [5,6,7,8]\nz = zip(list1,list2)\nprint(z)\nz_list = list(z)\nprint(z_list)\n\n#unzip\nun_zip = zip(*z_list)\nun_list1,un_list2 = list(un_zip) # unzip returns tuple\nprint(un_list1)\nprint(un_list2)\nprint(type(un_list2))\nprint(type(list(un_list1))) # converting un_list1 type from tuple to list ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> LIST COMPREHENSION\n\nWe use list comprehension for data analysis often. \nlist comprehension: collapse for loops for building lists into a single line \nEx: num1 = [1,2,3] and we want to make it num2 = [2,3,4]. This can be done with for loop. However it is unnecessarily long. We can make it one line code that is list comprehension."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example of list comprehension\nnum1 = [1,2,3]\nnum2 = [i + 1 for i in num1 ] # list comprehension\nprint(num2)\n\n\"\"\"\n[i + 1 for i in num1 ]: list of comprehension \ni +1: list comprehension syntax \nfor i in num1: for loop syntax \ni: iterator \nnum1: iterable object\n\"\"\"\n# Conditionals on iterable\nnum1 = [5,10,15]\nnum2 = [i**2 if i == 10 else i-5 if i < 7 else i+5 for i in num1]\nprint(num2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets return world-happiness-report-2019.csv and make one more list comprehension \n# example lets classify countries whether they have less or much freedom. \n# Our threshold is average freedom.\nthreshold = sum(happy[\"SD of Ladder\"])/len(happy[\"SD of Ladder\"])\nprint(\"threshold \", threshold)\nhappy[\"SD_level\"] = [\"much\" if i > threshold else \"less\" for i in happy[\"SD of Ladder\"]]\nhappy.loc[50:60,[\"SD_level\",\"SD of Ladder\"]] # we will learn loc more detailed later","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"EXPLORATORY DATA ANALYSIS\n\nvalue_counts(): Frequency counts \noutliers: the value that is considerably higher or lower from rest of the data\n\nLets say value at 75% is Q3 and value at 25% is Q1.\nOutlier are smaller than Q1 - 1.5(Q3-Q1) and bigger than Q3 + 1.5(Q3-Q1). (Q3-Q1) = IQR \nWe will use describe() method. Describe method includes:\ncount: number of entries\nmean: average of entries\nstd: standart deviation\nmin: minimum entry\n25%: first quantile\n50%: median or second quantile\n75%: third quantile\nmax: maximum entry\n\nWhat is quantile?\n\n1,4,5,6,8,9,11,12,13,14,15,16,17\n\nThe median is the number that is in middle of the sequence. In this case it would be 11.\n\nThe lower quartile is the median in between the smallest number and the median i.e. in between 1 and 11, which is 6.\n\nThe upper quartile, you find the median between the median and the largest number i.e. between 11 and 17, which will be 14 according to the question above."},{"metadata":{"trusted":true},"cell_type":"code","source":"# For example lets look frequency of Social Support\nprint(happy['Social support'].value_counts(dropna =False))  # if there are nan values that also be counted\n# this is not a suitable data frame, please consider more carefully while choosing your exercise data set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"happy.describe() #ignore null entries","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"happy.boxplot(column='SD of Ladder',by = 'Positive affect')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TIDY DATA\nhappy_new = happy.head()\nhappy_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets melt\n# id_vars = what we do not wish to melt\n# value_vars = what we want to melt\nmelted = pd.melt(frame=happy_new,id_vars = 'Country (region)', value_vars= ['Positive affect','Corruption'])\nmelted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PIVOTING DATA\n# Reverse of melting.\n# Index is name\n# I want to make that columns are variable\n# Finally values in columns are value\nmelted.pivot(index = 'Country (region)', columns = 'variable',values='value')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CONCATENATING DATA\n# Firstly lets create 2 data frame\ndata1 = happy.head()\ndata2= happy.tail()\nconc_data_row = pd.concat([data1,data2],axis =0,ignore_index =True) # axis = 0 : adds dataframes in row\nconc_data_row","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"happy1 = happy['SD of Ladder'].head()\nhappy2= happy['Social support'].head()\nconc_happy_col = pd.concat([happy1,happy2],axis =1) # axis = 0 : adds dataframes in row\nconc_happy_col","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"DATA TYPES\n\nThere are 5 basic data types: object(string), boolean, integer, float and categorical. \nWe can make conversion data types like from str to categorical or from int to float. \n\nWhy is category important:\n* make dataframe smaller in memory\n* can be utilized for analysis especially for sklearn (we will learn later)"},{"metadata":{"trusted":true},"cell_type":"code","source":"happy.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets convert object(str) to categorical and int to float.\nhappy['Country (region)'] = happy['Country (region)'].astype('category')\nhappy['SD of Ladder'] = happy['SD of Ladder'].astype('float')\n\nhappy.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MISSING DATA and TESTING WITH ASSERT\n\nIf we encounter with missing data, what we can do:\n\n* leave as is\n* drop them with dropna()\n* fill missing value with fillna()\n* fill missing values with test statistics like mean \n\nAssert statement: check that you can turn on or turn off when you are done with your testing of the program"},{"metadata":{"trusted":true},"cell_type":"code","source":"happy.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets check Type 2\nhappy[\"Corruption\"].value_counts(dropna = False) # show the number of NaN values\n# As you can see, there are 8 NAN value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets drop nan values\nhappy1 = happy   # also we will use data to fill missing value so I assign it to data1 variable\nhappy1[\"Corruption\"].dropna(inplace = True)  # inplace = True means we do not assign it to new variable. \n# Changes automatically assigned to data\n# So does it work ?\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets check with assert statement\n# Assert statement:\nassert 1==1 # return nothing because it is true\n\n# In order to run all code, we need to make this line comment\n# assert 1==2 # return error because it is false\n\nassert happy['Corruption'].notnull().all() # returns nothing because we drop nan values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"happy[\"Corruption\"].fillna('empty',inplace = True)\n\nassert happy['Corruption'].notnull().all() # returns nothing because we do not have nan values\n\n# With assert statement we can check a lot of thing. For example\nassert happy.columns[0] == 'Country (region)'\nassert happy.Ladder.dtypes == np.int","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# BUILDING DATA FRAMES FROM SCRATCH\n\n# data frames from dictionary\ncountry = [\"Spain\",\"France\"]\npopulation = [\"11\",\"12\"]\nlist_label = [\"country\",\"population\"]\nlist_col = [country,population]\nzipped = list(zip(list_label,list_col))\nzipped\ndata_dict = dict(zipped)\ndf = pd.DataFrame(data_dict)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add new columns\ndf[\"capital\"] = [\"madrid\",\"paris\"] # Broadcasting: Create new column and assign different value to entire column\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Broadcasting\ndf[\"income\"] = 0 #Broadcasting entire column with same value\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"VISUAL EXPLORATORY DATA ANALYSIS\n\nPlot\n\nSubplot\n\nHistogram:\n* bins: number of bins\n* range(tuble): min and max values of bins\n* normed(boolean): normalize or not\n* cumulative(boolean): compute cumulative distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting all data \nhappy1 = happy.loc[:,[\"Ladder\",\"Freedom\",\"Corruption\"]]\nhappy1.plot()\n# it is confusing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# subplots\nhappy1.plot(subplots = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scatter plot  \nhappy1.plot(kind = \"scatter\", x = \"Freedom\", y = \"Corruption\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hist plot\nhappy1.plot(kind = \"hist\",y = \"Freedom\",bins = 50,range= (0,160),normed = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# histogram subplot with non cumulative and cumulative\nfig, axes = plt.subplots(nrows=2,ncols=1)\nhappy1.plot(kind = \"hist\",y = \"Freedom\",bins = 50,range= (0,160),normed = True,ax = axes[0])\nhappy1.plot(kind = \"hist\",y = \"Freedom\",bins = 50,range= (0,160),normed = True,ax = axes[1],cumulative = True)\nplt.savefig('graph.png')\nplt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"happy.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Indexing Pandas Time Series\ntime_list = [\"1992-03-08\",\"1992-04-12\"]\nprint(type(time_list[1])) # As you can see date is string\n# however we want it to be datetime object\ndatetime_object = pd.to_datetime(time_list)\nprint(type(datetime_object))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# close warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# In order to practice lets take head of happiness data and add it a time list\nhappy2 = happy.head()\ndate_list = [\"1992-01-10\",\"1992-02-10\",\"1992-03-10\",\"1993-03-15\",\"1993-03-16\"]\ndatetime_object = pd.to_datetime(date_list)\nhappy2[\"date\"] = datetime_object\n# lets make date as index\nhappy2= happy2.set_index(\"date\")\nhappy2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we can select according to our date index\nprint(happy2.loc[\"1993-03-16\"])\nprint(happy2.loc[\"1992-03-10\":\"1993-03-16\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"RESAMPLING PANDAS TIME SERIES\n\n* Resampling: statistical method over different time intervals\n* Needs string to specify frequency like \"M\" = month or \"A\" = year\n* Downsampling: reduce date time rows to slower frequency like from daily to weekly\n* Upsampling: increase date time rows to faster frequency like from daily to hourly\n* Interpolate: Interpolate values according to different methods like ‘linear’, ‘time’ or index’\nhttps://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.interpolate.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will use happy2 that we create at previous part\nhappy2.resample(\"A\").mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets resample with month\nhappy2.resample(\"M\").mean()\n# As you can see there are a lot of nan because happy2 does not include all months","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# In real life (data is real. Not created from us like happy2) we can solve this problem with interpolate\n# We can interpolete from first value\nhappy2.resample(\"M\").first().interpolate(\"linear\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Or we can interpolate with mean()\nhappy2.resample(\"M\").mean().interpolate(\"linear\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nINDEXING DATA FRAMES\n\n* Indexing using square brackets\n* Using column attribute and row label\n* Using loc accessor\n* Selecting only some columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# read data\nhappy = pd.read_csv('../input/world-happiness-report-2019.csv')\n#happy.head()\nhappy = happy.set_index(\"Ladder\")\nhappy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# indexing using square brackets\nhappy[\"Freedom\"][1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using column attribute and row label\nhappy.Freedom[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using loc accessor\nhappy.loc[1,[\"Freedom\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**MANIPULATING DATA FRAMES WITH PANDAS**\n\nINDEXING DATA FRAMES\n* Indexing using square brackets\n* Using column attribute and row label\n* Using loc accessor\n* Selecting only some columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# read data\nhappy = pd.read_csv('../input/world-happiness-report-2019.csv')\nhappy['#'] = range(1, len(happy) + 1)\nhappy = happy.set_index(\"#\")\nhappy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# indexing using square brackets\nhappy['Generosity'][2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using column attribute and row label\nhappy.Generosity[2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using loc accessor\nhappy.loc[1,[\"Generosity\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting only some columns\nhappy[[\"Country (region)\", \"Generosity\", \"Negative affect\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SLICING DATA FRAME\n\n*  Difference between selecting columns\n    * Series and data frames\n* Slicing and indexing series\n* Reverse slicing\n* From something to end"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Difference between selecting columns: series and dataframes\nprint(type(happy[\"Generosity\"]))     # series\nprint(type(happy[[\"Generosity\"]]))   # data frames","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Slicing and indexing series\nhappy.loc[1:10,\"Country (region)\":\"Positive affect\"]   # 10 and \"Positive affect\" are inclusive","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reverse slicing \nhappy.loc[10:1:-1,\"Country (region)\":\"Positive affect\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# From something to end\nhappy.loc[1:10,\"Freedom\":] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**FILTERING DATA FRAMES**\n\n* Creating boolean series Combining filters Filtering column based others"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating boolean series\nboolean = happy.Corruption > 140\nhappy[boolean]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combining filters\nfirst_filter = happy.Corruption > 140\nsecond_filter = happy.Generosity > 80\nhappy[first_filter & second_filter]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filtering column based others\nhappy.Corruption[happy.Generosity>150]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**TRANSFORMING DATA**\n\n* Plain python functions\n* Lambda function: to apply arbitrary python function to every element\n* Defining column using other columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plain python functions\n# all the values at the Corruption column divided by 2\ndef div(n):\n    return n/2\nhappy.Corruption.apply(div)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Or we can use lambda function\nhappy.Corruption.apply(lambda n : n/2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining column using other columns\nhappy[\"average_affect\"] = happy[\"Positive affect\"] - happy[\"Negative affect\"]\nhappy.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**INDEX OBJECTS AND LABELED DATA**\n\n* index: sequence of label"},{"metadata":{"trusted":true},"cell_type":"code","source":"# our index name is this:\nprint(happy.index.name)\n# lets change it\nhappy.index.name = \"index_name\"\nhappy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Overwrite index\n# if we want to modify index we need to change all of them.\nhappy.head()\n# first copy of our data to data3 then change index \nhappy3 = happy.copy()\n# lets make index start from 100. It is not remarkable change but it is just example\nhappy3.index = range(10,166,1)\nhappy3.head()\n\n# We can make one of the column as index. I actually did it at the beginning of manipulating data frames with pandas section\n# It was like this\n# data= data.set_index(\"#\")\n# also you can use \n# data.index = data[\"#\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**HIERARCHICAL INDEXING**\n\n* Setting indexing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets read data frame one more time to start from beginning\nhappy = pd.read_csv('../input/world-happiness-report-2019.csv')\nhappy.head()\n# As you can see there is index. However we want to set one or \n# more column to be index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting index : Country (region) is outer Negative affect is inner index\nhappy1 = happy.set_index([\"Country (region)\",\"Negative affect\"]) \nhappy1.head(10)\n# happy1.loc[\"Freedom\",\"Corruption\"] # how to use indexes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n**PIVOTING DATA FRAMES\n**\n* pivoting: reshape tool"},{"metadata":{"trusted":true},"cell_type":"code","source":"dic = {\"treatment\":[\"A\",\"A\",\"B\",\"B\"],\"gender\":[\"F\",\"M\",\"F\",\"M\"],\"response\":[10,45,5,9],\"age\":[15,4,72,65]}\ndf = pd.DataFrame(dic)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pivoting\ndf.pivot(index=\"treatment\",columns = \"gender\",values=\"response\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**STACKING and UNSTACKING DATAFRAME**\n\n* deal with multi label indexes\n* level: position of unstacked index\n* swaplevel: change inner and outer level index position"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df.set_index([\"treatment\",\"gender\"])\ndf1\n# lets unstack it","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# level determines indexes\ndf1.unstack(level=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.unstack(level=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# change inner and outer level index position\ndf2 = df1.swaplevel(0,1)\ndf2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n**MELTING DATA FRAMES**\n\n* Reverse of pivotin"},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df.pivot(index=\"treatment\",columns = \"gender\",values=\"response\")\npd.melt(df,id_vars=\"treatment\",value_vars=[\"age\",\"response\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**CATEGORICALS AND GROUPBY**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will use df\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# according to treatment take means of other features\ndf.groupby(\"treatment\").mean()   # mean is aggregation / reduction method\n# there are other methods like sum, std,max or min","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can only choose one of the feature\ndf.groupby(\"treatment\").age.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Or we can choose multiple features\ndf.groupby(\"treatment\")[[\"age\",\"response\"]].min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()\n# as you can see gender is object\n# However if we use groupby, we can convert it categorical data. \n# Because categorical data uses less memory, speed up operations like groupby\n#df[\"gender\"] = df[\"gender\"].astype(\"category\")\n#df[\"treatment\"] = df[\"treatment\"].astype(\"category\")\n#df.info()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}