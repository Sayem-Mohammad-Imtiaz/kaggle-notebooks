{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"cell_type":"markdown","source":"<h1><center><font size=\"6\">Chinese MNIST Exploratory Data Analysis</font></center></h1>\n\n<center><img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F769452%2Ffae77a81c057fe419de60f5e2b20be25%2Fchinese_mnist_profile_small.png?generation=1596963542354014&alt=media\"></img></center>\n \n \n\n# <a id='0'>Content</a>\n\n- <a href='#1'>Introduction</a>  \n- <a href='#2'>Prepare the data analysis</a>  \n - <a href='#21'>Load packages</a>  \n - <a href='#21'>Load the data</a>  \n - <a href='#21'>Preprocessing data</a>  \n- <a href='#3'>Data exploration</a>   \n - <a href='#31'>Check for missing data</a>  \n - <a href='#32'>Explore image data</a>  \n - <a href='#33'>Suits, samples, characters distribution</a>  \n- <a href='#4'>Conclusions</a>      "},{"metadata":{"_uuid":"a8e77ace65f04c89a878bf18249e4d8e23fec996"},"cell_type":"markdown","source":"# <a id='1'>Introduction</a>  \n\n\nIn this Kernel, we will explore a dataset with adnotated images of Chinese numbers, handwritten by a number of 100 volunteers, each providing a number of 10 samples, each sample with a complete set of 15 Chinese characters for numbers.\n\nThe Chinese characters are the following:\n* 零 - for 0  \n* 一 - for 1\n* 二 - for 2  \n* 三 - for 3  \n* 四 - for 4  \n* 五 - for 5  \n* 六 - for 6  \n* 七 - for 7  \n* 八 - for 8  \n* 九 - for 9  \n* 十 - for 10\n* 百 - for 100\n* 千 - for 1000\n* 万 - for 10 thousands\n* 亿 - for 100 millions\n\n\nThe objective of the Kernel is to take us through the first steps of a machine learning analysis. We start by preparing the analysis (load the libraries and the data), continue with an Exploratory Data Analysis (EDA) where we highlight various data features, spending some time to try to understand the data.\n\nThe first step is to prepare the data analysis.\n\n<a href=\"#0\"><font size=\"1\">Go to top</font></a>  "},{"metadata":{"_uuid":"4e97555eb77978a29a51c41f39cec67136b18157"},"cell_type":"markdown","source":"# <a id='2'>Prepare the data analysis</a>   \n\n\nBefore starting the analysis, we need to make few preparation: load the packages, load and inspect the data.\n\n"},{"metadata":{"_uuid":"cb2e73fe056a3dda7eb48eeac2facf0c441816d1"},"cell_type":"markdown","source":"# <a id='21'>Load packages</a>\n\nWe load the packages used for the analysis.\n"},{"metadata":{"_kg_hide-input":true,"_uuid":"af08260bfbe163f9132f39d09627899bbc4c1dae","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport sys\nimport os\nimport cv2 as cv\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport skimage\nimport skimage.io","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also set the image path."},{"metadata":{"_kg_hide-input":true,"_uuid":"a2082fb1e56fc6cfc91d40820b905267bc1ca468","trusted":true},"cell_type":"code","source":"IMAGE_PATH = '..//input//chinese-mnist//data//data//'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"307f656565365ff05faf226e5a447875dd0dfead"},"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n\n# <a id='22'>Load the data</a>  \n\nLet's see first what data files do we have in the root directory."},{"metadata":{"_kg_hide-input":true,"_uuid":"9f1df6658b17558179d8a9016f544410de16c354","trusted":true},"cell_type":"code","source":"os.listdir(\"..//input//chinese-mnist\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"241b8735a85a25e16421fda8c35bc3d3c69e7ea8"},"cell_type":"markdown","source":"There is a dataset file and a folder with images.  \n\nLet's load the dataset file first."},{"metadata":{"_kg_hide-input":true,"_uuid":"d7b9f11a014428e56e422d97a5b3ef70efec007e","trusted":true},"cell_type":"code","source":"data_df=pd.read_csv('..//input//chinese-mnist//chinese_mnist.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22b3984ccc3e29daaf77a796d9d7966cd798e1a8"},"cell_type":"markdown","source":"Let's glimpse the data. First, let's check the number of columns and rows."},{"metadata":{"_kg_hide-input":true,"_uuid":"535f3f9cea3b26428bec3ede4ed49009bdb91889","trusted":true},"cell_type":"code","source":"data_df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b4405ddcce03ee722f05234d508188997817f8d"},"cell_type":"markdown","source":"There are 15000 rows and 5 columns. Let's look to the data."},{"metadata":{"_kg_hide-input":true,"_uuid":"4d326f747f0a14580b20c2e034e6c3368edcd18b","trusted":true},"cell_type":"code","source":"data_df.sample(100).head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c97047b17cda76e346e444229485ac91ec966423"},"cell_type":"markdown","source":"The data contains the following values:  \n\n* suite_id - each suite corresponds to a set of handwritten samples by one volunteer;  \n* sample_id - each sample wil contain a complete set of 15 characters for Chinese numbers;\n* code - for each Chinese character we are using a code, with values from 1 to 15;\n* value - this is the actual numerical value associated with the Chinese character for number;  \n* character - the Chinese character;  \n\nWe index the files in the dataset by forming a file name from suite_id, sample_id and code. The pattern for a file is as following:\n\n> \"input_{suite_id}_{sample_id}_{code}.jpg\""},{"metadata":{"_uuid":"55dd26f919decca9d67daec9895a5d9e11f1d28b"},"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n# <a id='3'>Data exploration</a>  \n\n\n\nLet's start by checking if there are missing data, unlabeled data or data that is inconsistently labeled. \n"},{"metadata":{"_uuid":"14443450ba96e12ad8e18ce4dd1779f18d5f914b"},"cell_type":"markdown","source":"## <a id='31'>Check for missing data</a>  \n\nLet's create a function that check for missing data in the dataset."},{"metadata":{"_kg_hide-input":true,"_uuid":"4544dd470d743c54f815faaee863038ad5e8398f","trusted":true},"cell_type":"code","source":"def missing_data(data):\n    total = data.isnull().sum().sort_values(ascending = False)\n    percent = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False)\n    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data(data_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3cb0410e8b9afd75ac7b50d0489d90eda6e1b109"},"cell_type":"markdown","source":"There is no missing (null) data in the dataset. Still it might be that some of the data labels are misspelled; we will check this when we will analyze each data feature."},{"metadata":{"_uuid":"1fbab44688fb2ab073aac8f964e534f90ce1dfff"},"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n## <a id='32'>Explore image data</a>  \n\nLet's also check the image data. First, we check how many images are stored in the image folder."},{"metadata":{"_kg_hide-input":true,"_uuid":"46f15681887fa82ab13224e52df69d91119fc9ad","trusted":true},"cell_type":"code","source":"image_files = list(os.listdir(IMAGE_PATH))\nprint(\"Number of image files: {}\".format(len(image_files)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68523860593e9a64059b51d40a316454e6937a68"},"cell_type":"markdown","source":"Let's also check that each line in the dataset has a corresponding image in the image list.  \nFirst, we will have to compose the name of the file from the indexes."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def create_file_name(x):\n    file_name = f\"input_{x[0]}_{x[1]}_{x[2]}.jpg\"\n    return file_name","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"data_df[\"file\"] = data_df.apply(create_file_name, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"data_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"457cd17212904bb96f86ec1770cbdbefc5ffb395","trusted":true},"cell_type":"code","source":"file_names = list(data_df['file'])\nprint(\"Matching image names: {}\".format(len(set(file_names).intersection(image_files))))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b20cd791ede3f23d0c9275aafc75827b9424df4"},"cell_type":"markdown","source":"Let's also check the image sizes."},{"metadata":{"_kg_hide-input":true,"_uuid":"64f4416a8e20197d60f7dbc9dd41a5e73049bfd0","trusted":true},"cell_type":"code","source":"def read_image_sizes(file_name):\n    image = skimage.io.imread(IMAGE_PATH + file_name)\n    return list(image.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c72a7d125efb51e00a58554692dbd99adc74b55","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"m = np.stack(data_df['file'].apply(read_image_sizes))\ndf = pd.DataFrame(m,columns=['w','h'])\ndata_df = pd.concat([data_df,df],axis=1, sort=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the distribution of images width and height."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Images widths #: {data_df.w.nunique()},  heights #: {data_df.h.nunique()}\")\nprint(f\"Images widths values: {data_df.w.unique()},  heights values: {data_df.h.unique()}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's also glimpse the dataframe with the new columns."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2b88af0c239ca3d9e37e159889836a4f38913c8"},"cell_type":"markdown","source":"## <a id='33'>Suites, Samples, Characters distribution</a>  \n\nLet's check the suites of the images. For this, we will group by `suite`."},{"metadata":{"_kg_hide-input":true,"_uuid":"6f1c39d0398275215f92f61542544132a0d574a0","trusted":true},"cell_type":"code","source":"print(f\"Number of suites: {data_df.suite_id.nunique()}\")\nprint(f\"Samples: {data_df.sample_id.nunique()}: {list(data_df.sample_id.unique())}\")\nprint(f\"Characters codes: {data_df.code.nunique()}: {list(data_df.code.unique())}\")\nprint(f\"Characters: {data_df.character.nunique()}: {list(data_df.character.unique())}\")\nprint(f\"Numbers: {data_df.value.nunique()}: {list(data_df.value.unique())}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd421a7d1872af204c26588d1a15eaddca08a396"},"cell_type":"markdown","source":"We have 100 suites, each with 10 samples."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def plot_count(feature, title, df, size=1):\n    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n    total = float(len(df))\n    g = sns.countplot(df[feature], order = df[feature].value_counts().index[:20], palette='Set2')\n    g.set_title(\"Number and percentage of {}\".format(title))\n    if(size > 2):\n        plt.xticks(rotation=90, size=8)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height/total),\n                ha=\"center\") \n    plt.show()  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plot_count(\"code\", \"character code\", data_df, size=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plot_count(\"value\", \"number value\", data_df, size=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"frequence of each character:\")\ndata_df.character.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def show_images(df, isTest=False):\n    f, ax = plt.subplots(10,15, figsize=(15,10))\n    for i,idx in enumerate(df.index):\n        dd = df.iloc[idx]\n        image_name = dd['file']\n        image_path = os.path.join(IMAGE_PATH, image_name)\n        img_data = cv.imread(image_path)\n        ax[i//15, i%15].imshow(img_data)\n        ax[i//15, i%15].axis('off')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We show here the samples drawn by volunteer number 1."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df = data_df.loc[data_df.suite_id==1].sort_values(by=[\"sample_id\",\"value\"]).reset_index()\nshow_images(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And here are the samples drawn by volunteer number 37."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df = data_df.loc[data_df.suite_id==37].sort_values(by=[\"sample_id\",\"value\"]).reset_index()\nshow_images(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For volunteer number 75:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df = data_df.loc[data_df.suite_id==75].sort_values(by=[\"sample_id\",\"value\"]).reset_index()\nshow_images(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look now to a selection of writings for number 0."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df = data_df.loc[data_df.code==1].sample(150).reset_index()\nshow_images(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see now a collection of writings for number 4."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df = data_df.loc[data_df.code==5].sample(150).reset_index()\nshow_images(df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd6bd3fba3f98e9775dac4f313371af4701febf3"},"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n# <a id='6'>Conclusions</a>  \n\nWe analyzed the dataset, focusing on understanding the data distribution. In the next Notebooks, we will see how we can use this data to train a model to classify new images by character (number value, code or an echivalent label associated).\n\n\n<a href=\"#0\"><font size=\"1\">Go to top</font></a>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}