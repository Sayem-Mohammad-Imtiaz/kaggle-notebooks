{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. EDA","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('../input/used-car-dataset-ford-and-mercedes/skoda.csv')\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"model\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.hist(bins=20, figsize=(20,15));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Train/test sample & stratification","metadata":{}},{"cell_type":"code","source":"train_set, test_set = train_test_split(data, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"tax_cat\"] = pd.cut(data[\"tax\"], bins=[-1, 65, 130, 195, 260, np.inf], labels=[1, 2, 3, 4, 5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"tax_cat\"].hist();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(data, data[\"tax_cat\"]):\n    strat_train_set = data.loc[train_index]\n    strat_test_set = data.loc[test_index]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"strat_test_set[\"tax_cat\"].value_counts() / len(strat_test_set)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for set_ in (strat_train_set, strat_test_set):\n    set_.drop(\"tax_cat\", axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Visualisation","metadata":{}},{"cell_type":"code","source":"data_train = strat_train_set.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\n\nsns.catplot(kind=\"bar\", x=\"model\", y=\"mileage\", data=data_train, alpha=.25, height=8, palette=\"bright\")\nplt.xticks(rotation=70)\nplt.tight_layout();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Correlations","metadata":{}},{"cell_type":"code","source":"corr_matrix = data_train.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_matrix[\"price\"].sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pandas.plotting import scatter_matrix\n\nattributes = [\"price\", \"year\", \"engineSize\", \"tax\", \"mpg\", \"mileage\"]\nscatter_matrix(data_train[attributes], figsize=(16,10));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train.plot(kind=\"scatter\", x=\"year\", y=\"price\", alpha=0.1);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train[\"miles_per_year\"] = data_train.mileage / (2021 - data_train.year)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_matrix = data_train.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_matrix[\"price\"].sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train.plot(kind=\"scatter\", x=\"miles_per_year\", y=\"price\", alpha=0.1);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Data Cleaning","metadata":{}},{"cell_type":"code","source":"data_train = strat_train_set.drop(\"price\", axis=1)\ndata_train_labels = strat_train_set[\"price\"].copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dataset complete","metadata":{}},{"cell_type":"markdown","source":"### 5.1 Creating numbers for categories","metadata":{}},{"cell_type":"code","source":"data_train_cat = data_train[[\"model\", \"transmission\", \"fuelType\"]]\ndata_train_cat.sample(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\n\nordinal_encoder = OrdinalEncoder()\ndata_train_cat_encoded = ordinal_encoder.fit_transform(data_train_cat)\ndata_train_cat_encoded[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ordinal_encoder.categories_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.2 Use OneHotEncoding","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\ncat_encoder = OneHotEncoder()\ndata_train_cat_1hot = cat_encoder.fit_transform(data_train_cat)\ndata_train_cat_1hot","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train_cat_1hot.toarray()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.3 Using a pipeline","metadata":{}},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\n\ncat_attribs = [\"model\", \"transmission\", \"fuelType\"]\n\nfull_pipeline = ColumnTransformer([\n    (\"cat\", OneHotEncoder(), cat_attribs),\n])\n\ndata_train_prepared = full_pipeline.fit_transform(data_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Run regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(data_train_prepared, data_train_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.1 Trial on some instances of the training set","metadata":{}},{"cell_type":"code","source":"some_data = data_train.iloc[:5]\nsome_labels = data_train_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(\"Predictions: \", lin_reg.predict(some_data_prepared))\nprint(\"Labels: \", list(some_labels))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Measuring prediction error with RMSE","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\ndata_predictions = lin_reg.predict(data_train_prepared)\nlin_mse = mean_squared_error(data_train_labels, data_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Use a different Model (Decision Tree)","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(data_train_prepared, data_train_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_predictions = tree_reg.predict(data_train_prepared)\ntree_rmse = mean_squared_error(data_train_labels, data_predictions)\ntree_rmse = np.sqrt(tree_rmse)\ntree_rmse","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### => Better model als RMSE is lower than with linear regression!","metadata":{}},{"cell_type":"markdown","source":"# 9. Split the training set in a smaller training set and validation set (K-fold cross-validation)","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(tree_reg, data_train_prepared, data_train_labels, scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results:","metadata":{}},{"cell_type":"code","source":"def display_scores(scores):\n    print(f\"Scores: {scores}\")\n    print(\"Mean: \", scores.mean())\n    print(\"Standard deviation: \", scores.std())\n\ndisplay_scores(tree_rmse_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 9.1 Comparing these results to the Linear Regression model","metadata":{}},{"cell_type":"code","source":"lin_scores = cross_val_score(lin_reg, data_train_prepared, data_train_labels, scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\n\ndisplay_scores(lin_rmse_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 10. Use a different Model (Random Forest)","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nforest_reg = RandomForestRegressor()\nforest_reg.fit(data_train_prepared, data_train_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"forest_scores = cross_val_score(forest_reg, data_train_prepared, data_train_labels, scoring=\"neg_mean_squared_error\", cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\n\ndisplay_scores(forest_rmse_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 11. Grid Search","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    {\"n_estimators\": [3, 10, 30], \"max_features\": [2, 4, 6, 8]},\n    {\"bootstrap\": [False], \"n_estimators\": [3, 10], \"max_features\": [2, 3, 4]},\n]\n\nforest_reg = RandomForestRegressor()\n\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring=\"neg_mean_squared_error\", return_train_score=True)\n\ngrid_search.fit(data_train_prepared, data_train_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 11.1 Get best estimator directly","metadata":{}},{"cell_type":"code","source":"grid_search.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 12. Selecting the best model","metadata":{}},{"cell_type":"code","source":"feature_importances = grid_search.best_estimator_.feature_importances_\n\nextra_attribs = [\"miles_per_year\"]\ncat_encoder = full_pipeline.named_transformers_[\"cat\"]\ncat_one_hot_attribs = list(cat_encoder.categories_[0])\nattributes = extra_attribs + cat_one_hot_attribs\nsorted(zip(feature_importances, attributes), reverse=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 13. Evaluation on the test set","metadata":{}},{"cell_type":"code","source":"final_model = grid_search.best_estimator_\n\nX_test = strat_test_set.drop(\"price\", axis=1)\ny_test = strat_test_set[\"price\"].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)\n\nfinal_predictions =final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_rmse","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 13.1 How confident are we in generalising?","metadata":{}},{"cell_type":"code","source":"from scipy import stats\nconfidence = 0.95\nsquared_errors = (final_predictions - y_test) ** 2\nnp.sqrt(stats.t.interval(confidence, len(squared_errors) - 1, loc=squared_errors.mean(), scale=stats.sem(squared_errors)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 14. Result","metadata":{}},{"cell_type":"markdown","source":"### Based on the test set, the model predicts prices for Skoda cars with a RMSE of ~3,085, which is off by about 22%.","metadata":{}}]}