{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/heart-disease-prediction-using-logistic-regression/framingham.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We notice that there are nulls in columns. Lets replace them with mean, median and mode of a particular variable.\n\nThe way to input data in NULL Value:\n\nDelete data with complete.cases () if the amount of NULL Value data is not too much\n\nReplace the data with mean () if the data is numeric and there is no outlier\n\nReplace the data with median () if the data is numeric and there is outlier\n\nReplace data with mode(modus) if the data is factorial orstring and the distribution of variations in the data varies. The table () function can help with the mode(modus) of data\n\nFor simplicity we will drop all null containing rows. The result is a dataset reduced by 12%\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data=df#.drop(['education'],axis=1)\ndata.dropna(axis=0,inplace=True)\n\ndata.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For simplicity we will drop education and all null containing columns. The result is a dataset reduced by 12%\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"category=['male','currentSmoker','BPMeds','prevalentStroke','prevalentHyp','diabetes','education']\ncols=list(data.columns)\ncols.remove('TenYearCHD')\nnumeric=list(set(cols)-set(category))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets do some Chisquared test for independance between categorical variables and predictor variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in category:\n    print(\"-------\"+i+\"-------\")\n    contingency_table=pd.crosstab(data[i],data[\"TenYearCHD\"])\n    #print('contingency_table :-\\n',contingency_table)\n\n    Observed_Values = contingency_table.values \n    #print(\"Observed Values :-\\n\",Observed_Values)\n\n    #Expected Values\n    import scipy.stats\n    b=scipy.stats.chi2_contingency(contingency_table)\n    Expected_Values = b[3]\n    #print(\"Expected Values :-\\n\",Expected_Values)\n\n    deg_freedom=b[2]\n    print(\"Degree of Freedom:-\",deg_freedom)\n\n    from scipy.stats import chi2\n    chi_square=sum([(o-e)**2./e for o,e in zip(Observed_Values,Expected_Values)])\n    chi_square_statistic=chi_square[0]+chi_square[1]\n    print(\"chi-square statistic:-\",chi_square_statistic)\n\n    p_value=1-chi2.cdf(x=chi_square_statistic,df=deg_freedom)\n    print('p-value:',p_value)\n\n    if p_value<=0.05:\n        print(\"There is a relationship\")\n    else:\n        print(\"There is no relationship \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above results. All categorical variables except currSmoker has a significant relationship with Heart Disease","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nfor i in numeric:\n    print(\"------\"+i+\"--------\")\n    df_anova = data[[\"TenYearCHD\",i]]\n\n    grps = pd.unique(df_anova[\"TenYearCHD\"].values)\n    d_data = {grp:df_anova[i][df_anova[\"TenYearCHD\"] == grp] for grp in grps}\n\n    F, p = stats.f_oneway(d_data[0], d_data[1])\n    print(\"p-value for significance is: \", round(p,4))\n    if p<0.05:\n        print(\"reject null hypothesis\")\n    else:\n        print(\"accept null hypothesis\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Heart rate shows no relation with Heart disease.\nLets see the variation inflation factor for Numeric data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Lets take two approaches here.A model with the above filtered variables. One with feature selection by backward elimination.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Case 1 (backward elimination)\n\nWe feed the dataset without scrutinizing the independant variable and apply backward elimination using p-value to come with a set of variables for which p values are below (0.05)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tools import add_constant\nfrom statsmodels.discrete.discrete_model import Logit\nx=data[cols]\nx = add_constant(x)\ny=data.TenYearCHD\nmodel=Logit(y,x)\nresult=model.fit()\nresult.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The message box shows it took 10 iterations to fit the best possible model and also shows current function value (objective function). This value is helpful when after certain number of iterations the model wont learn so the objective function remains the same\n\nMaximum Likelihood estimation: To find an optimal way to fit a distribution on the data. the optimal place at which the distribution must be placed to define the data well. Bigger the likelihood higher the probability that our model is correct.\n\nLog likely hood- its almost negative always. Bigger the better\nLL null- log likely hood null\t- log likely hood when there is no independant variable. Aka. useless model\n\nThe comparison between these two metrics gives \nLLR p-value- show how statistically significant the model is compared to LL-NULL\n\nPseudo R squared(McFaddenâ€™s R-squared) - somewhere between 0.2 and 0.4 good. Useful only to comapare different variation of same model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cols=data.columns[:-1]\nprint(cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef back_feature_elem (data_frame,dep_var,col_list):\n    while len(col_list)>0 :\n        x=data[col_list]\n        x = add_constant(x)\n        y=data.TenYearCHD\n        model=Logit(y,x)\n        result=model.fit(disp=0)\n        largest_pvalue=round(result.pvalues,3).nlargest(1)\n        if largest_pvalue[0]<(0.05):\n            return result\n            break\n        else:\n            col_list=col_list.drop(largest_pvalue.index)\n    \nresult=back_feature_elem(data,data.TenYearCHD,cols)\nresult.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature Selection: Backward elemination (P-value approach)\nTakes in the dataframe, the dependent variable and a list of column names, \nruns the regression repeatedly eliminating feature with the highest\nP-value above alpha one at a time and returns the regression summary \nwith all p-values below alpha\n\nLets interpret the coefficients of the variables.\nThe exponential off the coeffcients give the odds ratio.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.exp(result.params))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the values mean that\nfor males there is a 75% (1.75-1 =0.75) higher chances of heart disease than for females\n\nsimilarly,\nfor 1 year increase in age there is 6.8% higher chance of getting heart disease.\n\nand so on..\n\nLets do some predictions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets get the confusion matrix based on statsmodel\n\nconf_sm=pd.DataFrame(result.pred_table())\nconf_sm.columns=['predicted 0','predicted 1']\nconf_sm=conf_sm.rename(index={0:'Actual 0',1:'Actual 1'})\nconf_sm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train split\n\nimport sklearn\nnew_features=data[['age','male','cigsPerDay','totChol','sysBP','glucose','TenYearCHD']]\nx=new_features.iloc[:,:-1]\ny=new_features.iloc[:,-1]\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.20,random_state=5)\n\nfrom sklearn.linear_model import LogisticRegression\nlogreg=LogisticRegression()\nlogreg.fit(x_train,y_train)\ny_pred=logreg.predict(x_test)\n\n\nfrom sklearn import metrics\nconfusion_matrix = metrics.confusion_matrix(y_test,y_pred)\n\nconf=pd.DataFrame(confusion_matrix)\nconf.columns=['predicted 0','predicted 1']\nconf=conf.rename(index={0:'Actual 0',1:'Actual 1'})\nprint(conf)\n\nprint(\"model accuracy:\")\nprint(sklearn.metrics.accuracy_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model predicts the correct output 86.3% of the time\n\n\nCase 2-\n\nLets try for model with feature selection done manually. Based on our hypothesis testing. We rejected currentSmoker and heart rate to be of significant predictor.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x=data.drop(['TenYearCHD','currentSmoker','heartRate'],axis=1)\nx=add_constant(x)\ny=data.TenYearCHD\nmodel=Logit(y,x)\nresult=model.fit(disp=0)\nresult.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing all insignificant variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x=data.drop(['TenYearCHD','currentSmoker','heartRate','glucose','prevalentStroke','education','BPMeds','prevalentHyp','diabetes','diaBP','BMI'],axis=1)\nx=add_constant(x)\ny=data.TenYearCHD\nmodel=Logit(y,x)\nresult=model.fit(disp=0)\nresult.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train split\n\nimport sklearn\nnew_features=data[['age','male','cigsPerDay','totChol','sysBP','TenYearCHD']]\nx=new_features.iloc[:,:-1]\ny=new_features.iloc[:,-1]\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.20,random_state=5)\n#print(x_test)\nfrom sklearn.linear_model import LogisticRegression\nlogreg=LogisticRegression()\nlogreg.fit(x_train,y_train)\ny_pred=logreg.predict(x_test)\n\nfrom sklearn import metrics\nconfusion_matrix = metrics.confusion_matrix(y_test,y_pred)\n\nconf=pd.DataFrame(confusion_matrix)\nconf.columns=['predicted 0','predicted 1']\nconf=conf.rename(index={0:'Actual 0',1:'Actual 1'})\nprint(conf)\n\nprint(\"model accuracy:\")\nprint(sklearn.metrics.accuracy_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is only slight difference in accuracy between the two models.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}