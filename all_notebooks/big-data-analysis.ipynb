{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Big Data Analysis- Proyecto Big-Data**\n\nIntegrantes: Daniel Fernando Silva Avila - 20201395008.\n\n# ENMARCAR LA PREOCUPACIÓN\n1. Identificación de los interesados: Se encuentran directamente interesados los directivos del comercio electrónico, principalmente los responsables del sistema de distribución de productos.\n2. Pregunta inicial.\n*¿El producto llegará tarde a su destino?*\n\nEsta es la generalización de la pregunta de negocio con respuesta binaria (si o no).\n\n3. Análisis de por qué y para qué.\n* ¿Por qué?: Se han identificado falencias en el sistema de distribución de mercancías (entregas tardías) que generan quejas y reclamos por parte del cliente que adquiere mercancía a través de nuestra plataforma de comercio electrónico. Estas quejas se traducen finalmente en perdidas económicas (los clientes se rehúsan a usar nuevamente el servicio y comparte experiencias negativas en redes sociales).\n* ¿Para qué?: Con el objetivo de mejorar el proceso de distribución y prometer al cliente un tiempo de entrega que sea cumplido por nuestra organización.\n* Objetivos del análisis: Construir e implementar un modelo de predicción de la estimación del riesgo de entrega tardía.\n\n# Objetivo de negocio: \nReducir el numero de entregas tardias (esto implica reducir tambien reducir el numero de quejas y reclamos por tal motivo)\n# Objetivos especificos del notebook\n\n-Desarrollar análisis exploratorio de datos EDA en el dataset elegido para el proyecto usando la libreria de python Pandas-Profiling, el análisis debe incluir las conclusiones de las operaciones de revisión efectuadas con cada libreria.\n\n-Basado en el análisis exploratorio, realizar el proceso de limpieza de datos\n\n-Realizar análisis de dos o más variables y crear graficas de visualización con las principales librerias dispuesta para tal fin en python\n\n-Plantear hipotesis  nulas y alternativas que sean rechazadas o aceptadas basados en metodos de estadistica inferencial\n\n-Construir algoritmos de clasificación que permitan estimar el riesgo de entrega tardia\n\n# Dataset\nEl conjunto de datos contiene información de la cadena de suministro para un comercio electrónico. Los datos describen procesos de aprovisionamiento, producción, ventas y distribución comercial. También permite la correlación de Datos Estructurados con Datos No Estructurados para la generación de conocimiento. El dataset  ha sido tomado del repositorio kaggle y se encuentra disponible a través del siguiente enlace: https://www.kaggle.com/shashwatwork/dataco-smart-supply-chain-for-big-data-analysis\n\n","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pandas_profiling as pp # exploratory data analysis EDA\nimport matplotlib.pyplot as plt # data visualisation\nimport seaborn as sns #data visualisation\nimport plotly.express as px #data visualisation\nimport plotly.graph_objects as go\nfrom scipy.stats import chi2_contingency, norm # Calculo de chi2\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Leer el dataset que ha sido importado directamente desde kaggle ","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/dataco-smart-supply-chain-for-big-data-analysis/DataCoSupplyChainDataset.csv\", engine='python')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Explorar los primeros 5 registros del dataset","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Crear reporte de Pandas profile","metadata":{}},{"cell_type":"code","source":"profile = pp.ProfileReport(df, title = \"EDA\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# profile.to_notebook_iframe() # Genera el reporte directamente en el notebook.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Guardar el reporte como un archivo con extensión html","metadata":{}},{"cell_type":"code","source":"profile.to_file(output_file=\"reporte.html\") ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Datos nulos**\nSe encontraron datos nulos en tres columnas, Order Zipcode ( 86% valores nulos, es decir un total de 155679 registros), Product description (100% de valores nulos, es decir un total de 180519 registros) y customer zipcode (<0.1% es decir un total de 3 registros), en conjunto estas celdas vacias corresponden al 3.5% del total del dataset\n\n**Variables categorcas**\nExisten 28 variables que han sido identificadas como tipo categorico, no obstante, algunas variables se clasificaron como categoricas cuando son numericas, por ejemplo Days for shipping y Order item quantity, esto sucede debido a que todos los valores se agrupan en 4 varoles discretos, lo que genera una confusión por parte de pandas profiling. Con el fin de efectuar análisis estas variables deben ser consideradas como numericas\n\n**Variables numericas**\nExisten 24 variables  que han sido identificadas como tipo numerico\n\n**Conclusiones EDA**\nNo incluir en el análisis las columnas order zipcode (casi vacia), product description (totalmente vacias), customer email (constante sin información), Customer password (Constante sin información), adicionalmente podemos reemplazar los valores nulos de la columna customer zipcode. \nFinalmente existen algunas  columnas que pueden obviarse a la hora de hacer el análisis ya que no aportan información relevante para el objeto del estudio, dichas columnas son: Product Status,Customer Street,Customer Fname,Customer Lname,Latitude,Longitude,Product Image.","metadata":{}},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"code","source":"# shape and data types of the data\nprint(df.shape)\nprint(df.dtypes)\n \n    # se puede revisar la clasificación de pandas profiling contra el tipo de dato ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Eliminaremos las columnas innecesarias","metadata":{}},{"cell_type":"code","source":"data=df.drop(['Order Zipcode','Product Description', 'Customer Email','Customer Password','Product Status','Customer Street','Customer Fname','Customer Lname',\n           'Latitude','Longitude','Product Image',],axis=1)\ndata.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Los valores nulos de zipcode seran reemplazados por el valor mas común","metadata":{}},{"cell_type":"code","source":"column_index=data.columns.get_loc(\"Customer Zipcode\")\n# Get the index of the column \"Customer Zipcode\"\ndata['Customer Zipcode']=data['Customer Zipcode'].fillna(data.mode().iloc[column_index])\n#Filling NaN columns with most common value","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Descriptive analysis and visualisation\n\nSe elabora una matriz de correlación","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(24,12))         # figsize\nsns.heatmap(data.corr(),annot=True,linewidths=.5,fmt='.1g',cmap= sns.diverging_palette(230, 20, as_cmap=True)) # Heatmap for correlation matrix\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Se elaboran las tablas de frecuencia relativa para variables categoricas","metadata":{}},{"cell_type":"code","source":"count=data['Delivery Status'].value_counts()  #change categoric variable\nprint(count / len(data))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"¿Cual es la categoria que más se entrega de forma tardia?","metadata":{}},{"cell_type":"code","source":"#Filtering columns with late delivery status\nlate_delivery = data[(data['Delivery Status'] == 'Late delivery')]\n#Top 10 products with most late deliveries\nfig = px.bar(late_delivery['Category Name'].value_counts().nlargest(10), \n             title=\"Top 10 products with most late deliveries\",\n            labels={'value':'Number of late deliveries','index':'Category'})\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No obstante, la grafica anterior representa la situación real del comercio electronico debido a que no se tiene en cuenta que el datset se encuentra desbalanceado, por lo que la mayor cantidad de registros se agrupan en unas pocas categorias, al tener en cuenta la proporcionalidad (porcentage de entregas tardias respecto al total de entregas de dicha categoria) se obtiene una grafica diferente que muestra mejor la situación real","metadata":{}},{"cell_type":"code","source":"#Calculating proproptional late deliveries\nlate_count=late_delivery['Category Name'].value_counts()\ntotal_count=data['Category Name'].value_counts()\nproportional_count=late_count/total_count*100\nfig = px.bar(proportional_count.nlargest(15), \n             title=\"Top 10 products with highest rate of late delivery\",\n            labels={'value':'Percentage of late deliveries','index':'Category'})\nfig.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Con esta grafica se puede deducir que no existe relación entre la categoria del producto y la entrega tardia, y que todas las categorias tienen aproximadamente un 50 % de entregas tardias","metadata":{}},{"cell_type":"markdown","source":"¿Cual es el numero de entregas tardias de acuerdo con el tipo de envío y la región?\nPara esta grafica se realizan dos versiones usando dos librerias diferentes de python con el fin de comparar sus resultados, en la primera se crea una grafica estatica usando matplotlib mientas que la segunda es una grafica dinamica creada usando plotly","metadata":{}},{"cell_type":"code","source":"#Filtering late delivery orders with standard class shipping\nxyz1 = data[(data['Delivery Status'] == 'Late delivery') & (data['Shipping Mode'] == 'Standard Class')]\n#Filtering late delivery orders with first class shipping\nxyz2 = data[(data['Delivery Status'] == 'Late delivery') & (data['Shipping Mode'] == 'First Class')]\n#Filtering late delivery orders with second class shipping\nxyz3 = data[(data['Delivery Status'] == 'Late delivery') & (data['Shipping Mode'] == 'Second Class')]\n#Filtering late delivery orders with same day shipping\nxyz4 = data[(data['Delivery Status'] == 'Late delivery') & (data['Shipping Mode'] == 'Same Day')]\n#Counting total values\ncount1=xyz1['Order Region'].value_counts()\ncount2=xyz2['Order Region'].value_counts()\ncount3=xyz3['Order Region'].value_counts()\ncount4=xyz4['Order Region'].value_counts()\n#Index names\nnames=data['Order Region'].value_counts().keys()\nn_groups=23\nfig,ax = plt.subplots(figsize=(20,8))\nindex=np.arange(n_groups)\nbar_width=0.2\nopacity=0.6\ntype1=plt.bar(index,count1,bar_width,alpha=opacity,color='b',label='Standard Class')\ntype2=plt.bar(index+bar_width,count2,bar_width,alpha=opacity,color='r',label='First class')\ntype3=plt.bar(index+bar_width+bar_width,count3,bar_width,alpha=opacity,color='g',label='second class')\ntype4=plt.bar(index+bar_width+bar_width+bar_width,count4,bar_width,alpha=opacity,color='y',label='same day')\nplt.xlabel('Order Regions')\nplt.ylabel('Number of shipments')\nplt.title('Different Types of shipping methods used in all regions')\nplt.legend()\nplt.xticks(index+bar_width,names,rotation=90)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Filtering late delivery orders with standard class shipping\nxyz1 = data[(data['Delivery Status'] == 'Late delivery') & (data['Shipping Mode'] == 'Standard Class')]\n#Filtering late delivery orders with first class shipping\nxyz2 = data[(data['Delivery Status'] == 'Late delivery') & (data['Shipping Mode'] == 'First Class')]\n#Filtering late delivery orders with second class shipping\nxyz3 = data[(data['Delivery Status'] == 'Late delivery') & (data['Shipping Mode'] == 'Second Class')]\n#Filtering late delivery orders with same day shipping\nxyz4 = data[(data['Delivery Status'] == 'Late delivery') & (data['Shipping Mode'] == 'Same Day')]\n#Counting total values\ncount1=xyz1['Order Region'].value_counts()\ncount2=xyz2['Order Region'].value_counts()\ncount3=xyz3['Order Region'].value_counts()\ncount4=xyz4['Order Region'].value_counts()\n#Index names\nnames=data['Order Region'].value_counts().keys()\n\nfig = go.Figure()\nfig.add_trace(go.Bar(x=names,\n                y=count1,\n                name='Standard Class',\n                marker_color='rgb(55, 83, 109)'\n                ))\nfig.add_trace(go.Bar(x=names,\n                y=count2,\n                name='First Class',\n                marker_color='rgb(26, 118, 255)'\n                ))\nfig.add_trace(go.Bar(x=names,\n                y=count3,\n                name='Second Class',\n                marker_color='rgb(100, 231, 186)'\n                ))\nfig.add_trace(go.Bar(x=names,\n                y=count4,\n                name='Same Day',\n                marker_color='rgb(243, 134, 59)'\n                ))\n\nfig.update_layout(\n    title='Different Types of shipping methods used in all regions',\n    xaxis={'categoryorder':'total descending'},\n    yaxis=dict(\n        title='Number of shipments',\n        titlefont_size=16,\n        tickfont_size=14,\n    ),\n    legend=dict(\n        x=1,\n        y=1.0,\n        bgcolor='rgba(255, 255, 255, 0)',\n        bordercolor='rgba(255, 255, 255, 0)'\n    ),\n    barmode='group',\n    bargap=0.15, # gap between bars of adjacent location coordinates.\n    bargroupgap=0.1 # gap between bars of the same location coordinate.\n)\nfig.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Estadistica inferencial**\n\nPrueba de hipotesis\n\n*Hipotesis Nula H0*:\n\n1. La region a la cual se realiza el envío no influye en la entrega tardia\n\n2. El tipo de envío no influye en la entrega tardia\n\n3. La categoria del producto no influye e la entrega tardia\n\n*Hipotesis Alternativa Ha*: \n\n1. La region a la cual se realiza el envío  influye en la entrega tardia\n\n2. El tipo de envío  influye en la entrega tardia\n\n3. La categoria del producto  influye e la entrega tardia","metadata":{}},{"cell_type":"code","source":"def calcular_chi2(dependiente,independientes):\n    for var in independientes:\n        primary_location_cross = pd.crosstab(data[dependiente], data[var])\n        g, p, dof, expctd = chi2_contingency(primary_location_cross)\n        print(\"p-value de Chi-square test para \" + dependiente + \" vs \" + var + \" = \" , p)\n\n\ncolumnas = ['Order Region','Shipping Mode','Category Name','Type','Customer City']\n\ncalcular_chi2('Delivery Status', columnas)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Basados en la anterior prueba de hipotesis en la cual se utiliza la distribución Chi2 y se calcularon los niveles de significancia entre\nlas distintas variables categoricas se puede concluir lo siguiente:\n\n1. Rechazar la hipotesis nula 1\n2. Rechazar la hipotesis nula 2\n3. Aceptar la hipotesis nula 3","metadata":{}},{"cell_type":"markdown","source":"# Algoritmos de CLASIFICACIÓN\n\n*El primer algoritmo a utilizar es regresion logistica","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select the columns needed for the model\nprediction_data=data[['Order Region','Shipping Mode','Delivery Status']]\nprediction_data.columns=['Order_Region','Shipping_Mode','Delivery_Status'] #al incluir la columna Type el modelo baja su precision, al incluir Customer City el modelo no converge\nprediction_data=prediction_data[prediction_data.Delivery_Status!='Shipping canceled']\nprediction_data=prediction_data[prediction_data.Delivery_Status!='Advance shipping']\nprediction_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature enginering, one hot encoding\n#usar one hot encoding cuando la variable categorica es NOMINAL\nprediction_data=pd.get_dummies(prediction_data, drop_first=True)\nprediction_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Test train split\nX_train, X_test, Y_train, Y_test=train_test_split(prediction_data.drop('Delivery_Status_Shipping on time',axis=1),prediction_data['Delivery_Status_Shipping on time'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train the model\nLogReg=LogisticRegression()\nLogReg.fit(X_train, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#score the model\nLogReg.score(X_test, Y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_pred=LogReg.predict(X_test)\nprint (classification_report(Y_test,Y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"El modelo creado usando regresion logistica tiene un 75.59% de exactitud","metadata":{}},{"cell_type":"markdown","source":"*El segundo algoritmo a utilizar es Random Forest ","metadata":{}},{"cell_type":"code","source":"#Import dependencies and train the model\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier()\nclassifier.fit(X_train,Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#score the model\nclassifier.score(X_test,Y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_pred_RF=classifier.predict(X_test)\nprint (classification_report(Y_test,Y_pred_RF))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"El modelo creado usando el algoritmo de Random Forest tiene un 75.76% de exactitud","metadata":{}},{"cell_type":"markdown","source":"**Referencias**\nhttps://www.kaggle.com/skloveyyp/comparison-of-classification-regression-rnn\n\nhttps://towardsdatascience.com/data-cleaning-in-python-the-ultimate-guide-2020-c63b88bf0a0d\n\nhttps://towardsdatascience.com/log-book-guide-to-hypothesis-testing-802b1980d0b8","metadata":{}}]}