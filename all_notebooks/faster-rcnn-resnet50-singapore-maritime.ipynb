{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Installs and Imports","metadata":{"_uuid":"d48de61b-4764-40d2-bf20-4ee98e21b805","_cell_guid":"8d9bb9fa-6703-45e4-a62c-b49c15610303","trusted":true}},{"cell_type":"code","source":"# Download TorchVision repo to use some files from\n# references/detection\n!pip install pycocotools --quiet\n!git clone https://github.com/pytorch/vision.git\n!git checkout v0.3.0\n\n!cp vision/references/detection/utils.py ./\n!cp vision/references/detection/transforms.py ./\n!cp vision/references/detection/coco_eval.py ./\n!cp vision/references/detection/engine.py ./\n!cp vision/references/detection/coco_utils.py ./","metadata":{"_uuid":"8f5cdc95-2d4a-4d95-89e0-8313af6edca3","_cell_guid":"63db7693-a0af-42e5-8ae5-c3d170342d30","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-01T17:52:21.858631Z","iopub.execute_input":"2021-07-01T17:52:21.85897Z","iopub.status.idle":"2021-07-01T17:52:46.936079Z","shell.execute_reply.started":"2021-07-01T17:52:21.858941Z","shell.execute_reply":"2021-07-01T17:52:46.934946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install torch","metadata":{"_uuid":"20f34d4d-87e7-403a-a4c9-0483c8635917","_cell_guid":"142be91a-a5da-4556-8cf3-536ccc03941c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-01T17:52:46.94Z","iopub.execute_input":"2021-07-01T17:52:46.94029Z","iopub.status.idle":"2021-07-01T17:52:52.994987Z","shell.execute_reply.started":"2021-07-01T17:52:46.940259Z","shell.execute_reply":"2021-07-01T17:52:52.994207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"8f48941d-1804-46e5-b140-746e5127fbc8","_cell_guid":"cbf93bb4-9ff8-4428-82fc-b1e86567e52c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Basic python and ML Libraries\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\n# for ignoring warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# We will be reading images using OpenCV\nimport cv2\n\n# xml library for parsing xml files\nfrom xml.etree import ElementTree as et\n\n# matplotlib for visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# torchvision libraries\nimport torch\nimport torchvision\nfrom torchvision import transforms as torchtrans  \nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n# these are the helper libraries imported.\nfrom engine import train_one_epoch, evaluate\nimport utils\nimport transforms as T\n\n# for image augmentations\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2","metadata":{"_uuid":"2a3a1ff4-f400-4a05-80be-c8969d3587a0","_cell_guid":"ae5bf9ef-2113-402f-baf5-52d54b95da28","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-01T17:52:52.997466Z","iopub.execute_input":"2021-07-01T17:52:52.997859Z","iopub.status.idle":"2021-07-01T17:52:56.247096Z","shell.execute_reply.started":"2021-07-01T17:52:52.997817Z","shell.execute_reply":"2021-07-01T17:52:56.245923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{"_uuid":"f4dc4066-fd77-4d50-b6ff-d046c56734b2","_cell_guid":"a614c08a-2ace-499f-978b-a4d400621e2d","trusted":true}},{"cell_type":"code","source":"# defining the files directory and testing directory\nfiles_dir = '../input/singapore-test/images2/train'\ntest_dir = '../input/singapore-test/images2/test'\n\n\nclass Singapore_maritime_ds(torch.utils.data.Dataset):\n\n    def __init__(self, files_dir, width, height, transforms=None):\n        self.transforms = transforms\n        self.files_dir = files_dir\n        self.height = height\n        self.width = width\n        \n        # sorting the images for consistency\n        # To get images, the extension of the filename is checked to be jpg\n        self.imgs = [image for image in sorted(os.listdir(files_dir))\n                        if image[-4:]=='.jpg']\n        \n        \n        # classes: 0 index is reserved for background\n        self.classes = [_,'Ferry','Buoy','Vessel/ship','Speed boat','Boat', 'Kayak', 'Sail boat', 'Swimming person', 'Flying bird/plane', 'Other']\n        \n    def __getitem__(self, idx): \n\n        img_name = self.imgs[idx]\n        image_path = os.path.join(self.files_dir, img_name)\n\n        # reading the images and converting them to correct size and color    \n        img = cv2.imread(image_path)\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n        img_res = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_AREA)\n        # diving by 255\n        #img_res = img_rgb/255.0\n        img_res /= 255.0\n        \n        # annotation file\n        annot_filename = img_name[:-4] + '.xml'\n        annot_file_path = os.path.join(self.files_dir, annot_filename)\n        \n        boxes = []\n        labels = []\n        tree = et.parse(annot_file_path)\n        root = tree.getroot()\n        \n        # cv2 image gives size as height x width\n        wt = img.shape[1]\n        ht = img.shape[0]\n        \n        # box coordinates for xml files are extracted and corrected for image size given\n        for member in root.findall('object'):\n            labels.append(self.classes.index(member.find('name').text))\n            \n            # bounding box\n            xmin = float(member.find('bndbox').find('xmin').text)\n            \n            xmax = float(member.find('bndbox').find('xmax').text)\n            \n            ymin = float(member.find('bndbox').find('ymin').text)\n            \n            ymax = float(member.find('bndbox').find('ymax').text)\n            \n            \n                    \n            xmin_corr = (xmin/wt)*self.width\n            \n            xmax_corr = (xmax/wt)*self.width\n            \n            ymin_corr = (ymin/ht)*self.height\n            \n            ymax_corr = (ymax/ht)*self.height\n            \n            \n            boxes.append([xmin_corr, ymin_corr, xmax_corr, ymax_corr])\n        \n        # convert boxes into a torch.Tensor\n        #boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        boxes = torch.tensor(boxes, dtype=torch.float32)\n        # getting the areas of the boxes\n        #area = boxes[:,2] * boxes[:,3]\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n      \n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n        \n        labels = torch.as_tensor(labels, dtype=torch.int64)\n\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"area\"] = area\n        #target[\"area\"] = torch.tensor(area, dtype=torch.float32)\n        target[\"iscrowd\"] = iscrowd\n        # image_id\n        image_id = torch.tensor([idx])\n        target[\"image_id\"] = image_id\n\n\n        if self.transforms:\n            \n            sample = self.transforms(image = img_res,\n                                     bboxes = target['boxes'],\n                                     labels = labels)\n            \n            img_res = sample['image']\n            target['boxes'] = torch.Tensor(sample['bboxes'])\n            \n            \n            \n        return img_res, target\n\n    def __len__(self):\n        return len(self.imgs)\n\n\n# check dataset\ndataset = Singapore_maritime_ds(files_dir, 1920, 1080)\nprint('length of dataset = ', len(dataset), '\\n')\n\n# getting the image and target for a test index.  Feel free to change the index.\nimg, target = dataset[10]\nprint(img.shape, '\\n',target)","metadata":{"_uuid":"802fe60b-9abc-40c3-ac61-da48c96b8a8a","_cell_guid":"7cfb9dde-6b1d-4d09-b7d6-e242f1eeaa47","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-01T17:52:56.248448Z","iopub.execute_input":"2021-07-01T17:52:56.248843Z","iopub.status.idle":"2021-07-01T17:52:56.638631Z","shell.execute_reply.started":"2021-07-01T17:52:56.248801Z","shell.execute_reply":"2021-07-01T17:52:56.6379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset[10]","metadata":{"_uuid":"3abb36ae-15aa-4589-935e-f2e6750e9f6a","_cell_guid":"1989e63e-9af4-47eb-bfc2-eaf8470e5fda","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-01T17:52:56.642819Z","iopub.execute_input":"2021-07-01T17:52:56.643076Z","iopub.status.idle":"2021-07-01T17:52:56.720241Z","shell.execute_reply.started":"2021-07-01T17:52:56.64305Z","shell.execute_reply":"2021-07-01T17:52:56.719566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization","metadata":{"_uuid":"59aa91e7-06a2-4d8b-88b0-06bbc0ebb33d","_cell_guid":"c70c988f-e4fa-40f4-b3d4-213db0b338ce","trusted":true}},{"cell_type":"code","source":"# Function to visualize bounding boxes in the image\n\ndef plot_img_bbox(img, target):\n    # plot the image and bboxes\n    # Bounding boxes are defined as follows: x-min y-min width height\n    fig, a = plt.subplots(1,1)\n    fig.set_size_inches(20,40)\n    a.imshow(img)\n    for box in (target['boxes']):\n        x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n        rect = patches.Rectangle((x, y),\n                                 width, height,\n                                 linewidth = 2,\n                                 edgecolor = 'r',\n                                 facecolor = 'none')\n\n        # Draw the bounding box on top of the image\n        a.add_patch(rect)\n        \n    plt.show()\n    \n# plotting the image with bboxes. Feel free to change the index\nimg, target = dataset[1]\nplot_img_bbox(img, target)","metadata":{"_uuid":"cb60b43a-5ea1-4c34-a6a8-fecfc71cbd2a","_cell_guid":"4803746f-857e-4f40-b9e5-6ac8c292d5ac","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-01T17:52:56.721894Z","iopub.execute_input":"2021-07-01T17:52:56.722177Z","iopub.status.idle":"2021-07-01T17:52:57.517258Z","shell.execute_reply.started":"2021-07-01T17:52:56.722148Z","shell.execute_reply":"2021-07-01T17:52:57.516248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"_uuid":"df60c6f7-3ce1-4717-90a8-eea0ce4a1e9d","_cell_guid":"960334bc-cd04-419b-8a7e-c428c33f11b3","trusted":true}},{"cell_type":"code","source":"\ndef get_object_detection_model(num_classes):\n\n    # load a model pre-trained pre-trained on COCO\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n    \n    num_classes = 11\n    # get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n\n    return model","metadata":{"_uuid":"b588e365-834e-4fc7-aea3-6224ca097843","_cell_guid":"44398ca0-aa8c-476c-8c19-902588af75a3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-01T17:52:57.518437Z","iopub.execute_input":"2021-07-01T17:52:57.518771Z","iopub.status.idle":"2021-07-01T17:52:57.525097Z","shell.execute_reply.started":"2021-07-01T17:52:57.518737Z","shell.execute_reply":"2021-07-01T17:52:57.524227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Augmentations","metadata":{"_uuid":"6b6654ae-b3e6-458c-a9a2-18174659eb92","_cell_guid":"7548ea88-9dcf-4f94-8571-4bf6e8fb30cf","trusted":true}},{"cell_type":"code","source":"# Send train=True fro training transforms and False for val/test transforms\n\ndef get_transform(train):\n    \n    if train:\n        return A.Compose([\n                            A.HorizontalFlip(0.5),\n                            \n                     # ToTensorV2 converts image to pytorch tensor without div by 255\n                            ToTensorV2(p=1.0) \n                        ], bbox_params={'format': 'coco', 'label_fields': ['labels']})\n            #bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n    else:\n        return A.Compose([\n                            ToTensorV2(p=1.0)\n                        ], bbox_params={'format': 'coco', 'label_fields': ['labels']})\n            #bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})","metadata":{"_uuid":"4c4335c1-1d76-4384-94ff-8ea0e04f8a10","_cell_guid":"100722a0-57f1-487e-86a5-9153f84c06ff","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-01T17:52:57.526471Z","iopub.execute_input":"2021-07-01T17:52:57.527025Z","iopub.status.idle":"2021-07-01T17:52:57.53547Z","shell.execute_reply.started":"2021-07-01T17:52:57.526985Z","shell.execute_reply":"2021-07-01T17:52:57.534681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''def get_transform(train):\n    if train:\n        \n        return A.Compose([\n            #A.RandomCrop(width=450, height=450),\n            A.HorizontalFlip(p=0.5),\n            A.RandomBrightnessContrast(p=0.2),\n        ], bbox_params=A.BboxParams(format='coco', min_area=1024, min_visibility=0.1, label_fields=['labels']))\n    else:\n        return A.Compose([\n            #A.RandomCrop(width=450, height=450),\n            A.HorizontalFlip(p=0.5),\n            A.RandomBrightnessContrast(p=0.2),\n        ], bbox_params=A.BboxParams(format='coco', min_area=1024, min_visibility=0.1, label_fields=['labels']))'''","metadata":{"_uuid":"470d8672-8e89-4419-abec-f5fef36a40eb","_cell_guid":"c8cd7a38-aadd-41e7-a8f9-777bdb00dc39","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-01T17:52:57.53676Z","iopub.execute_input":"2021-07-01T17:52:57.537305Z","iopub.status.idle":"2021-07-01T17:52:57.549553Z","shell.execute_reply.started":"2021-07-01T17:52:57.537261Z","shell.execute_reply":"2021-07-01T17:52:57.548705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing dataset","metadata":{"_uuid":"a6f247b5-1b0b-4380-9cf5-06a6c41d6654","_cell_guid":"165e5936-ce31-4e62-aa46-67290354a04d","trusted":true}},{"cell_type":"code","source":"# use our dataset and defined transformations\ndataset = Singapore_maritime_ds(files_dir, 1920, 1080, transforms= get_transform(train=True))\ndataset_test = Singapore_maritime_ds(files_dir, 1920, 1080, transforms= get_transform(train=False))\n\n# split the dataset in train and test set\ntorch.manual_seed(1)\nindices = torch.randperm(len(dataset)).tolist()\n\n# train test split\ntest_split = 0.2\ntsize = int(len(dataset)*test_split)\ndataset = torch.utils.data.Subset(dataset, indices[:-50])\ndataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n\n# define training and validation data loaders\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=10, shuffle=True, num_workers=4,\n    collate_fn=utils.collate_fn)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test, batch_size=10, shuffle=False, num_workers=4,\n    collate_fn=utils.collate_fn)","metadata":{"_uuid":"dc79f254-4a73-4e27-a855-df951ca6aecf","_cell_guid":"e748a192-20fe-473d-b5f1-642b4183125d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-01T17:52:57.550931Z","iopub.execute_input":"2021-07-01T17:52:57.551494Z","iopub.status.idle":"2021-07-01T17:52:57.575705Z","shell.execute_reply.started":"2021-07-01T17:52:57.551454Z","shell.execute_reply":"2021-07-01T17:52:57.574933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dataset[10]","metadata":{"_uuid":"01550334-0275-4405-8d9e-d998cc6c7801","_cell_guid":"4fdd730d-b605-494e-8002-67c5fdd71e8a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-01T17:52:57.578329Z","iopub.execute_input":"2021-07-01T17:52:57.578566Z","iopub.status.idle":"2021-07-01T17:52:57.582419Z","shell.execute_reply.started":"2021-07-01T17:52:57.578542Z","shell.execute_reply":"2021-07-01T17:52:57.581532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Entrainement","metadata":{"_uuid":"d397df92-8caa-47aa-9d05-6ae45be37c75","_cell_guid":"00e80d81-a46f-4019-b059-95bdf7cbe165","trusted":true}},{"cell_type":"markdown","source":"Let's prepare the model for training","metadata":{"_uuid":"6cc172b9-4679-432d-81b7-063eaf3591db","_cell_guid":"dd0ebe76-07f9-4639-b8bd-a431c1deb641","trusted":true}},{"cell_type":"code","source":"# to train on gpu if selected.\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n\nnum_classes = 11\n\n# get the model using our helper function\nmodel = get_object_detection_model(num_classes)\n\n# move model to the right device\nmodel.to(device)\n\n# construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005,\n                            momentum=0.9, weight_decay=0.0005)\n\n# and a learning rate scheduler which decreases the learning rate by\n# 10x every 3 epochs\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=3,\n                                               gamma=0.1)","metadata":{"_uuid":"af22f21b-31c0-44bc-a0ca-3e0938a99802","_cell_guid":"122881a6-5800-4d22-b5a3-9a6399f0bbbe","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-01T17:52:57.583809Z","iopub.execute_input":"2021-07-01T17:52:57.584347Z","iopub.status.idle":"2021-07-01T17:53:09.68046Z","shell.execute_reply.started":"2021-07-01T17:52:57.584307Z","shell.execute_reply":"2021-07-01T17:53:09.679709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training for 3000 steps\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    # training for one epoch\n   # try :# train for one epoch, printing every 10 iterations\n        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n        # update the learning rate\n        lr_scheduler.step()\n        #evaluate on the test dataset\n        evaluate(model, data_loader_test, device=device)\n  #  except :\n     #   pass","metadata":{"_uuid":"93c05457-f5fa-47a6-8854-80c03e82bcd6","_cell_guid":"301aa3b5-7cd5-4c74-b20f-02505d25f90c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-01T18:04:21.599941Z","iopub.execute_input":"2021-07-01T18:04:21.600273Z","iopub.status.idle":"2021-07-01T18:04:22.069081Z","shell.execute_reply.started":"2021-07-01T18:04:21.600243Z","shell.execute_reply":"2021-07-01T18:04:22.065862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"5348c2e3-4fa1-4b80-b76a-51ab35bee95b","_cell_guid":"582d2302-dbe4-491d-926c-6ccee409e1bc","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decode predictions","metadata":{"_uuid":"d9d9d8e9-a922-409f-a8d7-c5139a570250","_cell_guid":"42e6b4aa-a57b-4158-a477-6aae6193ae60","trusted":true}},{"cell_type":"code","source":"# the function takes the original prediction and the iou threshold.\n\ndef apply_nms(orig_prediction, iou_thresh=0.3):\n    \n    # torchvision returns the indices of the bboxes to keep\n    keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n    \n    final_prediction = orig_prediction\n    final_prediction['boxes'] = final_prediction['boxes'][keep]\n    final_prediction['scores'] = final_prediction['scores'][keep]\n    final_prediction['labels'] = final_prediction['labels'][keep]\n    \n    return final_prediction\n\n# function to convert a torchtensor back to PIL image\ndef torch_to_pil(img):\n    return torchtrans.ToPILImage()(img).convert('RGB')","metadata":{"_uuid":"8636a295-23db-40a3-90bd-18912641f656","_cell_guid":"f5716aaa-a657-4c94-b74c-6edb3a405b01","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-01T18:04:27.650912Z","iopub.execute_input":"2021-07-01T18:04:27.651274Z","iopub.status.idle":"2021-07-01T18:04:27.659841Z","shell.execute_reply.started":"2021-07-01T18:04:27.651244Z","shell.execute_reply":"2021-07-01T18:04:27.658926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pick one image from the test set\nimg, target = dataset_test[12]\n# put the model in evaluation mode\nmodel.eval()\nwith torch.no_grad():\n    prediction = model([img.to(device)])[0]\n    \nprint('predicted #boxes: ', len(prediction['labels']))\nprint('real #boxes: ', len(target['labels']))","metadata":{"_uuid":"8f769cc6-2f03-43c4-a72d-b1b3b85f9fd6","_cell_guid":"c39e6be8-7f1d-45a9-9da3-1bfd9a99fc59","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-01T18:04:34.589509Z","iopub.execute_input":"2021-07-01T18:04:34.589885Z","iopub.status.idle":"2021-07-01T18:04:34.768426Z","shell.execute_reply.started":"2021-07-01T18:04:34.589852Z","shell.execute_reply":"2021-07-01T18:04:34.767535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('EXPECTED OUTPUT')\nplot_img_bbox(torch_to_pil(img), target)","metadata":{"_uuid":"1a9603e9-3e8e-44b1-b6a8-e034c2d9d0f4","_cell_guid":"edc84627-15c3-41b9-b376-2d35f55f45fb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-01T18:03:22.472538Z","iopub.execute_input":"2021-07-01T18:03:22.473498Z","iopub.status.idle":"2021-07-01T18:03:23.299483Z","shell.execute_reply.started":"2021-07-01T18:03:22.473448Z","shell.execute_reply":"2021-07-01T18:03:23.298733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('MODEL OUTPUT')\nplot_img_bbox(torch_to_pil(img), prediction)","metadata":{"_uuid":"0c962c9c-43bd-4684-aa3f-27b844d93b5f","_cell_guid":"6f80532e-27cb-4bd3-9bdd-2aa624bc2b77","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-01T18:03:28.534557Z","iopub.execute_input":"2021-07-01T18:03:28.534908Z","iopub.status.idle":"2021-07-01T18:03:29.331294Z","shell.execute_reply.started":"2021-07-01T18:03:28.534879Z","shell.execute_reply":"2021-07-01T18:03:29.33023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nms_prediction = apply_nms(prediction, iou_thresh=0.2)\nprint('NMS APPLIED MODEL OUTPUT')\nplot_img_bbox(torch_to_pil(img), nms_prediction)","metadata":{"_uuid":"5a413100-c38c-42dd-8b96-415b287753f0","_cell_guid":"a2c34596-26e1-4737-8d03-2aed4642862c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-01T18:03:33.946823Z","iopub.execute_input":"2021-07-01T18:03:33.947148Z","iopub.status.idle":"2021-07-01T18:03:34.567163Z","shell.execute_reply.started":"2021-07-01T18:03:33.947119Z","shell.execute_reply":"2021-07-01T18:03:34.566145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nms_prediction = apply_nms(nms_prediction, iou_thresh=0.2)\nprint('NMS APPLIED MODEL OUTPUT')\nplot_img_bbox(torch_to_pil(img), nms_prediction)","metadata":{"_uuid":"56c54ac0-2048-48bb-84f8-bbe1249fe352","_cell_guid":"c33fb4be-30f4-479c-9961-0575503efc84","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-01T18:03:38.752571Z","iopub.execute_input":"2021-07-01T18:03:38.752913Z","iopub.status.idle":"2021-07-01T18:03:39.483349Z","shell.execute_reply.started":"2021-07-01T18:03:38.752882Z","shell.execute_reply":"2021-07-01T18:03:39.482556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = Singapore_maritime_ds(test_dir, 1920, 1080, transforms= get_transform(train=True))\n# pick one image from the test set\nimg, target = test_dataset[20]\n# put the model in evaluation mode\nmodel.eval()\nwith torch.no_grad():\n    prediction = model([img.to(device)])[0]\n    \nprint('EXPECTED OUTPUT\\n')\nplot_img_bbox(torch_to_pil(img), target)\nprint('MODEL OUTPUT\\n')\nnms_prediction = apply_nms(prediction, iou_thresh=0.01)\n\nplot_img_bbox(torch_to_pil(img), nms_prediction)","metadata":{"_uuid":"6bb641e0-097d-422a-988f-df3c5cbe07d5","_cell_guid":"651f456c-3668-496b-b51d-9e42494f01ba","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-01T18:03:46.529279Z","iopub.execute_input":"2021-07-01T18:03:46.529593Z","iopub.status.idle":"2021-07-01T18:03:46.807506Z","shell.execute_reply.started":"2021-07-01T18:03:46.529564Z","shell.execute_reply":"2021-07-01T18:03:46.805644Z"},"trusted":true},"execution_count":null,"outputs":[]}]}