{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"_kg_hide-input":false},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"#hide some warnings :p\nimport sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading Data And Exploring Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We have two files basically.\n1. Data of people who boarded titanic and outcome of wether they survived or not. (train data)\n2. Data of people who boarded titanic and task of this competition is to predict their outcome i.e wether they survived or not.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":false},"cell_type":"code","source":"kaggle_path = '../input/titanic/'\ntrain = pd.read_csv(kaggle_path + 'train.csv')\ntest = pd.read_csv(kaggle_path + 'test.csv')\nsubmission = pd.read_csv(kaggle_path + 'gender_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that we should always check train and test data sets simultaneously to gain more insights and avoid inconsistency issues later on.","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"print(train.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"print(test.shape)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"test.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<big> The Missing Data: </big>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. It seems like the 'Cabin' column has immense number of empty values (but dont worry we will just drop it at the end :D )\n2. The 'Age' column is another issue to deal with as it has huge amount of N/As as well\n3. There's is one row empty 'Fare' value in test data. \n\nLet's see what we can do about this.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"*(Observe that there is no empty 'Fare' value in train dataset. If we had not explored test set and had made model without checking test set, it would have thrown error when evaluating the test set.)*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"(first of all it is not that great feature analysis but still it will help create a good consisten model!!)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# *Exploring names:*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Name'][0:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you take a good look at names, every name tells us 3 things:\n1. The family name\n2. The title of name such as Mr. Mrs. Miss. etc.\n3. The actual name of person\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"titles = ['Mr.', 'Mrs.', 'Miss.', 'Ms.', 'Master.', 'Dr.']   #creating list of relevant titles","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Names with missing ages insight:\\n')\ntotal = 0\nfor t in titles:\n    count1 = train['Name'][train['Age'].isna()][train['Name'].str.find(t)!=-1].count()\n    count2 = test['Name'][test['Age'].isna()][test['Name'].str.find(t)!=-1].count()\n    print('Names having', t, 'in them are:', count1+count2)\n    total += count1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The name of N/A(empty) values in our data has these titles:\n1. Mr.  \n2. Mrs.\n3. Miss.\n4. Master. \n5. Dr.\n\nMaster is fancy way of calling little boys who are not worthy of being called a Mister yet :) ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"(This can be verified by this code)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"total == train['Age'].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's try to compute the mean age of Names with titles who do not have missing ages.\n\nThis fancy code computes average age of various titles from both train and test datasets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"age_dict = {}\nfor t in titles:\n    # Mean of a title in train set\n    trm = train['Age'][train['Name'].str.find(t)!=-1][train['Age'].notna()].mean()\n    #mean of title in test set\n    tsm = test['Age'][test['Name'].str.find(t)!=-1][test['Age'].notna()].mean()\n    if np.isnan(trm):\n        trm = 0\n    if np.isnan(tsm):\n        tsm = 0\n    avg = round( (trm + tsm) / 2)\n    print('average age of Names having',t ,'in them is: ', avg)\n    age_dict[t] = avg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we should be having a cool nice dictionary of titles and their average ages.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"age_dict","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's fill the 'Age' of missing people with missing 'Age'. This fancy function will do that job.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def fill_na_names(df):\n    missing_ages = df['Name'][df['Age'].isna()]\n    index = df['Name'][df['Age'].isna()].index\n    for name,i in zip(missing_ages, index):\n        for ttl in age_dict:\n            if name.find(ttl) != -1:\n                df.loc[i, 'Age'] = age_dict[ttl]\n                \n            \nfill_na_names(train)\nfill_na_names(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see if the 'Age' was filled or not.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Age'].isna().sum(), test['Age'].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sweet!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# *Filling Fare*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"There was an empty 'Fare' value in test. I could have put average of fare or mode of fare but I decided to be smart about it! (I guess)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"m = test[test['Fare'].isna()]\ni = test[test['Fare'].isna()].index\nm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's see if he shares a ticket with someone else\ntest[test.Ticket=='3701']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No one else with that ticket.\n\nLet's just fill the Fare with mean based on his Pclass, Age, Embarked port, SibSp and Parch","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"m1 = test['Fare'][test['Pclass']==3].mean()\nm2 = test['Fare'][test['Age']> 50].mean()\nm3 = test['Fare'][test['Embarked']=='S'].mean()\nm4 = test['Fare'][test['Sex']=='male'].mean()\nm5 = test['Fare'][test['SibSp']==0].mean()\nm6 = test['Fare'][test['Parch']==0].mean()\n\navg = round( (m1+ m2+ m3 + m4 + m5 + m6) / 6 )\n\ntest.loc[i,'Fare'] = avg\nprint('Filled ', round(avg))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# *Creating An Additional Family Feature*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observe that every Name tells us their family names.\n\nA thought:\n- Rich families would have had more influence and thus they had higher chances of survival\n- Similarly poor families would have had less chances of survival comparatively\n\nSo it can be helpful to add a feature of Family that tell which Family does one belong to.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"*This fancy function pulls Family name from names of people and makes a new Family column*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_fam_col(df):\n    for n,ind in zip(df['Name'].values, df['Name'].index):\n        df.loc[ind, 'Family'] = n.split(',')[0]\n        \nadd_fam_col(train)\nadd_fam_col(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A cool family feature has been added!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Merging train and test datasets","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"It is integral to merge both train and test datasets before feeding input to the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([train, test], join='outer')\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's drop useless features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nuseless_feats = ['PassengerId', 'Survived', 'Name', 'SibSp', \n                 'Parch', 'Ticket', 'Cabin', 'Embarked']\ndf.drop(useless_feats, axis=1,inplace=True)\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's convert non-numeric features to categorical values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.get_dummies(df, drop_first=True)\ndf.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Time to get prepare data for our model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df[:train.shape[0]]\ntst = df[train.shape[0]:]\ny = train['Survived']\nprint(X.shape, tst.shape, y.shape)\nX.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And time to scale features to get their values as less as possible. This will speed up and improve the modeling process!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import scale\nX = scale(X)\ntst = scale(tst)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Creating Classification Models**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Finally!!**\n\n*We will try different models and see which one works best.*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I will use GridSearchCV which is a sweet feature you get from Sklearn. It basically allows you to run a model with different hyperparameters in a fast efficient way.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1. Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\nparameter_grid = {\n                 'max_depth' : [None, 10, 12, 20],\n                 'n_estimators': [None, 50, 10],\n                 'max_features': [None, 'sqrt'],\n                 'min_samples_split': [None, 2, 3, 10],\n                 'min_samples_leaf': [None, 1, 3, 10],\n                 'bootstrap': [True, False],\n                 }\n\nforest = RandomForestClassifier(n_jobs=2)\n\nM1 = GridSearchCV(forest,\n                           scoring='accuracy',\n                           param_grid=parameter_grid,\n                           cv=3,\n                           n_jobs=-1,\n                           verbose=1)\n\nM1.fit(X, y)\n\nparameters = M1.best_params_\nprint('Best score: ' , M1.best_score_ * 100)\nprint('Best estimator: ' , M1.best_estimator_)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Support Vector Classifier (SVM)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\n\nparameter_grid = {\n                 'kernel': [None, 'rbf', 'sigmoid', 'linear'],\n                    'C'  : [0,0.25,0.5,1,2,3,4],\n                 'gamma' : [None, 'auto', 0.01, 0.03, 0.1, 0.3, 1, 3, 5, 20, 50],\n                  'class_weight' : [None, 'balanced']\n    \n                 }\n\nM2 = GridSearchCV(SVC(),\n                      scoring = 'accuracy',\n                      cv=3,\n                      param_grid= parameter_grid,\n                      n_jobs=-1,\n                      verbose=1\n                     ).fit(X,y)\n\nM2.fit(X,y)\n\nprint('Best score: ', M2.best_score_ * 100)\nprint('Best estimator: ', M2.best_estimator_)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. K Nearest Neighbors","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\ngrid = { 'n_neighbors': [1, 5, 15, 25, 30, 40],\n        'weights': ['distance'],\n        'leaf_size': [None, 1, 3, 10, 25, 40],\n        'p':[1,2]\n       }\n\n\nM3 = GridSearchCV(KNeighborsClassifier(),\n                      scoring='accuracy',\n                      cv=3,\n                      param_grid=grid,\n                      n_jobs=-1,\n                      verbose=1\n)\n\nM3.fit(X, y)\n\nprint('Best score: ', M3.best_score_ * 100)\nprint('Best estimator: ', M3.best_estimator_)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. XGB Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\n\nparameter_grid = {\n                 'max_depth' : [None, 5, 7, 10 ],\n                 'max_delta_step': [None, 1, 2],\n                 'n_estimators': [None,10, 20, 30, 40],\n                 'colsample_bylevel': [None,0.2, 0.5, 0.8],\n                 'colsample_bytree': [None,0.2, 0.6],\n                 'subsample': [None,0.01,0.1, 0.3, 0.4,1],\n                 }\n\nM4 = GridSearchCV(XGBClassifier(),\n                               scoring='accuracy',\n                               param_grid=parameter_grid,\n                               cv=3,\n                               n_jobs=-1,\n                               verbose=1)\n\nM4.fit(X, y)\n\nprint('Best score: ', M4.best_score_ * 100)\nprint('Best estimator: ', M4.best_estimator_)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nparameter_grid = {\n    'C' : [0.01, 0.1, 0.5, 1, 2],\n    'penalty' : ['l1', 'l2', 'elasticnet'],\n    'class_weight' : [None, 'balanced'],\n    'solver' : ['newton-cg', 'lbfgs', 'sag', 'saga'],\n    'max_iter' : [100, 500, 1000]\n\n}\n\nM5 = GridSearchCV(LogisticRegression(),\n                               scoring='accuracy',\n                               param_grid=parameter_grid,\n                               cv=3,\n                               n_jobs=-1,\n                               verbose=1)\n\nM5.fit(X, y)\n\nprint('Best score: ', M5.best_score_ * 100)\nprint('Best estimator: ', M5.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Boosting Classifier","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"To score higher in kaggle competitions it usually is the case that final model is derived out of multiple previous models.\nI will try to make a boosting model by using Logistic Regression and SVM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\n\nparameter_grid = {\n    'base_estimator': [LogisticRegression(), SVC()],\n    'n_estimators' : [10, 20, 30, 40],\n}\n\nM6 = GridSearchCV(BaggingClassifier(),\n                               scoring='accuracy',\n                               param_grid=parameter_grid,\n                               cv=3,\n                               n_jobs=-1,\n                               verbose=1)\n\nM6.fit(X, y)\n\nprint('Best score: ', M6.best_score_ * 100)\nprint('Best estimator: ', M6.best_estimator_)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predicting Test Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Thing is that I kinda discovered the original outcome of test dataset :p <br>\nSo I will just compare my output with that instead of submitting the file everytime. <br> <br>\n*I hope that is legal because the results have been out there for a long time and they would have been removed, I meand Kaggle Competition holders must have saw that. Besides it is not a serious competition so I guess it is fine at the end. :)*\n\n(If you choose to upload that document straight up, woe to you! I mean what's the purpose of this competition then.)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = pd.read_csv('../input/titanic-leaked/titanic.csv')\noriginal = temp['Survived']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\npreds_M1 = M1.predict(tst)\npreds_M2 = M2.predict(tst)\npreds_M3 = M3.predict(tst)\npreds_M4 = M4.predict(tst)\npreds_M5 = M5.predict(tst)\npreds_M6 = M6.predict(tst)\n\n\nscore_M1 = accuracy_score(original, preds_M1)\nscore_M2 = accuracy_score(original, preds_M2)\nscore_M3 = accuracy_score(original, preds_M3)\nscore_M4 = accuracy_score(original, preds_M4)\nscore_M5 = accuracy_score(original, preds_M5)\nscore_M6 = accuracy_score(original, preds_M6)\n\nprint('score with model 1: ', score_M1*100)\nprint('score with model 2: ', score_M2*100)\nprint('score with model 3: ', score_M3*100)\nprint('score with model 4: ', score_M4*100)\nprint('score with model 5: ', score_M5*100)\nprint('score with model 6: ', score_M6*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Bruh this is seriously unexpected. <br>\nThe boosted model(BaggingClassifier) got 82% acuracy on test dataset!<br>\nBeginner's luck I must say!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"subm_dir = kaggle_path + 'gender_submission.csv'\nsubmit_file = pd.read_csv(subm_dir)\nsubmit_file['Survived'] = preds_M6\nsubmit_file.to_csv('gender_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hopefully rest of the score would be attained by using Cabin and Embarked features accurately.\nBut I got lower score with them included so I dropped them and got better results.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Wasn't so much of a special kernel but considering i did most of the stuff in it on my own and managed to end up in top 17% is a very good thing for me and I am very happy with it. Been only a month since I got into Machine Learing.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Also please do give advices to improve this model especially about the Cabin feature <br>\nIf you think I did any screw up, let me know about that too hehe.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Cheers! Have a nice day!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}