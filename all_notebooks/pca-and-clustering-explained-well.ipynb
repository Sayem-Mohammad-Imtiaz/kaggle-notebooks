{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Credit Card Clustering and Segmentation"},{"metadata":{},"cell_type":"markdown","source":"## This case requires to develop a customer segmentation to define marketing strategy. The sample Dataset summarizes the usage behavior of about 9000 active credit card holders during the last 6 months. The file is at a customer level with 18 behavioral variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# used to supress display of warnings\nimport warnings\n\n# os is used to provide a way of using operating system dependent functionality\n# We use it for setting working folder\nimport os\n\n# Pandas is used for data manipulation and analysis\nimport pandas as pd \n\n# Numpy is used for large, multi-dimensional arrays and matrices, along with mathematical operators on these arrays\nimport numpy as np\n\n# Matplotlib is a data visualization library for 2D plots of arrays, built on NumPy arrays \n# and designed to work with the broader SciPy stack\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib import pyplot\n\n# Seaborn is based on matplotlib, which aids in drawing attractive and informative statistical graphics.\nimport seaborn as sns\n\n\n## Scikit-learn features various classification, regression and clustering algorithms\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn.metrics import average_precision_score, confusion_matrix, accuracy_score, classification_report, plot_confusion_matrix\n\n\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt \nfrom sklearn.preprocessing import StandardScaler\nfrom IPython.display import display\nfrom sklearn.cluster import KMeans \nfrom sklearn.decomposition import PCA\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.metrics import homogeneity_score, completeness_score, \\\nv_measure_score, adjusted_rand_score, adjusted_mutual_info_score, silhouette_score\n%matplotlib inline\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# suppress display of warnings\nwarnings.filterwarnings('ignore')\n\n# display all dataframe columns\npd.options.display.max_columns = 50\n\n# to set the limit to 3 decimals\npd.options.display.float_format = '{:.7f}'.format\n\n# display all dataframe rows\npd.options.display.max_rows = None","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check a few observations and get familiar with the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/ccdata/CC GENERAL.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check the size and info of the data set."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- 8950 rows and 18 columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check for missing values. Impute the missing values if there is any."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'MINIMUM_PAYMENTS' and 'CREDIT_LIMIT' have null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"#imputing null values\n\ndf['CREDIT_LIMIT'].fillna((df['CREDIT_LIMIT'].median()), inplace=True)\ndf['MINIMUM_PAYMENTS'].fillna((df['MINIMUM_PAYMENTS'].median()), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Drop unnecessary columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop('CUST_ID', axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check correlation among features and comment your findings"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to plot correlation matrix\n\ndef correlation_plot(df):\n    \n    corr = abs(df.corr()) # correlation matrix\n    lower_triangle = np.tril(corr, k = -1)  # select only the lower triangle of the correlation matrix\n    mask = lower_triangle == 0  # to mask the upper triangle in the following heatmap\n\n    plt.figure(figsize = (15,10))  # setting the figure size\n    sns.set_style(style = 'white')  # Setting it to white so that we do not see the grid lines\n    sns.heatmap(lower_triangle, center=0.5, cmap= 'Blues', xticklabels = corr.index,\n                yticklabels = corr.columns,cbar = False, annot= True, linewidths= 1, mask = mask)   # Da Heatmap\n    plt.show()\n    \ncorrelation_plot(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check distribution of features and comment your findings"},{"metadata":{"trusted":true},"cell_type":"code","source":"#univariate analysis\n\nposition = 1\nplt.figure(figsize=(15,45))\n\nfor column in ['BALANCE', 'BALANCE_FREQUENCY', 'PURCHASES', 'ONEOFF_PURCHASES',\n       'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE', 'PURCHASES_FREQUENCY',\n       'ONEOFF_PURCHASES_FREQUENCY', 'PURCHASES_INSTALLMENTS_FREQUENCY',\n       'CASH_ADVANCE_FREQUENCY', 'CASH_ADVANCE_TRX', 'PURCHASES_TRX',\n       'CREDIT_LIMIT', 'PAYMENTS', 'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT',\n       'TENURE']:    \n    plt.subplot(9,2,position)\n    if column in ['TENURE']:\n        sns.countplot(df[column])\n    else:\n        sns.distplot(df[column])\n    plt.title(column)        \n    position += 1\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Standardize the data using appropriate methods"},{"metadata":{"trusted":true},"cell_type":"code","source":"#normalize the dataset\nscaler = StandardScaler()\ndata = scaler.fit_transform(df) # scaling the data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Build a k-means algorithm for clustering credit card data"},{"metadata":{},"cell_type":"markdown","source":"### Build k means model on various k values and plot the inertia against various k values"},{"metadata":{"trusted":true},"cell_type":"code","source":"sse = {}\nfor k in range(1, 10):\n    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(data)\n    sse[k] = kmeans.inertia_ # Inertia: Sum of distances of samples to their closest cluster center\nplt.figure()\nplt.plot(list(sse.keys()), list(sse.values()))\nplt.xlabel(\"Number of cluster\")\nplt.ylabel(\"SSE\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate the model using Silhouette coefficient"},{"metadata":{"trusted":true},"cell_type":"code","source":"silhouette_scores = [] \n\nfor n_cluster in range(2, 8):\n    silhouette_scores.append( \n        silhouette_score(data, KMeans(n_clusters = n_cluster).fit_predict(data))) \n    \n# Plotting a bar graph to compare the results \nk = [2, 3, 4, 5, 6,7] \nplt.bar(k, silhouette_scores) \nplt.xlabel('Number of clusters', fontsize = 10) \nplt.ylabel('Silhouette Score', fontsize = 10) \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot an elbow plot to find the optimal value of k"},{"metadata":{"trusted":true},"cell_type":"code","source":"ks = range(1, 10)\ninertias = [] # initializing an empty array\n\nfor k in ks:\n    model = KMeans(n_clusters=k)\n    model.fit(data)\n    inertias.append(model.inertia_)\n\nplt.figure(figsize=(8,5))\nplt.style.use('bmh')\nplt.plot(ks, inertias, '-o')\nplt.xlabel('Number of clusters, k')\nplt.ylabel('Inertia')\nplt.xticks(ks)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Which k value gives the best result?"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"- k = 3 gives the highest silhouette score and also comparable inertia value"},{"metadata":{},"cell_type":"markdown","source":"### Apply PCA to the dataset and repeat the above steps on the new features generated using PCA."},{"metadata":{"trusted":true},"cell_type":"code","source":"#variance explained with the number of features\npca = PCA(random_state=123)\npca.fit(data)\nfeatures = range(pca.n_components_)\n\nplt.figure(figsize=(8,4))\nplt.bar(features[:15], pca.explained_variance_[:15], color='lightskyblue')\nplt.xlabel('PCA feature')\nplt.ylabel('Variance')\nplt.xticks(features[:15])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying PCA\n\nfrom sklearn.decomposition import PCA\n\n# Reducing the dimensions of the data \npca = PCA(n_components = 2) \nx_principal = pca.fit_transform(df) \nx_principal = pd.DataFrame(x_principal) \nx_principal.columns = ['P1', 'P2'] \n  \nx_principal.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sse = {}\nfor k in range(1, 10):\n    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(x_principal)\n    sse[k] = kmeans.inertia_ # Inertia: Sum of distances of samples to their closest cluster center\nplt.figure()\nplt.plot(list(sse.keys()), list(sse.values()))\nplt.xlabel(\"Number of cluster\")\nplt.ylabel(\"SSE\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"silhouette_scores = [] \n\nfor n_cluster in range(2, 8):\n    silhouette_scores.append( \n        silhouette_score(x_principal, KMeans(n_clusters = n_cluster).fit_predict(x_principal))) \n    \n# Plotting a bar graph to compare the results \nk = [2, 3, 4, 5, 6,7] \nplt.bar(k, silhouette_scores) \nplt.xlabel('Number of clusters', fontsize = 10) \nplt.ylabel('Silhouette Score', fontsize = 10) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create a new column as a cluster label in the original data frame and perform cluster analysis. Check the correlation of cluster labels with various features and mention your inferences"},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 42)\ny_kmeans = kmeans.fit_predict(df)\ndf['Cluster'] = y_kmeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to plot correlation matrix\n\ndef correlation_plot(df):\n    \n    corr = abs(df.corr()) # correlation matrix\n    lower_triangle = np.tril(corr, k = -1)  # select only the lower triangle of the correlation matrix\n    mask = lower_triangle == 0  # to mask the upper triangle in the following heatmap\n\n    plt.figure(figsize = (15,10))  # setting the figure size\n    sns.set_style(style = 'white')  # Setting it to white so that we do not see the grid lines\n    sns.heatmap(lower_triangle, center=0.5, cmap= 'Blues', xticklabels = corr.index,\n                yticklabels = corr.columns,cbar = False, annot= True, linewidths= 1, mask = mask)   # Da Heatmap\n    plt.show()\n    \ncorrelation_plot(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cluster Analysis\n\ndf1 = df.values\nplt.scatter(df1[y_kmeans == 0][:,0], df1[y_kmeans == 0][:,12], c = 'red', label = 'Cluster 1')\nplt.scatter(df1[y_kmeans == 1][:,0], df1[y_kmeans == 1][:,12], c = 'blue', label = 'Cluster 2')\nplt.scatter(df1[y_kmeans == 2][:,0], df1[y_kmeans == 2][:,12], c = 'green', label = 'Cluster 3')\n\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroids')\nplt.title('Credit card Clustering')\nplt.xlabel('Balance')\nplt.ylabel('Credit Limit')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(5,8))\n\npp = sns.pairplot(data=df,\n                  y_vars=['Cluster'],\n                  x_vars=['BALANCE', 'BALANCE_FREQUENCY', 'PURCHASES', 'ONEOFF_PURCHASES',\n       'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE', 'PURCHASES_FREQUENCY',\n       'ONEOFF_PURCHASES_FREQUENCY', 'PURCHASES_INSTALLMENTS_FREQUENCY',\n       'CASH_ADVANCE_FREQUENCY', 'CASH_ADVANCE_TRX', 'PURCHASES_TRX',\n       'CREDIT_LIMIT', 'PAYMENTS', 'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT',\n       'TENURE'])\nplt.show()\n\n#save plot\npp.savefig('pp.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We can see that our Inertia and silhoutte scores drastically improve after doing Principal Component Analysis with n_component = 2 along with K means algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}