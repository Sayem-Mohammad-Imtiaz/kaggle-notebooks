{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train  = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train : \",train.shape)\nprint(\"*\"*10)\nprint(\"Test : \",test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(10)\n#train.tail(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop_duplicates().reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x= \"target\",data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.keyword.nunique(), test.keyword.nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.countplot(x=\"keyword\",\n              data=train,\n              order=train.keyword.value_counts().iloc[:15].index\n             )\nplt.title('Top Keywords')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dist = train[train.target==1].keyword.value_counts().head()\n#dist\nplt.figure(figsize=(9,6))\nsns.barplot(dist,dist.index)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nondist = train[train.target==0].keyword.value_counts().head()\n#nondist\nplt.figure(figsize=(9,6))\nsns.barplot(nondist,nondist.index)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distribution_dist = train.groupby('keyword').mean()['target'].sort_values(ascending=False).head(10)\n#distribution_dist\n\nplt.figure(figsize=(9,6))\nsns.barplot(distribution_dist,distribution_dist.index)\nplt.title(\"Distribution of keywords for higher risk\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distribution_nondist = train.groupby('keyword').mean()['target'].sort_values().head(10)\n#distribution_nondist\nplt.figure(figsize=(9,6))\nsns.barplot(distribution_nondist,distribution_nondist.index)\nplt.title(\"Distribution of Non Disasters keywords for lower risk\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (train.location.nunique(), test.location.nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(9,6))\nsns.countplot(y=train.location, order = train.location.value_counts().iloc[:15].index)\nplt.title('Top locations')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for clmn in ['keyword','location']:\n    train[clmn]= train[clmn].fillna('None')\n    test[clmn]= test[clmn].fillna('None')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def setlocname(x):\n    if x == 'None':\n        return 'None'\n    elif x == 'Earth' or x =='Worldwide' or x == 'Everywhere':\n        return 'World'\n    elif 'New York' in x or 'NYC' in x:\n        return 'New York'    \n    elif 'London' in x:\n        return 'London'\n    elif 'Mumbai' in x:\n        return 'Mumbai'\n    elif 'Washington' in x and 'D' in x and 'C' in x:\n        return 'Washington DC'\n    elif 'San Francisco' in x:\n        return 'San Francisco'\n    elif 'Los Angeles' in x:\n        return 'Los Angeles'\n    elif 'Seattle' in x:\n        return 'Seattle'\n    elif 'Chicago' in x:\n        return 'Chicago'\n    elif 'Toronto' in x:\n        return 'Toronto'\n    elif 'Sacramento' in x:\n        return 'Sacramento'\n    elif 'Atlanta' in x:\n        return 'Atlanta'\n    elif 'California' in x:\n        return 'California'\n    elif 'Florida' in x:\n        return 'Florida'\n    elif 'Texas' in x:\n        return 'Texas'\n    elif 'United States' in x or 'USA' in x:\n        return 'USA'\n    elif 'United Kingdom' in x or 'UK' in x or 'Britain' in x:\n        return 'UK'\n    elif 'Canada' in x:\n        return 'Canada'\n    elif 'India' in x:\n        return 'India'\n    elif 'Kenya' in x:\n        return 'Kenya'\n    elif 'Nigeria' in x:\n        return 'Nigeria'\n    elif 'Australia' in x:\n        return 'Australia'\n    elif 'Indonesia' in x:\n        return 'Indonesia'\n    else: return 'Others'\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['locations'] = train['location'].apply(lambda x: setlocname(str(x)))\ntest['locations'] = test['location'].apply(lambda x:setlocname(str(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(9,6))\nsns.countplot(y=train.locations, order = train.locations.value_counts().iloc[:15].index)\nplt.title('Top Updated locations')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_l2 = train.groupby('locations').mean()['target'].sort_values(ascending=False)\nplt.figure(figsize=(14,6))\nsns.barplot(x=top_l2.index, y=top_l2)\nplt.axhline(np.mean(train.target))\nplt.xticks(rotation=80)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leak = pd.read_csv(\"../input/disasters-on-social-media/socialmedia-disaster-tweets-DFE.csv\", encoding='latin_1')\nleak['target'] = (leak['choose_one']=='Relevant').astype(int)\nleak['id'] = leak.index\nleak = leak[['id', 'target','text']]\nmerged_df = pd.merge(test, leak, on='id')\nsub1 = merged_df[['id', 'target']]\nsub1.to_csv('submit_1.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Clean text data berfore converting as vector\nimport re\n    \ndef preprocessing_text(text):\n    text = re.sub(r'https?://\\S+','',text)\n    text = re.sub(r'\\n',' ',text)\n    text = re.sub('\\s+',' ',text).strip()\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummystr = train.loc[417,'text']\nprint(dummystr)\nprint(preprocessing_text(dummystr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_hashtags(tweet):\n    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"#\\w+\", tweet)]) or 'no'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_mentions(tweet):\n    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"@\\w+\", tweet)]) or 'no'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_links(tweet):\n    return \" \".join([match.group(0)[:] for match in re.finditer(r\"https?://\\S+\", tweet)]) or 'no'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_process(df):\n    df['text_clean'] = df['text'].apply(lambda x: preprocessing_text(x))\n    df['hash'] = df['text'].apply(lambda x: find_hashtags(x))\n    df['mention'] = df['text'].apply(lambda x: find_mentions(x))\n    df['links'] = df['text'].apply(lambda x: find_links(x))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = text_process(train)\ntest = text_process(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()\nfrom wordcloud import STOPWORDS\nimport string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_wordcloud(df):\n    df['text_len'] = df['text_clean'].apply(len)\n    df['wordcount'] = df['text_clean'].apply(lambda x : len(str(x).split()))\n    df['stop_word_count'] = df['text_clean'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n    df['punctuation_count'] = df['text_clean'].apply(lambda x :len([c for c in str(x) if c in string.punctuation]))\n    df['hashtag_count'] = df['hash'].apply(lambda x :len(str(x).split()))\n    df['mention_count'] = df['mention'].apply(lambda x:len(str(x).split()))\n    df['link_count'] = df['links'].apply(lambda x:len(str(x).split()))\n    df['caps_count'] = df['text_clean'].apply(lambda x: sum(1 for c in str(x) if c.isupper()))\n    df['caps_ratio'] = df['caps_count'] / df['text_len']\n    return df\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = make_wordcloud(train)\ntest = make_wordcloud(test)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TCor = train.corr()\nmask = np.triu(np.ones_like(TCor, dtype=np.bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(TCor, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.corr()['target'].drop('target').sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#http://www.clker.com/cliparts/a/1/a/5/1242249442627091102Flammable-symbol.svg.med.png\n#from PIL import Image\n#import requests\n#from wordcloud import WordCloud\n#mask = np.array(Image.open(requests.get('http://www.clker.com/cliparts/a/1/a/5/1242249442627091102Flammable-symbol.svg.med.png', stream=True).raw))\n\n# This function takes in your text and your mask and generates a wordcloud. \n#def generate_wordcloud(words, mask):\n #   word_cloud = WordCloud(width = 512, height = 512, background_color='white', stopwords=STOPWORDS, mask=mask).generate(words)\n #   plt.figure(figsize=(10,8),facecolor = 'white', edgecolor='blue')\n #   plt.imshow(word_cloud)\n #   plt.axis('off')\n #   plt.tight_layout(pad=0)\n#  plt.show()\n\n#generate_wordcloud(train['text_clean'], mask)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import category_encoders as ce","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['keyword','locations']\nencoder = ce.TargetEncoder(cols=features)\nencoder.fit(train[features],train['target'])\n\ntrain = train.join(encoder.transform(train[features]).add_suffix('_target'))\ntest = test.join(encoder.transform(test[features]).add_suffix('_target'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vec_links = CountVectorizer(min_df=5,analyzer='word',token_pattern = r'https?://\\S+')\nlink_vec = vec_links.fit_transform(train['links'])\nlink_vec_test = vec_links.transform(test['links'])\nX_train_link = pd.DataFrame(link_vec.toarray(), columns=vec_links.get_feature_names())\nX_test_link = pd.DataFrame(link_vec_test.toarray(), columns=vec_links.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vec_men = CountVectorizer(min_df = 5)\nmen_vec = vec_men.fit_transform(train['mention'])\nmen_vec_test = vec_men.transform(test['mention'])\nX_train_men = pd.DataFrame(men_vec.toarray(), columns=vec_men.get_feature_names())\nX_test_men = pd.DataFrame(men_vec_test.toarray(), columns=vec_men.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vec_hash = CountVectorizer(min_df = 5)\nhash_vec = vec_hash.fit_transform(train['hash'])\nhash_vec_test = vec_hash.transform(test['hash'])\nX_train_hash = pd.DataFrame(hash_vec.toarray(), columns=vec_hash.get_feature_names())\nX_test_hash = pd.DataFrame(hash_vec_test.toarray(), columns=vec_hash.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hash_rank = (X_train_hash.transpose().dot(train['target']) / X_train_hash.sum(axis=0)).sort_values(ascending=False)\nprint('Hashtags with which 100% of Tweets are disasters: ')\nprint(list(hash_rank[hash_rank==1].index))\nprint('Total: ' + str(len(hash_rank[hash_rank==1])))\nprint('Hashtags with which 0% of Tweets are disasters: ')\nprint(list(hash_rank[hash_rank==0].index))\nprint('Total: ' + str(len(hash_rank[hash_rank==0])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vec_text = TfidfVectorizer(min_df = 10 ,ngram_range=(1,2),stop_words= 'english')\ntext_vec = vec_text.fit_transform(train['text_clean'])\ntext_vec_test = vec_text.transform(test['text_clean'])\nX_train_text = pd.DataFrame(text_vec.toarray(), columns=vec_text.get_feature_names())\nX_test_text = pd.DataFrame(text_vec_test.toarray(), columns=vec_text.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train_text.shape,X_test_text.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.join(X_train_link, rsuffix='_link')\ntrain = train.join(X_train_men, rsuffix='_mention')\ntrain = train.join(X_train_hash ,rsuffix='_hashtag')\ntrain = train.join(X_train_text ,rsuffix='_text')\n\ntest = test.join(X_test_link, rsuffix='_link')\ntest = test.join(X_test_men, rsuffix='_mention')\ntest = test.join(X_test_hash ,rsuffix='_hashtag')\ntest = test.join(X_test_text ,rsuffix='_text')\n\nprint (train.shape, test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Logistic","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_to_drop = ['id', 'keyword','location','text','locations','text_clean', 'hash', 'mention','links']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scale = MinMaxScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()\n#/X_train = train.drop(columns = features_to_drop + ['target'] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.drop(columns = features_to_drop + ['target'] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = test.drop(columns = features_to_drop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train.target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(solver='liblinear', random_state=777)\npipeline = Pipeline ([('scale', scale),('lr',lr)])\npipeline.fit(X_train,y_train)\ny_test = pipeline.predict(X_test)\n\nsub_sample = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nsubmit = sub_sample.copy()\nsubmit.target = y_test\nsubmit.to_csv('submit_lr.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('Accuracy for Train: %.4f' % pipeline.score(X_train, y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\nprint ('F1 score: %.4f' % f1_score(y_train, pipeline.predict(X_train)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\npd.DataFrame(confusion_matrix(y_train, pipeline.predict(X_train)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import ShuffleSplit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = ShuffleSplit(n_splits=12, test_size=0.2,random_state=143)\ncv_score = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='f1')\n\nprint('Cross validation F-1 score: %.3f' %np.mean(cv_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFECV\n\nsteps = 20\nn_features = len(X_train.columns)\nX_range = np.arange(n_features - (int(n_features/steps)) * steps, n_features+1, steps)\n\nrfecv = RFECV(estimator=lr, step=steps, cv=cv, scoring='f1')\n\npipeline2 = Pipeline([('scale',scale), ('rfecv', rfecv)])\npipeline2.fit(X_train, y_train)\nplt.figure(figsize=(10,6))\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(np.insert(X_range, 0, 1), rfecv.grid_scores_)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_features = X_train.columns[rfecv.ranking_ == 1]\nX_train2 = X_train[selected_features]\nX_test2 = X_test[selected_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline.fit(X_train2, y_train)\ncv2 = ShuffleSplit(n_splits=5, test_size=0.2, random_state=456)\ncv_score2 = cross_val_score(pipeline, X_train2, y_train, cv=cv2, scoring='f1')\nprint('Cross validation F-1 score: %.3f' %np.mean(cv_score2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\ngrid={\"C\":np.logspace(-2,2,5), \"penalty\":[\"l1\",\"l2\"]}\nlr_cv = GridSearchCV(LogisticRegression(solver='liblinear', random_state=20), grid, cv=cv2, scoring = 'f1')\n\npipeline_grid = Pipeline([('scale',scale), ('gridsearch', lr_cv),])\n\npipeline_grid.fit(X_train2, y_train)\n\nprint(\"Best parameter: \", lr_cv.best_params_)\nprint(\"F-1 score: %.3f\" %lr_cv.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test2 = pipeline_grid.predict(X_test2)\nsubmit2 = sub_sample.copy()\nsubmit2.target = y_test2\nsubmit2.to_csv('submit_lr2.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat = pipeline_grid.predict_proba(X_train2)[:,1]\nchecker = train.loc[:,['text','keyword','location','target']]\nchecker['pred_prob'] = y_hat\nchecker['error'] = np.abs(checker['target'] - checker['pred_prob'])\n\n# Top 50 mispredicted tweets\nerror50 = checker.sort_values('error', ascending=False).head(50)\nerror50 = error50.rename_axis('id').reset_index()\nerror50.target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.max_colwidth = 200\n\nerror50.loc[0:10,['text','target','pred_prob']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}