{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2021-06-25T18:24:52.611256Z","iopub.execute_input":"2021-06-25T18:24:52.611902Z","iopub.status.idle":"2021-06-25T18:25:00.178597Z","shell.execute_reply.started":"2021-06-25T18:24:52.611639Z","shell.execute_reply":"2021-06-25T18:25:00.177614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import nn \nfrom torch import optim\nimport torchvision.transforms as transforms\nimport torch, torchvision\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchsummary import summary\n\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport os\nfrom matplotlib import pyplot as plt\nimport albumentations as A\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-06-25T18:25:00.180452Z","iopub.execute_input":"2021-06-25T18:25:00.180793Z","iopub.status.idle":"2021-06-25T18:25:03.399156Z","shell.execute_reply.started":"2021-06-25T18:25:00.18075Z","shell.execute_reply":"2021-06-25T18:25:03.398251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/makeup-lips-segmentation-28k-samples/set-lipstick-original/list.csv')\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T18:25:03.401073Z","iopub.execute_input":"2021-06-25T18:25:03.401427Z","iopub.status.idle":"2021-06-25T18:25:03.505877Z","shell.execute_reply.started":"2021-06-25T18:25:03.401374Z","shell.execute_reply":"2021-06-25T18:25:03.505134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMAGES_PATH = '/kaggle/input/makeup-lips-segmentation-28k-samples/set-lipstick-original/720p/'\nMASKS_PATH = '/kaggle/input/makeup-lips-segmentation-28k-samples/set-lipstick-original/mask'","metadata":{"execution":{"iopub.status.busy":"2021-06-25T18:25:03.508761Z","iopub.execute_input":"2021-06-25T18:25:03.509016Z","iopub.status.idle":"2021-06-25T18:25:03.514921Z","shell.execute_reply.started":"2021-06-25T18:25:03.508985Z","shell.execute_reply":"2021-06-25T18:25:03.514114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imgs_set = set(os.listdir(IMAGES_PATH))\nmasks_set = set(os.listdir(MASKS_PATH))\n\nimgs_set = set(''.join(filter(lambda x: x.isdigit(), i)) for i in imgs_set)\nmasks_set = set(''.join(filter(lambda x: x.isdigit(), i)) for i in masks_set)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T18:25:03.517862Z","iopub.execute_input":"2021-06-25T18:25:03.518259Z","iopub.status.idle":"2021-06-25T18:25:04.741975Z","shell.execute_reply.started":"2021-06-25T18:25:03.518225Z","shell.execute_reply":"2021-06-25T18:25:04.740796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(imgs_set), len(masks_set)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T18:25:04.745806Z","iopub.execute_input":"2021-06-25T18:25:04.746073Z","iopub.status.idle":"2021-06-25T18:25:04.756397Z","shell.execute_reply.started":"2021-06-25T18:25:04.746047Z","shell.execute_reply":"2021-06-25T18:25:04.75516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(imgs_set.difference(masks_set)), len(masks_set.difference(imgs_set))","metadata":{"execution":{"iopub.status.busy":"2021-06-25T18:25:04.758745Z","iopub.execute_input":"2021-06-25T18:25:04.759161Z","iopub.status.idle":"2021-06-25T18:25:04.78026Z","shell.execute_reply.started":"2021-06-25T18:25:04.75912Z","shell.execute_reply":"2021-06-25T18:25:04.779361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Есть 54 изображения, для которых нет масок. Уберем их из датафрейма.","metadata":{}},{"cell_type":"code","source":"not_mask = imgs_set.difference(masks_set)\n\nnot_mask = [f'image{i}.jpg' for i in not_mask]\nnot_mask","metadata":{"execution":{"iopub.status.busy":"2021-06-25T18:25:04.785795Z","iopub.execute_input":"2021-06-25T18:25:04.787747Z","iopub.status.idle":"2021-06-25T18:25:04.801156Z","shell.execute_reply.started":"2021-06-25T18:25:04.787695Z","shell.execute_reply":"2021-06-25T18:25:04.800285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.loc[~df['filename'].isin(not_mask)]\ndf.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T18:25:04.806047Z","iopub.execute_input":"2021-06-25T18:25:04.808089Z","iopub.status.idle":"2021-06-25T18:25:04.831232Z","shell.execute_reply.started":"2021-06-25T18:25:04.808046Z","shell.execute_reply":"2021-06-25T18:25:04.830318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T18:25:04.835387Z","iopub.execute_input":"2021-06-25T18:25:04.837409Z","iopub.status.idle":"2021-06-25T18:25:04.857915Z","shell.execute_reply.started":"2021-06-25T18:25:04.83735Z","shell.execute_reply":"2021-06-25T18:25:04.857112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx = np.random.randint(len(df))\n\nsample = df.iloc[idx]\n\nimg_path = os.path.join(IMAGES_PATH, sample['filename'])\nmask_path = os.path.join(MASKS_PATH, sample['mask'])\n\nimg = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_RGB2BGR)\nmask = cv2.imread(mask_path)[:, :, 1]\n\nplt.imshow(img)\nplt.show()\n\nplt.imshow(mask, cmap='gray')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T18:25:04.861289Z","iopub.execute_input":"2021-06-25T18:25:04.863325Z","iopub.status.idle":"2021-06-25T18:25:05.47363Z","shell.execute_reply.started":"2021-06-25T18:25:04.863281Z","shell.execute_reply":"2021-06-25T18:25:05.472686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LipsSegmentationDataset(Dataset):\n    \n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n    \n    def __getitem__(self, idx):\n        data = self.df.iloc[idx]\n        \n        img_path = os.path.join(IMAGES_PATH, data['filename'])\n        mask_path = os.path.join(MASKS_PATH, data['mask'])\n        \n        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_RGB2BGR)\n        mask = cv2.imread(mask_path)[:, :, 1]\n        \n        if self.transform:\n            img, mask = self.transform(img, mask)\n            \n        return img, mask\n    \n    def __len__(self):\n        return len(self.df)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T18:25:05.47513Z","iopub.execute_input":"2021-06-25T18:25:05.475474Z","iopub.status.idle":"2021-06-25T18:25:05.483013Z","shell.execute_reply.started":"2021-06-25T18:25:05.475429Z","shell.execute_reply":"2021-06-25T18:25:05.481698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n\ntrain_df.reset_index(drop=True, inplace=True)\ntest_df.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T18:25:05.484621Z","iopub.execute_input":"2021-06-25T18:25:05.485065Z","iopub.status.idle":"2021-06-25T18:25:05.503694Z","shell.execute_reply.started":"2021-06-25T18:25:05.485028Z","shell.execute_reply":"2021-06-25T18:25:05.502784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T18:25:05.506276Z","iopub.execute_input":"2021-06-25T18:25:05.506605Z","iopub.status.idle":"2021-06-25T18:25:05.521073Z","shell.execute_reply.started":"2021-06-25T18:25:05.506579Z","shell.execute_reply":"2021-06-25T18:25:05.520084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T18:25:05.52255Z","iopub.execute_input":"2021-06-25T18:25:05.523006Z","iopub.status.idle":"2021-06-25T18:25:05.537903Z","shell.execute_reply.started":"2021-06-25T18:25:05.522971Z","shell.execute_reply":"2021-06-25T18:25:05.536939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Augmentation","metadata":{}},{"cell_type":"code","source":"transform = A.Compose([\n    A.RandomBrightnessContrast(),\n    A.HorizontalFlip(),\n    A.Rotate(limit=90, border_mode=0, value=0),\n    A.GaussNoise(var_limit=(50.0, 100.0)),\n    A.ChannelShuffle(),\n    A.OpticalDistortion(), \n    A.GridDistortion(distort_limit=(-0.15, 0.15), border_mode=0, value=0), \n    A.Transpose()\n])\n\nto_tensor = transforms.ToTensor()\n\ndef tensor_img_mask(img, mask, shape=(256, 256)):\n    img, mask = cv2.resize(img, shape), cv2.resize(mask, shape)\n    return to_tensor(img), to_tensor(mask)\n\ndef transform_img_mask(img, mask, shape=(256, 256)):\n    img, mask = cv2.resize(img, shape), cv2.resize(mask, shape)\n    transformed = transform(image=img, mask=mask)\n    img, mask = transformed['image'], transformed['mask']\n    img, mask = to_tensor(img), to_tensor(mask)\n    return img, mask","metadata":{"execution":{"iopub.status.busy":"2021-06-25T18:25:05.53933Z","iopub.execute_input":"2021-06-25T18:25:05.539721Z","iopub.status.idle":"2021-06-25T18:25:05.550627Z","shell.execute_reply.started":"2021-06-25T18:25:05.53967Z","shell.execute_reply":"2021-06-25T18:25:05.549624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = LipsSegmentationDataset(train_df, transform=transform_img_mask)\ntest_dataset = LipsSegmentationDataset(test_df, transform=tensor_img_mask)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T18:25:05.552044Z","iopub.execute_input":"2021-06-25T18:25:05.552726Z","iopub.status.idle":"2021-06-25T18:25:05.559019Z","shell.execute_reply.started":"2021-06-25T18:25:05.552664Z","shell.execute_reply":"2021-06-25T18:25:05.557896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx = int(np.random.random() * len(train_dataset))\n\nimg, mask = train_dataset[idx]\n\nplt.imshow(img.permute(1, 2, 0))\nplt.show()\n\nplt.imshow(mask.permute(1, 2, 0), cmap='gray')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T18:25:05.560374Z","iopub.execute_input":"2021-06-25T18:25:05.560903Z","iopub.status.idle":"2021-06-25T18:25:05.920176Z","shell.execute_reply.started":"2021-06-25T18:25:05.560865Z","shell.execute_reply":"2021-06-25T18:25:05.919066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Реализуем сеть с архитектурой **LinkNet**.","metadata":{}},{"cell_type":"markdown","source":"Зададим классы блоков, из которых будем строить сеть.","metadata":{}},{"cell_type":"markdown","source":"**EncoderBlock:**\n\n* сверточный слой (ядро (4, 4), шаг (2, 2), padding (1, 1));\n* функция активации LeakyReLU (чтобы не занулять отрицательные значения);\n* слой BatchNormalization.\n\nТаким образом, на выходе блока получаем тензор размерностью:\n\noutput_shape = (n_samples, n_channels, input_width/2, input_height/2)","metadata":{}},{"cell_type":"markdown","source":"**DecoderBlock:**\n\n* слой обратной свертки (ядро (4, 4), шаг (2, 2), padding (1, 1));\n* функция активации LeakyReLU (чтобы не занулять отрицательные значения);\n* слой BatchNormalization.\n\nТаким образом, на выходе блока получаем тензор размерностью:\n\noutput_shape = (n_samples, n_channels, input_width * 2, input_height * 2)","metadata":{}},{"cell_type":"code","source":"class EncoderBlock(nn.Module):\n    \n    def __init__(self, in_channels, out_channels, batchnorm=True):\n        super().__init__()\n        \n        self.batchnorm=batchnorm\n        \n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            padding=(1, 1), \n            padding_mode='zeros', \n            kernel_size=(4, 4), \n            stride=(2, 2)\n        )\n\n        self.activation = nn.LeakyReLU(0.1)\n        \n        if self.batchnorm:\n            self.batchnorm = nn.BatchNorm2d(\n                num_features=out_channels\n            )\n        \n        \n    def forward(self, x):\n        \n        x = self.conv(x)\n        x = self.activation(x)\n\n        if self.batchnorm:\n            x = self.batchnorm(x)\n        \n        return x\n    \n    \nclass DecoderBlock(nn.Module):\n    \n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        \n        self.upconv = nn.ConvTranspose2d(\n            in_channels=in_channels, \n            out_channels=out_channels, \n            padding=(1, 1), \n            padding_mode='zeros', \n            kernel_size=(4, 4), \n            stride=(2, 2)\n        )\n        \n        \n        self.activation = nn.LeakyReLU(0.1)\n\n        self.batchnorm = nn.BatchNorm2d(\n            num_features=out_channels\n        )\n        \n        \n    def forward(self, x, skip_in):\n        \n        x = self.upconv(x)\n        x = x + skip_in\n        x = self.activation(x)\n        \n        x = self.batchnorm(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2021-06-25T18:25:05.921586Z","iopub.execute_input":"2021-06-25T18:25:05.921951Z","iopub.status.idle":"2021-06-25T18:25:05.932817Z","shell.execute_reply.started":"2021-06-25T18:25:05.921914Z","shell.execute_reply":"2021-06-25T18:25:05.93161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LinkNet(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        \n        # (3, 256, 256) -> (32, 128, 128)\n        self.e1 = EncoderBlock(\n            in_channels=3, \n            out_channels=32\n        )\n        \n        # (32, 128, 128) -> (64, 64, 64)\n        self.e2 = EncoderBlock(\n            in_channels=32, \n            out_channels=64\n        )\n        \n        # (64, 64, 64) -> (128, 32, 32)\n        self.e3 = EncoderBlock(\n            in_channels=64, \n            out_channels=128\n        )\n        \n        # (128, 32, 32) -> (256, 16, 16)\n        self.e4 = EncoderBlock(\n            in_channels=128, \n            out_channels=256\n        )\n        \n        # (256, 16, 16) -> (512, 8, 8)\n        self.e5 = EncoderBlock(\n            in_channels=256, \n            out_channels=512\n        )\n        \n        # bottleneck (512, 8, 8) -> (512, 4, 4)\n        self.b = nn.Conv2d(\n            in_channels=512,\n            out_channels=512,\n            padding=(1, 1), \n            padding_mode='zeros', \n            kernel_size=(4, 4), \n            stride=(2, 2)\n        )\n        \n        # (512, 4, 4) -> (512, 8, 8)\n        self.d1 = DecoderBlock(\n            in_channels=512, \n            out_channels=512\n        )\n        \n        # (512, 8, 8) -> (256, 16, 16)\n        self.d2 = DecoderBlock(\n            in_channels=512, \n            out_channels=256\n        )\n        \n        # (256, 16, 16) -> (128, 32, 32)\n        self.d3 = DecoderBlock(\n            in_channels=256, \n            out_channels=128\n        )\n        \n        # (128, 32, 32) -> (64, 64, 64)\n        self.d4 = DecoderBlock(\n            in_channels=128, \n            out_channels=64\n        )\n        \n        # (64, 64, 64) -> (32, 128, 128)\n        self.d5 = DecoderBlock(\n            in_channels=64, \n            out_channels=32\n        )\n        \n        # output (32, 128, 128) -> (1, 256, 256)\n        self.o = nn.ConvTranspose2d(\n            in_channels=32, \n            out_channels=1, \n            padding=(1, 1), \n            padding_mode='zeros', \n            kernel_size=(4, 4), \n            stride=(2, 2)\n        )\n        \n    def forward(self, x):\n        \n        e1 = self.e1(x)\n        e2 = self.e2(e1)\n        e3 = self.e3(e2)\n        e4 = self.e4(e3)\n        e5 = self.e5(e4)\n        \n        b = self.b(e5)\n        b = F.relu(b)\n        \n        d1 = self.d1(b, e5)\n        d2 = self.d2(d1, e4)\n        d3 = self.d3(d2, e3)\n        d4 = self.d4(d3, e2)\n        d5 = self.d5(d4, e1)\n        \n        output = self.o(d5)\n        output = F.sigmoid(output)\n        \n        return output","metadata":{"execution":{"iopub.status.busy":"2021-06-25T18:25:05.934378Z","iopub.execute_input":"2021-06-25T18:25:05.934769Z","iopub.status.idle":"2021-06-25T18:25:05.951665Z","shell.execute_reply.started":"2021-06-25T18:25:05.93473Z","shell.execute_reply":"2021-06-25T18:25:05.950714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T18:25:05.953454Z","iopub.execute_input":"2021-06-25T18:25:05.953896Z","iopub.status.idle":"2021-06-25T18:25:06.004502Z","shell.execute_reply.started":"2021-06-25T18:25:05.953859Z","shell.execute_reply":"2021-06-25T18:25:06.003425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"linknet = LinkNet().to(device)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T18:25:06.006003Z","iopub.execute_input":"2021-06-25T18:25:06.006459Z","iopub.status.idle":"2021-06-25T18:25:10.476304Z","shell.execute_reply.started":"2021-06-25T18:25:06.006408Z","shell.execute_reply":"2021-06-25T18:25:10.475429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(linknet, input_size=(3, 256, 256), batch_size=8, device='cuda')","metadata":{"execution":{"iopub.status.busy":"2021-06-25T18:25:10.479727Z","iopub.execute_input":"2021-06-25T18:25:10.479989Z","iopub.status.idle":"2021-06-25T18:25:11.348458Z","shell.execute_reply.started":"2021-06-25T18:25:10.479964Z","shell.execute_reply":"2021-06-25T18:25:11.347657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Функция потерь **Dice**.","metadata":{}},{"cell_type":"code","source":"class DiceLoss(nn.Module):\n    \n    def __init__(self, weight=None, size_average=True):\n        super(DiceLoss, self).__init__()\n\n    def forward(self, logits, targets):\n        smooth = 1\n        num = targets.size(0)\n        probs = logits\n        m1 = probs.reshape(num, -1)\n        m2 = targets.reshape(num, -1)\n        intersection = (m1 * m2)\n\n        score = (2. * intersection.sum(1) + smooth) / (m1.sum(1) + m2.sum(1) + smooth)\n        score = 1 - (score.sum() / num)\n        return score","metadata":{"execution":{"iopub.status.busy":"2021-06-25T18:25:11.350195Z","iopub.execute_input":"2021-06-25T18:25:11.350554Z","iopub.status.idle":"2021-06-25T18:25:11.356958Z","shell.execute_reply.started":"2021-06-25T18:25:11.350515Z","shell.execute_reply":"2021-06-25T18:25:11.356138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(\n    dataset=train_dataset,\n    batch_size=8, \n    shuffle=True, \n    num_workers=8\n)\n\ntest_loader = torch.utils.data.DataLoader(\n    dataset=test_dataset,\n    batch_size=8, \n    shuffle=False, \n    num_workers=8\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T18:25:11.358411Z","iopub.execute_input":"2021-06-25T18:25:11.358803Z","iopub.status.idle":"2021-06-25T18:25:11.367113Z","shell.execute_reply.started":"2021-06-25T18:25:11.358766Z","shell.execute_reply":"2021-06-25T18:25:11.366245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = DiceLoss()\noptimizer = torch.optim.Adam(linknet.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T18:25:11.368597Z","iopub.execute_input":"2021-06-25T18:25:11.369018Z","iopub.status.idle":"2021-06-25T18:25:11.376573Z","shell.execute_reply.started":"2021-06-25T18:25:11.36898Z","shell.execute_reply":"2021-06-25T18:25:11.375749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Вспомогательные функции для обучения модели, валидации и отрисовки картинок.","metadata":{}},{"cell_type":"code","source":"import time\n\n\ndef draw_imgs(img, mask, pred_mask):\n    \n    fig, ax = plt.subplots(1, 3, figsize=(17, 5))\n\n    ax[0].imshow(img.cpu().permute(1, 2, 0))\n    ax[0].set_title('Image')\n\n    ax[1].imshow(mask.cpu().permute(1, 2, 0), cmap='gray')\n    ax[1].set_title('Mask')\n\n    ax[2].imshow(pred_mask.detach().cpu().permute(1, 2, 0), cmap='gray')\n    ax[2].set_title('Predicted Mask')\n\n    plt.show()\n\n\ndef evaluate_model(model, loader, device, criterion):\n    loss = 0.\n    model.eval()\n    with torch.no_grad():\n        for data, label in loader:\n            data, label = data.to(device), label.to(device)\n\n            output = model(data)\n            loss += criterion(output, label).item()\n    \n    val_loss = loss / len(loader)\n    \n    print(\n        f'- val_loss: {val_loss:.4f}', \n        end='\\n')\n    \n    idx = np.random.randint(len(data))\n    draw_imgs(data[idx], label[idx], output[idx])\n    \n    model.train()\n    return loss/len(loader)\n\n\ndef epoch_training(model, criterion, optimizer, train_set, device):\n    \n    running_loss = 0.0\n    \n    n_steps = len(train_set)\n    steps_n_signs = len(str(n_steps))\n    \n    for i, data in enumerate(train_set):\n        start_time = time.time()\n        inputs, labels = data[0].to(device), data[1].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        epoch_loss = running_loss / (i + 1)\n        \n        step_duration = time.time() - start_time\n        duration = step_duration * n_steps\n\n        print(\n            '\\r' \\\n            f'step [{str(i + 1).zfill(steps_n_signs)}/{n_steps}] ' \\\n            f'- loss: {epoch_loss:.4f} ',\n            end='', flush=True)\n    \n    return epoch_loss\n\n\ndef model_training(model, criterion, optimizer, epochs, device, train_set, valid_set=None):\n    history = {\n        'loss': []\n    }\n    if valid_set:\n        history['val_loss'] = []\n    \n    for epoch in range(epochs):\n\n        model.train()\n        start_time = time.time()\n        \n        print(\n            f'Epoch {epoch + 1}/{epochs}', \n            end='\\n')\n        \n        epoch_loss = epoch_training(model, criterion, optimizer, train_set, device)\n        \n        duration = round(time.time() - start_time)\n        \n        if valid_set:\n            val_loss = evaluate_model(model, valid_set, device, criterion)\n            history['val_loss'].append(val_loss)\n        \n        history['loss'].append(epoch_loss)\n        \n        print(\n            '\\n' \\\n            f'duration: {duration}s ({round((duration/len(train_set)) * 1000)}ms/step)\\n', \n            end='\\n')\n\n    print('Training is finished!')\n    \n    return history","metadata":{"execution":{"iopub.status.busy":"2021-06-25T18:25:11.378016Z","iopub.execute_input":"2021-06-25T18:25:11.378475Z","iopub.status.idle":"2021-06-25T18:25:11.400195Z","shell.execute_reply.started":"2021-06-25T18:25:11.378433Z","shell.execute_reply":"2021-06-25T18:25:11.39931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_training(linknet, \n                         criterion, \n                         optimizer, \n                         epochs=7, \n                         device=device, \n                         train_set=train_loader, \n                         valid_set=test_loader)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T18:25:11.401587Z","iopub.execute_input":"2021-06-25T18:25:11.40223Z","iopub.status.idle":"2021-06-25T19:36:44.673158Z","shell.execute_reply.started":"2021-06-25T18:25:11.402178Z","shell.execute_reply":"2021-06-25T19:36:44.672121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Уменьшим learning rate.","metadata":{}},{"cell_type":"code","source":"optimizer.param_groups[0]['lr'] /= 10","metadata":{"execution":{"iopub.status.busy":"2021-06-25T19:36:44.674668Z","iopub.execute_input":"2021-06-25T19:36:44.675047Z","iopub.status.idle":"2021-06-25T19:36:44.680023Z","shell.execute_reply.started":"2021-06-25T19:36:44.675008Z","shell.execute_reply":"2021-06-25T19:36:44.679094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_training(linknet, \n                         criterion, \n                         optimizer, \n                         epochs=3, \n                         device=device, \n                         train_set=train_loader, \n                         valid_set=test_loader)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T19:36:44.681421Z","iopub.execute_input":"2021-06-25T19:36:44.681769Z","iopub.status.idle":"2021-06-25T20:07:52.49775Z","shell.execute_reply.started":"2021-06-25T19:36:44.681734Z","shell.execute_reply":"2021-06-25T20:07:52.49641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indices = np.random.randint(len(test_dataset), size=10)\n\nlinknet.eval()\nwith torch.no_grad():\n    for idx in indices:\n        img, mask = test_dataset[idx]\n        pred_mask = linknet(img[np.newaxis, :, :, :].to(device))\n\n        draw_imgs(img, mask, pred_mask[0, ...])","metadata":{"execution":{"iopub.status.busy":"2021-06-25T20:08:32.276852Z","iopub.execute_input":"2021-06-25T20:08:32.277165Z","iopub.status.idle":"2021-06-25T20:08:36.474306Z","shell.execute_reply.started":"2021-06-25T20:08:32.277137Z","shell.execute_reply":"2021-06-25T20:08:36.473381Z"},"trusted":true},"execution_count":null,"outputs":[]}]}