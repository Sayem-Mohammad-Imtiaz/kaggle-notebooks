{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Natural Language Processing with Disaster Tweets ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:07.822747Z","iopub.execute_input":"2021-06-28T08:13:07.823351Z","iopub.status.idle":"2021-06-28T08:13:07.843491Z","shell.execute_reply.started":"2021-06-28T08:13:07.823249Z","shell.execute_reply":"2021-06-28T08:13:07.842577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import spaCy and load the language library. Remember to use a larger model!\n\nimport spacy\nnlp =spacy.load('en_core_web_lg')","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:07.84524Z","iopub.execute_input":"2021-06-28T08:13:07.845514Z","iopub.status.idle":"2021-06-28T08:13:17.38667Z","shell.execute_reply.started":"2021-06-28T08:13:07.845488Z","shell.execute_reply":"2021-06-28T08:13:17.385654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nimport numpy as np\nimport pandas as pd\n\nimport nltk\nimport re\nimport string","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:17.387963Z","iopub.execute_input":"2021-06-28T08:13:17.38823Z","iopub.status.idle":"2021-06-28T08:13:19.207367Z","shell.execute_reply.started":"2021-06-28T08:13:17.388203Z","shell.execute_reply":"2021-06-28T08:13:19.206239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_disaster =pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\" ,encoding='unicode_escape',error_bad_lines=False)\ntrain_disaster","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.20879Z","iopub.execute_input":"2021-06-28T08:13:19.209234Z","iopub.status.idle":"2021-06-28T08:13:19.346773Z","shell.execute_reply.started":"2021-06-28T08:13:19.209198Z","shell.execute_reply":"2021-06-28T08:13:19.342005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_disaster['target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.348779Z","iopub.status.idle":"2021-06-28T08:13:19.349254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_disaster['target'].hist()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.350438Z","iopub.status.idle":"2021-06-28T08:13:19.350868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that  class 1 is almost 50 percent of class 0 hence we shall not proceed with class balancing","metadata":{}},{"cell_type":"markdown","source":"# 1. Data cleaning on text data","metadata":{}},{"cell_type":"code","source":"# check for null values\ntrain_disaster.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.351762Z","iopub.status.idle":"2021-06-28T08:13:19.352203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define a function to clean the text data\n# use re. sub() function which is used to replace occurrences of a particular sub-string with another sub-string.\n\ndef text_cleaning(text):\n    text =text.lower()                     # make in lower case\n    text = re.sub('\\[.*?@\\]','',text)      # remove text in square brackets\n    text =re.sub('\\n' ,'',text)\n    text = re.sub('\\w*\\d\\w*','' ,text)      # remove words containing numbers\n    text.lstrip(\"$\")                        # removes $ sign from start of string   \n    text.strip()\n    text =re.sub('[!@#$]','',text)          # replace given characters from string\n    return text\n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.353116Z","iopub.status.idle":"2021-06-28T08:13:19.353545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_disaster['text'] = train_disaster['text'].apply( lambda x:text_cleaning(x))","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.35454Z","iopub.status.idle":"2021-06-28T08:13:19.354966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_disaster","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.355833Z","iopub.status.idle":"2021-06-28T08:13:19.356287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenization","metadata":{}},{"cell_type":"code","source":"def tokenization(text):\n    tokens= re.split('W+',text)\n    return tokens\n\ntrain_disaster['tokenized_text'] =train_disaster['text'].apply(lambda x : tokenization(x))","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.357222Z","iopub.status.idle":"2021-06-28T08:13:19.357634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stemming","metadata":{}},{"cell_type":"code","source":"from nltk.stem.porter import PorterStemmer\nporter =PorterStemmer()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.358437Z","iopub.status.idle":"2021-06-28T08:13:19.358858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stemming(text):\n    stemtext= [porter.stem(i) for i in text]\n    return stemtext\n\ntrain_disaster['stemmed_text'] =train_disaster['tokenized_text'].apply(lambda x : stemming(x))","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.359686Z","iopub.status.idle":"2021-06-28T08:13:19.360103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lemmatization","metadata":{}},{"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.361063Z","iopub.status.idle":"2021-06-28T08:13:19.361489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lemma =WordNetLemmatizer()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.362374Z","iopub.status.idle":"2021-06-28T08:13:19.362776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lemmatization(text):\n    lem_text = [lemma.lemmatize(i) for i in text]\n    return lem_text\n\ntrain_disaster['lemmatized_text'] =train_disaster['tokenized_text'].apply(lambda x : lemmatization(x))","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.363624Z","iopub.status.idle":"2021-06-28T08:13:19.36404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_disaster","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.365007Z","iopub.status.idle":"2021-06-28T08:13:19.365412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stp words removal","metadata":{}},{"cell_type":"code","source":"stopwords =(nlp.Defaults.stop_words)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.366233Z","iopub.status.idle":"2021-06-28T08:13:19.366628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stopwords = nltk.corpus.stopwords.words('english')","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.367491Z","iopub.status.idle":"2021-06-28T08:13:19.367913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stopwords[0:300]=['whereupon', 'n‘t', 'whoever', 'ca', 'serious', 'seemed', 'been', 'few', 'which', 'there', 'myself', 'part', 'seeming', 'indeed', 'call', 'another', 'namely', 'show', 'used', 'for', 'sometime', 'wherever', 'bottom', 'ever', 'fifteen', 'ten', 'top', 'done', 'noone', 'not', 'yourself', 'beyond', 'afterwards', 'move', 'more', 'most', 'therein', 'back', \"'ve\", 'my', 'himself', '‘ll', 'any', 'perhaps', 'something', 'last', 'until', 'anyhow', 'nobody', 'our', 'hereby', 're', 'hers', 'does', 'put', 'every', 'into', 'such', 'they', 'everywhere', 'one', 'always', 'has', 'full', 'anyway', 'third', 'us', 'it', 'towards', 'almost', 'on', 'out', 'her', 'as', 'might', 'same', 'your', 'me', 'hundred', 'together', 'the', 'already', 'an', 'eight', 'mostly', 'have', 'further', 'only', 'using', 'what', 'whereas', 'though', 'name', 'being', 'became', 'regarding', 'side', 'moreover', 'under', 'did', 'whether', 'amongst', 'that', 'whence', 'when', 'we', 'empty', 'well', 'herself', 'eleven', 'whither', 'say', 'him', 'even', 'off', 'against', 'give', 'below', 'beforehand', 'really', \"'ll\", 'itself', 'made', 'thus', 'toward', 'his', '‘d', 'you', 'get', 'whole', 'a', 'would', 'ours', 'becomes', 'nevertheless', 'many', 'unless', 'throughout', 'either', 'over', 'these', 'and', 'so', 'them', '’ll', 'those', 'since', 'somehow', '’re', 'alone', 'neither', 'without', 'forty', 'cannot', 'make', 'he', 'twelve', 'front', 'in', 'none', 'down', 'after', 'was', 'thereupon', 'keep', 'around', 'go', 'however', 'no', 'becoming', 'yourselves', 'else', 'just', 'between', 'yet', 'whereby', '’m', 'others', 'who', 'former', 'had', 'amount', 'among', 'everyone', 'herein', 'two', 'nor', 'other', 'could', 'thereafter', 'still', 'thereby', 'anyone', 'because', 'before', 'rather', 'will', 'hereafter', 'latterly', '‘m', 'how', 'may', 'three', 'across', 'do', \"'m\", 'become', 'whom', 'up', 'along', 'each', 'due', 'sometimes', 'anything', 'within', 'is', 'several', 'should', 'latter', 'themselves', 'are', 'by', 'whereafter', 'she', 'someone', 'nothing', 'nowhere', 'behind', 'or', 'too', 'twenty', 'wherein', 'be', 'except', 'once', 'enough', 'besides', 'first', 'am', \"'s\", 'quite', 'anywhere', 'from', 'can', 'about', 'onto', '’s', 'this', 'then', 'than', 'all', 'ourselves', 'at', 'while', 'also', '‘re', 'if', 'five', 'upon', 'yours', 'least', 'very', 'although', 'where', 'less', 'above', 'nine', 'much', '’d', 'hence', 'of', '‘ve', 'whose', '’ve', 'meanwhile', 'see', 'doing', 'per', 'elsewhere', 'their', 'mine', 'whatever', 'via', 'to', 'were', 'some', 'thence', 'various', '‘s', 'here', 'why', 'please', 'thru', 'through', 'seems', 'take', 'again', 'during', 'seem', 'six', \"n't\", 'formerly', 'sixty', \"'re\", 'four', 'n’t', 'but', 'everything', 'whenever', \"'d\", 'often', 'never', 'with', 'next', 'hereupon', 'otherwise', 'i', 'somewhere', 'both', 'beside', 'fifty', 'therefore', 'its', 'now', 'own', 'must']","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.368931Z","iopub.status.idle":"2021-06-28T08:13:19.369344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stopword_removal(text):\n    removed_stopwords=[ i for i in text if i not in stopwords]\n    return removed_stopwords\n    \ntrain_disaster['final_cleaned_text'] =train_disaster['lemmatized_text'].apply(lambda x : stopword_removal(x))","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.370387Z","iopub.status.idle":"2021-06-28T08:13:19.370794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_disaster","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.371718Z","iopub.status.idle":"2021-06-28T08:13:19.372127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So our final cleaned text is train_disaster['final_cleaned_text']","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. MODELLING","metadata":{}},{"cell_type":"markdown","source":"# Using different machine language algorithm we will try to predict targets","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.372942Z","iopub.status.idle":"2021-06-28T08:13:19.373342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=train_disaster['final_cleaned_text']\ny=train_disaster['target']","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.37417Z","iopub.status.idle":"2021-06-28T08:13:19.374603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# since stock['final_cleaned_text'] is in array of array , we need to convert into array of strings\n#Join all items in a tuple into a string\n\ntrain_disaster['final_cleaned_text']=[\" \".join(i) for i in train_disaster['final_cleaned_text'].values]","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.375411Z","iopub.status.idle":"2021-06-28T08:13:19.375839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We will further divide our train data into train and validation data\n\nx_train ,x_val ,y_train,y_val =train_test_split(x,y,test_size=0.25 ,random_state=40 )","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.376713Z","iopub.status.idle":"2021-06-28T08:13:19.377124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.378027Z","iopub.status.idle":"2021-06-28T08:13:19.378679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_val","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.379801Z","iopub.status.idle":"2021-06-28T08:13:19.380272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.a)  Use linear support vector machine along with pipeline","metadata":{}},{"cell_type":"markdown","source":"The TfidfVectorizer will tokenize documents, learn the vocabulary and inverse document frequency weightings(Inverse Document Frequency (IDF) is a weight indicating how commonly a word is used.The more frequent its usage across documents, the lower its score), and allow you to encode new documents.","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score,classification_report","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.38126Z","iopub.status.idle":"2021-06-28T08:13:19.381695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline1 = Pipeline([('Tfidf',TfidfVectorizer()),\n                   ('Svm',LinearSVC())])\n\npipeline1","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.382599Z","iopub.status.idle":"2021-06-28T08:13:19.383046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline1.fit(x_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.383874Z","iopub.status.idle":"2021-06-28T08:13:19.384281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict1 =pipeline1.predict(x_val)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.385221Z","iopub.status.idle":"2021-06-28T08:13:19.385624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(accuracy_score(y_val,predict1))","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.38645Z","iopub.status.idle":"2021-06-28T08:13:19.386934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy2 =accuracy_score(y_val,predict1)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.3878Z","iopub.status.idle":"2021-06-28T08:13:19.388235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_val,predict1))","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.389075Z","iopub.status.idle":"2021-06-28T08:13:19.389501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.b) Using Logistic regression and vectorization","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.390401Z","iopub.status.idle":"2021-06-28T08:13:19.39083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline2 = Pipeline([('Tfidf',TfidfVectorizer()),\n                   ('logisticregression',LogisticRegression(penalty ='l2',solver ='saga'))])\n\npipeline2","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.391636Z","iopub.status.idle":"2021-06-28T08:13:19.392076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline2.fit(x_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.392916Z","iopub.status.idle":"2021-06-28T08:13:19.393338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict2 =pipeline2.predict(x_val)\npredict2","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.394144Z","iopub.status.idle":"2021-06-28T08:13:19.394555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(accuracy_score(y_val,predict2))","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.395311Z","iopub.status.idle":"2021-06-28T08:13:19.395732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy3 =accuracy_score(y_val,predict2)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.396667Z","iopub.status.idle":"2021-06-28T08:13:19.397095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_val,predict2))","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.398455Z","iopub.status.idle":"2021-06-28T08:13:19.398974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.c) Using Naive Bayes and vectorization","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.400079Z","iopub.status.idle":"2021-06-28T08:13:19.400661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline3 = Pipeline([('Tfidf',TfidfVectorizer()),\n                   ('naivebayes',MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None))])\n\npipeline3","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.401971Z","iopub.status.idle":"2021-06-28T08:13:19.402462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline3.fit(x_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.403502Z","iopub.status.idle":"2021-06-28T08:13:19.403989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict3 =pipeline3.predict(x_val)\npredict3","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.404945Z","iopub.status.idle":"2021-06-28T08:13:19.405562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(accuracy_score(y_val,predict3))","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.40652Z","iopub.status.idle":"2021-06-28T08:13:19.407019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy4= accuracy_score(y_val,predict3)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.408091Z","iopub.status.idle":"2021-06-28T08:13:19.408574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_val,predict3))","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.409673Z","iopub.status.idle":"2021-06-28T08:13:19.410204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.d) Using Stochastic Gradiant Descent and vectorization","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.411439Z","iopub.status.idle":"2021-06-28T08:13:19.411914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline4 = Pipeline([('Tfidf',TfidfVectorizer()),\n                   ('SGD',SGDClassifier(loss = 'hinge', penalty = 'l2', random_state=0))])\n\npipeline4","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.412936Z","iopub.status.idle":"2021-06-28T08:13:19.413395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline4.fit(x_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.414474Z","iopub.status.idle":"2021-06-28T08:13:19.414934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict4 =pipeline4.predict(x_val)\npredict4","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.415913Z","iopub.status.idle":"2021-06-28T08:13:19.416344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(accuracy_score(y_val,predict4))","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.417326Z","iopub.status.idle":"2021-06-28T08:13:19.417771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy5= accuracy_score(y_val,predict4)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.418642Z","iopub.status.idle":"2021-06-28T08:13:19.419088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_val,predict4))","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.420087Z","iopub.status.idle":"2021-06-28T08:13:19.420511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.e) Using Random Forest classifier and vectorization","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.421422Z","iopub.status.idle":"2021-06-28T08:13:19.421885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline5 = Pipeline([('Tfidf',TfidfVectorizer()),\n                   ('RFC',RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None,min_samples_split=4))])\n\npipeline5","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.422939Z","iopub.status.idle":"2021-06-28T08:13:19.423375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fit =pipeline5.fit(x_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.42426Z","iopub.status.idle":"2021-06-28T08:13:19.424877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict5 =pipeline5.predict(x_val)\npredict5","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.425761Z","iopub.status.idle":"2021-06-28T08:13:19.426204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(accuracy_score(y_val,predict5))","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.427157Z","iopub.status.idle":"2021-06-28T08:13:19.427585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy6= accuracy_score(y_val,predict5)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.428426Z","iopub.status.idle":"2021-06-28T08:13:19.42888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_val,predict5))","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.429791Z","iopub.status.idle":"2021-06-28T08:13:19.430234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Compare performances of all models","metadata":{}},{"cell_type":"code","source":"all_accuracies =[accuracy2,accuracy3 ,accuracy4,accuracy5,accuracy6]\nmodels =['LinearSVM','Logistic Regression','Naive Bayes','SGD classifier','RandomForestClassifier']\n\ndf =pd.DataFrame( {'Model':models ,'Accuracy': all_accuracies })\ndf","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.431257Z","iopub.status.idle":"2021-06-28T08:13:19.431662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# We can see that Naive Bayes model gave best accuracy most of the time whenever we run all models  hence we will proceed with that model","metadata":{}},{"cell_type":"markdown","source":"# Perform on given test data with Naive Bayes's model","metadata":{}},{"cell_type":"code","source":"test_disaster =pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\" ,encoding='unicode_escape',error_bad_lines=False)\ntest_disaster","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.432615Z","iopub.status.idle":"2021-06-28T08:13:19.433064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtest = test_disaster['text']\nxtest","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.434018Z","iopub.status.idle":"2021-06-28T08:13:19.434476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pipeline3 is the model for Naive Bayes\n\npredict_test =pipeline3.predict(xtest)  ","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.435324Z","iopub.status.idle":"2021-06-28T08:13:19.435743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Attach that predicted vale to test data set","metadata":{}},{"cell_type":"code","source":"test_disaster['target'] =predict_test\n\ntest_disaster","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.436679Z","iopub.status.idle":"2021-06-28T08:13:19.437143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_test_disaster =test_disaster[['id','target']]","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.438012Z","iopub.status.idle":"2021-06-28T08:13:19.438424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_test_disaster","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.43929Z","iopub.status.idle":"2021-06-28T08:13:19.439721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_test_disaster.to_csv('sample_submission.csv' ,index= False)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T08:13:19.440679Z","iopub.status.idle":"2021-06-28T08:13:19.44114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}