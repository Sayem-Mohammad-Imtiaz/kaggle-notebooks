{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Setting up the requirements","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Importing libraries\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objs as go\nfrom matplotlib import pyplot as plt\nfrom mlxtend.plotting import plot_confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explore Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare your file\nparent_dir: str = os.path.join('/kaggle', 'input', 'heart-failure-clinical-data')\ndataset_name: str = \"heart_failure_clinical_records_dataset.csv\"\ndataset_path: str = os.path.join(parent_dir, dataset_name)\nprint(f\"Dataset directory: {dataset_path}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load data\nheart_failure_df: pd.DataFrame = pd.read_csv(dataset_path)\nheart_failure_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Additional Information\n\nAccording to the description of the dataset, some of the aforementioned columns have been binarized for data analysis purposes including:\n* **Sex** - Gender of patient Male = 1, Female =0\n* **Diabetes** - 0 = No, 1 = Yes\n* **Anaemia** - 0 = No, 1 = Yes\n* **High_blood_pressure** - 0 = No, 1 = Yes\n* **Smoking** - 0 = No, 1 = Yes\n* **DEATH_EVENT** - 0 = No, 1 = Yes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_survival_vs_binary_variable(data: pd.DataFrame, predicted_col: str, response_col: str, unique_labels: list):\n    # Preprocess the data\n    positive = data[data[predicted_col]==1]\n    negative = data[data[predicted_col]==0]\n    \n    # Extract values and labels\n    data_values = zip([positive, negative, positive, negative.copy()],\n                      [0,0,1,1])\n    values = [i[i[response_col]==j].shape[0] for i, j in data_values]\n    \n    # Extract labels\n    labels = [f\"{label} - Survived\" for label in unique_labels] + [f\"{label} - not Survived\" for label in unique_labels]\n    \n    # Plot Figure\n    fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.4)])\n    fig.update_layout(\n        title_text=f\"Analysis on Survival - {predicted_col.title()}\")\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analyse Survival By Gender\n\nLet's see whether the gender of the person is relevant when predicting a heart failure","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# By Sex\npredicted_col: str = \"sex\"\nresponse_col: str = \"DEATH_EVENT\"\nunique_labels: list = [\"Male\", \"Female\"]\n\n# Call Function\nplot_survival_vs_binary_variable(data=heart_failure_df, \n                                 predicted_col=predicted_col,\n                                 response_col=response_col,\n                                 unique_labels=unique_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# By Diabetes\npredicted_col: str = \"diabetes\"\nresponse_col: str = \"DEATH_EVENT\"\nunique_labels: list = [\"Yes\", \"No\"]\n\n# Call Function\nplot_survival_vs_binary_variable(data=heart_failure_df, \n                                 predicted_col=predicted_col,\n                                 response_col=response_col,\n                                 unique_labels=unique_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# By anaemia\npredicted_col: str = \"anaemia\"\nresponse_col: str = \"DEATH_EVENT\"\nunique_labels: list = [\"Yes\", \"No\"]\n\n# Call Function\nplot_survival_vs_binary_variable(data=heart_failure_df, \n                                 predicted_col=predicted_col,\n                                 response_col=response_col,\n                                 unique_labels=unique_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Process Data via Pipelines\n\nLet's preprocess a little bit our dataset in order to build a robust pipeline to perform the final classification task:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import StackingClassifier, GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Prepare the data\n\nNow let's prepare our training and validation sets including X and y values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"response_variable: str = \"DEATH_EVENT\"\n\n# Get Predictive columns\npredictive_variables: list = list(heart_failure_df.columns)\npredictive_variables.remove(response_variable)\nprint(predictive_variables)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare data (Cross-validation purposes)\nX: pd.DataFrame = heart_failure_df[predictive_variables]\ny: pd.Series = heart_failure_df[[response_variable]]\ntest_size: float = 0.25\n\n# Split data into separate sets (Training purposes)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n\nprint(f\"X_train shape: {X_train.shape}\\n\")\nprint(f\"X_test shape: {X_test.shape}\\n\")\nprint(f\"y_train shape: {y_train.shape}\\n\")\nprint(f\"y_test shape: {y_test.shape}\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Build the Pipeline\n### 2.1 Preprocessor\n\nIn this section we are going to build the first part of the Pipeline which is going to be the preprocessor. It will include a dummy simpleInputer which will fill any empty value with the median of the feature and it will scale the input features using a scaler from Sklearn. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaler\nscaler: RobustScaler = RobustScaler()\nn_features: int = int(X.shape[1])\n\n# Numerical transformer\nnumerical_transformer:Pipeline = Pipeline(steps=[('imputer', SimpleImputer(strategy='median',\n                                                                            fill_value=-99)),\n                                                 ('scaler', scaler)\n                                                 ])\n# Preprocessor Transformer\npreprocessor_transformer: ColumnTransformer = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, predictive_variables)])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2 Design and implement An Ensemble model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ensemble model (stacking)\n\nrandom_state=123\nestimators: list = [('RF', RandomForestClassifier(max_features=\"sqrt\",\n                                                  n_estimators=150,\n                                                  random_state=random_state)),\n                    ('GB', GradientBoostingClassifier(max_depth=5,\n                                                     random_state=random_state)),\n                    ('SVM', SVC(C=2))]\nfinal_estimator: LogisticRegression = LogisticRegression()\n\nensemble_model: StackingClassifier = StackingClassifier(estimators=estimators,\n                                                        final_estimator=final_estimator\n                                                        )\nprint(ensemble_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3 Organise the final pipeline","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Complete pipeline\nsteps: list = [(\"preprocessor\", preprocessor_transformer),\n               ('classifier', ensemble_model)]\nml_model: Pipeline = Pipeline(steps=steps)\nprint(ml_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train the model with Cross-validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_validate\ny_np: np.array = np.array(y).ravel()\ncv: int = 5\nscoring: tuple = ('balanced_accuracy', 'f1', 'precision', 'recall', 'roc_auc')\nscores = cross_validate(ml_model, X, y_np, cv=cv, scoring=scoring, return_train_score=True)\nfor metric_name, score in scores.items():\n    print(f\"{metric_name} mean: {np.mean(score)}, {metric_name} std: {np.std(score)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final Training and testing of the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the model\nml_model.fit(X=X_train, y=np.array(y_train).ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=ml_model.predict(X_test)\nprint(f\"Ensemble Model score: {ml_model.score(X_test, y_test)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's plot our confusion matrix using the predictions we got from the test samples","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Visualizing the Confusion Matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nplt.figure()\nplot_confusion_matrix(cm, figsize=(12,8), hide_ticks=True, cmap=plt.cm.Blues)\nplt.title(\"Ensemble Model - Confusion Matrix\")\nplt.xticks(range(2), [\"Heart Not Failed\",\"Heart Fail\"], fontsize=16)\nplt.yticks(range(2), [\"Heart Not Failed\",\"Heart Fail\"], fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute \nTP: float = cm[0,0]\nFP: float = cm[1,0]\nFN: float = cm[0,1]\nTN: float = cm[1,1]\n\nsensitivity = (TP / (TP + FN))\nprint(f\"Sensitivity: {sensitivity}\")\nspecificity = ( TN / (TN + FP))\nprint(f\"Specificity: {specificity}\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}