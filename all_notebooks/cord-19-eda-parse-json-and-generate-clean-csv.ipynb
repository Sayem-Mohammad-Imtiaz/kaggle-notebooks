{"cells":[{"metadata":{},"cell_type":"markdown","source":"## About this notebook\n\nIn this notebook, I quickly explore the `biorxiv` subset of the papers. Since it is stored in JSON format, the structure is likely too complex to directly perform analysis. Thus, I not only explore the structure of those files, but I also provide the following helper functions for you to easily format inner dictionaries from each file:\n* `format_name(author)`\n* `format_affiliation(affiliation)`\n* `format_authors(authors, with_affiliation=False)`\n* `format_body(body_text)`\n* `format_bib(bibs)`\n\nFeel free to reuse those functions for your own purpose! If you do, please leave a link to this notebook.\n\nThroughout the EDA, I show you how to use each of those files. At the end, I show you how to generate a clean version of the `biorxiv` as well as all the other datasets, which you can directly use by choosing this notebook as a data source (\"File\" -> \"Add or upload data\" -> \"Kernel Output File\" tab -> search the name of this notebook).\n\n### Update Log\n\n* V9: First release.\n* V10: Updated paths to include the [14k new papers](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/discussion/137474)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('../input/cord-19-eda-parse-json-and-generate-clean-csv'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Biorxiv: Exploration\n\nLet's first take a quick glance at the `biorxiv` subset of the data. We will also use this opportunity to load all of the json files into a list of **nested** dictionaries (each `dict` is an article)."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np #\nimport pandas as pd\nimport seaborn as sns #visualisation\nimport matplotlib.pyplot as plt \n%matplotlib inline\nsns.set(color_codes=True)\nfrom sklearn import datasets , linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nlr=LinearRegression(normalize=True)\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/cord-19-eda-parse-json-and-generate-clean-csv/biorxiv_clean.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=pd.read_csv('../input/cord-19-eda-parse-json-and-generate-clean-csv/clean_comm_use.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2=pd.read_csv('../input/cord-19-eda-parse-json-and-generate-clean-csv/clean_noncomm_use.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3=pd.read_csv('../input/cord-19-eda-parse-json-and-generate-clean-csv/clean_pmc.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.describe(include ='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.describe(include ='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3.describe(include ='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['paper_id','affiliations','abstract','raw_bibliography','raw_authors'], axis=1) #these are the colomn to be drop which meant to no use to me .\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df1.drop(['paper_id','affiliations','abstract','raw_bibliography','raw_authors'], axis=1) #these are the colomn to be drop which meant to no use to me .\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = df2.drop(['paper_id','affiliations','abstract','raw_bibliography','raw_authors'], axis=1) #these are the colomn to be drop which meant to no use to me .\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3 = df3.drop(['paper_id','affiliations','abstract','raw_bibliography','raw_authors'], axis=1) #these are the colomn to be drop which meant to no use to me .\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(10) #data after rename","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"duplicate_rows_df = df[df.duplicated()]\nprint(\"number of duplicate rows: \", duplicate_rows_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"duplicate_rows_df1 = df1[df1.duplicated()]\nprint(\"number of duplicate rows: \", duplicate_rows_df1.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"duplicate_rows_df2 = df2[df2.duplicated()]\nprint(\"number of duplicate rows: \", duplicate_rows_df2.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"duplicate_rows_df3 = df3[df3.duplicated()]\nprint(\"number of duplicate rows: \", duplicate_rows_df3.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop_duplicates()\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df1.drop_duplicates()\ndf1.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = df2.drop_duplicates()\ndf2.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.dropna()\ndf.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df1.dropna()\ndf1.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = df2.dropna()\ndf2.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3 = df3.dropna()\ndf3.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum() #checking if there are some null values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('../input/novel-corona-virus-2019-dataset'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_main = pd.read_csv('../input/novel-corona-virus-2019-dataset/time_series_covid_19_confirmed.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_main.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_main.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"df_m =df_main.drop(['Lat','Long'], axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_main.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#using groupby command set the country/region as index of the dataframe.\ncorona_dataset_aggregated = df_main.groupby(\"Country/Region\").sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_dataset_aggregated.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_dataset_aggregated.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set(color_codes=True)\nfrom sklearn import datasets , linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nlr=LinearRegression(normalize=True)\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_dataset_aggregated.loc[\"China\"].plot()\ncorona_dataset_aggregated.loc[\"Italy\"].plot()\ncorona_dataset_aggregated.loc[\"Spain\"].plot()\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_dataset_aggregated.loc['China'].plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_dataset_aggregated.loc[\"China\"][:3].plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_dataset_aggregated.loc[\"China\"].diff().plot() #calculating the first derivative of the curve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_dataset_aggregated.loc[\"China\"].diff().max() #find maxmimum infection rate for China","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_dataset_aggregated.loc[\"Italy\"].diff().max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_dataset_aggregated.loc[\"Spain\"].diff().max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find maximum infection rate for all of the countries\ncountries =list(corona_dataset_aggregated.index)\n\nmax_infection_rates=[]\nfor c in countries:\n     max_infection_rates.append(corona_dataset_aggregated.loc[c].diff().max())\ncorona_dataset_aggregated[\"max_infection_rate\"]= max_infection_rates","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_dataset_aggregated.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create a new dataframe with only needed column\ncorona_data = pd.DataFrame(corona_dataset_aggregated[\"max_infection_rate\"])\ncorona_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#new task\n#Importing the WorldHappinessReport.csv dataset---1\n#selecting needed columns for our analysis--2\n#join the datasets---3\n#calculate the correlations as the result of our analysis---4\nhappiness_report_csv = pd.read_csv('../input/happiness/worldwide_happiness_report.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"happiness_report_csv.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"happiness_report_csv.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#changing the indices of the dataframe\nhappiness_report_csv.set_index(\"Country or region\",inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"happiness_report_csv.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now let's join two dataset we have prepared\n#Corona Dataset :\ncorona_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corona_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#world happiness report Dataset :\nhappiness_report_csv.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"happiness_report_csv.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = corona_data.join(happiness_report_csv,how=\"inner\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#BOX PLOT GRAPHS FOR DIFFERENT CATEGORIEs BEFORE REMOVING OUTLIERS\nsns.boxplot(x=data['max_infection_rate'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=data['GDP per capita'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=data['Generosity'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=data['Healthy life expectancy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=data['Overall rank'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#STEP 5 REMOVING OUTLIERS USING IQR METHOD\nQ1 = data.quantile(0.25)\nQ3 = data.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CHECKING THE SHAPE OF THE DATAFRAME AFTER REMOVING OUTLIERS¶\ndata = data[~((data < (Q1 - 1.5 * IQR)) |(data > (Q3 + 1.5 * IQR))).any(axis=1)]\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking whether the outliers are removed are not¶\nsns.boxplot(x=data['max_infection_rate'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data= data.reset_index()\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#STEP 6 CHECKING TOP 15 Countries MOST REPRESENTED IN DATASET¶\ncounts = data['index'].value_counts()*100/sum(data['index'].value_counts())\npopular_labels = counts.index[:20]\nplt.figure(figsize=(10,5))\nplt.barh(popular_labels, width=counts[:20])\nplt.title('Top 25 Countries mostly represented in dataset')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#correlation matrix\ndata.corr() ##FINDING THE CORRELATION MATRIX FROM HERE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corrMatrix = data.corr()\nsns.heatmap(corrMatrix,annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"DOCUMENTING INSIGHTS **\nfrom the above it is concluded that max_infection_rate positively depends on  Healthy Life Expectancy(+0.23),GDP/capita(+0.21),Score(+0.20) . Even they three are positiviely depend on each others as well. \nOverall rank(-0.19) have negatively strong correlation with max_infection_rate.\n\npositive correlation holds directly proportion relation and negative coorealtion holds inversely relation with thier respeccitve factors affecting them."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(data['max_infection_rate'],data['Healthy life expectancy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting diff graphs and also checking how other variables affect the Pandemic's Max_infection_rate (EDA)\n# rate v/s  GDP\nsns.barplot(data['max_infection_rate'],data['GDP per capita'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(data['max_infection_rate'],data['Score'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(data['max_infection_rate'],data['Overall rank'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['max_infection_rate'].plot.hist()\nplt.xlabel('max_infection_rate', fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(data['max_infection_rate'].loc[data['max_infection_rate']<4.223125e+04 ]).plot.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Healthy life expectancy'].plot.hist()\nplt.xlabel('Life expectancy', fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['GDP per capita'].plot.hist()\nplt.xlabel('GDP', fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Score'].plot.hist()\nplt.xlabel('rank', fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Overall rank'].plot.hist()\nplt.xlabel('rank', fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax=plt.subplots(figsize=(5,5))\nax.scatter(data['max_infection_rate'],data['Healthy life expectancy'])\nplt.title('Scatter between life_expectancy and max_infection_rate')\nax.set_xlabel=('max_infection_rate')\nax.set_ylabel=('life_expectancy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax=plt.subplots(figsize=(5,5))\nax.scatter(data['max_infection_rate'],data['GDP per capita'])\nplt.title('Scatter between GDP and max_infection_rate')\nax.set_xlabel=('max_infection_rate')\nax.set_ylabel=('GDP')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax=plt.subplots(figsize=(5,5))\nax.scatter(data['max_infection_rate'],data['Overall rank'])\nplt.title('Scatter between Rank and max_infection_rate')\nax.set_xlabel=('max_infection_rate')\nax.set_ylabel=('Rank')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = data['Healthy life expectancy']\ny = data[\"max_infection_rate\"]\nsns.scatterplot(x,np.log(y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.regplot(x,np.log(y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = data['GDP per capita']\ny = data[\"max_infection_rate\"]\nsns.scatterplot(x,np.log(y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.regplot(x,np.log(y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = data['Overall rank']\ny = data[\"max_infection_rate\"]\nsns.scatterplot(x,np.log(y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.regplot(x,np.log(y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just Splitting Data set In 80:20 using Pareto principle¶"},{"metadata":{"trusted":true},"cell_type":"code","source":"pm = data.select_dtypes(exclude=[np.number]) \npm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels9.2 . Encode target labels with value between 0 and n_classes-1.\n#\n#This transformer should be used to encode target values, i.e. y, and not the input X. It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels\n#9.3 PREPROCESSSING THE DATA BEFORE SPLITTING¶\nfrom sklearn.preprocessing import LabelEncoder\nlabel_enc = LabelEncoder()\nfor i in pm:\n  data[i] = label_enc.fit_transform(data[i])\nprint('Label Encoded Data')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nX = np.asarray(data[['Healthy life expectancy','GDP per capita','Score','Overall rank']])\ny = np.asarray(data['max_infection_rate'])\nX = preprocessing.StandardScaler().fit(X).transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=0)\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#9.4 LINEAR REGRESSION MODEL WITH PRICE AS A TARGET VARIABLE¶\n# Our algorithm should be helping us to draw a regression line to predict the cities which can provide the best profitability • We will use scikitlearn’s Linear Regression to train our model on both the training and test\n\nfrom sklearn import linear_model\nlm = linear_model.LinearRegression()\nmodel = lm.fit(X_train,y_train)\npredictions = lm.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nmodel = LinearRegression().fit(X_train,y_train)\ny_pred = model.predict(X_test)\nmodel.score(X_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier \nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report \nd_m = DecisionTreeClassifier(random_state = 0)\nd_m.fit(X_train,y_train)\ny_pred = d_m.predict(X_test)\nprint(\"Confusion Matrix:\\n\\n\", confusion_matrix(y_test, y_pred)) \nprint (\"\\nAccuracy : \", accuracy_score(y_test,y_pred)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.intercept_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ny_pred = model.predict(X_test) \nplt.plot(y_test, y_pred, '.')\n\n# plot a line, a perfit predict would all fall on this line\nx = np.linspace(0, 330, 100)\ny = x\nplt.plot(x, y)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test) \nplt.plot(y_test, y_pred, '.')\n\n# plot a line, a perfit predict would all fall on this line\nx = np.linspace(0, 330, 100)\ny = x\nplt.plot(x, y)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nimport math\n\nprint('MSE: %.2f' % mean_squared_error(y_test, y_pred))\nprint('R Squared : %.2f' % r2_score(y_test, y_pred))\nprint('MAE :%.2f' % mean_absolute_error(y_test, y_pred))\nprint('RMSE : %.2f' % math.sqrt(mean_squared_error(y_test, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#multiple regression\n\ndef abss(x):\n    if x>=0: return x\n    else: return -1*x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.head())\nheaders=data.dtypes.index\nheader=headers.tolist()\nfeatures=[data['Healthy life expectancy'],data['GDP per capita']]\nY=np.array(data['max_infection_rate'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=[]\ndepth=len(Y)\nwidth=len(features)\nfor i in range(depth):\n    X.append([1])\n    for j in range(width):\n        X[i].append(features[j][i])\n    #X[i].append(Y[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=np.matrix(X)\nY=np.mat(Y).T\nprint(X)\n#XT_X=np.dot(X.T,X)\n#XT_Y=np.dot(X.T,Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inverse=np.dot(X.T,X).I\nintermediate_rep=np.dot(inverse,X.T)\nbeta=np.dot(intermediate_rep,Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"beta","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_y=np.dot(X,beta)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(new_y)\nerror=Y-new_y\nprint(error)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correct=0\nfor val in error:\n    if val<=5:\n        correct+=1\naccuracy=correct/float(len(error))\nprint(accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(np.array(X[:,1]),np.array(new_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(np.array(X[:,1]),np.array(Y-new_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(error)\nfor val in error:\n    val*=val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum(error)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s=0\nfor val in error:\n    s+=val\nprint(s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(30):\n    print(Y[i],new_y[i], Y[i]-new_y[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_y[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Logistic regression\nz=np.arange(-6,6,0.1)\nsigmoid=1/(1+np.exp(-z))\nfig=plt.figure('cost function')\nplt.plot(z,sigmoid)\nplt.grid(True)\nplt.xlabel('input z')\nplt.ylabel('output')\nplt.title('sigmoid fucntion graph')\nplt.show","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t=np.ones((2,3))\nt.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def hypothesis(theta, X, n):\n    h = np.ones((X.shape[0],1))\n    theta = theta.reshape(1,n+1)\n    for i in range(0,X.shape[0]):\n        h[i] = 1 / (1 + np.exp(-float(np.matmul(theta, X[i]))))\n    h = h.reshape(X.shape[0])\n    return h","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def BGD(theta, alpha, num_iters, h, X, y, n):\n    theta_history = np.ones((num_iters,n+1))\n    cost = np.ones(num_iters)\n    for i in range(0,num_iters):\n        theta[0] = theta[0] - (alpha/X.shape[0]) * sum(h - y)\n        for j in range(1,n+1):\n            theta[j]=theta[j]-(alpha/X.shape[0])*sum((h-y)\n                               *X.transpose()[j])\n        theta_history[i] = theta\n        h = hypothesis(theta, X, n)\n        cost[i]=(-1/X.shape[0])*sum(y*np.log(h)+(1-y)*np.log(1 - h))\n    theta = theta.reshape(1,n+1)\n    return theta, theta_history, cost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def logistic_regression(X, y, alpha, num_iters):\n    n = X.shape[1]\n    one_column = np.ones((X.shape[0],1))\n    X = np.concatenate((one_column, X), axis = 1)\n    # initializing the parameter vector...\n    theta = np.zeros(n+1)\n    # hypothesis calculation....\n    h = hypothesis(theta, X, n)\n    # returning the optimized parameters by Gradient Descent...\n    theta,theta_history,cost = BGD(theta,alpha,num_iters,h,X,y,n)\n    return theta, theta_history, cost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X[:,[1,2]] # feature-set\ny_train = new_y # label-set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x0 = np.ones((np.array([x for x in y_train if x == 0]).shape[0], \n              X_train.shape[1]))\nx1 = np.ones((np.array([x for x in y_train if x == 1]).shape[0], \n              X_train.shape[1]))\n#x0 and x1 are matrices containing +ve and -ve examples from the\n#dataset, initialized to 1\nk0 = k1 = 0\nfor i in range(0,y_train.shape[0]):\n    if y_train[i] == 0:\n        x0[k0] = X_train[i]\n        k0 = k0 + 1\n    else:\n        x1[k1] = X_train[i]\n        k1 = k1 + 1\nX = [x0, x1]\ncolors = [\"green\", \"blue\"] # 2 distinct colours for 2 classes \nimport matplotlib.pyplot as plt\nfor x, c in zip(X, colors):\n    if c == \"green\":\n        plt.scatter(x[:,0],x[:,1],color = c,label = \"Not Admitted\")\n    else:\n        plt.scatter(x[:,0], x[:,1], color = c, label = \"Admitted\")\nplt.xlabel(\"Marks obtained in 1st Exam\")\nplt.ylabel(\"Marks obtained in 2nd Exam\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.replace('?',np.nan,inplace=True)\n\n#delete all the rows that have NaN in them\ndk=data.dropna()\n\n#deleting the column with id, 1 in the argument indicates 'column', so this will delete the 'column' containing 'id'\n#df.drop(['id'],1,inplace=True)\nfull_data=data.values.tolist()\nheaders = data.dtypes.index\nheader=headers.tolist()\n\nhle=np.array(data['Healthy life expectancy'], dtype=np.int64)\nmir=np.array(data['max_infection_rate'], dtype=np.int64)\nxs=hle\nys=mir\nprint(data['Healthy life expectancy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def shift_origin(xs,ys):\n    x_mean=np.mean(xs)\n    y_mean=np.mean(ys)\n    shifted_x=xs-x_mean\n    shifted_y=ys-y_mean\n    #plt.scatter(shifted_x,shifted_y)\n    return shifted_x,shifted_y,x_mean,y_mean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shifted_x,shifted_y,x_mean,y_mean=shift_origin(xs,ys)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sum_squared_mean(xs,ys):\n    m=0     #initial slope\n    m_list=[]    #list of all slopes throughout the program\n    learning=0.05  #initial learning rate\n    squared_error=[]  #\n    squared_error_list=[] #list of all squared errors\n    sum=0\n    \n    #shift of mean of the passed dataset\n    shifted_x,shifted_y,x_mean,y_mean=shift_origin(xs,ys)\n    \n    #limit for the minimum learning rate\n    while learning>0.000001:\n        \n        \n        #calculate the squared error\n        sum=0\n        for i in range(1,len(xs)+1):\n            sum+=(m*shifted_x[i]-shifted_y[i])**2\n         \n        #append error and slope in lists\n        squared_error_list.append(sum)\n        m_list.append(m)\n        \n        #if errror is 0 then the slope is perfect\n        if sum==0:\n            return m\n        \n        if len(squared_error)==0:\n                squared_error.append(sum)\n        \n        #if previous error = new error then stop\n        elif sum==squared_error[-1]:\n            return m\n        else:\n            #if new error is greater than previous error, take 2 steps back and reduce the learning rate\n            #also remove the squared errors for the last 2 steps \n            if squared_error[-1]<sum:\n                m-=learning*2\n                learning=learning/float(10)\n                del(squared_error[-2:])\n                squared_error.append(sum)\n            else:\n                squared_error.append(sum)\n        \n        #increment the slope by learning rate\n        m+=learning\n        \n    \n    return m,m_list,squared_error_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m,m_list,squared_error_list=sum_squared_mean(np.abs(xs),np.abs(ys))\n\n#best slope returned by the function\nm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Support Vector Machines (SVM)\nfrom sklearn.svm import SVC\nsvc = SVC(kernel='linear')\nsvc.fit(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report \nconfusion_matrix(y_pred,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = svc.predict(np.random.random((3,8)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)\n#Classification Report\n#from sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import neighbors\nknn = neighbors.KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = knn.predict_proba(X_test)\nknn.score(X_test, y_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))\n#Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=0.95)\n# K Means\nfrom sklearn.cluster import KMeans\nk_means = KMeans(n_clusters=3, random_state=0)\nk_means.fit(X_train)\npca_model = pca.fit_transform(X_train)\n>>> y_pred = k_means.predict(X_test)\nfrom sklearn.metrics import accuracy_score\n>>> accuracy_score(y_test, y_pred)\n Classification Report\n>>> from sklearn.metrics import classification_report\n>>> print(classification_report(y_test, y_pred))\n Confusion Matrix\n>>> from sklearn.metrics import confusion_matrix\n>>> print(confusion_matrix(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=0.95)\npca_model = pca.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}