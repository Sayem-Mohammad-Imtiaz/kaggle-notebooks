{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install tensorflow\nimport os","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining each of these directories"},{"metadata":{"trusted":true},"cell_type":"code","source":"#base_dir = '../input/mechanical-tools-dataset/Mechanical Tools Image dataset'\n\ntrain_dir = os.path.join('../input/mechanical-tools-dataset/train_data_V2/train_data_V2')\nvalidation_dir = os.path.join('../input/mechanical-tools-dataset/validation_data/validation_data')\n\n# Directory with our training screwdriver/wrench pictures\ntrain_screwdriver_dir = os.path.join('../input/mechanical-tools-dataset/train_data_V2/train_data_V2/screwdriver')\ntrain_wrench_dir = os.path.join('../input/mechanical-tools-dataset/train_data_V2/train_data_V2/wrench')\n\n# Directory with our validation screwdriver/wrench pictures\nvalidation_screwdriver_dir = os.path.join('../input/mechanical-tools-dataset/validation_data/validation_data/screwdriver')\nvalidation_wrench_dir = os.path.join('../input/mechanical-tools-dataset/validation_data/validation_data/wrench')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now, let's see what the filenames look like in the training directories:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_screwdriver_fnames = os.listdir(train_screwdriver_dir )\ntrain_wrench_fnames = os.listdir( train_wrench_dir )\n\nprint(train_screwdriver_fnames[:20])\nprint(train_wrench_fnames[:20])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's find out the number of wrench and pliers images in the directory**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('total training screwdriver images :', len(os.listdir(train_screwdriver_dir)))\nprint('total training wrench images :', len(os.listdir(train_wrench_dir)))\n\nprint('total validation screwdriver images :', len(os.listdir( validation_screwdriver_dir ) ))\nprint('total validation wrench images :', len(os.listdir( validation_wrench_dir) ))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now let's take a look at a few pictures to get a better sense of what they look like. First, configure the matplot parameters:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n# Parameters for our graph; we'll output images in a 10x10 configuration\nnrows = 4\nncols = 4\n\n# Index for iterating over images\npic_index = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now, display a batch of 50 wrench and 50 pliers pictures. You can rerun the cell to see a fresh batch each time:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up matplotlib fig, and size it to fit 4x4 pics\nfig = plt.gcf()\nfig.set_size_inches(ncols * 4, nrows * 4)\n\npic_index += 8\nnext_screwdriver_pix = [os.path.join(train_screwdriver_dir, fname) \n                for fname in train_screwdriver_fnames[pic_index-8:pic_index]]\nnext_wrench_pix = [os.path.join(train_wrench_dir, fname) \n                for fname in train_wrench_fnames[pic_index-8:pic_index]]\n\nfor i, img_path in enumerate(next_screwdriver_pix+next_wrench_pix):\n  # Set up subplot; subplot indices start at 1\n  sp = plt.subplot(nrows, ncols, i + 1)\n  sp.axis('Off') # Don't show axes (or gridlines)\n\n  img = mpimg.imread(img_path)\n  plt.imshow(img)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The \"output shape\" column shows how the size of your feature map evolves in each successive layer. The convolution layers reduce the size of the feature maps by a bit due to padding, and each pooling layer halves the dimensions.**"},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# All images will be rescaled by 1./255.\ntrain_datagen = ImageDataGenerator( rescale = 1.0/255. )\ntest_datagen  = ImageDataGenerator( rescale = 1.0/255. )\n\n# --------------------\n# Flow training images in batches of 20 using train_datagen generator\n# --------------------\ntrain_generator = train_datagen.flow_from_directory(train_dir,\n                                                    batch_size=30,\n                                                    class_mode='binary',\n                                                    target_size=(300, 300))     \n# --------------------\n# Flow validation images in batches of 20 using test_datagen generator\n# --------------------\nvalidation_generator =  test_datagen.flow_from_directory(validation_dir,\n                                                         batch_size=30,\n                                                         class_mode  = 'binary',\n                                                         target_size = (300, 300))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We then add convolutional layers as in the previous example, and flatten the final result to feed into the densely connected layers.**\n**Note that because we are facing a two-class classification problem, i.e. a binary classification problem, we will end our network with a sigmoid activation, so that the output of our network will be a single scalar between 0 and 1, encoding the probability that the current image is class 1 (as opposed to class 0).**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.models.Sequential([\n    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(300, 300, 3)),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Dropout(0.30),\n    tf.keras.layers.Conv2D(32, (4,4), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Dropout(0.30),\n    tf.keras.layers.Conv2D(128,(4,4), activation='relu'), \n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Dropout(0.30),\n    tf.keras.layers.Conv2D(128,(4,4), activation='relu'), \n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Dropout(0.30),\n    tf.keras.layers.Conv2D(128,(4,4), activation='relu'), \n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Dropout(0.30),\n    tf.keras.layers.Conv2D(128,(3,3), activation='relu'), \n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Dropout(0.30),\n    # Flatten the results to feed into a DNN\n    tf.keras.layers.Flatten(), \n    # 512 neuron hidden layer\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.30),\n    tf.keras.layers.Dense(1, activation='sigmoid')  \n]) \n    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('Screw driver') and 1 for the other ('Wrench')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.optimizers import RMSprop\n\nmodel.compile(optimizer=RMSprop(lr=0.001),\n              loss='binary_crossentropy',\n              metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(train_generator,\n                              validation_data=validation_generator,\n                              steps_per_epoch=15,\n                              epochs=8,\n                              validation_steps=15,\n                              verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-----------------------------------------------------------\n# Retrieve a list of list results on training and test data\n# sets for each training epoch\n#-----------------------------------------------------------\nacc      = history.history[     'accuracy' ]\nval_acc  = history.history[ 'val_accuracy' ]\nloss     = history.history[    'loss' ]\nval_loss = history.history['val_loss' ]\n\nepochs   = range(len(acc)) # Get number of epochs\n\n#------------------------------------------------\n# Plot training and validation accuracy per epoch\n#------------------------------------------------\nplt.plot  ( epochs,     acc )\nplt.plot  ( epochs, val_acc )\nplt.title ('Training and validation accuracy')\nplt.figure()\n\n#------------------------------------------------\n# Plot training and validation loss per epoch\n#------------------------------------------------\nplt.plot  ( epochs,     loss )\nplt.plot  ( epochs, val_loss )\nplt.title ('Training and validation loss'   )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\nimport numpy as np\nimg = cv2.imread('../input/mechanical-tools-dataset/test_data/test_data/Screwdriver (1404).JPEG')\nplt.imshow(img)\nimg = cv2.resize(img,(300, 300))\nimg = np.reshape(img,[1,300, 300,3])\n\nclasses = model.predict_classes(img)\n\nprint(classes)\nif classes[0]>0:\n    print(\"This is a Wrench\")\nelse :\n    print(\"This is a Screw driver\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = cv2.imread('../input/mechanical-tools-dataset/test_data/test_data/Screwdriver (1407).JPEG')\nplt.imshow(img)\nimg = cv2.resize(img,(300, 300))\nimg = np.reshape(img,[1,300, 300,3])\n\nclasses = model.predict_classes(img)\n\nprint(classes)\n\nif classes[0]>0:\n    print(\"This is a Wrench\")\nelse :\n    print(\"This is a Screwdriver\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = cv2.imread('../input/mechanical-tools-dataset/test_data/test_data/Screwdriver (1408).JPEG')\nplt.imshow(img)\nimg = cv2.resize(img,(300, 300))\nimg = np.reshape(img,[1,300, 300,3])\n\nclasses = model.predict_classes(img)\n\nprint(classes)\nif classes[0]>0:\n    print(\"This is a Wrench\")\nelse :\n    print(\"This is a Screwdriver\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = cv2.imread('../input/mechanical-tools-dataset/test_data/test_data/Screwdriver (1410).JPEG')\nplt.imshow(img)\nimg = cv2.resize(img,(300, 300))\nimg = np.reshape(img,[1,300, 300,3])\n\nclasses = model.predict_classes(img)\n\nprint(classes)\nif classes[0]>0:\n    print(\"This is a Wrench\")\nelse :\n    print(\"This is a Screwdriver\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = cv2.imread('../input/mechanical-tools-dataset/test_data/test_data/Screwdriver (1415).JPEG')\nplt.imshow(img)\nimg = cv2.resize(img,(300, 300))\nimg = np.reshape(img,[1,300, 300,3])\n\nclasses = model.predict_classes(img)\n\nprint(classes)\nif classes[0]>0:\n    print(\"This is a Wrench\")\nelse :\n    print(\"This is a Screwdriver\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = cv2.imread('../input/mechanical-tools-dataset/test_data/test_data/Screwdriver (1401).JPEG')\nplt.imshow(img)\nimg = cv2.resize(img,(300, 300))\nimg = np.reshape(img,[1,300, 300,3])\n\nclasses = model.predict_classes(img)\n\nprint(classes)\nif classes[0]>0:\n    print(\"This is a Wrench\")\nelse :\n    print(\"This is a Screwdriver\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = cv2.imread('../input/mechanical-tools-dataset/test_data/test_data/Screwdriver (1402).JPEG')\nplt.imshow(img)\nimg = cv2.resize(img,(300, 300))\nimg = np.reshape(img,[1,300, 300,3])\n\nclasses = model.predict_classes(img)\n\nprint(classes)\nif classes[0]>0:\n    print(\"This is a Wrench\")\nelse :\n    print(\"This is a Screwdriver\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = cv2.imread('../input/mechanical-tools-dataset/test_data/test_data/Wrench (286).JPEG')\nplt.imshow(img)\nimg = cv2.resize(img,(300, 300))\nimg = np.reshape(img,[1,300, 300,3])\n\nclasses = model.predict_classes(img)\n\nprint(classes)\nif classes[0]>0:\n    print(\"This is a Wrench\")\nelse :\n    print(\"This is a Screwdriver\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = cv2.imread('../input/mechanical-tools-dataset/test_data/test_data/Wrench (293).JPEG')\nplt.imshow(img)\nimg = cv2.resize(img,(300, 300))\nimg = np.reshape(img,[1,300, 300,3])\n\nclasses = model.predict_classes(img)\n\nprint(classes)\nif classes[0]>0:\n    print(\"This is a Wrench\")\nelse :\n    print(\"This is a Screwdriver\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = cv2.imread('../input/mechanical-tools-dataset/test_data/test_data/Wrench (276).JPEG')\nplt.imshow(img)\nimg = cv2.resize(img,(300, 300))\nimg = np.reshape(img,[1,300, 300,3])\n\nclasses = model.predict_classes(img)\n\nprint(classes)\nif classes[0]>0:\n    print(\"This is a Wrench\")\nelse :\n    print(\"This is a Screwdriver\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = cv2.imread('../input/mechanical-tools-dataset/test_data/test_data/Wrench (268).JPEG')\nplt.imshow(img)\nimg = cv2.resize(img,(300, 300))\nimg = np.reshape(img,[1,300, 300,3])\n\nclasses = model.predict_classes(img)\n\nprint(classes)\nif classes[0]>0:\n    print(\"This is a Wrench\")\nelse :\n    print(\"This is a Screwdriver\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = cv2.imread('../input/mechanical-tools-dataset/test_data/test_data/Wrench (313).JPEG')\nplt.imshow(img)\nimg = cv2.resize(img,(300, 300))\nimg = np.reshape(img,[1,300, 300,3])\n\nclasses = model.predict_classes(img)\n\nprint(classes)\nif classes[0]>0:\n    print(\"This is a Wrench\")\nelse :\n    print(\"This is a Screwdriver\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = cv2.imread('../input/mechanical-tools-dataset/test_data/test_data/Wrench (324).JPEG')\nplt.imshow(img)\nimg = cv2.resize(img,(300, 300))\nimg = np.reshape(img,[1,300, 300,3])\n\nclasses = model.predict_classes(img)\n\nprint(classes)\nif classes[0]>0:\n    print(\"This is a Wrench\")\nelse :\n    print(\"This is a Screwdriver\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.predict_classes(validation_generator)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualizing Intermediate Representations\nTo get a feel for what kind of features our convnet has learned, one fun thing to do is to visualize how an input gets transformed as it goes through the convnet.\nLet's pick a random image from the training set, and then generate a figure where each row is the output of a layer, and each image in the row is a specific filter in that output feature map. Rerun this cell to generate intermediate representations for a variety of training images.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport random\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img\n\n# Let's define a new Model that will take an image as input, and will output\n# intermediate representations for all layers in the previous model after\n# the first.\nsuccessive_outputs = [layer.output for layer in model.layers[1:]]\n#visualization_model = Model(img_input, successive_outputs)\nvisualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n# Let's prepare a random input image from the training set.\nwrench_img_files = [os.path.join(train_wrench_dir, f) for f in train_wrench_fnames]\nscrewdriver_img_files = [os.path.join(train_wrench_dir, f) for f in train_screwdriver_fnames]\nimg_path = random.choice(wrench_img_files + screwdriver_img_files)\n\nimg = load_img(img_path, target_size=(300, 300))  # this is a PIL image\nx = img_to_array(img)  # Numpy array with shape (150, 150, 3)\nx = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)\n\n# Rescale by 1/255\nx /= 255\n\n# Let's run our image through our network, thus obtaining all\n# intermediate representations for this image.\nsuccessive_feature_maps = visualization_model.predict(x)\n\n# These are the names of the layers, so can have them as part of our plot\nlayer_names = [layer.name for layer in model.layers[1:]]\n\n# Now let's display our representations\nfor layer_name, feature_map in zip(layer_names, successive_feature_maps):\n    if len(feature_map.shape) == 4:\n    # Just do this for the conv / maxpool layers, not the fully-connected layers\n        n_features = feature_map.shape[-1]  # number of features in feature map\n    # The feature map has shape (1, size, size, n_features)\n        size = feature_map.shape[1]\n    # We will tile our images in this matrix\n        display_grid = np.zeros((size, size * n_features))\n        for i in range(n_features):\n      # Postprocess the feature to make it visually palatable\n            x = feature_map[0, :, :, i]\n            x -= x.mean()\n            x /= x.std()\n            x *= 64\n            x += 128\n            x = np.clip(x, 0, 255).astype('uint8')\n      # We'll tile each filter into this big horizontal grid\n            display_grid[:, i * size : (i + 1) * size] = x\n    # Display the grid\n        scale = 20. / n_features\n        plt.figure(figsize=(scale * n_features, scale))\n        plt.title(layer_name)\n        plt.grid(False)\n        plt.imshow(display_grid, aspect='auto', cmap='summer')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}