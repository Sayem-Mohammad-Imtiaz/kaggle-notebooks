{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1. Introduction"},{"metadata":{},"cell_type":"markdown","source":"This notebook is a simple look at the dataset **videogamesales** from GregorySmith. We will do some simple Data Visualization and also try to create a simple model to guess a game's plateform according to the other attributs."},{"metadata":{},"cell_type":"markdown","source":"## 1.1. Imports "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt #Simple plots\nimport seaborn as sns #Pretty plots\nimport altair as alt #Interactive plots\nimport IPython #for JS\n\n#Data science\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense,Input\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.2. A look at the dataset with Pandas"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"vgDB = pd.read_csv(\"/kaggle/input/videogamesales/vgsales.csv\")\nvgDB","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Data Visualization"},{"metadata":{},"cell_type":"markdown","source":"## 2.1. Using Matplotlib"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,5))\nplt.subplot(2, 2, 1)\nplt.title('Top 5 of the sum of sales per Publisher')\nplt.plot(vgDB.groupby([\"Publisher\"]).sum().filter([\"Global_Sales\"]).sort_values(by=[\"Global_Sales\"],ascending=False).head(5))\n\nplt.figure(figsize=(25,2))\nplt.subplot(2, 2, 2)\nplt.title('Top 10 sales per Platform')\nplt.plot(vgDB.groupby([\"Platform\"]).sum().filter([\"Global_Sales\"]).sort_values(by=[\"Global_Sales\"],ascending=False).head(10))\n\nplt.figure(figsize=(20,5))\nplt.subplot(2, 2, 3)\nplt.title('Sum of sales per Year')\nplt.plot(vgDB.query(\"Year < 2015\").groupby([\"Year\"]).sum().filter([\"Global_Sales\"]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2. Using Seaborn (for pretty graphs)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Ventes par an')\nan = vgDB.query(\"Year < 2015 & Publisher in ['Electronic Arts','Ubisoft','Nintendo','Activision','Sony Computer Entertainment']\").filter([\"Year\",\"Global_Sales\",\"Publisher\"])\nsns.scatterplot(an.iloc[:,0],an.iloc[:,1]/80,hue=an.iloc[:,2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2. Using Altair (for interative graphs)"},{"metadata":{"trusted":true},"cell_type":"code","source":"source = an\n\nan['Year'] = an['Year'].astype(float).astype(int).astype(str)\n\nselection = alt.selection_multi(fields=['Publisher'], bind='legend')\n\nalt.Chart(source).mark_point().encode(\n  alt.X('Year:T'),\n  alt.Y('Global_Sales'),\n  alt.Color('Publisher'),\n  opacity=alt.condition(selection, alt.value(1), alt.value(0.1))\n).add_selection(\n    selection\n).interactive()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Data Science"},{"metadata":{},"cell_type":"raw","source":"Here, we will try to find the platform according to all other attributes of a raw in the dataset. In order to see if our model is relevant, we can see if choosing a publisher returns a platform of its own, for instance, i will choose Nintendo and check if it returns a Nintendo's platform."},{"metadata":{},"cell_type":"markdown","source":"## 3.1. Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"Our neural network has to handle numbers rather than strings, we have to categorize *Publisher*, *Genre* and *Platform*, meaning for instance we change \"Nintendo\" to 0, \"Sega\" to 1 ect... using dictionnaries.\nHowever, for this notebook, I will only keep the top 10 publishers."},{"metadata":{"trusted":true},"cell_type":"code","source":"List_dev = vgDB.groupby([\"Publisher\"]).sum().filter([\"Global_Sales\"]).sort_values(by=[\"Global_Sales\"],ascending=False).head(10)\nclass_to_dev = { List_dev.index.values[i]:i  for i in range(len(List_dev.index.values))}\nDB = vgDB.copy()\nDB[\"Publisher\"].replace(class_to_dev,inplace=True)\nL =[i for i in range(len(List_dev.index.values))]\nDB = DB.drop(DB.query(\"Publisher not in @L\").index)\nDB = DB.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"List_platf = vgDB[\"Platform\"].unique()\nclass_to_platf = { List_platf[i]:i  for i in range(len(List_platf))}\nDB[\"Platform\"].replace(class_to_platf,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"List_genre = vgDB[\"Genre\"].unique()\nclass_to_genre = { List_genre[i]:i  for i in range(len(List_genre))}\nDB[\"Genre\"].replace(class_to_genre,inplace=True)\nDB.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then, we shall normalize the value of *Year* and all the sales in order to make it more understandable for the network."},{"metadata":{"trusted":true},"cell_type":"code","source":"def Normalization(column):\n  return (column-column.mean())/column.std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DB[\"Year\"],DB[\"NA_Sales\"],DB[\"EU_Sales\"],DB[\"JP_Sales\"],DB[\"Other_Sales\"],DB[\"Global_Sales\"]= Normalization(DB[\"Year\"]),Normalization(DB[\"NA_Sales\"]),Normalization(DB[\"EU_Sales\"]),Normalization(DB[\"JP_Sales\"]),Normalization(DB[\"Other_Sales\"]),Normalization(DB[\"Global_Sales\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is common to use sklearn's *train_test_split* function in order to separate the dataset. This way, the network will be tested on never seen before data to have a better view of its accuracy/score."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = DB.reset_index().filter([\"Year\",\"Genre\",\"Publisher\",\"NA_Sales\",\"EU_Sales\",\"JP_Sales\",\"Other_Sales\",\"Global_Sales\"]).to_numpy()\ny = DB.reset_index().filter([\"Platform\"]).to_numpy()\nX_train, X_test, y_train, y_test = train_test_split(X,y)\nprint(\"Separation from {} elements to : train = {} ; test = {}.\".format(X.shape[0],X_train.shape[0],X_test.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2. Using a K-nearest Neighbour with sklearn"},{"metadata":{},"cell_type":"markdown","source":"This model has correct score but we will see later that classifiers are way more relevent for this problem, furthermore we need to already know the number of clusters (different platforms possible) we are searching for which will not be necessary for classifiers."},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(len(List_platf))\nknn.fit(X_train, y_train.ravel())\nscore = knn.score(X_test, y_test)\nprint(\"Score :\", score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = np.array([5,1,0,1,2,2,1,2]) #random test with Nintendo as a publisher and \ntest = test.reshape((1,8))\n\nprint(\"For \" + str(List_dev.index.values[test[0,2]]) + \" as a publisher and \" + str(List_genre[test[0,1]]) + \" as a genre, the platform predicted is : \\n\" + str(List_platf[np.argmax(knn.predict(test))]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2. Using a MLPClassifier Neighbour with sklearn"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_sk = MLPClassifier(hidden_layer_sizes=(8,16,32,64))\nmodel_sk.fit(X_train,y_train.ravel())\nscore = model_sk.score(X_test,y_test)\nprint(\"Score :\", score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = np.array([5,1,0,1,2,2,1,2]) #random test with Nintendo as a publisher and \ntest = test.reshape((1,8))\n\nprint(\"For \" + str(List_dev.index.values[test[0,2]]) + \" as a publisher and \" + str(List_genre[test[0,1]]) + \" as a genre, the platform predicted is : \\n\" + str(List_platf[np.argmax(model_sk.predict(test))]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.3. Using TensorFlow"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train= np.array(X_train).astype('float32')\nX_test=np.array(X_test).astype('float32')\ny_train=np.array(y_train).astype('float32')\ny_test =np.array(y_test).astype('float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Input(shape=(8,)))\nmodel.add(Dense(8,activation=\"relu\"))\nmodel.add(Dense(16,activation=\"relu\"))\nmodel.add(Dense(32,activation=\"relu\"))\nmodel.add(Dense(64,activation=\"relu\"))\nmodel.add(Dense(len(List_platf),activation=\"softmax\"))\nmodel.build(X[0].shape)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\nmodel.compile(loss=loss_fn, optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(X_train,y_train,epochs=150,validation_data=(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_curve = history.history[\"loss\"]\nacc_curve = history.history[\"accuracy\"]\nloss_val_curve = history.history[\"val_loss\"]\nacc_val_curve = history.history[\"val_accuracy\"]\n\nplt.plot(loss_curve,label=\"Train\")\nplt.plot(loss_val_curve,label=\"Validation\")\nplt.legend(loc='upper right')\nplt.title(\"Loss\")\nplt.show()\nplt.plot(acc_curve,label=\"Train\")\nplt.plot(acc_val_curve,label=\"Validation\")\nplt.legend(loc='lower right')\nplt.title(\"Accuracy\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the curves of accuracy and loss from both our training and test. Here we can see a sign of the beginning of overfitting as training accuracy increase without test accuracy which stay stuck around 0.55. However, this remains a better score than our two previous models."},{"metadata":{"trusted":true},"cell_type":"code","source":"test = np.array([5,1,0,1,2,2,1,2]) #random test with Nintendo as a publisher and \ntest = test.reshape((1,8))\n\nprint(\"For \" + str(List_dev.index.values[test[0,2]]) + \" as a publisher and \" + str(List_genre[test[0,1]]) + \" as a genre, the platform predicted is : \\n\" + str(List_platf[np.argmax(model.predict(test))]))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}