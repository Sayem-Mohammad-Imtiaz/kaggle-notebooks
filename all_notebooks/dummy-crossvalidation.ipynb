{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You canq write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/delaysenc/delays20enc.csv', index_col=False, warn_bad_lines=True, error_bad_lines=False)\ndf=df.drop(['Unnamed: 0'], axis=1)\ndf = df.sample(frac =.02).reset_index().drop(['index'], axis=1)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.simplefilter('ignore')\n\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X,y = df.loc[:, df.columns != 'ArrDelay'],df.loc[:, df.columns == 'ArrDelay']\n\nX_train,X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.dummy import DummyClassifier\n\n# Using the Dummy Classifier to set the lowest accuracy and therefore find the best model\ndc = DummyClassifier(random_state=42) # define the model\n\n# Train/fit the model to the data\ndc.fit(X_train, y_train)\n\n# Apply the model to the testing data to get a new set of predictions\n# and show the accuracy score\nprediction = dc.predict(X_test)\nprint('Arrival delay Dummy Classifier Accuracy:', metrics.accuracy_score(prediction, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import svm\nfrom sklearn.neighbors import KNeighborsClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Support Vector Classification\nkernel= ['linear', 'poly', 'rbf', 'sigmoid']\n\nskf10 = StratifiedKFold(n_splits=10)\nskf5 = StratifiedKFold(n_splits=5)\n\nfor k in kernel:\n    clf = svm.SVC(kernel=k, random_state=42, gamma='auto')\n    scores = cross_val_score(clf, X, y, cv=skf5, scoring='accuracy').mean()\n    print(k, 'accuracy with 5 splits: ',scores)\n    scores = cross_val_score(clf, X, y, cv=skf10, scoring='accuracy').mean()\n    print(k, 'accuracy with 10 splits: ',scores)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Kneighbors Classifier\nskf = StratifiedKFold(n_splits=5)\n\nk_range = list(range(5, 35))\nk_scores = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, X, y, cv=skf, scoring='accuracy')\n    k_scores.append(scores.mean())\n    print(k,'range: ',k_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\n# plot the value of K for KNN (x-axis) versus the cross-validated accuracy (y-axis)\nplt.plot(k_range, k_scores)\nplt.xlabel('Value of K for KNN')\nplt.ylabel('Cross-Validated Accuracy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 10-fold cross-validation with logistic regression\nfrom sklearn.linear_model import LogisticRegression\nsolv=['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n\nfor ss in solv:\n    logreg = LogisticRegression(solver=ss)\n    print(ss,'10:   ',cross_val_score(logreg, X, y, cv=skf10, scoring='accuracy').mean())\n    print(ss,'5:    ',cross_val_score(logreg, X, y, cv=skf5, scoring='accuracy').mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlm = LinearRegression()\nskf10 = StratifiedKFold(n_splits=10)\nscores = -cross_val_score(lm, X, y, cv=skf10, scoring='neg_mean_squared_error')\nprint(scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndepth=list(range(3, 13))\n\nfor dd in depth:\n    clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=dd, random_state=42)\n    print('Entropy, max depth ',dd,': ',cross_val_score(clf, X, y, cv=skf10, scoring='accuracy').mean())\n    \n    clf = DecisionTreeClassifier(criterion=\"gini\", max_depth=dd, random_state=42)\n    print('Gini, max depth ',dd,': ',cross_val_score(clf, X, y, cv=skf10, scoring='accuracy').mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\n#MLPClassifier(alpha=1, max_iter=1000)\n\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\n# GaussianProcessClassifier(1.0 * RBF(1.0))\n\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Initializing the MLPClassifier\n\nlst=[(20,10,5),(150,100,50),(64,64,64)]\niter=(500,200,50)\ncvs=(5,10)\n\nfor hls in lst:\n    for it in iter:\n        for cv in cvs:\n            mlp = MLPClassifier(hidden_layer_sizes=hls, max_iter=it,activation = 'relu',solver='adam',random_state=1)\n            print ('hidden layers: ',hls,' max iterations: ', it, ' number of folds: ',cv)\n            print('mean accuracy: ',cross_val_score(mlp, X, y, cv=cv, scoring='accuracy').mean())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gaussian Process Classifier\ngpc = GaussianProcessClassifier(kernel=1*RBF(1.0))\n","metadata":{},"execution_count":null,"outputs":[]}]}