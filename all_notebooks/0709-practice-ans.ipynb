{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 실습 -  Decision Tree model을 이용하여 wine class 예측","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2021-07-09T07:25:04.075291Z","iopub.execute_input":"2021-07-09T07:25:04.075675Z","iopub.status.idle":"2021-07-09T07:25:04.09417Z","shell.execute_reply.started":"2021-07-09T07:25:04.075646Z","shell.execute_reply":"2021-07-09T07:25:04.093118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier\nimport graphviz \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.tree import plot_tree\nfrom sklearn.model_selection import GridSearchCV","metadata":{"execution":{"iopub.status.busy":"2021-07-09T07:25:04.096007Z","iopub.execute_input":"2021-07-09T07:25:04.096676Z","iopub.status.idle":"2021-07-09T07:25:04.101922Z","shell.execute_reply.started":"2021-07-09T07:25:04.096623Z","shell.execute_reply":"2021-07-09T07:25:04.101118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data loading 안될 때, [winequality-red.csv]복사 -> 오른쪽 상단 +Add data click -> 검색창에 붙여넣기 후 winequality-red를 찾아 오른쪽 add click, \n# 오른쪽 input 창에서 winequalityred를 누르고 하단의 winequality-red.csv에 마우스를 갖다대면 우측에 경로복사 click\n# 아래 \" \"안의 내용을 모두 드래그 해 다시 붙여넣는다.\nwine = pd.read_csv(\"../input/winequalityred/winequality-red.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-09T07:25:04.103536Z","iopub.execute_input":"2021-07-09T07:25:04.104099Z","iopub.status.idle":"2021-07-09T07:25:04.134169Z","shell.execute_reply.started":"2021-07-09T07:25:04.104066Z","shell.execute_reply":"2021-07-09T07:25:04.133267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wine.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T07:25:04.135568Z","iopub.execute_input":"2021-07-09T07:25:04.136004Z","iopub.status.idle":"2021-07-09T07:25:04.172537Z","shell.execute_reply.started":"2021-07-09T07:25:04.135971Z","shell.execute_reply":"2021-07-09T07:25:04.171739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y= quality에 대한 빈도 수\nquality_dist = wine['quality'].value_counts()\nplt.bar(quality_dist.index, quality_dist)\nplt.xlabel('quality')\nplt.ylabel('frequency')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T07:25:04.173709Z","iopub.execute_input":"2021-07-09T07:25:04.174145Z","iopub.status.idle":"2021-07-09T07:25:04.346386Z","shell.execute_reply.started":"2021-07-09T07:25:04.174103Z","shell.execute_reply":"2021-07-09T07:25:04.345649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. Data를 X, y 로 구분\n2. X, y에 대해 각각 train, test set 분할 (train: test = 70: 30, random_state = 42)\n3. (1)기본 Hyper-parameter의 DecisionTreeClassifier(random_state = 0)를 dt변수에 정의, (2) Train data fitting, (3) X_test 셋에 대한 예측 값을 도출하여 y_pred에 저장\n4. 하기 시각화 code를 이용하여 plotting (% runnig 시간 소요됨)\n5. classification_report()이용하여 test set에 대한 performance 확인\n6. Best hyper-parameter 도출(code 참조, (1) GridSearchCV()는 grid_tree에 정의하고, cv=3 으로 설정, (2) grid_tree를 train set에 fitting, (3) print(grid_tree.best_params_) )\n7. (1) Best parameter 값이 입력된 DecisionTreeClassifier(random_state = 0)를 dt_best 변수에 정의,(2) Train data fitting, (3) X_test 셋에 대한 예측 값을 도출하여 y_pred_best에 저장\n8. 시각화 code를 이용하여 plotting\n9. classification_report()이용하여 y_pred_best에 대한 개선된 performance 확인\n10. 기본 모델의 성능과 hyper-parameter 개선 후 성능의 차이에 대해 논의","metadata":{}},{"cell_type":"code","source":"#code copy 시 '''은 제외 후 붙여넣기, 모델명 자리에 dt or dt_best 입력\n'''\nplt.figure(figsize=(20, 15))\nplot_tree(decision_tree=모델명, filled=True)\nplt.show()\n# fig.savefig('imagename.png') # <- 그림 확인하고 싶은 경우, 저장 후 확인\n'''","metadata":{"execution":{"iopub.status.busy":"2021-07-09T07:25:04.347449Z","iopub.execute_input":"2021-07-09T07:25:04.347854Z","iopub.status.idle":"2021-07-09T07:25:04.353183Z","shell.execute_reply.started":"2021-07-09T07:25:04.347813Z","shell.execute_reply":"2021-07-09T07:25:04.352062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#code copy 시 '''은 제외 후 붙여넣기\n'''\nparams = {\n    \"criterion\" : ['gini', 'entropy'],\n    \"max_depth\" : [3, 4, 5],\n    \"min_samples_split\" : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n}\n'''","metadata":{"execution":{"iopub.status.busy":"2021-07-09T07:25:04.354501Z","iopub.execute_input":"2021-07-09T07:25:04.354835Z","iopub.status.idle":"2021-07-09T07:25:04.371713Z","shell.execute_reply.started":"2021-07-09T07:25:04.35478Z","shell.execute_reply":"2021-07-09T07:25:04.370586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Data를 X, y 로 구분","metadata":{}},{"cell_type":"code","source":"X = wine.drop('quality',axis=1)\ny = wine['quality']","metadata":{"execution":{"iopub.status.busy":"2021-07-09T07:25:04.374259Z","iopub.execute_input":"2021-07-09T07:25:04.374545Z","iopub.status.idle":"2021-07-09T07:25:04.383517Z","shell.execute_reply.started":"2021-07-09T07:25:04.374517Z","shell.execute_reply":"2021-07-09T07:25:04.382283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. X, y에 대해 각각 train, test set 분할 (train: test = 70: 30, random_state = 42)","metadata":{}},{"cell_type":"code","source":"#Choosing 40% as training data.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T07:25:04.385567Z","iopub.execute_input":"2021-07-09T07:25:04.386005Z","iopub.status.idle":"2021-07-09T07:25:04.397629Z","shell.execute_reply.started":"2021-07-09T07:25:04.385929Z","shell.execute_reply":"2021-07-09T07:25:04.396464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. (1)기본 Hyper-parameter의 DecisionTreeClassifier()를 dt변수에 정의, (2) Train data fitting, (3) X_test 셋에 대한 예측 값을 도출하여 y_pred에 저장","metadata":{}},{"cell_type":"code","source":"#(1)\ndt = DecisionTreeClassifier(random_state = 0)\n#(2)\ndt.fit(X_train,y_train)\n#(3)\ny_pred = dt.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T07:25:04.399312Z","iopub.execute_input":"2021-07-09T07:25:04.399646Z","iopub.status.idle":"2021-07-09T07:25:04.423457Z","shell.execute_reply.started":"2021-07-09T07:25:04.399616Z","shell.execute_reply":"2021-07-09T07:25:04.422162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. 하기 시각화 code를 이용하여 plotting","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 15))\nplot_tree(decision_tree= dt, filled=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T07:25:04.424971Z","iopub.execute_input":"2021-07-09T07:25:04.425412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5.classification_report()이용하여 performance 확인","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Best hyper-parameter 도출","metadata":{}},{"cell_type":"code","source":"params = {\n    \"criterion\" : ['gini', 'entropy'],\n    \"max_depth\" : [3, 4, 5],\n    \"min_samples_split\" : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#(1)\ngrid_tree = GridSearchCV(dt, param_grid=params, cv=5, refit=True)\n#(2)\ngrid_tree.fit(X_train, y_train)\n#(3)\nprint( grid_tree.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('best parameters : ', grid_tree.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. (1) Best model을 dt_best 변수에 정의,(2) Train data fitting, (3) X_test 셋에 대한 예측 값을 도출하여 y_pred_best에 저장","metadata":{"execution":{"iopub.status.busy":"2021-07-08T09:15:15.134202Z","iopub.status.idle":"2021-07-08T09:15:15.134797Z"}}},{"cell_type":"code","source":"#(1)\ndt_best = DecisionTreeClassifier(criterion= 'gini', max_depth = 5, min_samples_split = 2,random_state = 0)\n#(2)\ndt_best.fit(X_train,y_train)\n#(3)\ny_pred_best = dt_best.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. 시각화 code를 이용하여 plotting","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 15))\nplot_tree(decision_tree=dt_best, filled=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9. classification_report()이용하여 개선된 performance 확인","metadata":{}},{"cell_type":"code","source":"print(classification_report(y_test, y_pred_best))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 10. 성능에 대한 논의\n- 개선된 모델은 다수의 인스턴스가 밀집된 class의 성능을 높이는데 파라미터를 조정한 것으로 보여짐, Major class의 성능을 올리는데 주력하면 과적합을 방지하는 효과가 있음\n- 그러나 다른 class의 예측 성능이 현저히 낮으므로 소수의 class 분류에 중요도를 높이고 싶다면 다시 튜닝을 시도해야 함","metadata":{}}]}