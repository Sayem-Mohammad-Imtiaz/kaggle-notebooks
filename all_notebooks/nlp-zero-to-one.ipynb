{"cells":[{"metadata":{},"cell_type":"markdown","source":"Anything we express (verbally or in writtern) carries huge amount of information. And that information can be used to predict human behaviour. In this notebook I'll try to explain how Natural Language Processing can be used to see multiple facet of data and get meaningful information out of that.\n"},{"metadata":{},"cell_type":"markdown","source":"# NLP\nNatural Language Processing is a field of Artificial Intelligence that gives the machines the ability to read, understand and derive meaning from human languages.\n\nIn simple words, NLP represents the automatic handling of natural human language like speech or text.\n\n## Use cases of NLP\n- Sentiment analysis\n- Cognitive assistant\n- Identifying fake news/spams \n- Personalized chat bots\netc."},{"metadata":{},"cell_type":"markdown","source":"# NLP with spaCy\n\nspaCy is the leading library for NLP, and it has quickly become one of the most popular Python frameworks. Most people find it intuitive, and it has excellent [documentation](https://spacy.io/usage).\n\nspaCy relies on models that are language-specific and come in different sizes. You can load a spaCy model with `spacy.load.`\n\nFor example, here's how you would load the English language model.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nnlp = spacy.load('en')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the model loaded, you can process text like this:"},{"metadata":{"trusted":true},"cell_type":"code","source":"doc = nlp(\"Tea is healthy and calming, don't you think?\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There's a lot you can do with the doc object you just created."},{"metadata":{},"cell_type":"markdown","source":"# Tokenizing\nThis returns a document object that contains tokens. A token is a unit of text in the document, such as individual words and punctuation. SpaCy splits contractions like \"don't\" into two tokens, \"do\" and \"n't\".\n\nYou can see the tokens by iterating through the document."},{"metadata":{"trusted":true},"cell_type":"code","source":"for token in doc:\n    print(token)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Why are lemmas and identifying stopwords important? Language data has a lot of noise mixed in with informative content. In the sentence above, the important words are tea, healthy and calming. Removing stop words might help the predictive model focus on relevant words. Lemmatizing similarly helps by combining multiple forms of the same word into one base form (\"calming\", \"calms\", \"calmed\" would all change to \"calm\").\n\nHowever, lemmatizing and dropping stopwords might result in your models performing worse. So you should treat this preprocessing as part of your hyperparameter optimization process."},{"metadata":{},"cell_type":"markdown","source":"# Pattern Matching\nAnother common NLP task is matching tokens or phrases within chunks of text or whole documents. You can do pattern matching with regular expressions, but spaCy's matching capabilities tend to be easier to use.\n\nTo match individual tokens, you create a `Matcher`. When you want to match a list of terms, it's easier and more efficient to use `PhraseMatcher`. For example, if you want to find where different smartphone models show up in some text, you can create patterns for the model names of interest. First you create the `PhraseMatcher`itself."},{"metadata":{"trusted":true},"cell_type":"code","source":"from spacy.matcher import PhraseMatcher\nmatcher = PhraseMatcher(nlp.vocab, attr='LOWER')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The matcher is created using the vocabulary of your model. Here we're using the small English model you loaded earlier. Setting `attr='LOWER'` will match the phrases on lowercased text. This provides case insensitive matching.\n\nNext you create a list of terms to match in the text. The phrase matcher needs the patterns as document objects. The easiest way to get these is with a list comprehension using the `nlp` model."},{"metadata":{"trusted":true},"cell_type":"code","source":"terms = ['Galaxy Note', 'iPhone 11', 'iPhone XS', 'Google Pixel']\npatterns = [nlp(text) for text in terms]\nmatcher.add(\"TerminologyList\", None, *patterns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then you create a document from the text to search and use the phrase matcher to find where the terms occur in the text."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Borrowed from https://daringfireball.net/linked/2019/09/21/patel-11-pro\ntext_doc = nlp(\"Glowing review overall, and some really interesting side-by-side \"\n               \"photography tests pitting the iPhone 11 Pro against the \"\n               \"Galaxy Note 10 Plus and last yearâ€™s iPhone XS and Google Pixel 3.\") \nmatches = matcher(text_doc)\nprint(matches)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The matches here are a tuple of the match id and the positions of the start and end of the phrase."},{"metadata":{"trusted":true},"cell_type":"code","source":"match_id, start, end = matches[0]\nprint(nlp.vocab.strings[match_id], text_doc[start:end])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Classification with SpaCy\nA common task in NLP is text classification. This is \"classification\" in the conventional machine learning sense, and it is applied to text. Examples include spam detection, sentiment analysis, and tagging customer queries.\n\nIn this tutorial, you'll learn text classification with spaCy. The classifier will detect spam messages, a common functionality in most email clients. Here is an overview of the data you'll use:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n# Loading the spam data\nspam = pd.read_csv('../input/nlp-course/spam.csv')\n# Read data\nspam.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bag of Words\nMachine learning models don't learn from raw text data. Instead, you need to convert the text to something numeric.\n\nThe simplest common representation is a variation of one-hot encoding. You represent each document as a vector of term frequencies for each term in the vocabulary. The vocabulary is built from all the tokens (terms) in the corpus (the collection of documents).\n\nAs an example, take the sentences \"Tea is life. Tea is love.\" and \"Tea is healthy, calming, and delicious.\" as our corpus. The vocabulary then is `{\"tea\", \"is\", \"life\", \"love\", \"healthy\", \"calming\", \"and\", \"delicious\"}` (ignoring punctuation).\n\nFor each document, count up how many times a term occurs, and place that count in the appropriate element of a vector. The first sentence has \"tea\" twice and that is the first position in our vocabulary, so we put the number 2 in the first element of the vector. Our sentences as vectors then look like\n\n\\begin{align}\nv_1 &= \\left[\\begin{matrix} 2 & 2 & 1 & 1 & 0 & 0 & 0 & 0 \\end{matrix}\\right] \\\\\nv_2 &= \\left[\\begin{matrix} 1 & 1 & 0 & 0 & 1 & 1 & 1 & 1 \\end{matrix}\\right]\n\\end{align}\n \nThis is called the **bag of words** representation. You can see that documents with similar terms will have similar vectors. Vocabularies frequently have tens of thousands of terms, so these vectors can be very large.\n\nAnother common representation is **TF-IDF (Term Frequency - Inverse Document Frequency)**. TF-IDF is similar to bag of words except that each term count is scaled by the term's frequency in the corpus."},{"metadata":{},"cell_type":"markdown","source":"## Building a Bag of Words model\nOnce you have your documents in a bag of words representation, you can use those vectors as input to any machine learning model. spaCy handles the bag of words conversion and building a simple linear model for you with the `TextCategorizer` class.\n\nThe TextCategorizer is a spaCy **pipe**. Pipes are classes for processing and transforming tokens. When you create a spaCy model with `nlp = spacy.load('en_core_web_sm')`, there are default pipes that perform part of speech tagging, entity recognition, and other transformations. When you run text through a model `doc = nlp(\"Some text here\")`, the output of the pipes are attached to the tokens in the `doc` object. The lemmas for `token.lemma_` come from one of these pipes.\n\nYou can remove or add pipes to models. What we'll do here is create an empty model without any pipes (other than a tokenizer, since all models always have a tokenizer). Then, we'll create a TextCategorizer pipe and add it to the empty model."},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\n\n# Create an empty model\nnlp = spacy.blank('en')\n\n# Create the TextCategorizer with exclusive classes and \"bow\" architecture\ntextcat = nlp.create_pipe(\"textcat\", config={\"exclusive_classes\": True, \"architecture\":\"bow\"})\n\n# Add the TextCategorizer to the empty model\nnlp.add_pipe(textcat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the classes are either ham or spam, we set `\"exclusive_classes\"` to `True`. We've also configured it with the bag of words (`\"bow\"`) architecture. spaCy provides a convolutional neural network architecture as well, but it's more complex than you need for now.\n\nNext we'll add the labels to the model. Here \"ham\" are for the real messages, \"spam\" are spam messages."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add labels to text classifier\ntextcat.add_label(\"ham\")\ntextcat.add_label(\"spam\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training the Text Categorizer Model\nNext, you'll convert the labels in the data to the form TextCategorizer requires. For each document, you'll create a dictionary of boolean values for each class.\n\nFor example, if a text is \"ham\", we need a dictionary `{'ham': True, 'spam': False}`. The model is looking for these labels inside another dictionary with the key `'cats'`."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_texts = spam['text'].values\ntrain_labels = [{'cats':{'ham':label == 'ham', 'spam':label == 'spam'}}\n               for label in spam['label']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we combine the texts and labels into a single list."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = list(zip(train_texts, train_labels))\ntrain_data[:3]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}