{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"------------------------------------------------------------------------\nThis notebook is a Multi Layer Perceptron(MLP) implementation for a Tabular data classification problem using Pytorch . Are **Neural Nets an overkill or do they lack performance on Tabular data** ? \n\nWell recently there has been a lot of buzz around utility of NN's in tabular data.In a study , Gradient boosting methods still outperform most scenarios of tabular data world . Related reading in this paper - [Deep Learning is not all you need](https://arxiv.org/pdf/2106.03253.pdf)\n\nPlease remember to **upvote this notebook** for encouragement , i'll be grateful :-)\n\n--------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"### Background on the Dataset üìù\n**What is Phishing ?**\n\nIt is a form of fraud in which the attacker tries to learn sensitive information such as login credentials or account information by pretending as a reputable entity or person via email or other communication channel means\n\nPhishing is popular among attackers, since it is easier to trick someone into clicking a malicious link which seems legitimate\n\nThe **URL** of phishing websites may be **very similar to real websites** to the human eye, but they are different in IP. ","metadata":{"execution":{"iopub.status.busy":"2021-06-28T07:20:51.027869Z","iopub.execute_input":"2021-06-28T07:20:51.028601Z","iopub.status.idle":"2021-06-28T07:20:51.039995Z","shell.execute_reply.started":"2021-06-28T07:20:51.028451Z","shell.execute_reply":"2021-06-28T07:20:51.038853Z"}}},{"cell_type":"markdown","source":"#### URL Basics\n\n![URL Structure](https://i.imgur.com/elrD4Vl.png)\n\n* **Domain name** portion is constrained since it has to be registered with a domain name Registrar\n* **Subdomain name** and **Path** are fully controllable by the phisher","metadata":{}},{"cell_type":"markdown","source":"### Phishing dataset Exploration","metadata":{}},{"cell_type":"code","source":"\nimport os\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.patches import Patch\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import preprocessing\nfrom sklearn.metrics import f1_score as f1\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import *\n\n#-- Pytorch specific libraries import -----#\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset\nfrom torch.utils.data import DataLoader\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T14:30:45.644812Z","iopub.execute_input":"2021-06-28T14:30:45.645421Z","iopub.status.idle":"2021-06-28T14:30:45.653143Z","shell.execute_reply.started":"2021-06-28T14:30:45.645384Z","shell.execute_reply":"2021-06-28T14:30:45.652217Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_data = pd.read_csv(\"../input/web-page-phishing-detection-dataset/dataset_phishing.csv\")\ndf_data.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-28T14:30:45.771887Z","iopub.execute_input":"2021-06-28T14:30:45.772381Z","iopub.status.idle":"2021-06-28T14:30:45.899185Z","shell.execute_reply.started":"2021-06-28T14:30:45.772333Z","shell.execute_reply":"2021-06-28T14:30:45.898407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_data.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T14:30:45.900357Z","iopub.execute_input":"2021-06-28T14:30:45.900798Z","iopub.status.idle":"2021-06-28T14:30:45.924219Z","shell.execute_reply.started":"2021-06-28T14:30:45.900747Z","shell.execute_reply":"2021-06-28T14:30:45.92251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Quick Observations\n* **URL** itself is present as a field\n* Target Variable is : **status** with \n    * legitimate (considered as Label 1)\n    * phishing   (considered as Label 0)\n* **87 Features** around URL , **all Numerical** ,are from three different classes\n    * 56 extracted from the Structure and Syntax of URLs (fields starting with : *nb_* , *shortest/longest* , *ratio/length* etc)\n    * 24 extracted from the Content of their correspondent pages \n    * 7 are extracted by querying External Services","metadata":{}},{"cell_type":"code","source":"#Encoding 'status' as label 1 & 0 , naming the field as target\ndf_data['target'] = pd.get_dummies(df_data['status'])['legitimate'].astype('int')\ndf_data.drop('status',axis = 1, inplace=True)\ndf_data[['url','target']].head(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T14:30:45.926165Z","iopub.execute_input":"2021-06-28T14:30:45.926588Z","iopub.status.idle":"2021-06-28T14:30:45.964019Z","shell.execute_reply.started":"2021-06-28T14:30:45.92654Z","shell.execute_reply":"2021-06-28T14:30:45.963037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**No Missing values in the dataset**","metadata":{}},{"cell_type":"code","source":"tmp = df_data.isnull().sum().reset_index(name='missing_val')\ntmp[tmp['missing_val']!= 0]","metadata":{"execution":{"iopub.status.busy":"2021-06-28T14:30:45.966479Z","iopub.execute_input":"2021-06-28T14:30:45.966929Z","iopub.status.idle":"2021-06-28T14:30:45.98175Z","shell.execute_reply.started":"2021-06-28T14:30:45.966896Z","shell.execute_reply":"2021-06-28T14:30:45.980942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Idenitfying Categorical columns** : If column has unique values lower than 0.002% of total records then categorizing it as Categorical","metadata":{}},{"cell_type":"code","source":"likely_cat = {}\nfor var in df_data.iloc[:,1:].columns:\n    likely_cat[var] = 1.*df_data[var].nunique()/df_data[var].count() < 0.002 \n\nnum_cols = []\ncat_cols = []\nfor col in likely_cat.keys():\n    if (likely_cat[col] == False):\n        num_cols.append(col)\n    else:\n        cat_cols.append(col)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-28T14:30:45.983127Z","iopub.execute_input":"2021-06-28T14:30:45.9839Z","iopub.status.idle":"2021-06-28T14:30:46.033999Z","shell.execute_reply.started":"2021-06-28T14:30:45.983866Z","shell.execute_reply":"2021-06-28T14:30:46.032512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Correlation Plot for the Numerical(continuous) features\n","metadata":{}},{"cell_type":"code","source":"#Taking all columns except URL \ncorr = df_data[num_cols].corr()\n\nfig = plt.figure(figsize=(12,12),dpi=80)\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask=mask, cmap='BuPu', robust=True, center=0,\n            square=True, linewidths=.5)\nplt.title('Correlation of Numerical(Continous) Features', fontsize=15,font=\"Serif\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T14:30:46.035736Z","iopub.execute_input":"2021-06-28T14:30:46.036096Z","iopub.status.idle":"2021-06-28T14:30:47.244525Z","shell.execute_reply.started":"2021-06-28T14:30:46.036066Z","shell.execute_reply":"2021-06-28T14:30:47.243594Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations** :Few highly correlated features as one would expect.Nothing jumps out extraordinary here.\n\n*length_words_raw* is expected to be highly correlated with *length_url* . Similar behaviour for other correlated feature pairs\n","metadata":{}},{"cell_type":"markdown","source":"#### Distribution of Mean values of the Numerical features across Target variable","metadata":{}},{"cell_type":"code","source":"df_distr =df_data.groupby('target')[num_cols].mean().reset_index().T\ndf_distr.rename(columns={0:'0_Label',1:\"1_Label\"}, inplace=True)\n\n#plt.style.use('ggplot')\nplt.rcParams['axes.facecolor']='w'\nax = df_distr[1:-3][['0_Label','1_Label']].plot(kind='bar', title =\"Distribution of Average values across Target\", figsize=(12, 8), legend=True, fontsize=12)\nax.set_xlabel(\"Numerical Features\", fontsize=14)\nax.set_ylabel(\"Average Values\", fontsize=14)\n#ax.set_ylim(0,500000)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T14:30:47.245619Z","iopub.execute_input":"2021-06-28T14:30:47.246042Z","iopub.status.idle":"2021-06-28T14:30:47.834736Z","shell.execute_reply.started":"2021-06-28T14:30:47.246012Z","shell.execute_reply":"2021-06-28T14:30:47.833133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**\n\n* Higher the length of URL ,or words in URL then more likely to be *Phishing* URL\n* Clear distinctive pattern of *nb_links* field.Higher implies *Legitimate* URL\n* *links_in_tags ,safe_anchor*  higher volume signals more towards *Legitmate* site\n","metadata":{}},{"cell_type":"markdown","source":"**Exploring Page Ranks feature w.r.t Target variable**\n\n* Legitimate URLs form more of a gaussian distribution as the ranking increases , whereas the Phishing ones present a right skewed distribution.Presence of differentiating patterns here","metadata":{}},{"cell_type":"code","source":"sns.catplot(\"page_rank\", hue=\"target\", data=df_data, kind=\"count\", \n            palette={1:\"green\", 0:\"blue\"} ,height=5.0, aspect=11.7/8.27 )","metadata":{"execution":{"iopub.status.busy":"2021-06-28T14:30:47.83659Z","iopub.execute_input":"2021-06-28T14:30:47.837014Z","iopub.status.idle":"2021-06-28T14:30:48.474809Z","shell.execute_reply.started":"2021-06-28T14:30:47.836979Z","shell.execute_reply":"2021-06-28T14:30:48.473716Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Note:**\n\n* Aspects such has multicollinearity , variance captured by the features etc are applicable to Linear & Tree models as their performance tends to vary with these aspects.Since we are progressing with a Multi Layer Perceptron (MLP) , a neural net doesn't really care about all these transformations and raw data is what it needs to find the underlying patterns on it's own","metadata":{}},{"cell_type":"markdown","source":"#### Splitting dataset into Train & Test","metadata":{}},{"cell_type":"code","source":"#Train & Test Set\nX= df_data.iloc[: , 1:-1]\n#y = upsampled_df['Churn']\ny= df_data['target']\n\ntrain_x,test_x,train_y,test_y = train_test_split(X,y,random_state=42)\nprint(\"\\n--Training data samples--\")\nprint(train_x.shape)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-28T14:30:48.475932Z","iopub.execute_input":"2021-06-28T14:30:48.476247Z","iopub.status.idle":"2021-06-28T14:30:48.50489Z","shell.execute_reply.started":"2021-06-28T14:30:48.476186Z","shell.execute_reply":"2021-06-28T14:30:48.503577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pytorch Neural Net Model\n**Convert data into Pytorch Tensors**","metadata":{}},{"cell_type":"code","source":"###First use a MinMaxscaler to scale all the features of Train & Test dataframes\n\nscaler = preprocessing.MinMaxScaler()\nx_train = scaler.fit_transform(train_x.values)\nx_test =  scaler.fit_transform(test_x.values)\n\nprint(\"Scaled values of Train set \\n\")\nprint(x_train)\nprint(\"\\nScaled values of Test set \\n\")\nprint(x_test)\n\n\n###Then convert the Train and Test sets into Tensors\n\nx_tensor =  torch.from_numpy(x_train).float()\ny_tensor =  torch.from_numpy(train_y.values.ravel()).float()\nxtest_tensor =  torch.from_numpy(x_test).float()\nytest_tensor =  torch.from_numpy(test_y.values.ravel()).float()\n\nprint(\"\\nTrain set Tensors \\n\")\nprint(x_tensor)\nprint(y_tensor)\nprint(\"\\nTest set Tensors \\n\")\nprint(xtest_tensor)\nprint(ytest_tensor)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T14:30:48.506481Z","iopub.execute_input":"2021-06-28T14:30:48.506785Z","iopub.status.idle":"2021-06-28T14:30:48.566264Z","shell.execute_reply.started":"2021-06-28T14:30:48.506756Z","shell.execute_reply":"2021-06-28T14:30:48.565303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DataLoader to pass the data in batches to the model","metadata":{}},{"cell_type":"code","source":"#Define a batch size , \nbs = 64\n#Both x_train and y_train can be combined in a single TensorDataset, which will be easier to iterate over and slice\ny_tensor = y_tensor.unsqueeze(1)\ntrain_ds = TensorDataset(x_tensor, y_tensor)\n#Pytorch‚Äôs DataLoader is responsible for managing batches. \n#You can create a DataLoader from any Dataset. DataLoader makes it easier to iterate over batches\ntrain_dl = DataLoader(train_ds, batch_size=bs)\n\n\n#For the validation/test dataset\nytest_tensor = ytest_tensor.unsqueeze(1)\ntest_ds = TensorDataset(xtest_tensor, ytest_tensor)\ntest_loader = DataLoader(test_ds, batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T14:30:48.567658Z","iopub.execute_input":"2021-06-28T14:30:48.567978Z","iopub.status.idle":"2021-06-28T14:30:48.575093Z","shell.execute_reply.started":"2021-06-28T14:30:48.567945Z","shell.execute_reply":"2021-06-28T14:30:48.574009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### MLP (Model)\nDefine the Layers , Activation function , Number of nodes for the MultiLayerPerceptron\n\n#### Structure of MLP\n\n* 2 Hidden Layers\n* Normalizing the batch data usign batchnorm in between each layer\n* Using ReLU Activation function between the layers\n* Using dropout before sending to output\n* Sigmoid at the output layer to make probabilities between 0 to 1","metadata":{}},{"cell_type":"code","source":"n_input_dim = train_x.shape[1]\n\n#Layer size\nn_hidden1 = 300  # Number of hidden nodes\nn_hidden2 = 100\nn_output =  1   # Number of output nodes = for binary classifier\n\n\nclass ChurnModel(nn.Module):\n    def __init__(self):\n        super(ChurnModel, self).__init__()\n        self.layer_1 = nn.Linear(n_input_dim, n_hidden1) \n        self.layer_2 = nn.Linear(n_hidden1, n_hidden2)\n        self.layer_out = nn.Linear(n_hidden2, n_output) \n        \n        \n        self.relu = nn.ReLU()\n        self.sigmoid =  nn.Sigmoid()\n        self.dropout = nn.Dropout(p=0.1)\n        self.batchnorm1 = nn.BatchNorm1d(n_hidden1)\n        self.batchnorm2 = nn.BatchNorm1d(n_hidden2)\n        \n        \n    def forward(self, inputs):\n        x = self.relu(self.layer_1(inputs))\n        x = self.batchnorm1(x)\n        x = self.relu(self.layer_2(x))\n        x = self.batchnorm2(x)\n        x = self.dropout(x)\n        x = self.sigmoid(self.layer_out(x))\n        \n        return x\n    \n\nmodel = ChurnModel()\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T14:30:48.577274Z","iopub.execute_input":"2021-06-28T14:30:48.57789Z","iopub.status.idle":"2021-06-28T14:30:48.603247Z","shell.execute_reply.started":"2021-06-28T14:30:48.577853Z","shell.execute_reply":"2021-06-28T14:30:48.602223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Defining parameters (pretty much standard setting for Binary class problem)\n* Loss computation function : Here using Binary Cross Entropy (BCE) which is defacto for Binary class problems\n* Learning rate : Setting as 0.001 (can be optimized further)\n* Optimizer : Using Adam and\n* Epochs of Training : setting as 50","metadata":{}},{"cell_type":"code","source":"#Loss Computation\nloss_func = nn.BCELoss()\n#Optimizer\nlearning_rate = 0.001\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nepochs = 50","metadata":{"execution":{"iopub.status.busy":"2021-06-28T14:30:48.6048Z","iopub.execute_input":"2021-06-28T14:30:48.605321Z","iopub.status.idle":"2021-06-28T14:30:48.611789Z","shell.execute_reply.started":"2021-06-28T14:30:48.60527Z","shell.execute_reply":"2021-06-28T14:30:48.610851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training the MLP Model\nNN Steps flow\n\n* Forward Propagation\n* Loss computation\n* Backpropagation\n* Updating the parameters","metadata":{}},{"cell_type":"code","source":"model.train()\ntrain_loss = []\nfor epoch in range(epochs):\n    #Within each epoch run the subsets of data = batch sizes.\n    for xb, yb in train_dl:\n        y_pred = model(xb)            # Forward Propagation\n        loss = loss_func(y_pred, yb)  # Loss Computation\n        optimizer.zero_grad()         # Clearing all previous gradients, setting to zero \n        loss.backward()               # Back Propagation\n        optimizer.step()              # Updating the parameters \n    #print(\"Loss in iteration :\"+str(epoch)+\" is: \"+str(loss.item()))\n    train_loss.append(loss.item())\nprint('Last iteration loss value: '+str(loss.item()))","metadata":{"execution":{"iopub.status.busy":"2021-06-28T14:30:48.613157Z","iopub.execute_input":"2021-06-28T14:30:48.613661Z","iopub.status.idle":"2021-06-28T14:31:44.686403Z","shell.execute_reply.started":"2021-06-28T14:30:48.613614Z","shell.execute_reply":"2021-06-28T14:31:44.685497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plotting the loss shows that model pretty much stabilized after 30 epochs itself**","metadata":{}},{"cell_type":"code","source":"plt.plot(train_loss)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T14:31:44.687728Z","iopub.execute_input":"2021-06-28T14:31:44.688314Z","iopub.status.idle":"2021-06-28T14:31:44.90841Z","shell.execute_reply.started":"2021-06-28T14:31:44.688274Z","shell.execute_reply":"2021-06-28T14:31:44.907636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Test Dataset prediction on trained NN","metadata":{}},{"cell_type":"code","source":"import itertools\n\ny_pred_list = []\nmodel.eval()\n#Since we don't need model to back propagate the gradients in test set we use torch.no_grad()\n# reduces memory usage and speeds up computation\nwith torch.no_grad():\n    for xb_test,yb_test  in test_loader:\n        y_test_pred = model(xb_test)\n        y_pred_tag = torch.round(y_test_pred)\n        y_pred_list.append(y_pred_tag.detach().numpy())\n\n#Takes arrays and makes them list of list for each batch        \ny_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n#flattens the lists in sequence\nytest_pred = list(itertools.chain.from_iterable(y_pred_list))","metadata":{"execution":{"iopub.status.busy":"2021-06-28T14:31:44.90961Z","iopub.execute_input":"2021-06-28T14:31:44.910125Z","iopub.status.idle":"2021-06-28T14:31:44.983754Z","shell.execute_reply.started":"2021-06-28T14:31:44.910091Z","shell.execute_reply":"2021-06-28T14:31:44.982816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Metrics that matter\n\n* Precision\n* Recall\n* F1 Score\n* Confusion Matrix","metadata":{}},{"cell_type":"code","source":"y_true_test = test_y.values.ravel()\nconf_matrix = confusion_matrix(y_true_test ,ytest_pred)\nprint(\"Confusion Matrix of the Test Set\")\nprint(\"-----------\")\nprint(conf_matrix)\nprint(\"Precision of the MLP :\\t\"+str(precision_score(y_true_test,ytest_pred)))\nprint(\"Recall of the MLP    :\\t\"+str(recall_score(y_true_test,ytest_pred)))\nprint(\"F1 Score of the Model :\\t\"+str(f1_score(y_true_test,ytest_pred)))","metadata":{"execution":{"iopub.status.busy":"2021-06-28T14:31:44.985309Z","iopub.execute_input":"2021-06-28T14:31:44.986019Z","iopub.status.idle":"2021-06-28T14:31:45.040076Z","shell.execute_reply.started":"2021-06-28T14:31:44.98597Z","shell.execute_reply":"2021-06-28T14:31:45.039248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So here we used a Neural Net for a Tabular data classification problem and got pretty good performance.\n\n**If you found this notebook helpful then please upvote , i'll be grateful** :-)","metadata":{}}]}