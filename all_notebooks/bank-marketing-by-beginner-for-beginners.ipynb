{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This is a classification problem... i think it is about bank marketing analysis (it wasn't specified but there is a similar dataset available on kaggle by the name)","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.metrics import confusion_matrix, classification_report","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/beginner-datasets/beginner_datasets/bank.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.poutcome.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lets go through each feature","metadata":{}},{"cell_type":"markdown","source":"### Ok guys, i want to explore some more plots today so i'll be referecing the seaborn documentation about how to plot data when one of the features is categorical...\n\nhttps://seaborn.pydata.org/tutorial/categorical.html","metadata":{}},{"cell_type":"code","source":"# catplot\n\nsns.catplot(x=\"deposit\", y=\"age\", data=df);\n\n# using jitter = False option doesn't give any info here and","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As the size of the dataset grows, categorical scatter plots become limited in the information they can provide about the distribution of values within each category. When this happens, there are several approaches for summarizing the distributional information in ways that facilitate easy comparisons across the category levels","metadata":{}},{"cell_type":"code","source":"#boxplot\n\nsns.catplot(y=\"deposit\", x=\"age\", kind=\"box\", data=df);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://www.statisticshowto.com/probability-and-statistics/descriptive-statistics/box-plot/","metadata":{}},{"cell_type":"markdown","source":"The first is the familiar boxplot(). This kind of plot shows the three quartile values of the distribution along with extreme values. The “whiskers” extend to points that lie within 1.5 IQRs of the lower and upper quartile, and then observations that fall outside this range are displayed independently. This means that each value in the boxplot corresponds to an actual observation in the data.","metadata":{}},{"cell_type":"markdown","source":"**How to read? or what info you get?**\n\nThe box and whiskers chart shows you how your data is spread out. Five pieces of information (the “five number summary“) are generally included in the chart:\n\n- The minimum (the smallest number in the data set). The minimum is shown at the far left of the chart, at the end of the left “whisker.” or sleeping T shape\n\n- First quartile, Q1, is the far left of the box (or the far right of the left whisker).\n\n- The median is shown as a line in the center of the box.\n\n- Third quartile, Q3, shown at the far right of the box (at the far left of the right whisker).\n\n- The maximum (the largest number in the data set), shown at the far right of the box","metadata":{}},{"cell_type":"markdown","source":"**Note on Outliers:**\n\nData sets can sometimes contain outliers that are suspected to be anomalies (perhaps because of data collection errors or just plain old flukes). If outliers are present, the whisker on the appropriate side is drawn to 1.5 * IQR rather than the data minimum or the data maximum. Small circles or unfilled dots are drawn on the chart to indicate where suspected outliers lie. Filled circles are used for known outliers.\n\nother info... do read\n\nhttps://www.purplemath.com/modules/boxwhisk3.htm#:~:text=The%20%22interquartile%20range%22%2C%20abbreviated,box%2Dand%2Dwhisker%20plot.&text=The%20IQR%20tells%20how%20spread,far%22%20from%20the%20central%20value.","metadata":{}},{"cell_type":"code","source":"# hue = \"education\"\n\nsns.catplot(x=\"deposit\", y=\"age\",hue=\"education\", kind=\"box\", data=df);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A related function, boxenplot(), draws a plot that is similar to a box plot but optimized for showing more information about the shape of the distribution. It is best suited for larger datasets:","metadata":{}},{"cell_type":"code","source":"# kind=\"boxen\"\n\nsns.catplot(x=\"deposit\", y=\"age\",hue=\"education\", kind=\"boxen\", data=df);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.catplot(x=\"education\", y=\"age\", hue=\"deposit\",\n            kind=\"violin\", data=df);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://mode.com/blog/violin-plot-examples/\n\nThe box plot is an old standby for visualizing basic distributions. It's convenient for comparing summary statistics (such as range and quartiles), but it doesn't let you see variations in the data. For multimodal distributions (those with multiple peaks) this can be particularly limiting.\n\nBut fret not—this is where the violin plot comes in. A violin plot is a hybrid of a box plot and a kernel density plot, which shows peaks in the data.\n\nthe same summary statistics as box plots:\n\n - the white dot represents the median\n\n - the thick gray bar in the center represents the interquartile range\n\n - the thin gray line represents the rest of the distribution, except for points that are determined to be “outliers” using a method that is a function of the interquartile range.","metadata":{}},{"cell_type":"code","source":"# to be more space effecient use split=True\n\nsns.catplot(x=\"education\", y=\"age\", hue=\"deposit\",\n            kind=\"violin\",split=True, data=df);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.catplot(x=\"education\", y=\"age\", hue=\"deposit\", kind=\"bar\", data=df);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.catplot(x=\"education\", kind=\"count\", palette=\"ch:.25\", data=df);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.catplot(x=\"education\", y=\"age\", hue=\"deposit\", kind=\"point\", data=df);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A point plot represents an estimate of central tendency for a numeric variable by the position of scatter plot points and provides some indication of the uncertainty around that estimate using error bars.","metadata":{}},{"cell_type":"code","source":"sns.violinplot(x=df.deposit, y=df.age)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# using row instead of col cause there are a lot of values\n\nsns.catplot(x=\"education\", y=\"age\", hue=\"deposit\",\n            row=\"job\", aspect=0.7,\n            kind=\"box\", data=df);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# got my hands on this cool code from the website abt violine plot\n\nf, ax = plt.subplots(figsize=(8, 8))\n\n# Show each distribution with both violins and points\nsns.violinplot(x=\"deposit\",y=\"age\",data=df, palette=\"Set3\", inner=\"points\", bw =.2, cut=2, linewidth=3)\n\nsns.despine(left=True)\n\nf.suptitle('Deposit by age', fontsize=18, fontweight='bold')\nax.set_xlabel(\"Weight (g)\",size = 16,alpha=0.7)\nax.set_ylabel(\"Feed\",size = 16,alpha=0.7);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# i have seen a guy decrease the coding by writing a function where he feels necessary so i am going to try the same...\n# (i haven't been using functions a lot... it means i practice more but if i save time i can work on something else)\n# a function to do the ploting:\n\n\ndef eda(col):\n    print(\"--------------\",col,\"-----------------\")\n    if df[col].dtype == 'object':\n        # bar\n        df[col].value_counts().plot.bar(figsize=(10,5)); \n        plt.show()\n        # count\n        cat = sns.catplot(y=col, hue=\"deposit\", kind=\"count\",\n            palette=\"pastel\", edgecolor=\".6\",\n            data=df);\n        cat.set_xlabels(fontsize = 15)\n        cat.set_ylabels(fontsize = 15)\n        plt.show()\n        print(\"ratio of yes/no based on category of \" + col, end='\\n\\n')\n        for cat in df[col].unique():\n            print(cat,\": \" ,df.groupby([col]).deposit.value_counts()[cat]['yes']/df.groupby([col]).deposit.value_counts()[cat]['no'])\n    else:\n        sns.displot(df[col], color='r', kde=True,);\n        plt.show()        \n        sns.barplot(x=df['deposit'], y=df[col]);\n        plt.show()        \n        # box\n        print(\"box\")\n        sns.catplot(y=\"deposit\", x=col, kind=\"box\", data=df);\n        plt.show()\n        # voilin\n        sns.catplot(x=\"deposit\", y=col,\n            kind=\"violin\",split=True, data=df);\n        plt.show()\n        sns.boxplot(x=df[col]);\n        plt.show()\n    print(\"\\n\\n\\n\\n\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda(\"age\")\n\n# right skewed\n# there are obviously a lot of outliers...\n# violine plot says the ones with has have a bit of greater density of outliers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda('job')\n# gonna try one hot encoding...","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda('marital')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda(\"education\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda(\"default\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda(\"balance\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda(\"housing\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda(\"loan\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda(\"contact\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda(\"day\")\n# we have 3 peaks","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda(\"month\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda(\"duration\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda(\"campaign\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda(\"pdays\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda(\"previous\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda(\"poutcome\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" cat = sns.catplot(y='deposit', kind=\"count\",\n            palette=\"pastel\", edgecolor=\".6\",\n            data=df);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['deposit'].value_counts().plot.bar(figsize=(10,5)); ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"### we can try to transform the features but logistic is not a good option here at all... most of the numeric data is really skewed, decision tree will work, if we use something like k nearest neighbours then it will be helpful if we perform feature scaling so lets just start off with that","metadata":{}},{"cell_type":"code","source":"# get dataframe of categorical values after one hot encoding\nencoder = OneHotEncoder(drop='first')\narry = []\nfor col in df.columns:\n    if df[col].dtype == 'object':\n        arry.append(pd.DataFrame(encoder.fit_transform(df[[col]]).toarray()).add_suffix(col))\n        \ncat_df = pd.concat([arry[0],arry[1],arry[2],arry[3],arry[4],arry[5],arry[6],arry[7],arry[8],arry[9]], axis=1)\ncat_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get all numeric columns after normalizing\nscaler = StandardScaler()\narry =[]\nfor col in df.columns:\n    if df[col].dtype != 'object':\n        arry.append(pd.DataFrame(scaler.fit_transform(df[[col]]), columns=[col]))\n        \nnumeric_df = pd.concat([arry[0],arry[1],arry[2],arry[3],arry[4],arry[5],arry[6]], axis=1)\nnumeric_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = cat_df['0deposit']\nX = pd.concat([cat_df.drop(['0deposit'], axis=1), numeric_df], axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model building","metadata":{}},{"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X ,y, test_size = 0.3, random_state=42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt_model = DecisionTreeClassifier()\ndt_model.fit(X_train,y_train)\ny_pred = dt_model.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\ndt_model.score(X_test,y_test)\n\n# then random forest will perform better","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_model = RandomForestClassifier()\nrf_model.fit(X_train,y_train)\ny_pred = rf_model.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nrf_model.score(X_test,y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn_model = KNeighborsClassifier()\nknn_model.fit(X_train,y_train)\ny_pred = knn_model.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nknn_model.score(X_test,y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xg_model = XGBClassifier(verbosity = 0)\nxg_model.fit(X_train,y_train)\ny_pred = knn_model.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nknn_model.score(X_test,y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### There is a clustering approach that we can use... first we cluster the data then we determine which model performs best for each of the clusters.... later this way we can while predicting find the cluster a set of data belongs to and use the model which performs best for that cluster\n\nand ofcourse we need more metrics then just accuracy score","metadata":{}},{"cell_type":"code","source":"def clustering_approach(X,y, models,type = \"none\"):\n    \n    dfs = {}\n    X_cls = {}\n    y_cls = {}\n    X_scaled = {}\n    X_train, X_test, y_train, y_test = {},{},{},{}\n    y_pred = {}\n    models_out = {}\n    \n    # create knn model and predict\n    knn_clf = KNeighborsClassifier()\n    knn_clf.fit(X,y)\n    df = pd.concat([X,y],axis=1) # so we can later separate x and y for each cluster\n    df['knn_clf'] = knn_clf.predict(X)\n    no_cls = knn_clf.classes_\n    # get the dataframes, apply std.scaler, form train, test sets, apply models\n    for cls in knn_clf.classes_:\n        print(\"--------------The {} cluster's results-------------------\".format(cls),end=\"\\n\\n\")\n        dfs[cls] = df[df['knn_clf'] == cls].iloc[:,:-1]\n        \n        X_cls[cls] = dfs[cls].iloc[:,:-1]\n        y_cls[cls] = dfs[cls].iloc[:,-1]\n        scaler = StandardScaler()\n        X_scaled[cls] = scaler.fit_transform(X_cls[cls])\n#         X_scaled[cls] = pd.DataFrame(X_scaled[cls],columns=df.columns[:-1])\n    \n        X_train[cls],X_test[cls],y_train[cls],y_test[cls] = train_test_split(X_scaled[cls],y_cls[cls],test_size=0.3,random_state=3)\n        print(\"here\")\n        # type can be used for analyzing... eg: confusion matrix\n        for model in models:\n            model.fit(X_train[cls], y_train[cls])\n            y_pred[cls] = model.predict(X_test[cls])\n            print(model)\n            print(model.score(X_test[cls],y_test[cls]))\n            print(confusion_matrix(y_pred[cls], y_test[cls]), end=\"\\n\\n\")\n            models_out[str(model) + str(cls)] = model\n            \n    \n    return [X_train, X_test, y_train, y_test,knn_clf, models_out]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = [LogisticRegression(), RandomForestClassifier(), KNeighborsClassifier()\n          , XGBClassifier(verbosity = 0),LGBMClassifier(),SVC()] \n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_clus, X_test_clus, y_train_clus, y_test_clus,clusterer, models = clustering_approach(X,y,models)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### XGBoost performs the best individually on each cluster","metadata":{}},{"cell_type":"markdown","source":"# Thank you","metadata":{}},{"cell_type":"markdown","source":"### Don't forget to leave a like or upvote if this was worth your time","metadata":{}}]}