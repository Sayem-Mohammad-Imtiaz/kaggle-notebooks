{"cells":[{"metadata":{},"cell_type":"markdown","source":"# CNNs for Intrusion Detection"},{"metadata":{},"cell_type":"markdown","source":"###### (work in progress)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mpl_toolkits import mplot3d\nimport warnings\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Data Exploration"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/nslkdd/kdd_train.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How many classes and how many values per class:"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"print('number of classes:', df['labels'].nunique())\nprint('')\nlabel_counts = df['labels'].value_counts()\nplt.figure(figsize=(18,6));\nsns.barplot(y=label_counts.index, x=label_counts.values, color='Grey');\nplt.title('values per class');\ndisplay(label_counts)\n\n#binary traffic proportions\nbinary_class = []\nfor label in df['labels']:\n    if label !='normal':\n        binary_class.append('malicious')\n    else:\n        binary_class.append('normal')\nbinary_class = pd.Series(binary_class)\nplt.figure()\nbinary_class.value_counts().plot(kind='pie', label='traffic proportions', autopct='%.2f%%' );","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unique values and value counts of categorical variables:"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"df['protocol_type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"df['service'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#how many different categories in column 'service'\nprint('number of categories in column \\'service\\':', df['service'].nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['flag'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Numerical features--> summary statistics, distribution boxenplots (per class), feature means (per class), and correlation heatmap:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#summary statistics\ndisplay(df.iloc[:,:10].describe())\ndisplay(df.iloc[:,10:17].describe())\ndisplay(df.iloc[:,17:24].describe())\ndisplay(df.iloc[:,24:31].describe())\ndisplay(df.iloc[:,31:36].describe())\ndisplay(df.iloc[:,36:].describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_columns = []\ncategorical_columns = []\nfor column in df.columns:\n    if df[column].dtype != 'object':\n        numeric_columns.append(column)\n    else:\n        categorical_columns.append(column)\n\ncategorical_columns = categorical_columns[:-1]       \nlabels=df['labels'].unique()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#distribution boxenplots (per class)\nfor column in numeric_columns:\n    plt.figure(figsize=(18,7))\n    sns.boxenplot(x='labels', y=df[column], data=df);\n    plt.title(column);\n    #for label in labels:\n    #    plt.figure(figsize=(18,7))\n    #    sns.kdeplot(df[column][df['labels']==label]);\n    #    plt.title(label);","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#feature means (per class)\ngroup_mean = df.groupby(by='labels').mean()\nfor column in numeric_columns:\n    plt.figure(figsize=(16,5));\n    sns.barplot(y=group_mean[column].squeeze().index, x=group_mean[column].squeeze().values, \n                color='Gray');\n    plt.title(column);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#correlation heatmap\nplt.figure(figsize=(18,12));\nsns.heatmap(df.corr(), annot=True, fmt='1.1f');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#column 'num_outbound_cmds' is zero everywhere, we will delete it\ndf.drop(columns='num_outbound_cmds', inplace=True)\n\n#remove from list of numeric columns\nnumeric_columns.remove('num_outbound_cmds')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"PCA visualization:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.decomposition import PCA\n\n#standardize data\ndf_num = df[numeric_columns].copy()\nsc = StandardScaler()\ndf_num = sc.fit_transform(df_num)\n\n#ordinalize labels\nordinal = OrdinalEncoder()\nord_labels = ordinal.fit_transform(df['labels'].values[:,np.newaxis])\nord_labels = np.squeeze(ord_labels.astype(int))\n\n#PCA\npca = PCA()\ndf_pca = pca.fit_transform(df_num)\n\n#PCA 3d-scatterplot\nplt.figure(figsize=(12,6));\nax=plt.axes(projection='3d')\nax.scatter(df_pca[:,0], df_pca[:,1], df_pca[:,2], \n           c=ord_labels, cmap='winter');\nplt.title('3D PCA Visualization');\n\n#explained variance\nvar_index = np.arange(pca.explained_variance_.shape[0])+1\nplt.figure(figsize=(14,6));\nsns.barplot(x=var_index, y=pca.explained_variance_ratio_, color='gray');\nplt.title('Explained Variance Ratio');\nplt.figure(figsize=(14,6));\nsns.lineplot(x=var_index, y=pca.explained_variance_ratio_.cumsum());\nplt.title('Cumulative Explained Variance Ratio');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the explained variance graphs we see that our 3-dimensional scatterplot captured only a small portion of the information. It would take at least 10-15 Principal Components to capture a meaningfull amount of the information. We could make a few more 3d scatterplots with other Principal Components but again, as seen in the explained variance graphs, these would offer far less insight than the first three components."},{"metadata":{},"cell_type":"markdown","source":"Time series graph"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nenc = LabelEncoder()\ny = enc.fit_transform(df['labels'])\nx = np.arange(y.shape[0])\n\nplt.figure(figsize=(18,8));\nsns.lineplot(x=x[:800], y=y[:800]); #for visual clarity, only a small slice is selected\nplt.title('traffic in time')\n\nclass_labels = pd.DataFrame(data=enc.classes_,columns=['class'])\nclass_labels['label'] = np.unique(y)\ndisplay(class_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### CNN Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Conv1D, MaxPool1D, Flatten, Dense, BatchNormalization, Dropout\n#from tensorflow.keras.layers.core import Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.metrics import accuracy_score, f1_score\n#from tensorflow.keras.metrics import ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll try both multiclass and binary classification. In the case of binary, we will cluster all malicious data in a single class, using the 'binary_class' variable that we created earlier.\n\nFirst, let's prepare the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\ndummies = pd.get_dummies(df[categorical_columns])\nx = pd.concat((df[numeric_columns], dummies), axis=1).values\n\nenc_bin = LabelEncoder()\ny_bin = enc_bin.fit_transform(binary_class)\nenc_multi = LabelEncoder()\ny_multi = enc_multi.fit_transform(df['labels'].values)\n\n# for manual train_test_split, splitting indices instead of actual values\nnp.random.RandomState(seed=0)\ntrain_indexes = np.random.choice(np.arange(x.shape[0]), size=x.shape[0]*8//10, replace=False)\ntest_indexes = np.delete(np.arange(x.shape[0]), np.arange(x.shape[0])[train_indexes])\nprint('train size:', train_indexes.shape[0])\nprint('test size:  ', test_indexes.shape[0])\n\nx_tr = x[train_indexes]\nx_ts = x[test_indexes]\ny_bin_tr =y_bin[train_indexes]\ny_bin_ts =y_bin[test_indexes]\ny_multi_tr = y_multi[train_indexes]\ny_multi_ts = y_multi[test_indexes]\n\n# scale x\nsc=StandardScaler()\nx_tr = sc.fit_transform(x_tr)\nx_ts = sc.transform(x_ts)\n\n#make x 3-dimensional for the CNN to process\nx_tr = x_tr[:,:,np.newaxis]\nx_ts = x_ts[:,:,np.newaxis]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Binary Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Sequential()\nmodel.add(Conv1D(128,2, activation='relu',input_shape=x_tr[0].shape))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool1D(2))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv1D(256,2, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool1D(2))\nmodel.add(Dropout(0.4))\n\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.4))\n\nmodel.add(Dense(1,activation='sigmoid'))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stop = EarlyStopping(patience=5,verbose=1)\nmodel.fit(x_tr, y_bin_tr, epochs=50, validation_split=0.1, batch_size=16, callbacks=[early_stop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(x_ts)\npred_d = []\nfor prediction in pred:\n    if prediction <0.5:\n        pred_d.append(0)\n    else:\n        pred_d.append(1)\n        \npred = np.array(pred_d)\nprint('accuracy:', accuracy_score(y_bin_ts, pred))\nprint('f1-score:', f1_score(y_bin_ts, pred, average='macro'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Multiclass Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"model1=Sequential()\nmodel1.add(Conv1D(180,2, activation='relu',input_shape=x_tr[0].shape))\nmodel1.add(BatchNormalization())\nmodel1.add(MaxPool1D(2))\nmodel1.add(Dropout(0.2))\n\nmodel1.add(Conv1D(300,2, activation='relu'))\nmodel1.add(BatchNormalization())\nmodel1.add(MaxPool1D(2))\nmodel1.add(Dropout(0.4))\n\nmodel1.add(Flatten())\nmodel1.add(Dense(256, activation='relu'))\nmodel1.add(Dropout(0.4))\n\nmodel1.add(Dense(len(np.unique(enc.classes_)),activation='softmax'))\n\nmodel1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nmodel1.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stop = EarlyStopping(patience=4,verbose=1)\nmodel1.fit(x_tr, y_multi_tr, epochs=30, validation_split=0.1, batch_size=16, callbacks=[early_stop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model1.predict(x_ts)\npred = np.argmax(pred,axis=1)\n\nprint('accuracy:', accuracy_score(y_multi_ts, pred))\nprint('f1-score:', f1_score(y_multi_ts, pred, average='weighted'))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}