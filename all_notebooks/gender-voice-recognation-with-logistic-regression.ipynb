{"cells":[{"metadata":{"_uuid":"839a69b7651825135458c81a427fdf5d4506ff23"},"cell_type":"markdown","source":"# Introduction to Gender Voice Recognation with Logistic Regression\n\n## Index of Contents\n\n* [Read Data and Check Features](#1)\n* [Adjustment of Label values (male = 1, female = 0)](#2)\n* [Data Normalization](#3)\n* [Split Operation for Train and Test Data](#4)\n* [Matrix creation function for initial weight values](#5)\n* [Sigmoid function declaration](#6)\n* [Forward and Backward Propogation](#7)\n* [Updating Parameters](#8)\n* [Prediction with Test Data](#9)\n* [Logistic Regression Implementation](#10)\n* [Logistic Regression with sklearn](#11)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5334bc160566cd186cfdf4d65286a28d6ea9769b"},"cell_type":"markdown","source":"<a id=\"1\"></a> \n**Read Data and Check Features**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/voice.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4738ce11e1e2fd16fe59d558b632f953eb365f8e"},"cell_type":"code","source":"data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"576bc0ea789edd20f9d13aefd5bf213822944f1e"},"cell_type":"code","source":"# Get some information about our data\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed28988c374b299e96d8e27aa349acefd7629403"},"cell_type":"markdown","source":"<a id=\"2\"></a> \n***Adjustment of Label values (male = 1, female = 0***\n* After getting information about data we'll call male as 1 and female as 0***"},{"metadata":{"trusted":true,"_uuid":"a91843c4807b599bf3020fc1227695052fb971c1"},"cell_type":"code","source":"data.label = [1 if each == \"male\" else 0 for each in data.label]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c350e653e70bea0b7f33e2d76575f9d0a26e643"},"cell_type":"code","source":"data.info() # now we have label as integer","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48954755819b1f73ab7083cdda88fa607368656e"},"cell_type":"markdown","source":"<a id=\"3\"></a> \n***Data Normalization***"},{"metadata":{"trusted":true,"_uuid":"656d3c4234125ba4513f86b78aca81d8d9c4c88e"},"cell_type":"code","source":"y = data.label.values # main results male or female\nx_data = data.drop([\"label\"], axis = 1) # prediction components","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"938e6ec31c8000003f980880b08ae4938c78c6a2"},"cell_type":"code","source":"x = (x_data - np.min(x_data))/(np.max(x_data)-np.min(x_data)).values # all data evaluated from 1 to 0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70f1697f87fd892d14d380e8475b7113c2089970"},"cell_type":"markdown","source":"<a id=\"4\"></a>\n***Split Operation for Train and Test Data***\n* Data is splitted for training and testing operations. We'll have %20 of data for test and %80 of data for train after split operation."},{"metadata":{"trusted":true,"_uuid":"6e74868757632d2632b29c094b41572c706547c6"},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7f861fd38284d8c52f231a15303aaa6c13113a1"},"cell_type":"code","source":"# Data Shapes\nprint(\"x_train.shape : \", x_train.shape)\nprint(\"x_test.shape : \", x_test.shape)\nprint(\"y_train.shape : \", y_train.shape)\nprint(\"y_test.shape : \", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26575e74004e146397625112872fe57185b59973"},"cell_type":"code","source":"# Transform features to rows (Transpose)\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2909fe6188abf4c20a4b2ddfc720886f3b097495"},"cell_type":"markdown","source":"<a id=\"5\"></a> \n***Matrix creation function for initial weight values***"},{"metadata":{"trusted":true,"_uuid":"728b9750553610747f9efac3a4909391a72b628b"},"cell_type":"code","source":"def initializeWeightsAndBias(dimension): # according to our data dimension will be 20\n    w = np.full((dimension, 1), 0.01) \n    b = 0.0\n    return w,b","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fae20128d00b98847f1681db17f2ce29d1640970"},"cell_type":"markdown","source":"<a id=\"6\"></a> \n***Sigmoid function declaration***"},{"metadata":{"trusted":true,"_uuid":"10a9b62ba2da5aa381c2946941a550b6c55dfcba"},"cell_type":"code","source":"def sigmoid(z):\n    y_head = (1 / (1 + np.exp(-z)))\n    return y_head","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c281b97f2a5a5f9e6f7cd037266bd0fdcb92add"},"cell_type":"markdown","source":"<a id=\"7\"></a> \n***Forward and Backward Propogation***\n* Get z values from sigmoid function and calculate loss and cost. "},{"metadata":{"trusted":true,"_uuid":"317114b17460fc3c69754313de05f649965dfe28"},"cell_type":"code","source":"x_train.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"051a00b7a86cbfe4b854904d91938151009169e9"},"cell_type":"code","source":"def forward_backward_propogation(w, b, x_train, y_train):\n    \n    #forward propogation\n    z = np.dot(w.T, x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train * np.log(y_head) - (1 - y_train) * np.log(1 - y_head)\n    cost = (np.sum(loss)) / x_train.shape[1] # x_train.shape[1] is for scaling\n    \n    #backward propogation\n    derivative_weight = (np.dot(x_train, ((y_head - y_train).T))) / x_train.shape[1] # x_train.shape[1] is for scaling\n    derivative_bias = np.sum(y_head - y_train) / x_train.shape[1] # x_train.shape[1] is for scaling\n    gradients = {\"derivative_weight\" : derivative_weight, \"derivative_bias\" : derivative_bias}\n    \n    return cost, gradients","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"013c954ee07dd2ec4d7093e43b6871b932752cc2"},"cell_type":"markdown","source":"<a id=\"8\"></a> \n***Updating parameters***\n* Our purpose is find to optimum weight and bias values using derivative of these values."},{"metadata":{"trusted":true,"_uuid":"218bef79577ce732668bfa06e43aa32dd980dd2c"},"cell_type":"code","source":"def update(w, b, x_train, y_train, learningRate, numberOfIteration):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    # updating(learning) parameters is number_of_iteration times\n    for i in range(numberOfIteration):\n        # make forward and backward propogation and find costs and gradients\n        cost,gradients = forward_backward_propogation(w, b, x_train, y_train)\n        cost_list.append(cost)\n        #lets update\n        w = w - learningRate * gradients[\"derivative_weight\"]\n        b = b - learningRate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n            \n    # we update(learn) paramters weights and bias\n    parameters = {\"weight\" : w, \"bias\" : b}\n    plt.plot(index, cost_list2)\n    plt.xticks(index, rotation = 'vertical')\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da763eb12409f035d044adbc04149deb32bdd5f9"},"cell_type":"markdown","source":"<a id=\"9\"></a> \n***Prediction with Test Data***\n* Prediction using test data which is splitted first."},{"metadata":{"trusted":true,"_uuid":"9b8ab849b6351eb3b5572a03bbd4a5e004b37ab1"},"cell_type":"code","source":"def predict(w,b, x_test):\n    # x_test is an input for forward propogation\n    z = sigmoid(np.dot(w.T, x_test) + b)\n    Y_prediction = np.zeros((1, x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is Male (y_head = 1)\n    # if z is smaller than 0.5, our prediction is Female (y_head = 0)\n    for i in range(z.shape[1]):\n        if z[0, i] <= 0.5:\n            Y_prediction[0, i] = 0\n        else:\n            Y_prediction[0, i] = 1\n    \n    return Y_prediction","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"add8bf37deff341ff80f5c05baa0f916c742f68c"},"cell_type":"markdown","source":"<a id=\"10\"></a> \n***Logistic Regression Implementation***"},{"metadata":{"trusted":true,"_uuid":"8a8e14bd9b83b526cf2caf83e62651439d423817"},"cell_type":"code","source":"def logistic_regression(x_train, y_train, x_test, y_test, learningRate, numberOfIterations):\n    dimension = x_train.shape[0] # that is 20 (feature count of data)\n    w,b = initializeWeightsAndBias(dimension)\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learningRate, numberOfIterations)\n    y_prediction_test = predict(parameters[\"weight\"], parameters[\"bias\"], x_test)\n    print(\"test accuracy : {} %.\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"005c3d6e8b8eac8987543825398a37adead0cfca"},"cell_type":"code","source":"#Let's try our model and check costs and prediction results.\nlogistic_regression(x_train, y_train, x_test, y_test, learningRate = 1, numberOfIterations = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6ac302dbc64cc2f0e1f90524b66e46122888110"},"cell_type":"code","source":"logistic_regression(x_train, y_train, x_test, y_test, learningRate = 1, numberOfIterations = 1000)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d30c993263ff1fd042d5b4bd6206bca012aecd12"},"cell_type":"markdown","source":"As you see above, when the iteration is increased, accuracy increasing too. "},{"metadata":{"_uuid":"9ad6a62681d066b148f236baf8ca0e65d502a3c9"},"cell_type":"markdown","source":"<a id=\"11\"></a> \n***Logistic Regression with sklearn***\n* Logistic Regression Classification can be done with sklearn library. All codes which are written above correspond to the codes below.\n"},{"metadata":{"trusted":true,"_uuid":"caeaa3f11a99514d49079c945b2c67b002a350b6"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train.T, y_train.T)\nprint(\"test accuracy {}\".format(lr.score(x_test.T, y_test.T)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}