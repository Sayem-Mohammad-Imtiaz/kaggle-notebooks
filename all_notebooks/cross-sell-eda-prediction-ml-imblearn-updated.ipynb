{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Importing the libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder, LabelEncoder, OneHotEncoder\nfrom sklearn.preprocessing import PolynomialFeatures, PowerTransformer, FunctionTransformer\nfrom sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold, RandomizedSearchCV, StratifiedShuffleSplit\nfrom sklearn.feature_selection import SelectFromModel, SelectKBest, VarianceThreshold\nfrom sklearn.metrics import roc_auc_score, roc_curve, f1_score, accuracy_score, classification_report\nfrom sklearn.decomposition import PCA, FactorAnalysis, TruncatedSVD\nfrom sklearn.manifold import TSNE\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier, VotingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\npd.set_option('display.max_columns', None)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/av-janatahack-crosssell-prediction/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/av-janatahack-crosssell-prediction/test.csv\")\ntrain.drop('id', axis=1, inplace=True)\ntest.drop('id', axis=1, inplace=True)\nsample = pd.read_csv(\"/kaggle/input/av-janatahack-crosssell-prediction/sample.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train data head"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing values check"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- No missing values in the data. Will confirm the same using unique values"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in train.columns:\n    print(f\"{col} : {train[col].nunique()}\")\n    print(train[col].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#separating continuous and categorical variables\ncat_var = [\"Gender\",\"Driving_License\",\"Previously_Insured\",\"Vehicle_Age\",\"Vehicle_Damage\"]\ncon_var = list(set(train.columns).difference(cat_var+[\"Response\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Response.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Around 12.26 % of customer have given a positive response"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train.Response)\nplt.title(\"Class count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### we can see that the data is imbalanced classification, hence we will not use accuracy as scoring metric. Instead we will use f1-score or more preferrably roc-auc"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(train, hue='Response', diag_kind='hist')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def map_val(data):\n    data[\"Gender\"] = data[\"Gender\"].replace({\"Male\":1, \"Female\":0})\n    data[\"Vehicle_Age\"] = data[\"Vehicle_Age\"].replace({'> 2 Years':2, '1-2 Year':1, '< 1 Year':0 })\n    data[\"Vehicle_Damage\"] = data[\"Vehicle_Damage\"].replace({\"Yes\":1, \"No\":0})\n    return data\n\ntrain = map_val(train)\ntest = map_val(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2,3 , figsize=(16,6))\nax = ax.flatten()\ni = 0\nfor col in cat_var:\n    sns.pointplot(col, 'Response', data=train, ax = ax[i])\n    i+=1\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Males or customers with license or not previously insured or the vehicle was previously damaged have better response rate\n- With increasing vehicle age response improved"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot('Gender', 'Response',hue='Vehicle_Age', row = 'Previously_Insured',col='Vehicle_Damage',data=train, kind='point', height=3, aspect=2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Customers who were not previously insured and their vehicle has been damaged have shown much better response, as expected"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2,3 , figsize=(16,6))\nax = ax.flatten()\ni = 0\nfor col in con_var:\n    sns.boxplot( 'Response', col, data=train, ax = ax[i])\n    i+=1\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot('Gender', 'Vintage',hue='Response', row = 'Previously_Insured',col='Vehicle_Damage',data=train, kind='box', height=3, aspect=2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot('Gender', 'Age',hue='Response', row = 'Previously_Insured',col='Vehicle_Damage',data=train, kind='box', height=3, aspect=2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot('Gender', 'Annual_Premium',hue='Response', row = 'Previously_Insured',col='Vehicle_Damage',data=train, kind='box', height=3, aspect=2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,5))\nsns.heatmap(pd.crosstab([train['Previously_Insured'], train['Vehicle_Damage']], train['Region_Code'],\n                        values=train['Response'], aggfunc='mean', normalize='columns'), annot=True, cmap='inferno')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can easily identify the regions where the response rate is high compared to others"},{"metadata":{},"cell_type":"markdown","source":"## Correlation Heatmap"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = train.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)]=True\nplt.figure(figsize=(10,6))\nsns.heatmap(corr, annot=True, fmt='.2f', cmap='YlGnBu', mask=mask)\nplt.title(\"Correlation Heatmap\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Current Age/ Vintage/ Annual Premium distributions are not helping very much so we will try mean transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.skew()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['log_premium'] = np.log(train.Annual_Premium)\ntrain['log_age'] = np.log(train.Age)\ntest['log_premium'] = np.log(test.Annual_Premium)\ntest['log_age'] = np.log(test.Age)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby(['Previously_Insured','Gender'])['log_premium'].plot(kind='kde')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby(['Previously_Insured','Gender'])['log_age'].plot(kind='kde')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_engineering(data, col):\n    mean_age_insured = data.groupby(['Previously_Insured','Vehicle_Damage'])[col].mean().reset_index()\n    mean_age_insured.columns = ['Previously_Insured','Vehicle_Damage','mean_'+col+'_insured']\n    mean_age_gender = data.groupby(['Previously_Insured','Gender'])[col].mean().reset_index()\n    mean_age_gender.columns = ['Previously_Insured','Gender','mean_'+col+'_gender']\n    mean_age_vehicle = data.groupby(['Previously_Insured','Vehicle_Age'])[col].mean().reset_index()\n    mean_age_vehicle.columns = ['Previously_Insured','Vehicle_Age','mean_'+col+'_vehicle']\n    data = data.merge(mean_age_insured, on=['Previously_Insured','Vehicle_Damage'], how='left')\n    data = data.merge(mean_age_gender, on=['Previously_Insured','Gender'], how='left')\n    data = data.merge(mean_age_vehicle, on=['Previously_Insured','Vehicle_Age'], how='left')\n    data[col+'_mean_insured'] = data['log_age']/data['mean_'+col+'_insured']\n    data[col+'_mean_gender'] = data['log_age']/data['mean_'+col+'_gender']\n    data[col+'_mean_vehicle'] = data['log_age']/data['mean_'+col+'_vehicle']\n    data.drop(['mean_'+col+'_insured','mean_'+col+'_gender','mean_'+col+'_vehicle'], axis=1, inplace=True)\n    return data\n\ntrain = feature_engineering(train, 'log_age')\ntest = feature_engineering(test, 'log_age')\n\ntrain = feature_engineering(train, 'log_premium')\ntest = feature_engineering(test, 'log_premium')\n\ntrain = feature_engineering(train, 'Vintage')\ntest = feature_engineering(test, 'Vintage')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## If you find my Kernel useful, please do upvote. Any suggestions are welcome."},{"metadata":{},"cell_type":"markdown","source":"## Preparing the data for training"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop([\"Response\"], axis=1)\nY = train[\"Response\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy = [\"Vehicle_Age\"]\npassthru = con_var = list(set(X.columns).difference(dummy))\n\nonehot = OneHotEncoder(handle_unknown='ignore')\nlabel = OrdinalEncoder()\nscaler = StandardScaler()\n\nfeat_rf = RandomForestClassifier(n_jobs=4, random_state=1, class_weight='balanced_subsample')\nfeat_xgb = XGBClassifier(n_jobs=4, random_state=1, objective='binary:logistic')\nselector_rf = SelectFromModel(feat_xgb, threshold=0.001)\n\ntransformers_onehot = [('pass','passthrough',passthru),\n                       ('onehot', onehot, dummy) ]\nct_onehot = ColumnTransformer( transformers=transformers_onehot )\n\ntransformers_label = [('pass','passthrough',passthru),\n                      ('onehot', label, dummy) ]\nct_label = ColumnTransformer( transformers=transformers_label )\n\npipe = Pipeline([('ct', ct_onehot),\n                 ('scaler', scaler)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"poly = PolynomialFeatures(degree= 2, interaction_only=True)\npca = PCA(n_components=0.99)\nkbest = SelectKBest(k=6)\n\npipe_pca = Pipeline([('ct', ct_onehot),\n                      ('poly', poly),\n                      ('scaler', scaler),\n                      ('pca',pca)])\n\npipe_kbest = Pipeline([('ct', ct_onehot),\n                       ('poly', poly),\n                       ('scaler', scaler),\n                       ('kbest',kbest)])\n\npipe_union = FeatureUnion([('pca',pipe_pca),\n                           ('kbest',pipe_kbest)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### PCA + KBest pipeline output"},{"metadata":{"trusted":true},"cell_type":"code","source":"# merging the PCA components and KBest features from the data\npipe_union.fit(X, Y)\nX_union = pipe_union.transform(X)\ntest_union = pipe_union.transform(test)\n#np.cumsum(pipe_union.transformer_list[0][1].named_steps['pca'].explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Onehot and Feature selection Pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"ct_onehot.fit(X)\ncategories = ct_onehot.named_transformers_['onehot'].categories_\nonehot_cols = [col+\"_\"+str(cat) for col,cats in zip(dummy, categories) for cat in cats]\nall_columns = passthru + onehot_cols\n\nX_transform = pd.DataFrame(pipe.fit_transform(X), columns = all_columns)\ntest_transform = pd.DataFrame(pipe.transform(test), columns = all_columns)\n\nselector_rf.fit(X_transform, Y)\nrf_cols = [col for col, flag in zip(X_transform.columns, selector_rf.get_support()) if flag]\nprint(rf_cols)\nX_select = pd.DataFrame(selector_rf.transform(X_transform), columns = rf_cols)\ntest_select = pd.DataFrame(selector_rf.transform(test_transform), columns = rf_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Establishing Baseline accuracy for different models\nIt's good to establish baseline as we can check if model has improved with feature engineering or not"},{"metadata":{"trusted":true},"cell_type":"code","source":"def submission(preds, model):\n    sample[\"Response\"] = preds\n    sample.to_csv(\"model_\"+model+\".csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_lr = LogisticRegression(n_jobs=4, random_state=1, class_weight='balanced')\nmodel_rfc = RandomForestClassifier(n_jobs=4, random_state=1, class_weight='balanced_subsample')\n# scale pos weight for class imbalance\nmodel_xgb = XGBClassifier(n_jobs=4, random_state=1, scale_pos_weight=7, objective='binary:logistic')\nmodel_lgbm = LGBMClassifier(n_jobs=4, random_state=1, is_unbalance=True, objective='binary')\nmodel_cat = CatBoostClassifier(random_state=1, verbose=0, scale_pos_weight=7, custom_metric=['AUC'])\n\nmodels = []\nmodels.append((\"LR\",model_lr))\nmodels.append((\"RF\",model_rfc))\nmodels.append((\"XGB\",model_xgb))\nmodels.append((\"LGBM\",model_lgbm))\nmodels.append((\"CAT\",model_cat))\n\ncv = StratifiedShuffleSplit(n_splits=5, random_state=1, train_size=0.8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = []\nnames = []\nfor name, model in models:\n    print(\"Training...\"+name)\n    scores = cross_val_score(model, X_select, Y, scoring='roc_auc', n_jobs=-1, cv = cv, verbose=0)\n    results.append(scores)\n    names.append(name)\n    print(\"Model %s mean score : %.4f variance error: %.4f\"%(name, np.mean(scores), np.std(scores)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.boxplot(results)\nplt.xticks(np.arange(1,len(names)+1), names)\nplt.title(\"Model comparison\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing models with PCA + KBest features"},{"metadata":{"trusted":true},"cell_type":"code","source":"results_union = []\nnames = []\nfor name, model in models:\n    print(\"Training...\"+name)\n    scores = cross_val_score(model, X_union, Y, scoring='roc_auc', n_jobs=-1, cv = cv, verbose=0)\n    results_union.append(scores)\n    names.append(name)\n    print(\"Model %s mean score : %.4f variance error: %.4f\"%(name, np.mean(scores), np.std(scores)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.boxplot(results_union)\nplt.xticks(np.arange(1,len(names)+1), names)\nplt.title(\"Model comparison\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training models for submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_model(model, x, Y):\n    model.fit(x, Y)\n\n    trainpred  = model.predict(x)\n    proba = model.predict_proba(x)[:,1]\n\n    print(\"Accuracy score : %.4f\"%accuracy_score(Y, trainpred))\n    print(\"ROC AUC score : %.4f\"%roc_auc_score(Y, proba))\n    print(\"Classification report\")\n    print(classification_report(Y, trainpred))\n    \ndef metrics_score(model, X, Y):\n    pred = model.predict_proba(X)[:,1]\n    print(\"ROC AUC score : %.4f\"%roc_auc_score(Y, pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_xgb = XGBClassifier(n_jobs=4, random_state=1, scale_pos_weight=7, objective='binary:logistic')\nmodel_lgbm = LGBMClassifier(n_jobs=4, random_state=1, is_unbalance=True, objective='binary')\nmodel_cat = CatBoostClassifier(random_state=1, verbose=0, scale_pos_weight=7, custom_metric=['AUC'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_xgb.fit(X_select, Y)\nmodel_lgbm.fit(X_select, Y)\nmodel_cat.fit(X_select, Y)\n\npred_xgb = model_xgb.predict_proba(test_select)[:,1]\npred_lgbm = model_lgbm.predict_proba(test_select)[:,1]\npred_cat = model_cat.predict_proba(test_select)[:,1]\n\nsubmission(pred_xgb, 'xgb')\nsubmission(pred_lgbm, 'lgbm')\nsubmission(pred_cat, 'cat')\n\nprediction = np.mean((pred_xgb, pred_lgbm, pred_cat), axis=0)\nsubmission(prediction, 'all')\n\nmetrics_score(model_xgb, X_select, Y)\nmetrics_score(model_lgbm, X_select, Y)\nmetrics_score(model_cat, X_select, Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = StratifiedShuffleSplit(n_splits=10, random_state=1, train_size=0.7)\npredictions_lgbm = []\n\nfor train_index, test_index in cv.split(X_select, Y):\n    xtrain, xtest = X_select.iloc[train_index], X_select.iloc[test_index]\n    ytrain, ytest = Y[train_index], Y[test_index]\n    \n    model_lgbm.fit(xtrain, ytrain)\n    trainpred = model_lgbm.predict_proba(xtrain)[:,1]\n    testpred = model_lgbm.predict_proba(xtest)[:,1]\n    print(\"Train ROC AUC : %.4f Test ROC AUC : %.4f\"%(roc_auc_score(ytrain, trainpred),roc_auc_score(ytest, testpred)))\n    prediction = model_lgbm.predict_proba(test_select)[:,1]\n    predictions_lgbm.append(prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission(np.mean(predictions_lgbm, axis=0), 'lgbm_stack')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run this again\n#cv = StratifiedShuffleSplit(n_splits=5, random_state=1, train_size=0.9)\ncv = StratifiedKFold(n_splits=5, random_state=1, shuffle=True)\npredictions_xgb = []\n\nfor train_index, test_index in cv.split(X_select, Y):\n    xtrain, xtest = X_select.iloc[train_index], X_select.iloc[test_index]\n    ytrain, ytest = Y[train_index], Y[test_index]\n    \n    model_xgb.fit(xtrain, ytrain)\n    trainpred = model_xgb.predict_proba(xtrain)[:,1]\n    testpred = model_xgb.predict_proba(xtest)[:,1]\n    print(\"Train ROC AUC : %.4f Test ROC AUC : %.4f\"%(roc_auc_score(ytrain, trainpred),roc_auc_score(ytest, testpred)))\n    prediction = model_xgb.predict_proba(test_select)[:,1]\n    predictions_xgb.append(prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission(np.mean(predictions_xgb, axis=0), 'xgb_stack')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = StratifiedShuffleSplit(n_splits=10, random_state=1, train_size=0.7)\npredictions_cat = []\n\nfor train_index, test_index in cv.split(X_select, Y):\n    xtrain, xtest = X_select.iloc[train_index], X_select.iloc[test_index]\n    ytrain, ytest = Y[train_index], Y[test_index]\n    \n    model_cat.fit(xtrain, ytrain)\n    trainpred = model_cat.predict_proba(xtrain)[:,1]\n    testpred = model_cat.predict_proba(xtest)[:,1]\n    print(\"Train ROC AUC : %.4f Test ROC AUC : %.4f\"%(roc_auc_score(ytrain, trainpred),roc_auc_score(ytest, testpred)))\n    prediction = model_cat.predict_proba(test_select)[:,1]\n    predictions_cat.append(prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission(np.mean(predictions_cat, axis=0), 'cat_stack')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using ANN"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_nn(history, metric):\n    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(16,5))\n    ax1.plot(history.history['loss'], color='r', label='Train loss')\n    ax1.plot(history.history['val_loss'], color='g', label='Validation loss')\n    ax1.legend()\n\n    ax2.plot(history.history[metric], color='r', label='Train '+metric)\n    ax2.plot(history.history['val_'+metric], color='g', label='Validation '+metric)\n    ax2.legend()\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import Adam, SGD\nfrom keras.callbacks import EarlyStopping\nfrom keras.wrappers.scikit_learn import KerasClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = X_select.shape[1]\n\nes = EarlyStopping(monitor='val_loss', min_delta=0.01, patience = 50, mode='auto', baseline=0.85, restore_best_weights=True)\n\noptimizer = Adam(learning_rate=0.01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add( Dense( 64, input_dim = inputs, activation='relu', kernel_initializer='random_normal'))\nmodel.add( Dense( 128, input_dim = inputs, activation='relu', kernel_initializer='random_normal'))\nmodel.add( Dense( 256, input_dim = inputs, activation='relu', kernel_initializer='random_normal'))\nmodel.add( Dropout(0.01))\nmodel.add( Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer = optimizer, loss='binary_crossentropy', metrics = ['AUC'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_select, Y, batch_size=128, epochs = 20, validation_split=0.3, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_nn(history, 'auc')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_nn = model.predict_proba(test_select)\nsubmission(pred_nn, 'nn')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_stack = np.mean((pred_xgb, pred_lgbm, pred_cat, pred_nn[:,0]), axis=0)\nsubmission(pred_stack, 'stack')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using Imblearn"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.ensemble import BalancedBaggingClassifier, BalancedRandomForestClassifier,EasyEnsembleClassifier\nfrom imblearn.metrics import classification_report_imbalanced","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_bbag = BalancedBaggingClassifier(n_jobs=4, random_state=1, base_estimator=model_xgb)\nmodel_brf = BalancedRandomForestClassifier(n_jobs=4, random_state=1, class_weight='balanced')\nmodel_easy = EasyEnsembleClassifier(n_jobs=4, random_state=1, base_estimator=model_xgb)\n\nimb_models = []\nimb_models.append(('Bag', model_bbag))\nimb_models.append(('BagRF', model_brf))\nimb_models.append(('Easy', model_easy))\n\ncv = StratifiedShuffleSplit(n_splits=5, random_state=1, train_size=0.8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_imb = []\nnames_imb = []\nfor name, model in imb_models:\n    print(\"Training...\"+name)\n    scores = cross_val_score(model, X_select, Y, scoring='roc_auc', n_jobs=-1, cv = cv, verbose=0)\n    results_imb.append(scores)\n    names_imb.append(name)\n    print(\"Model %s mean score : %.4f variance error: %.4f\"%(name, np.mean(scores), np.std(scores)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.boxplot(results_imb)\nplt.xticks(np.arange(1,len(names_imb)+1), names_imb)\nplt.title(\"Model comparison Imblearn\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_bbag.fit(X_select, Y)\nmodel_brf.fit(X_select, Y)\nmodel_easy.fit(X_select, Y)\n\nmetrics_score(model_bbag, X_select, Y)\nmetrics_score(model_brf, X_select, Y)\nmetrics_score(model_easy, X_select, Y)\n\npred_bbag = model_bbag.predict_proba(test_select)[:,1]\npred_brf = model_brf.predict_proba(test_select)[:,1]\npred_easy = model_easy.predict_proba(test_select)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission(pred_bbag, 'imb_bbag')\nsubmission(pred_brf, 'imb_brf')\nsubmission(pred_easy, 'imb_easy')\n\npred_imb_stack = np.mean((pred_bbag, pred_brf, pred_easy), axis=0)\nsubmission(pred_imb_stack, 'imb_stack')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}