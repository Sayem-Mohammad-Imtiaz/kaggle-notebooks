{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Import the data and library","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/bigmart-sales-data/Train.csv')\ntest_data = pd.read_csv('/kaggle/input/bigmart-sales-data/Test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\nShape of training data :',train_data.shape)\nprint('\\nShape of testing data :',test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Create column & Combining the data set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[\"source\"]=\"train\"\ntest_data[\"source\"]=\"test\"\ndata = pd.concat([train_data,test_data], sort= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (train_data.shape, test_data.shape, data.shape , sep = \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Data Exploration","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_col = data.select_dtypes(include=\"object\")\n\nfor c in cat_col:\n    if c not in( 'Item_Identifier','Outlet_Identifier','source'):\n        print( \"\\n Feature:\",c)\n        print(data[c].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"   - All the categorical columns needs to be converted. \n   - In \"Item_Fat_Content\" column, ( Low Fat    : LF : low fat  ) , (Regular:reg) needs to be changed\n   - We can combine some cat. in \"Item_Type\" columns\n   - In \"Outlet_Type\" column, Supermarket Type2 and Type3 can be combined. But we should check if thatâ€™s a good idea before doing it.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 4 Data Visualization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.1 Univariant Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### a)Target Column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train_data['Item_Outlet_Sales'])\nplt.show()\n\nprint('Skewness: %f' % train_data['Item_Outlet_Sales'].skew())\nprint('Kurtsis: %f' %train_data['Item_Outlet_Sales'].kurt())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observetions:\n- Deviation from the normal distribution.\n- There is  positive skewness.\n- Shows peakedness.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### b) Numeric variables","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"train_data['Item_Weight'].hist(bins = 100);\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['Item_Visibility'].hist(bins = 100);\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['Item_MRP'].hist(bins = 100);\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Observations:\n    Item_Weight does not have any clear pattern. \n    Item_Visibility is right-skewed and should be transformed.\n    There are  4 different distributions for Item_MRP","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### c) Categorical variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x= \"Item_Type\", data= train_data, kind = \"count\", aspect=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x= \"Outlet_Size\", data= train_data, kind = \"count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x= \"Item_Fat_Content\", data= train_data, kind = \"count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x= \"Outlet_Location_Type\", data= train_data, kind = \"count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x= \"Outlet_Type\", data= train_data, kind = \"count\",aspect=2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"Outlet_Establishment_Year\"]=data[\"Outlet_Establishment_Year\"].astype(\"category\")\nsns.catplot(x= \"Outlet_Establishment_Year\", data= train_data, kind = \"count\", aspect=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observations:\n    In \"Item_Fat_Content\" column Low Fat, LF , low fat  means the same. Regular, reg means the same\n    In \"Outlet_Type\" Supermarket_Typee is the most popular Type.\n    \"Outlet_Establishment_Year\" - 1998 has less data\n    ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 4.2 Bivariant Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### a) Target Variable vs Numerical Variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x = \"Item_Weight\" , y =\"Item_Outlet_Sales\" , data = train_data, alpha = 0.3, color = \"r\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x = \"Item_Visibility\" , y =\"Item_Outlet_Sales\" , data = train_data, alpha = 0.3, color = \"y\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x = \"Item_MRP\" , y =\"Item_Outlet_Sales\" , data = train_data, alpha = 0.3, color = \"g\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Observations:\n    Item_Outlet_Sales is spread well across the entire range of the Item_Weight. \n    In Item_Visibility vs Item_Outlet_Sales, there is a string of points at Item_Visibility = 0.0.But Item visibility cannot be completely zero.\n    In Item_MRP vs Item_Outlet_Sales, we can  see 4 segments of prices. We can use this to create a new variable.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### b) Target Variable vs Categorical Variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x= \"Outlet_Size\", y = \"Item_Outlet_Sales\" ,data= train_data, kind = \"box\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x= \"Outlet_Establishment_Year\", y = \"Item_Outlet_Sales\" ,data= train_data, kind = \"box\", aspect=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x= \"Outlet_Type\", y = \"Item_Outlet_Sales\" ,data= train_data, kind = \"box\", aspect=3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x= \"Item_Fat_Content\", y = \"Item_Outlet_Sales\" ,data= train_data, kind = \"box\")\nplt.show","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.catplot(x= \"Item_Type\", y = \"Item_Outlet_Sales\" ,data= train_data, kind = \"violin\", aspect=3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### c ) Categorical vs Categorical","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x= \"Outlet_Type\", data= train_data, kind = \"count\", aspect = 2, hue=\"Outlet_Size\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x= \"Outlet_Type\", data= train_data, kind = \"count\", aspect = 2, hue=\"Outlet_Location_Type\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Treating Missing Values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### a) Item_Weight\n        Lets replace them with the mean weight of that particular item","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"item_mean_weight = data.pivot_table( index = \"Item_Identifier\" , values = \"Item_Weight\",aggfunc='mean')\n\nprint(\"Missing Item_Weight : \" , data['Item_Weight'].isnull().sum()  )\ndata.loc[data['Item_Weight'].isnull(), \"Item_Weight\"] =   data.loc[data['Item_Weight'].isnull(), \"Item_Identifier\"]. apply ( lambda x : item_mean_weight.loc[x])\nprint(\"Missing Item_Weight : \" , data['Item_Weight'].isnull().sum()  )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### b) Outlet_Size                  \n        Lets's replace them with the most frequent outlet size of that particular Outlet Type","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import mode\n\noutlet_mode= data.pivot_table( index = \"Outlet_Type\" , values = \"Outlet_Size\",aggfunc= lambda x : mode(x).mode[0]  )\n\nprint(\"Missing Outlet_Size : \" , data['Outlet_Size'].isnull().sum()  )\ndata.loc[data['Outlet_Size'].isnull(), \"Outlet_Size\"] =   data.loc[data['Outlet_Size'].isnull(), \"Outlet_Type\"]. apply ( lambda x : outlet_mode.loc[x])\nprint(\"Missing Outlet_Size : \" , data['Outlet_Size'].isnull().sum()  )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Feature Engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### a )Item_Visibility\n    There are some records with \"0\" Visibility. Item_visibilty contributes Sales\n    So we can consider them as missing value and treat them\n    Lets replace them with the mean visibility of that particular item","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"item_mean_visibility = data.pivot_table( index = \"Item_Identifier\" , values = \"Item_Visibility\",aggfunc='mean')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Rows with '''0''' visbility \" , (data['Item_Visibility'] == 0).sum()  )\ndata.loc[data['Item_Visibility'] == 0, \"Item_Visibility\"] =   data.loc[data['Item_Visibility'] == 0, \"Item_Identifier\"]. apply ( lambda x : item_mean_visibility.loc[x])\nprint(\"Rows with '''0''' visbility \" , (data['Item_Visibility'] == 0).sum()  )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### b ) Item_Identifier\n\n    Here all the values start with \"FD\", \"DR\",\"NC\" followed by alphanumeric\n        - FD : Food\n        - DR : Drink\n        - NC : Non- Edible\n    We can create a new column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"Item_combined\"] = data [\"Item_Identifier\"].apply( lambda  x : x[0:2])\ndata[\"Item_combined\"]  = data[\"Item_combined\"]. map ( { \"FD\" : \"Food\", \"DR\" : \"Drink\",  \"NC\" : \"Non_Edible\"} )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### c) Item_Fat_Content\n    In \"Item_Fat_Content\" column, ( Low Fat    : LF : low fat  ) , (Regular:reg) needs to be changed\n    But we have foods that are not edible (\"Item_combined\"). So we need to take that into consideration and change that value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data[\"Item_Fat_Content\"].unique())\ndata[\"Item_Fat_Content\"].replace ( {\"low fat\": \"Low Fat\" , \"LF\": \"Low Fat\", \"reg\": \"Regular\"} , inplace = True)\nprint(data[\"Item_Fat_Content\"].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[data[\"Item_combined\"] == \"Non_Edible\" , \"Item_Fat_Content\" ] = \"Non_Edible\"\nprint(data[\"Item_Fat_Content\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### d) Outlet_Type\n    Supermarket Type2 and Type3 can be combined. But we should check if thatâ€™s a good idea before doing it.\n    One way of checking is , by using \"mean\"(Sales.\n    If the Mean Sale of both are same, then we can combine them. But if they are different we cannot do it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.pivot_table(values= \"Item_Outlet_Sales\", index = \"Outlet_Type\",aggfunc='mean' )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"     Here there is a drastic difference , so we should  not combine them","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### e) Outlet_Establishment_Year\n    Given data is for 2013, so we can calculate the no of years","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"year\"] = data[\"Outlet_Establishment_Year\"].apply( lambda x : 2013-x )\ndata[\"year\"]=data[\"year\"].astype(\"int8\")\ndata[\"year\"].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### f) Outlet_Identifier\n\n    Identifier columns cannot be deleted since , we need them for submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"Outlet_Identifier\"].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder().fit(data[\"Outlet_Identifier\"])\ndata[\"Outlet\"]= le.transform(data[\"Outlet_Identifier\"])\ndata[\"Outlet\"] = data[\"Outlet\"].astype(\"category\")\ndata[\"Outlet\"].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cor = data.corr()\nsns.heatmap(cor,cmap=\"bone\" )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data[['Item_Weight', 'Item_Visibility', 'Item_MRP', 'year', 'Item_Outlet_Sales']] )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7. One Hot Coding:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data= pd.get_dummies (data = data , columns = ['Item_Fat_Content','Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type', 'Item_combined',  'Outlet'] , drop_first = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8. Droping Columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop ( [\"Item_Type\",\"Outlet_Establishment_Year\"] , inplace = True , axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 9. Splitting the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = data.loc [data[\"source\"] == \"train\"]\ntest = data.loc [data[\"source\"] == \"test\"]\n\n#Drop unnecessary columns:\ntest.drop(['Item_Outlet_Sales','source'],axis=1,inplace=True)\ntrain.drop(['source'],axis=1,inplace=True)\n\n#Export files as modified versions:\ntrain.to_csv(\"train_modified.csv\",index=False)\ntest.to_csv(\"test_modified.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 10 . Model Building\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport warnings \nwarnings.filterwarnings('ignore')\n\ntrain = pd.read_csv('train_modified.csv')\ntest= pd.read_csv('test_modified.csv')\n\nprint('\\nShape of training data :',train.shape)\nprint('\\nShape of testing data :',test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 10.1 Base Model\nSince we are working on a regression problem, we can use a central tendency measure as the result for all predictions, such as the mean or the median.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model = test[[\"Item_Identifier\", \"Outlet_Identifier\"]]\nbase_model[\"Item_Outlet_Sales\"] =  train[\"Item_Outlet_Sales\"].median()\nbase_model.to_csv(\"base_model.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 10.2 Models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.linear_model import LinearRegression , Lasso ,Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_validate,cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function \ndef models(algorithm,X_val,y_val,X_train,y_train ,file_name,X_test):\n    model = algorithm\n    model.fit(X_train,y_train)\n    \n    ytrain_pred = model.predict(X_train)\n    yval_pred = model.predict(X_val)\n    rmse_train = np.sqrt(mean_squared_error(y_train, ytrain_pred) )\n    rmse_val = np.sqrt(mean_squared_error(y_val, yval_pred) )\n    \n    scores_train = model.score(X_train,y_train)\n    scores_val = model.score(X_val,y_val)\n    \n    accuracy = cross_val_score(estimator=model, X=X_train, y=y_train,cv=10)\n#     print(f\"The accuracy of the Polynomial Regression Model is \\t {accuracy.mean()}\")\n#     print(f\"The deviation in the accuracy is \\t {accuracy.std()}\")\n\n\n    score.loc[file_name] = [ scores_train ,  scores_val,rmse_train, rmse_val,accuracy.mean(),accuracy.std()  ]\n    \n    #submission\n    submission = test[[\"Item_Identifier\", \"Outlet_Identifier\"]]\n    submission[\"Item_Outlet_Sales\"] =  model.predict(X_test)\n    file_name = file_name + \".csv\"\n    submission.to_csv(file_name,index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nscore = pd.DataFrame ( columns = [\"Train_Score\", \"Validate_Score\", \"Train_RMSE\",\"Validate_RMSE\", \"Accuuracy_Mean\", \"Accuracy_Std\"])\ny_train = train [\"Item_Outlet_Sales\"]\n\nX= train.drop([\"Item_Identifier\",\"Outlet_Identifier\",\"Item_Outlet_Sales\"] , axis = 1)\ny= train[\"Item_Outlet_Sales\"]\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2,random_state = 0)\n\nX_test= test.drop([\"Item_Identifier\",\"Outlet_Identifier\"] , axis = 1)\n\nmodels (LinearRegression() , X_val,y_val, X_train,y_train ,\"LinearRegression\",X_test)\nmodels (Lasso() , X_val,y_val, X_train,y_train ,\"Lasso\",X_test)\nmodels (Ridge() , X_val,y_val, X_train,y_train ,\"Ridge\",X_test)\nmodels(DecisionTreeRegressor(max_depth=15, min_samples_leaf=100) ,  X_val,y_val, X_train,y_train ,\"DecisionTreeRegressor\",X_test)\nmodels(DecisionTreeRegressor(max_depth=8, min_samples_leaf=150) ,  X_val,y_val, X_train,y_train ,\"DecisionTreeRegressor2\",X_test)\nalg_RFR = RandomForestRegressor(n_estimators=200,max_depth=5, min_samples_leaf=100,n_jobs=4) \nmodels (alg_RFR , X_val,y_val, X_train,y_train ,\"RandomForestRegressor\",X_test)\nalg_RFR2 = RandomForestRegressor(n_estimators=400,max_depth=6, min_samples_leaf=100,n_jobs=4)\nmodels (alg_RFR2 , X_val,y_val, X_train,y_train ,\"RandomForestRegressor2\",X_test)\n\nscore","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import ensemble\n\nparams = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2, 'learning_rate': 0.01, 'loss': 'ls'}\nclf = ensemble.GradientBoostingRegressor(**params)\nmodels (clf , X_val,y_val, X_train,y_train ,\"GradientBoostingRegressor\",X_test)\n\n\nparams = {'n_estimators': 750, 'max_depth': 4, 'min_samples_split': 2, 'learning_rate': 0.00999, 'loss': 'ls', 'criterion':'mse', 'random_state' : 1}\nclf = ensemble.GradientBoostingRegressor(**params )\nmodels (clf , X_val,y_val, X_train,y_train ,\"GradientBoostingRegressor2\",X_test)\nscore\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import ElasticNetCV, ElasticNet\n\ncv_model = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, .995, 1], eps=0.001, n_alphas=100, fit_intercept=True, \n                        normalize=True, precompute='auto', max_iter=2000, tol=0.0001, cv=5, \n                        copy_X=True, verbose=0, n_jobs=-1, positive=False, random_state=None, selection='cyclic')\ncv_model.fit(X_train, y_train)\n\ne_net = ElasticNet(l1_ratio=cv_model.l1_ratio_, alpha = cv_model.alpha_, max_iter=cv_model.n_iter_, fit_intercept=True, normalize = True)\n\nmodels (e_net , X_val,y_val, X_train,y_train ,\"ElasticNet\",X_test)\nscore","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}