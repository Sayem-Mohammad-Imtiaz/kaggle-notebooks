{"cells":[{"metadata":{},"cell_type":"markdown","source":"<pre>\n           _.-------._\n        _-'_.------._ `-_\n      _- _-          `-_/\n     -  -\n ___/  /______________\n/___  .______________/\n ___| |_____________\n/___  .____________/\n    \\  \\\n     -_ -_             /|\n       -_ -._        _- |\n         -._ `------'_./\n            `-------'\n            \n  <b>Bank Loan Approval</b>\n  Ensemble methods\n</pre>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nfrom numpy import mean, std, hstack\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_theme(style=\"white\")\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import VotingClassifier\n\nfrom warnings import simplefilter\nfrom sklearn.exceptions import ConvergenceWarning\nsimplefilter(\"ignore\", category=ConvergenceWarning)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:black; background:white; border:0.2px dotted;\" role=\"tab\" aria-controls=\"home\"><center>Prepare Dataset</center></h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/credit-risk-analysis-for-extending-bank-loans/bankloans.csv'\n\ndf = pd.read_csv(path)\n\ndf = df[df.default.notnull()]\ndf = shuffle(df)\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def z_score(df):\n    df_std = df.copy()\n    # apply the z-score method\n    for column in df_std.columns:\n        df_std[column] = (df_std[column] - df_std[column].mean()) / df_std[column].std()\n    return df_std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = df[list(df.columns)[:-1]]\nfeatures = z_score(features).values\nlabels = df['default'].values\n\nX_train, X_test, y_train, y_test = train_test_split(\n    features, labels, test_size=0.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Compute the correlation matrix\ncorr = df.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:black; background:white; border:0.2px dotted;\" role=\"tab\" aria-controls=\"home\"><center>Ensemble Methods</center></h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"estimators, accs = [], []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Bagging Algorithms\n\nThe three bagging models covered in this section are as follows:\n1. Bagged Decision Trees\n2. Random Forest\n3. Extra Trees"},{"metadata":{},"cell_type":"markdown","source":"## Decision Trees"},{"metadata":{"trusted":true},"cell_type":"code","source":"kfold = model_selection.KFold(n_splits=10)\ncart = DecisionTreeClassifier()\nnum_trees = 100\nmodel = BaggingClassifier(base_estimator=cart, n_estimators=num_trees)\nresults = model_selection.cross_val_score(model, features, labels, cv=kfold)\nacc = results.mean()\nprint(\"Accuracy:{:1.3f}\".format(acc))\n\n# append model and accuracy\nestimators.append(('decision_tree', model))\naccs.append(['decision_tree', round(acc, 3)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features = 3\nkfold = model_selection.KFold(n_splits=10)\nmodel = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\nresults = model_selection.cross_val_score(model, features, labels, cv=kfold)\nacc = results.mean()\nprint(\"Accuracy:{:1.3f}\".format(acc))\n\n# append model and accuracy\nestimators.append(('random_forest', model))\naccs.append(['random_forest', round(acc, 3)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extra Trees"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_trees = 100\nmax_features = 7\nkfold = model_selection.KFold(n_splits=10)\nmodel = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)\nresults = model_selection.cross_val_score(model, features, labels, cv=kfold)\nacc = results.mean()\nprint(\"Accuracy:{:1.3f}\".format(acc))\n\n# append model and accuracy\nestimators.append(('extra_trees', model))\naccs.append(['extra_trees', round(acc, 3)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Boosting Algorithms\n\nThe two most common boosting ensemble machine learning algorithms are:\n\n1. AdaBoost\n2. Stochastic Gradient Boosting"},{"metadata":{},"cell_type":"markdown","source":"## AdaBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_trees = 30\nkfold = model_selection.KFold(n_splits=10)\nmodel = AdaBoostClassifier(n_estimators=num_trees)\nresults = model_selection.cross_val_score(model, features, labels, cv=kfold)\nacc = results.mean()\nprint(\"Accuracy: {:1.3f}\".format(acc))\n\n# append model and accuracy\nestimators.append(('ada_boost', model))\naccs.append(['ada_boost', round(acc, 3)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stochastic Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_trees = 100\nkfold = model_selection.KFold(n_splits=10)\nmodel = GradientBoostingClassifier(n_estimators=num_trees)\nresults = model_selection.cross_val_score(model, features, labels, cv=kfold)\nacc = results.mean()\nprint(\"Accuracy: {:1.3f}\".format(acc))\n\n# append model and accuracy\nestimators.append(('gradient_boost', model))\naccs.append(['gradient_boost', round(acc, 3)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Voting Ensemble"},{"metadata":{"trusted":true},"cell_type":"code","source":"kfold = model_selection.KFold(n_splits=10)\nensemble = VotingClassifier(estimators)\nresults = model_selection.cross_val_score(ensemble, features, labels, cv=kfold)\nacc = results.mean()\nprint(\"Accuracy: {:1.3f}\".format(acc))\n\n# append accuracy\naccs.append(['voting', round(acc, 3)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Stacking Ensemble"},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_ensemble(estimators, X_train, X_test, y_train, y_test):\n    meta_X = list()\n    for name, model in estimators:\n        # train model\n        model.fit(X_train, y_train)\n        # predict model\n        yhat = model.predict(X_test)\n        yhat = yhat.reshape(len(yhat), 1)\n        meta_X.append(yhat)\n    # stack all the yhat outputs\n    meta_X = hstack(meta_X)\n    blender = LogisticRegression()\n    blender.fit(meta_X, y_test)\n    return blender","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_ensemble(estimators, blender, X_test):\n    meta_X = list()\n    for name, model in estimators:\n        # predict model\n        yhat = model.predict(X_test)\n        yhat = yhat.reshape(len(yhat), 1)\n        meta_X.append(yhat)\n    # stack all the yhat outputs\n    meta_X = hstack(meta_X)\n    return blender.predict(meta_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"blender = fit_ensemble(estimators, X_train, X_test, y_train, y_test)\nyhat = predict_ensemble(estimators, blender, X_test)\nscore = mean_absolute_error(y_test, yhat)\nprint('Blending Mean Absolute Error: {:1.3f}'.format(score))\n\nresults = model_selection.cross_val_score(blender, features, labels, cv=kfold)\nacc = results.mean()\nprint(\"Accuracy: {:1.3f}\".format(acc))\n\n# append accuracy\naccs.append(['blending', round(acc, 3)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:black; background:white; border:0.2px dotted;\" role=\"tab\" aria-controls=\"home\"><center>Analyze</center></h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"accs = np.array(accs)\n\ndf_accs = pd.DataFrame({ \"id\" : np.arange(len(accs)),\n                         \"label\" : accs[:,0], \n                         \"accuracy\" : accs[:,1] })","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,4))\nax = sns.barplot(x=\"id\", y=\"accuracy\", hue=\"label\", data=df_accs)\nax.set_title('Accuracy per Ensemble')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}