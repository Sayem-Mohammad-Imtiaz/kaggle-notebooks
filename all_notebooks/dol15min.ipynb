{"cells":[{"metadata":{},"cell_type":"markdown","source":"# The 15 min binary option","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import f1_score, mean_absolute_error\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nThis project aims to identify oportunities to enter into a 15 min binary option by predicting either if the future price of the asset, in this especific case DOL contracts, will be higher or what that price will be.\nThis model would be of interest to technical investors while building automated trading bots.\nI will try to answer 2 questions:\n\n- In 15 minutes the price will be higher or lower from now?\n- In 15 minutes what will be the price?\n\nIt is important to notice this notebook main intent is to learn and document the entire process of data analysis and as such does not hides some hypothesis that was later discarded in the proccess. \n\n## Data\n\nI will be using the \"Dollar Stock Prices and infos\" dataset avaiable at Kaggle.com - <https://www.kaggle.com/icarofreire/dollar-prices-and-infos>\nThis is the only dataset that will be used because the time window to analyse is too short and specific and it would be hard to fina any other dataset to aggregate information.\nI also believe that this dataset has enough information for the prediction, even though it should be possible to aggregate other assets prices for future models.\nI start by loading the dataset and naming the columns acccordingly to data creator description:","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/dollar-prices-and-infos/database_15min.csv', header=None, names=['time','opPrice_min_candle', 'maxPrice_min_candle', 'minPrice_min_candle', 'cloPrice_min_candle','volume', 'financial_information','negotiation', 'ma_last13', 'ma_last72', 'avg_last15_high', \n                                                                                                'avg_last15_low','diffMACD','deaMACD','MACDlh','difflh','dealh','opPrice_future15'])\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The columns descriptions are as follow:\n\n- opPrice_min_candle - Opening Price of 1 Minute Candle\n- maxPrice_min_candle - Maximum Price of 1 Minute Candle\n- minPrice_min_candle - Minimum Price of 1 Minute Candle\n- cloPrice_min_candle - Closing Price of 1 Minute Candle\n- volume - Number of trades in 1 minute candle\n- financial_information - volume multiplied by trade price in 1 minute candle\n- negotiation - no information on this feature\n- ma_last13 - price moving average in the last 13 minutes\n- ma_last72 - price moving average in the last 72 minutes\n- avg_last15_high - average max candle in the last 15 minutes\n- avg_last15_low - average min candle in the last 15 minutes\n- diffMACD - moving average convergence divergence variation\n- deaMACD - no information on this feature\n- MACDlh - no information on this feature\n- difflh - no information on this feature\n- dealh - no information on this feature\n- opPrice_future15 - target - price in 15 minutes from candle","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Methodology\n\nI started by looking at the correlation matrix of the features:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df.corr())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a lot of high correlation between features, but we should look at it with care since a lot of features in the dataset are calculations of others.\nFor this first analysis I will focus at the last line only, that show the correlation between features and target.\nAt this first look, for the linear part of the problem, we have a few strong features candidates in all the minute candle features, in the moving averages and in the avg of last 15 highs and lows.\n\nI will take a look at the target distribution:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.distplot(df['opPrice_future15'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will proceed to feature engineering.\nFirst of all I need to create a boolean target for my first question:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['target'] = df['opPrice_future15'] > df['cloPrice_min_candle']\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I begin the feature engineering.\nFirst I'm multiplying the volume to 10 to get the real number (when we divide financial_information to volume we get the average price).\nThen we get the average price, the delta price between candle opening to closing and the delta price between maximum and minimum price.\nThose are all visual indicators in technical analysis.\nWe also get the difference between the tops and bottoms of each candle to the last one as it is another visual indicator in technical analysis.\nIt is important to notice here that an analysis of the last few tops and bottoms could bring more insight as to identify tendecies in the market.\nI brought the MACD indicators even though some of their meaning are are not clear. We can clean those features if they are not significative in the future.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feat = pd.DataFrame()\n#feat['neg_diffMACD'] = df['negotiation']*df['diffMACD']\nfeat['volume'] = df['volume']*10\nfeat['avg_canPrice'] = df['financial_information']/feat['volume']\nfeat['delta_price'] = df['cloPrice_min_candle']-df['opPrice_min_candle']\nfeat['maxmin'] = df['maxPrice_min_candle']-df['minPrice_min_candle']\n#feat['delta_maxmin'] = feat['maxmin'].diff()\nfeat['movAvg13_closePrice'] = df['ma_last13'].diff()\nfeat['movAvg72_closePrice'] = df['ma_last72'].diff()\n#feat['delta_avgtops'] = df['avg_last15_high'].diff()\n#feat['delta_avgbottoms'] = df['avg_last15_low'].diff()\nfeat['delta_tops'] = df['maxPrice_min_candle'].diff()\nfeat['delta_bottoms'] = df['minPrice_min_candle'].diff()\nfeat[['diffMACD','deaMACD','MACDlh','difflh','dealh']] = df[['diffMACD','deaMACD','MACDlh','difflh','dealh']]\n#feat[['difflh','dealh']] = df[['difflh','dealh']]\nfeat.fillna(0, inplace=True)\nfeat.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how the engineered features are correlated now:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(feat.corr())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking better!\nI still have difflh and dealh that I have little information about and have high correlation between them so I will drop those for now.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feat.drop(columns=['difflh','dealh'], inplace=True)\nsns.heatmap(feat.corr())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Even though the moving averages have somewhat high correlation we will keep them for now and see how they fare when we test permutation importance.\nLet's see how our feature set correlates to the target:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_correl = feat.copy()\nfeat_correl['target'] = df['opPrice_future15']\nsns.heatmap(feat_correl.corr())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"My transformed features, even though they don't correlate between themselves also doesnt have a high correlation index with the target, and that could be bad to our linear model.\nI will also create another feature set with the high correlation original features:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_org = df[['opPrice_min_candle','maxPrice_min_candle', 'minPrice_min_candle', 'cloPrice_min_candle','ma_last13', 'ma_last72', 'avg_last15_high', 'avg_last15_low']]\nfeat_correl = feat_org.copy()\nfeat_correl['target'] = df['opPrice_future15']\n\n\nsns.heatmap(feat_org.corr())              ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will start by standarizing the features in both sets for both targets (boolean and linear):","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\ny_linear = df['opPrice_future15']\ny_boolean = df['target']\n\nscaler = StandardScaler()\nscaler.fit(feat)\nX = scaler.transform(feat)\nscaler.fit(feat_org)\nXorg = scaler.transform(feat_org)\n\nXlin_train, Xlin_test, ylin_train, ylin_test = train_test_split(X, y_linear, random_state = 0)\nXorglin_train, Xorglin_test, yorglin_train, yorglin_test = train_test_split(Xorg, y_linear, random_state = 0)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y_boolean, random_state = 0)\nXorg_train, Xorg_test, yorg_train, yorg_test = train_test_split(Xorg, y_boolean, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Methodology\n\nI will start fitting both features sets(transformed and original) with some of my default parameters and models and compare scores:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegressionCV\n\nhummerGBC = GradientBoostingClassifier(random_state=0,n_estimators=1000, n_iter_no_change=3, learning_rate=1)\nhummerGBC.fit(X_train, y_train)\nprint('Hummer features GBC score:', hummerGBC.score(X_test, y_test))\n\norgGBC = GradientBoostingClassifier(random_state=0,n_estimators=1000, n_iter_no_change=3, learning_rate=1)\norgGBC.fit(Xorg_train, yorg_train)\nprint('Original features GBC score:', orgGBC.score(Xorg_test, yorg_test))\n\nhummerSVC = LinearSVC(random_state=0, dual=False)\nhummerSVC.fit(X_train, y_train)\nprint('Hummer features SVC score:', hummerSVC.score(X_test, y_test))\n\norgSVC = LinearSVC(random_state=0, dual=False)\norgSVC.fit(Xorg_train, yorg_train)\nprint('Original features SVC score:', orgSVC.score(Xorg_test, yorg_test))\n\nhummerAda = AdaBoostClassifier(n_estimators=100, random_state=0)\nhummerAda.fit(X_train,y_train)\nprint('Hummer features ADA score:',hummerAda.score(X_test, y_test))\n\norgAda = AdaBoostClassifier(n_estimators=100, random_state=0)\norgAda.fit(Xorg_train,yorg_train)\nprint('Original features ADA score:',orgAda.score(Xorg_test, yorg_test))\n\nhummerLog = LogisticRegressionCV(cv=5, random_state=0)\nhummerLog.fit(X_train, y_train)\nprint('Hummer features Log score:', hummerLog.score(X_test, y_test))\n\norgLog = LogisticRegressionCV(cv=5, random_state=0)\norgLog.fit(Xorg_train, yorg_train)\nprint('Original features Log score:', orgLog.score(Xorg_test, yorg_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our best score was with the original high correlation feature set with GBC.\nWill proceed to a permutation importance analysis to see the most important features in the choosen model an also to see if we can remove some features and keep a high model score:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nXorg_test_df = pd.DataFrame(Xorg_test, columns=feat_org.columns, index = None)\nperm = PermutationImportance(orgGBC, random_state=0).fit(Xorg_test, yorg_test)\neli5.show_weights(perm, feature_names = Xorg_test_df.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I decided to keep all features.\nIt is interesting to note that the most important features are:\n- Moving average of last 72 mins (medium term curve)\n- Average last 15 low\n- Average last 15 high\n- Closing candle price\n- Moving average of last 13 mins (short term curve)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's follow on to the linear model.\nAs with the classification one I will run a few default models and see how they perform:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import LinearSVR\n\nhummerGBR = GradientBoostingRegressor(random_state=0)\nhummerGBR.fit(Xlin_train, ylin_train)\nprint('Hummer feature GBR score:',hummerGBR.score(Xlin_test, ylin_test))\n\norgGBR = GradientBoostingRegressor(random_state=0)\norgGBR.fit(Xorglin_train, yorglin_train)\nprint('Original feature GBR score:',orgGBR.score(Xorglin_test, yorglin_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I got the best score with the original feature set.\nLet's see the permutation importance:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Xorg_test_df = pd.DataFrame(Xorg_test, columns=feat_org.columns, index = None)\nperm = PermutationImportance(orgGBR, random_state=0).fit(Xorg_test, yorg_test)\neli5.show_weights(perm, feature_names = Xorg_test_df.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once again the main drivers are candle analysis and moving average. \nI decided to not discard any features.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Results\n\n### Classification problem\nThe classification model didn't have enough accuracy in the test set. In any feature selection made it did not perform well.\nIt was interesting to note that even though, from the feature set that best performed, we could identify the candle caracteristics and moving averages as the main drivers for prediction.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Linear problem\nThe linear model had a very high accuracy in the test set in both feature sets selected, but the score was higher in the original feature set(with intern correlations).\nIt is important to note that it confirmed the drivers from the classification model, that is, candle caracteristics and moving averages.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Discussion\n\nBy using usual technical analysis indicators we managed to predict the asset price in 15 minutes with a very high confidence but we did not manage to classify correctly ups and downs.\nThe classification problem is minor since we managed to predict the scalar value, and consequently the up and down outcome of the price.\nThe feature weight analysis showed that technical analysis indicators are indeed drivers for the future price of the target asset, even though it would be needed to verify that with other assets and with a bigger dataset.\nI think some more data could enrich the model like tape reading (buying and selling orders) and future analysis should consider that.\nAlso, the model could use a bigger data volume. The high correlation and linear model score should no sustain in real world data for too long.\n\n## Conclusion\n\nGiven the limited data I had I managed to see some technical analysis indicators actually helping to predict the future price of the target asset.\nEven though the dataset does not seem diverse and big enough to bring the real world noise and diversity into the model. \nThe almost perfect correlation scores and regression scores makes me wonder that the model would not hold toghether in a real world test for too long.\nNeverthless it is a good enough starting point for more robust models with a bigger dataset and maybe more relevant data like buy and sell orders volume and prices.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}