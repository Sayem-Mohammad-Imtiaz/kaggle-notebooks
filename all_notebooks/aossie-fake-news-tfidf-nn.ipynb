{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np, pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom tqdm import tqdm\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainFile=pd.read_csv('../input/train_merged.csv',encoding='latin-1')\ntestFile=pd.read_csv('../input/test_merged.csv', encoding='latin-1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"totaldata= trainFile['articleBody'].tolist() + trainFile['Headline'].tolist()+testFile['articleBody'].tolist()+testFile['Headline'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=80, stop_words='english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_vect.fit(totaldata)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_head_vec= tfidf_vect.transform(trainFile['Headline'])\ntrain_body_vec= tfidf_vect.transform(trainFile['articleBody'])\ntest_head_vec= tfidf_vect.transform(testFile['Headline'])\ntest_body_vec= tfidf_vect.transform(testFile['articleBody'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run and tested\nimport os\nimport re\nimport nltk\nimport numpy as np\nfrom sklearn import feature_extraction\nfrom tqdm import tqdm\n\n\n_wnl = nltk.WordNetLemmatizer()\n\n\ndef normalize_word(w):\n    return _wnl.lemmatize(w).lower()\n\n\ndef get_tokenized_lemmas(s):\n    return [normalize_word(t) for t in nltk.word_tokenize(s)]\n\n\ndef clean(s):\n    # Cleans a string: Lowercasing, trimming, removing non-alphanumeric\n\n    return \" \".join(re.findall(r'\\w+', s, flags=re.UNICODE)).lower()\n\n\ndef remove_stopwords(l):\n    # Removes stopwords from a list of tokens\n    return [w for w in l if w not in feature_extraction.text.ENGLISH_STOP_WORDS]\n\n\ndef gen_or_load_feats(feat_fn, headlines, bodies, feature_file):\n    if not os.path.isfile(feature_file):\n        feats = feat_fn(headlines, bodies)\n        np.save(feature_file, feats)\n\n    return np.load(feature_file)\n\n\n\n\ndef word_overlap_features(features, headline, body):\n    # common word/ total word\n    clean_headline = clean(headline)\n    clean_body = clean(body)\n    clean_headline = get_tokenized_lemmas(clean_headline)\n    clean_body = get_tokenized_lemmas(clean_body)\n    feature = len(set(clean_headline).intersection(clean_body)) / float(len(set(clean_headline).union(clean_body)))\n    features.append(feature)\n    return features\n\n\n\n\ndef ngrams(input, n):\n    input = input.split(' ')\n    output = []\n    for i in range(len(input) - n + 1):\n        output.append(input[i:i + n])\n    return output\n\n\ndef chargrams(input, n):\n    output = []\n    for i in range(len(input) - n + 1):\n        output.append(input[i:i + n])\n    return output\n\n\ndef append_chargrams(features, text_headline, text_body, size):\n    grams = [' '.join(x) for x in chargrams(\" \".join(remove_stopwords(text_headline.split())), size)]\n    grams_hits = 0\n    grams_early_hits = 0\n    grams_first_hits = 0\n    for gram in grams:\n        if gram in text_body:\n            grams_hits += 1\n        if gram in text_body[:255]:\n            grams_early_hits += 1\n        if gram in text_body[:100]:\n            grams_first_hits += 1\n    features.append(grams_hits)\n    features.append(grams_early_hits)\n    features.append(grams_first_hits)\n    return features\n\n\ndef append_ngrams(features, text_headline, text_body, size):\n    grams = [' '.join(x) for x in ngrams(text_headline, size)]\n    grams_hits = 0\n    grams_early_hits = 0\n    for gram in grams:\n        if gram in text_body:\n            grams_hits += 1\n        if gram in text_body[:255]:\n            grams_early_hits += 1\n    features.append(grams_hits)\n    features.append(grams_early_hits)\n    return features\n\n\ndef hand_features(headlines, bodies):\n    def binary_co_occurence(headline, body):\n        # Count how many times a token in the title\n        # appears in the body text.\n        bin_count = 0\n        bin_count_early = 0\n        for headline_token in clean(headline).split(\" \"):\n            if headline_token in clean(body):\n                bin_count += 1\n            if headline_token in clean(body)[:255]:\n                bin_count_early += 1\n        return [bin_count, bin_count_early]\n\n    def binary_co_occurence_stops(headline, body):\n        # Count how many times a token in the title\n        # appears in the body text. Stopwords in the title\n        # are ignored.\n        bin_count = 0\n        bin_count_early = 0\n        for headline_token in remove_stopwords(clean(headline).split(\" \")):\n            if headline_token in clean(body):\n                bin_count += 1\n                bin_count_early += 1\n        return [bin_count, bin_count_early]\n\n    def count_grams(headline, body):\n        # Count how many times an n-gram of the title\n        # appears in the entire body, and intro paragraph\n\n        clean_body = clean(body)\n        clean_headline = clean(headline)\n        features = []\n        features = append_chargrams(features, clean_headline, clean_body, 2)\n        features = append_chargrams(features, clean_headline, clean_body, 8)\n        features = append_chargrams(features, clean_headline, clean_body, 4)\n        features = append_chargrams(features, clean_headline, clean_body, 16)\n        features = append_ngrams(features, clean_headline, clean_body, 2)\n        features = append_ngrams(features, clean_headline, clean_body, 3)\n        features = append_ngrams(features, clean_headline, clean_body, 4)\n        features = append_ngrams(features, clean_headline, clean_body, 5)\n        features = append_ngrams(features, clean_headline, clean_body, 6)\n        return features\n\n    def other_feature(headline, body):\n        features=[]\n        features= word_overlap_features(features, headline, body)\n        return features\n    X = []\n    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):\n        lst= binary_co_occurence(headline, body) + binary_co_occurence_stops(headline, body) + count_grams(headline, body)+ other_feature(headline, body)\n        X.append(lst)\n\n\n    X =np.array(X)\n    return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_handF= hand_features(trainFile['Headline'].tolist(), trainFile['articleBody'].tolist())\ntest_handF=hand_features(testFile['Headline'].tolist(), testFile['articleBody'].tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.sparse import  hstack","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_final= hstack([train_head_vec,train_body_vec]).toarray()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_final= hstack([test_head_vec,test_body_vec]).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_final =np.concatenate((train_final,train_handF), axis=1)\ntest_final =np.concatenate((test_final,test_handF), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_final= np.reshape(train_final,(train_final.shape[0],1,train_final.shape[1]))\n# test_final= np.reshape(test_final,(test_final.shape[0],1,test_final.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(type(train_final), train_final.shape, test_final.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainFile.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels= trainFile['Stance'].copy()\ntest_labels= testFile['Stance'].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Input, concatenate,LSTM,Flatten, Dense,Dropout\nfrom keras.models import Model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inp = Input(shape=(train_final.shape[1],))\n# lstm = LSTM(150,return_sequences=True)(inp)\nlay1= Dense(200, activation= 'tanh')(inp)\nlay1= Dense(100, activation= 'sigmoid')(lay1)\n# lay1= Dropout(0.3)(lay1)\nlay2= Dense(40, activation= 'sigmoid')(lay1)\noutp= Dense(4,activation='sigmoid')(lay2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model= Model(inputs=[inp], outputs=[outp])\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = pd.get_dummies(trainFile['Stance']).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testY = pd.get_dummies(testFile['Stance'] ).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Y= np.reshape(Y,(Y.shape[0],1,Y.shape[1]))\n# testY= np.reshape(testY,(testY.shape[0],1,testY.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit([train_final],[Y],epochs=5,batch_size=10,verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction= model.predict([test_final],verbose=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scor, acc = model.evaluate([test_final],[testY])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cls= prediction.argmax(axis=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(cls), cls.shape, type(cls[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainFile['Stance'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# np.unique(cls)\nacls=[]\nfor i in range(len(cls)):\n    if cls[i]== 0:\n        acls.append(\"agree\")\n    elif cls[i]== 1:\n        acls.append(\"disagree\")\n    elif cls[i]== 2:\n        acls.append(\"discuss\")\n    elif cls[i]== 3:\n        acls.append(\"unrelated\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(acls)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.utils.multiclass import unique_labels\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels= trainFile['Stance'].copy()\ntest_labels= testFile['Stance'][:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix1 = confusion_matrix(test_labels, acls)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(cm,\n                          target_names,\n                          title='Confusion matrix',\n                          cmap=None,\n                          normalize=True):\n    \"\"\"\n    given a sklearn confusion matrix (cm), make a nice plot\n\n    Arguments\n    ---------\n    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n\n    target_names: given classification classes such as [0, 1, 2]\n                  the class names, for example: ['high', 'medium', 'low']\n\n    title:        the text to display at the top of the matrix\n\n    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n                  see http://matplotlib.org/examples/color/colormaps_reference.html\n                  plt.get_cmap('jet') or plt.cm.Blues\n\n    normalize:    If False, plot the raw numbers\n                  If True, plot the proportions\n\n    Usage\n    -----\n    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n                                                              # sklearn.metrics.confusion_matrix\n                          normalize    = True,                # show proportions\n                          target_names = y_labels_vals,       # list of names of the classes\n                          title        = best_estimator_name) # title of graph\n\n    Citiation\n    ---------\n    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import itertools\n\n    accuracy = np.trace(cm) / float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n\n    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(cm=matrix1,target_names=['agree', 'disagree', 'discuss', 'unrelated'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainFile['Stance'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testFile['Stance'].value_counts()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}