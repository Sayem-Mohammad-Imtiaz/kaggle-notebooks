{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.inspection import permutation_importance\n\nfrom imblearn.over_sampling import SMOTE\n\nimport matplotlib.pyplot as plt\n\nimport random\nimport time\n\npd.set_option('display.max_columns', 50)\npd.set_option('display.max_rows', 50)\npd.options.mode.chained_assignment = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"init_notebook = time.time()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# 1. Introduction\n\nThis notebook presents an implementation of two modules of the Data Driven Activity Scheduler (DDAS) as presented by Drchal, Certicky and Jakob (2019) (https://doi.org/10.1016/j.trc.2018.12.002).\n\nInput includes the Federal District Urban Mobility Survey (FDUMS) dataset, which contains socioeconomic features and activity diaries for the population in the Federal District, Brazil. (https://doi.org/10.34740/kaggle/dsv/1315731)"},{"metadata":{},"cell_type":"markdown","source":"# 2. Data Preparation\nThe first process in this project's pipeline is data preparation, which consists in importing the FDUMS dataset and performing some pre-processing such as data cleansing, re-labeling and feature selection. The goal is to end up with a dataset that is compatible with the input required by the DDAS framework. In the referenced paper (Drchal et al, 2019), it can be observed that the input dataset that was used is composed by three parts, which the authors call *soc*, *reach* and *counts*, in addition to the typical information of a travel diary.\n\n## 2.1 Creating the *soc* dataset\n\nFirst, the *soc* dataset is created. In order to do so, the available dataset must be analyzed and the features needed must be extracted. For this step, the tables *Person* and *Household* are used."},{"metadata":{"trusted":true},"cell_type":"code","source":"fdums_household = pd.read_csv('../input/urban-mobility-survey-federal-district-brazil/Household.csv', sep=';')\nfdums_person = pd.read_csv('../input/urban-mobility-survey-federal-district-brazil/Person.csv', sep=';')\nfdums_trip = pd.read_csv('../input/urban-mobility-survey-federal-district-brazil/Trip.csv', sep=';')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Taking a look into the columns of each imported table:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fdums_person.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fdums_household.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fdums_trip.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The columns needed for the *soc* dataset must be extracted:"},{"metadata":{"trusted":true},"cell_type":"code","source":"soc_columns = ['person_id', 'household_id', 'age', 'gender', 'area_of_occupation', 'education_level', 'has_driver_license']\nsoc = fdums_person[soc_columns]\n\nprint('The number of rows in the soc dataframe is '\n      + str(len(soc)))\n\nsoc.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to Drchal et al. (2019), in addition to the columns that were already extracted from the Person table, information about the each person's household, specifically the number of people living in the household and the number of vehicles available. We can derive this information from the Household table, with the columns \"people_in_household\" and \"vehicles\":"},{"metadata":{"trusted":true},"cell_type":"code","source":"import_columns = ['household_id','people_in_household','vehicles']\nhousehold_info = fdums_household[import_columns]\nhousehold_info.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we merge this imported dataset to the *soc* dataset that we were developing"},{"metadata":{"trusted":true},"cell_type":"code","source":"soc = pd.merge(soc, household_info, on='household_id')\nsoc.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have to create the *soc* dataset only for the individuals for which there is information in the Trips table (not all individuals in the People table are also covered by the Trips table)"},{"metadata":{"trusted":true},"cell_type":"code","source":"individuals = fdums_trip.person_id.unique()\nsoc = soc.loc[soc['person_id'].isin(individuals)]\n\nprint('The number of rows in the soc dataframe is '\n      + str(len(soc)))\n\nsoc.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we know that our *soc* dataframe has 44,101 rows. Let's check how many of those have missing values:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The number of rows in the soc dataframe with missing values is '\n      + str(soc.shape[0] - soc.dropna().shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since only a few rows (<1%) have missing values, we'd better just drop these rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"soc = soc.dropna()\nprint('The number of rows in the soc dataframe is '\n      + str(len(soc)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the DDAS framework, we will be using the Decision Tree Classifier model from the scikit-learn library. This model, however, does not accept categorical features as input. Therefore, we need to relabel all categorical values in our soc dataframe. We will start with the 'age' feature, which we will map into ordinal categories. Let's check what are the possible values, so we can map them:"},{"metadata":{"trusted":true},"cell_type":"code","source":"soc.age.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"map_age = {'0 to 4 years old': 1,\n           '5 to 9 years old': 2,\n           '10 to 14 years old': 3,\n           '15 to 17 years old': 4,\n           '18 to 19 years old': 5,\n           '20 to 24 years old': 6,\n           '25 to 29 years old': 7,\n           '30 to 39 years old': 8,\n           '40 to 49 years old': 9,\n           '50 to 59 years old': 10,\n           '60 to 69 years old': 11,\n           '70 to 79 years old': 12,\n           'More than 80 years old': 13}\n\nsoc.age = soc.age.map(map_age)\n\nsoc.rename(columns={'age':'age_group'}, inplace=True)\n\nsoc.head()\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that we also changed the column name to avoid misunderstandings.    \nWe must do the same thing with the 'education_level' column, mapping is values to ordinal integers"},{"metadata":{"trusted":true},"cell_type":"code","source":"soc.education_level.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"map_education = {'Younger than 6 years old / not student': 1,\n                 'Illiterate': 1,\n                 'Literate, but with no education level': 2,\n                 'Early childhood/kindergarden level': 3,\n                 'Incomplete lower-secondary/middle school': 4,\n                 'Complete lower-secondary/middle school': 5,\n                 'Incomplete upper-secondary/high school': 6,\n                 'Complete upper-secondary/high school': 7, \n                 'Incomplete undergraduate school': 8,\n                 'Complete undergraduate school': 9,\n                 'Complete graduate school': 10}\n\nsoc.education_level = soc.education_level.map(map_education)\nsoc.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have to transform the values in the columns 'gender' and 'has_driver_license' into binary values (0 or 1). At the same time, we will change the column name of 'gender' to 'is_female' to mantain consistency."},{"metadata":{"trusted":true},"cell_type":"code","source":"map_gender = {'Male':0,\n              'Female':1}\n\nmap_has_driver_license = {'No':0,\n                          'Yes':1}\n\nsoc.gender = soc.gender.map(map_gender)\nsoc.has_driver_license = soc.has_driver_license.map(map_has_driver_license)\n\nsoc.rename(columns={'gender':'is_female'}, inplace=True)\n\nsoc.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need to transform the column 'area_of_occupation' into a column that indicates whether the person is student. In order to do so, we will map all values that indicates that the person is a student into '1' and the other values into '0'. Finally, we change the name of the column."},{"metadata":{"trusted":true},"cell_type":"code","source":"soc.area_of_occupation.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"soc = soc.rename(columns={'area_of_occupation': 'is_student'})\n\nmap_is_student = {'Retired': 0,\n                  'Other': 0,\n                  'Self-employed (professional)': 0,\n                  'Student (regular courses)': 1,\n                  'Homekeeper': 0,\n                  'Businessperson': 0,\n                  'Private worker': 0,\n                  'Civil servent': 0,\n                  'No activity': 0,\n                  'Unemployed': 0,\n                  'Domestic worker': 0,\n                  'Self-employed (casual)': 0,\n                  'Student (extension courses)': 1,\n                  'Volunteer work':0}\n\nsoc.is_student = soc.is_student.map(map_is_student)\n\nsoc.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, the column 'vehicles', which originally indicates the number of vehicles in the household, must be transformed to indicate instead whether there is a vehicle available (binary, 0 or 1)."},{"metadata":{"trusted":true},"cell_type":"code","source":"soc = soc.rename(columns={'vehicles': 'is_car_available'})\n\ndef map_is_car_available(row):\n    if row.is_car_available>=1:\n        row.is_car_available = 1\n    else:\n        row.is_car_available = 0\n    return row\n\nsoc = soc.apply(map_is_car_available, axis='columns')\n\nsoc.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we are interested in investigating people's behavior regarding mobility, it is important to exclude the children from the dataset, as we understand they do not make decisions about their mobility patterns, they simply follow their parents. Hence, we need to drop all rows that regard young people, being values 1, 2 and 3 of the column 'age_group'"},{"metadata":{"trusted":true},"cell_type":"code","source":"soc = soc.drop(soc[soc.age_group <= 3].index)\nprint('The number of rows in the soc dataframe is '+str(len(soc)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that after data cleansing, we kept only 36,582 individuals from the 61,358 on the original dataset."},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Creating the *reach* dataset\n\nIn order to create the *reach* dataset, we have to determine the average time each person would take to go from their homes to the location where they perform their main activity of the day, at 8 AM of a typical weekday, for each transportation mode. The first thing we have to do, then, is extract from the Trip table relevant information for our analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"fdums_trip.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import_columns = ['person_id',\n                  'trip_id',\n                  'ar_origin',\n                  'ar_destination',\n                  'activity_origin',\n                  'activity_destination',\n                  'includes_walk',\n                  'includes_bicycle',\n                  'includes_subway',\n                  'includes_brt',\n                  'includes_bus',\n                  'includes_unlicensed_service',\n                  'includes_private_charter',\n                  'includes_school_bus',\n                  'includes_car_as_driver',\n                  'includes_car_as_passenger',\n                  'includes_motorcycle_as_driver',\n                  'includes_motorcycle_as_passenger',\n                  'includes_taxi',\n                  'includes_motorbicycle_taxi',\n                  'includes_private_driver',\n                  'includes_other_modes']\n\ntrips = fdums_trip[import_columns]\n\nprint('The trips dataframe contains information about '\n      + str(len(trips))\n      + ' trips performed by '\n      + str(len(trips.person_id.unique()))\n      + ' people')\n\ntrips.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first thing we need to do is delete rows that regard people that have at least one missing information. This means that even if only one trip of the person is missing data, we will remove all trips for that person from the dataset, because we won't be able to perform proper analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"person_missing_values = trips[pd.isnull(trips.activity_destination)].person_id\ntrips = trips.drop(trips[trips.person_id.isin(person_missing_values)].index)\n\nprint('The trips dataframe contains information about '\n      + str(len(trips))\n      + ' trips performed by '\n      + str(len(trips.person_id.unique()))\n      + ' people')\n\ntrips.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also need to check if there are rows with incomplete diaries. In order to do so, we will look for people who only perform 1 trip"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Slow cell\nincomplete_diaries = trips.copy()\nincomplete_diaries = incomplete_diaries[incomplete_diaries.groupby('person_id').person_id.transform(len) == 1]\n\nprint('There are '\n      + str(len(incomplete_diaries))\n      + ' people with incomplete diaries within the trips dataset')\n\nincomplete_diaries.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the number of people with incomplete diaries represent less than 3% of the total population surveyed, we decide to drop these rows"},{"metadata":{"trusted":true},"cell_type":"code","source":"person_incomplete_diary = incomplete_diaries.person_id\ntrips = trips.drop(trips[trips.person_id.isin(person_incomplete_diary)].index)\n\nprint('The trips dataframe contains information about '\n      + str(len(trips))\n      + ' trips performed by '\n      + str(len(trips.person_id.unique()))\n      + ' people')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's check the possible purposes of the trips in our dataset, that is the possible values for 'activities' performed on the destination."},{"metadata":{"trusted":true},"cell_type":"code","source":"trips.activity_destination.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe that in the FDUMS dataset, there are many more categories for activity types than what is described for the DDAS framework: 'sleep', 'work', 'school', 'leisure', 'shop'. Then, we now must map the values in our dataset in order to match the ones in DDAS. We also create an 'other' type to comprise the categories that do not fit in the existing labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"map_activity_type = {'Shop':'shop',\n                     'Home':'sleep', \n                     'Main workplace':'work', \n                     'Main study place': 'school',\n                     'Taking someone somewhere':'other',\n                     'Other':'other', \n                     'Eating out':'shop',\n                     'Secondary study place':'school',\n                     'Personal matters':'other', \n                     'Leisure':'leisure',\n                     'Secondary workplace':'work', \n                     'Health':'other', \n                     'Business':'work'}\n\ntrips.activity_origin = trips.activity_origin.map(map_activity_type)\ntrips.activity_destination = trips.activity_destination.map(map_activity_type)\n\ntrips.activity_origin.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have to define which is the main activity each person performs. If the person works, \"Work\" is their main activity. If not, and if they study, \"Study\" is their main activity. In all other cases, \"Other\" is their main activity. If the person performs the same kind of activity in different places during the same day, we compute the one that happens first. Therefore, we create a function to compute that, by iterating over the *trips* dataset and returning the Administrative Region where the each person performs their main activity."},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_main_activ_places(input_trips):\n    '''Returns a dictionary with the Administrative Region where\n    each person performs their main activity.\n    \n    Keyword arguments:\n    input_trips -- dataframe containing trip information\n    '''\n    trips = input_trips.copy()\n\n    dict_activity = dict.fromkeys(trips.person_id.unique())\n    dict_places = dict.fromkeys(trips.person_id.unique())\n    \n    for row in trips.to_numpy():\n        person_id = row[0]\n        activity = row[5]\n        cur_main_activity = dict_activity[person_id]\n        \n        if activity == 'work':\n            if cur_main_activity == 'work':\n                pass\n            else:\n                dict_activity[person_id]='work'\n                dict_places[person_id]=row[3]\n        \n        elif activity =='school':\n            if cur_main_activity == 'work':\n                pass\n            elif cur_main_activity == 'school':\n                pass\n            else:\n                dict_activity[person_id]='school'\n                dict_places[person_id]=row[3]     \n        \n        else:\n            if cur_main_activity == 'work':\n                pass\n            elif cur_main_activity == 'school':\n                pass\n            elif cur_main_activity == 'other':\n                pass\n            else:\n                dict_activity[person_id]='other'\n                dict_places[person_id]=row[3]\n        \n    return dict_places\n\nmain_places = compute_main_activ_places(trips)\n\n# Displaying the first 5 entries of the dictionary:\ndict(list(main_places.items())[0:5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using this result, which is a dictionary that indicates the Administrative Region where each person performs their main activity, we start creating the *reach* Dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"reach = pd.DataFrame.from_dict(main_places, orient='index', columns=['main_actv_ar'])\nreach.reset_index(inplace=True)\nreach.rename(columns={'index':'person_id'}, inplace=True)\nreach.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next information we need regards the Adminstrative Region where the person lives. This may be obtained from the *fdums_household* dataframe. We now write a function to compute that."},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_home_place(input_person, input_household):\n    '''Returns a dictionary with the Administrative Region where\n    each person lives.\n    \n    Keyword arguments:\n    input_person -- dataframe with information about each person\n    input_household -- dataframe with information about each household\n    '''\n    person = input_person.copy()\n    household = input_household.copy()\n\n    dict_home_ar = dict.fromkeys(person.person_id.unique())\n    household.set_index('household_id', inplace=True)\n    \n    for row in person.to_numpy():\n        this_person = row[0]\n        this_household = row[1]\n        dict_home_ar[this_person] = household.loc[this_household, 'administrative_region']\n    \n    return dict_home_ar\n\nhome_ar = compute_home_place(fdums_person, fdums_household)\n\n# Displaying the first 5 entries of the dictionary:\ndict(list(home_ar.items())[0:5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use the dictionary that was obtained from the previous function to create a new column in the *reach* dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"reach['home_ar'] = reach['person_id'].map(home_ar)\nreach = reach[['person_id', 'home_ar', 'main_actv_ar']]\nreach.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to compute the average duration of the trip between the pairs of administrative regions in Federal District (*Distance Matrix* tables), we used information retrieved from the Google Maps API. The detailed procedure is described in https://github.com/danielefm/DistanceMatrix.     \n    \nIn the Distance Matrices we are using as input, the first column represents the administrative region of origin and each possible destination is represented in each of the following columns. In the cell where origin and destination cross, we get the value for the average time of travel between these two locations on a common weekday, at 8 AM."},{"metadata":{"trusted":true},"cell_type":"code","source":"bike_distance_matrix = pd.read_csv('../input/distance-matrix-distrito-federal/reach_bicycling.csv', sep=';')\n\n# Displaying the first five rows and columns\nbike_distance_matrix.iloc[:5,:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need to rename the first column and turn it into the index for the dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"bike_distance_matrix.rename(columns={'Unnamed: 0':'origin'}, inplace=True)\nbike_distance_matrix.set_index('origin', inplace=True)\n\nprint('This Distance Matrix has '\n      + str(len(bike_distance_matrix)) \n      + ' rows (possible origins) and ' \n      + str(len(bike_distance_matrix.columns))\n      + ' columns (possible destinations)')\n\n# Displaying the first five rows and columns\nbike_distance_matrix.iloc[:5,:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we do the same for the other Distance Matrices"},{"metadata":{"trusted":true},"cell_type":"code","source":"car_distance_matrix = pd.read_csv('../input/distance-matrix-distrito-federal/reach_driving.csv', sep=';')\ncar_distance_matrix.rename(columns={'Unnamed: 0':'origin'}, inplace=True)\ncar_distance_matrix.set_index('origin', inplace=True)\n\ntransit_distance_matrix = pd.read_csv('../input/distance-matrix-distrito-federal/reach_transit.csv', sep=';')\ntransit_distance_matrix.rename(columns={'Unnamed: 0':'origin'}, inplace=True)\ntransit_distance_matrix.set_index('origin', inplace=True)\n\nwalk_distance_matrix = pd.read_csv('../input/distance-matrix-distrito-federal/reach_walking.csv', sep=';')\nwalk_distance_matrix.rename(columns={'Unnamed: 0':'origin'}, inplace=True)\nwalk_distance_matrix.set_index('origin', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We must now create columns in the *reach* dataframe to present the *reach* values from the Distance Matrices for each transportation mode, for each person."},{"metadata":{"trusted":true},"cell_type":"code","source":"def reach_from_distance_matrix(input_person, input_distance_matrix):\n    '''Returns a dictionary with reach information (average trip duration\n    between the person's Administrative Region (AR) and the AR where\n    he/she performs their main activity, for a given mode).\n    \n    Keyword arguments:\n    input_person -- dataframe with information about the person\n    input_distance_matrix -- dataframe with the distance matrix for a given mode\n    '''\n    person = input_person.copy()\n    distance_matrix = input_distance_matrix.copy()\n\n    dict_reach = dict.fromkeys(person.person_id.unique())\n    \n    for row in person.to_numpy():\n        this_person = row[0]\n        origin = row[1]\n        destination = row[2]\n       \n        try:\n            dict_reach[this_person] = distance_matrix.loc[origin, destination]   \n        except:\n            dict_reach[this_person] = -1\n    \n    return dict_reach\n\nbike_reach_dict = reach_from_distance_matrix(reach, bike_distance_matrix)\n\n# Displaying the first 5 entries of the dictionary:\ndict(list(bike_reach_dict.items())[0:5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And then for the other Distance Matrices:"},{"metadata":{"trusted":true},"cell_type":"code","source":"car_reach_dict = reach_from_distance_matrix(reach, car_distance_matrix)\ntransit_reach_dict = reach_from_distance_matrix(reach, transit_distance_matrix)\nwalk_reach_dict = reach_from_distance_matrix(reach, walk_distance_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And finally we append those *reach* columns into the *reach* dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"reach['reach_bike'] = reach['person_id'].map(bike_reach_dict)\nreach['reach_car'] = reach['person_id'].map(car_reach_dict)\nreach['reach_transit'] = reach['person_id'].map(transit_reach_dict)\nreach['reach_walk'] = reach['person_id'].map(walk_reach_dict)\nreach.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The *home_ar* and the *main_actv_ar* are actually not needed for the model, they were only auxiliary on the process of obtaining the *reach* columns. Hence, now we drop them."},{"metadata":{"trusted":true},"cell_type":"code","source":"reach = reach.drop(['home_ar','main_actv_ar'], axis=1)\nreach.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we create a unique table by merging the *soc* and *reach* dataframes"},{"metadata":{"trusted":true},"cell_type":"code","source":"soc_and_reach = pd.merge(soc, reach, on='person_id')\nsoc_and_reach.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Organizing the *trips* dataframe\n"},{"metadata":{},"cell_type":"markdown","source":"First, let's check how our *trips* dataframe is right now"},{"metadata":{"trusted":true},"cell_type":"code","source":"trips.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trips.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The *trips* dataframe that we currently have is still not compatible to what we need to implement DDAS. We start the data cleansing process by dropping the columns that we won't need"},{"metadata":{"trusted":true},"cell_type":"code","source":"trips.drop(columns=['ar_origin', 'ar_destination'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On the original study, only four types of transport modes were considered: *bike, car, pt* (transit) and *walk*. So, our first task for organizing this dataframe is to map the transport modes we have on *trips* into the categories considered in DDAS."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Slow cell\ndef create_pt_column(row):\n    if (row.includes_subway==1 or row.includes_brt==1 or row.includes_bus==1):\n        row.includes_pt = 1\n    else:\n        row.includes_pt = 0\n    \n    return row\n\n# Create a new empty column in the trips dataframe\ntrips['includes_pt'] = ''\n\n# Check if any subtype of the public transportation mode is included in the trip\n# for each row\ntrips = trips.apply(create_pt_column, axis='columns')\n\n# Drop the subtype columns\ntrips.drop(columns=['includes_subway', 'includes_brt', 'includes_bus'],\n           inplace=True)\n\ntrips.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Slow cell\ndef create_car_column(row):\n    if (row.includes_car_as_passenger==1 or row.includes_car_as_driver==1):\n        row.includes_car = 1\n    else:\n        row.includes_car = 0\n    \n    return row\n\n# Create a new empty column in the trips dataframe\ntrips['includes_car'] = ''\n\n# Check if any subtype of the car mode is included in the trip\n# for each row\ntrips = trips.apply(create_car_column, axis='columns')\n\n# Drop the subtype columns\ntrips.drop(columns=['includes_car_as_passenger', 'includes_car_as_driver'],\n           inplace=True)\n\ntrips.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Slow cell\ndef create_other_column(row):\n    if (row.includes_unlicensed_service==1 or\n        row.includes_private_charter==1 or\n        row.includes_school_bus==1 or\n        row.includes_motorcycle_as_driver==1 or\n        row.includes_motorcycle_as_passenger==1 or\n        row.includes_taxi==1 or\n        row.includes_motorbicycle_taxi==1 or\n        row.includes_private_driver==1 or\n        row.includes_other_modes==1):\n        row.includes_other_modes = 1\n    else:\n        row.includes_other_modes = 0\n    \n    return row\n\n# Check if any subtype of other modes is included in the trip\n# for each row\ntrips = trips.apply(create_other_column, axis='columns')\n\n# Drop the subtype columns\ntrips.drop(columns=['includes_unlicensed_service', \n                    'includes_private_charter', \n                    'includes_school_bus', \n                    'includes_motorcycle_as_driver', \n                    'includes_motorcycle_as_passenger',\n                    'includes_taxi',\n                    'includes_motorbicycle_taxi',\n                    'includes_private_driver'], inplace=True)\ntrips.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have to create a *Y* column for when our DDAS implementation predicts the transportation mode. This would be the reverse process of One-Hot-Encoding, because we would have to check which transportation mode column has the value of 1 for each trip and get the respective transport mode. However, some people might have more than one transport mode for the same trip. Let's check how many of these occurences we find in this dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The trips dataframe contains information about '\n      + str(len(trips))\n      + ' trips performed by '\n      + str(len(trips.person_id.unique()))\n      + ' people')\n\ntrips.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transfer_trips = trips.loc[trips.loc[:,['includes_walk',\n                                        'includes_bicycle',\n                                        'includes_other_modes',\n                                        'includes_pt',\n                                        'includes_car']].sum(axis=1)>1]\nprint('In the trips dataframe, '\n      + str(len(transfer_trips.person_id.unique()))\n      + ' use more than one transport mode on the same trip')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since there are only 422 individuals that use more than 1 transport mode type in the same trip, it is easier to just drop those rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"trips = trips.loc[~trips['person_id'].isin(transfer_trips.person_id)]\n\nprint('The trips dataframe contains information about '\n      + str(len(trips))\n      + ' trips performed by '\n      + str(len(trips.person_id.unique()))\n      + ' people')\n\ntrips.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it is possible to create the *Y* column for the MCM model, which we will call \"mode\""},{"metadata":{"trusted":true},"cell_type":"code","source":"# Slow cell\ndef create_mode_column(row):\n    if (row.includes_car == 1):\n        row.mode_type = 'car'\n    elif (row.includes_pt == 1):\n        row.mode_type = 'pt'\n    elif (row.includes_bicycle) == 1:\n        row.mode_type = 'bike'\n    elif (row.includes_other_modes) == 1:\n        row.mode_type = 'other_mode'\n    else:\n        row.mode_type = 'walk'\n    \n    return row\n\n# Create a new empty column in the trips dataframe\ntrips['mode_type'] = ''\n\n# Check which of the mode columns has the value of 1 and\n# get the name of that mode\ntrips = trips.apply(create_mode_column, axis='columns')\n\n# Drop the subtype columns\ntrips.drop(columns=['includes_walk',\n                    'includes_bicycle',\n                    'includes_other_modes',\n                    'includes_pt',\n                    'includes_car'], inplace=True)\ntrips.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to proper train the Decision Tree Classifier in the future, we have to set the value for the last destination for each person as 'none'."},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_last_actv_to_none(input_df):\n    '''Returns a dataframe in which the last activity performed\n    by each person is set as 'none'\n    \n    Keyword arguments:\n    input_df -- dataframe containing trip information\n    '''\n    df = input_df.copy()\n    \n    df['next_person'] = df.person_id.shift(-1)\n    df['activity_destination'].loc[(df['person_id'] != df['next_person'])] = 'none'\n    df.drop(columns=['next_person'], inplace=True)\n    return df\n\ntrips = set_last_actv_to_none(trips)\ntrips.head(13)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have to create *count* columns for activity types, as described in the DDAS framework proposition. Those columns indicate how many of each activity have been performed by the agent up to that point in the trip."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_actv_counts(input_df, input_count):\n    '''Returns a dataframe with activity counts columns, which\n    represent the number of times each activity has been performed\n    up to that point of the trip\n    \n    Keyword arguments:\n    input_df -- dataframe containing trip information\n    input_count -- OHE dataframe of the activities performed, obtained\n                   from the input_df\n    '''\n    df = input_df.copy()\n    count = input_count.copy()\n    \n    # Transform dataframes into lists of lists for speed performance\n    df_list = df.to_numpy()\n    count_list = count.to_numpy()\n    \n    # If the person_id of the current row is the same of the one on\n    # the previous row (if we're dealing with the same activity diary\n    # for a certain person) we must sum up the activities that have\n    # been performed up to now\n    \n    for i in range(1,len(df_list)):\n        if (df_list[i,0] == df_list[i-1,0]):\n            count_list[i] = count_list[i] + count_list[i-1]\n        else:\n            pass\n    \n    # From the list, we create an output dataframe\n    output_df = pd.DataFrame(count_list, columns=count.columns)\n    \n    # We set new indexes for both de the original dataframe and\n    # the counts dataframe, so they can be concatenated\n    df = df.reindex(range(0,len(df)))\n    output_df = output_df.reindex(range(0,len(output_df)))\n    \n    # The output is the concatenated df\n    output_df = pd.concat((df, output_df), axis=1)\n    return output_df\n\n# We initiate the activity counts dataframe as the 'one-hot encoded'\n# version of the 'activity_origin' column\nactv_count = pd.get_dummies(trips['activity_origin'])\n\ntrips = create_actv_counts(trips, actv_count)\ntrips.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have to rename the count columns to make it easier to understand"},{"metadata":{"trusted":true},"cell_type":"code","source":"trips.columns = ['person_id', 'trip_id', 'activity_origin',\n                 'activity_destination', 'mode_type', 'count_leisure',\n                 'count_other', 'count_school', 'count_shop',\n                 'count_sleep', 'count_work']\ntrips.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And we do the same for creating the mode counts"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_mode_counts(input_df, input_count):\n    '''Returns a dataframe with mode counts columns, which\n    represent the number of times each trasport mode has been\n    used up to that point of the trip\n    \n    Keyword arguments:\n    input_df -- dataframe containing trip information\n    input_count -- OHE dataframe of the mode choice, obtained\n                   from the input_df\n    '''\n    df = input_df.copy()\n    count = input_count.copy()\n    \n    # The 'count' columns have to be shifted by because we want the\n    # counts for the previous trips, not including the current one\n    count = count.shift(1)\n    \n    # Transform dataframes into lists of lists for speed performance\n    df_list = df.to_numpy()\n    count_list = count.to_numpy()\n    \n    # The first row of the count_list is initiated empty because the\n    # first person has not performed any trip yet\n    count_list[0] = [0,0,0,0,0]\n    \n    for i in range(1,len(df_list)):\n        if (df_list[i,0] != df_list[i-1,0]):\n            count_list[i] = [0,0,0,0,0]\n        else:\n            count_list[i] = count_list[i] + count_list[i-1]\n    \n    output_df = pd.DataFrame(count_list, columns=count.columns)\n    \n    df = df.reindex(range(0,len(df)))\n    output_df = output_df.reindex(range(0,len(output_df)))\n                           \n    output_df = pd.concat((df, output_df), axis=1)\n    return output_df\n\n# We initiate the activity counts dataframe as the 'one-hot encoded'\n# version of the 'mode_type' column\nmode_count = pd.get_dummies(trips['mode_type'])\ntrips = create_mode_counts(trips, mode_count)\ntrips.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, we have to rename the count columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"trips.columns = ['person_id', 'trip_id', 'activity_origin', 'activity_destination',\n                 'mode_type', 'count_leisure', 'count_other', 'count_school',\n                 'count_shop', 'count_sleep', 'count_work',\n                 'count_bike', 'count_car',\n                 'count_other_mode', 'count_pt', 'count_walk']\ntrips.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have to One-Hot Encode the columns activity_origin and activity_destination because they will be used as input fot the Decision Tree Classifier and all input to this model must be numerical."},{"metadata":{"trusted":true},"cell_type":"code","source":"trips = pd.concat([trips, pd.get_dummies(trips['activity_origin'], prefix='ohe_origin')], axis=1)\ntrips = pd.concat([trips, pd.get_dummies(trips['activity_destination'], prefix='ohe_destin')], axis=1)\ntrips.drop('activity_origin', axis=1, inplace=True)\ntrips.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.4 Creating the input dataframe, training and test sets\nNow we merge the *soc*, *reach* and *trips* dataframes"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_df = pd.merge(soc_and_reach, trips, how='inner', on='person_id')\ninput_df.drop(['household_id'], axis=1, inplace=True)\ninput_df.set_index('person_id', inplace=True)\n\nprint('The organized input dataframe contains information about '\n      + str(len(input_df.index.unique().values))\n      + ' individuals and ' \n      + str(len(input_df)) \n      + ' trips')\n\ninput_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's export this input table as csv"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_df.to_csv(\"input_df.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to create *train* and *test* sets, we will randomly select from the indexes the people who will compose each of these datasets. The proportion will be 80% for training and 20% for testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"index_list = input_df.index.unique().values\nrandom.Random(123).shuffle(index_list)\n\n# The element in which the test and training sets are separated \n# is the one on the first fifth of the list length\nseparation_element = len(index_list)//5\n\ntest_index = index_list[:separation_element]\ntrain_index = index_list[separation_element:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then, we create train and test dataframes using these index lists"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Because of the random selection, the trips were out of order,\n# so we have to sort them again\ntest_df = input_df.loc[test_index]\ntest_df.sort_values(by=['trip_id'], inplace=True)\n\ntrain_df = input_df.loc[train_index]\ntrain_df.sort_values(by=['trip_id'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We then define which columns are part of the ATM training set and the MCM training set"},{"metadata":{"trusted":true},"cell_type":"code","source":"ATM_X_columns = ['age_group',\n                 'is_female',\n                 'is_student',\n                 'education_level',\n                 'has_driver_license',\n                 'people_in_household', \n                 'is_car_available',\n                 'reach_bike',\n                 'reach_car',\n                 'reach_transit',\n                 'reach_walk', \n                 'count_leisure', \n                 'count_other',\n                 'count_school', \n                 'count_shop',\n                 'count_sleep',\n                 'count_work',\n                 'ohe_origin_leisure',\n                 'ohe_origin_other', \n                 'ohe_origin_school', \n                 'ohe_origin_shop',\n                 'ohe_origin_sleep', \n                 'ohe_origin_work']\n\nATM_Y_columns = ['activity_destination']\n\nX_train_ATM = train_df[ATM_X_columns]\nY_train_ATM = train_df[ATM_Y_columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MCM_X_columns = ['age_group',\n                 'is_female',\n                 'is_student',\n                 'education_level',\n                 'has_driver_license',\n                 'people_in_household', \n                 'is_car_available',\n                 'reach_bike',\n                 'reach_car',\n                 'reach_transit',\n                 'reach_walk', \n                 'count_leisure', \n                 'count_other',\n                 'count_school', \n                 'count_shop',\n                 'count_sleep',\n                 'count_work',\n                 'ohe_origin_leisure',\n                 'ohe_origin_other', \n                 'ohe_origin_school', \n                 'ohe_origin_shop',\n                 'ohe_origin_sleep', \n                 'ohe_origin_work',\n                 'ohe_destin_leisure',\n                 'ohe_destin_none', \n                 'ohe_destin_other', \n                 'ohe_destin_school',\n                 'ohe_destin_shop',\n                 'ohe_destin_sleep',\n                 'ohe_destin_work',\n                 'count_bike',\n                 'count_car',\n                 'count_other_mode',\n                 'count_pt', \n                 'count_walk']\n\nMCM_Y_columns = ['mode_type']\n\nX_train_MCM = train_df[MCM_X_columns]\nY_train_MCM = train_df[MCM_Y_columns]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Model 1: Standard DDAS\n\n## 3.1 Model 1: Specification\n\nOur first implementation is similar to the original DDAS proposition, which defines both ATM and MCM as Decision Tree classifiers. In order to select the optmal maximum tree depth, we will perform cross-validation on the training set. The following code is based on the algorithms presented in https://towardsdatascience.com/how-to-find-decision-tree-depth-via-cross-validation-2bf143f0f3d6"},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_cross_validation_on_trees(X, y, tree_depths, cv=5, scoring='f1_micro'):\n    \"\"\" A function to perform the cross-validation procedure\n    for selecting the best specification of the model\n    \n    Keyword arguments:\n    X -- features used as input\n    y -- features that has to be predicted\n    tree_depths -- list of integers to be used as tree depths\n                   in cross-validation\n    cv -- number of folds (default = 5)\n    scoring -- scoring function to be used as evaluation metric\n    \"\"\"\n    \n    cv_scores_list = []\n    cv_scores_std = []\n    cv_scores_mean = []\n    accuracy_scores = []\n    \n    for depth in tree_depths:\n        tree_model = DecisionTreeClassifier(max_depth=depth, random_state=123)\n        cv_scores = cross_val_score(tree_model, X, y, cv=cv, scoring=scoring)\n        cv_scores_list.append(cv_scores)\n        cv_scores_mean.append(cv_scores.mean())\n        cv_scores_std.append(cv_scores.std())\n        accuracy_scores.append(tree_model.fit(X, y).score(X, y))\n        \n    cv_scores_mean = np.array(cv_scores_mean)\n    cv_scores_std = np.array(cv_scores_std)\n    accuracy_scores = np.array(accuracy_scores)\n    \n    return cv_scores_mean, cv_scores_std, accuracy_scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also define a function to visualize those results"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_cross_validation_on_trees(depths, cv_scores_mean, cv_scores_std, accuracy_scores, title, score):\n    \n    \"\"\"Returns a plot with the results from cross validation \n    \"\"\"\n    \n    fig, ax = plt.subplots(1,1, figsize=(12,6))\n    ax.plot(depths, cv_scores_mean, '-o', label='mean cross-validation score', alpha=0.9)\n    ax.fill_between(depths, cv_scores_mean-2*cv_scores_std, cv_scores_mean+2*cv_scores_std, alpha=0.2)\n    ylim = plt.ylim()\n    ax.plot(depths, accuracy_scores, '-*', label='train score', alpha=0.9)\n    ax.set_title(title, fontsize=16)\n    ax.set_xlabel('Tree depth', fontsize=14)\n    ax.set_ylabel(score, fontsize=14)\n    ax.set_ylim(ylim)\n    ax.set_xticks(depths)\n    ax.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We first run these codes for the ATM model"},{"metadata":{"trusted":true},"cell_type":"code","source":"depths_list = range(1,51)\n\ninit_time = time.time()\nATM_dt_cv_scores_mean, ATM_dt_cv_scores_std, ATM_dt_accuracy_scores = run_cross_validation_on_trees(X_train_ATM,\n                                                                                                    Y_train_ATM,\n                                                                                                    depths_list)\nend_time = time.time()\nellapsed_time = end_time - init_time\n\nprint('Cross-validation took ' + str(ellapsed_time) + ' seconds to run')\n\n#plotting accuracy\nplot_cross_validation_on_trees(depths_list, \n                               ATM_dt_cv_scores_mean, \n                               ATM_dt_cv_scores_std, \n                               ATM_dt_accuracy_scores, \n                               'F1 score per decision tree depth on ATM cross-validation data',\n                               'f1-score')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_f1 = np.amax(np.around(ATM_dt_cv_scores_mean,3))\n\noptimal_depth_ATM = np.argmax(np.around(ATM_dt_cv_scores_mean,3)) + 1\n\nprint('The maximum F1 score for the ATM model is '\n      + str(max_f1)\n      + ' for tree depth '\n      + str(optimal_depth_ATM))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the confusion matrix for the model with optimal tree depth"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_cv_ATM, X_val_ATM, Y_train_cv_ATM, Y_val_ATM = train_test_split(X_train_ATM,\n                                                                        Y_train_ATM,\n                                                                        test_size = 0.20,\n                                                                        random_state=123)\n\nATM_dt_cv_model = DecisionTreeClassifier(max_depth=optimal_depth_ATM, random_state=123)\nATM_dt_cv_model.fit(X_train_cv_ATM, Y_train_cv_ATM)\n\nY_pred_val_ATM = ATM_dt_cv_model.predict(X_val_ATM)\n\nclass_names = ['leisure', 'none', 'other', 'school', 'shop', 'sleep', 'work']\n\ndisp = plot_confusion_matrix(ATM_dt_cv_model, X_val_ATM, Y_val_ATM,\n                             display_labels=class_names,\n                             cmap=plt.cm.Blues,\n                             normalize=None)\ndisp.ax_.set_title('ATM confusion matrix')\nplt.show()\n\nprint(classification_report(Y_val_ATM, Y_pred_val_ATM, digits=3, labels=class_names))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We do the same for the MCM module"},{"metadata":{"trusted":true},"cell_type":"code","source":"init_time = time.time()\nMCM_dt_cv_scores_mean, MCM_dt_cv_scores_std, MCM_dt_accuracy_scores = run_cross_validation_on_trees(X_train_MCM,\n                                                                                                    Y_train_MCM,\n                                                                                                    depths_list)\nend_time = time.time()\nellapsed_time = end_time - init_time\n\nprint('Cross-validation took ' + str(ellapsed_time) + ' seconds to run')\n\n# plotting accuracy\nplot_cross_validation_on_trees(depths_list, \n                               MCM_dt_cv_scores_mean, \n                               MCM_dt_cv_scores_std, \n                               MCM_dt_accuracy_scores, \n                               'F1 score per decision tree depth on MCM cross-validation data',\n                               'f1-score')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_f1 = np.amax(np.around(MCM_dt_cv_scores_mean,3))\n\noptimal_depth_MCM = np.argmax(np.around(MCM_dt_cv_scores_mean,3)) + 1\n\nprint('The maximum F1 score for the MCM model is '\n      + str(max_f1)\n      + ' for tree depth '\n      + str(optimal_depth_MCM))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_cv_MCM, X_val_MCM, Y_train_cv_MCM, Y_val_MCM = train_test_split(X_train_MCM,\n                                                                        Y_train_MCM,\n                                                                        test_size = 0.20,\n                                                                        random_state=123)\n\nMCM_dt_cv_model = DecisionTreeClassifier(max_depth=optimal_depth_MCM, random_state=123)\nMCM_dt_cv_model.fit(X_train_cv_MCM, Y_train_cv_MCM)\n\nY_pred_val_MCM = MCM_dt_cv_model.predict(X_val_MCM)\n\nclass_names = ['bike', 'car','other_mode', 'pt', 'walk']\n\ndisp = plot_confusion_matrix(MCM_dt_cv_model, X_val_MCM, Y_val_MCM,\n                             display_labels=class_names,\n                             cmap=plt.cm.Blues,\n                             normalize=None)\ndisp.ax_.set_title('MCM confusion matrix, without normalization')\nplt.show()\n\nprint(classification_report(Y_val_MCM, Y_pred_val_MCM, digits=3, labels=class_names))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we train both the ATM and the MCM modules using the optimal configuration we found and the whole training set"},{"metadata":{"trusted":true},"cell_type":"code","source":"init_time = time.time()\n\nATM_dt_model = DecisionTreeClassifier(max_depth=optimal_depth_ATM, random_state=123)\nATM_dt_model.fit(X_train_ATM, Y_train_ATM)\n\nMCM_dt_model = DecisionTreeClassifier(max_depth=optimal_depth_MCM, random_state=123)\nMCM_dt_model.fit(X_train_MCM, Y_train_MCM)\n\nend_time = time.time()\nellapsed_time = end_time - init_time\n\nprint('Model training for both the ATM and MCM took ' + str(ellapsed_time) + ' seconds to run')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2 Model 1: Inference\n### 3.2.1 Organizing *X_test*"},{"metadata":{},"cell_type":"markdown","source":"Our current *test_df* table contains a lot of information that we won't need during the inference process. This is because the dataframe already contains all the 'answers', the complete activity diaries for each person. Our goal is to use the DDAS framework to predict those diaries. Therefore, we have to create a X_test dataframe containing only the required information for each person, so the model can predict their diaries"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_columns = ['age_group',\n                  'is_female',\n                  'is_student',\n                  'education_level',\n                  'has_driver_license',\n                  'people_in_household', \n                  'is_car_available',\n                  'reach_bike',\n                  'reach_car',\n                  'reach_transit',\n                  'reach_walk']\n\nX_test = test_df[X_test_columns].copy()\n\n# We have to drop duplicate columns because the same person\n# appears on multiple rows, as the original dataset contained\n# all trips that person performed\nX_test.reset_index(inplace=True)\nX_test.drop_duplicates(inplace=True)\nX_test.set_index('person_id', inplace=True)\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2.2 Building the inference framework\nNow we will define a *Person* object, which will be useful during the inference phase. Each person has individual attributes such as *soc* features, activity and mode counts, current and next activity, etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Person:\n    def __init__(self, soc_features):\n        self.id = soc_features.index # Series type\n        self.soc_features = soc_features\n        \n        self.origin_act_type = 'sleep' # String\n        self.origin_act_type_ohe = pd.DataFrame(data={'ohe_origin_leisure':[0],\n                                                      'ohe_origin_other':[0],\n                                                      'ohe_origin_school':[0],\n                                                      'ohe_origin_shop':[0],\n                                                      'ohe_origin_sleep':[1],\n                                                      'ohe_origin_work':[0]},\n                                                index=self.id)\n\n        self.destination_act_type = '' # String\n        self.destination_act_type_ohe = pd.DataFrame(data={'ohe_destin_leisure':[0],\n                                                           'ohe_destin_other':[0],\n                                                           'ohe_destin_none':[0],\n                                                           'ohe_destin_school':[0],\n                                                           'ohe_destin_shop':[0],\n                                                           'ohe_destin_sleep':[0],\n                                                           'ohe_destin_work':[0]},\n                                                     index=self.id)\n        \n        self.act_type_counts = pd.DataFrame(data={'count_leisure':[0],\n                                                  'count_other':[0],\n                                                  'count_school':[0],\n                                                  'count_shop':[0],\n                                                  'count_sleep':[1],\n                                                  'count_work':[0]},\n                                            index=self.id)\n        \n        self.mode_counts = pd.DataFrame(data={'count_bike':[0],\n                                              'count_car':[0],\n                                              'count_other_mode':[0],\n                                              'count_pt':[0],\n                                              'count_walk':[0]},\n                                       index=self.id)\n        \n        self.cur_mode = '' # String\n        \n        self.total_trips = 0\n    \n    def update_act_counts(self):\n        if self.destination_act_type == 'none':\n            pass\n\n        else:\n            self.act_type_counts.loc[:,'count_'+self.destination_act_type] = self.act_type_counts.loc[:,'count_'+self.destination_act_type] + 1\n    \n    def update_destination_act_type_ohe(self):\n        self.destination_act_type_ohe = pd.DataFrame(data={'ohe_destin_leisure':[0],\n                                                           'ohe_destin_none':[0],\n                                                           'ohe_destin_other':[0],\n                                                           'ohe_destin_school':[0],\n                                                           'ohe_destin_shop':[0],\n                                                           'ohe_destin_sleep':[0],\n                                                           'ohe_destin_work':[0]},\n                                                    index=self.id)\n        self.destination_act_type_ohe.loc[:,'ohe_destin_' + self.destination_act_type] = self.destination_act_type_ohe.loc[:,'ohe_destin_' + self.destination_act_type] + 1\n    \n    def update_mode_counts(self):\n        self.mode_counts.loc[:,'count_'+self.cur_mode] = self.mode_counts.loc[:,'count_'+self.cur_mode] + 1\n\n    def update_origin(self):\n        self.origin_act_type_ohe = pd.DataFrame(data={'ohe_origin_leisure':[0],\n                                                      'ohe_origin_other':[0],\n                                                      'ohe_origin_school':[0],\n                                                      'ohe_origin_shop':[0],\n                                                      'ohe_origin_sleep':[0],\n                                                      'ohe_origin_work':[0]},\n                                               index=self.id)\n        self.origin_act_type_ohe.loc[:,'ohe_origin_' + self.destination_act_type] = 1\n        self.origin_act_type = self.destination_act_type","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next function summarizes the DDAS framework. We use the trained ATM and MCM models to make inferences about the activity diaries for each person on a *X_test* dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"def ddas_framework(X_test, ATM_model, MCM_model):\n    # We start by initiating an empty df for the results\n    \n    results = pd.DataFrame(columns=['person_id',\n                                    'destination',\n                                    'mode'])\n\n    results = results.set_index('person_id')\n\n    for i in range (0, len(X_test)):\n    \n        # For each row of the X_test dataframe we define a\n        # new Person object\n        person = Person(X_test[i:i+1].copy())\n    \n        # Here we set a hard-coded limit so no agent can have\n        # more than 12 trips in their activity diary\n        while(person.destination_act_type != 'none'):\n            if person.total_trips == 12:\n                person.destination_act_type = 'none'\n\n            # If the person has not conducted 12 trips yet, let's\n            # predict the next trip\n            else:\n                ATM_input = pd.concat([person.soc_features,\n                                       person.act_type_counts,\n                                       person.origin_act_type_ohe],\n                                      axis = 1)\n\n                person.destination_act_type = ATM_model.predict(ATM_input)[0]\n\n            person.update_destination_act_type_ohe()\n\n            MCM_input = pd.concat([ATM_input,\n                                   person.destination_act_type_ohe,\n                                   person.mode_counts],\n                                  axis = 1)\n\n            person.cur_mode = MCM_model.predict(MCM_input)[0]\n\n            result_row = pd.DataFrame(data={'destination':[person.destination_act_type],\n                                            'mode':[person.cur_mode]},\n                                     index=person.id)\n\n            results = results.append(result_row)\n\n            person.update_act_counts()\n            person.update_mode_counts()\n            person.update_origin()\n            person.total_trips = person.total_trips + 1\n\n    return results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2.3 Using the trained models to make predictions\n\nNow we will run the DDAS framework using the three machine learning models that we have trained to make predictions on the *X_test* dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"init_time = time.time()\n\nresults_dt = ddas_framework(X_test, ATM_dt_model, MCM_dt_model)\n\nend_time = time.time()\nellapsed_time = end_time - init_time\n\nprint('The DDAS framework took ' + str(ellapsed_time) + ' seconds to run')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.3 Model 1: Evaluation\n## 3.3.1 ATM Model\n### a) Expected and observed distribution of trips\n\nThe first thing we will analyse on the results obtained from the DDAS framework is the distribution of trips"},{"metadata":{"trusted":true},"cell_type":"code","source":"observed_list = round(results_dt.destination.value_counts()*100/len(results_dt), 6)\nobserved_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"expected_list = round(test_df[['activity_destination','mode_type']].activity_destination.value_counts()*100/len(test_df),6)\nexpected_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_expected_and_observed(input_expected, input_observed, x_label, y_label, legend, color, dist=0):\n    '''Returns plots of the given lists\n    \n    Keyword arguments:\n    input_expected -- list of the expected values of each category\n    input_observed -- list of the observed values of each category\n    '''\n    \n    fig, axs = plt.subplots(figsize=(10, 5))\n        \n    chart_title = ''\n    df = pd.concat([input_expected, input_observed], axis=1)\n        \n    N = len(df)\n    ind = np.arange(N)  # the x locations for the groups\n    width = 0.25\n\n    x = df.index\n\n    y_expected = df.iloc[:,0]\n    rects_expected = axs.bar(ind, y_expected, width, color='lightgreen')\n\n    y_observed = df.iloc[:,1]\n    rects_observed = axs.bar(ind+width, y_observed, width, color=color)\n\n    axs.set_title(chart_title)\n    axs.set_ylabel(y_label, fontsize=12)\n    axs.set_xlabel(x_label, fontsize=12)\n    axs.set_xticks(ind+width)\n    axs.set_xticklabels(x)\n    axs.legend((rects_expected[0], rects_observed[0]), ('expected', legend), loc=1)\n    \n    def autolabel(rects, offset=0):\n        for rect in rects:\n            try:\n                h = rect.get_height()\n                if (h >= 0.1):\n                    axs.text(rect.get_x()+rect.get_width()/2 + offset, 1.00*h, '%g'%round(h,1),\n                             ha='center', va='bottom')\n                else:\n                    axs.text(rect.get_x()+rect.get_width()/2 + offset, 1.00*h, '%g'%round(h,3),\n                             ha='center', va='bottom')\n            except:\n                pass\n\n    autolabel(rects_expected, -dist)\n    autolabel(rects_observed, +dist)\n        \nplot_expected_and_observed(expected_list, observed_list, 'activity types', 'proportion (%)', 'Model 1', 'lightblue', dist=0.05)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### b) Expected and observed frequency of chains"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_actv_chains(input_df):\n    '''Returns a list with the activity chains performed by each person\n    (e.g. H-W-N refers to chain home-work-none)\n    \n    Keyword arguments:\n    input_df -- dataframe with trip information\n    '''\n    df = input_df.reset_index()\n    \n    chains_list = dict.fromkeys(df.iloc[:,0].unique())\n    df_list = df.to_numpy()\n    \n    try:\n        # Read each row of the input dataframe\n        for i in range(0,len(df_list)):\n            # If the person id is different from the previous\n            if (df_list[i,0] != df_list[i-1,0]):\n                #The chain of activitis for the person is initiated at home\n                chains_list[df_list[i,0]] = 'home-'+df_list[i,1]\n            # If is the same person of the previous row computed\n            else:\n                # The chain is updated with a new activity\n                chains_list[df_list[i,0]] = chains_list[df_list[i,0]]+'-'+df_list[i,1]\n    # Exception to deal with the first row\n    except:\n        chains_list[df_list[i,0]] = 'home-'+df_list[i,1]\n    \n    return chains_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chains_test = pd.Series(get_actv_chains(test_df[['activity_destination','mode_type']]))\nchains_test.value_counts().head(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(chains_test.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chains_dt = pd.Series(get_actv_chains(results_dt))\nchains_dt.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(chains_dt.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_actv_chain_legnths(input_df):\n    '''Returns a list the length of chain of activities\n    performed by each person\n    (e.g.: home-work-home is a chain of size = 2\n    because there are two trips: home-work and\n    work-home)\n    \n    Keyword arguments:\n    input_df -- dataframe with trip information\n    '''\n    df = input_df.reset_index()\n    \n    len_list = dict.fromkeys(df.iloc[:,0].unique())\n    df_list = df.to_numpy()\n    \n    try:\n        # Read each row of the input dataframe\n        for i in range(0,len(df_list)):\n            # If the person id is different from the previous\n            if (df_list[i,0] != df_list[i-1,0]):\n                #The length of the person's chain is initiated at one\n                len_list[df_list[i,0]] = 1\n                \n            # If it is the same person of the previous row computed\n            else:\n                # The chain is updated with a new activity\n                len_list[df_list[i,0]] = len_list[df_list[i,0]] + 1\n    # Exception to deal with the first row\n    except:\n        len_list[df_list[i,0]] = 1\n    \n    return len_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len_chains_test = pd.Series(get_actv_chain_legnths(test_df[['activity_destination','mode_type']]))\nlen_chains_test.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len_chains_dt = pd.Series(get_actv_chain_legnths(results_dt))\nlen_chains_dt.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_expected_and_observed(len_chains_test.value_counts(), len_chains_dt.value_counts(), 'chain length', 'counts', 'Model 1', 'lightblue', dist=0.075)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### c) Importance of features"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances = permutation_importance(ATM_dt_model,\n                                             X_train_ATM,\n                                             Y_train_ATM,\n                                             scoring='f1_micro',\n                                             n_repeats=5,\n                                             random_state=123)\n\ncol1 = feature_importances.importances_mean\ncol2 = ATM_X_columns\n\ndf = pd.DataFrame(col1, index=col2, columns=['importance'])\ndf.sort_values(by='importance', ascending=False, inplace=True)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### d) VALFRAM activity count validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def actv_count_validation(input_df):\n    '''Returns a dictionary with activity types as keys and\n    frequency counts of those activities as values\n    \n    Keyword arguments:\n    input_df -- dataframe containing trip information\n    '''\n    # Gets dataframe as input, turns index into column and renames columns\n    df = input_df.copy()\n    df.reset_index(inplace=True)\n    df.columns = ['person_id','destination','mode']\n    \n    # Creates a dictionary with the keys being the number of times that the\n    # activity is performed\n    count_dict = {}\n    activity_list = ['other', 'sleep', 'none', 'school', 'work', 'shop', 'leisure']\n    for activity_type in activity_list:\n        count_dict[activity_type] = {1:0,2:0,3:0,4:0,\n                                     5:0,6:0,7:0,8:0,9:0,\n                                     10:0,11:0,12:0}\n    \n    # Creates a new dataframe from the count of how many times each person performs\n    # each type of activity. Renames the columns of this new dataframe\n    x = pd.DataFrame(df.groupby(['person_id', 'destination'])['destination'].count())\n    x.rename(columns={\"destination\": \"counts\"}, inplace=True)\n    \n    # From the previous dataframe, creates a new count of how many people perform\n    # a certain number of times each activity. For instance, how many people perform\n    # only 1 time the 'work' activity, etc\n    y = x.groupby(['destination', 'counts'])['counts'].count()\n    \n    # Puts those results into the dictionary\n    for activity in df.destination.unique():\n        for key in y.loc[activity].index:\n            count_dict[activity][key] = y.loc[activity][key]\n        residual = len(df.person_id.unique()) - sum(count_dict[activity].values())\n        count_dict[activity].update({0: residual})\n    \n    return count_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"activ_counts_dt = actv_count_validation(results_dt)\nactiv_counts_test = actv_count_validation(test_df[['activity_destination','mode_type']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have also to write a function to compute chi-square values from the dictionaries that were created."},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_chi_square_from_values(input_expected, input_observed):\n    '''Returns a list containing partial chi-square values for each\n    parameter being computed (activities or models) and a total\n    chi-square value being the sum of those partials.\n    \n    Keywork arguments:\n    input_expected -- dict with expected counts for the parameter\n    input_observed -- dict with observed counts for the parameter\n    '''\n    total_chi_square = 0\n    chi_square_list = []\n    \n    for key in input_expected:\n       \n        expected = pd.Series(input_expected[key], name='expected')\n        expected.sort_index(inplace=True)\n        \n        observed = pd.Series(input_observed[key], name='observed')\n        observed.sort_index(inplace=True)\n        \n        chi_square = ((expected-observed-1).pow(2))/expected\n        chi_square[np.isnan(chi_square)] = 0\n        \n        total_chi_square = total_chi_square + sum(chi_square.replace(np.inf,0))\n        \n        chi_square.rename('chi_square_' + key)\n        chi_square_list.append([key, expected, observed, sum(chi_square.replace(np.inf,0))])\n    \n    return [total_chi_square, chi_square_list]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chi_square_activity_counts_dt = compute_chi_square_from_values(activ_counts_test, activ_counts_dt)[0]\nprint('The total chi-square value for activity counts validation in the decision tree model is '\n      + str(chi_square_activity_counts_dt))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We know the total chi-square value, but let's see how the partial totals for each activity type are presented (the first instance of each list is the activity type):"},{"metadata":{"trusted":true},"cell_type":"code","source":"chi_square_activ_subtotals = compute_chi_square_from_values(activ_counts_test, activ_counts_dt)[1]\nchi_square_activ_subtotals[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.3.2 MCM Model\nNext on the VALFRAM method, we have the mode choice validation, in which we analyze, for each destination activity type, what is the proportion of trips performed by each transportation mode. For instance, for each 100 trips performed with the purpose of 'leisure', how many of those were conducted by car? And by transit? It is important to compute those values in terms of proportion (for each 100), because we can get comparable chi-square values for the test and validation sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"def mode_count_validation(input_df):\n    '''Returns a dictionary with transportation modes as keys and\n    frequency counts of those modes as values\n    \n    Keyword arguments:\n    input_df -- dataframe containing trip information\n    '''\n    df = input_df.copy()\n    df.reset_index(inplace=True)\n    df.columns = ['person_id','destination','mode']\n    \n    count_dict = {}\n    activity_list = ['other', 'sleep', 'none', 'school', 'work', 'shop', 'leisure']\n\n    for activity_type in activity_list:\n        count_dict[activity_type] = {'bike':0,\n                                     'car':0,\n                                     'other_mode':0,\n                                     'pt':0,\n                                     'walk':0}\n    \n    x = pd.DataFrame(df.groupby(['destination', 'mode'])['mode'].count())\n    \n    for activity in df.destination.unique():\n        for transp_mode in x.loc[activity].index:\n            count_dict[activity][transp_mode] = x.loc[activity].loc[transp_mode][0]\n    \n    return count_dict\n\nmode_counts_dt = mode_count_validation(results_dt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mode_counts_dt = mode_count_validation(results_dt)\nmode_counts_test = mode_count_validation(test_df[['activity_destination','mode_type']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We must define another function to compute chi_squared value from proportions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_chi_square_from_proportions(input_expected, input_observed):\n    '''Returns a list containing partial chi-square values for each\n    parameter being computed (activities or models) and a total\n    chi-square value being the sum of those partials.\n    \n    Keywork arguments:\n    input_expected -- dict with expected counts for the parameter\n    input_observed -- dict with observed counts for the parameter\n    '''\n    total_chi_square = 0\n    chi_square_list = []\n    \n    for key in input_expected:\n        \n        total_expected = sum(input_expected[key].values())\n        expected_proportions = {k:v/total_expected for (k,v) in input_expected[key].items()} \n        \n        total_observed = sum(input_observed[key].values())\n        expected_values = {k:round(v*total_observed, 0) for (k,v) in expected_proportions.items()}\n        \n        expected = pd.Series(expected_values, name='expected')\n        expected.sort_index(inplace=True)\n        \n        observed = pd.Series(input_observed[key], name='observed')\n        observed.sort_index(inplace=True)\n        \n        chi_square = ((expected-observed-1).pow(2))/expected\n        chi_square[np.isnan(chi_square)] = 0\n        \n        total_chi_square = total_chi_square + sum(chi_square.replace(np.inf,0))\n        \n        chi_square.rename('chi_square_' + key)\n        chi_square_list.append([key, expected, observed, sum(chi_square.replace(np.inf,0))])\n    \n    return [total_chi_square, chi_square_list]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chi_square_mode_counts = compute_chi_square_from_proportions(mode_counts_test, mode_counts_dt)[0]\n\nprint('The total chi-square value for mode counts validation in this set of results is '\n      + str(chi_square_mode_counts))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, let's check how partial values of chi-square are presented"},{"metadata":{"trusted":true},"cell_type":"code","source":"chi_square_mode_subtotals = compute_chi_square_from_proportions(mode_counts_test, mode_counts_dt)[1]\nchi_square_mode_subtotals[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_chi_square_results(input_list, label, color):\n    '''Returns plots of the given lists\n    \n    Keyword arguments:\n    input_list -- list to be plotted\n    '''\n    \n    fig, axs = plt.subplots(4,2,figsize=(20, 30))\n    \n    for i in range(0,len(input_list)):\n        \n        line = (i//2)\n        col = (i%2)-1\n        \n        chart_title = input_list[i][0]\n        chi_2_value = int(input_list[i][3])\n        df = pd.concat([input_list[i][1],input_list[i][2]], axis=1)\n        df.drop(df[df.expected == 0].index, inplace=True)\n        \n        N = len(df)\n        ind = np.arange(N)  # the x locations for the groups\n        width = 0.25\n\n        x = df.index\n\n        y_expected = df['expected']\n        rects_expected = axs[line,col].bar(ind, y_expected, width, color='lightgreen')\n\n        y_observed = df['observed']\n        rects_observed = axs[line,col].bar(ind+width, y_observed, width, color=color)\n\n        axs[line,col].set_title(chart_title+', chi square='+str(chi_2_value), fontsize=20)\n        axs[line,col].set_ylabel(chart_title+' frequency', fontsize=16)\n        axs[line,col].set_xlabel('counts', fontsize=16)\n        axs[line,col].set_xticks(ind+width)\n        axs[line,col].set_xticklabels(x, fontsize=14)\n        axs[line,col].legend((rects_expected, rects_observed), ('expected', label), fontsize=16, loc=1)\n\n        def autolabel(rects):\n            for rect in rects:\n                h = rect.get_height()\n                if (h>0):\n                    axs[line,col].text(rect.get_x()+rect.get_width()/2., 1*h, '%d'%int(h),\n                                       ha='center', va='bottom', fontsize=14)\n                else:\n                    pass\n\n        autolabel(rects_expected)\n        autolabel(rects_observed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_chi_square_results(chi_square_activ_subtotals, 'Model 1', 'lightblue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_chi_square_results(chi_square_mode_subtotals, 'Model 1', 'lightblue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"end_notebook = time.time()\ntotal_duration = end_notebook - init_notebook\nprint('The total duration of this notebook is '+ str(total_duration))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Model 2: improving the Decision Tree Classifier\n\n## 4.1 Testing a different score function"},{"metadata":{"trusted":true},"cell_type":"code","source":"init_time = time.time()\nATM_dt_cv_scores_mean, ATM_dt_cv_scores_std, ATM_dt_accuracy_scores = run_cross_validation_on_trees(X_train_ATM,\n                                                                                                    Y_train_ATM,\n                                                                                                    depths_list,\n                                                                                                    scoring='balanced_accuracy')\nend_time = time.time()\nellapsed_time = end_time - init_time\n\nprint('Cross-validation took ' + str(ellapsed_time) + ' seconds to run')\n\n# plotting accuracy\nplot_cross_validation_on_trees(depths_list, \n                               ATM_dt_cv_scores_mean, \n                               ATM_dt_cv_scores_std, \n                               ATM_dt_accuracy_scores, \n                               'Balanced accuracy score per decision tree depth on ATM cross-validation data',\n                               'balanced accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_balanced_accuracy = np.amax(np.around(ATM_dt_cv_scores_mean,3))\n\noptimal_depth_ATM = np.argmax(np.around(ATM_dt_cv_scores_mean,3)) + 1\n\nprint('The maximum balanced_accuracy score for the ATM model is '\n      + str(max_balanced_accuracy)\n      + ' for tree depth '\n      + str(optimal_depth_ATM))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_cv_ATM, X_val_ATM, Y_train_cv_ATM, Y_val_ATM = train_test_split(X_train_ATM,\n                                                                        Y_train_ATM,\n                                                                        test_size = 0.20,\n                                                                        random_state=123)\n\nATM_dt_cv_model = DecisionTreeClassifier(max_depth=optimal_depth_ATM, random_state=123)\nATM_dt_cv_model.fit(X_train_cv_ATM, Y_train_cv_ATM)\n\nY_pred_val_ATM = ATM_dt_cv_model.predict(X_val_ATM)\n\nclass_names = ['leisure', 'none', 'other', 'school', 'shop', 'sleep', 'work']\n\ndisp = plot_confusion_matrix(ATM_dt_cv_model, X_val_ATM, Y_val_ATM,\n                             display_labels=class_names,\n                             cmap=plt.cm.Blues,\n                             normalize=None)\ndisp.ax_.set_title('ATM confusion matrix')\nplt.show()\n\nprint(classification_report(Y_val_ATM, Y_pred_val_ATM, digits=3, labels=class_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"init_time = time.time()\nMCM_dt_cv_scores_mean, MCM_dt_cv_scores_std, MCM_dt_accuracy_scores = run_cross_validation_on_trees(X_train_MCM,\n                                                                                                    Y_train_MCM,\n                                                                                                    depths_list,\n                                                                                                    scoring='balanced_accuracy')\nend_time = time.time()\nellapsed_time = end_time - init_time\n\nprint('Cross-validation took ' + str(ellapsed_time) + ' seconds to run')\n\n# plotting accuracy\nplot_cross_validation_on_trees(depths_list, \n                               MCM_dt_cv_scores_mean, \n                               MCM_dt_cv_scores_std, \n                               MCM_dt_accuracy_scores, \n                               'Balanced accuracy per decision tree depth on MCM cross-validation data',\n                               'balanced accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_balanced_accuracy = np.amax(np.around(MCM_dt_cv_scores_mean,3))\n\noptimal_depth_MCM = np.argmax(np.around(ATM_dt_cv_scores_mean,3)) + 1\n\nprint('The maximum balanced accuracy score for the MCM model is '\n      + str(max_balanced_accuracy)\n      + ' for tree depth '\n      + str(optimal_depth_MCM))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2 Model 2: Training with the SMOTE technique"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_cv_ATM, X_val_ATM, Y_train_cv_ATM, Y_val_ATM = train_test_split(X_train_ATM,\n                                                                        Y_train_ATM,\n                                                                        test_size = 0.20,\n                                                                        random_state=123)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sm = SMOTE(random_state=123)\nX_train_cv_smote_ATM, Y_train_cv_smote_ATM = sm.fit_resample(X_train_cv_ATM, Y_train_cv_ATM)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"init_time = time.time()\nATM_dt_cv_scores_mean, ATM_dt_cv_scores_std, ATM_dt_accuracy_scores = run_cross_validation_on_trees(X_train_cv_smote_ATM,\n                                                                                                    Y_train_cv_smote_ATM,\n                                                                                                    depths_list,\n                                                                                                    scoring='balanced_accuracy')\nend_time = time.time()\nellapsed_time = end_time - init_time\n\nprint('Cross-validation took ' + str(ellapsed_time) + ' seconds to run')\n\n# plotting accuracy\nplot_cross_validation_on_trees(depths_list, \n                               ATM_dt_cv_scores_mean, \n                               ATM_dt_cv_scores_std, \n                               ATM_dt_accuracy_scores, \n                               'Balanced accuracy per decision tree depth on ATM cross-validation data',\n                               'balanced-accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_balanced_accuracy = np.amax(np.around(ATM_dt_cv_scores_mean,3))\n\noptimal_depth_ATM = np.argmax(np.around(ATM_dt_cv_scores_mean,3)) + 1\n\nprint('The maximum balanced_accuracy score for the ATM model is '\n      + str(max_balanced_accuracy)\n      + ' for tree depth '\n      + str(optimal_depth_ATM))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ATM_dt_cv_model = DecisionTreeClassifier(max_depth=optimal_depth_ATM, random_state=123)\nATM_dt_cv_model.fit(X_train_cv_smote_ATM,\n                    Y_train_cv_smote_ATM)\n\nY_pred_val_ATM = ATM_dt_cv_model.predict(X_val_ATM)\n\nclass_names = ['leisure', 'none', 'other', 'school', 'shop', 'sleep', 'work']\n\ndisp = plot_confusion_matrix(ATM_dt_cv_model, X_val_ATM, Y_val_ATM,\n                             display_labels=class_names,\n                             cmap=plt.cm.Blues,\n                             normalize=None)\ndisp.ax_.set_title('ATM confusion matrix')\nplt.show()\n\nprint(classification_report(Y_val_ATM, Y_pred_val_ATM, digits=3, labels=class_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_cv_MCM, X_val_MCM, Y_train_cv_MCM, Y_val_MCM = train_test_split(X_train_MCM,\n                                                                        Y_train_MCM,\n                                                                        test_size = 0.20,\n                                                                        random_state=123)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_cv_smote_MCM, Y_train_cv_smote_MCM = sm.fit_resample(X_train_cv_MCM, Y_train_cv_MCM)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"init_time = time.time()\nMCM_dt_cv_scores_mean, MCM_dt_cv_scores_std, MCM_dt_accuracy_scores = run_cross_validation_on_trees(X_train_cv_smote_MCM,\n                                                                                                    Y_train_cv_smote_MCM,\n                                                                                                    depths_list,\n                                                                                                    scoring='balanced_accuracy')\nend_time = time.time()\nellapsed_time = end_time - init_time\n\nprint('Cross-validation took ' + str(ellapsed_time) + ' seconds to run')\n\n# plotting accuracy\nplot_cross_validation_on_trees(depths_list, \n                               MCM_dt_cv_scores_mean, \n                               MCM_dt_cv_scores_std, \n                               MCM_dt_accuracy_scores, \n                               'Balanced accuracy per decision tree depth on MCM cross-validation data',\n                               'balanced accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_balanced_accuracy = np.amax(np.around(MCM_dt_cv_scores_mean,3))\n\noptimal_depth_MCM = np.argmax(np.around(MCM_dt_cv_scores_mean,3)) + 1\n\nprint('The maximum balanced accuracy score for the MCM model is '\n      + str(max_balanced_accuracy)\n      + ' for tree depth '\n      + str(optimal_depth_MCM))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MCM_dt_cv_model = DecisionTreeClassifier(max_depth=optimal_depth_MCM, random_state=123)\nMCM_dt_cv_model.fit(X_train_cv_smote_MCM,\n                    Y_train_cv_smote_MCM)\n\nY_pred_val_MCM = MCM_dt_cv_model.predict(X_val_MCM)\n\nclass_names = ['bike', 'car','other_mode', 'pt', 'walk']\n\ndisp = plot_confusion_matrix(MCM_dt_cv_model, X_val_MCM, Y_val_MCM,\n                             display_labels=class_names,\n                             cmap=plt.cm.Blues,\n                             normalize=None)\ndisp.ax_.set_title('MCM confusion matrix, without normalization')\nplt.show()\n\nprint(classification_report(Y_val_MCM, Y_pred_val_MCM, digits=3, labels=class_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"init_time = time.time()\n\nX_train_ATM_smote, Y_train_ATM_smote = sm.fit_resample(X_train_ATM, Y_train_ATM)\nATM_dt_model_smote = DecisionTreeClassifier(max_depth=optimal_depth_ATM, random_state=123)\nATM_dt_model_smote.fit(X_train_ATM_smote, Y_train_ATM_smote)\n\nX_train_MCM_smote, Y_train_MCM_smote = sm.fit_resample(X_train_MCM, Y_train_MCM)\nMCM_dt_model_smote = DecisionTreeClassifier(max_depth=optimal_depth_MCM, random_state=123)\nMCM_dt_model_smote.fit(X_train_MCM_smote, Y_train_MCM_smote)\n\nend_time = time.time()\nellapsed_time = end_time - init_time\n\nprint('Model training for both the ATM and MCM took ' + str(ellapsed_time) + ' seconds to run')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.3 Model 2: Inference\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"init_time = time.time()\n\nresults_smote = ddas_framework(X_test,ATM_dt_model_smote, MCM_dt_model_smote)\n\nend_time = time.time()\nellapsed_time = end_time - init_time\n\nprint('The DDAS framework with SMOTE took ' + str(ellapsed_time) + ' seconds to run')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.4 Model 2: Evaluation\n## 4.4.1 ATM Model\n### a) Expected and observed distribution of trips\n\nThe first thing we will analyse on the results obtained from Model 2 is the distribution of trips"},{"metadata":{"trusted":true},"cell_type":"code","source":"observed_list_smote = round(results_smote.destination.value_counts()*100/len(results_smote), 6)\nobserved_list_smote","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_expected_and_observed3(input_expected, input_observed1, input_observed2, x_label, y_label, color1, color2, legend1, legend2):\n    '''Returns plots of the given lists\n    \n    Keyword arguments:\n    input_expected -- list of the expected values of each category\n    input_observed1 -- first list of the observed values of each category\n    input_observed2 -- second list of the observed values of each category\n    '''\n    \n    fig, axs = plt.subplots(figsize=(10, 5))\n        \n    chart_title = ''\n    df = pd.concat([input_expected, input_observed1], axis=1)\n    df = pd.concat([df, input_observed2], axis=1)\n        \n    N = len(df)\n    ind = np.arange(N)  # the x locations for the groups\n    width = 0.25\n\n    x = df.index\n\n    y_expected = df.iloc[:,0]\n    rects_expected = axs.bar(ind-width, y_expected, width, color='lightgreen')\n\n    y_observed1 = df.iloc[:,1]\n    rects_observed1 = axs.bar(ind, y_observed1, width, color=color1)\n    \n    y_observed2 = df.iloc[:,2]\n    rects_observed2 = axs.bar(ind+width, y_observed2, width, color=color2)\n\n    axs.set_title(chart_title)\n    axs.set_ylabel(y_label, fontsize=12)\n    axs.set_xlabel(x_label, fontsize=12)\n    axs.set_xticks(ind)\n    axs.set_xticklabels(x)\n    axs.legend((rects_expected[0], rects_observed1[0], rects_observed2[0]), ('expected', legend1, legend2), loc=1)\n    \n    def autolabel(rects, offset=0):\n        for rect in rects:\n            try:\n                h = rect.get_height()\n                if (h >= 0.1):\n                    axs.text(rect.get_x() + (rect.get_width()/2) + offset, 1.00*h, '%g'%round(h,1),\n                             ha='center', va='bottom')\n                else:\n                    axs.text(rect.get_x() + (rect.get_width()/2) + offset, 1.00*h, '%g'%round(h,3),\n                             ha='center', va='bottom')\n            except:\n                pass\n\n    autolabel(rects_expected, -0.05)\n    autolabel(rects_observed1)\n    autolabel(rects_observed2, 0.05)\n        \nplot_expected_and_observed3(expected_list, observed_list, observed_list_smote, 'activity types', 'proportion (%)', 'lightblue', 'lightsalmon',\n                            'Model 1', 'Model 2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### b) Expected and observed frequency of chains"},{"metadata":{"trusted":true},"cell_type":"code","source":"chains_smote = pd.Series(get_actv_chains(results_smote))\nchains_smote.value_counts().head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(chains_smote.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len_chains_smote = pd.Series(get_actv_chain_legnths(results_smote))\nlen_chains_smote.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_expected_and_observed(len_chains_test.value_counts(),\n                           len_chains_smote.value_counts(),\n                           'chain length',\n                           'counts',\n                           'Model 2',\n                           'lightsalmon',\n                            dist=0.15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### c) Importance of features"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances = permutation_importance(ATM_dt_model_smote,\n                                             X_train_ATM_smote,\n                                             Y_train_ATM_smote,\n                                             scoring='balanced_accuracy',\n                                             n_repeats=5,\n                                             random_state=123)\n\ncol1 = feature_importances.importances_mean\ncol2 = ATM_X_columns\n\ndf = pd.DataFrame(col1, index=col2, columns=['importance'])\ndf.sort_values(by='importance', ascending=False, inplace=True)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### d) VALFRAM activity count validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"activ_counts_smote = actv_count_validation(results_smote)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chi_square_activity_counts_smote = compute_chi_square_from_values(activ_counts_test, activ_counts_smote)[0]\nprint('The total chi-square value for activity counts validation in the decision tree model is '\n      + str(chi_square_activity_counts_smote))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chi_square_activ_subtotals = compute_chi_square_from_values(activ_counts_test, activ_counts_smote)[1]\nchi_square_activ_subtotals[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_chi_square_results(chi_square_activ_subtotals, 'Model 2', 'lightsalmon')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4.2 MCM module"},{"metadata":{"trusted":true},"cell_type":"code","source":"mode_counts_smote = mode_count_validation(results_smote)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chi_square_mode_counts = compute_chi_square_from_proportions(mode_counts_test, mode_counts_smote)[0]\n\nprint('The total chi-square value for mode counts validation in this set of results is '\n      + str(chi_square_mode_counts))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chi_square_mode_subtotals = compute_chi_square_from_proportions(mode_counts_test, mode_counts_smote)[1]\nchi_square_mode_subtotals[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_chi_square_results(chi_square_mode_subtotals, 'Model 2', 'lightsalmon')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Model 3: Random Forests\n\n## 5.1 Model 3: Training the Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_cv_ATM, X_val_ATM, Y_train_cv_ATM, Y_val_ATM = train_test_split(X_train_ATM,\n                                                                        Y_train_ATM,\n                                                                        test_size = 0.20,\n                                                                        random_state=123)\n\nsm = SMOTE(random_state=123)\nX_train_cv_smote_ATM, Y_train_cv_smote_ATM = sm.fit_resample(X_train_cv_ATM, Y_train_cv_ATM)\n\nX_train_cv_MCM, X_val_MCM, Y_train_cv_MCM, Y_val_MCM = train_test_split(X_train_MCM,\n                                                                        Y_train_MCM,\n                                                                        test_size = 0.20,\n                                                                        random_state=123)\n\nX_train_cv_smote_MCM, Y_train_cv_smote_MCM = sm.fit_resample(X_train_cv_MCM, Y_train_cv_MCM)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_cross_validation_on_forests(X, y, tree_depths, cv=5, scoring='balanced_accuracy'):\n    \n    cv_scores_list = []\n    cv_scores_std = []\n    cv_scores_mean = []\n    accuracy_scores = []\n    \n    for depth in tree_depths:\n        tree_model = RandomForestClassifier(max_depth=depth, random_state=123)\n        cv_scores = cross_val_score(tree_model, X, y, cv=cv, scoring=scoring)\n        cv_scores_list.append(cv_scores)\n        cv_scores_mean.append(cv_scores.mean())\n        cv_scores_std.append(cv_scores.std())\n        accuracy_scores.append(tree_model.fit(X, y).score(X, y))\n        \n    cv_scores_mean = np.array(cv_scores_mean)\n    cv_scores_std = np.array(cv_scores_std)\n    accuracy_scores = np.array(accuracy_scores)\n    \n    return cv_scores_mean, cv_scores_std, accuracy_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"depths_list = [5,10,15,20,25,30,35,40]\n\ninit_time = time.time()\n\nATM_dt_cv_scores_mean, ATM_dt_cv_scores_std, ATM_dt_accuracy_scores = run_cross_validation_on_forests(X_train_cv_smote_ATM,\n                                                                                                      Y_train_cv_smote_ATM,\n                                                                                                      depths_list,\n                                                                                                      cv=2,\n                                                                                                      scoring='balanced_accuracy')\nend_time = time.time()\nellapsed_time = end_time - init_time\n\nprint('Cross-validation took ' + str(ellapsed_time) + ' seconds to run')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting accuracy\nplot_cross_validation_on_trees(depths_list, \n                               ATM_dt_cv_scores_mean, \n                               ATM_dt_cv_scores_std, \n                               ATM_dt_accuracy_scores, \n                               'Balanced accuracy per decision tree depth on ATM cross-validation data',\n                               'balanced-accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_balanced_accuracy = np.amax(np.around(ATM_dt_cv_scores_mean,3))\n\noptimal_depth_ATM = depths_list[np.argmax(np.around(ATM_dt_cv_scores_mean,3))]\n\nprint('The maximum balanced_accuracy score for the ATM model is '\n      + str(max_balanced_accuracy)\n      + ' for tree depth '\n      + str(optimal_depth_ATM))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ATM_rf_cv_model = RandomForestClassifier(max_depth=optimal_depth_ATM, random_state=123)\nATM_rf_cv_model.fit(X_train_cv_smote_ATM,\n                    Y_train_cv_smote_ATM)\n\nY_pred_val_ATM = ATM_rf_cv_model.predict(X_val_ATM)\n\nclass_names = ['leisure', 'none', 'other', 'school', 'shop', 'sleep', 'work']\n\ndisp = plot_confusion_matrix(ATM_rf_cv_model, X_val_ATM, Y_val_ATM,\n                             display_labels=class_names,\n                             cmap=plt.cm.Blues,\n                             normalize=None)\ndisp.ax_.set_title('ATM confusion matrix')\nplt.show()\n\nprint(classification_report(Y_val_ATM, Y_pred_val_ATM, digits=3, labels=class_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"init_time = time.time()\n\nMCM_dt_cv_scores_mean, MCM_dt_cv_scores_std, MCM_dt_accuracy_scores = run_cross_validation_on_forests(X_train_cv_smote_MCM,\n                                                                                                      Y_train_cv_smote_MCM,\n                                                                                                      depths_list,\n                                                                                                      cv=2,\n                                                                                                      scoring='balanced_accuracy')\nend_time = time.time()\nellapsed_time = end_time - init_time\n\nprint('Cross-validation took ' + str(ellapsed_time) + ' seconds to run')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting accuracy\nplot_cross_validation_on_trees(depths_list, \n                               MCM_dt_cv_scores_mean, \n                               MCM_dt_cv_scores_std, \n                               MCM_dt_accuracy_scores, \n                               'Balanced accuracy per decision tree depth on ATM cross-validation data',\n                               'balanced-accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_balanced_accuracy = np.amax(np.around(MCM_dt_cv_scores_mean,3))\n\noptimal_depth_MCM = depths_list[np.argmax(np.around(MCM_dt_cv_scores_mean,3))]\n\nprint('The maximum balanced_accuracy score for the ATM model is '\n      + str(max_balanced_accuracy)\n      + ' for tree depth '\n      + str(optimal_depth_MCM))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MCM_rf_cv_model = RandomForestClassifier(max_depth=optimal_depth_MCM, random_state=123)\nMCM_rf_cv_model.fit(X_train_cv_smote_MCM,\n                    Y_train_cv_smote_MCM)\n\nY_pred_val_MCM = MCM_rf_cv_model.predict(X_val_MCM)\n\nclass_names = ['bike', 'car','other_mode', 'pt', 'walk']\n\ndisp = plot_confusion_matrix(MCM_rf_cv_model, X_val_MCM, Y_val_MCM,\n                             display_labels=class_names,\n                             cmap=plt.cm.Blues,\n                             normalize=None)\ndisp.ax_.set_title('MCM confusion matrix')\nplt.show()\n\nprint(classification_report(Y_val_MCM, Y_pred_val_MCM, digits=3, labels=class_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"init_time = time.time()\n\nX_train_ATM_smote, Y_train_ATM_smote = sm.fit_resample(X_train_ATM, Y_train_ATM)\nATM_rf_model_smote = RandomForestClassifier(max_depth=optimal_depth_ATM, random_state=123)\nATM_rf_model_smote.fit(X_train_ATM_smote, Y_train_ATM_smote)\n\nX_train_MCM_smote, Y_train_MCM_smote = sm.fit_resample(X_train_MCM, Y_train_MCM)\nMCM_rf_model_smote = RandomForestClassifier(max_depth=optimal_depth_MCM, random_state=123)\nMCM_rf_model_smote.fit(X_train_MCM_smote, Y_train_MCM_smote)\n\nend_time = time.time()\nellapsed_time = end_time - init_time\n\nprint('Model training for both the ATM and MCM took ' + str(ellapsed_time) + ' seconds to run')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.3 Model 3: Inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"init_time = time.time()\n\nresults_rf = ddas_framework(X_test, ATM_rf_model_smote, MCM_rf_model_smote)\n\nend_time = time.time()\nellapsed_time = end_time - init_time\n\nprint('The DDAS framework with Random Forests and SMOTE took ' + str(ellapsed_time) + ' seconds to run')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.4 Model 3: Evaluation\n## 5.4.1 ATM Model\n### a) Expected and observed distribution of trips\n\nThe first thing we will analyse on the results obtained from Model 2 is the distribution of trips"},{"metadata":{"trusted":true},"cell_type":"code","source":"observed_list_rf = round(results_rf.destination.value_counts()*100/len(results_rf), 6)\nobserved_list_rf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_expected_and_observed4(input_expected, input_observed1, input_observed2, input_observed3, x_label, y_label):\n    '''Returns plots of the given lists\n    \n    Keyword arguments:\n    input_expected -- list of the expected values of each category\n    input_observed1 -- first list of the observed values of each category\n    input_observed2 -- second list of the observed values of each category\n    '''\n    \n    fig, axs = plt.subplots(figsize=(16, 6))\n        \n    chart_title = ''\n    df = pd.concat([input_expected, input_observed1], axis=1)\n    df = pd.concat([df, input_observed2], axis=1)\n    df = pd.concat([df, input_observed3], axis=1)\n        \n    N = len(df)\n    ind = np.arange(N)  # the x locations for the groups\n    width = 0.2\n\n    x = df.index\n\n    y_expected = df.iloc[:,0]\n    rects_expected = axs.bar(ind-width, y_expected, width, color='lightgreen')\n\n    y_observed1 = df.iloc[:,1]\n    rects_observed1 = axs.bar(ind, y_observed1, width, color='lightblue')\n    \n    y_observed2 = df.iloc[:,2]\n    rects_observed2 = axs.bar(ind+width, y_observed2, width, color='lightsalmon')\n    \n    y_observed3 = df.iloc[:,3]\n    rects_observed3 = axs.bar(ind+2*width, y_observed3, width, color='violet')\n\n    axs.set_title(chart_title)\n    axs.set_ylabel(y_label, fontsize=12)\n    axs.set_xlabel(x_label, fontsize=12)\n    axs.set_xticks(ind+width)\n    axs.set_xticklabels(x)\n    axs.legend((rects_expected[0], rects_observed1[0], rects_observed2[0], rects_observed3[0]),\n               ('expected', 'Model 1', 'Model 2', 'Model 3'),\n               loc=1)\n    \n    def autolabel(rects, offset=0):\n        for rect in rects:\n            try:\n                h = rect.get_height()\n                if (h >= 0.1):\n                    axs.text(rect.get_x() + (rect.get_width()/2) + offset, 1.00*h, '%g'%round(h,1),\n                             ha='center', va='bottom')\n                else:\n                    axs.text(rect.get_x() + (rect.get_width()/2) + offset, 1.00*h, '%g'%round(h,3),\n                             ha='center', va='bottom')\n            except:\n                pass\n\n    autolabel(rects_expected, -0.03)\n    autolabel(rects_observed1)\n    autolabel(rects_observed2, 0.03)\n    autolabel(rects_observed3, 0.06)\n    \n        \nplot_expected_and_observed4(expected_list, observed_list, observed_list_smote, observed_list_rf, 'activity types', 'proportion (%)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TR Part B Article\nplot_expected_and_observed3(expected_list,\n                            observed_list,\n                            observed_list_rf, \n                            'activity types',\n                            'proportion (%)',\n                            'lightblue',\n                            'violet',\n                            'Model 1',\n                            'Model 2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### b) Expected and observed frequency of chains"},{"metadata":{"trusted":true},"cell_type":"code","source":"chains_rf = pd.Series(get_actv_chains(results_rf))\nchains_rf.value_counts().head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(chains_rf.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len_chains_rf = pd.Series(get_actv_chain_legnths(results_rf))\nlen_chains_rf.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_expected_and_observed(len_chains_test.value_counts(),\n                           len_chains_rf.value_counts(),\n                           'chain length',\n                           'counts',\n                           'Model 3',\n                           'violet',\n                            dist=0.15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### c) Importance of features"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances = permutation_importance(ATM_rf_model_smote,\n                                             X_train_ATM_smote,\n                                             Y_train_ATM_smote,\n                                             scoring='balanced_accuracy',\n                                             n_repeats=5,\n                                             random_state=123)\n\ncol1 = feature_importances.importances_mean\ncol2 = ATM_X_columns\n\ndf = pd.DataFrame(col1, index=col2, columns=['importance'])\ndf.sort_values(by='importance', ascending=False, inplace=True)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### d) VALFRAM activity count validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"activ_counts_rf = actv_count_validation(results_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chi_square_activity_counts_rf = compute_chi_square_from_values(activ_counts_test, activ_counts_rf)[0]\nprint('The total chi-square value for activity counts validation in the decision tree model is '\n      + str(chi_square_activity_counts_smote))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chi_square_activ_subtotals = compute_chi_square_from_values(activ_counts_test, activ_counts_rf)[1]\nchi_square_activ_subtotals[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_chi_square_results(chi_square_activ_subtotals, 'Model 3', 'violet')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TR Part B Article\nplot_chi_square_results(chi_square_activ_subtotals, 'Model 2', 'violet')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.4.2 MCM module"},{"metadata":{"trusted":true},"cell_type":"code","source":"mode_counts_rf = mode_count_validation(results_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chi_square_mode_counts = compute_chi_square_from_proportions(mode_counts_test, mode_counts_rf)[0]\n\nprint('The total chi-square value for mode counts validation in this set of results is '\n      + str(chi_square_mode_counts))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chi_square_mode_subtotals = compute_chi_square_from_proportions(mode_counts_test, mode_counts_rf)[1]\nchi_square_mode_subtotals[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_chi_square_results(chi_square_mode_subtotals, 'Model 3', 'violet')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TR Part B article\nplot_chi_square_results(chi_square_mode_subtotals, 'Model 2', 'violet')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}