{"cells":[{"metadata":{},"cell_type":"markdown","source":"# My First Shared Notebook - Code for Titanic\n***Putting Everything Together From What I've Learned from the Kaggle Course [Intermediate Marchine Learning](https://www.kaggle.com/learn/intermediate-machine-learning)***\n\nI finished the course of Intermediate Marchine Learning recently. The course is one of the best ML courses I've seen. It'll be a great choice for you if you have learned those ML theory courses and would like to gain some hands-on experience and deal with real-world problems.\n\nIn this notebook, I made predications for the Tinanic Competition by combining the knowledge learned from the course, i.e. by processing missing values and categorial variables, using XGBoost as the model and pipeline to link everything together, I can achieve a score of 0.77 in the Leaderboard. \n\nIn this notebook, I didn't use 'Cabin' and 'Ticket' from the Titanic datasets as they have high cardinality. In the coming notebook, I'm planning to use those two features and also add feature engineering into the code to get better prediction results.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor\nfrom sklearn import model_selection","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save filepath to variable for easier access\ntitanic_train_path = '/kaggle/input/titanic/train.csv'\ntitanic_test_path = '/kaggle/input/titanic/test.csv'\n\n# read the data and store data in DataFrame \ntrain_data = pd.read_csv(titanic_train_path, index_col='PassengerId') \nX_test_full = pd.read_csv(titanic_test_path, index_col='PassengerId') \n\n# drop the row if the predict value Survived is NULL\ntrain_data.dropna(axis = 0, subset =['Survived'], inplace = True)\n\n# get the feature set X and predict value set y\nX = train_data.drop(['Survived'], axis = 1)\ny = train_data['Survived']\n\n#valid/training set split\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# select categorical columns with cardinality less than 10\ncategorical_cols = [cname for cname in X_train_full.columns if\n                    X_train_full[cname].nunique() < 10 and \n                    X_train_full[cname].dtype == \"object\"]\nprint(categorical_cols)\n\n# select numerical columns\nnumerical_cols = [cname for cname in X_train_full.columns if \n                X_train_full[cname].dtype in ['int64', 'float64']]\nprint (numerical_cols)\n\n# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing for numerical data - will fill nan with median value \nnumerical_transformer = SimpleImputer(strategy='median')\n\n# Preprocessing for categorical data - will fill nan with the most frequent value then do onehot encoding\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nMODEL SELECTION : this code can be commented out once the best parameters is selected for the XGBoost model\n'''\n# Define the parameters\nxgb_params = {\n'learning_rate': [0.001,0.005,0.01],\n'n_estimators': np.arange(0, 1500, 250).tolist(),\n'max_depth': [3, 5, 7, 9],\n'gamma': np.arange(0, 1.1, 0.2).tolist(),\n'subsample': [0.5, 0.7, 1],\n'colsample_bytree': [0.5, 0.7, 1]\n }\n\n# Model selection\ngrid = model_selection.RandomizedSearchCV(XGBRegressor(), xgb_params, n_jobs = 4, cv=5)\n\n# Bundle preprocessing and modeling code in a pipeline\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', grid)\n                     ])\n# Preprocessing of training data, fit model \nclf.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = clf.predict(X_valid).round(0)\n\nprint('MAE:', mean_absolute_error(y_valid, preds))\nprint(grid.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use XGB Regressor as the model\nmodel = XGBRegressor(n_estimators = 1250, subsample = 0.7, max_depth = 9, learning_rate = 0.005, gamma =0.4, colsample_bytree = 0.5 )\n# Bundle preprocessing and modeling code in a pipeline\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)\n                     ])\n# Preprocessing of training data, fit model \nclf.fit(X_train, y_train)\n# Preprocessing of validation data, get predictions\npreds = clf.predict(X_valid).round(0)\nprint('MAE:', mean_absolute_error(y_valid, preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing of test data, fit model\npreds_test = clf.predict(X_test).round(0) \n\n# Save test predictions to file\noutput = pd.DataFrame({'PassengerId': X_test.index,\n                       'Survived': preds_test})\noutput = output.astype({'Survived': 'int64'})\noutput.to_csv('submission.csv', index=False)\nprint(output)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}