{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os, sys, gc, time, warnings, pickle, psutil, random\n\nfrom multiprocessing import Pool    \n\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_seed(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n\n#runs func parallely for all items in data\ndef parallelize(func, data):\n    cores_number = np.min([core_count,len(data)])\n    pool = Pool(cores_number)\n    df = pd.concat(pool.map(func, data), axis=1)\n    pool.close()\n    pool.join()\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#setting the mask for splitting dataset\ntrain_set_start = 0  \ntrain_set_end = 1913               \nprediction_set = 28 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creation of rolling window columns\ndef lag_roll(days):\n    shift = days[0]\n    roll = days[1]\n    lag = test_base[['id','d','sales']]\n    column = 'rolling_mean_tmp_'+str(shift)+'_'+str(roll)\n    lag[column] = lag.groupby(['id'])['sales'].transform(lambda x: x.shift(shift).rolling(roll).mean())\n    return lag[[column]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We get the data by a particular store id\ndef get_store_data(store):\n    \n    df = data[data['store_id']==store]\n    df['temp_d'] = pd.to_numeric(data['d'].str[2:])\n\n    final_features = [col for col in list(df) if col not in remove]\n    \n    #Data fetched using start date \n    df = df[df['temp_d']>=train_set_start].reset_index(drop=True)\n    category_columns=['id','item_id','dept_id','cat_id','store_id','state_id','event_name_1','event_type_1','event_name_2','event_type_2','event_name_1_lag_1', 'event_type_1_lag_1',\n                   'event_name_1_lag_2', 'event_type_1_lag_2', 'event_name_1_lag_3', 'event_type_1_lag_3']\n    for col in category_columns:\n        df[col] = df[col].astype('category')\n    \n    return df, final_features\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test data being fetched for Training\ndef test_data():\n    test_data = pd.DataFrame()\n\n    for id in store_ids:\n        temp_df = pd.read_pickle('test_'+id+'.pkl')\n        temp_df['store_id'] = id\n        test_data = pd.concat([test_data, temp_df]).reset_index(drop=True)\n    \n    return test_data\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# setting the parameters for LGBM model\nimport lightgbm as lgb\nlgb_params ={\n         'boosting_type': 'gbdt',\n         'objective': 'tweedie',\n         'tweedie_variance_power': 1.4,\n         'metric': 'rmse',\n         'subsample': 0.5,\n         'subsample_freq': 1,\n         'learning_rate': 0.01,\n         'num_leaves': 2**11-1,\n         'min_data_in_leaf': 2**12-1,\n         'feature_fraction': 0.5,\n         'max_bin': 100,\n         'n_estimators': 700,\n         'boost_from_average': False,\n         'verbose': -1,\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"version = 1                          \nseed = 42                        \nmodel_seed(seed)             \nlgb_params['seed'] = seed        \ncore_count = psutil.cpu_count()     \n\n# remove certain features for modelling\n\nremove = ['id','state_id','store_id','date','wm_yr_wk','d','sales']\n\ndata_dir = '../input/bestfitting-da-dataset/data.csv'\n\ndata = pd.read_csv(data_dir)\n\nprint(data.info())\n\ncategory_columns=['id','item_id','dept_id','cat_id','store_id','state_id','event_name_1','event_type_1','event_name_2','event_type_2','event_name_1_lag_1', 'event_type_1_lag_1',\n                   'event_name_1_lag_2', 'event_type_1_lag_2', 'event_name_1_lag_3', 'event_type_1_lag_3']\nfor col in category_columns:\n    data[col] = data[col].astype('category')\n\n#fetching STORES ids\nstore_ids = data['store_id']\nstore_ids = list(store_ids.unique())\n\n\n#SPLITS \nrol_split = []\nfor i in [1,7,14]:\n    for j in [7,14,30,60]:\n        rol_split.append([i,j])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for id in store_ids:\n    print('Training data for Store: ', id)\n    \n    # fetching data for current store\n    store_df, features_columns = get_store_data(id)\n    \n    # dividing data into training and testing data\n    training_mask = store_df['temp_d']<=train_set_end\n    validation_mask = training_mask&(store_df['temp_d']>(train_set_end-prediction_set))\n    preds_mask = store_df['temp_d']>(train_set_end-100)\n    \n    \n    training_data = lgb.Dataset(store_df[training_mask][features_columns], \n                       label=store_df[training_mask]['sales'])\n    training_data.save_binary('training_data.bin')\n    training_data = lgb.Dataset('training_data.bin')\n    \n    validation_data = lgb.Dataset(store_df[validation_mask][features_columns], \n                       label=store_df[validation_mask]['sales'])\n    \n    #Dataset saved for later predictions\n    #Removing features\n    store_df = store_df[preds_mask].reset_index(drop=True)\n    keep_cols = [col for col in list(store_df) if '_tmp_' not in col]\n    store_df = store_df[keep_cols]\n    store_df.to_pickle('test_'+id+'.pkl')\n    del store_df\n    \n    # Launch seed again to ensure lgb training 100% deterministic\n    model_seed(seed)\n    model_estm = lgb.train(lgb_params,\n                          training_data,\n                          valid_sets = [validation_data],\n                          verbose_eval = 100,\n                          )\n    \n    model_name = 'lgb_model_'+id+'_v'+str(version)+'.bin'\n    pickle.dump(model_estm, open(model_name, 'wb'))\n\n    # Remove temporary files and objects to free some space\n    \n    !rm training_data.bin\n    del training_data, validation_data, model_estm\n    gc.collect()\n    \n    # Models features used for predictions\n    predict_features = features_columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Creating DataFrame to store predictions\nprediction = pd.DataFrame()\n\n# Join back the Test dataset with a small part of the training data to make recursive features\ntest_base = test_data()\n\n# Measuring prediction time \nprediction_time = time.time()\n\n#Each prediction day \n\nfor day in range(1,29):    \n    print('Prediction Day:', day)\n    start_time = time.time()\n\n    # Make temporary grid to calculate rolling lags\n    store_df = test_base.copy()\n    parallelize(lag_roll, rol_split)\n    store_df = pd.concat([store_df, parallelize(lag_roll, rol_split)], axis=1)\n        \n    for id in store_ids:\n        \n        # Read all our models and make predictions for each day\n        model_path = 'lgb_model_'+id+'_v'+str(version)+'.bin' \n        \n        model_estm = pickle.load(open(model_path, 'rb'))\n        \n        day_mask = test_base['temp_d']==(train_set_end+day)\n        store_mask = test_base['store_id']==id\n        \n        mask = (day_mask)&(store_mask)\n        test_base['sales'][mask] = model_estm.predict(store_df[mask][predict_features])\n\n    temp_df = test_base[day_mask][['id','sales']]\n    temp_df.columns = ['id','F'+str(day)]\n    if 'id' in list(prediction):\n        prediction = prediction.merge(temp_df, on=['id'], how='left')\n    else:\n        prediction = temp_df.copy()\n        \n    print(' %0.2f min round ' % ((time.time() - start_time) / 60),\n          ' %0.2f min total ' % ((time.time() - prediction_time) / 60),\n          ' %0.2f day sales ' % (temp_df['F'+str(day)].sum()))\n    del temp_df\n    \n#saving the predictions in a csv file\nprediction = prediction.reset_index(drop=True)\nprediction.to_csv('final_prediction.csv')\nprediction","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}