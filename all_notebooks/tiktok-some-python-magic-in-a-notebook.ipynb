{"cells":[{"metadata":{},"cell_type":"markdown","source":"<img src=\"./magical-place.png\" />"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"## Import Libraries\nimport os\nimport json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport seaborn as sns\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot, plot\nfrom plotly.subplots import make_subplots\n\nfrom IPython.core.display import HTML\nfrom IPython.display import Image, Audio, Video\n\n%matplotlib inline\n\n############################\n## Start Helper functions ##\n############################\n\n# Wrapper around pandas cut() method.\ndef my_cut (x, bins, lower_infinite=False, upper_infinite=False, **kwargs):\n    \"\"\"\n    Wrapper around pandas cut() to create infinite lower/upper bounds with proper labeling.\n\n    Takes all the same arguments as pandas cut(), plus two more.\n\n    Args :\n        lower_infinite (bool, optional) : set whether the lower bound is infinite\n            Default is True. If true, and your first bin element is something like 20, the\n            first bin label will be '<= 20' (depending on other cut() parameters)\n        upper_infinite (bool, optional) : set whether the upper bound is infinite\n            Default is True. If true, and your last bin element is something like 20, the\n            first bin label will be '> 20' (depending on other cut() parameters)\n        **kwargs : any standard pandas cut() labeled parameters\n\n    Returns :\n        out : same as pandas cut() return value\n        bins : same as pandas cut() return value\n    \n    Code slidely modified from sparc_spread, StackOverflow: https://stackoverflow.com/a/30199132/1843511\n    \"\"\"\n\n    # Quick passthru if no infinite bounds\n    if not lower_infinite and not upper_infinite:\n        return pd.cut(x, bins, **kwargs)\n\n    # Setup\n    num_labels      = len(bins) - 1\n    include_lowest  = kwargs.get(\"include_lowest\", False)\n    right           = kwargs.get(\"right\", True)\n\n    # Prepend/Append infinities where indiciated\n    bins_final = bins.copy()\n    if upper_infinite:\n        bins_final.insert(len(bins),float(\"inf\"))\n        num_labels += 1\n    if lower_infinite:\n        bins_final.insert(0,float(\"-inf\"))\n        num_labels += 1\n\n    # Decide all boundary symbols based on traditional cut() parameters\n    symbol_lower  = \"<=\" if include_lowest and right else \"<\"\n    left_bracket  = \"(\" if right else \"[\"\n    right_bracket = \"]\" if right else \")\"\n    symbol_upper  = \">\" if right else \">=\"\n\n    # Inner function reused in multiple clauses for labeling\n    def make_label(i, lb=left_bracket, rb=right_bracket):\n        return \"{0} - {1}\".format(bins_final[i], bins_final[i+1])\n\n    # Create custom labels\n    labels=[]\n    for i in range(0,num_labels):\n        new_label = None\n\n        if i == 0:\n            if lower_infinite:\n                new_label = \"{0} {1}\".format(symbol_lower, bins_final[i+1])\n            elif include_lowest:\n                new_label = make_label(i, lb=\"[\")\n            else:\n                new_label = make_label(i)\n        elif upper_infinite and i == (num_labels - 1):\n            new_label = \"{0} {1}\".format(symbol_upper, bins_final[i])\n        else:\n            new_label = make_label(i)\n\n        labels.append(new_label)\n\n    # Pass thru to pandas cut()\n    return pd.cut(x, bins_final, labels=labels, **kwargs)\n\n# Import images\nimport shutil \ndest = shutil.copy('../input/images/magical-place.png', './magical-place.png')\ndest = shutil.copy('../input/images/dabbing-unicorn.png', './dabbing-unicorn.png')\ndest = shutil.copy('../input/images/erd-tiktok-data-800.PNG', './erd-tiktok-data-800.png')\ndest = shutil.copy('../input/images/erd-audd-data-800.PNG', './erd-audd-data-800.png')\n\n##########################\n## END Helper functions ##\n##########################\n\n## Set CSS Styles\nHTML(\"\"\"<style>\n    @import url('https://fonts.googleapis.com/css2?family=Cookie&display=swap');\n\n    .credits {\n        display: block;\n        border-radius: 4px;\n        font-weight: bold;\n        background: #452756;\n        padding: 10rem;\n        position: relative;\n    }\n    \n    .creditsunicorn {\n        display: inline-block;\n        background: url('./dabbing-unicorn.png') no-repeat;\n        width: 190px;\n        height: 100px;\n        position: absolute;\n        right:0;\n        bottom:0;\n    }\n    \n    .credits-title {\n        font-family: 'Cookie', cursive;\n        position: relative;\n        top: -10px;\n        font-size: 50px;\n        color: #A2D2FF;\n        margin-bottom: 20px;\n    }\n    \n    .credits-title > span {\n        color: #FFAFCC;\n    }\n    \n    .credits-text {\n        color: white;\n        line-height: 1.8 !important;\n    }\n    \n    .credits-title-star {\n        position: absolute;\n        right: 10px;\n        top: 10px;\n        font-size: 80px;\n    }\n    \n    strong {\n        font-weight: bold!important\n    }\n    </style>\"\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> About the banner: whoever watches or has watched Agents of S.H.I.E.L.D., probably knows this phrase. Agent Coulson keeps rephrasing to Tahiti as a *magical place*, after his holiday over there. He thinks he was sent over there by Nick Fury after he almost died, to recover. But is that true? Has he even been there? Who knows... Anyway, because this notebook is all about performing some Python magic on my TikTok dataset, I thought it would be a nice banner to use."},{"metadata":{},"cell_type":"markdown","source":"# What's this all about ü§∑‚Äç‚ôÇÔ∏è\nIt isn't that hard to just create a Dataset and post it on Kaggle for anyone to use. So, I would like to do a bit extra. I will show the structure of the dataset (because there are different files, each containing a part of the whole set) and how you can start using it. I'll try doing some first analysis as well. So in this notebook, I will:\n- show the dataset structure\n- show alternatives of combining the data\n- enrich the data\n- do some first analysis"},{"metadata":{},"cell_type":"markdown","source":"# Our Data Structure üìÅ\n\n\nSo let's get started with the current data structure. The previous version of our dataset consisted of mulitple files, but because this ended up in data errors, I decided replace the files with the raw data and work from there:\n* `trending.json`, this is the raw data file. It contains all scraped information of the TikTok videos.\n\nThis file contains a list `collectors`, which contains a JSON object of each video with the following fields:\n* `id`: the unique identifier of the video\n* `text`: the text below of the video\n* `createTime`: timestamp of the datetime when the video was created\n* `authorMeta`: an object with detailed information about the author\n* `musicMeta`: an object with detailed information about the music used with the video\n* `covers`: an object with all covers of the video\n* `webVideoUrl`: link to the TikTok video\n* `videoUrl`: exact link to the TikTok video (not reachable directly)\n* `videoUrlNoWaterMark`: the URL of the video without a watermark\n* `videoMeta`: an object with dimensions and duration of the video\n* `diggCount`: amount of likes\n* `shareCount`: how many times the video has been shared\n* `playCount`: how many times the video has been watched\n* `commentCount`: amount of comments\n* `downloaded`: if the video is downloaded using the scraper\n* `mentions`: list with users mentioned in the video\n* `hashtags`: list with hashtags used in the video\n\n<div class=\"alert alert-info\" role=\"alert\">\n    <strong>Note:</strong> The new version of the current dataset contains a folder \"audd\", which contains the enrichments from the <a href=\"#Enrich-Data-%E2%9C%A8\">enrichment chapter below</a>\n</div><br/>"},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"# Open file with the raw data\nfile = open('../input/tiktok-trending-december-2020/trending.json', encoding=\"utf8\")\n\n# Load data as JSON\nraw_data = json.load(file)\n\n# Close the original file\nfile.close()\n\n# Select only the list with the video data\ntrending_videos_list = raw_data['collector']\n\n# Example of a video object\nprint(json.dumps(trending_videos_list[15], indent=4, sort_keys=True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Convert JSON to DataFrame üê±‚Äçüíª"},{"metadata":{},"cell_type":"markdown","source":"# Let's explode() the cell üí£\nDon't worry, we are not destroying all our stuff! We like to keep things clean. \n\nNow we have merged the one-to-one relationships into one dataframe, we would like to add the hashtags as well (a many-to-one relationship), which is a bit trickier.\nThe hashtags are comma-separated id's inside the **hashtag_ids** column of `trending_dec_2020.csv`.\n\nThe `explode()` function is used to transform each element of a list-like to a row, replicating the index values. \nSo we would like to perform the following steps:\n\n1. Set each column we would like to keep as index, temporarily.\n2. Converting the comma separated values to a list\n3. Create a new row for each value by using the `explode()` function, which copies all index fields as well\n4. Resetting the index\n\nSo let's get started."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a DataFrame of the data\ndf_tiktok_dataset = pd.DataFrame(trending_videos_list)\n\n# Let's expand the hashtag cell containing lists to multiple rows\ndf_tiktok_dataset = df_tiktok_dataset.explode('hashtags').explode('mentions')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def object_to_columns(dfRow, **kwargs):\n    '''Function to expand cells containing dictionaries, to columns'''\n    for column, prefix in kwargs.items():\n        if isinstance(dfRow[column], dict):\n            for key, value in dfRow[column].items():\n                columnName = '{}.{}'.format(prefix, key)\n                dfRow[columnName] = value\n    return dfRow\n\n# Expand certain cells containing dictionaries to columns\ndf_tiktok_dataset = df_tiktok_dataset.apply(object_to_columns, \n                            authorMeta='authorMeta',  \n                            musicMeta='musicMeta',\n                            covers='cover',\n                            videoMeta='videoMeta',\n                            hashtags='hashtag', axis = 1)\n\n# Remove the original columns containing the dictionaries\ndf_tiktok_dataset = df_tiktok_dataset.drop(['authorMeta','musicMeta','covers','videoMeta','hashtags'], axis = 1)\ndf_tiktok_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tiktok_dataset.info()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"# Get unique rows from dataset\ndf_unique_videos = df_tiktok_dataset.drop_duplicates(subset='id', keep=\"first\")\ndf_unique_music = df_tiktok_dataset.drop_duplicates(subset='musicMeta.musicId', keep=\"first\")\ndf_unique_authors = df_tiktok_dataset.drop_duplicates(subset='authorMeta.id', keep=\"first\")\n\n# Show amount of rows per dataset\n{\n    'df_tiktok_dataset': df_tiktok_dataset.shape,\n    'df_unique_videos': df_unique_videos.shape,\n    'df_unique_music': df_unique_music.shape,\n    'df_unique_authors': df_unique_authors.shape\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Some first Analysis üìà"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Set bucket ranges\nbuckets = list(range(0,105000,5000))\n\n# Count videos with likes and comments per bucket range\nlikes = df_unique_videos.groupby( my_cut( df_unique_videos['diggCount'], buckets, upper_infinite=True ) ).diggCount.count()\ncomments = df_unique_videos.groupby( my_cut( df_unique_videos['commentCount'], buckets, upper_infinite=True ) ).diggCount.count()\n\n# Transform from series to dataframe with some small modifications\nlikes = likes.rename('likes').to_frame().reset_index() \ncomments = comments.rename('comments').to_frame().reset_index() \n\n# create subplots, two rows and 1 column each row\nfig = make_subplots(2,1,subplot_titles=(\"Distribution of Likes\", \"Distribution of Comments\"))\n\n# First plot\nfig.add_trace(\n    go.Bar(y = likes['diggCount'], \n           x = likes['likes'], \n           name=\"Likes\",\n           text = likes['likes'], \n           orientation='h',\n           texttemplate='%{text:.2s}', \n           textposition='outside', \n           marker_color='rgb(162, 210, 255)'\n    ),\n    row=1,col=1\n)\n\n# Second plot\nfig.add_trace(\n    go.Bar(y = comments['commentCount'], \n           x = comments['comments'], \n           name=\"Comments\",\n           text = comments['comments'], \n           orientation='h',\n           texttemplate='%{text:.2s}', \n           textposition='outside', \n           marker_color='rgb(205, 180, 219)'\n    ),\n    row=2,col=1\n)\n\nfig.update_layout(uniformtext_minsize=8, \n                  uniformtext_mode='hide', \n                  title_text=\"Multiple Subplots with Titles\",\n                  height=1200,\n                  template='plotly_white',\n                  margin=go.layout.Margin(\n                      l=130,r=5,b=5,t=100,pad=10\n                  ))\n\nfig.update_xaxes(title_text='Videos')\nfig.update_yaxes(title_text='Likes', col=1, row=1, automargin=False)\nfig.update_yaxes(title_text='Comments', col=1, row=2, automargin=False)\n\nfig.show(config={'displayModeBar': False})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we started with the most obvious: how many videos have received how many likes, i.e. the like distribution. And how many videos have received how many comments, i.e. the comment distribution. \n\nSo far most of the videos seem to contain below 50.000 comments and likes. It still doesn't tell us how many of the enormous amount in the **0-5000** bucket are close to zero, somewhere in the middle or close to 5000. \n\nLet's see if we can create a scatter plot to have a better idea of the relation and distribution between the amount of comments and likes of all buckets below and including 50.000"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Focus on dataset from 0 till 50.000 likes\ndf_videos_users_focus = df_unique_videos[df_unique_videos['diggCount'] <= 50000]\n\n# Create a scatter plot with a trendline\nfig = px.scatter(df_videos_users_focus, trendline=\"ols\",\n                 x=\"diggCount\", \n                 y=\"commentCount\",\n                 labels={\n                     \"diggCount\": \"Likes\",\n                     \"commentCount\": \"Comments\"\n                 },\n                 log_y=True,\n                 trendline_color_override=\"#ff7096\", \n                 template='plotly_white')\n\nfig.update_traces(marker=dict(\n                     color='#4cc9f0',\n                     opacity=0.6,\n                 ))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I use a log-scale for the y-axis, to allow a large range to be displayed without the small values being compressed down into bottom of the graph.\n\nWe see that the low $R^2$ value indicates that the independent variable (likes) is not explaining much in the variation of the dependent variable (comments). \n\nWhile it seemed obvious that a video with more likes, would also result in a higher amount of comments as well... The contents of the video might still be the biggest factor. Even though a higher amount of likes would result in a higher amount of viewers. And hitting a like button is easier than leaving a comment. We do see, however, there isn't a single video with over 20k likes in this dataset which contains less than 50 comments.\n\nAnd even with our own eye we can see: \n- there are just more videos under 20.000 likes\n- even videos with a low amount of likes can still be very high in comments\n- there are two interesting outliers around 35k likes who have a large amount of comments.\n"},{"metadata":{},"cell_type":"markdown","source":"## Popular Hashtags üè∑"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a DataFrame of the data\ndf_hashtags = pd.DataFrame(trending_videos_list)\n\n# Let's expand the hashtag cell containing lists to multiple rows\ndf_hashtags = df_hashtags.explode('hashtags')\n\n# Expand certain cells containing dictionaries to columns\ndf_hashtags = df_hashtags.apply(object_to_columns, \n                                hashtags='hashtag', axis = 1)\n\nhashtags = df_hashtags[['hashtag.name']].copy().dropna()\nhashtags.info()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Add column with default value\nhashtags['count'] = 1\n\n# Count all hashtags, group and replace the count column value with the sum\nhashtags = hashtags.groupby([\"hashtag.name\"])[\"count\"].count().reset_index()\n\n# Sort by most popular hashtags and keep the top 15\nhashtags = hashtags.sort_values(by='count', ascending=False)[:15]\n\n# Set colours\n\n# Create a Pie Chart with all values\nfig = go.Figure(data=[go.Pie(\n                        labels=hashtags[\"hashtag.name\"], \n                        values=hashtags[\"count\"], \n                        textinfo='label+percent',\n                        insidetextorientation='radial'\n                )], \n                layout={\"colorway\": [\"#f72585\",\"#b5179e\",\n                                     \"#7209b7\",\"#560bad\",\n                                     \"#480ca8\",\"#3a0ca3\",\n                                     \"#3f37c9\",\"#4361ee\",\n                                     \"#4895ef\",\"#4cc9f0\"]})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Enrich Data ‚ú®\nThe music from the data contains not only a link to the actual sound, but a title as well. The \"origineel geluid\", which is Dutch for \"original sound\", might contain speech or perhaps some music (in the background) as well. But we don't know that just by looking at this title.\n\nOn top of that, we might be interested in more information, besides the title of the sound/music used in the TikTok video, to get a better idea if and how much value the music adds to those videos. I could use some more information like:\n\n* genre of the music\n* artist\n* original name\n* popularity\n* etc.\n\nWe are probably all familiar with SoundHound and Shazam, which are able to recognize the sound of a playing song and return the artist and name. If they only had an API which we could use to enrich our data... Unfortunately, SoundHound and Shazam don't have an API (as far as I know off), but there are alternatives!\n\n<div class=\"alert alert-info\" role=\"alert\"><strong>Note:</strong> I used <a href=\"https://audd.io\">audd.io</a> to enrich my music.csv with their information about the song. I added the information to the most recent version of the dataset, inside the `audd` folder.</div>\n<div></div>"},{"metadata":{},"cell_type":"markdown","source":"## Shazam/Soundhound-like Data üéº\nLet's say the **trending_dec_2020.csv** is the dataframe we created from the `trending.json` file.\n\n<img src='./erd-audd-data-800.png' />"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import Audd Data\ndf_audd_music = pd.read_csv('../input/tiktok-trending-december-2020/audd/audd_music.csv', index_col='id')\ndf_audd_music_apple = pd.read_csv('../input/tiktok-trending-december-2020/audd/audd_music_apple_music.csv')\ndf_audd_music_spotify = pd.read_csv('../input/tiktok-trending-december-2020/audd/audd_music_spotify_music.csv')\ndf_audd_music_spotify_artists = pd.read_csv('../input/tiktok-trending-december-2020/audd/audd_music_spotify_music_artists.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The current version of the dataset contains duplicated rows, let's remove them\ndf_audd_music = df_audd_music.drop_duplicates()\n\n# Add prefix to this dataset, before merging\ndf_audd_music = df_audd_music.add_prefix('_audd_music.')\ndf_audd_music.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a DataFrame of the data\ndf_tiktok_music = pd.DataFrame(trending_videos_list)\n\n# Expand certain cells containing dictionaries to columns\ndf_tiktok_music = df_tiktok_music.apply(object_to_columns, \n                                        musicMeta='musicMeta', axis = 1)\n\n# Convert the column dtype to int64 so we can merge\ndf_tiktok_music['musicMeta.musicId'] = df_tiktok_music['musicMeta.musicId'].astype('int64')\ndf_tiktok_music.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tiktok_audd_music = df_tiktok_music.merge(df_audd_music, how='left', right_on='id', left_on='musicMeta.musicId')\ndf_tiktok_audd_music.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Original sound examples üîä\nAs you might hear in the background: the audio file contains other sounds as well.\nAs you can see when opening the link, it seems the audio is added to the video while maintaining the original audio. So the data says \"origineel geluid\" (= original sound), but audd.io recognized the music being used."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tiktok_audd_music = df_tiktok_audd_music[(df_tiktok_audd_music['musicMeta.musicName'] == 'origineel geluid') & df_tiktok_audd_music['_audd_music.artist'].notna()]\ndf_tiktok_audd_music","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"videoUrl = df_tiktok_audd_music.iloc[2]['musicMeta.playUrl']\nurl = df_tiktok_audd_music.iloc[2]['webVideoUrl']\nprint('Url to full video: ', url)\nprint('Sound recognised by Audd: ', df_tiktok_audd_music.iloc[2]['_audd_music.artist'], '-', df_tiktok_audd_music.iloc[2]['_audd_music.title'])\nprint('original sound:‚Ü¥')\nAudio(df_tiktok_audd_music.iloc[2]['musicMeta.playUrl'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"videoUrl = df_tiktok_audd_music.iloc[9]['musicMeta.playUrl']\nurl = df_tiktok_audd_music.iloc[9]['webVideoUrl']\nprint('Url to full video: ', url)\nprint('Sound recognised by Audd: ', df_tiktok_audd_music.iloc[9]['_audd_music.artist'], '-', df_tiktok_audd_music.iloc[9]['_audd_music.title'])\nprint('original sound:‚Ü¥')\nAudio(df_tiktok_audd_music.iloc[9]['musicMeta.playUrl'])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"videoUrl = df_tiktok_audd_music.iloc[15]['musicMeta.playUrl']\nurl = df_tiktok_audd_music.iloc[15]['webVideoUrl']\nprint('Url to full video: ', url)\nprint('Sound recognised by Audd: ', df_tiktok_audd_music.iloc[15]['_audd_music.artist'], '-', df_tiktok_audd_music.iloc[15]['_audd_music.title'])\nprint('original sound:‚Ü¥')\nAudio(df_tiktok_audd_music.iloc[15]['musicMeta.playUrl'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# To be continued ... ‚è≥\n\nI still want to do some more analysis on the Audd data. Quite some, to be honest. But this notebook already took quite some time, and it's always good to have some feedback, even in this early stage!\n\nSo if anyone has any feedback already, go ahead! Leave a comment, it will sure help me a lot! And analysis ideas you would like to see, if there are any, are very welcome as well!\n"},{"metadata":{},"cell_type":"markdown","source":"# Thank you and Credits üí∞\nI am a member of Kaggle for four years now and haven't published a single \"real\" notebook ever since. Why? Because I thought I couldn't do it, I wasn't good enough or I just didn't know how or where to start. Last Saturday's meetup by Andrada and Parul was most inspiring. It encouraged me to start this notebook. \n\nBut there are more people to thank, so for all of you, some most deserved credit:\n\n<div class=\"credits\">\n    <div class=\"credits-title\"><span>Awesome</span> Unicorns</div><div class=\"credits-title-star\" style=\"color: rgba(0,0,0,1);\">‚ú®</div>\n    <div class=\"creditsunicorn\"></div>\n    <p class=\"credits-text\" style=\"color: rgba(255,255,255,1);\">A huge thank you to <a style=\"color:#BDE0FE\" href=\"https://www.kaggle.com/andradaolteanu\">Andrada Olteanu</a> and <a style=\"color:#BDE0FE\" href=\"https://www.kaggle.com/parulpandey\">Parul Pandey</a> for sharing their valuable experience and knowledge with the rest of the community, during the meetup of last Saturday, and of course Team Kaggle Days Meetup Delhi NCR for organising the event. <br/><br/>Thanks <a style=\"color:#BDE0FE\" href=\"https://www.kaggle.com/andradaolteanu\">Andrada Olteanu</a>, again, for her awesome tips on improving the Notebooks Flow and her helpful and most inspiring notebooks. I most probably used a lot of her ideas for this notebook, so make sure to check out her profile.<br/><br/>\n    Thank you <a style=\"color:#BDE0FE\" href=\"https://www.kaggle.com/bariscal\">Baris Cal</a> for his great notebook on visualisations with the plotly library, which gave me great advice on how to create and write cleaner code for the visualisations with this library.<br/><br/>\n    Thank you every member of the KaggleNoobs Slack Channel! It's awesome to see how many Masters and Grandmasters are still member of this slack channel and are always happy to help others out, no matter their experience, background or skillset.<br/><br/> You guys make me love Data Science and the people in it, even more ‚ù§</p>\n</div>\n\n<div></div>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}