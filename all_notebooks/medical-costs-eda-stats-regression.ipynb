{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importing Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as vif\nfrom scipy import stats\nimport statsmodels.stats.api as sms\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import accuracy_score, roc_auc_score, mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Lasso, LassoCV, Ridge, RidgeCV, ElasticNet, ElasticNetCV\nfrom sklearn.metrics import r2_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Importing Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/insurance/insurance.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.get_dummies(df, prefix = [\"sex\", \"smoker\", \"region\"], drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Charges for Smokers and Non-Smokers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1,2, figsize=(14,5))\n\nsns.distplot(df[df['smoker_yes']==1]['charges'], ax=axes[0], color='r').set_title('Distribution of Charges for Smokers')\n\nsns.distplot(df[df['smoker_yes']==0]['charges'], ax=axes[1], color='g')\nplt.title('Distribution of Charges for Non-Smokers')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For  maximum number of smokers the charges lie between 15000 to 50000, thus revealing that smokers face charges higher than that of non-smokers.\n\nFor non-smokers the graph shows that the target variable is skewed to the right i.e. maximum number of non-smokers face little charges. The graph also shows that there are a many outliers in the data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Charges for Males vs Females","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.boxplot(df['sex_male'], df['charges'])\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is not much difference in the charges between the charges of men and women. We could also observe the presence of a lot of outliers specially in Female part.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### BMI vs Charges","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.scatterplot(x='bmi', y='charges', data=df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"BMI above 30 is considered obese and we can see that as the BMI increases above 30 the charges rate shoots up.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Variation of Charges with number of Children","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,7))\nsns.violinplot(x='children', y='charges', data=df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The violin plot clearly shows that people with 5 children have the lowest charges, but it is difficult to say anything about the highest charges. Lets plor a barchart to clarify things.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.barplot(df.groupby('children').mean()['charges'].index, df.groupby('children').mean()['charges'].values)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Bingo.. The barchart clearly shows that people with 3 children have the highest charges. It also validates the result of the violinplot saying that people with 5 children have the lowest charges.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Correlation Matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.heatmap(df.corr(), annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The highest correlation with Charges is with Smokers and the lowest correlation is with different regions.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Pre-processing the Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The categorical features were converted into dummy features as the first step towards pre-processing.\n\nThe next step would be towards defining the features as independent and dependent in the form of X and y respectively, and finally scale the independent features X.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('charges', axis=1)\ny = df['charges']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=3)\n\nss = StandardScaler()\n\nX_trains = ss.fit_transform(X_train)\nX_tests = ss.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Regression -  Statistics Approach ","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"Xc = sm.add_constant(X)\nmodel = sm.OLS(y, Xc)\nlr = model.fit()\nlr.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Conclusions from the Statisticale Summary\n\n1. The pvalue for the sex region is 0.692 and hence it is irrelevant. We had also deduced the same from the visualization that the charges was not biased to any gender.\n2. The charges was very much dependent on smoking and hence the pvalue for that region is 0.00.\n3. The pvalue for the children is also 0 hence we can confidently say that the charges vary with the number of children a person has. Same goes with age and BMI.\n4. Coming to the region column as the pvalue is large it can be again concluded that the variation in charges is not dependent on the region.\n5. To our surprise all the visualization conclusion allign with the statistical summary, but it should be kept in mind that the visualization sometimes may be confusing and may not always tell us what really is going on. We always should perform statistical analysis to confirm our beliefs.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Assumptions of Linear Regression","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 1. Multicollinearity","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vif = [vif(Xc.values, i) for i in range(Xc.shape[1])]\npd.DataFrame(vif, index=Xc.columns, columns=['VIF'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the features have VIF below 5. So we can safely conclude that all the features are relevant as none of them show multimollinearity.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 2. Linearity of the relationship","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = lr.predict()\nsns.regplot(x=pred, y=y, line_kws={'color':'red'})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fstat, pvalue = sm.stats.diagnostic.linear_rainbow(lr)\nprint(\"The p-value is: \",pvalue)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The pvalue is above 0.05, hence we can conclude that our Null Hypothesis is true which is the fit of the model using full sample is the same as using a central subset and hence a Linear regression model can be built.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 3. Normality of the residuals","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"stats.probplot(lr.resid, plot=plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(lr.resid)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stat, pvalue = stats.jarque_bera(lr.resid)\n\nprint(\"The p-value is: \",pvalue)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The pvalue is much below the significance level. Hence the residuals are not normally distributed and we need to normalize it before proceeding further.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 4. Homoscedasticity","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.residplot(lr.predict(), lr.resid, lowess=True, line_kws={'color':'red'})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fval, pval, res = sms.het_goldfeldquandt(lr.resid, Xc)\n\nprint(\"The p-value is: \",pval)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the pvalue is above the significance level, hence we conclude that we fail to reject the null hypothesis which is the variance of errors is constant across the range of data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 5. Autocollinearity","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The Durbin Watson value of 2.088 in the model summary shows that there is no Autocollinearity.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Removing insignificant Features\nNow we try to remove the insignificant features whose pvalue is greater than 0.05 in the t-test performed in the statistical summary.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"while (len(X.columns)>0):\n    Xc1 = sm.add_constant(X)\n    ols = sm.OLS(y, Xc1)\n    model = ols.fit()\n    f = model.pvalues[1:].idxmax()\n    if (model.pvalues[1:].max()>0.05):\n        X = X.drop(f, axis=1)\n    else:\n        break\n\nprint(\"The final features are:\",X.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xc2 = sm.add_constant(X)\nols = sm.OLS(y, Xc2)\nlr = ols.fit()\nlr.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The Rsquare value is 0.75.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"error = lr.resid\nmse = np.mean(error**2)\nrmse = np.sqrt(mse)\nrmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The Root Mean Squared Error is 6056.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Regression - Machine Learning Approach","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Linear Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['charges'], axis = 1)\ny = df.charges\n\nX_train,X_test,y_train,y_test = train_test_split(X,y, random_state = 0)\nlr = LinearRegression().fit(X_train,y_train)\n\ny_train_pred = lr.predict(X_train)\ny_test_pred = lr.predict(X_test)\n\nprint(\"The score is:\",lr.score(X_test,y_test))\nprint(\"The RMSE for the training set is:\",np.sqrt(mean_squared_error(y_train, y_train_pred)))\nprint(\"The RMSE for the testing set is:\",np.sqrt(mean_squared_error(y_test, y_test_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### A little bit of Preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"quad = PolynomialFeatures (degree = 2)\nx_quad = quad.fit_transform(X)\n\nX_train,X_test,y_train,y_test = train_test_split(x_quad,y, random_state = 0)\n\nplr = LinearRegression().fit(X_train,y_train)\n\ny_train_pred = plr.predict(X_train)\ny_test_pred = plr.predict(X_test)\n\nrmseLinear = np.sqrt(mean_squared_error(y_test, y_test_pred))\n\nprint(\"The score is:\",plr.score(X_test,y_test))\nprint(\"The RMSE for the training set is:\",np.sqrt(mean_squared_error(y_train, y_train_pred)))\nprint(\"The RMSE for the testing set is:\",np.sqrt(mean_squared_error(y_test, y_test_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A little step of preprocessing can give us wonderful results. This shows the importance of preprocessing.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Feature Selection\nRecursive Feature Selection","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LinearRegression()\nrfe = RFE(lr, n_features_to_select=4)\nrfe.fit(X, y)\npd.DataFrame(rfe.ranking_, index=X.columns, columns=['Select'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hyperparameter Tuning is required as the number of features to select is not known. We will use GridSearchCV to tune the hyperparameters.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LinearRegression()\nparam_grid = [{'n_features_to_select':list(range(1,len(df.columns)+1))}]\n\nrfe = RFE(lr)\ngsearch = GridSearchCV(rfe, param_grid=param_grid, cv=3, return_train_score=True)\ngsearch.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(gsearch.best_params_)\npd.DataFrame(gsearch.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Putting the n_features_to_select value as best_params and building the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LinearRegression()\nrfe = RFE(lr, n_features_to_select=8)\nrfe.fit(X, y)\npd.DataFrame(rfe.ranking_, index=X.columns, columns=['Rank'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally after finding the best features, we move towards regularization methods.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Regularization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Lasso:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(x_quad, y, random_state = 0)\n\nlassoModel = Lasso(max_iter=5000)\nlasso = lassoModel.fit(X_train, y_train)\nlassoPred = lasso.predict(X_test)\nmseLasso = mean_squared_error(y_test, lassoPred)\nrmseLasso = mseLasso**(1/2)\n\nprint(\"The RMSE for the model is:\",rmseLasso)\nprint(\"The Rsquare for the model is:\",lasso.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The rmse is 4267 for Lasso Regression.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Ridge:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ridgeModel = Ridge(max_iter=5000)\nridge = ridgeModel.fit(X_train, y_train)\nridgePred = ridge.predict(X_test)\nmseRidge = mean_squared_error(y_test, ridgePred)\nrmseRidge = mseRidge**(1/2)\n\nprint(\"The RMSE for the model is:\",rmseRidge)\nprint(\"The Rsquare for the model is:\",ridge.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The RMSE is 4278 for Ridge Regression.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### ElasticNet","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"elasticNetModel = ElasticNet(alpha = 0.01, l1_ratio = 0.9, max_iter = 5000)\nElasticNet = elasticNetModel.fit(X_train, y_train)\nElasticNetPred = ElasticNet.predict(X_test)\nmseElasticNet = mean_squared_error(y_test, ElasticNetPred)\nrmseElasticNet = mseElasticNet**(1/2)\n\nprint(\"The RMSE for the model is:\",rmseElasticNet)\nprint(\"The Rsquare for the model is:\",ElasticNet.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The RMSE is 4278 for Ridge Regression.\n\n#### It is to be seen that there is almost negligible difference between the results of Lasso, Ridge and ElasticNet Regressions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"performanceData = pd.DataFrame({\"Regrssion\":[\"Linear\", \"Lasso\", \"Ridge\", \"Elasticnet\"], \n                                \"RMSE\":[rmseLinear, rmseLasso, rmseRidge, rmseElasticNet]})\nperformanceData","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}