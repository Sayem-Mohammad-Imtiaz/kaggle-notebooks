{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Alchohol Sales Prediction\n![alchohol](https://i0.wp.com/xtalks.com/wp-content/uploads/2020/04/drinks-e1585833542715.jpg?resize=1098%2C600&ssl=1)\n\nIn this notebook we are going to predict Alchohol Sales Time Series Data using LSTM Model in PyTorch.\n\n[Dataset Link](https://www.kaggle.com/bulentsiyah/for-simple-exercises-time-series-forecasting)\n\nThanks [@bulentsiyah](https://www.kaggle.com/bulentsiyah/) for the Dataset"},{"metadata":{},"cell_type":"markdown","source":"# Intro to LSTMs\n> Long short-term memory (LSTM) is a recurrent neural network (RNN) architecture used in the field of deep learning LSTM networks are well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series.\n\n                                                                    Wikipedia\n\nLSTM networks are a type of recurrent neural network capable of learning order dependence in sequence prediction problems.\n\nWe'll not be going into details of LSTM. If you're curious follow [this link](https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/)"},{"metadata":{},"cell_type":"markdown","source":"# Standard Imports\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style(\"darkgrid\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = pd.read_csv(\"../input/for-simple-exercises-time-series-forecasting/Alcohol_Sales.csv\",index_col=0,parse_dates=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.plot(figsize=(16,5),grid=True,legend = False);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe that the Sales has a pattern for each year but overall Sales has increased from 1994 to 2019"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = sales.index\nY = sales['S4248SM144NCEN'].values.astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have stored the Dates in special data type defined in Numpy"},{"metadata":{},"cell_type":"markdown","source":"# Defining Training and Testing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_size = 12\ntrain_set = Y[:-test_size]\ntest_set = Y[-test_size:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Normalizing the Train Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler(feature_range=(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_norm = scaler.fit_transform(train_set.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_norm = train_norm.flatten()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_norm = torch.FloatTensor(train_norm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing the Train Data"},{"metadata":{},"cell_type":"markdown","source":"We'll be diving our Dataset into windows of size 12<br>\nAfter our model has been trained, we will predict the Sales for the next 12 months i.e. an Year"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_windows(data,ws):\n    out = []\n    L = len(data)\n    for i in range(L-ws):\n        out.append((data[i:i+ws],data[i+ws:i+ws+1]))\n    return out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the code above, we are inserting a tuples into an array. Each tuple has Sales data for the given window size which in our case is an Year. We are also inserting the Sales for the next immediate month of the given window "},{"metadata":{"trusted":true},"cell_type":"code","source":"window_size = 12\ntrain_data = get_windows(train_norm,window_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Defining and Instatiating the LSTM Model,Optimizing and Loss Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"class LSTM(nn.Module):\n    def __init__(self,in_size = 1,hidden_size = 100,out_size = 1):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.lstm = nn.LSTM(in_size,hidden_size)\n        self.linear = nn.Linear(hidden_size,out_size)\n        self.hidden = (torch.zeros(1,1,self.hidden_size).cuda(),\n                       torch.zeros(1,1,self.hidden_size).cuda())\n    def forward(self,X):\n        lstm_out,self.hidden = self.lstm(X.view(len(X),1,-1),self.hidden)\n        pred = self.linear(lstm_out.view(len(X),-1))\n        return pred[-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LSTM().cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(),lr = 0.001)\ncriterion = nn.MSELoss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training the Model\nHere we are training our model based on the Data excluding the last window. We'll be predicting the last window of our dataset which is also our Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nstart = time.time()\nepochs = 100\nfor i in range(epochs):\n    for X_train,Y_train in train_data:\n        X_train = X_train.cuda()\n        Y_train = Y_train.cuda()\n        optimizer.zero_grad()\n        model.hidden = (torch.zeros(1,1,model.hidden_size).cuda(),\n                        torch.zeros(1,1,model.hidden_size).cuda())\n        Y_pred = model(X_train)\n        loss = criterion(Y_pred,Y_train)\n        loss.backward()\n        optimizer.step()\n    print(f\"Epoch : {i+1} LOSS : {loss.item():.7f}\")\nend = time.time()\ndur = end-start\nprint(f\"Duration : {int(dur/60)} minutes and {int(dur%60)} seconds\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Testing on the last Window Size\nIn the code given below, we are predicting the values based off of the last window and then adding this predicted value to the previous window thus sliding the window forward"},{"metadata":{"trusted":true},"cell_type":"code","source":"future = 12\npreds = train_norm[-window_size:].tolist()\nmodel.eval()\nfor i in range(future):\n    X_test = torch.FloatTensor(preds[-window_size:]).cuda()\n    with torch.no_grad():\n        model.hidden = (torch.zeros(1,1,model.hidden_size).cuda(),\n                      torch.zeros(1,1,model.hidden_size).cuda())\n        preds.append(model(X_test).item())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds[-window_size:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that our predicted data is normalized. Let's invert the normalization"},{"metadata":{},"cell_type":"markdown","source":"## Inverting the Normalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"true_predictions = scaler.inverse_transform(np.array(preds[-window_size:]).reshape(-1,1))\ntrue_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales['S4248SM144NCEN'][-12:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dates = np.arange('2018-02-01', '2019-02-01', dtype='datetime64[M]').astype('datetime64[D]')\ndates","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,7))\nplt.grid(True)\nplt.plot(sales['S4248SM144NCEN'],label = 'Original')\nplt.plot(dates,true_predictions,label = 'Predicted')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,7))\nplt.grid(True)\nplt.plot(sales['S4248SM144NCEN']['2018-01-01':],label = \"Original\")\nplt.plot(dates,true_predictions,label = \"Predicted\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The predicted Data closely resembles our Original Data"},{"metadata":{},"cell_type":"markdown","source":"## Training with the entire Dataset\nFor predicting the next Year Sales we are going to train our model over the entire Dataset this time! "},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_norm = scaler.fit_transform(Y.reshape(-1,1))\nY_norm = torch.FloatTensor(Y_norm).view(-1)\nfull_train_data = get_windows(Y_norm,window_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nepochs = 100\nmodel.train()\nfor i in range(epochs):\n    for X_train,Y_train in full_train_data:\n        X_train = X_train.cuda()\n        Y_train = Y_train.cuda()\n        optimizer.zero_grad()\n        model.hidden = (torch.zeros(1,1,model.hidden_size).cuda(),\n                        torch.zeros(1,1,model.hidden_size).cuda())\n        Y_pred = model(X_train)\n        loss = criterion(Y_pred,Y_train)\n        loss.backward()\n        optimizer.step()\n    print(f\"Epoch {i+1} LOSS : {loss.item():.8f}\")\nend = time.time()\nprint(f\"Train Duration {int((end-start)/60)} minutes {int((end-start)%60)} seconds\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predicting into the Unknown Future\nUsing the same approach as explained earlier we are going to predict the Sales for the next year"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()\npreds = Y_norm[-window_size:].tolist()\nfor i in range(future):\n    X_test = torch.FloatTensor(preds[-window_size:]).cuda()\n    with torch.no_grad():\n        model.hidden = (torch.zeros(1,1,model.hidden_size).cuda(),\n                      torch.zeros(1,1,model.hidden_size).cuda())\n        preds.append(model(X_test).item())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds[-window_size:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inverting the normalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"true_predictions = scaler.inverse_transform(np.array(preds[-window_size:]).reshape(-1,1))\ntrue_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true_predictions = true_predictions.flatten()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dates = np.arange('2019-02-01', '2020-02-01', dtype='datetime64[M]').astype('datetime64[D]')\ndates","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,6))\nplt.grid(True)\nplt.plot(sales['S4248SM144NCEN'])\nplt.plot(dates,true_predictions)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,6))\nplt.grid(True)\nplt.plot(sales['S4248SM144NCEN']['2017-01-01':])\nplt.plot(dates,true_predictions)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that the gap here is due to the fact that both data are from different sources! Actually they are continous"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}