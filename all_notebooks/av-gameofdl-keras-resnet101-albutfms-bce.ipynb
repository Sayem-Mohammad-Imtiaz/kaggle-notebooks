{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install albumentations > /dev/null\n!git clone https://github.com/qubvel/efficientnet.git\n!pip install image-classifiers==0.2.2 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport numpy as np\nfrom skimage.io import imread\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport keras\nimport pandas as pd\nfrom PIL import Image\nimport cv2\n\nfrom keras.applications import Xception,ResNet50\nfrom keras.applications.imagenet_utils import decode_predictions\n\nfrom efficientnet import EfficientNetB0,EfficientNetB3\nfrom classification_models.resnet import ResNet101,preprocess_input\n\n\n# from efficientnet import center_crop_and_resize, preprocess_input\nfrom keras.optimizers import SGD, Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataGenerator(keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, list_IDs, labels,augmentations, batch_size=32, dim=(32,32,32), n_channels=3,\n                 n_classes=5, shuffle=True):\n        'Initialization'\n        self.dim = dim\n        self.batch_size = batch_size\n        self.labels = labels\n        self.list_IDs = list_IDs\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.shuffle = shuffle\n        self.path = '../input/game-of-deep-learning-ship-datasets/train/images/'\n        self.augment = augmentations\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n\n        # Generate data\n        X, y = self.data_generation(list_IDs_temp)\n\n        return np.stack([\n            self.augment(image=x)[\"image\"] for x in X\n        ], axis=0), np.array(y)\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n#         print(X.shape,self.dim)\n        y = np.empty((self.batch_size), dtype=int)\n\n        # Generate data\n        for i, ID in enumerate(list_IDs_temp):\n            \n            im = np.array(Image.open(self.path+ID))\n            if len(im.shape)==2:\n                im = cv2.cvtColor(im,cv2.COLOR_GRAY2RGB)\n\n#             # Resize sample\n            X[i,] = cv2.resize(im,(self.dim[0],self.dim[1]))\n\n            # Store class\n            y[i] = self.labels.loc[ID].category\n\n#         print(X.shape)\n        return np.uint8(X), keras.utils.to_categorical(y, num_classes=self.n_classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\nfrom albumentations import (\n    Compose, HorizontalFlip, CLAHE, HueSaturationValue,\n    RandomBrightness, RandomContrast, RandomGamma,OneOf,\n    ToFloat, ShiftScaleRotate,GridDistortion, ElasticTransform, JpegCompression, HueSaturationValue,\n    RGBShift, RandomBrightness, RandomContrast, Blur, MotionBlur, MedianBlur, GaussNoise,CenterCrop,\n    IAAAdditiveGaussianNoise,GaussNoise,Cutout\n)\n\nAUGMENTATIONS_TRAIN = Compose([\n    HorizontalFlip(p=0.5),\n    RandomContrast(limit=0.2, p=0.5),\n    RandomGamma(gamma_limit=(80, 120), p=0.5),\n    RandomBrightness(limit=1.2, p=0.5),\n    HueSaturationValue(hue_shift_limit=5, sat_shift_limit=20,\n                       val_shift_limit=10, p=.5),\n#     CenterCrop(height=128, width=128, p=0.5),\n    Cutout(p=0.5),\n    OneOf([\n            MotionBlur(p=0.2),\n            MedianBlur(blur_limit=3, p=0.1),\n            Blur(blur_limit=3, p=0.1),\n        ], p=0.3),\n    OneOf([\n            IAAAdditiveGaussianNoise(),\n            GaussNoise(),\n        ], p=0.2),\n    # CLAHE(p=1.0, clip_limit=2.0),\n    ShiftScaleRotate(\n        shift_limit=0.0625, scale_limit=0.1, \n        rotate_limit=15, border_mode=cv2.BORDER_REFLECT_101, p=0.5), \n   # ToFloat(max_value=1)\n],p=1)\n\n\nAUGMENTATIONS_TEST = Compose([\n    # CLAHE(p=1.0, clip_limit=2.0),\n  #  ToFloat(max_value=1)\n],p=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TestDataGenerator(keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, list_IDs,augmentations, batch_size=32, dim=(32,32,32), n_channels=3,\n                 n_classes=5, shuffle=False,flip=False,path=None):\n        'Initialization'\n        self.dim = dim\n        self.batch_size = batch_size\n        self.list_IDs = list_IDs\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.shuffle = shuffle\n        if path is not None:\n            self.path = path\n        else:\n            self.path = '../input/avgameofdltestjpg/test-jpg/test-jpg/'\n        self.on_epoch_end()\n        self.augment = augmentations\n        self.flip = flip\n#         print(len(self.list_IDs))\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.ceil(len(self.list_IDs) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n#         if  im_left < self.batch_size:\n#             print('*','index',index,len(self.list_IDs) - (index)*self.batch_size )\n#             indexes = self.indexes[(index)*self.batch_size:]            \n#         elif im_left<0: return \n#         else:\n#             indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        indexes = self.indexes[index*self.batch_size:min((index+1)*self.batch_size,len(self.list_IDs))]\n\n        # Find list of IDs\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n\n        # Generate data\n        X = self.data_generation(list_IDs_temp)\n        \n        return np.stack([\n                    self.augment(image=x)[\"image\"] for x in X\n                ], axis=0)\n#         return X\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n\n    def data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.empty((len(list_IDs_temp), *self.dim, self.n_channels))\n\n        # Generate data\n        for i, ID in enumerate(list_IDs_temp):\n            \n            im = np.array(Image.open(self.path+ID))\n            if len(im.shape)==2:\n                im = cv2.cvtColor(im,cv2.COLOR_GRAY2RGB)\n\n            # Resize sample\n            if self.flip:\n                X[i,] = np.fliplr(cv2.resize(im,(self.dim[0],self.dim[1])))\n            else:\n                X[i,] = cv2.resize(im,(self.dim[0],self.dim[1]))\n\n\n        return np.uint8(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/game-of-deep-learning-ship-datasets/train/train.csv')\ntrain['category'] = train['category'] - 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import plot_model\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Activation\nfrom keras.layers import Dropout\nfrom keras.layers import Maximum\nfrom keras.layers import ZeroPadding2D\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras import regularizers\nfrom keras.layers import BatchNormalization\nfrom keras.optimizers import Adam, SGD\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.utils import to_categorical\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom skimage.transform import resize as imresize\nfrom tqdm import tqdm\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_loss_acc(history):\n    plt.figure(figsize=(20,7))\n    plt.subplot(1,2,1)\n    plt.plot(history.history['loss'][1:])    \n    plt.plot(history.history['val_loss'][1:])    \n    plt.title('model loss')    \n    plt.ylabel('val_loss')    \n    plt.xlabel('epoch')    \n    plt.legend(['Train','Validation'], loc='upper left')\n    \n    plt.subplot(1,2,2)\n    plt.plot(history.history['acc'][1:])\n    plt.plot(history.history['val_acc'][1:])\n    plt.title('Model Accuracy')\n    plt.ylabel('val_acc')\n    plt.xlabel('epoch')\n    plt.legend(['Train','Validation'], loc='upper left')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/game-of-deep-learning-ship-datasets/test_ApKoW4T.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K\nimport tensorflow as tf\n'''\nCompatible with tensorflow backend\n'''\ndef focal_loss(gamma=2., alpha=.25):\n    def focal_loss_fixed(y_true, y_pred):\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n    return focal_loss_fixed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SnapshotCallbackBuilder:\n    def __init__(self, nb_epochs, nb_snapshots, init_lr=0.1):\n        self.T = nb_epochs\n        self.M = nb_snapshots\n        self.alpha_zero = init_lr\n\n    def get_callbacks(self, model_prefix='Model'):\n\n        callback_list = [\n#             callbacks.ModelCheckpoint(\"./keras.model\",monitor='val_loss', \n#                                    mode = 'min', save_best_only=True, verbose=1),\n            swa,\n            callbacks.LearningRateScheduler(schedule=self._cosine_anneal_schedule)\n        ]\n\n        return callback_list\n\n    def _cosine_anneal_schedule(self, t):\n        cos_inner = np.pi * (t % (self.T // self.M))  # t - 1 is used when t has 1-based indexing.\n        cos_inner /= self.T // self.M\n        cos_out = np.cos(cos_inner) + 1\n        return float(self.alpha_zero / 2 * cos_out)\n\nimport keras.callbacks as callbacks\n\nclass SWA(keras.callbacks.Callback):\n    \n    def __init__(self, filepath, swa_epoch):\n        super(SWA, self).__init__()\n        self.filepath = filepath\n        self.swa_epoch = swa_epoch \n    \n    def on_train_begin(self, logs=None):\n        self.nb_epoch = self.params['epochs']\n        print('Stochastic weight averaging selected for last {} epochs.'\n              .format(self.nb_epoch - self.swa_epoch))\n        \n    def on_epoch_end(self, epoch, logs=None):\n        \n        if epoch == self.swa_epoch:\n            self.swa_weights = self.model.get_weights()\n            \n        elif epoch > self.swa_epoch:    \n            for i in range(len(self.swa_weights)):\n                self.swa_weights[i] = (self.swa_weights[i] * \n                    (epoch - self.swa_epoch) + self.model.get_weights()[i])/((epoch - self.swa_epoch)  + 1)  \n\n        else:\n            pass\n        \n    def on_train_end(self, logs=None):\n        self.model.set_weights(self.swa_weights)\n        print('Final model parameters set to stochastic weight average.')\n        self.model.save_weights(self.filepath)\n        print('Final stochastic averaged weights saved to file.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers.core import Layer\nfrom keras.engine import InputSpec\nfrom keras import backend as K\ntry:\n    from keras import initializations\nexcept ImportError:\n    from keras import initializers as initializations\n\nclass Scale(Layer):\n    '''Learns a set of weights and biases used for scaling the input data.\n    the output consists simply in an element-wise multiplication of the input\n    and a sum of a set of constants:\n        out = in * gamma + beta,\n    where 'gamma' and 'beta' are the weights and biases larned.\n    # Arguments\n        axis: integer, axis along which to normalize in mode 0. For instance,\n            if your input tensor has shape (samples, channels, rows, cols),\n            set axis to 1 to normalize per feature map (channels axis).\n        momentum: momentum in the computation of the\n            exponential average of the mean and standard deviation\n            of the data, for feature-wise normalization.\n        weights: Initialization weights.\n            List of 2 Numpy arrays, with shapes:\n            `[(input_shape,), (input_shape,)]`\n        beta_init: name of initialization function for shift parameter\n            (see [initializations](../initializations.md)), or alternatively,\n            Theano/TensorFlow function to use for weights initialization.\n            This parameter is only relevant if you don't pass a `weights` argument.\n        gamma_init: name of initialization function for scale parameter (see\n            [initializations](../initializations.md)), or alternatively,\n            Theano/TensorFlow function to use for weights initialization.\n            This parameter is only relevant if you don't pass a `weights` argument.\n    '''\n    def __init__(self, weights=None, axis=-1, momentum = 0.9, beta_init='zero', gamma_init='one', **kwargs):\n        self.momentum = momentum\n        self.axis = axis\n        self.beta_init = initializations.get(beta_init)\n        self.gamma_init = initializations.get(gamma_init)\n        self.initial_weights = weights\n        super(Scale, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.input_spec = [InputSpec(shape=input_shape)]\n        shape = (int(input_shape[self.axis]),)\n\n        # Compatibility with TensorFlow >= 1.0.0\n        self.gamma = K.variable(self.gamma_init(shape), name='{}_gamma'.format(self.name))\n        self.beta = K.variable(self.beta_init(shape), name='{}_beta'.format(self.name))\n        #self.gamma = self.gamma_init(shape, name='{}_gamma'.format(self.name))\n        #self.beta = self.beta_init(shape, name='{}_beta'.format(self.name))\n        self.trainable_weights = [self.gamma, self.beta]\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n\n    def call(self, x, mask=None):\n        input_shape = self.input_spec[0].shape\n        broadcast_shape = [1] * len(input_shape)\n        broadcast_shape[self.axis] = input_shape[self.axis]\n\n        out = K.reshape(self.gamma, broadcast_shape) * x + K.reshape(self.beta, broadcast_shape)\n        return out\n\n    def get_config(self):\n        config = {\"momentum\": self.momentum, \"axis\": self.axis}\n        base_config = super(Scale, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K\nfrom keras.callbacks import  EarlyStopping, Callback\nfrom keras.utils import np_utils\nfrom keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom keras import optimizers\nfrom keras.models import Sequential, Model\nfrom keras.layers import  Input, Conv2D, add , MaxPooling2D,Activation,Dropout,Flatten,Dense,BatchNormalization,AveragePooling2D,ZeroPadding2D,merge,Reshape\nfrom keras.optimizers import Adam, SGD\nimport h5py\nimport sys\nsys.setrecursionlimit(3000)\n\n### Image Flow Generator\n\nclass resnet101:\n    def __init__ (self, width,height,channel,num_classes):\n        self.width = width\n        self.height = height\n        self.channel = channel\n        self.eps = 1.1e-5\n        self.num_classes = num_classes\n        self.bn_axis = 3\n\n    def identity_block(self, input_tensor, kernel_size, filters, stage, block,strides=(2,2)):\n        '''\n        Identity Block : This block create convolutional layers that has no convolutional layer at shortcut, where shortcut is used if __name__ == '__main__':\n            refrence to the paper describing Resnet Models : https://arxiv.org/pdf/1512.03385.pdf .\n        #Args:\n            input_tensor: input tensor\n            kernel_size : (default value is 3 ) , the kernel size of middle convolutional layer in the Block\n            filters : lists of integer , Number of filters\n            stage : integer , current state label , used for generating layer names\n            block: current block label , used for generating layer names\n       #Output : a tensor flow\n         '''\n\n        nb_filter1, nb_filter2, nb_filter3 = filters\n\n        conv_name_base = 'res101' + str(stage) + block + '_branch'\n        bn_name_base = 'bn' + str(stage) + block + '_branch'\n        scale_name_base = 'scale' + str(stage) + block + '_branch'\n\n        t = Conv2D(nb_filter1,(1,1), name=conv_name_base + '2a', use_bias = False)(input_tensor)\n        t = BatchNormalization(epsilon=self.eps, axis = self.bn_axis, name= bn_name_base + '2a' )(t)\n        t = Scale(axis = self.bn_axis, name=scale_name_base + '2a')(t)\n        t = Activation('relu', name=conv_name_base + '2a_relu' )(t)\n\n        t = ZeroPadding2D((1,1),name=conv_name_base + '2b_zeropadding')(t)\n        t = Conv2D(nb_filter2,(kernel_size,kernel_size), use_bias=False,name=conv_name_base + '2b' )(t)\n        t = BatchNormalization(epsilon=self.eps, axis= self.bn_axis, name=bn_name_base + '2b')(t)\n        t = Scale(axis = self.bn_axis, name=scale_name_base + '2b')(t)\n        t = Activation('relu', name=conv_name_base + '2b_relu')(t)\n\n        t = Conv2D(nb_filter3,(1,1), use_bias=False,name=conv_name_base + '2c' )(t)\n        t = BatchNormalization(epsilon=self.eps, axis= self.bn_axis, name=bn_name_base + '2c')(t)\n        t = Scale(axis = self.bn_axis, name=scale_name_base + '2c')(t)\n\n        t = add([t,input_tensor])\n        t = Activation('relu', name='res' + str(stage) + block +'_relu')(t)\n        return t\n\n    def conv_block(self, input_tensor, kernel_size, filters, stage, block,strides=(2,2)):\n        '''\n        Conv Block : This block create convolutional layers that has a convolutional layer at shortcut, where shortcut is used if __name__ == '__main__':\n            refrence to the paper describing Resnet Models : https://arxiv.org/pdf/1512.03385.pdf .\n        #Args:\n            input_tensor: input tensor\n            kernel_size : (default value is 3 ) , the kernel size of middle convolutional layer in the Block\n            filters : lists of integer , Number of filters\n            stage : integer , current state label , used for generating layer names\n            block: current block label , used for generating layer names\n       #Output : a tensor flow\n         '''\n\n        nb_filter1, nb_filter2, nb_filter3 = filters\n\n        conv_name_base = 'res101' + str(stage) + block + '_branch'\n        bn_name_base = 'bn' + str(stage) + block + '_branch'\n        scale_name_base = 'scale' + str(stage) + block + '_branch'\n\n        t = Conv2D(nb_filter1,(1,1), strides= strides , use_bias=False, name=conv_name_base + '2a')(input_tensor)\n        t = BatchNormalization(epsilon=self.eps, axis = self.bn_axis, name= bn_name_base + '2a' )(t)\n        t = Scale(axis = self.bn_axis, name=scale_name_base + '2a')(t)\n        t = Activation('relu', name=conv_name_base + '2a_relu' )(t)\n\n        t = ZeroPadding2D((1,1),name=conv_name_base + '2b_zeropadding')(t)\n        t = Conv2D(nb_filter2,(kernel_size,kernel_size), use_bias=False,name=conv_name_base + '2b' )(t)\n        t = BatchNormalization(epsilon=self.eps, axis= self.bn_axis, name=bn_name_base + '2b')(t)\n        t = Scale(axis = self.bn_axis, name=scale_name_base + '2b')(t)\n        t = Activation('relu', name=conv_name_base + '2b_relu')(t)\n\n        t = Conv2D(nb_filter3,(1,1), use_bias=False,name=conv_name_base + '2c' )(t)\n        t = BatchNormalization(epsilon=self.eps, axis= self.bn_axis, name=bn_name_base + '2c')(t)\n        t = Scale(axis = self.bn_axis, name=scale_name_base + '2c')(t)\n\n        shortcut = Conv2D(nb_filter3, (1,1), strides=strides,use_bias=False,  name=conv_name_base + '1' )(input_tensor)\n        shortcut = BatchNormalization(epsilon=self.eps, axis= self.bn_axis, name=bn_name_base + '1')(shortcut)\n        shortcut = Scale(axis=self.bn_axis,  name=scale_name_base + '1')(shortcut)\n\n        t = add([t,shortcut])\n        t = Activation('relu', name='res' + str(stage) + block +'_relu')(t)\n        return t\n\n\n    def ResNet101(self,color_type= 1, num_classes = None):\n        '''\n            ResNet101_v2 Model for Keras\n            436 are the Number of layers of model() that were frozen . From  Input layer (conv1) to conv4_x are frozen and conv5_x and\n            average pool are trained on input image data .\n        '''\n        img_input = Input(shape=(self.width,self.height, 3 ), name='data')\n\n        t = ZeroPadding2D((3,3), name='conv1_zeropadding')(img_input)\n        t = Conv2D(64, (7,7), strides = (2,2) , use_bias = False , name='conv1' )(t)\n        t = BatchNormalization(epsilon = self.eps, axis= self.bn_axis, name='bn_conv1')(t)\n        t = Scale(axis = self.bn_axis, name='scale_conv1')(t)\n        t = Activation('relu', name= 'conv1_relu')(t)\n        t = MaxPooling2D((3,3),strides = (2,2) , name= 'pool1')(t)\n\n        t = self.conv_block(t, 3, [64,64,256], stage=2 , block= 'a' , strides=(1,1))\n        t = self.identity_block(t,3,[64,64,256], stage=2, block='b')\n        t = self.identity_block(t,3,[64,64,256], stage=2, block='c')\n\n        t = self.conv_block(t, 3, [128,128,512], stage=3 , block= 'a' )\n        for i in range(1,4):\n            t = self.identity_block(t,3,[128,128,512], stage=3 , block = 'b'+str(i))\n\n        t = self.conv_block(t,3,[256,256,1024], stage=4, block='a')\n        for i in range(1,23):\n            t = self.identity_block(t,3,[256,256,1024], stage = 4 , block='b' + str(i))\n\n        t = self.conv_block(t,3,[512,512,2048], stage= 5, block= 'a')\n        t = self.identity_block(t,3, [512,512,2048], stage=5, block= 'b')\n        t = self.identity_block(t,3, [512,512,2048], stage=5, block= 'c')\n\n        x_fc = AveragePooling2D((7,7), name= 'avg_pool')(t)\n        x_fc = Flatten()(x_fc)\n        x_fc = Dense(1000, activation='softmax', name='fc1000')(x_fc)\n\n\n        #Pretrained weights\n#         weights_path = '../resnet101_weights_tf.h5'\n\n\n        # Truncate and replace softmax layer for transfer learning\n        # The method below works since pre-trained weights are stored in layers but not in the model\n\n        x_newfc = AveragePooling2D((7,7), name='avg_pool')(t)\n        x_newfc = Flatten()(x_newfc)\n        x_newfc = Dense(self.num_classes, activation='softmax', name= 'fc8')(x_newfc)\n\n        model = Model(img_input, x_newfc)\n\n#         model.load_weights(weights_path, by_name= True)\n\n        for i in range(436):\n            model.layers[i].trainable = False\n\n        sgd = SGD(lr= 1e-3, decay=1e-6, momentum=0.9, nesterov= True)\n        model.compile(optimizer=sgd , loss='categorical_crossentropy', metrics=['accuracy'])\n\n        return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications import resnet50","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dense, Activation, Flatten, Dropout,GlobalAveragePooling2D\nfrom keras.models import Sequential, Model\n\ndef build_finetune_model(base_model, dropout, fc_layers, num_classes):\n#     for layer in base_model.layers:\n#         layer.trainable = False\n\n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n#     print(x.shape)\n#     x = Flatten()(x)\n    for fc in fc_layers:\n        # New FC layer, random init\n        x = Dense(fc, activation='relu')(x) \n        x = Dropout(dropout)(x)\n\n    # New softmax layer\n    predictions = Dense(num_classes, activation='softmax')(x) \n    \n    finetune_model = Model(inputs=base_model.input, outputs=predictions)\n\n    return finetune_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skf = StratifiedKFold(5)\nall_preds,fold = [],1\n\nHEIGHT = 128\nWIDTH = 128\n\ninput_shape=(HEIGHT, WIDTH, 3)\n\nFC_LAYERS = [2048]\ndropout = 0.25\nepochs = 60\nswa = SWA('./keras_swa.model',epochs-3)\n\noof_preds = np.zeros((len(train), 6))\n\n\n# Parameters\nparams = {'dim': (256,256),\n          'batch_size': 32,\n          'n_classes': 5,\n          'n_channels': 3,\n          'shuffle': True}\n\nfor train_indices,val_indices in skf.split(train.image.values,train.category.values):\n    \n    print('*'*50)\n    print('Fold',fold)\n    fold += 1\n    \n#     # Datasets\n    partition_train = train.loc[train_indices].image.values\n    labels_train = train.loc[train_indices].set_index('image')\n    \n    partition_valid = train.loc[val_indices].image.values\n    labels_valid = train.loc[val_indices].set_index('image')\n\n#     # Generators\n    training_generator = DataGenerator(partition_train, labels_train,augmentations=AUGMENTATIONS_TRAIN, **params)\n    validation_generator = DataGenerator(partition_valid, labels_valid,augmentations=AUGMENTATIONS_TEST, **params)\n    \n#     base_model = ResNet101(weights='imagenet',\n#                                 include_top=False,\n#                                 input_shape=(HEIGHT, WIDTH, 3))\n\n#     res101 = resnet101(256,256,3,5)\n#     finetune_model = res101.ResNet101(3,5)\n    base_model = resnet50.ResNet50(include_top=False,input_shape=(256,256,3))\n\n\n    finetune_model = build_finetune_model(base_model, \n                                          dropout=dropout, \n                                          fc_layers=FC_LAYERS, \n                                          num_classes=5)\n    \n    finetune_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    \n#     for layer in finetune_model.layers[:-3]:\n#         layer.trainable = False\n        \n#     for layer in finetune_model.layers[-3:]:\n#         layer.trainable = True\n    \n    snapshot = SnapshotCallbackBuilder(nb_epochs=epochs,nb_snapshots=1,init_lr=1e-3)\n\n\n    history = finetune_model.fit_generator(generator=training_generator,\n                                            validation_data=validation_generator,\n                                            use_multiprocessing=True,\n                                            workers=8,epochs=epochs,verbose=0,callbacks=snapshot.get_callbacks())\n    \n    finetune_model.load_weights('./keras_swa.model')\n    \n    try:\n        plot_loss_acc(history)\n    except:\n        print('no plot')\n    \n    test_generator = TestDataGenerator(test.image.values,augmentations=AUGMENTATIONS_TEST, **params)\n    test_generator_flipped = TestDataGenerator(test.image.values,augmentations=AUGMENTATIONS_TEST, **params,\n                                               flip=True)\n    \n    \n    validation_generator = TestDataGenerator(partition_valid,augmentations=AUGMENTATIONS_TEST, **params,\n                                            path='../input/game-of-deep-learning-ship-datasets/train/images/')\n    validation_generator_flipped = TestDataGenerator(partition_valid,augmentations=AUGMENTATIONS_TEST,\n                                             **params,flip=True,\n                                                path='../input/game-of-deep-learning-ship-datasets/train/images/')\n    preds1 = []\n    for im in validation_generator:    \n        preds1.extend(finetune_model.predict(im))\n        \n    preds2 = []\n    for im in validation_generator_flipped:    \n        preds2.extend(finetune_model.predict(im))\n        \n    preds1 = np.array(preds1)\n    preds2 = np.array(preds2)\n    \n    preds = (preds1 + preds2)/2\n    \n    oof_preds[val_indices, :5] = preds\n    oof_preds[val_indices, 5] = train.loc[val_indices,'image'].map(lambda x: x[:-4])\n    \n    preds1 = []\n    for im in test_generator:    \n        preds1.extend(finetune_model.predict(im))\n        \n    preds2 = []\n    for im in test_generator_flipped:    \n        preds2.extend(finetune_model.predict(im))\n        \n    preds1 = np.array(preds1)\n    preds2 = np.array(preds2)\n    \n    preds = (preds1 + preds2)/2\n    \n    pd.DataFrame(preds).to_csv(f'raw_preds_{fold}.csv',index=False)\n        \n    all_preds.append(preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pd.DataFrame(preds).to_csv('raw_preds_fold5.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(oof_preds).to_csv('oof_preds.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = np.ones((2680,5))\nfor preds_fold in all_preds:\n    preds = preds*preds_fold\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = preds**(1/skf.n_splits)\ntest = pd.read_csv('../input/game-of-deep-learning-ship-datasets/test_ApKoW4T.csv')\nlabelled_preds = np.argmax(preds,1)+1\n# fnames = [f.name for f in learn.data.test_ds.items]\nfnames = test.image.values\ndf = pd.DataFrame({'image':fnames, 'category':labelled_preds}, columns=['image', 'category'])\ndf.to_csv('submission_gm.csv', index=False)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['class1_prob'] = preds[:,0].numpy()\ndf['class2_prob'] = preds[:,1].numpy()\ndf['class3_prob'] = preds[:,2].numpy()\ndf['class4_prob'] = preds[:,3].numpy()\ndf['class5_prob'] = preds[:,4].numpy()\ndf.to_csv('raw_prob.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -rf efficientnet/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}