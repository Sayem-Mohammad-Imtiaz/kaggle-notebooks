{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Analysing New York  Technology, Data & Innovation Industry\n### This kernel, we will examine job posting for New York  Technology, Data & Innovation Industry and its popular fields, Jobs Types (Part-Time or Full-Time ), Most demand Positions , location, salary , skills which are preferred and  Minimum Qual Requirements"},{"metadata":{},"cell_type":"markdown","source":"# Importing Libs "},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport re \nimport nltk as nlp\nfrom plotly.offline import iplot\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading data and getting information from it"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/nyc-jobs.csv',sep=',')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf=df[df['Job Category']=='Technology, Data & Innovation'].reset_index()\ndf = df.drop(['index'],axis=1)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part time or Full time ? \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='Full-Time/Part-Time indicator', data=df)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='Salary Frequency', data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Most demand Positions , locations, salaries and Units"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['civil_service'] =df['Civil Service Title']\n\n\ngroups = df.groupby(['civil_service']).size()\nplt.figure(figsize=(10, 10))\ngroups.plot.barh()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['business_title']=df['Business Title']\n\n\ngroups = df.groupby(['business_title']).size()\nplt.figure(figsize=(10, 30))\ngroups.plot.barh()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['location']=df['Work Location']\ngroups = df.groupby(['location']).size()\nplt.figure(figsize=(10, 10))\ngroups.plot.barh()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['workUnit']=df['Division/Work Unit']\ngroups = df.groupby(['workUnit']).size()\nplt.figure(figsize=(10, 15))\ngroups.plot.barh()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['mean_salary_from']=df['Salary Range From']\na=df.groupby('business_title')['mean_salary_from'].mean()\nplt.figure(figsize=(10, 30))\na.plot.barh()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Which skills are preferred ? \n# What is Minimum Qual Requirements ?\n### Lets find \n\n#### Firstly, we make data cleaning, we use regular expression for this\n### select random text and convert it to lower case"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['skills']=df['Preferred Skills']\n\n\nfirst_description=df.skills[4]\ndescription=re.sub(\"[^a-zA-Z]\",\" \",first_description)\n\ndescription = description.lower()\nprint(\"First value :::::::::::::: {0}   \\nSecond value :::::::::  {1}\".format(first_description,description))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## We drop unnecessary characters like â€¢  and we converted text to lower case \n## Now we apply this action to  all data.\n### Firstly, we create a list in order to keep all words in text \n### Secondly, we apply lemmatization "},{"metadata":{"trusted":true},"cell_type":"code","source":"description_list=[]\nimport nltk\nfor description in df.skills:\n     description=str(description)\n     description = description.lower()\n     description = nltk.word_tokenize(description)\n     lemma = nlp.WordNetLemmatizer()\n     description = [lemma.lemmatize(word) for word in description]\n     description =\" \".join(description)\n     description_list.append(description)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nmax_features=60\ncount_vectroizer =CountVectorizer(max_features=max_features,stop_words=\"english\")# -----> stopwords unmeaning words\nsparce_matrix = count_vectroizer.fit_transform(description_list).toarray()\ndictionary = count_vectroizer.vocabulary_.items()  \nvocab = []\ncount = []\nfor key, value in dictionary:\n    vocab.append(key)\n    count.append(value)\nvocab_bef_stem = pd.Series(count, index=vocab)\nvocab_bef_stem = vocab_bef_stem.sort_values(ascending=False)\ntop_vacab = vocab_bef_stem.head(50)\ntop_vacab.plot(kind = 'barh', figsize=(10,20))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## We make this process again for Minimum Qual Requirements "},{"metadata":{"trusted":true},"cell_type":"code","source":"description_list2=[]\ndf['minR']=df['Minimum Qual Requirements']\nfor description2 in df.minR:\n     description2=str(description2)\n     description2 = description2.lower()\n     description2 = nltk.word_tokenize(description2)\n     lemma2 = nlp.WordNetLemmatizer()\n     description2 = [lemma2.lemmatize(word) for word in description2]\n     description2 =\" \".join(description2)\n     description_list2.append(description2)\n        \n        \nfrom sklearn.feature_extraction.text import CountVectorizer\nmax_features=60\ncount_vectroizer2 =CountVectorizer(max_features=max_features,stop_words=\"english\")# -----> stopwords unmeaning words\nsparce_matrix2 = count_vectroizer2.fit_transform(description_list2).toarray()\ndictionary2 = count_vectroizer2.vocabulary_.items()  \nvocab2 = []\ncount2 = []\nfor key, value in dictionary2:\n    vocab2.append(key)\n    count2.append(value)\nvocab_bef_stem2 = pd.Series(count2, index=vocab2)\nvocab_bef_stem2 = vocab_bef_stem2.sort_values(ascending=False)\ntop_vacab2 = vocab_bef_stem2.head(50)\ntop_vacab2.plot(kind = 'barh', figsize=(10,20))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# As a result for New York,  "},{"metadata":{},"cell_type":"markdown","source":"## Full time jobs  more than part time jobs\n## Most wanted Civil Service is Computer Specialist (software)\n## Most wanted Business Title is Computer Specialist (software) and Computer Specialist Manager\n## Popular Location is Brooklyn\n## Positions which have salary more than 100k/yearly  are Vedor Managers,Full-stack developers,Unit Managers, eDx developers\n## Prefered Skills are java,sql,web\n## Min Qua. College Degree ,  experience"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}