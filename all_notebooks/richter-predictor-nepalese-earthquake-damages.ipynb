{"cells":[{"metadata":{},"cell_type":"markdown","source":"I am very attached to Nepal as I did a great travel there back in the days. When I was there, hanging out in Kathmandu, I could see the damages of the 2015 earthquake and all the efforts put in to rebuild the city.\n\nThis dataset and classification problem come from DrivenData.org, a platform which hosts data science competition for the social good. Don't hesitate to check them out, they host great projects !\n\nLink of the competition : https://www.drivendata.org/competitions/57/nepal-earthquake","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# IMPORTING USEFUL LIBRARIES","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport pprint\n\nimport seaborn as sb\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nimport xgboost as xgb\n\nfrom sklearn.metrics import classification_report, f1_score, confusion_matrix\n\nfrom sklearn import ensemble, tree, linear_model, svm, naive_bayes, neural_network, neighbors\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom IPython.core.interactiveshell import InteractiveShell  \nInteractiveShell.ast_node_interactivity = \"all\"\n#allows to, among other functionnalities,print head and info of a df in the same cell \nfrom IPython.display import display_html ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LOADING DATA","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/richters-predictor-modeling-earthquake-damage/train_values.csv')\ntarget = pd.read_csv('/kaggle/input/richters-predictor-modeling-earthquake-damage/train_labels.csv')\ntest = pd.read_csv('/kaggle/input/richters-predictor-modeling-earthquake-damage/test_values.csv')\nsub_format = pd.read_csv('/kaggle/input/richters-predictor-modeling-earthquake-damage/submission_format.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.merge(train, target, on = 'building_id', how = 'left')\ntrain.set_index('building_id', drop = True, inplace = True)\ntest.set_index('building_id', drop = True, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a total of 37 parameters, mostly numerical values, with a few of them being string values. We'll transform these strings to numerical / categorical values further.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Problem description\n\nWe're trying to predict the ordinal variable damage_grade, which represents a level of damage to the building that was hit by the earthquake.\n\n**There are 3 grades of the damage:**\n1. represents low damage\n1. represents a medium amount of damage\n1. represents almost complete destruction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Performance metric\n\nWe are predicting the level of damage from 1 to 3. The level of damage is an ordinal variable meaning that ordering is important. This can be viewed as a classification or an ordinal regression problem. (Ordinal regression is sometimes described as an problem somewhere in between classification and regression.)\n\nTo measure the performance of our algorithms, we'll use the F1 score which balances the precision and recall of a classifier. Traditionally, the F1 score is used to evaluate performance on a binary classifier, but since we have three possible labels we will use a variant called the micro averaged F1 score.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Few vizualizations to discover the data we have to work with","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sb.countplot(train['damage_grade'])\nprint(train['damage_grade'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can note that damage_grade = 2 is much more representated than 1 and about twice more than 3. Let's investigate more, using the geo_level as another variable : according to the host of the competition, the 'geo level' data represents the geographic region in which building exists, from largest (level 1) to most specific sub-region (level 3). Let's see what does it mean on results :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\n\nplt.subplot(1,3,1)\nsb.barplot(train['damage_grade'], train['geo_level_1_id'])\n\nplt.subplot(1,3,2)\nsb.barplot(train['damage_grade'], train['geo_level_2_id'])\n\nplt.subplot(1,3,3)\nsb.barplot(train['damage_grade'], train['geo_level_3_id'])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\n\nplt.subplot(1,3,1)\nsb.distplot(train['age'], kde = False)\n\nplt.subplot(1,3,2)\nplt.hist(train['age'], range=(0,200))\n\nplt.subplot(1,3,3)\nsb.barplot(train['damage_grade'],train['age'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Building are almost all above 100 years of age. For each damage grade, only a few are a thousand years old. With no suprise, newer buildings where less damaged.\n\nLet's now look at height / area percentage and floor data :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,5))\n\nplt.subplot(1,3,1)\nsb.barplot(train['damage_grade'], train['height_percentage'])\n\nplt.subplot(1,3,2)\nsb.barplot(train['damage_grade'], train['area_percentage'])\n\nplt.subplot(1,3,3)\nsb.barplot(train['damage_grade'], train['count_floors_pre_eq'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see a sligh correlation between the height/area data and the damage grade level. Plus, higher buildings tend to get more damaged.\n\nWhat about the 'superstructure' data ?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"superstructure_cols = [x for x in train.columns if 'super' in x]\nsecondary_use_cols = [x for x in train.columns if 'secondary' in x]\n\nsuperstructure_corr = train[superstructure_cols+['damage_grade']].corr()\nsecondary_use_corr = train[secondary_use_cols+['damage_grade']].corr()\n\nplt.figure(figsize=(30,8))\n\nplt.subplot(1,2,1)\nsb.heatmap(secondary_use_corr)\n\nplt.subplot(1,2,2)\nsb.heatmap(superstructure_corr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation between damages and secondary_use is weak but we can see that there might be something interesting with superstructures :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,5))\n\nplt.subplot(1,3,1)\nsb.barplot(train['damage_grade'], train['has_superstructure_adobe_mud'])\n\nplt.subplot(1,3,2)\nsb.barplot(train['damage_grade'], train['has_superstructure_mud_mortar_stone'])\n\nplt.subplot(1,3,3)\nsb.barplot(train['damage_grade'], train['has_superstructure_cement_mortar_brick'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can clearly see that mud structures were much more damaged than more solid ones like cement.\n\nFrom now, we'll move to data preparation and a first modeling approach.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data preparation (split, cleaning, ...)#\n\nTreating categorical data with pandas 'get_dummies' function.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"text_features = []\nfor column in train.columns:\n    if train[column].dtype == 'object':\n        text_features.append(column)\n\nfor feature in text_features:\n    train = train.join(pd.get_dummies(train[feature], prefix = feature))\n    test = test.join(pd.get_dummies(test[feature], prefix = feature))\n    \n    train.drop(feature, axis = 1, inplace = True)\n    test.drop(feature, axis = 1, inplace = True)\n\n\nfeatures = train.drop('damage_grade', axis = 1).columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our train dataset is ready for train/test split :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(train[features], train.damage_grade, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will fit the train data to common classifiers to see which one of these performs better on a first approach :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classifiers = [neighbors.KNeighborsClassifier(),\n               tree.DecisionTreeClassifier(),\n               ensemble.RandomForestClassifier(),\n               ensemble.GradientBoostingClassifier(),\n               xgb.XGBClassifier()]\n\ndef test_models(classifiers):\n    \n    for model in classifiers:\n        \n        model.fit(X_train, Y_train)\n        Y_pred = model.predict(X_test)\n        \n        print(model)\n        score = f1_score(Y_test, Y_pred, average='micro')\n        print(score)\n        print('############')\n        \ntest_models(classifiers)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like RandomForest and XGBoost performs pretty well and both give a f1 score which will get you a minima of top 20% in the competition.\n\nBut I am sure we can do better than that. Let's try to tune both of these results by performing feature engineering and parameters optimization.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering\n\nLet's dive into some changes / removals we could perform on our features to get a better result.\n\nFirst, let's re-train our 2 best base-models and compare their confusion matrices :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_clf = ensemble.RandomForestClassifier()\nxgb_clf = xgb.XGBClassifier()\n\nrf_clf.fit(X_train, Y_train)\ny_pred_rf = rf_clf.predict(X_test)\n\nxgb_clf.fit(X_train, Y_train)\ny_pred_xgb = xgb_clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cm_rf = pd.DataFrame(confusion_matrix(Y_test, y_pred_rf), columns=np.unique(Y_test), index = np.unique(Y_test))\ndf_cm_rf.index.name = 'Real'\ndf_cm_rf.columns.name = 'Predicted'\n\ndf_cm_xgb = pd.DataFrame(confusion_matrix(Y_test, y_pred_xgb), columns=np.unique(Y_test), index = np.unique(Y_test))\ndf_cm_xgb.index.name = 'Real'\ndf_cm_xgb.columns.name = 'Predicted'\n\nplt.figure(figsize=(20,8))\n\nplt.subplot(1,2,1)\nsb.heatmap(df_cm_rf, annot=True, fmt='d', annot_kws={\"size\": 24})\n\nplt.subplot(1,2,2)\nsb.heatmap(df_cm_xgb, annot=True, fmt='d', annot_kws={\"size\": 24})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both of our models over-estimate the damage_grade level 2 (42300 / 43813 predicted for 36994 real), which is almost + 15%.\n\nOn the other hand, they tend to under-estimate both damage_grade levels 1 and 3.\n\nLet's look at the classification reports for both of our models :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Random Forest\")\nprint(classification_report(Y_test, y_pred_rf))\nprint('############################################################')\nprint(\"XG Boost\")\nprint(classification_report(Y_test, y_pred_xgb))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These classification reports show that the prediction for damage_grade = 1 isn't really good, which can appear as a predictle result, looking at the damage_grade repartition in the training set seen before.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"importance_rf = pd.DataFrame({\"Features\":features, \"Importance_RF\":rf_clf.feature_importances_}).sort_values(by='Importance_RF', ascending = False).head(15)\nimportance_xgb = pd.DataFrame({\"Features\":features, \"Importance_XGB\":xgb_clf.feature_importances_}).sort_values(by='Importance_XGB', ascending = False).head(15)\n\nRF_styler = importance_rf.style.set_table_attributes(\"style='display:inline'\").set_caption('Top 15 Random Forest importance')\nXGB_styler = importance_xgb.style.set_table_attributes(\"style='display:inline'\").set_caption('Top 15 XGBoost importance')\n\ndisplay_html(RF_styler._repr_html_()+XGB_styler._repr_html_(), raw=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see here that, for the Random Forest, the geographical data is unmissable for our model. On the other hand, for XGBoost, categorical data look like to have more influence.\n\nLet's just check that 'foundation_type_r' feature, which has a high importance for XGB :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['foundation_type_r'].value_counts()\nsb.barplot(train['damage_grade'], train['foundation_type_r'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This feature seems to be important to predict an output of damage_grade=3.\n\nLet's now see if we find outliers in other numerical values :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"num_features = ['geo_level_1_id','geo_level_2_id','geo_level_3_id','age','area_percentage','height_percentage']\ni = 1\n\nplt.figure(figsize=(20,10))\n\nfor col in num_features:\n    plt.subplot(3,3,i)\n    ax=sb.boxplot(train[col].dropna())\n    plt.xlabel(col)\n    i+=1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TUNING XGBOOST CLF","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's take back our base-xgboost classifier perform a few tuning manipulations on its parameters :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Baseline f1 score :')\nprint(f1_score(Y_test, y_pred_xgb, average='micro'))\nprint('Parameters associated :')\nxgb_clf.get_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_1 = {'max_depth' : [10, 20, 40, 60, 80]}\n\nxgb_gs = GridSearchCV(xgb_clf, param_1, n_jobs=4,verbose=5, scoring='f1_micro', cv=3)\n\nxgb_gs.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_submission(test_data, classifier):\n    \n    classifier.fit(X_train, Y_train)\n    \n    test_data['damage_grade'] = classifier.predict(test_data[features])\n\n    test_data['damage_grade'].to_csv('submission.csv', index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"make_submission(test, xgb_gs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thanks for reading. Do not hesitate to comment if you have questions about the competition or my notebook. I will regularly update this with better classifiers in order to gain, I hope, a few ranks up. :D\n\nRemarks about what I could do better are greatly appreciated !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}