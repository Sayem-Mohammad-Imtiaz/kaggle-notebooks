{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This notebook is an extension of the previous notebook. It mostly deals with distributed training approach and will contain a standard framework which we will carry out in our successive notebooks.Any production level system should be horizontally scalable . This means that the training jobs should be distributed into clusters of devices like CPU's , GPU's , TPU's . Doing it alone by hand would be a monumentous task and hence the nice guys from google wrapped up all the hard work into their estimator api.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_test_split_write(dataset,train_split = 0.8,eval_split = 0.1):\n    np.random.seed(0)\n    mask_train = [x < train_split for x in np.random.random(len(dataset))]\n    mask_eval = [x >= train_split and x < (train_split + eval_split) for x in np.random.random(len(dataset))]\n    mask_test = [x >= (train_split + eval_split) for x in np.random.random(len(dataset))]\n    dataset[mask_train].to_csv('../working/mushrooms_train.csv')\n    dataset[mask_eval].to_csv('../working/mushrooms_eval.csv')\n    dataset[mask_test].to_csv('../working/mushrooms_test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we create our training , evaluation and testing files and write it down to disk","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"CSV_DEFAULTS = ['?' for item in range(24)]\nCSV_COLUMN_NAMES = ['cap-shape', 'cap-surface', 'cap-color', 'odor',\n       'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color',\n       'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',\n       'stalk-surface-below-ring', 'stalk-color-above-ring',\n       'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number',\n       'ring-type', 'spore-print-color', 'habitat']\nimport tensorflow as tf\ndef generate_feature_columns(dataset):\n    temp = dataset[CSV_COLUMN_NAMES]\n    features = []\n    for item in temp:\n        col_name = item\n        col_classes = dataset[item].unique()\n        feat_col = tf.feature_column.categorical_column_with_vocabulary_list(col_name,col_classes)\n        one_hot = tf.feature_column.indicator_column(feat_col)\n        features.append(one_hot)\n    return features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above code generates the feature columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def format_label(x):\n    if(x=='p'):\n        return 1\n    else:\n        return 0\n\ndef read_dataset(csv_path):  \n    data = pd.read_csv(csv_path,index_col=[0])\n    data['class'] = data['class'].map(format_label)\n    data = tf.data.Dataset.from_tensor_slices((dict(data[CSV_COLUMN_NAMES]),data['class']))\n    return data\n\ndef train_input_funnction(batch_size,epochs = 10):\n    data = read_dataset('../working/mushrooms_train.csv')\n    data = data.shuffle(buffer_size=6600).repeat(count=epochs).batch(batch_size)\n    return data\n    \ndef eval_input_funnction(batch_size,epochs = 10):\n    data = read_dataset('../working/mushrooms_eval.csv')\n    data = data.shuffle(buffer_size=810).repeat(count=epochs).batch(batch_size)\n    return data\n\ndef predict_input_funnction(batch_size,epochs = 10):\n    data = read_dataset('../working/mushrooms_test.csv')\n    data = data.shuffle(buffer_size=880).repeat(count=epochs).batch(batch_size)\n    return data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preprocessing functions for data sources","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\ndataset = pd.read_csv('/kaggle/input/mushroom-classification/mushrooms.csv')\ndef load_data(dataset):\n    train_test_split_write(dataset)\n    return generate_feature_columns(dataset)\n\nfeat_cols = load_data(dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here most of the preprocessing steps are completed and the appropriate feature columns are generated. Up till now most of the functions match that of the previous notebook . From here onwards we shall go for the distributed training approach and make necessary modifications to the code. Also we. will use custom estimators which are not part of the standard premade estimators library","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_fn(features, labels, mode):\n    model = tf.keras.Sequential([\n      tf.keras.layers.DenseFeatures(feat_cols),\n      tf.keras.layers.Dense(1,activation = 'relu'),\n      tf.keras.layers.Dense(1,activation = 'softmax')\n    ])\n    \n    logits = model(features, training=False)\n    \n    if mode == tf.estimator.ModeKeys.PREDICT:\n        predictions = {'logits': logits}\n        return tf.estimator.EstimatorSpec(labels=labels, predictions=predictions)\n    \n    optimizer = tf.compat.v1.train.AdamOptimizer()\n    \n    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(labels, logits)\n    \n    if mode == tf.estimator.ModeKeys.EVAL:\n        return tf.estimator.EstimatorSpec(mode = mode, loss=loss)\n\n    return tf.estimator.EstimatorSpec(\n          mode=mode,\n          loss=loss,\n          train_op=optimizer.minimize(\n          loss, tf.compat.v1.train.get_or_create_global_step()))\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model function returns and estimator spec which will determine how the model will run. We can add a ton of configs here , cutom loggers , custom metrices , custom loss functions , the list goes on. We wont worry much about the performance as of now . We will only structure our code here and create a standardised framework which we will follow in the next notebooks.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The model function defines our models. Its losses , optimizers ect. As with the previous notrbooks we have mixed the best of both worlds and created our model. Which means that we have taken the keras approach for model building and then wrapped it up with the estimator api. The end result of this api is an estimator spec which will help tensorflow to build an estimator and run it a distributed manner depening on the devices it has access to. This code can easily be run on cloud Machine Learning Engine , which is googles home grown model training and hosting infrastructure. \n\nWe wont go into the details of this now , and only focus on the code structure.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"strategy = tf.distribute.MirroredStrategy()\nprint(strategy)\n\nprint('Number of devices: {}'.format(strategy.num_replicas_in_sync))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This defines that tensor flow will use a distributed approach for training the data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"config = tf.estimator.RunConfig(save_checkpoints_steps = 100,log_step_count_steps=10)\n\nclassifier = tf.estimator.Estimator(\n    model_fn=model_fn, model_dir='model/', config=config)\n\n\ntf.estimator.train_and_evaluate(\n    classifier,\n    train_spec=tf.estimator.TrainSpec(input_fn=lambda : train_input_funnction(10),max_steps = 100),\n    eval_spec=tf.estimator.EvalSpec(input_fn=lambda : eval_input_funnction(10),\n                                   steps = None,start_delay_secs = 1,throttle_secs = 1)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The final code block trains the model for the given number of steps and saves checkpoints in a pre defined folder.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%bash\nrm -r model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Code to clean up log files and checkpoints to retrain the model.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}