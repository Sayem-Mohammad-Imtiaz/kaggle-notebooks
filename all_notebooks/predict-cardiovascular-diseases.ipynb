{"cells":[{"metadata":{},"cell_type":"markdown","source":"Cardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worldwide.\nHeart failure is a common event caused by CVDs and this dataset contains 12 features that can be used to predict mortality by heart failure.\n\nMost cardiovascular diseases can be prevented by addressing behavioral risk factors such as tobacco use, unhealthy diet and obesity, physical inactivity, and harmful use of alcohol using population-wide strategies.\n\nPeople with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidemia, or already established disease). \n\n* source information and data : https://www.kaggle.com/andrewmvd/heart-failure-clinical-data"},{"metadata":{},"cell_type":"markdown","source":"![https://www.udmi.net/wp-content/uploads/2020/02/UDMI_Cardiovascular-Disease.png](https://www.udmi.net/wp-content/uploads/2020/02/UDMI_Cardiovascular-Disease.png)\n\n\n\nSource Image: https://www.udmi.net/cardiovascular-disease-risk/"},{"metadata":{},"cell_type":"markdown","source":"This work, we tried to classifying cardiovascular diseases using Random Forest classification. Hopefully can be of great help for early detection and management of cardiovascular diseases.\n\nPredictor variable use in classifying Cardiovascular diseases :\n\n1. age                       \n2. anaemia                     \n3. creatinine_phosphokinase    \n4. diabetes                    \n5. ejection_fraction          \n6. high_blood_pressure         \n7. platelets                 \n8. serum_creatinine          \n9. serum_sodium               \n10. sex                         \n11. smoking                     \n12. time                       \n"},{"metadata":{},"cell_type":"markdown","source":"Import Library"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Read Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv')\nprint('Dataset :',data.shape)\ndata.info()\ndata[0:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**VISUALIZING THE DATA\n**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Distribution of DEATH_EVENT\ndata.DEATH_EVENT.value_counts()[0:30].plot(kind='bar')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting Heatmap\nHeatmap can be defined as a method of graphically representing numerical data where individual data points contained in the matrix are represented using different colors. The colors in the heatmap can denote the frequency of an event, the performance of various metrics in the data set, and so on. Different color schemes are selected by varying businesses to present the data they want to be plotted on a heatmap [2]."},{"metadata":{"trusted":true},"cell_type":"code","source":"data1 = data[['age','anaemia','creatinine_phosphokinase','diabetes','ejection_fraction','high_blood_pressure',\n'platelets','serum_creatinine','serum_sodium','sex','smoking','time']] #Subsetting the data\ncor = data1.corr() #Calculate the correlation of the above variables\nsns.heatmap(cor, square = True) #Plot the correlation as heat map","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see above, we obtain the heatmap of correlation among the variables. The color palette in the side represents the amount of correlation among the variables. The lighter shade represents a high correlation."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style(\"whitegrid\")\nsns.pairplot(data,hue=\"DEATH_EVENT\",size=3);\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SPLITING DATA\nData for training and testing\nTo select a set of training data that will be input in the Machine Learning algorithm, to ensure that the classification algorithm training can be generalized well to new data. For this study using a sample size of 5% ( aims to reduce the overfitting effect)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nY = data['DEATH_EVENT']\nX = data.drop(columns=['DEATH_EVENT'])\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.05, random_state=9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('X train shape: ', X_train.shape)\nprint('Y train shape: ', Y_train.shape)\nprint('X test shape: ', X_test.shape)\nprint('Y test shape: ', Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Random forest classification\n\nRandom forest is a supervised learning algorithm that creates a forest randomly. This forest, is a set of decision trees, most of the times trained with the bagging method. The essential idea of bagging is to average many noisy but approximately impartial models, and therefore reduce the variation. Each tree is constructed using the following algorithm:\n\n* Let $N$ be the number of test cases, $M$ is the number of variables in the classifier.\n* Let $m$ be the number of input variables to be used to determine the decision in a given node; $m<M$.\n* Choose a training set for this tree and use the rest of the test cases to estimate the error.\n* For each node of the tree, randomly choose $m$ variables on which to base the decision. Calculate the best partition of the training set from the $m$ variables.\n\nFor prediction a new case is pushed down the tree. Then it is assigned the label of the terminal node where it ends. This process is iterated by all the trees in the assembly, and the label that gets the most incidents is reported as the prediction. We define the number of trees in the forest in 100. \n\nAdvantages Random Forest:\n* runtimes are quite fast\n* Are able to deal with unbalanced and missing data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\n\n# We define the model\nrfcla = RandomForestClassifier(n_estimators=100,random_state=9,n_jobs=-1)\n\n# We train model\nrfcla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict5 = rfcla.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_acc_rfcla = round(rfcla.fit(X_train,Y_train).score(X_test, Y_test)* 100, 2)\ntrain_acc_rfcla = round(rfcla.fit(X_train, Y_train).score(X_train, Y_train)* 100, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = pd.DataFrame({\n    'Model': ['Random Forest'],\n    'Train Score': [train_acc_rfcla],\n    'Test Score': [test_acc_rfcla]\n})\nmodel1.sort_values(by='Test Score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(Y_test, Y_predict5)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The confusion matrix\nrfcla_cm = confusion_matrix(Y_test, Y_predict5)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(rfcla_cm, annot=True, linewidth=0.7, linecolor='black', fmt='g', ax=ax, cmap=\"BuPu\")\nplt.title('Random Forest Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. SVM (Support Vector Machine) classification\n\nSVMs (Support Vector Machine) have shown a rapid proliferation during the last years. The learning problem setting for SVMs corresponds to a some unknown and nonlinear dependency (mapping, function) $y = f(x)$ between some high-dimensional input vector $x$ and scalar output $y$. It is noteworthy that there is no information on the joint probability functions, therefore, a free distribution learning must be carried out. The only information available is a training data set $D = {(x_i, y_i) ∈ X×Y }, i = 1$, $l$, where $l$ stands for the number of the training data pairs and is therefore equal to the size of the training data set $D$, additionally, $y_i$ is denoted as $d_i$, where $d$ stands for a desired (target) value. Hence, SVMs belong to the supervised learning techniques.\n\nFrom the classification approach, the goal of SVM is to find a hyperplane in an N-dimensional space that clearly classifies the data points. Thus hyperplanes are decision boundaries that help classify the data points. Data points falling on either side of the hyperplane can be attributed to different classes.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\n\n# We define the SVM model\nsvmcla = OneVsRestClassifier(BaggingClassifier(SVC(C=10,kernel='rbf',random_state=9, probability=True), \n                                               n_jobs=-1))\n\n# We train model\nsvmcla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict2 = svmcla.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_acc_svm = round(svmcla.fit(X_train,Y_train).score(X_test, Y_test)* 100, 2)\ntrain_acc_svm = round(svmcla.fit(X_train, Y_train).score(X_train, Y_train)* 100, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = pd.DataFrame({\n    'Model': ['SVM'],\n    'Train Score': [train_acc_svm],\n    'Test Score': [test_acc_svm]\n})\nmodel2.sort_values(by='Test Score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(Y_test, Y_predict2)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The confusion matrix\nsvm = confusion_matrix(Y_test, Y_predict5)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(svm, annot=True, linewidth=0.7, linecolor='black', fmt='g', ax=ax, cmap=\"BuPu\")\nplt.title('SVM Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Features Selection"},{"metadata":{},"cell_type":"markdown","source":"1.In here we drop 1.age, 2.anaemia, 4.diabetes, 6.high_blood_pressure  from data. We use features :\n                \n3. creatinine_phosphokinase    \n5. ejection_fraction          \n7. platelets                 \n8. serum_creatinine          \n9. serum_sodium               \n10. sex                         \n11. smoking                     \n12. time                       \n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"Y1 = data['DEATH_EVENT']\nX1 = data.drop(columns=['age','anaemia','diabetes','high_blood_pressure'])\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\n\nlsvc = LinearSVC(C=0.06, penalty=\"l1\", dual=False,random_state=10).fit(X1, Y1)\nmodel = SelectFromModel(lsvc, prefit=True)\nX_new = model.transform(X1)\ncc = list(X1.columns[model.get_support(indices=True)])\nprint(cc)\nprint(len(cc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Principal component analysis\nfrom sklearn.decomposition import PCA\n\npca = PCA().fit(X1)\nplt.figure()\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel(\"'age','anaemia','diabetes','high_blood_pressure'\")\nplt.ylabel('% Variance Explained')\nplt.title('PCA Analysis')\nplt.grid(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Percentage of total variance explained\nvariance = pd.Series(list(np.cumsum(pca.explained_variance_ratio_)), \n                        index= list(range(0,9))) \nprint(variance[20:80])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X1 = data[cc] \nfrom sklearn.model_selection import train_test_split\nX1_train, X1_test, Y1_train, Y1_test = train_test_split(X1, Y1, test_size=0.05, random_state=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random forest classification\nrfcla.fit(X1_train, Y1_train)\nY1_predict5 = rfcla.predict(X1_test)\nrfcla_cm = confusion_matrix(Y1_test, Y1_predict5)\nscore1_rfcla = rfcla.score(X1_test, Y1_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_acc_rfcla = round(rfcla.fit(X1_train,Y1_train).score(X1_test, Y1_test)* 100, 2)\ntrain_acc_rfcla = round(rfcla.fit(X1_train, Y1_train).score(X1_train, Y1_train)* 100, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SVM classification\nsvmcla.fit(X1_train, Y1_train)\nY1_predict2 = svmcla.predict(X1_test)\nsvmcla_cm = confusion_matrix(Y1_test, Y1_predict2)\nscore1_svmcla = svmcla.score(X1_test, Y1_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_acc_svm2 = round(svmcla.fit(X_train,Y_train).score(X_test, Y_test)* 100, 2)\ntrain_acc_svm2 = round(svmcla.fit(X_train, Y_train).score(X_train, Y_train)* 100, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model3 = pd.DataFrame({\n    'Model': ['Random Forest','SVM'],\n    'Train Score': [train_acc_rfcla,train_acc_svm2 ],\n    'Test Score': [test_acc_rfcla, test_acc_svm2]\n})\nmodel3.sort_values(by='Test Score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15,15))\nax1 = fig.add_subplot(3, 3, 1) \nax1.set_title('Random Forest') \nax2 = fig.add_subplot(3, 3, 2) \nax2.set_title('SVM Classification')\n\n\nsns.heatmap(data=rfcla_cm, annot=True, linewidth=0.7, linecolor='black',cmap=\"BuPu\" ,fmt='g', ax=ax1)\nsns.heatmap(data=svmcla_cm, annot=True, linewidth=0.7, linecolor='black',cmap=\"BuPu\" ,fmt='g', ax=ax2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nRandom Forest performs better than SVM. The test accuracy of SVM  high even if the training accuracy (training error) is lesser than the expectation it might because of dissimilarity between the test and training pattern."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}