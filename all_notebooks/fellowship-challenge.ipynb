{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# magics \n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fellowship.ai Challenge: ULMFiT applied to the Twitter US Airline Sentiment dataset\n\n## Transfer learning is no longer limited to computer vision\n\nUniversal Language Model Fine-tuning (ULMFiT), a novel approach towards applying transfer learning to natural language processing tasks, was originally introduced by Jeremy Howard and Sebastian Ruder in early 2018.  \n\nTransfer learning is a method that takes advantage of models that have already been pre-trained to learn a given task on large datasets and re-purposes them to learn a (new) related task. This dramatically reduces the amount training data and time needed to train a model.\n\nHoward and Ruder demonstrated that transfer learning, which had previously only been successful with computer vision problems, can be applied to natural language processing tasks as well. At the time of ULMFiT's initial release, it outperformed the state-of-the-art on six text classification benchmarks. \n\n<strong>Classifying the sentiment of tweets</strong>\n\nThis notebook is a documented attempt at creating an ULMFiT model that can correctly classify the sentiment of tweets from from the [Twitter US Airline Sentiment dataset](https://www.kaggle.com/crowdflower/twitter-airline-sentiment). The data consists of 14640 labeled tweets directed at six major US airline companies (Virgin America, United, Southwest, Delta, US Airways and American Airlines). We will train a model to identify whether the tweets are positive, neutral or negative. \n\nWithout transfer-learning and ULMFiT, this task would have been considered to be very challenging due to the small dataset size. 14640 tweets is regarded as small, given that previous attempts to fine-tune language models required millions of in-domain documents to achieve an acceptable performance [(Dai and Le 2015)](http://arxiv.org/abs/1511.01432). We also need to take into account that tweets are relatively short text samples, meaning that the model has less information to work with relative to longer documents like IMDB movie reviews. (Tweets were still limited to 140 characters at the time the data was collected.)\n\nPreliminary data-exploration showed that the dataset is highly imbalanced, with the negative tweets amounting to more than the positive and neutral tweets combined with 9178 negative tweets, 3099 neutral tweets and 2363 positive tweets respectively.    \n\nA study by [Buda et al.](https://arxiv.org/pdf/1710.05381.pdf) demonstrated that class imbalance significantly decreases the performance of CNNs (Convolutional Neural Network), an architecture primarily used for computer vision tasks. As such it is very likely that the class imbalance of this dataset may have a negative impact on model performance as well. \n\n<strong>Creating a multi-label classifier with data-augmentation and ensembles</strong>\n\n\nWe will try solving this multi-label classification problem according to the approach outlined in following steps::\n\n1. Create a artificially balanced dataset using nlp data-augmentation.\n2. Fine-tune an AWD-LSTM language model trained on the Wikitext 103 corpus to the data-augmented airline sentiment dataset to train bi-directional language model (an ensemble of the forward and backward language models).\n3. Train two separate text classifiers using both the forward and backward language model encoders to create an ensemble classification model.\n\nReferences:\n\n1. [Andrew M. Dai and Quoc V. Le. 2015. Semi- supervised Sequence Learning. Advances in Neu- ral Information Processing Systems (NIPS ’15)](https://arxiv.org/abs/1511.01432)\n2. [Mateusz Buda, Atsuto Maki, and Maciej A Mazurowski. A systematic study of the class imbalance problem in convolutional neural networks](https://arxiv.org/pdf/1710.05381.pdf)\n\n\n## Model Evaluation\n\n<em>Error Rate</em> \n\nIn order to get a good understanding of how the model is performing, I've opted to use accuracy and a weighted f1 score to evaluate model performance. The F1 score was included, because accuracy can be misleading when working with imbalanced datasets. Note: Consider user error-rate instead because it's what's used in the ULMFiT paper \n\nIn order to get an understanding of how well our model is performing, I’ve decided to use the error rate (1-accuracy) to evaluate the model’s  performance. This way it can be compared the the ULMFiT paper’s benchmark of a 5% validation error on the IMDb sentiment classification task  and other models.\n\n<em>Classification Matrix</em>\n\nWe will also look at a classification confusion matrix, because accuracy can be a misleading performance indicator for models with imbalanced datasets."},{"metadata":{},"cell_type":"markdown","source":"> Before we start, we'll have to import couple of libraries and the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# install nlp data augmentation library and dependencies\n!pip install git+https://github.com/makcedward/nlpaug.git numpy matplotlib python-dotenv; pip install nltk","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# fastai library\nfrom fastai import *\nfrom fastai.text import *\n# plotting confusion matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils.multiclass import unique_labels\n\n# data visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# nlp word augmentation libraries\nimport nlpaug.augmenter.word as naw\nfrom nlpaug.util import Action\n\n# classification report\nfrom sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# set data path\npath = Path('../input/twitter-airline-sentiment')\npath.ls()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create pandas dataframe and \ndf = pd.read_csv(path/'Tweets.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Exploration\n\nWe'll start by looking at the shape and a few examples of the dataset\n\nThe dataset has 14640 rows and 15 columns. For our ULMFiT model we will only be using the <code>text</code> and <code>airline_sentiment</code> columns. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# check dataframe shape\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# look at first 5 data samples\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we'll create some visualizations to explore the data "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check number of posible lables\nsenti_types = df['airline_sentiment'].unique()\nprint('No. of sentiment labels: {}'.format(len(senti_types)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set graph style\nsns.set(palette='Pastel2', style='dark')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot class distribution\ndist = df.groupby('airline_sentiment')\n\n# plot label distribution to see if dataset is un-balanced\nsns.countplot(df.airline_sentiment)\nplt.xlabel('Sentiment Distribution')\nplt.show()\n\ndist.size()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Class distribution\n\nThe classes distribution is very imbalanced with the dataset comprising of more negative tweets than neutral and positive tweets combined. \n\n>It wouldn't be unreasonable to assume that people are far less likely to tweet about an airline if it's not a complaint. \n\nThe next section isn't necessarily relevant to the ULMFiT model. Feel free to skip ahead to the Algorithms &amp; Tenchiques section of this notebook. \n\nWe will visualize other observations made when looking at the data such as: \n\n1. Which airline was mentioned the most and is this proportional to the market capitalization of the airline?\n2. Sentiment distribution by airline\n3. The top 3 reasons for a complaint\n4. Cause of complaints by airline"},{"metadata":{},"cell_type":"markdown","source":"### Which Airline was the most popular on Twitter in 2015?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot how often an airline has been mentioned\nsns.countplot(df.airline)\nplt.title('Airline mention distribution')\nplt.tick_params(axis='x', rotation=45)\nplt.xlabel('')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When looking at the graph above it's interesting to see that the number of tweets isn't proportional to the market capitalization of the companies.\n\nMarket capitalization of airlines in billions of U.S. dollars: Sept 17 2019 \n\n[Source | https://www.statista.com/statistics/275948/market-capitalization-of-selected-airlines](https://www.statista.com/statistics/275948/market-capitalization-of-selected-airlines/)\n\n1. Delta 38.41\n2. Southwest 30.00\n3. United 23.5\n4. American Airlines + US Airways 12.3\n5. Virgin America 2.55\n\n<em>This needs to be taken with a grain of salt because the data is from 2015 and the market cap. figures are from 2017</em>"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Tweet distribution by airline \\n', df.groupby('airline')['airline_sentiment'].count().sort_values(ascending=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sentiment Distribution by Airline"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby(['airline', 'airline_sentiment']).size().unstack().plot(kind='bar', stacked=True, colormap='Pastel1' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sentiment distribution by airline\nplt.figure(1, figsize=(15,10))\nairlines = list(df['airline'].unique())\nfor i in df.airline.unique():\n    idxs = airlines.index(i)\n    plt.subplot(2,3,idxs+1)\n    pl_df = df[df.airline==i]\n    count = pl_df['airline_sentiment'].value_counts()\n    index = [1,2,3]\n    plt.bar(index, count, color='lightblue')\n    plt.xticks(index, ['negative','neutral','positive'])\n    plt.ylabel('')\n    plt.title('Sentiment Distribution of ' + i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Negative sentiment tends to outweigh the other classes for all airlines.\n\nTwitter sentiment towards American Airlines, US Airways and United Airways appears to be mainly negative.\n\nDelta and Southwest display more balanced sentiment distributions, whilst Virgin America has the most balanced sentiment distribution amongst the airlines. \n\n"},{"metadata":{},"cell_type":"markdown","source":"## Quantifying Customer Complaints"},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the number of negative reasons\ndf['negativereason'].nunique()\n\nnr_count = dict(df['negativereason'].value_counts(sort=False))\ndef reason_count(airline):\n    if airline=='All':\n        a = df\n    else:\n        a = df[df['airline']==airline]\n    count = dict(a['negativereason'].value_counts())\n    unique_reason=list(df['negativereason'].unique())\n    unique_reason=[x for x in unique_reason if str(x) != 'nan']\n    reason_frame=pd.DataFrame({'reasons':unique_reason})\n    reason_frame['count']=reason_frame['reasons'].apply(lambda x: count[x])\n    return reason_frame\n\ndef plot_reason(airline):\n    \n    a = reason_count(airline)\n    count = a['count']\n    index = range(1,(len(a)+1))\n    plt.bar(index,count, color='lightblue')\n    plt.xticks(index,a['reasons'],rotation=90)\n    plt.ylabel('count')\n    plt.xlabel(' ')\n    plt.title('Cause of complaint for ' + airline)\n    \nplot_reason('All')\nplt.figure(2,figsize=(15, 12))\nfor i in airlines:\n    indices= airlines.index(i)\n    plt.subplot(2,3,indices+1)\n    plt.subplots_adjust(hspace=0.9)\n    plot_reason(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Top 3 Airlines - Most Late Flight Complaints\n1. Delta\n2. United\n3. US Airways\n\n#### Top 3 Airlines - Most Lost Luggage Complaints\n\n1. United \n2. Southwest\n3. Delta"},{"metadata":{},"cell_type":"markdown","source":"## Algorithms &amp; Techniques\n\nThis section outlines the techniques used to improve model performance in addition to the methods suggested by the ULMFiT paper.\n\n<strong>Data Augmentation using \"Synonym Word Replacement\"</strong>\n\nI initially tried to create a classifier with the ULMFiT approach outlined in [this notebook](https://github.com/fastai/fastai/blob/master/examples/ULMFit.ipynb) by Sylvain Gugger, which achieves a 4.6% validation error on the [IMDB movie reviews dataset](http://ai.stanford.edu/~amaas/data/sentiment). I also tried the method proposed in the [fast.ai NLP MOOC](https://www.youtube.com/playlist?list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9). Both attempts got nowhere close to a 4.6% error rate. \n\nOne should note that the IMDB dataset only has two classes to predict (one less than our Airline Sentiment dataset) and a significantly larger dataset size, making it \"easier\" to achieve lower error rates.\n\nI suspected that the class imbalance may be one of the reasons behind the initial validation accuracy of 28.7 (> 80% error rate!?!) the first few models were returning.  I’ve also been wanting to explore how data augmentation could be applied to NLP problems. After some research I chanced upon an [open source library built for NLP data augmentation](https://github.com/makcedward/nlpaug) called nlpaug by Edward Ma.\n\nThe library has a function that creates new text samples by replacing words in the origial text with their synonyms.  \n\n<em>Examples of positive and neutral tweets generated by the model</em>\n\n<em>positive tweets</em>\n\n<code>Original: @SouthwestAir now flying non stop CMH-OAK has me daydreaming<br> about a trip to the bay...especially in this weather. #OhioProbz</code>\n\n<code>Augmented: @SouthwestAir instantly flying non stop CMH-OAK has me daydreaming<br> about a slip to the bay...especially in this weather. #OhioProbz</code>\n\n<em>neutral tweets</em>\n\n<code>Original: @AmericanAir forgot to select my ksml on my flight to LA and<br> it's a few minutes under the 24 hour mark. Is there any way to change it?</code>\n\n<code>Augmented: @AmericanAir forgot to select my ksml on my flying to LOUISIANA and<br> it's a few minutes under the 24 60 minutes mark. Be there any way to shift it?</code>\n\nWe will try to balance the dataset classes using data-augmentation. The idea is to generate enough positive and neutral tweets using synonym word replacements to match the negative tweets, instead of handling the imbalance by subsampling or simply creating duplicate copies of the original data. \n\nI’m hoping for this to create more variety in the dataset, which may in turn help the model generalize better. \nIn theory creating new tweets via synonym replacement should maintain the sentiment of the original tweets.\n\n\n<strong>ULMFiT: Transfer Learning and fine-tuning an AWD LSTM Language Model</strong>\n\nTo build our sentiment classifier we will make use of a model that has trained on a larger and more diverse dataset and “fine-tune” it to learn how to solve our classification problem.\n\nIn computer vision, models that have been trained to tell the difference between a thousand different classes (using millions of Imagenet images) are fine-tuned to do medical-image screening tasks like [diagnosing causes for blindness](https://ai.googleblog.com/2016/11/deep-learning-for-detection-of-diabetic.html).\n\nOne of the key discoveries of the ULMFiT paper is that a pre-trained AWD-LSTM language model is NLP’s equivalent to the pre-trained Imagenet model in computer vision, meaning that it serves as a good base model to transfer knowledge from.\n\nLanguage models are models used to predict the next n elements in a sequence, like the next word in a sentence for example.\n\nHoward and Ruder reasoned that [Stephen Merity’s AWD LSTM language model](https://arxiv.org/abs/1708.02182) (which outperformed previous approaches to language modeling by a significant margin) would inherently need to have a more complete and implicit understanding of natural language and it’s semantics in order to learn how to predict the next word(s) in a sentence. \n\nAt a high level the ULMFiT approach can be summarized in the following 3 steps:\n\n1. Use a general and large langauge corpus, like Stephen Merity's Wikitext 103 dataset, which contains a pre-processed subset of the English wikipedia to train a language model. \n2. Fine-tune the langauge model on our target langauge corpus (in this case our 14640 tweets). We’re training a model that can predict the next n words in a tweet.\n3. Use the langauge model's encoder to train a classifier that can distinguish between negative, positive and neutral tweets.\n\n>Wikitext103 consists of 28,595 articles and 103 million words\n\n<strong>Ensemble Training</strong>\n\nTo further increase the potential performance of our model, we will train not one but two classifiers to predict the sentiment of a tweet by training both a forward and a backward langauge model and classifier. A forward model is simply a regular language model that predicts the next word in a sentence. A backward model does the same but backwards, meaning that it predicts the previous word in a sentence given the words that that follow the predicted word. The idea is that the two models combined will have even more implicit knowledge about the language. The resulting prediction will be the mean of the two classifier predictions. \n\nThe paper states that the impact of bi-directionality at the cost of training a second model and ensembling the predictions of a forward and backwards LM-classifier brings a performance boost of around 0.5–0.7, lowering the test error from 5.30 of a single model to 4.58 for the bi-directional model.\n\n<strong>Techniques used to fine-tune the language model</strong>\n\n<em>Discriminative fine-tuning</em>\n\nBecause different layers capture different types of information [(Yosinski et al., 2014)](https://arxiv.org/abs/1411.1792), they should be fine-tuned to different extents. The authors of the ULMFiT paper propose a novel fine-tuning method called discriminative fine-tuning. Instead of using the same learning rate for all layers of the model, discriminative fine-tuning allows for the tuning of each layer with different learning rates.\n\n\n\nThe authors of the ULMFiT paper stated that langauge-model fine-tuning had previously been very difficult, as they tend to overfit to small datasets and suffer from some form of deep learning amnesia (catastrophic forgetting) when fine-tuned with a classifier. \n\nThe following techniques are further suggested methods for fine-tuning a language model with a classifier:\n\n- Slanted triangular learning rates\n- Gradual unfreezing\n\n<em>These methods are described in greater detail in the model implementation section of this notebook.</em>"},{"metadata":{},"cell_type":"markdown","source":"## Benchmark\n\nIn addition to keeping the 4.6% validation error rate from the ULMFiT paper in mind, I decided on using the Kaggle notebook linked below as a benchmark for our ULMFiT model, as it was one of the highest ranked Kaggle notebooks with the ULMFiT approach applied to our [target dataset](https://www.kaggle.com/crowdflower/twitter-airline-sentiment). The benchmark notebook uses 15% of the data for the test set and reports a test accuracy of 82.78% or an error rate of 17.2%. \n\n[Benchmark Link: Twitter-Us-Airlines-Sentiment-With-Ulmfit-Approach](https://www.kaggle.com/davidbankom/twitter-us-airlines-sentiment-with-ulmfit-approach)"},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing\n\nBefore training the data needs to be tokenized and turned into a numericalized representation so that the model can make use of the data.\n\nTokenization refers to chopping up phrases, sentences, paragraphs or entire text documents into smaller units like words.\n\nThis is done using the following pre-processing rules before tokenization:\n- substituting html ```<br>``` tags with <code>\\n</code> \n- adding spaces before and after hashes<code>#</code> and slashes<code>/</code>\n- removing additional spaces\n- replacing character and word repetitions with repetition tokens, for example: <code>cccc</code> is converted to <code>TK_REP 4 c</code> or <code>repetition repetition repetition</code> with <code>TK_WREP 3 repetition</code>\n- further string cleaning methods\n\n...and adding some additional meta-data tokens so that can help the model learn better:\n\n- tokens that indicate the beginning and the end of a sentence\n- tokens that indicate capitalization and ALL CAPS spelling\n- tokens that indicate padding to ensure data-input consistency\n\nThe tokenization process itself is handled using the [spacy tokenizer](https://spacy.io/usage/linguistic-features#how-tokenizer-works), which uses spaces as the splitpoints for text documents.\n\nFastai also limits the total vocabluary of the training data by substituting rare words with the unk token. When looking at the pre-processed data, tokens can be recognized by the <code>xx</code> at the start of each token, <code>xxunk</code> is the token for unknown words, whilst <code>xxbos</code> is the token for the beginning of a sentence.\n\nFurther specifics and code on how the fastai library pre-processes text lists can be found [here](https://github.com/fastai/course-v3/blob/master/nbs/dl2/12_text.ipynb)\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Model Implementation\n\nWe'll beginn by removing the columns we won't be using from the dataframe and split our data into the a training and a test set with a 85/15 split ratio. We will keep the test set squirreled away for now so that we can later see how well our model generalizes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# first we ensure that we're only working with the columns we need from the dataframe\ndf = df[['airline_sentiment', 'text']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# determine split point\ntrain_len = int(len(df)*0.85)\n\n# set training dataframe\ndf_train = df[:train_len]\n\n# set test dataframe\ndf_test = df[train_len:]\n\n# test if split left samples behind\nassert len(df) == sum([len(df_train), len(df_test)])\n\n# print lengths\nprint(len(df_train), len(df_test), sum([len(df_train), len(df_test)]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we will split our training dataframe into positive, neutral and negative tweets to determine how many tweets to generate, which will be the difference between the negative and the positive tweets and the negative and the neutral tweets respectively."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pos = df_train.loc[df_train.airline_sentiment == 'positive']\ndf_neu = df_train.loc[df_train.airline_sentiment == 'neutral']\ndf_neg = df_train.loc[df_train.airline_sentiment == 'negative']\n\nprint('There are {} positive tweets, {} neutral tweets and {} negative tweets'.format(len(df_pos), len(df_neu), len(df_neg)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate how many positive and neutral augmented tweets we want to create to balance the datasets\npos_aug = len(df_neg)-len(df_pos)\nneu_aug = len(df_neg)-len(df_neu)\n\nprint('Tweets to create: {} postive tweets, {} neutral tweets'.format(pos_aug,neu_aug))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data augmentation function\n\ndef train_aug(n_aug, source_df, train_df):\n    # print and store initial df length\n    train_len = len(train_df)\n    print('Data length before augmentation: {} \\n'.format(train_len))\n    \n    for n in range(n_aug):\n        # get a random tweet from the source dataframe\n        randn = random.randint(0, len(source_df)-1)\n        row = source_df.iloc[randn].values\n        # do synonym augmentation\n        aug = naw.SynonymAug(aug_src='wordnet')\n        augmented_text = aug.augment(row[1])\n        # add new row to training dataframe\n        new_row = {'airline_sentiment': row[0], 'text': augmented_text}\n        train_df = train_df.append(new_row, ignore_index=True)\n    \n    new_added = len(train_df) - train_len\n    # print \n    print('{} items added to training datafrane. Training dataframe now has {} rows'.format(new_added, len(train_df)))\n    \n    return train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add postive tweets to training data\ndf_train = train_aug(pos_aug, df_pos, df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add neutral tweets to training data\ndf_train = train_aug(neu_aug, df_neu, df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len_df_all = len(df_train) + len(df_test)\nprint('Train length: {}, Test length: {}, Combined data length {}'.format(len(df_train), len(df_test), len_df_all))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To better train and improve language model fine-tuning we will use all the data to train our Airline Tweet language model by concatenating the train and test sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"# concat train and test set for language model\ndf_lm = pd.concat([df_train, df_test])\nprint(len(df_lm))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set batchsize and backprop through time\nbs = 256\nbptt = 80","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we will use the fastai datablock api to prepare the data for our langauge model with only 5% of the data in the validation set so it can learn as much as possible from our target langauge corpus."},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare/pre-process langauge model data\ndata_lm = (TextList.from_df(df_lm, cols='text')\n                   .split_by_rand_pct(0.05, seed=42) # default for valid_pct=0.2, we're using 0.05, so we have more data to train lm\n                   .label_for_lm() # the text is the label\n                   .databunch())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# look at dataset length after split and pre-processing\nprint('Vocab size: {}, training set length: {}, validation set length: {}'.format(len(data_lm.vocab.itos), len(data_lm.train_ds), len(data_lm.valid_ds)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save data\ndata_lm.save('data_lm.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# look at data batch\ndata_lm.show_batch()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Because we will be building a bi-directional model we want to create an instance of the data in reverse, luckily the [fastai library](https://docs.fast.ai) makes this very easy by loading the data with <code>backwards=True</code> "},{"metadata":{"trusted":true},"cell_type":"code","source":"# backward lm data - *the ../../working is specific to working in a Kaggle notebook\ndata_lm_bwd = load_data(path, '../../working/data_lm.pkl', bs=bs, bptt=bptt, backwards=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's look at the data in reverse\ndata_lm_bwd.show_batch()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We initiate and load our pre-trained AWD LSTM language model learner and load our data into the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# initiate learner\nlm_learner = language_model_learner(data_lm, AWD_LSTM, model_dir='../../working', metrics=[accuracy, error_rate])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we will use the fastai learning rate finder to determine how to set our learning rate.\n\nThe learning rate finder works by training the model over the duration of 1 epoch, starting at a very low learning rate e.g. 1e-8 and gradually increasing the learning rate over time, multiplying it by a given factor at each mini-batch, until it reaches a value of 1, whilst recording the loss at each timestep. We then plot a smoothed version of the loss (in this case an exponentially weighted moving average) against the learning rate, which is illustrated below.\n\n![](https://sgugger.github.io/images/art2_courbe_lr.png)\n\nOne can observe the loss gradually decreasing until it hits a minimum and then rapidly increasing. \nWe're looking to set our learning rate at a point that is about a factor of 10 lower than the minimum, as the values near the minimum may be too close to the point at which the model loss increases."},{"metadata":{"trusted":true},"cell_type":"code","source":"# find the learning rate\nlm_learner.lr_find()\nlm_learner.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use mixed-precision training so that we can train faster and use a higher batch-size\nlm_learner = lm_learner.to_fp16(clip=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we will train our model for 1 epoch at a learning rate of 4e-2, a weight decay of 0.1 and a momentum range between 0.8 and 0.7. \n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train \nlm_learner.fit_one_cycle(1, 4e-2, moms=(0.8,0.7), wd=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lm_learner.recorder.plot_lr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<em>The plot above depicts the behavior of our learning rate during training</em>\n\nOne of the training methods proposed by Howard and Ruder is slanted triangular learning rates, a dynamic learning rate with a short increase and a long decay period.\n\nThis helps the model adapt parameters to task-specific features, making it quickly converge to a suitable region of the parameter space in the beginning of training and then refining its parameters. \n\nThis slight derivation (because the fastai implementation has a smoother, curved and not strictly linear learning rate schedule) of [Leslie Smith's](https://arxiv.org/search/cs?searchtype=author&query=Smith%2C+L+N) triangular learning rates, which first linearly increase the learning rate and then linearly decay it according a dynamic update schedule.\n\n![](https://miro.medium.com/max/548/1*QptmUluWXteT6oI5bD22rw.png)\n\nSTLR modifies triangular learning rates [(Smith, 2017)](https://arxiv.org/abs/1506.01186) with a short increase and a long decay period, which we found key for good performance.\n\nWe will then unfreeze the langauge model and train it for another 10 epochs at a lower learning rate of 2e-3."},{"metadata":{"trusted":true},"cell_type":"code","source":"# unfreeze model\nlm_learner.unfreeze()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lm_learner.fit_one_cycle(10, 2e-3, moms=(0.8,0.7), wd=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lm_learner.save_encoder('fwd_enc_sg')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building the reverse language model\n\nNext we will do the same with the backwards model data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# init backwards model\nlm_backward = language_model_learner(data_lm_bwd, AWD_LSTM, metrics=[accuracy, error_rate]).to_fp16(clip=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train for 1 epoch\nlm_backward.fit_one_cycle(1, 4e-2, moms=(0.8,0.7), wd=0.1)\n# unfreeze model \nlm_backward.unfreeze()\n# train for another 10 epochs\nlm_backward.fit_one_cycle(10, 2e-3, moms=(0.8,0.7), wd=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set model directory so that encoder saves in the right directory\nlm_backward.model_dir = '../../working'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lm_backward.save_encoder('bwd_enc_sg')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Next we will train and fine-tune the classifier\n\n<em>Classifier tuning</em>\n\nTo fine-tune the classifier the pre-trained language model has two extra linear blocks added to it. These are the only parameters that are trained from scratch, taking as inputs the pooled states from the last hidden layer in the language model. Each block uses batch normalization [(Ioffe and Szegedy, 2015)](https://arxiv.org/abs/1502.03167) and dropout, with ReLU activations for the intermediate layer and a softmax activation that outputs a probability distribution over target classes at the last layer."},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the data for the forward classifier model using the language model vocabulary\nfwd_data_clas = (TextList.from_df(df_train, vocab=data_lm.vocab, cols='text')\n                          .split_by_rand_pct(0.15, seed=42) # 20% goes to validation set\n                          .label_from_df(cols='airline_sentiment')\n                          .databunch(bs=128))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fwd_data_clas.save('fwd_data_clas.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# look at a batch of the data\nfwd_data_clas.show_batch()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# saving the backward data for the backwards model\nbwd_data_clas = load_data(path, '../../working/fwd_data_clas.pkl', bs=bs, backwards=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# look at a batch of the backward model data\nbwd_data_clas.show_batch()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# initiate the classifier for the forward data\nfwd_clas = text_classifier_learner(fwd_data_clas, AWD_LSTM, drop_mult=0.5, pretrained=False, metrics=[accuracy, error_rate])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">We need to move some files around to make this work in the Kaggle notebook"},{"metadata":{"trusted":true},"cell_type":"code","source":"# move encoder to correct directory\n!mv ../../working/fwd_enc_sg.pth ./fwd_enc_sg.pth","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create models directory and move encoder there\n!mkdir models; mv ./fwd_enc_sg.pth models/fwd_enc_sg.pth","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">Next we will load the encoder we trained earlier into the classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the language model encoder into the classifer\nfwd_clas.load_encoder('fwd_enc_sg')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<em>Gradual unfreezing</em>\n\nInstead of fine-tuning all the layers at once (which may break our model and cause catastophic forgetting) Ruder and Howard propose training the model by gradually unfreezing layers, layer by layer - starting at the last layer. We also decrease our learning rate as we train deeper layers in the model. Models in the earlier layers contain more general knowledge and therefore need to be trained less [(Yosinski et al., 2014)](https://arxiv.org/abs/1411.1792)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# train last layer\nlr = 1e-1\nfwd_clas.fit_one_cycle(1, lr, moms=(0.8,0.7), wd=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train last two layers\nfwd_clas.freeze_to(-2)\nlr /= 2\nfwd_clas.fit_one_cycle(1, slice(lr/(2.6**4),lr), moms=(0.8,0.7), wd=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train last three layers\nfwd_clas.freeze_to(-3)\nlr /= 2\nfwd_clas.fit_one_cycle(1, slice(lr/(2.6**4),lr), moms=(0.8,0.7), wd=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train all layers\nfwd_clas.unfreeze()\nlr /= 5\nfwd_clas.fit_one_cycle(2, slice(lr/(2.6**4),lr), moms=(0.8,0.7), wd=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fwd_clas.save('fwd_clas_cp')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train the classifer a little more\nfwd_clas.fit_one_cycle(3, slice(lr/(2.6**4),lr), moms=(0.8,0.7), wd=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train the classifer a little more\n#fwd_clas.fit_one_cycle(3, slice(lr/(2.6**4),lr), moms=(0.8,0.7), wd=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fwd_clas.save('fwd_clas_fin')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our forward classifier has a validation error rate of 5.5%."},{"metadata":{},"cell_type":"markdown","source":"### Classifier in reverse\n\n>Training the reverse clasifier with the same hyper parameters as the forward model"},{"metadata":{"trusted":true},"cell_type":"code","source":"bwd_clas = text_classifier_learner(bwd_data_clas, AWD_LSTM, drop_mult=0.5, pretrained=False, metrics=[accuracy, error_rate])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bwd_clas.model_dir = '../../working'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bwd_clas.load_encoder('bwd_enc_sg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train only the last layer\nbwd_clas.fit_one_cycle(1, lr, moms=(0.8,0.7), wd=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Gradual unfreezing of the layers for the backward model..."},{"metadata":{"trusted":true},"cell_type":"code","source":"# train the last two layers\nbwd_clas.freeze_to(-2)\nlr /= 2\nbwd_clas.fit_one_cycle(1, slice(lr/(2.6**4),lr), moms=(0.8,0.7), wd=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train the last 3 layers\nbwd_clas.freeze_to(-3)\nlr /= 2\nbwd_clas.fit_one_cycle(1, slice(lr/(2.6**4),lr), moms=(0.8,0.7), wd=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train all the layers\nbwd_clas.unfreeze()\nlr /= 5\nbwd_clas.fit_one_cycle(2, slice(lr/(2.6**4),lr), moms=(0.8,0.7), wd=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train a little bit more\nbwd_clas.fit_one_cycle(3, slice(lr/(2.6**4),lr), moms=(0.8,0.7), wd=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# and a little bit more\nbwd_clas.fit_one_cycle(3, slice(lr/(2.6**4),lr), moms=(0.8,0.7), wd=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training the backward classifier for longer appears to improve model performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"# train an extra 15 epochs\nbwd_clas.fit_one_cycle(15, slice(lr/(2.6**4),lr), moms=(0.8,0.7), wd=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checkpoint\nbwd_clas.save('bwd_clas_fin')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<strong>Next we will calculate the ensemble model predictions</strong>\n\nWe do this by taking the mean predictions of both models combined."},{"metadata":{"trusted":true},"cell_type":"code","source":"# get forward classifier predictions and accuracy for validation set\nfwd_preds, fwd_targets = fwd_clas.get_preds(ordered=True)\nprint('Forward classifier results (validation set): \\nValidation accuracy: {:.2f}, Validation error rate: {:.2f}'.format(accuracy(fwd_preds, fwd_targets), error_rate(fwd_preds, fwd_targets)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get backward classifier predictions and accuracy for validation set\nbwd_preds, bwd_targets = bwd_clas.get_preds(ordered=True)\nprint('Backward classifier results (validation set): \\nValidation accuracy: {:.2f}, Validation error rate: {:.2f}'.format(accuracy(bwd_preds, bwd_targets), error_rate(bwd_preds, bwd_targets)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get combined(mean) predictions\nensemble_preds = (fwd_preds + bwd_preds)/2\n# get combined(mean) accuracy on validation set\nprint('Ensemble classifier results (validation set): \\nValidation accuracy: {:.2f}, Validation error rate: {:.2f}'.format(accuracy(ensemble_preds, fwd_targets), error_rate(ensemble_preds, fwd_targets)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## How well does the model generalize?\n\nNow that we've built our bi-directional classifier we want to see how well it generalizes. Let's test the model's performance on our test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the test data using datablock api - a somewhat hacky but quick way to load the test set into the model\ndata_test = (TextList.from_df(df_test, vocab=data_lm.vocab, cols='text')\n                          .split_subsets(train_size=0.01, valid_size=0.99) # the hacky part\n                          .label_from_df(cols='airline_sentiment')\n                          .databunch(bs=128))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load data into forward classifier\nfwd_clas.data = data_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hacky solution comes at a price - sometimes not all labels are loaded into the model\nassert fwd_clas.data.c == 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save test data\ndata_test.save('test_data.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save backward test data\nbwd_test = load_data(path, '../../working/test_data.pkl', bs=bs, backwards=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load data into backward classifier\nbwd_clas.data = bwd_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hacky solution comes at a price - sometimes not all labels are loaded into the model\nassert bwd_clas.data.c == 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get foward model predictions on test set\ntest_preds, test_targs = fwd_clas.get_preds(ds_type=DatasetType.Valid)\nprint('Forward classifier results (test set): \\nTest accuracy: {:.2f}, Test error rate: {:.2f}'.format(accuracy(test_preds, test_targs), error_rate(test_preds,test_targs)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get backward model predictions on test set\nbwdt_preds, bwdt_targs = bwd_clas.get_preds(ds_type=DatasetType.Valid)\nprint('Backward classifier results (test set): \\nTest accuracy: {:.2f}, Test error rate: {:.2f}'.format(accuracy(bwdt_preds, bwdt_targs),error_rate(bwdt_preds, bwdt_targs)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get bi-directional model predictions\nensemble_preds = (test_preds + bwdt_preds)/2\n# Get ensemble error\n# higher precision reporting - to be able to inspect model performance closer\nprint('Ensemble classifier results (test set): \\nTest accuracy: {:.4f}, Test error rate: {:.4f}'.format(accuracy(ensemble_preds, test_targs),error_rate(ensemble_preds, test_targs)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Evaluation\n\nOur ensemble model returns an error rate of 13.98% on the test set and 5.5% error on our validation set, an improvement on our benchmark's test error rate of 17.2%. Let's inspect the model's performance a little closer by looking at a classification matrix that plots the true vs. the predicted labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare data for confusion matrix\ny_pred = np.array(np.argmax(ensemble_preds, axis=1))\ny_true = np.array(test_targs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# quick check if all classes are present\nprint(y_pred)\nprint(y_true)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classification Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\ncmap = 'Reds'\ncm = confusion_matrix(y_true, y_pred)\n\ndef plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=cmap, normalize=False):\n  acc = np.trace(cm) / float(np.sum(cm))\n  err_rate = 1 - acc\n  plt.figure(figsize=(8, 6))\n  plt.imshow(cm, interpolation='nearest', cmap=cmap)\n  plt.title(title)\n  plt.colorbar()\n  if target_names is not None:\n      tick_marks = np.arange(len(target_names))\n      plt.xticks(tick_marks, target_names, rotation=45)\n      plt.yticks(tick_marks, target_names)\n  if normalize:\n      cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n  thresh =  cm.max() / 1.5 if normalize else cm.max() / 2\n  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):      \n      if normalize:\n          plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                   horizontalalignment=\"center\",\n                   color=\"grey\" if cm[i, j] > thresh else \"grey\")\n      else:\n          plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                   horizontalalignment=\"center\",\n                   color=\"grey\" if cm[i, j] > thresh else \"grey\")\n  plt.tight_layout()\n  plt.ylabel('True label')\n  plt.xlabel('Predicted label\\naccuracy={:0.4f}; error_rate={:0.4f}'.format(acc, err_rate))\n  plt.show()\n    \n    \nplot_confusion_matrix(cm=cm, target_names=['negative', 'neutral', 'positive'], \n                      title='Classifation Confusion Matrix')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The figure above shows both the test set's true labels and our model's predictions.\n\nOur model makes the least errors when given a negative tweet with the lowest error rate of around 4.4%.\n\nIt makes significantly more mistakes when the inputs are neutral or positive tweets with an error rate of 49% for neutral tweets (nearly half), and an eror rate of 30.6% (around a third) when given positive tweets.\n\nThe most common error seems to be confusing neutral with negative tweets, which may still be a symptom of the class imbalance. I'm also guessing that neutral tweets probably contain fewer or no words that signal the tweet's sentiment, possibly making it harder for the network to classify?"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(cm=cm, target_names=['negative', 'neutral', 'positive'], \n                      title='Classifation Distribution in Percent(Normalized)', normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n\nThis was a fun challenge and a great learning experience, given that this was my first time working on a natural language processing (NLP) related task. I hope that this notebook can be of use to others looking to learn more about creating an ULMFiT classifier. \n\nIt would be interesting to see if the model can be improved by testing more recent approaches to generating text using language models or possibly transformers.\n\nBelow is a list of helpful links with NLP learning resources.\n\n- [ULMFiT Paper Arxiv Link](https://arxiv.org/abs/1801.06146)\n- [Fast.ai NLP Classification Resources](http://nlp.fast.ai/category/classification.html)\n- [Sebastian Ruder's Blog](http://ruder.io/)\n- [Fast.ai NLP MOOC](https://www.youtube.com/playlist?list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}