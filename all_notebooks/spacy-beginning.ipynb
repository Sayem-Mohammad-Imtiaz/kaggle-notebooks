{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd488586eece0a871c9716d4b283ec56635f38db"},"cell_type":"code","source":"import spacy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"718a6a583671965461b3abee5a476154b687c3ce"},"cell_type":"code","source":"nlp = spacy.load(\"en\")\ndoc = nlp(\"Let's go to N.Y.!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3e1c4453627c046a31944d05883c47ad97b39e8"},"cell_type":"code","source":"doc.text.split()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb05d4b1b9997a013fd37ed9e6c50f740dd96392"},"cell_type":"code","source":"[token.orth_ for token in doc]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2248ee8cee7382726d1994599020627406e034d"},"cell_type":"code","source":"[(token, token.orth_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha) for token in doc]\n#when orth or any method is used without underscore then the hash value is shown because spacy does not \n#store the words but stores the hashes of the words. except methods starting with \"is\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4394c8c31462a1dc306de065facee84f64372193"},"cell_type":"code","source":"[(token,token.is_stop) for token in doc]\n#stop words are common words that dont have any influence over the context.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e7831ada62a8f87087925474a9062f63da224fc"},"cell_type":"code","source":"prac = \"practice practicing practiced\"\nnlp_p = nlp(prac)\n[word.lemma_ for word in nlp_p]\n#lemma brings down the word to its root word","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"675d125c447a03ae75e304c14c3b00a959d25a63"},"cell_type":"code","source":"doc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3527f7929ab65b25c8374935900ba98e750dbdd"},"cell_type":"code","source":"print([token for token in doc[5].lefts])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9dfbc475c21873cf066b30151de0a75785fd6851"},"cell_type":"code","source":"doc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41dbce429c68eec296f9bc1105bfcf8868bd7685"},"cell_type":"code","source":"nlp = spacy.load('en_core_web_sm')\ndoc = nlp(u\"Credit and mortgage account holders must submit their requests\")\n\nroot = [token for token in doc if token.head == token][0]\nsubject = list(root.lefts)[0]\nfor descendant in subject.subtree:\n    assert subject is descendant or subject.is_ancestor(descendant)\n    print(descendant.text, descendant.dep_, descendant.n_lefts,\n          descendant.n_rights,\n          [ancestor.text for ancestor in descendant.ancestors])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ecb5b0e2d6a8753b548faa042f01b85f7e7d9b5"},"cell_type":"code","source":"from spacy import displacy\ndisplacy.render(doc, style='dep', jupyter=True)\n#Visualising the dependencies (can also pass a list of docs in .render())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6444a769d04264b3de49dda108faa24256730e5f"},"cell_type":"code","source":"#Disabling the parser will make spaCy load and run much faster.\n#If you want to load the parser, but need to disable it for specific documents, \n#you can also control its use on the nlp object.\n#nlp = spacy.load('en', disable=['parser'])\n#doc = nlp(\"I don't want to be parsed\", disable=['parser'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a215ad81796de110df075ce98ad49e9a7f60430"},"cell_type":"code","source":"doc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a84abcdddba167be19cd65217c21384e34301f37"},"cell_type":"code","source":"#named entities are available as .ents property of the doc.\nfor ent in doc.ents:\n    print(ent, ent.start_char, ent.end_char, ent.label_)\n#does'nt prints anything because does not recognize the entities or maybe the sentence doesn't have any.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f5d655f7cb777162387eae3eae5e3a96dbc2f6b"},"cell_type":"code","source":"from spacy.tokens import Span\n\nnlp = spacy.load('en_core_web_sm')\ndoc = nlp(u\"FB is hiring a new Vice President of global policy\")\nents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\nprint('Before', ents)\n# the model didn't recognise \"FB\" as an entity :(\n\nORG = doc.vocab.strings[u'ORG']  # get hash value of entity label\nfb_ent = Span(doc, 0, 1, label=ORG) # create a Span for the new entity\ndoc.ents = list(doc.ents) + [fb_ent]\n\nents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\nprint('After', ents)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2e4caf938ffa09f6b4de39dfa970c3ed25b4bcc"},"cell_type":"code","source":"from spacy.attrs import ENT_IOB, ENT_TYPE\n\nnlp = spacy.load('en_core_web_sm')\ndoc = nlp.make_doc(u'London is a big city in the United Kingdom.')\nprint('Before', list(doc.ents))  # []\n\nheader = [ENT_IOB, ENT_TYPE]\nattr_array = np.zeros((len(doc), len(header)))\nattr_array[0, 0] = 3  # B\nattr_array[0, 1] = doc.vocab.strings[u'GPE']\ndoc.from_array(header, attr_array)\nprint('After', list(doc.ents))  # [London]\n#In this case, \"FB\" is token (0, 1) â€“ but at the document level, the entity will have the start and end indices (0, 2).","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"23ee6c777e0b90198793a2f6ce4eb58cbd8e71bf"},"cell_type":"code","source":"from spacy.lang.en import English\nfrom spacy.pipeline import SentenceSegmenter\n\ndef split_on_newlines(doc):\n    start = 0\n    seen_newline = False\n    for word in doc:\n        if seen_newline and not word.is_space:\n            yield doc[start:word.i]\n            start = word.i\n            seen_newline = False\n        elif word.text == '\\n':\n            seen_newline = True\n        if start < len(doc):\n            yield doc[start:len(doc)]\n\nnlp = English()\nsbd = SentenceSegmenter(nlp.vocab, strategy=split_on_newlines)\nnlp.add_pipe(sbd)\ndoc = nlp(u\"This is a sentence.\\nThis is another one.\\nand more\")\nfor sent in doc.sents:\n    print([token.text for token in sent])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41631b58a890508fccde115262163a3092b92e86"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}