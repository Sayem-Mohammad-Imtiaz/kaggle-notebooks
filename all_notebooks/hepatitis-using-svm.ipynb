{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Attribute information:**\n\n1. **target**: DIE (1), LIVE (2)\n2. **age**: 10, 20, 30, 40, 50, 60, 70, 80\n3. **gender**: male (1), female (2)\n\n           ------ no = 2,   yes = 1 ------\n\n4. **steroid**: no, yes \n5. **antivirals**: no, yes \n6. **fatique**: no, yes \n7. **malaise**: no, yes \n8. **anorexia**: no, yes \n9. **liverBig**: no, yes \n10. **liverFirm**: no, yes \n11. **spleen**: no, yes \n12. **spiders**: no, yes\n13. **ascites**: no, yes \n14. **varices**: no, yes\n15. **histology**: no, yes\n\n\n16. **bilirubin**: 0.39, 0.80, 1.20, 2.00, 3.00, 4.00 -- \n17. **alk**: 33, 80, 120, 160, 200, 250 ---\n18. **sgot**: 13, 100, 200, 300, 400, 500, ---\n19. **albu**: 2.1, 3.0, 3.8, 4.5, 5.0, 6.0, --- \n20. **protime**: 10, 20, 30, 40, 50, 60, 70, 80, 90, --- \n\n        NA's are represented with \"?\""},{"metadata":{},"cell_type":"markdown","source":"## Dataset Reading and Pre-Processing steps"},{"metadata":{},"cell_type":"markdown","source":"import required libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Code to ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### 1. Read the HEPATITIS dataset and check the data shapes"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Read \"hepatitis.csv\" using pandas\n# target =  1: Die; 2: Live \ndata = pd.read_csv(\"../input/hepatitis-dataset/hepatitis.csv\", na_values=\"?\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### 2. Check basic summary statistics of the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### 3. Check for value counts in target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4. Check the datatype of each variable"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"cat_cols = data.columns[data.nunique() < 5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols = data.columns[data.nunique() >= 5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5. Drop columns which are not significant"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop([\"ID\"], axis = 1, inplace=True)\nnum_cols = data.columns[data.nunique() >= 5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 6. Identify the Categorical Columns and store them in a variable cat_cols and numerical into num_cols"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols = [\"age\", \"bili\", \"alk\", \"sgot\", \"albu\", \"protime\"]\ncat_cols = ['gender', 'steroid', 'antivirals', 'fatigue', 'malaise', 'anorexia', 'liverBig', \n            'liverFirm', 'spleen', 'spiders', 'ascites', 'varices', 'histology']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 7. Checking the null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 8. Split the data into X and y"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop([\"target\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data[\"target\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X.shape, y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 9. Split the data into X_train, X_test, y_train, y_test with test_size = 0.20 using sklearn"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Print the shape of X_train, X_test, y_train, y_test\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 10. Check null values in train and test, check value_counts in y_train and y_test"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_train.value_counts()/X_train.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(y_test.value_counts()/X_test.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# null values in train\nX_train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# null values in test\nX_test.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 11. Impute the Categorical Columns with mode and Numerical columns with mean"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_cat_train = X_train[cat_cols]\ndf_cat_test = X_test[cat_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Impute on train\n# df_cat_train = df_cat_train.fillna(df_cat_train.mode().iloc[0])\n\n# Impute on test\n# df_cat_test = df_cat_test.fillna(df_cat_train.mode().iloc[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\ncat_imputer = SimpleImputer(strategy='most_frequent')\ncat_imputer.fit(df_cat_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cat_train = pd.DataFrame(cat_imputer.transform(df_cat_train), columns=cat_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cat_test = pd.DataFrame(cat_imputer.transform(df_cat_test), columns=cat_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_num_train = X_train[num_cols]\ndf_num_test = X_test[num_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Impute on train\n# df_num_train = df_num_train.fillna(df_num_train.mean())\n\n#Impute on test\n# df_num_test = df_num_test.fillna(df_num_train.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_imputer = SimpleImputer(strategy='median')\nnum_imputer.fit(df_num_train[num_cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_num_train = pd.DataFrame ( num_imputer.transform(df_num_train), columns= num_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_num_test =  pd.DataFrame(num_imputer.transform(df_num_test), columns=num_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combine numeric and categorical in train\nX_train = pd.concat([df_num_train, df_cat_train], axis = 1)\n\n# Combine numeric and categorical in test\nX_test = pd.concat([df_num_test, df_cat_test], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Convert all the categorical columns to Integer Format before dummification (2.0 as 2 etc.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train\nX_train[cat_cols] = X_train[cat_cols].astype('int')\n\n# Test\nX_test[cat_cols] = X_test[cat_cols].astype('int')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 12. Dummify the Categorical columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Convert Categorical Columns to Dummies\n# Train\nX_train = pd.get_dummies(X_train, columns=cat_cols, drop_first=True)\n\n# Test\nX_test = pd.get_dummies(X_test, columns=cat_cols, drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 13. Scale the numeric attributes [\"age\", \"bili\", \"alk\", \"sgot\", \"albu\", \"protime\"]"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#num_cols = [\"age\", \"bili\", \"alk\", \"sgot\", \"albu\", \"protime\"]\nscaler = StandardScaler()\n\nscaler.fit(X_train.loc[:,num_cols])\n\n# scale on train\nX_train.loc[:,num_cols] = scaler.transform(X_train.loc[:,num_cols])\n#X_train[num_cols] = scaler.transform(X_train[num_cols])\n\n# scale on test\nX_test.loc[:,num_cols] = scaler.transform(X_test.loc[:,num_cols])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MODEL BUILDING - SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a SVC classifier using a linear kernel\nlinear_svm = SVC(kernel='linear', C=1, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the classifier\nlinear_svm.fit(X=X_train, y= y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Predict\ntrain_predictions = linear_svm.predict(X_train)\ntest_predictions = linear_svm.predict(X_test)\n\n### Train data accuracy\nfrom sklearn.metrics import accuracy_score,f1_score,confusion_matrix\n\nprint(\"TRAIN Conf Matrix : \\n\", confusion_matrix(y_train, train_predictions))\nprint(\"\\nTRAIN DATA ACCURACY\",accuracy_score(y_train,train_predictions))\nprint(\"\\nTrain data f1-score for class '1'\",f1_score(y_train,train_predictions,pos_label=1))\nprint(\"\\nTrain data f1-score for class '2'\",f1_score(y_train,train_predictions,pos_label=2))\n\n### Test data accuracy\nprint(\"\\n\\n--------------------------------------\\n\\n\")\n\nprint(\"TEST Conf Matrix : \\n\", confusion_matrix(y_test, test_predictions))\nprint(\"\\nTEST DATA ACCURACY\",accuracy_score(y_test,test_predictions))\nprint(\"\\nTest data f1-score for class '1'\",f1_score(y_test,test_predictions,pos_label=1))\nprint(\"\\nTest data f1-score for class '2'\",f1_score(y_test,test_predictions,pos_label=2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"####  Non Linear SVM (RBF)"},{"metadata":{},"cell_type":"markdown","source":"Radial Basis Function is a commonly used kernel in SVC:<br>\n\n<img src=\"rbf_kernel.png\">\n\nwhere <math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n  <mrow class=\"MJX-TeXAtom-ORD\">\n    <mo stretchy=\"false\">|</mo>\n  </mrow>\n  <mrow class=\"MJX-TeXAtom-ORD\">\n    <mo stretchy=\"false\">|</mo>\n  </mrow>\n  <mrow class=\"MJX-TeXAtom-ORD\">\n    <mi mathvariant=\"bold\">x</mi>\n      <sub>i</sub>\n  </mrow>\n  <mo>&#x2212;<!-- − --></mo>\n  <mrow class=\"MJX-TeXAtom-ORD\">\n    <msup>\n      <mi mathvariant=\"bold\">x</mi>\n      <sub>j</sub>\n    </msup>\n  </mrow>\n  <mrow class=\"MJX-TeXAtom-ORD\">\n    <mo stretchy=\"false\">|</mo>\n  </mrow>\n  <msup>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mo stretchy=\"false\">|</mo>\n    </mrow>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <sup>2</sup>\n    </mrow>\n  </msup>\n</math>  is the squared Euclidean distance between two data points x<sub>i</sub> and x<sub>j</sub>\n\nIt is only important to know that an SVC classifier using an RBF kernel has two parameters: gamma and C.\n\n<strong>Gamma:</strong>\n\n- Gamma is a parameter of the RBF kernel and can be thought of as the ‘spread’ of the kernel and therefore the decision region. When gamma is low, the ‘curve’ of the decision boundary is very low and thus the decision region is very broad. When gamma is high, the ‘curve’ of the decision boundary is high, which creates islands of decision-boundaries around data points.\n\n<strong>C:</strong>\n\n- C is a parameter of the SVC learner and is the penalty for misclassifying a data point. When C is small, the classifier is okay with misclassified data points (high bias, low variance). When C is large, the classifier is heavily penalized for misclassified data and therefore bends over backwards avoid any misclassified data points (low bias, high variance).\n\n\n<strong>Kernel Trick:</strong><br>\nImage you have a two-dimensional non-linearly separable dataset, you would like to classify it using SVM. It looks like not possible because the data is not linearly separable. However, if we transform the two-dimensional data to a higher dimension, say, three-dimension or even ten-dimension, we would be able to find a hyperplane to separate the data.\n\n<img src=\"kernel_trick.png\">\n\nThe problem is, if we have a large dataset containing, say, millions of examples, the transformation will take a long time to run.<br>\nTo solve this problem, we actually only care about the result of the dot product (x<sub>i</sub>.x<sub>j</sub>)<br>\n<br>If there is a function which could calculate the dot product and the result is the same as when we transform the data into higher dimension, it would be fantastic. This function is called a kernel function.<br>\n<br>In essence, what the kernel trick does for us is to offer a more efficient and less expensive way to transform data into higher dimensions."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Create an SVC object and print it to see the arguments\nsvc = SVC(kernel='rbf', random_state=0, gamma=0.01, C=1)\nsvc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Train the model\nsvc.fit(X=X_train, y= y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Predict\ntrain_predictions = svc.predict(X_train)\ntest_predictions = svc.predict(X_test)\n\n### Train data accuracy\n\nprint(\"TRAIN Conf Matrix : \\n\", confusion_matrix(y_train, train_predictions))\nprint(\"\\nTRAIN DATA ACCURACY\",accuracy_score(y_train,train_predictions))\nprint(\"\\nTrain data f1-score for class '1'\",f1_score(y_train,train_predictions,pos_label=1))\nprint(\"\\nTrain data f1-score for class '2'\",f1_score(y_train,train_predictions,pos_label=2))\n\n### Test data accuracy\nprint(\"\\n\\n--------------------------------------\\n\\n\")\n\nprint(\"TEST Conf Matrix : \\n\", confusion_matrix(y_test, test_predictions))\nprint(\"\\nTEST DATA ACCURACY\",accuracy_score(y_test,test_predictions))\nprint(\"\\nTest data f1-score for class '1'\",f1_score(y_test,test_predictions,pos_label=1))\nprint(\"\\nTest data f1-score for class '2'\",f1_score(y_test,test_predictions,pos_label=2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SVM with Grid Search for Paramater Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Use Grid Search for parameter tuning\n\nfrom sklearn.model_selection import GridSearchCV\n\nsvc_grid = SVC()\n \nparam_grid = { \n                'C': [0.001, 0.01, 0.1, 1, 10, 100 ],\n                'gamma': [0, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100], \n                'kernel':['linear', 'rbf', 'poly' ]\n             }\n\nsvc_cv_grid = GridSearchCV(estimator = svc_grid, param_grid = param_grid, cv = 5, verbose=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Fit the grid search model\nsvc_cv_grid.fit(X=X_train, y=y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the best parameters\nsvc_cv_grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_best = svc_cv_grid.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Predict\ntrain_predictions = svc_best.predict(X_train)\ntest_predictions = svc_best.predict(X_test)\n\nprint(\"TRAIN DATA ACCURACY\",accuracy_score(y_train,train_predictions))\nprint(\"\\nTrain data f1-score for class '1'\",f1_score(y_train,train_predictions,pos_label=1))\nprint(\"\\nTrain data f1-score for class '2'\",f1_score(y_train,train_predictions,pos_label=2))\n\n### Test data accuracy\nprint(\"\\n\\n--------------------------------------\\n\\n\")\nprint(\"TEST DATA ACCURACY\",accuracy_score(y_test,test_predictions))\nprint(\"\\nTest data f1-score for class '1'\",f1_score(y_test,test_predictions,pos_label=1))\nprint(\"\\nTest data f1-score for class '2'\",f1_score(y_test,test_predictions,pos_label=2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}