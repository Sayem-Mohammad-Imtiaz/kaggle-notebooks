{"cells":[{"metadata":{"id":"hgAs0F-D2aBB","outputId":"50a3fbe0-b8eb-4416-9486-9a7c02190f1b","trusted":true},"cell_type":"code","source":"file_review = \"../input/yelp-reviews/yelp_reviews_100_thousand.csv\"","execution_count":null,"outputs":[]},{"metadata":{"id":"s8hv8eg_2wXH","trusted":true},"cell_type":"code","source":"# DataFrame\nimport pandas as pd\n\n# Matplot\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Scikit-learn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.manifold import TSNE\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM, Layer\nfrom keras import utils\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\n# nltk\nimport nltk\nfrom nltk.corpus import stopwords\nfrom  nltk.stem import SnowballStemmer\n\n# Word2vec\nimport gensim\n\n# Utility\nimport re\nimport numpy as np\nimport seaborn as sns\nimport os\nfrom collections import Counter\nimport logging\nimport time\nimport pickle\nimport itertools\n\n# Set log\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)","execution_count":null,"outputs":[]},{"metadata":{"id":"67iZGyHz1XUb","outputId":"a01221f6-954e-4702-8a86-647ed3272088","trusted":true},"cell_type":"code","source":"df = pd.read_csv(file_review)\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.text.tolist()[0] #review example","execution_count":null,"outputs":[]},{"metadata":{"id":"htd3KSnV2sYV","outputId":"909e91eb-72d5-422f-8079-f468dcab24e5","trusted":true},"cell_type":"code","source":"df.business_id.value_counts() #Number of reviews for each restaurant","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['stars'] == 1, 'stars'] = 0\ndf.loc[df['stars'] == 2, 'stars'] = 0\ndf.loc[df['stars'] == 3, 'stars'] = 1\ndf.loc[df['stars'] == 4, 'stars'] = 2\ndf.loc[df['stars'] == 5, 'stars'] = 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.stars.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.stars.value_counts().sort_values(ascending=False).plot(kind='bar', title='Number of reviews with each rating')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"Cleaning up the review"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install contractions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install inflect","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing required libraries\nimport nltk\nimport inflect\nimport contractions\nfrom bs4 import BeautifulSoup\nimport re, string, unicodedata\nfrom nltk import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import LancasterStemmer, WordNetLemmatizer\nfrom sklearn.preprocessing import LabelEncoder\n\n# First function is used to denoise text\ndef denoise_text(text):\n    # Strip html if any. For ex. removing <html>, <p> tags\n    soup = BeautifulSoup(text, \"html.parser\")\n    text = soup.get_text()\n    # Replace contractions in the text. For ex. didn't -> did not\n    text = contractions.fix(text)\n    return text\n\n# Check the function \nsample_text = \"<p>he didn't say anything </br> about what's gonna <html> happen in the climax\"\ndenoise_text(sample_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('punkt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Text normalization includes many steps.\n# Each function below serves a step.\ndef remove_non_ascii(words):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n        new_words.append(new_word)\n    return new_words\ndef to_lowercase(words):\n    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = word.lower()\n        new_words.append(new_word)\n    return new_words\ndef remove_punctuation(words):\n    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = re.sub(r'[^\\w\\s]', '', word)\n        if new_word != '':\n            new_words.append(new_word)\n    return new_words\ndef replace_numbers(words):\n    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n    p = inflect.engine()\n    new_words = []\n    for word in words:\n        if word.isdigit():\n            new_word = p.number_to_words(word)\n            new_words.append(new_word)\n        else:\n            new_words.append(word)\n    return new_words\ndef remove_stopwords(words):\n    \"\"\"Remove stop words from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        if word not in stopwords.words('english'):\n            new_words.append(word)\n    return new_words\ndef stem_words(words):\n    \"\"\"Stem words in list of tokenized words\"\"\"\n    stemmer = LancasterStemmer()\n    stems = []\n    for word in words:\n        stem = stemmer.stem(word)\n        stems.append(stem)\n    return stems\ndef lemmatize_verbs(words):\n    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n    lemmatizer = WordNetLemmatizer()\n    lemmas = []\n    for word in words:\n        lemma = lemmatizer.lemmatize(word, pos='v')\n        lemmas.append(lemma)\n    return lemmas\ndef normalize_text(words):\n    words = remove_non_ascii(words)\n    words = to_lowercase(words)\n    words = remove_punctuation(words)\n    words = replace_numbers(words)\n    words = remove_stopwords(words)\n    #words = stem_words(words)\n    words = lemmatize_verbs(words)\n    return words\n\n# Testing the functions\nprint(\"remove_non_ascii results: \", remove_non_ascii(['h', 'ॐ', '©', '1']))\nprint(\"to_lowercase results: \", to_lowercase(['HELLO', 'hiDDen', 'wanT', 'GOING']))\nprint(\"remove_punctuation results: \", remove_punctuation(['hello!!', 'how?', 'done,']))\nprint(\"replace_numbers results: \", replace_numbers(['1', '2', '3']))\nprint(\"remove_stopwords results: \", remove_stopwords(['this', 'and', 'amazing']))\nprint(\"stem_words results: \", stem_words(['beautiful', 'flying', 'waited']))\nprint(\"lemmatize_verbs results: \", lemmatize_verbs(['hidden', 'walking', 'ran']))\nprint(\"normalize_text results: \", normalize_text(['hidden', 'in', 'the', 'CAVES', 'he', 'WAited', '2', 'ॐ', 'hours!!']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenize review into words\ndef tokenize(text):\n    return nltk.word_tokenize(text)\n# check the function\nsample_text = 'he did not say anything  about what is going to  happen'\nprint(\"tokenize results :\", tokenize(sample_text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_prepare(text): #This code takes very long to run\n    text = denoise_text(text)\n    text = ' '.join([x for x in normalize_text(tokenize(text))])\n    return text\ndf['text'] = [text_prepare(x) for x in df['text']]\nle = LabelEncoder()\ndf['stars'] = le.fit_transform(df['stars'])\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Glove"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dropout, Dense, Embedding, LSTM, Bidirectional, Layer, Input\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model\nfrom tensorflow.keras.regularizers import l2\nimport keras.backend as K\nfrom sklearn.metrics import matthews_corrcoef, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.utils import shuffle\nimport numpy as np\nimport pickle\nimport matplotlib.pyplot as plt\nimport warnings\nimport logging\nlogging.basicConfig(level=logging.INFO)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_model_input(X_train, X_test,MAX_NB_WORDS=75000,MAX_SEQUENCE_LENGTH=500):\n    np.random.seed(7)\n    text = np.concatenate((X_train, X_test), axis=0)\n    text = np.array(text)\n    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n    tokenizer.fit_on_texts(text)\n    # pickle.dump(tokenizer, open('text_tokenizer.pkl', 'wb'))\n    # Uncomment above line to save the tokenizer as .pkl file \n    sequences = tokenizer.texts_to_sequences(text)\n    word_index = tokenizer.word_index\n    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n    print('Found %s unique tokens.' % len(word_index))\n    indices = np.arange(text.shape[0])\n    # np.random.shuffle(indices)\n    text = text[indices]\n    print(text.shape)\n    X_train_Glove = text[0:len(X_train), ]\n    X_test_Glove = text[len(X_train):, ]\n    embeddings_dict = {}\n    f = open(\"../input/glove-embedding/glove.6B.50d.txt\", encoding=\"utf8\")\n    for line in f:\n        values = line.split()\n        word = values[0]\n        try:\n            coefs = np.asarray(values[1:], dtype='float32')\n        except:\n            pass\n        embeddings_dict[word] = coefs\n    f.close()\n    print('Total %s word vectors.' % len(embeddings_dict))\n    return (X_train_Glove, X_test_Glove, word_index, embeddings_dict)\n\n# Check function\nx_train_sample = [\"Lorem Ipsum is simply dummy text of the printing and typesetting industry\", \"It is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout\"]\nx_test_sample = [\"I’m creating a macro and need some text for testing purposes\", \"I’m designing a document and don’t want to get bogged down in what the text actually says\"]\nX_train_Glove_s, X_test_Glove_s, word_index_s, embeddings_dict_s = prepare_model_input(x_train_sample, x_test_sample, 100, 20)\nprint(\"\\n X_train_Glove_s \\n \", X_train_Glove_s)\nprint(\"\\n X_test_Glove_s \\n \", X_test_Glove_s)\nprint(\"\\n Word index of the word testing is : \", word_index_s[\"testing\"])\nprint(\"\\n Embedding for thw word want \\n \\n\", embeddings_dict_s[\"want\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_eval_report(labels, preds):\n    mcc = matthews_corrcoef(labels, preds)\n    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n    precision = (tp)/(tp+fp)\n    recall = (tp)/(tp+fn)\n    f1 = (2*(precision*recall))/(precision+recall)\n    return {\n        \"mcc\": mcc,\n        \"true positive\": tp,\n        \"true negative\": tn,\n        \"false positive\": fp,\n        \"false negative\": fn,\n        \"pricision\" : precision,\n        \"recall\" : recall,\n        \"F1\" : f1,\n        \"accuracy\": (tp+tn)/(tp+tn+fp+fn)\n    }\ndef compute_metrics(labels, preds):\n    assert len(preds) == len(labels)\n    return get_eval_report(labels, preds)\ndef plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_'+string], '')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string, 'val_'+string])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.text\ny = df.stars\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n\nprint(\"Preparing model input ...\")\nX_train_Glove, X_test_Glove, word_index, embeddings_dict = prepare_model_input(X_train,X_test)\nprint(\"Done!\")\nprint(\"Building Model!\")","execution_count":null,"outputs":[]},{"metadata":{"id":"0c60AGBO9DSY","outputId":"687ed3b6-f86f-4bc7-fc92-8233b9eac900","trusted":true},"cell_type":"code","source":"print(\"Longest review's length is \", max([len(review) for review in df['text']]))","execution_count":null,"outputs":[]},{"metadata":{"id":"r5BDWDfT6qCy","outputId":"59920edf-4f4b-4385-8d2e-7e672811df0b","trusted":true},"cell_type":"code","source":"# Max number of words in each review.\nMAX_SEQUENCE_LENGTH = 3540\n# Make the embedding matrix using the embedding_dict\nEMBEDDING_DIM = 200\n\nembedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_dict.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        if len(embedding_matrix[i]) != len(embedding_vector):\n            print(\"could not broadcast input array from shape\", str(len(embedding_matrix[i])),\n                  \"into shape\", str(len(embedding_vector)), \" Please make sure your\"\n                                                            \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n            exit(1)\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_layer = Embedding(len(word_index) + 1,EMBEDDING_DIM,weights=[embedding_matrix],input_length=MAX_SEQUENCE_LENGTH,trainable=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preprocessing Word2Vec"},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DATASET\nDATASET_COLUMNS = [\"id\", \"sentiment\",\"content\"]\nDATASET_ENCODING = \"ISO-8859-1\"\nTRAIN_SIZE = 0.8\n\n# TEXT CLENAING\nTEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n\n# WORD2VEC \nW2V_SIZE = 300\nW2V_WINDOW = 7\nW2V_EPOCH = 32\nW2V_MIN_COUNT = 10\n\n# KERAS\nSEQUENCE_LENGTH = 300\nEPOCHS = 30\nBATCH_SIZE = 1024","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = stopwords.words(\"english\")\nstemmer = SnowballStemmer(\"english\")\ndef preprocess(text, stem=False):\n    # Remove link,user and special characters\n    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n    tokens = []\n    for token in text.split():\n        if token not in stop_words:\n            if stem:\n                tokens.append(stemmer.stem(token))\n            else:\n                tokens.append(token)\n    return \" \".join(tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.text = df.text.apply(lambda x: preprocess(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train, df_test = train_test_split(df, test_size=1-TRAIN_SIZE, random_state=42)\nprint(\"TRAIN size:\", len(df_train))\nprint(\"TEST size:\", len(df_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"documents = [_content.split() for _content in df_train.text] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model = gensim.models.word2vec.Word2Vec(size=W2V_SIZE, \n                                            window=W2V_WINDOW, \n                                            min_count=W2V_MIN_COUNT, \n                                            workers=8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.build_vocab(documents)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words = w2v_model.wv.vocab.keys()\nvocab_size = len(words)\nprint(\"Vocab size:\", vocab_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.most_similar(\"great\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(df_train.text)\n\nvocab_size = len(tokenizer.word_index) + 1\nprint(\"Total words: \", vocab_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = pad_sequences(tokenizer.texts_to_sequences(df_train.text), maxlen=SEQUENCE_LENGTH)\nx_test = pad_sequences(tokenizer.texts_to_sequences(df_test.text), maxlen=SEQUENCE_LENGTH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = df_train.stars.unique().tolist()\nprint(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = LabelEncoder()\nencoder.fit(df_train.stars.tolist())\n\ny_train = encoder.transform(df_train.stars.tolist())\ny_test = encoder.transform(df_test.stars.tolist())\n\ny_train = y_train.reshape(-1,1)\ny_test = y_test.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"x_train\", x_train.shape)\nprint(\"y_train\", y_train.shape)\nprint()\nprint(\"x_test\", x_test.shape)\nprint(\"y_test\", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.zeros((vocab_size, W2V_SIZE))\nfor word, i in tokenizer.word_index.items():\n  if word in w2v_model.wv:\n    embedding_matrix[i] = w2v_model.wv[word]\nprint(embedding_matrix.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_layer = Embedding(vocab_size, W2V_SIZE, weights=[embedding_matrix], input_length=SEQUENCE_LENGTH, trainable=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define model and train"},{"metadata":{"trusted":true},"cell_type":"code","source":"class attention(Layer):\n    def __init__(self,**kwargs):\n        super(attention,self).__init__(**kwargs)\n\n    def build(self,input_shape):\n        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n        super(attention, self).build(input_shape)\n\n    def call(self,x):\n        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n        at=K.softmax(et)\n        at=K.expand_dims(at,axis=-1)\n        output=x*at\n        return K.sum(output,axis=1)\n\n    def compute_output_shape(self,input_shape):\n        return (input_shape[0],input_shape[-1])\n\n    def get_config(self):\n        return super(attention,self).get_config()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_cnn_lstm(nclasses,  embedding_layer):\n    from keras.layers import Conv1D, MaxPooling1D\n    from tensorflow.keras.callbacks import ModelCheckpoint\n    from tensorflow.keras.regularizers import l2\n    # Initialize a sequebtial model\n    model = Sequential()\n    # model.add(Input(500,))\n    \n    # Add embedding layer\n    model.add(embedding_layer)\n    model.add(Conv1D(filters=64, kernel_size=2, padding='valid', activation='relu'))\n    model.add(Conv1D(filters=64, kernel_size=3, padding='valid', activation='relu'))\n    model.add(MaxPooling1D(pool_size=2))\n    model.add(LSTM(250, kernel_regularizer=l2(0.01)))\n    model.add(Dense(3, activation='softmax'))\n    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_cnn_lstm(3, embedding_layer)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Early Stopping\n#es = EarlyStopping(monitor='val_loss')\n# history = model.fit(X_train_Glove, y_train,\n#                     validation_data=(X_test_Glove,y_test),\n#                     epochs=30,\n#                     batch_size=128,\n#                     #callbacks=[es],\n#                     verbose=1)\nhistory = model.fit(x_train, y_train,\n                    validation_split=0.1,\n                    epochs=EPOCHS,\n                    batch_size=BATCH_SIZE,\n                    #callbacks=[es],\n                    verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted = model.predict_classes(X_test_Glove)\nindex = 16\nmax_index = max(predicted[index])\nle.inverse_transform([list(predicted[index]).index(max_index)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test[10:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"incorporating glove"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_Glove, X_test_Glove, word_index, embeddings_dict = prepare_model_input(Xtrain,Xtest)\n\nEMBEDDING_DIM = 50\nMAX_SEQUENCE_LENGTH = 3540\n\nembedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_dict.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        if len(embedding_matrix[i]) != len(embedding_vector):\n            print(\"could not broadcast input array from shape\", str(len(embedding_matrix[i])),\n                  \"into shape\", str(len(embedding_vector)), \" Please make sure your\"\n                                                            \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n            exit(1)\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"id":"JKvJm6fhouDm","outputId":"1f79f938-27c3-4296-ede1-79ce4b279793","trusted":true},"cell_type":"code","source":"from keras.layers import Conv1D, MaxPooling1D\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.regularizers import l2\nembedding_vecor_length = 32\nmodel = Sequential()\n#model.add(Embedding(MAX_NB_WORDS, embedding_vecor_length, input_length=X_train.shape[1]))\n# Add embedding layer\nmodel.add(Embedding(len(word_index) + 1,\n                            EMBEDDING_DIM,\n                            weights=[embedding_matrix],\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=True))\nmodel.add(Conv1D(filters=64, kernel_size=2, padding='valid', activation='relu'))\nmodel.add(Conv1D(filters=64, kernel_size=3, padding='valid', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(LSTM(250, kernel_regularizer=l2(0.01)))\nmodel.add(Dense(3, activation='softmax'))\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\nfilepath=\"weights_best_cnn.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max',save_weights_only=True)\ncallbacks_list = [checkpoint]\nmodel.fit(X_train_Glove, Ytrain, epochs=100, batch_size=128,verbose = 1,callbacks = callbacks_list,validation_data=(X_test_Glove,Ytest))","execution_count":null,"outputs":[]},{"metadata":{"id":"8jfKIkijXMo-","outputId":"1ccdd534-4342-40b5-aa09-cbcbcc16d457","trusted":true},"cell_type":"code","source":"from tensorflow import argmax\nfrom tensorflow.keras.backend import get_value\nresults = model.predict(X_test_Glove)\n#df.sentiment[argmax(x)]\nprint(set(results))\nsentiment_results = [get_value(argmax(x))+1 for x in results]\nlabel = [get_value(argmax(x))+1 for x in Ytest]\nprint(\"Predicted\\tstars\\ttext\")\n#Xtest = Xtest.tolist() #Uncomment or Comment this when there is error\nwrong = []\nright = []\nfor i in range(len(sentiment_results)):\n  if label[i]!=sentiment_results[i]:\n    wrong.append([sentiment_results[i], label[i], Xtest[i]])\n  else:\n    right.append([sentiment_results[i], label[i], Xtest[i]])\n    \n#print(len(sentiment_results),len(Y_test))\n    \nprint(\"WRONG\")\nprint(\"\\t\".join([str(x) for x in wrong[0]]))\nprint(\"\\t\".join([str(x) for x in wrong[1]]))\nprint(\"\\t\".join([str(x) for x in wrong[2]]))\nprint(\"\\n\")\n\nf = open(\"wrong.txt\", \"w\")\nf.write(\"Predicted\\tstars\\ttext\")\nf.write(\"\\n\")\nfor review in wrong:\n    f.write(\"\\t\".join([str(x) for x in review]))\n    f.write(\"\\n\")\nf.close()\n\ncount_model_sad = 0\ncount_model_happy = 0\nfor i in wrong:\n    if(i[0]<3 and i[1]>3):\n        count_model_sad += 1\n    elif(i[1]<3 and i[0]>3):\n        count_model_happy +=1\nprint(\"Proportion of model classifies positive as negative out of wrong: \", count_model_sad/len(wrong))\nprint(\"Proportion of model classifies negative as positive out of wrong: \", count_model_happy/len(wrong))\nprint(\"\\n\")\n\nf = open(\"right.txt\", \"w\")\nf.write(\"Predicted\\tstars\\ttext\")\nf.write(\"\\n\")\nfor review in right:\n    f.write(\"\\t\".join([str(x) for x in review]))\n    f.write(\"\\n\")\nf.close()\n\nprint(\"RIGHT\")\nprint(\"\\t\".join([str(x) for x in right[0]]))\nprint(\"\\t\".join([str(x) for x in right[1]]))\nprint(\"\\t\".join([str(x) for x in right[2]]))\nprint(\"\\n\")\n\nprint(\"accuracy: \",len(right)/(len(wrong)+len(right)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Can do confusion matrix ratings vs outputs\nAdd in yz's prepocessing"},{"metadata":{"id":"kDs57kOX1rjF","trusted":true},"cell_type":"code","source":"classifier_report(Y_test, results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}