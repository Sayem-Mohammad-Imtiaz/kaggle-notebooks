{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Table of Contents  <a class=\"anchor\" id=\"toc\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<div style=\"background: #f9f9f9 none repeat scroll 0 0;border: 1px solid #aaa;display: table;font-size: 95%;margin-bottom: 1em;padding: 20px;width: 400px;\">\n<h3>Contents</h3>\n</ul>\n<li style=\"list-style: outside none none !important;\"><a \n    \n>1 Data processing</a></li>\n      <ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\"><li style=\"list-style: outside none none !important;\"><a\n                                                                             href=\"#step1.1\">1.1 Handling Misspelled data</a></li>\n            <li style=\"list-style: outside none none !important;\"><a\nhref=\"#step1.2\">1.2 Handling Contractions</a></li>\n            <li style=\"list-style: outside none none !important;\"><a \nhref=\"#step1.3\">1.3 Replacing Abbreviations</a></li>\n            <li style=\"list-style: outside none none !important;\"><a                                                                                   href=\"#step1.4\">1.4 Visualizing length of tweets</a></li>\n            <li style=\"list-style: outside none none !important;\"><a \n                                                                     href=\"#step1.5\">1.5 Visualizing word count in each tweet</a></li>\n            <li style=\"list-style: outside none none !important;\"><a \nhref=\"#step1.6\">1.6 Collecting all words</a></li>\n </ul>\n<li style=\"list-style: outside none none !important;\"><a href=\"#step2.1\">2 Visualizing and data attributes</a></li>\n      <ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">\n            <li style=\"list-style: outside none none !important;\"><a href=\"#step2.1\">2.1 Viewing most common stop words used in tweets</a></li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#step2.2\">2.2 Viewing Punctuations in tweets</a></li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#step2.3\">2.3 Viewing Common words in tweets</a></li>  \n            <li style=\"list-style: outside none none !important;\"><a \nhref=\"#step2.4\">2.4 N-gram analysis</a></li>\n    \n </ul>\n<li style=\"list-style: outside none none !important;\"><a href=\"#step3.1\">3 Data cleaning</a></li>\n      <ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">    \n            <li style=\"list-style: outside none none !important;\"><a href=\"#step3.1\">3.1 Cleaning URLs and HTML tags</a></li>\n            <li style=\"list-style: outside none none !important;\"><a         href=\"#step3.2\">3.2 Cleaning Punctuations and emojis</a></li>\n            <li style=\"list-style: outside none none !important;\"><a \n                                                                     href=\"#step3.3\">3.3  Cleaning stop words </a></li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#step3.4\">3.4 Using Glo-Ve for word embeddings</a></li>\n            <li style=\"list-style: outside none none !important;\"><a \nhref=\"#step3.5\">3.5 Train-Test split </a></li>\n            <li style=\"list-style: outside none none !important;\"><a></li> \n</ul>\n<li style=\"list-style: outside none none !important;\"><a href=\"step4.1\">4 Creating Models</a></li>\n      <ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">\n            <li style=\"list-style: outside none none !important;\"><a href=\"#step4.1\">4.1 LSTM Model with Glove Embeddings</a></li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#step4.2\">4.2 Plotting accuracy and loss curves</a></li>\n            <li style=\"list-style: outside none none !important;\"><a  href=\"#step4.3\">4.3 LSTM with Glove Results</a></li>\n            <li style=\"list-style: outside none none !important;\"><a \n\n</ul>\n</div>","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport emoji\nimport keras\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport string\nimport re\nimport tensorflow as tf\n\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom nltk.tokenize import word_tokenize\n\nfrom collections import defaultdict\nfrom collections import Counter\n\nfrom sklearn import decomposition, model_selection,preprocessing, metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntest= pd.read_csv(\"../input/nlp-getting-started/test.csv\")\nprint(train.head(),train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(train['target'])\ntrain['target'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='step1.1'></a>\n1.1. Misspelled data\n\n<a class=\"anchor\" id=\"dp-re\"></a>\n<a href=\"#toc\">Back to the table of contents</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_PATH = '../input/spelling/aspell.txt'\nmisspell_data = pd.read_csv(DATA_PATH,\n                            sep=':',\n                            names=['correction', 'misspell'])\n\nmisspell_data.misspell = misspell_data.misspell.str.strip()\nmisspell_data.misspell = misspell_data.misspell.str.split(\" \")\nmisspell_data = misspell_data.explode(\"misspell\").reset_index(drop=True)\nmisspell_data.drop_duplicates(\"misspell\", inplace=True)\nmiss_corr = dict(zip(misspell_data.misspell, misspell_data.correction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def misspell_correction(inp):\n    for x in inp.split(): \n        if x in miss_corr.keys(): \n            inp = inp.replace(x, miss_corr[x])\n    return inp\n\ntrain[\"content\"] = train[\"text\"].apply(lambda x : misspell_correction(x))\ntest[\"content\"] = test[\"text\"].apply(lambda x : misspell_correction(x))\n\nprint(train[\"content\"].head())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='step1.2'></a>\n\n1.2. Contractions in words\n\n<a class=\"anchor\" id=\"dp-re\"></a>\n<a href=\"#toc\">Back to the table of contents</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"contractions = pd.read_csv(\"../input/contractions/contractions.csv\")\ncont_dic = dict(zip(contractions.Contraction, contractions.Meaning))\n\ndef cont_to_meaning(val): \n  \n    for x in val.split(): \n        if x in cont_dic.keys(): \n            val = val.replace(x, cont_dic[x]) \n    return val\n\ntrain[\"content\"] = train[\"content\"].apply(lambda x : cont_to_meaning(x))\ntest[\"content\"] = test[\"content\"].apply(lambda x : cont_to_meaning(x))\nprint(train[\"content\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='step1.3'></a>\n1.3. Replacing Abbreviations ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"abbreviations = pd.read_csv(\"../input/abbreviations-and-slangs-for-text-preprocessing/Abbreviations and Slang.csv\")\nabrevtn_dic = dict(zip(abbreviations.Abbreviations, abbreviations.Text))\n\ndef abbrev2_word(word):\n    word= word.lower()\n    if word in abrevtn_dic.keys():\n        return abrevtn_dic[word]\n    else: \n        return word\n\ndef abbrev2_text(text):\n    sentnc = word_tokenize(text)\n    sentnc = [abbrev2_word(word) for word in sentnc]\n    text = ' '.join(sentnc)\n    return text\n\ntrain[\"content\"] = train[\"content\"].apply(lambda x: abbrev2_text(x))\ntest[\"content\"] = test[\"content\"].apply(lambda x: abbrev2_text(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='step1.4'></a>\n\n1.4. Visualizing length of tweets  \n\n<a class=\"anchor\" id=\"dp-re\"></a>\n<a href=\"#toc\">Back to the table of contents</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain[\"length\"] = train[\"text\"].str.len()\nprint(train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfig, ax = plt.subplots(figsize=(10,5))\nplt.hist(train[train['target']==0]['length'],alpha = 0.6, bins=100, label='Not')\nplt.hist(train[train['target']==1]['length'],alpha = 1, bins=100, label='Yes')\nax.set(title= 'length of tweet Vs count for each length',xlabel= 'length',\n       ylabel='count',xlim=(0,160))\nax.legend(loc='upper right')\nplt.grid()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='step1.5'></a>\n\n1.5. Visualizing average length of word in each tweet\n\n<a class=\"anchor\" id=\"dp-re\"></a>\n<a href=\"#toc\">Back to the table of contents</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax=plt.subplots(figsize=(10,5))\n\ntrain['word1Len']=train[train['target']==1]['content'].str.count(' ') + 1\ntrain['word0Len']=train[train['target']==0]['content'].str.count(' ') + 1\nsns.distplot(train['word1Len'].map(lambda i: np.mean(i)),color='blue')\nsns.distplot(train['word0Len'].map(lambda i: np.mean(i)),color='red')\nax.set(xlabel='words in each tweet')\nprint(train.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='step1.6'></a>\n1.6. Collecting all words\n\n<a class=\"anchor\" id=\"dp-re\"></a>\n<a href=\"#toc\">Back to the table of contents</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"wordsIn0= []\nwordsIn1= []\n# collecting all words in each target\nfor row in train[train['target']==0]['content'].str.split():\n        for word in row:\n            wordsIn0.append(word)\nfor row in train[train['target']==1]['content'].str.split():\n        for word in row:\n            wordsIn1.append(word)\n            \nwords= wordsIn0+wordsIn1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='step2.1'></a>\n2.1. Viewing most common stop words used in tweets\n\n<a class=\"anchor\" id=\"dp-re\"></a>\n<a href=\"#toc\">Back to the table of contents</a>\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stop = stopwords.words('english')\ndic=defaultdict(int)\n\n#collecting stop words used in sentences            \nfor word in wordsIn0:\n    if word in stop:\n        dic[word]+=1\ntop0=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]\n\nfor word in wordsIn1:\n    if word in stop:\n        dic[word]+=1\ntop1=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n\n# plt.rcParams['figure.figsize'] = (18.0, 6.0)\nx,y=zip(*top0)\nplt.bar(x,y,color='pink',alpha=0.7)\nplt.title('Frequent Stopwords in Non didaster tweet')\nplt.show()\n\np,q=zip(*top1)\nplt.bar(p,q,color='brown')\nplt.title('Frequent Stopwords in disaster tweet')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2.2. Punctuations in tweets\n<a id='step2.2'></a>\n\n<a class=\"anchor\" id=\"dp-re\"></a>\n<a href=\"#toc\">Back to the table of contents</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dic=defaultdict(int)\npunc = string.punctuation\nfor i in wordsIn0:\n    if i in punc:\n        dic[i]+=1\nx,y=zip(*dic.items())\n\nfor i in wordsIn1:\n    if i in punc:\n        dic[i]+=1  \np,q=zip(*dic.items())\n\nplt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nplt.bar(x,y)\nplt.subplot(122)\nplt.bar(p,q)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2.3. Common words in tweets\n<a id='step2.3'></a>\n\n<a class=\"anchor\" id=\"dp-re\"></a>\n<a href=\"#toc\">Back to the table of contents</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"counter0=Counter(words)\ncounter1=Counter(wordsIn1)\n\ncommon0=counter0.most_common()\ncommon1=counter1.most_common()\n\nx0,x1=[],[]\ny0,y1=[],[]\nfor word,count in common0[:40]:\n    if (word not in stop) :\n        x0.append(word)\n        y0.append(count)\nfor word,count in common1[:40]:\n    if (word not in stop) :\n        x1.append(word)\n        y1.append(count)        \nplt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.barplot(x=y0,y=x0)\nplt.subplot(122)\nsns.barplot(x=y1,y=x1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2.4. N-gram analysis \n> checking for bi-grams\n<a id='step2.4'></a>\n\n<a class=\"anchor\" id=\"dp-re\"></a>\n<a href=\"#toc\">Back to the table of contents</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def textBiGrams(line, n=None):\n    c_vector = CountVectorizer(ngram_range=(2, 2)).fit(line)\n    words_set = c_vector.transform(line)\n    freq = words_set.sum(axis=0) \n    words_freq = [(x,freq[0, ind]) for x, ind in c_vector.vocabulary_.items()]\n    freq_used =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return freq_used[:n]\n\nplt.figure(figsize=(16,5))\ntop_bi_grams= textBiGrams(train['content'])[:10]\nx,y=map(list,zip(*top_bi_grams))\nsns.barplot(x=y,y=x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **3. DATA CLEANING**\n\n3.1. Cleaning URLs and HTML tags\n<a id='step3.1'></a>\n\n<a class=\"anchor\" id=\"dp-re\"></a>\n<a href=\"#toc\">Back to the table of contents</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def url_html_remove(line):\n    url = re.compile(r'https?://\\S+|www\\.\\S+<.*?>')\n    x= url.sub(r'',line)\n    html=  re.compile('<.*?>')\n    y=html.sub(r'',x)\n    return y\n    \ntrain[\"clean_content\"]=train.content.apply(lambda x: url_html_remove(x))\ntest[\"clean_content\"]=test.content.apply(lambda x: url_html_remove(x))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3.2. Cleaning Punctuations and Emojis\n<a id='step3.2'></a>\n\n<a class=\"anchor\" id=\"dp-re\"></a>\n<a href=\"#toc\">Back to the table of contents</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def punct_rem(val):   \n    for x in string.punctuation: \n        if x in val: \n            val = val.replace(x, \" \") \n    return val\ntrain['clean_content']= train['clean_content'].apply(lambda x:' '.join(punct_rem(emoji.demojize(x)).split()))\ntest['clean_content']= test['clean_content'].apply(lambda x:' '.join(punct_rem(emoji.demojize(x)).split()))\n\nprint(train.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3.3. Cleaning stop words\n<a id='step3.3'></a>\n\n<a class=\"anchor\" id=\"dp-re\"></a>\n<a href=\"#toc\">Back to the table of contents</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['cleaned_words']= train['clean_content'].apply(lambda x:[i for i in x.split() if i not in  stop])\n\ntest['cleaned_words']= test['clean_content'].apply(lambda x:[i for i in x.split() if i not in  stop])\nprint(train['cleaned_words'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3.4. Bag of words\n<a id='step3.4'></a>\n\n<a class=\"anchor\" id=\"dp-re\"></a>\n<a href=\"#toc\">Back to the table of contents</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def cv(data):\n    count_vectorizer = CountVectorizer()\n\n    emb = count_vectorizer.fit_transform(data)\n\n    return emb, count_vectorizer\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3.4. Using Glo-Ve for word embeddings\n<a id='step3.4'></a>\n\n<a class=\"anchor\" id=\"dp-re\"></a>\n<a href=\"#toc\">Back to the table of contents</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"word_embeddings={}\nwith open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt','r') as f:\n    for line in f:\n        data=line.split()\n        x = data[0]\n        vectors=np.asarray(data[1:],'float32')\n        word_embeddings[x]=vectors\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\n\ncorpus= []\nfor x in tqdm(train[\"clean_content\"]):\n    words=[word.lower() for word in word_tokenize(x)]\n    corpus.append(words)\nfor x in tqdm(test[\"clean_content\"]):\n    words=[word.lower() for word in word_tokenize(x)]\n    corpus.append(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nl=50\ntoken=Tokenizer()\ntoken.fit_on_texts(corpus)\nseq=token.texts_to_sequences(corpus)\n\ntext=pad_sequences(seq,maxlen=l,truncating='post',\n                        padding='post')\nindex=token.word_index\nprint(len(index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len_matrix=len(index)+1\nword_matrix=np.zeros((len_matrix,100))\n\nfor word,i in tqdm(index.items()):\n    if i < len_matrix:\n        vec=word_embeddings.get(word)\n        if vec is not None:\n            word_matrix[i]=vec","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3.5. Train-Test split\n<a id='step3.5'></a>\n\n<a class=\"anchor\" id=\"dp-re\"></a>\n<a href=\"#toc\">Back to the table of contents</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"length= train.shape[0]\nprint(\"length:\",length)\ntrain_data = text[:length]\ntest_data=text[length:]\n\nlabels = train[\"target\"].values\n\nx_train, x_val, y_train, y_val = train_test_split(train_data,labels, test_size=0.2, \n                                                random_state=10)\n\nprint('train data shape', x_train.shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.1. LSTM Model with GloVe embeddings\n<a id='step4.1'></a>\n\n<a class=\"anchor\" id=\"dp-re\"></a>\n<a href=\"#toc\">Back to the table of contents</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Embedding\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam\n\nmodel=Sequential()\n\nembedding=Embedding(len_matrix,100,embeddings_initializer=Constant(word_matrix),\n                   input_length=l,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(50, activation='relu'))\n\n\nmodel.add(Dense(1, activation='sigmoid'))\noptimzer=Adam(learning_rate=5e-5)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,\n              metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_history=model.fit(x_train,y_train,batch_size=16,epochs=15,\n                  validation_data=(x_val,y_val),verbose=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Epoch 10/10 batch_size=4, no extra layer\n1523/1523 - 274s - loss: 0.4250 - accuracy: 0.8148 - val_loss: 0.4205 - val_accuracy: 0.8227\n> Epoch 10/10 batch_size=4, 1 extra layer\n1523/1523 - 277s - loss: 0.4239 - accuracy: 0.8151 - val_loss: 0.4202 - val_accuracy: 0.8102\n> Epoch 10/10 batch_size=8, 1 extra layer\n762/762 - 142s - loss: 0.3759 - accuracy: 0.8378 - val_loss: 0.4352 - val_accuracy: 0.8148","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"4.2. Plotting accuracy and loss curves\n<a id='step4.2'></a>\n\n<a class=\"anchor\" id=\"dp-re\"></a>\n<a href=\"#toc\">Back to the table of contents</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_plots(neural_ntwk):\n    loss_vals = neural_ntwk['loss']\n    val_loss_vals = neural_ntwk['val_loss']\n    epochs = range(1, len(neural_ntwk['accuracy'])+1)\n    \n    f, ax = plt.subplots(nrows=1,ncols=2,figsize=(16,4))\n    \n    ax[0].plot(epochs, loss_vals, color='R',marker='o',\n               linestyle=' ', label='Train Loss')\n    ax[0].plot(epochs, val_loss_vals, color='B',\n               marker='*', label='Val Loss')\n    ax[0].set(title='Train & Val Loss', xlabel='Epochs',ylabel='Loss')\n    ax[0].legend(loc='best')\n    ax[0].grid(True)\n    \n    # plot accuracies\n    acc_vals = neural_ntwk['accuracy']\n    val_acc_vals = neural_ntwk['val_accuracy']\n\n    ax[1].plot(epochs, acc_vals, color='navy', marker='o',\n               ls=' ', label='Train Accuracy')\n    ax[1].plot(epochs, val_acc_vals, color='firebrick',\n               marker='*', label='Val Accuracy')\n    ax[1].set(title='Train & Val Accuracy',xlabel='Epochs',ylabel='Accuracy')\n    ax[1].legend(loc='best')\n    ax[1].grid(True)\n    \n    plt.show()\n    plt.close()\n    \n    del loss_vals, val_loss_vals, epochs, acc_vals, val_acc_vals\n# show_plots(neural_ntwk1.history)\nshow_plots(lstm_history.history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4.3 LSTM with Glove Results\n<a id='step4.3'></a>\n\n<a class=\"anchor\" id=\"dp-re\"></a>\n<a href=\"#toc\">Back to the table of contents</a>\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tGloVe = model.predict(test_data)\ntest_pred_GloVe_int = tGloVe.round().astype('int')\nprint(test_pred_GloVe_int)\npred= np.concatenate(test_pred_GloVe_int)\nprint(pred)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ids =test['id']\n\noutput = pd.DataFrame({'id':ids,\n                      'target': pred})\nprint(output)\noutput.to_csv('realfake_pred.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}