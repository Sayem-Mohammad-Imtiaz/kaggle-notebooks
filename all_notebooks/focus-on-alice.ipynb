{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Alice","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Alice competition\n# -> interpret weigths with eli5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! pwd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH_TO_DATA = '../input/'\nSEED = 17","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission 1: Bag of Sites","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(PATH_TO_DATA+'catch-me-if-you-can/train_sessions.csv',index_col='session_id')\ntest_df = pd.read_csv(PATH_TO_DATA+'catch-me-if-you-can/test_sessions.csv',index_col='session_id')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# change time columns into datetime format\n\ntime_col = ['time%s'%i for i in range(1,11)]\ntime_col","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[time_col] = train_df[time_col].apply(pd.to_datetime)\ntest_df[time_col] = test_df[time_col].apply(pd.to_datetime)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"site_col = ['site%s' %i for i in range(1,11)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# open file of sites and index\nwith open(PATH_TO_DATA+'mlcourse/site_dic.pkl','rb') as file :\n    site2id = pickle.load(file)\n    \n    \nsite2id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list of websites containing youtube\npd.Series(site2id)[pd.Series(site2id).index.str.contains('youtube')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the dict file is site(key): id(value) [site2id]\n# we want instead id2site because our dataframe contains id and we want to change it with sites\n\nid2site = {v:k for (k,v) in site2id.items()}\nid2site","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"id2site[12836]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## need to do a bag of words with sites name instead of id\n# BoW needs a list with each element being 1 observation to create several columns\n# simplest textformatting is countvectorizer \n# td idf is more complex text formatting\n\n# change id of site into name\n# -> fillna(0) for NAN values\n# in the dict create index 0 : unknown site \n\nid2site[0] = 'unknown'\n\n# before going to sparse data we need to sort by date to be able to do CV with timeseries\ntrain_df = train_df.sort_values(by = 'time1')\n\n\n# list train sites : 1 row = 1 element in the list separated by space\n#.tolist() : from series to list\n\n\ntrain_sessions = train_df[site_col].fillna(0).apply(lambda row : ' '.join([id2site[i] for i in row]),axis = 1).tolist()\n\ntest_sessions = test_df[site_col].fillna(0).apply(lambda row : ' '.join([id2site[i] for i in row]),axis = 1).tolist()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# diff btw countvectorizer and tfidvectorizer?\n# countvectorizer counts word frequency in the document (row)\n# tfidf vectorizer counts word frequency in the document and adjust inversely proportionate \n# to the freauency in the corpus (whole data)\n# Why? words like 'the' and 'we' appear in every document -> they dont tell us much about what makes a document unique\n# but if a word like \"crepuscular\" appear a lot in a document but not freaquently in the corpus\n# then it will give us much more info on the document \n\n# in countvectorizer 'the' has much more weights than 'crepuscular' which makes no sense\n# in conclusion : it is a way to penalize frequent words\n\n\nvectorizer = TfidfVectorizer(ngram_range=(1,5), max_features = 50000, tokenizer = lambda s : s.split())\n\n# we want to split words only by space not by dots or commas\n# -> tokenizer allows us to define how to split words\n# -> string.split() does it on spaces","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sessions[0].split()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Link : [Tf IDF](https://www.quora.com/What-is-the-difference-between-TfidfVectorizer-and-CountVectorizer-1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# only columns from sites into sparse format\nX_train_sites = vectorizer.fit_transform(train_sessions)\nX_test_sites = vectorizer.transform(test_sessions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sites","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train_df['target'].values #numpy\ny_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_times, test_times = train_df[time_col], test_df[time_col]\n\ntrain_times","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sites.shape, X_test_sites.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample of new features\nvectorizer.get_feature_names()[10000:10010]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## CV schedule with timeseries","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import TimeSeriesSplit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"time_split = TimeSeriesSplit(n_splits = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logit = LogisticRegression(random_state=SEED,solver ='liblinear',C=1)\n# liblinear is the typical gradient descent ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# we will do a cross validation with time split -> need to order train with time date\n\n# train only with sites data\n\ncv_score1 = cross_val_score(estimator=logit,X=X_train_sites,y=y_train, cv = time_split, n_jobs = -1,scoring = 'roc_auc')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_score1, cv_score1.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train with all sites data\n\nlogit.fit(X_train_sites,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install eli5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import eli5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# new columns name\nvectorizer.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model feature weigths with eli5\n\neli5.show_weights(estimator = logit, feature_names = vectorizer.get_feature_names(), top = 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import display_html","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Display the HTML representation of an object\ndisplay_html(eli5.show_weights(estimator = logit, feature_names = vectorizer.get_feature_names(), top = 30))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# these are the websites that are the most descriptive of Alice.\n# interesting enough she is not using gmail ...","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now let's predict and make a submission file \n\n# predict \nlogit_test_pred = logit.predict_proba(X_test_sites)[:,1]\n\nlogit_test_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submission\npredicted_df = pd.DataFrame(logit_test_pred,columns = ['target'], index = np.arange(1,logit_test_pred.shape[0]+1))\n\npredicted_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_df.to_csv('subm1.csv',index_label = 'session_id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We are going to repeat these steps several times so let's turn them into functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# a helper function for writing predictions to a file \n\ndef write_to_submission_file(predicted_labels,output_file,target_name = 'target',index_label='session_id'):\n    predicted_df = pd.DataFrame(predicted_labels,columns = [target_name], index = np.arange(1,predicted_labels.shape[0]+1))\n    predicted_df.to_csv(output_file,index_label = index_label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"write_to_submission_file(logit_test_pred,'subm1.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# a function that performs CV, model training, displaying feature importance,\n# making predictions and forming submission file\n\ndef train_and_predict(model,X_train,y_train,X_test,site_feature_names = vectorizer.get_feature_names(),new_feature_names = None,cv=time_split,scoring='roc_auc'\n                     ,top_n_features_to_show = 30,submission_file_name='submission.csv'):\n    \n    cv_scores = cross_val_score(estimator = model,X=X_train,y=y_train,cv=cv,scoring=scoring,n_jobs=-1)\n    print('CV scores', cv_scores)\n    print('CV mean: {}, CV std: {}'.format(cv_scores.mean(),cv_scores.std()))\n    \n    model.fit(X_train,y_train)\n    \n    if new_feature_names:\n        all_feature_names = site_feature_names + new_feature_names\n    else:\n        \n        all_feature_names = site_feature_names\n    \n    display_html(eli5.show_weights(model,feature_names = all_feature_names, top = top_n_features_to_show))\n    \n    if new_feature_names:\n        print('New feature weights:')\n        print(pd.DataFrame({'feature': new_feature_names,\n                            'coef': model.coef_.flatten()[-len(new_feature_names):]}))\n        \n    test_pred = model.predict_proba(X_test)[:,1]\n    \n    \n    write_to_submission_file(test_pred,submission_file_name)\n    \n    return cv_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_scores1 = train_and_predict(model = logit, X_train=X_train_sites,y_train=y_train,X_test=X_test_sites,\n                              site_feature_names= vectorizer.get_feature_names(),cv=time_split,\n                               submission_file_name= 'subm1.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission 2: Coming up with time features via EDA","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Really good link for EDA on this dataset [link](https://www.kaggle.com/adityaecdrid/initial-eda)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's focus now on time columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# intuition : people visit websites at specific moments \n# -> what is the distribution of visiting hours?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"session_start_hour = train_times['time1'].apply(lambda ts : ts.hour).values\nsession_start_hour","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport seaborn as sns\nsns.set()\nfrom matplotlib import pyplot as plt\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.countplot(session_start_hour);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compare now distribution of target 1 and target 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax =plt.subplots(1,2,figsize = (12,8))\n\nsns.countplot(session_start_hour[y_train == 1],ax=ax[0])\nax[0].set_title('Alice')\nax[0].set(xlabel = 'Session start hour')\nsns.countplot(session_start_hour[y_train == 0],ax=ax[1])\nax[1].set_title('Others')\nax[1].set(xlabel = 'Session start hour');\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Alice prefers 4-5 pm for browsing\n# -> create time features : morning, day, evening, night\n\nmorning = ((session_start_hour >= 7) & (session_start_hour <= 11)).astype('int')\n\nday = ((session_start_hour >= 12) & (session_start_hour <= 18)).astype('int')\n\nevening = ((session_start_hour >= 19) & (session_start_hour <= 23)).astype('int')\n\nnight = ((session_start_hour >= 0) & (session_start_hour <= 6)).astype('int')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab([morning,day,evening,night],y_train, rownames=['morning','day','evening','night'],colnames=['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will create and add new features (morning,...)\n# we will also keep a flag of whether we add an hour feature or not (overfitting???)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.sparse import hstack","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"objects_to_hstack = [X_train_sites, morning.reshape(-1,1),day.reshape(-1,1),evening.reshape(-1,1),night.reshape(-1,1)]\n    \n#add together -> the method to add new features with sparse matrix is to use hstack() from scipy.sparse \n# dont forget to reshape(-1,1) to be able to concatenate\nX_train_with_times1 = hstack(objects_to_hstack)\n\nX_train_with_times1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now that we added features for train set let's do it for test set \n# but now with a function","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_time_features(df_times,X_sparse,add_hour = True):\n    \"\"\"Add time features to sparse matrix\"\"\"\n    \n    hour = df_times['time1'].apply(lambda ts : ts.hour)\n    \n    morning = ((hour >= 7) & (hour <= 11)).astype(int).values.reshape(-1,1)\n    day = ((hour >= 12) & (hour <= 18)).astype(int).values.reshape(-1,1)\n    evening = ((hour >= 19) & (hour <= 23)).astype(int).values.reshape(-1,1)\n    night  = ((hour >= 0) & (hour <= 6)).astype(int).values.reshape(-1,1)\n    \n    objects_to_hstack = [X_sparse,morning,day,evening,night]\n    feature_names = ['morning','day','evening','night']\n    \n    if add_hour:\n        # adding hour features if required\n        \n        objects_to_hstack.append(hour.values.reshape(-1,1)/24)\n        # divided by 24 to normalize\n        \n        feature_names.append('hour')\n        \n    \n    X = hstack(objects_to_hstack)\n    \n    return X,feature_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add time features for train and test sparse matrix\n\nX_train_with_times1, new_feat_names = add_time_features(train_times, X_train_sites)\nX_test_with_times1 , _= add_time_features(test_times, X_test_sites)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# same number of columns\nX_train_with_times1.shape , X_test_with_times1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_feat_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## TimeSeries CrossValidation with ROC_AUC\n# we will use our function train_and_predict to make CV + graphs and feature importance + fit & predict_proba","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_scores2 =train_and_predict(model=logit,X_train=X_train_with_times1,y_train=y_train,\n                 X_test = X_test_with_times1, new_feature_names = new_feat_names,\n                 submission_file_name= 'subm2.csv',cv=time_split)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# it looks like hour colums is very important","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_scores2 > cv_scores1\n\n# we see an increase in nearly every folds compared to previous crossvalidation\n# better results ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LB : 0.91803 -> 0.93132\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission 3: Example of overfitting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# hour feature looks too important -> suspicious\n# redo the work without hour\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_with_times2, new_feat_names = add_time_features(df_times=train_times,\n                                                       X_sparse=X_train_sites,\n                                                      add_hour= False)\nX_test_with_times2, _ = add_time_features(df_times=test_times,X_sparse=X_test_sites,\n                                         add_hour = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_scores3 = train_and_predict(model = logit, X_train=X_train_with_times2,y_train=y_train,\n                              X_test=X_test_with_times2,new_feature_names=new_feat_names,\n                              submission_file_name='subm3.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CV is more stable without hour \n# and the prediction is better ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_scores3 > cv_scores1\n# better for every folds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_scores3 > cv_scores2\n# only better in half time \n# but we choose the third one because LESS VARIATION in CV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LB : 0.93132 -> 0.94522\n# -> hour feature leads to overfitting -> better not to add it","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Conclusion\n# Basically when you see a feature too important \n# first reflex is to drop it and see if the results drop as well\n# if not then this feature is causing overfitting","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Submissions 4 & 5 : The importance of feature scaling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# for this submission we will create a new feature : sesssion duration","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# first time we will do it incorrectly (without scaling)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# axis = 1 gives max row wise and timedelta64[ms] expressed in millisecond (float) then int to express it in int\n(train_times.max(axis = 1) - train_times.min(axis = 1)).astype('timedelta64[ms]').astype('int')\n                                                               ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# redo in a function form\n# remember hstack accepts a list of 2D arrays -> .values.reshape(-1,1)\n\ndef add_session_duration_incrorrect(df_times,X_sparse):\n    new_feat = (df_times.max(axis = 1) - df_times.min(axis = 1)).astype('timedelta64[ms]').astype('int')\n    return hstack([X_sparse,new_feat.values.reshape(-1,1)])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add feature for train and test sparse matrix\n\nX_train_with_time_incorrect = add_session_duration_incrorrect(train_times,X_train_with_times2)\n\nX_test_with_time_incorrect = add_session_duration_incrorrect(test_times,X_test_with_times2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_scores4 = train_and_predict(model=logit,X_train=X_train_with_time_incorrect,\n                              y_train=y_train,X_test= X_test_with_time_incorrect,\n                               new_feature_names=new_feat_names+['sess_duration'],\n                              cv = time_split, submission_file_name= 'subm4.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# HUGE DETERIORATION OF RESULTS !!!!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reason ? session duration is expressed in millisecond -> high values \n# Too much weight given -> more difficult to do optimal Gradient Descent\n# NEED to perform feature scaling","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_durations = (train_times.max(axis = 1) - train_times.min(axis=1)).astype('timedelta64[ms]').astype('int')\ntest_durations = (test_times.max(axis = 1) - test_times.min(axis=1)).astype('timedelta64[ms]').astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scaling features (fit_tansform on train and transform on test)\n\nscaler = StandardScaler()\n\ntrain_dur_scaled = scaler.fit_transform(train_durations.values.reshape(-1,1))\ntest_dur_scaled = scaler.transform(test_durations.values.reshape(-1,1))\n\n#.values.reshape is not necessary but it is for hstack -> we can do it now","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_durations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dur_scaled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_with_time_correct = hstack([X_train_with_times2,train_dur_scaled])\n\nX_test_with_time_correct = hstack([X_test_with_times2,test_dur_scaled])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_with_time_correct.shape, X_test_with_time_correct.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_scores5 = train_and_predict(model = logit, X_train=X_train_with_time_correct,\n                              y_train=y_train, X_test=X_test_with_time_correct,\n                              new_feature_names=new_feat_names+['sess_duration'],\n                              cv = time_split, submission_file_name='subm5.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_scores5 > cv_scores3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# New model better on 9 folds over 10 \n# LB : 0.94522 -> 0.94616\n# A bit better ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission 6 : Adding more time features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# a really good practice (especially to come up with new features)\n# is to look at new kernels (from competition) to get new ideas","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will add here month (june,july,...) and day of week (Monday,Tuesday,...) + yearmonth (202001,...202012)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# weekday\ntrain_times['time1'].apply(lambda ts: ts.weekday())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# month\ntrain_times['time1'].apply(lambda ts: ts.month)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# year month\n# divided by 100.000 for scalin reasons\ntrain_times['time1'].apply(lambda ts : 100 * ts.year + ts.month)/1e5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a function to apply on train and test\n\ndef add_day_month(df_times,X_sparse):\n    weekday = df_times['time1'].apply(lambda ts: ts.weekday())\n    month = df_times['time1'].apply(lambda ts: ts.month)\n    yearmonth = df_times['time1'].apply(lambda ts : 100 * ts.year + ts.month)/1e5\n    \n    objects_to_hstack = [X_sparse,weekday.values.reshape(-1,1),month.values.reshape(-1,1),\n                        yearmonth.values.reshape(-1,1)]\n    feature_names = ['weekday','month','yearmonth']\n    \n    X_new = hstack(objects_to_hstack)\n    \n    return X_new, feature_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_final, more_feat_names = add_day_month(train_times,X_train_with_time_correct)\n\nX_test_final,_ = add_day_month(test_times,X_test_with_time_correct)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_scores6 = train_and_predict(model = logit, X_train=X_train_final,y_train=y_train,\n                              X_test= X_test_final,new_feature_names= new_feat_names+ ['sess_duration'] + more_feat_names,\n                              cv =time_split, submission_file_name='subm6.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_scores6 > cv_scores5\n\n# new model better on 6 folds especially the last ones\n# but mean and std are lower\n\n# however LB is better : 0.94616 -> 0.95059","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission 7 : Tuning params","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Once we have no more ideas of feature engineering\n# we can start thinking about Hyperparametrization\n# with pipelines we can avoid data leakage on CV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we could hyperparam:\n# ngram_range\n# max_features\n# CountVectorizer vs TfidfVectorizer\n# C (logReg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# in this code now we will limit ourselves to hyperparam only C","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_values = np.logspace(-2,2,20) \n# we could have used wider range first\n\nlogit_grid_searcher = GridSearchCV(estimator = logit, param_grid= {'C':c_values},\n                                  scoring = 'roc_auc', cv =time_split,n_jobs = -1,\n                                  verbose = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nlogit_grid_searcher.fit(X_train_final,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logit_grid_searcher.best_score_, logit_grid_searcher.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# new model after hyperparametrization with best_params_\n\nfinal_model = logit_grid_searcher.best_estimator_\n\nfinal_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_scores7 = train_and_predict(model= final_model,X_train=X_train_final,y_train=y_train,\n                              X_test= X_test_final,new_feature_names= new_feat_names + ['sess_duration'] + more_feat_names,\n                              cv = time_split, submission_file_name='subm7.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_scores7 > cv_scores6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tuning hyperparam helped only for 6 folds\n# LB :0.95059 -> 0.95051\n\n# -> less than previous submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### HINT #####\n# our CV schema is not perfect !\n# hint : is all training set needed for a good prediction?\n\n\n# my personal answer : \n# No we need to learn only on latest data ! \n# Why? because very old data can be misleading\n# in other word people change their behavior through time\n# and it is not relevant to learn previous behavior\n# to predict new behavior\n\n# E.g. learning DAILY price fluctuation of bitcoin in 2012\n# will not help us make good trading decisions in 2020\n# it could even be misleading","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analyzing submission history","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize 7 submissions\n\ncv_means = [cv_score.mean() for cv_score in [cv_scores1, cv_scores2,cv_scores3,cv_scores4,cv_scores5,cv_scores6,cv_scores7]]\n\ncv_stds = [cv_score.std() for cv_score in [cv_scores1, cv_scores2,cv_scores3,cv_scores4,cv_scores5,cv_scores6,cv_scores7]]\n\npublic_lb_scores = [0.91803,0.93132,0.94522,0.67018,0.94616,0.95059,0.95051]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_means","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_stds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_scores = pd.DataFrame({'CV_mean':cv_means,'CV_stds':cv_stds,'LB':public_lb_scores}\n             ,index= range(1,len(cv_means)+1))\n\ndf_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlation exists between CV and LB\n# but not perfect\n# which submission to choose ?\n\n# ANSWER\n# a popular solution is to treat mean cv and LB results with weights,\n# proportionally to train and test size\n\n# However as here there is a time component\n# test set is then per se more important (latest data)\n# we will give arbitrary weight (60%) - based on no theory only prectical experience\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we could also use std scores to help selecting# the best submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weight = 0.6\ndf_scores['cv_lb_weighted'] = weight * df_scores['LB'] + (1-weight) * df_scores['CV_mean']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the idea to use weight is to avoid trusting only \n# LB because it could lead to overfitting the test set\n# we should trust our cv especially if the schema is well designed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# best value is submission 7","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! cp subm7.csv submission.csv","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. Keep track of cross-validation improvements for each fold (cv_scores7 > cv_scores6)\n\n2. Take also a look to standard deviation of CV (not only mean)\n\n3. VERY IMPORTANT : Build a CV scheme that CV improvements correlate with LB improvements (means that your schedule is correctly settled)\n\n4. Exploring feature importance might help to detect overfitting\n\n5. Spending MOST of time exploring data and building features ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# questions:\n# 1.how is it possible that with just using tfidf we have better results than countvectorizer\n# knowing that we are using entropy the same way we do with log reg\n# 2. how come by adding only one feature we are overfitting?\n# 3. how come that feature scaling improves our performance?","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}