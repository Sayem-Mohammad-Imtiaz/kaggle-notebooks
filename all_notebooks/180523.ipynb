{"cells":[{"metadata":{"_uuid":"0022e20299255d649bb94440d8cb1df89b355069"},"cell_type":"markdown","source":"# 두번째/."},{"metadata":{"_uuid":"9722b8450b94ce34cfa3d2e80d2db807e45a6b50"},"cell_type":"markdown","source":"# **결측값 처리 하기!**\n## 결측치를 처리하기 위한 방법\n### 1, 결측치가 있는 변수를 삭제한다\n### 2, 결측치를 다른 값으로 채워넣는다\n### 3, 결측치를 채워 넣는데 좀 이상한 방법을 거친다."},{"metadata":{"_uuid":"445d5f18a9874da3b4a0019b30732bbb5459e07f"},"cell_type":"markdown","source":"## 1, A Simple Option: Drop Columns with Missing Values\n    data_without_missing_values = original_data.dropna(axis=1)\n\n#### training dataset 과 test dataset에서 같은 열을 모두 drop할 경우\n\n    cols_with_missing = [col for col in original_data.columns \n                                 if original_data[col].isnull().any()]\n    redued_original_data = original_data.drop(cols_with_missing, axis=1)\n    reduced_test_data = test_data.drop(cols_with_missing, axis=1)\n\n#### 열의 대부분 값이 누락되는 경우는 유용할 수 있음\n#### 그러나 해당 열에 유용한 정보가 있는 경우 이 정보를 활용할 수 없으므로 좋은 방법은 아님\n\n## 2, A Better Option: Imputation\n#### missing value에 대표치 넣는 방법, 대부분 대표치로 평균을 활용\n#### 정확한 값을 넣는 것은 아니지만 열을 삭제하는 것 보다 정확한 모델을 만들 수 있다.\n    from sklearn.preprocessing import Imputer\n    my_imputer = Imputer()\n    data_with_imputed_values = my_imputer.fit_transform(original_data)\n#### scikit-learn Pipeline( 모델 구축, 모델 검증 및 모델 배치를 단순화)에 포함될 수 있다는 장점\n\n## 3, An Extension To Imputation\n#### 1) 원본 데이터를 복사한다.(원본데이터의 변화를 막기위해서)\n    new_data = original_data.copy()\n\n####  2) 새로운 열을 만든다. (채워넣을 지표가 되는..?)make new columns indicating what will be imputed\n    cols_with_missing = (col for col in new_data.columns \n                                 if new_data[c].isnull().any())\n    for col in cols_with_missing:\n        new_data[col + '_was_missing'] = new_data[col].isnull()\n\n#### 3) 채워넣는다. Imputation\n    my_imputer = Imputer()\n    # Imputer(missing_values=’NaN’, strategy=’mean’, axis=0, verbose=0, copy=True)[source]¶\n    new_data = my_imputer.fit_transform(new_data)\n    \n### 3번 방법은 어떤 경우 굉장히 효과적이지만 그 외에는 전혀 효과가 없다.. 어떤 경우가 뭔지는 모르겠다.."},{"metadata":{"trusted":true,"_uuid":"f2fd0fe28162ac2b160208c566e05232b0167e2c"},"cell_type":"code","source":"import pandas as pd\nmelb_data = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv')\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\nmelb_target = melb_data.Price\nmelb_predictors = melb_data.drop(['Price'], axis=1)\n# axis=0 이면 열을 삭제하고 , =1 이면 행을 삭제한다.\n\nmelb_numeric_predictors = melb_predictors.select_dtypes(exclude=['object'])\n# 예제를 쉽게 하기 위해 숫자형 데이터만 사용한다. ","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"e2f64a1ba72e5b818e452bb8e31ac4e62e496cd8"},"cell_type":"markdown","source":"    score_dataset(X_train, X_test, y_train, y_test) 를 사용.\n각각의 결측치 처리 방식이 어떤 결과를 가지고 오는지를 보여주기 위함"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"516f4deadc457ea5eef4e70afa13d7b041dac233"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(melb_numeric_predictors, \n                                                    melb_target,\n                                                    train_size=0.7, \n                                                    test_size=0.3, \n                                                    random_state=0) #random_state\n\ndef score_dataset(X_train, X_test, y_train, y_test):\n    model = RandomForestRegressor()\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return mean_absolute_error(y_test, preds)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f39f616773e16fada7e8b70a12b5da73b46b7e62"},"cell_type":"code","source":"cols_with_missing = [col for col in X_train.columns \n                                 if X_train[col].isnull().any()]\nreduced_X_train = X_train.drop(cols_with_missing, axis=1)\nreduced_X_test  = X_test.drop(cols_with_missing, axis=1)\n\nprint(\"Mean Absolute Error from dropping columns with Missing Values:\")\nprint(score_dataset(reduced_X_train, reduced_X_test, y_train, y_test))","execution_count":6,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eaac43e64d3825ba45dd15db9eb6635dbb6c5ec5"},"cell_type":"code","source":"from sklearn.preprocessing import Imputer\n\nmy_imputer = Imputer()\nimputed_X_train = my_imputer.fit_transform(X_train)\nimputed_X_test = my_imputer.transform(X_test)\nprint(\"Mean Absolute Error from Imputation:\")\nprint(score_dataset(imputed_X_train, imputed_X_test, y_train, y_test))","execution_count":7,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"68b47b8c7ed23b98a369c42252231778816489c3"},"cell_type":"code","source":"imputed_X_train_plus = X_train.copy()\nimputed_X_test_plus = X_test.copy()\n\ncols_with_missing = (col for col in X_train.columns \n                                 if X_train[col].isnull().any())\n\nfor col in cols_with_missing:\n    imputed_X_train_plus[col + '_was_missing'] = imputed_X_train_plus[col].isnull()\n    imputed_X_test_plus[col + '_was_missing'] = imputed_X_test_plus[col].isnull()\n\n# Imputation\nmy_imputer = Imputer()\nimputed_X_train_plus = my_imputer.fit_transform(imputed_X_train_plus)\nimputed_X_test_plus = my_imputer.transform(imputed_X_test_plus)\n\nprint(\"Mean Absolute Error from Imputation while Track What Was Imputed:\")\nprint(score_dataset(imputed_X_train_plus, imputed_X_test_plus, y_train, y_test))","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"eb6f31068433a03d2989b5688051d5495553f517"},"cell_type":"markdown","source":"# **범주형 데이터 다루기!**\n### 범주형 데이터는 제한된 값들만을 가진 데이터 입니다.\n#### ex. 소유한 자동차의 브랜드\n### 이러한 범주형 데이터를 인코딩 하지 않고 학습을 시키면 오류가 발생하기 때문에 인코딩이 필요합니다~\n\n## One-Hot Encoding\n#### 가장 표준화된 방법. 범주형 데이터의 변수가 많지 않은 경우에 사용 ( 15개 이하)\n#### 원본 데이터가 의미하는 내용을 새로운 2진 열(0,1)로 만들어서 인코딩함.\n![one-hot 예시](https://i.imgur.com/WAOnNSI.png)"},{"metadata":{"trusted":true,"_uuid":"e1623baf9f05e7df5867afba663c47db1ff7837a","collapsed":true},"cell_type":"code","source":"# Read the data\nimport pandas as pd\ntrain_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n\n# Drop houses where the target is missing\ntrain_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n\ntarget = train_data.SalePrice\n\n# Since missing values isn't the focus of this tutorial, we use the simplest\n# possible approach, which drops these columns. \n# For more detail (and a better approach) to missing values, see\n# https://www.kaggle.com/dansbecker/handling-missing-values\ncols_with_missing = [col for col in train_data.columns \n                                 if train_data[col].isnull().any()]                                  \ncandidate_train_predictors = train_data.drop(['Id', 'SalePrice'] + cols_with_missing, axis=1)\ncandidate_test_predictors = test_data.drop(['Id'] + cols_with_missing, axis=1)\n\n# \"cardinality\" means the number of unique values in a column.\n# We use it as our only way to select categorical columns here. This is convenient, though\n# a little arbitrary.\nlow_cardinality_cols = [cname for cname in candidate_train_predictors.columns if \n                                candidate_train_predictors[cname].nunique() < 10 and\n                                candidate_train_predictors[cname].dtype == \"object\"]\nnumeric_cols = [cname for cname in candidate_train_predictors.columns if \n                                candidate_train_predictors[cname].dtype in ['int64', 'float64']]\nmy_cols = low_cardinality_cols + numeric_cols\ntrain_predictors = candidate_train_predictors[my_cols]\ntest_predictors = candidate_test_predictors[my_cols]","execution_count":9,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f01b9b461d6f2e0d699ffa37ede284730c03a0ef"},"cell_type":"code","source":"train_predictors.dtypes.sample(10)\n# Pandas assigns a data type (called a dtype) to each column or Series. Let's see a random sample of dtypes from our prediction data:","execution_count":10,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"006b76bc2ebe2ef192325b4d0fc6c5c471b3a18e"},"cell_type":"code","source":"one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"ccd587bb1c9dba6a1fa594595d5bbbb758f45c6c"},"cell_type":"markdown","source":"### pd.get_dummies() 사용\n### 선형 회귀를 수행하고 범주형 변수를 인코딩 할 때 완벽한 공선성이 문제가 될 수 있다.\n### 이 문제를 해결하려면 n-1 개의 열을 사용하는 것이 좋다.\n#### 몰라 일단 범주형 변수를 인코딩 할 때 get_dummies() 함수를 쓰자...\n"},{"metadata":{"_uuid":"24774ccdcbc3e1c11430e6f43c37b190836f4684"},"cell_type":"markdown","source":"## 인코딩 하는거랑 범주형 변수를 drop 하는 것 비교하기\n#### 아래의 경우에는 별반 차이가 없다.."},{"metadata":{"trusted":true,"_uuid":"9d9b3193b114c2984929ab80b8924cef3742192b"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\n\ndef get_mae(X, y):\n    # multiple by -1 to make positive MAE score instead of neg value returned as sklearn convention\n    return -1 * cross_val_score(RandomForestRegressor(50), \n                                X, y, \n                                scoring = 'neg_mean_absolute_error').mean()\n\npredictors_without_categoricals = train_predictors.select_dtypes(exclude=['object'])\n\nmae_without_categoricals = get_mae(predictors_without_categoricals, target)\n\nmae_one_hot_encoded = get_mae(one_hot_encoded_training_predictors, target)\n\nprint('Mean Absolute Error when Dropping Categoricals: ' + str(int(mae_without_categoricals)))\nprint('Mean Abslute Error with One-Hot Encoding: ' + str(int(mae_one_hot_encoded)))","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"5e1af864036824b9253761ec661cb7ced85acffb"},"cell_type":"markdown","source":"# 여러 파일에 적용하기.\n#### 방금까지는 train_data 를 한번 인코딩 했음. 여러 파일(test_data set or some other data that you'd like to make predictions for) 이 있으면 어케해야 할까?\n### sklearn 은 열 순서에 민감하므로 각각의 파일들을 정렬해놓지 않으면 결과가 제대로 안나옴.\n\n## 정렬하는 명령을 사용하여 동일한 방식으로 인코딩 되었는지 확인하기"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1ea83daaf7e8e1cfbda3458e13d0ceb8998747d4"},"cell_type":"code","source":"one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\none_hot_encoded_test_predictors = pd.get_dummies(test_predictors)\nfinal_train, final_test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors,\n                                                                    join='left', \n                                                                    axis=1)","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"65f942d85ef317f3a8443e64ae28809a91eeab00"},"cell_type":"markdown","source":"### align 명령은 두 데이터 세트에서 동일한 순서로 열이 표시되도록 한다.\n    ### 이 열 이름을 보고 어떤식으로 정렬이 되어있는지, 잘 되어있는지를 파악한다.\n\n### 인수 join = 'left'는 SQL의 'left join' 과 동일한 작업을 수행하도록 지정함 \n    한 데이터 집합에 표시되는 열이 있고 다른 데이터 집합에 표시되는 열이 없으면, 우리는 우리의 교육 데이터에서 열을 기준으로 열들을 정확하게 유지"},{"metadata":{"_uuid":"df2e704f19d2e68766e6f7e677572a4588d9d049"},"cell_type":"markdown","source":"# Partial Dependence Plots\n### 부분 의존도 그래프는 각 변수 또는 예측 변수가 모델의 예측에 미치는 영향을 보여줍니다.\n### 다음과 같은 질문에서 유용하다.\n    - 남성과 여성의 임금 격차는 교육 배경이나 직장 경험의 차이와는 달리 성별에만 어느 정도 영향을 미칩니까?\n\n    - 주택 특성에 대한 통제, 경도 및 위도가 주택 가격에 미치는 영향은 무엇입니까? \n\n    - 두 그룹간에 건강상의 차이점은 다이어트의 차이 또는 다른 요인들 때문입니까?"},{"metadata":{"trusted":true,"_uuid":"7ffbd190f9440bb5d37dcad1232140089cc0d850"},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\nfrom sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\nfrom sklearn.preprocessing import Imputer\n\ncols_to_use = ['Distance', 'Landsize', 'BuildingArea']\n\ndef get_some_data():\n    data = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv')\n    y = data.Price\n    X = data[cols_to_use]\n    my_imputer = Imputer()\n    imputed_X = my_imputer.fit_transform(X)\n    return imputed_X, y\n    \nfrom sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\n\n# get_some_data is defined in hidden cell above.\nX, y = get_some_data()\n# scikit-learn originally implemented partial dependence plots only for Gradient Boosting models\n# this was due to an implementation detail, and a future release will support all model types.\nmy_model = GradientBoostingRegressor()\n# fit the model as usual\nmy_model.fit(X, y)\n# Here we make the plot\nmy_plots = plot_partial_dependence(my_model,       \n                                   features=[0, 2], # column numbers of plots we want to show\n                                   X=X,            # raw predictors data.\n                                   feature_names=['Distance', 'Landsize', 'BuildingArea'], # labels on graphs\n                                   grid_resolution=10) # number of values to plot on x axis","execution_count":14,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"24cabd01ddc1fea9688fbb12d9cc861069648889"},"cell_type":"markdown","source":"### 보면 우리가 구하고자 하는 Price 와 변수 Distance, BuildingArea 와의 관계를 볼 수 있다."},{"metadata":{"_uuid":"982bc9dc8609f9895dfc85fecc33da152d79ebcf"},"cell_type":"markdown","source":"![타이타닉 예시](https://i.imgur.com/mJex49E.png)"},{"metadata":{"_uuid":"be4fc7ad01754aea3b9d601bb9f50b8ddb3c562d"},"cell_type":"markdown","source":"### Partial dependence plots 은 복잡한 모델에서 통찰력을 얻을 수 있다.\n### 또한 친구들이나 비전공자들에게도 쉽게 설명해줄 수 있다.\n\n### 하지만 인과관계를 전혀 알 수 없다는 단점도 존재한다."},{"metadata":{"_uuid":"eeb997202f6faa79bb6d54432063172d2934d6c4"},"cell_type":"markdown","source":"# **Pipeline**\nhttp://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n\n#### fit and transform method\n#### 중간중간에 transform 및 fit 을 하고, 마지막 사용자는 적합성(fit)만 구현하면 됨.\n#### memory argument 를 이용하면 cach 를 할 수 있다.\n"},{"metadata":{"trusted":true,"_uuid":"a622c6dd9f1987366edb617021a06cb7fc082b9c","collapsed":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read Data\ndata = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv')\ncols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\nX = data[cols_to_use]\ny = data.Price\ntrain_X, test_X, train_y, test_y = train_test_split(X, y)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c63ac6e2f2b72358ee4f5a686ea144084ce770ce"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Imputer\n\nmy_pipeline = make_pipeline(Imputer(), RandomForestRegressor())","execution_count":16,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f5381abb572381f9493ddedf0a2e07425f743daa"},"cell_type":"code","source":"my_pipeline.fit(train_X, train_y)\npredictions = my_pipeline.predict(test_X)","execution_count":17,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dbf4d6614dca602e4ef31a1582cd3704ea9012e9"},"cell_type":"code","source":"# 비교를 위해 pipeline 이 없는 것도 만든다.\nmy_imputer = Imputer()\nmy_model = RandomForestRegressor()\n\nimputed_train_X = my_imputer.fit_transform(train_X)\nimputed_test_X = my_imputer.transform(test_X)\nmy_model.fit(imputed_train_X, train_y)\npredictions = my_model.predict(imputed_test_X)","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"fa2bd0dc956e23e2e5e37e6f89d54ca84e699dc4"},"cell_type":"markdown","source":"### Pipeline 을 만드는건 간단한 코드로 만들 수 있지만, 유용성은 모델이 복잡해질수록 높아진다."},{"metadata":{"_uuid":"03cda45915f891d2395adc231f308558504628a3"},"cell_type":"markdown","source":"# **Cross-Validation**\n### Cross-Validation and Train-Test Split\n#### 데이터의 수가 적으면 시간이 오래 걸리더라도 Cross-Validation 을 사용하는게 좋다.\n#### 데이터의 수가 적다를 구분하는 기준은 없지만, 보통 모델을 실행하는 데 몇 분정도 시간이 걸린다면 Cross-Validation 으로 전환하는게 좋다.\n#### 아니면 둘 다 해보고 성능이 비슷하다면, 시간이 덜 걸리는 Train-Test Split 을 사용하는게 낫다.\n\n![교차검증](https://i.imgur.com/pgVmXf7.png)"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bb6ef52880514a4a103bea406954439108136a40"},"cell_type":"code","source":"import pandas as pd\ndata = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv')\ncols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\nX = data[cols_to_use]\ny = data.Price","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"c4a22ab6361c50a59fa74660441647b7e9ad3218"},"cell_type":"markdown","source":"### Cross-Validation 을 할 때 Pipeline 을 안만들면 넘나리 어렵기 떄문에 만든다."},{"metadata":{"trusted":true,"_uuid":"065c1ccb5d3d397151adc33c123979fd6853e1b8"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Imputer\nmy_pipeline = make_pipeline(Imputer(), RandomForestRegressor())","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"aebb87a211cbaaf8d1d08d3b84489cf961fd5685"},"cell_type":"markdown","source":"### 점수는요~"},{"metadata":{"trusted":true,"_uuid":"588a6c8054c87e6f6ea6240e8a5623e4d739349e"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(my_pipeline, X, y, scoring='neg_mean_absolute_error')\nprint(scores)","execution_count":21,"outputs":[]},{"metadata":{"_uuid":"ad675e5d31c77744ce9b6d01fbffa5ced1657b58"},"cell_type":"markdown","source":"### 여기서 음의 MAE 를 이용하는게 특이한데, 이렇게 하면  해당 관례와 일관성을 유지할 수 있습니다....;;\n### 무슨말인가 하면, sklearn 에서의 convention 은 수치가 높을수록 좋은것인데, 여기서 음수로 돌려버리면 그 convention 과의 일관성이 유지된다는 말인듯"},{"metadata":{"trusted":true,"_uuid":"c996ea833d10e0256525a8a3ff168657526d21bf"},"cell_type":"code","source":"print('Mean Absolute Error %2f' %(-1 * scores.mean()))","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"0b45c891d24cbb89b07e9b00a548fa3e7452f78e"},"cell_type":"markdown","source":"## 그래서 결론은 Cross-Validation 을 이용하면 코드 퀄리티가 높아지고 정확도도 높아져서 좋다!"},{"metadata":{"_uuid":"4c738228006b98e2996b2752273c8afe38dedbe8"},"cell_type":"markdown","source":"# **Data Leakage**\nhttps://blog.naver.com/tjdudwo93/221085844907 -- 이거 보는걸 추천\n### 이 부분이 존재하면, 결과를 내기 전까진 데이터 모델이 정확해 보이지만 사실상 부정확한 모델이 구성된다.\n    1, Leaky Predict\n    2, Leaky Validation Stragies"},{"metadata":{"_uuid":"67620e8c683ffd0902f7b6f194662ab59bf879c1"},"cell_type":"markdown","source":"## Leaky Predict.\n#### 예측을 할 수 없는 데이터를 예측자가 예측하려고 할 때 발생..\n#### 예\n![폐암](https://i.imgur.com/PpEdi63.png)\n#### 해결방법\n![해결방법](https://i.imgur.com/Z5le4tI.png)\n"},{"metadata":{"_uuid":"9fda731aabf0617d6313d71ef719e8268d58b679"},"cell_type":"markdown","source":"##  Leaky Validation Stragies\n#### Train_data 와 test_data 를 나눌 때 많이 발생함.\n#### 해결방법으로는 그 데이터에 대한 지식을 가지고 있으면 됨.\n#### 보편적인 해결방법으로는 유효성 검사로 simple train_test_split 를 하는 경우에는 Validation_data 에 대해서는 모든 처리과정을 제외함\n#### Pipeline 을 이용하면 보다 쉽게 가능함.\n"},{"metadata":{"_uuid":"8478b6e1c67cb7d2824570ccc607519402155603"},"cell_type":"markdown","source":"## 예시"},{"metadata":{"trusted":true,"_uuid":"2afe4adbb74fd6f1117d2f4bb8f2bac3f278d262"},"cell_type":"code","source":"import pandas as pd\n\ndata = pd.read_csv('../input/aer-credit-card-data/AER_credit_card_data.csv', \n                   true_values = ['yes'],\n                   false_values = ['no'])\nprint(data.head())","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"cd6f777695d83d2423049a06d509c4ba42a91bd1"},"cell_type":"markdown","source":"### 데이터가 적어서 Cross-Validation 함"},{"metadata":{"trusted":true,"_uuid":"a75af154f653cb8e8c2925c0f3a12495400f41d3"},"cell_type":"code","source":"from sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\ny = data.card\nX = data.drop(['card'], axis=1)\n\n# Since there was no preprocessing, we didn't need a pipeline here. Used anyway as best practice\nmodeling_pipeline = make_pipeline(RandomForestClassifier())\ncv_scores = cross_val_score(modeling_pipeline, X, y, scoring='accuracy')\nprint(\"Cross-val accuracy: %f\" %cv_scores.mean())","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"1da2e5bdfd31f07a4f8f6862bbb47d9ae519cf0d"},"cell_type":"markdown","source":"#### 정확한 98 %의 모델을 찾는 것은 매우 드뭅니다. 그런 일이 발생하지만 데이터 유출 여부를 면밀히 조사해야 함."},{"metadata":{"trusted":true,"_uuid":"3914a581d19c402ac3bec4a1ec8bf64485cab86c"},"cell_type":"code","source":"expenditures_cardholders = data.expenditure[data.card]\nexpenditures_noncardholders = data.expenditure[~data.card]\n\nprint('Fraction of those who received a card with no expenditures: %.2f' \\\n      %(( expenditures_cardholders == 0).mean()))\nprint('Fraction of those who received a card with no expenditures: %.2f' \\\n      %((expenditures_noncardholders == 0).mean()))","execution_count":25,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"754d8ab168cc0f83f7b7732ca65200f0747c29ab"},"cell_type":"code","source":"potential_leaks = ['expenditure', 'share', 'active', 'majorcards']\nX2 = X.drop(potential_leaks, axis=1)\ncv_scores = cross_val_score(modeling_pipeline, X2, y, scoring='accuracy')\nprint(\"Cross-val accuracy: %f\" %cv_scores.mean())","execution_count":26,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c3615dab4b061024292582762352e72390873f45"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}