{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Variational Autoencoder with PyTorch v PCA "},{"metadata":{},"cell_type":"markdown","source":"In this notebook I want to show two types of dimensionality reduction for tabular data: PCA and Autoencoders.\n\nI use the wine dataset to show how Variational Autoencoder (VAE) with PyTorch on tabular data works and compare it to the classic PCA approach. I use the PCA/VAE to reduce the dimensionality of dataset, in this case don to 3 Variables (embeddings). I then plot the embeddings in a 3D graph to show how VAE is similar to a PCA but works in a non-linear way.\n"},{"metadata":{},"cell_type":"markdown","source":"## Imports "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import nn, optim\nfrom torch.autograd import Variable\nfrom sklearn.decomposition import PCA\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get a quick view of the data "},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_PATH = '/kaggle/input/wineuci/Wine.csv'\npd.read_csv(DATA_PATH, sep=',', header=None, names=['Wine', 'Alcohol','Malic.acid','Ash','Acl',\n                                                    'Mg', 'Phenols', 'Flavanoids','Nonflavanoid.phenols',\n                                                    'Proanth','Color.int','Hue', 'OD','Proline']).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define Functions "},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_data(path):\n    # read in from csv\n    df = pd.read_csv(DATA_PATH, sep=',', header=None, names=['Wine', 'Alcohol','Malic.acid','Ash','Acl',\n                                                    'Mg', 'Phenols', 'Flavanoids','Nonflavanoid.phenols',\n                                                    'Proanth','Color.int','Hue', 'OD','Proline'])\n    # replace nan with -99\n    df = df.fillna(-99)\n    df_base = df.iloc[:, 1:]\n    # get wine Label\n    df_wine = df.iloc[:,0].values\n    x = df_base.values.reshape(-1, df_base.shape[1]).astype('float32')\n    # stadardize values\n    standardizer = preprocessing.StandardScaler()\n    x = standardizer.fit_transform(x)    \n    return x, standardizer, df_wine","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def numpyToTensor(x):\n    x_train = torch.from_numpy(x).to(device)\n    return x_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create PCA with 3 dimensions"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_pca, standardizer, df_wine = load_data(DATA_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_pca.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=3)\nprincipalComponents = pca.fit_transform(x_pca)\nprincipalDf = pd.DataFrame(data = principalComponents\n             , columns = ['principal component 1', 'principal component 2', 'principal component 3'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"finalDf = pd.concat([principalDf, pd.DataFrame(df_wine, columns = ['wine'])], axis = 1)\nfinalDf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PyTorch Autoencoder "},{"metadata":{},"cell_type":"markdown","source":"## Build Data Loader "},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nclass DataBuilder(Dataset):\n    def __init__(self, path):\n        self.x, self.standardizer, self.wine = load_data(DATA_PATH)\n        self.x = numpyToTensor(self.x)\n        self.len=self.x.shape[0]\n    def __getitem__(self,index):      \n        return self.x[index]\n    def __len__(self):\n        return self.len","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_set=DataBuilder(DATA_PATH)\ntrainloader=DataLoader(dataset=data_set,batch_size=1024)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(trainloader.dataset.x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_set.x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build Model and train it"},{"metadata":{},"cell_type":"markdown","source":"Usually VAE are used for Image processing and creating (like GANS Models). This is explained here: https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73\nHowever, we can adapt this approach to use it on tabular data. This way the Autoencoder helps us reducing dimensionality of data and also reduce noise."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Autoencoder(nn.Module):\n    def __init__(self,D_in,H=50,H2=12,latent_dim=3):\n        \n        #Encoder\n        super(Autoencoder,self).__init__()\n        self.linear1=nn.Linear(D_in,H)\n        self.lin_bn1 = nn.BatchNorm1d(num_features=H)\n        self.linear2=nn.Linear(H,H2)\n        self.lin_bn2 = nn.BatchNorm1d(num_features=H2)\n        self.linear3=nn.Linear(H2,H2)\n        self.lin_bn3 = nn.BatchNorm1d(num_features=H2)\n        \n#         # Latent vectors mu and sigma\n        self.fc1 = nn.Linear(H2, latent_dim)\n        self.bn1 = nn.BatchNorm1d(num_features=latent_dim)\n        self.fc21 = nn.Linear(latent_dim, latent_dim)\n        self.fc22 = nn.Linear(latent_dim, latent_dim)\n\n#         # Sampling vector\n        self.fc3 = nn.Linear(latent_dim, latent_dim)\n        self.fc_bn3 = nn.BatchNorm1d(latent_dim)\n        self.fc4 = nn.Linear(latent_dim, H2)\n        self.fc_bn4 = nn.BatchNorm1d(H2)\n        \n#         # Decoder\n        self.linear4=nn.Linear(H2,H2)\n        self.lin_bn4 = nn.BatchNorm1d(num_features=H2)\n        self.linear5=nn.Linear(H2,H)\n        self.lin_bn5 = nn.BatchNorm1d(num_features=H)\n        self.linear6=nn.Linear(H,D_in)\n        self.lin_bn6 = nn.BatchNorm1d(num_features=D_in)\n        \n        self.relu = nn.ReLU()\n        \n    def encode(self, x):\n        lin1 = self.relu(self.lin_bn1(self.linear1(x)))\n        lin2 = self.relu(self.lin_bn2(self.linear2(lin1)))\n        lin3 = self.relu(self.lin_bn3(self.linear3(lin2)))\n\n        fc1 = F.relu(self.bn1(self.fc1(lin3)))\n\n        r1 = self.fc21(fc1)\n        r2 = self.fc22(fc1)\n        \n        return r1, r2\n    \n    def reparameterize(self, mu, logvar):\n        if self.training:\n            std = logvar.mul(0.5).exp_()\n            eps = Variable(std.data.new(std.size()).normal_())\n            return eps.mul(std).add_(mu)\n        else:\n            return mu\n        \n    def decode(self, z):\n        fc3 = self.relu(self.fc_bn3(self.fc3(z)))\n        fc4 = self.relu(self.fc_bn4(self.fc4(fc3)))\n\n        lin4 = self.relu(self.lin_bn4(self.linear4(fc4)))\n        lin5 = self.relu(self.lin_bn5(self.linear5(lin4)))\n        return self.lin_bn6(self.linear6(lin5))\n\n\n        \n    def forward(self, x):\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        # self.decode(z) ist sp√§ter recon_batch, mu ist mu und logvar ist logvar\n        return self.decode(z), mu, logvar","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class customLoss(nn.Module):\n    def __init__(self):\n        super(customLoss, self).__init__()\n        self.mse_loss = nn.MSELoss(reduction=\"sum\")\n    \n    # x_recon ist der im forward im Model erstellte recon_batch, x ist der originale x Batch, mu ist mu und logvar ist logvar \n    def forward(self, x_recon, x, mu, logvar):\n        loss_MSE = self.mse_loss(x_recon, x)\n        loss_KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n\n        return loss_MSE + loss_KLD","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# takes in a module and applies the specified weight initialization\ndef weights_init_uniform_rule(m):\n    classname = m.__class__.__name__\n    # for every Linear layer in a model..\n    if classname.find('Linear') != -1:\n        # get the number of the inputs\n        n = m.in_features\n        y = 1.0/np.sqrt(n)\n        m.weight.data.uniform_(-y, y)\n        m.bias.data.fill_(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"D_in = data_set.x.shape[1]\nH = 50\nH2 = 12\nmodel = Autoencoder(D_in, H, H2).to(device)\nmodel.apply(weights_init_uniform_rule)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_mse = customLoss()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Model "},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 1500\nlog_interval = 50\nval_losses = []\ntrain_losses = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(epoch):\n    model.train()\n    train_loss = 0\n    for batch_idx, data in enumerate(trainloader):\n        data = data.to(device)\n        optimizer.zero_grad()\n        recon_batch, mu, logvar = model(data)\n        loss = loss_mse(recon_batch, data, mu, logvar)\n        loss.backward()\n        train_loss += loss.item()\n        optimizer.step()\n#        if batch_idx % log_interval == 0:\n#            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n#                epoch, batch_idx * len(data), len(trainloader.dataset),\n#                       100. * batch_idx / len(trainloader),\n#                       loss.item() / len(data)))\n    if epoch % 200 == 0:        \n        print('====> Epoch: {} Average loss: {:.4f}'.format(\n            epoch, train_loss / len(trainloader.dataset)))\n        train_losses.append(train_loss / len(trainloader.dataset))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch in range(1, epochs + 1):\n    train(epoch)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate "},{"metadata":{"trusted":true},"cell_type":"code","source":"standardizer = trainloader.dataset.standardizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()\ntest_loss = 0\n# no_grad() bedeutet wir nehmen die vorher berechneten Gewichte und erneuern sie nicht\nwith torch.no_grad():\n    for i, data in enumerate(trainloader):\n        data = data.to(device)\n        recon_batch, mu, logvar = model(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"standardizer.inverse_transform(recon_batch[65].cpu().numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"standardizer.inverse_transform(data[65].cpu().numpy())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The recon_batch is the reconstructed data. So after putting the data through the encoder we compress the dimensions to 3 (embeddings). With these 3 representaions/embeddings of each row the decoder tries to convert it back to the original data. So basically, this recontructed row feeds itself from only three numbers."},{"metadata":{},"cell_type":"markdown","source":"## Get Embeddings "},{"metadata":{"trusted":true},"cell_type":"code","source":"mu_output = []\nlogvar_output = []\n\nwith torch.no_grad():\n    for i, (data) in enumerate(trainloader):\n            data = data.to(device)\n            optimizer.zero_grad()\n            recon_batch, mu, logvar = model(data)\n\n            \n            mu_tensor = mu   \n            mu_output.append(mu_tensor)\n            mu_result = torch.cat(mu_output, dim=0)\n\n            logvar_tensor = logvar   \n            logvar_output.append(logvar_tensor)\n            logvar_result = torch.cat(logvar_output, dim=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mu_result.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mu_result[1:5,:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are the embeddings calculated by our VAE."},{"metadata":{},"cell_type":"markdown","source":"# Plot Embeddings of PCA "},{"metadata":{"trusted":true},"cell_type":"code","source":"from mpl_toolkits import mplot3d\n\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nax = plt.axes(projection='3d')\n\n\n# Data for three-dimensional scattered points\nwinetype = finalDf.iloc[:,3].values\nzdata = finalDf.iloc[:,0].values\nxdata = finalDf.iloc[:,1].values\nydata = finalDf.iloc[:,2].values\nax.scatter3D(xdata, ydata, zdata, c=winetype);\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plot Embeddings from VAE "},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = plt.axes(projection='3d')\n\n\n# Data for three-dimensional scattered points\nwinetype = data_set.wine\nzdata = mu_result[:,0].cpu().numpy()\nxdata = mu_result[:,1].cpu().numpy()\nydata = mu_result[:,2].cpu().numpy()\nax.scatter3D(xdata, ydata, zdata, c=winetype);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If there are any questions, feel free to ask."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}