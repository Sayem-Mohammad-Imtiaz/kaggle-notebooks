{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Bayesian Hyperparameters Tuning"},{"metadata":{},"cell_type":"markdown","source":"The goal of this notebook is to present a simple implementation of bayesian hyperparameters tuning using the [hyperopt](https://github.com/hyperopt/hyperopt) library. Additionally, we compare bayesian tuning with grid search and random search optimization approaches in terms of performance and precision. The comparison is performed for the following estimators:\n\n* Logistic Regression;\n* Decision Tree Classifier;\n* Suporting Vector Classifier."},{"metadata":{},"cell_type":"markdown","source":"# Import Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Basic imports\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport time\n\n#Plotting imports\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#General Sklearn imports\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV\n\n#Hyperopt imports\nimport hyperopt as hp\nfrom hyperopt import fmin, tpe, Trials, hp, STATUS_OK, space_eval\nfrom hyperopt.pyll.base import scope\n\n#Classification imports\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ignore warnings\nimport sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load and Treat Data"},{"metadata":{},"cell_type":"markdown","source":"For this study, we are going to use the Heart Disease UCI dataset. Since our goal is not to evaluate the data preprocessing steps required, we quickly prepare the dataset for the classification algorithms."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Load dataset\ndata = pd.read_csv('../input/heart-disease-uci/heart.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Encode multiclass categorical features\nto_encode = ['cp', 'restecg', 'slope', 'ca', 'thal']\nfor col in to_encode:\n    new_feat = pd.get_dummies(data[col])\n    new_feat.columns = [col+str(x) for x in range(new_feat.shape[1])]\n    new_feat = new_feat.drop(columns = new_feat.columns.values[0])\n    \n    data[new_feat.columns.values] = new_feat\n    data = data.drop(columns = col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split data into attributes and target\natr = data.drop(columns = 'target')\ntarget = data['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Scale dataset\nscaler = MinMaxScaler()\natr = scaler.fit_transform(atr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preliminar Modeling"},{"metadata":{},"cell_type":"markdown","source":"Before we proceed to actually tuning the classification algorithms, a simple Naive-Bayes model is created in order to provides us a reference accuracy value for the posterior runs."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Preliminar modeling\npre_score = cross_val_score(estimator = GaussianNB(),\n                            X = atr, \n                            y = target,\n                            scoring = 'accuracy',\n                            cv = 10,\n                            verbose = 0)\n\nprint('Naive-Bayes mean score: %5.3f' %np.mean(pre_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameters Tuning"},{"metadata":{},"cell_type":"markdown","source":"Here, we finally compare bayesian, grid search and random search tuning methods. However, it is important to run each of the chosen algorithms before any optimization takes place. This way, we can have a better ideia of how each model benefits from each tuning approach."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Compare algorithms in their default configurations\nmodels = [LogisticRegression(), DecisionTreeClassifier(), SVC()]\nmodel_names = [type(x).__name__ for x in models]\n\nstd_score = []\nfor m in tqdm(models):\n    std_score.append(cross_val_score(estimator = m,\n                                 X = atr,\n                                 y = target,\n                                 scoring = 'accuracy',\n                                 cv = 10).mean())\n    \npd.Series(data = std_score, index = model_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bayesian Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Bayesian hyperparameters tuning: Define function\ndef bayes_tuning(estimator, xdata, ydata, cv, space, max_it):\n    \n    #Define objective function\n    def obj_function(params):\n        model = estimator(**params)\n        score = cross_val_score(estimator = model, X = xdata, y = ydata,\n                                scoring = 'accuracy',\n                                cv = cv).mean()\n        return {'loss': -score, 'status': STATUS_OK}\n    \n    start = time.time()\n    \n    #Perform tuning\n    hist = Trials()\n    param = fmin(fn = obj_function, \n                 space = space,\n                 algo = tpe.suggest,\n                 max_evals = max_it,\n                 trials = hist,\n                 rstate = np.random.RandomState(1))\n    param = space_eval(space, param)\n    \n    #Compute best score\n    score = -obj_function(param)['loss']\n    \n    return param, score, hist, time.time() - start","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define hyperparameters spaces for Bayesian tuning\nlr_params = {'tol': hp.loguniform('tol', np.log(1e-5), np.log(1e-2)),\n             'C': hp.uniform('C', 0.1, 2.0),\n             'fit_intercept': hp.choice('fit_intercept', [True, False]),\n             'solver': hp.choice('solver', ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']),\n             'max_iter': scope.int(hp.quniform('max_iter', 50, 500, 20))\n}\n\ndt_params = {'criterion': hp.choice('criterion', ['gini', 'entropy']),\n             'splitter': hp.choice('splitter', ['best', 'random']),\n             'max_depth': scope.int(hp.quniform('max_depth', 3, 50, 1)),\n             'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 50, 1)),\n             'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 50, 1)),\n             'max_features': hp.choice('max_features', ['auto', 'log2', None])\n}\n\nsv_params = {'C': hp.uniform('C', 0.1, 2.0),\n             'kernel': hp.choice('kernel', ['linear', 'poly', 'rbf', 'sigmoid']),\n             'degree': scope.int(hp.quniform('degree', 2, 5, 1)),\n             'gamma': hp.choice('gamma', ['auto', 'scale']),\n             'tol': hp.loguniform('tol', np.log(1e-5), np.log(1e-2)),\n             'max_iter': scope.int(hp.quniform('max_iter', -1, 100, 1))\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Apply bayesian tuning\nmodels = [LogisticRegression, DecisionTreeClassifier, SVC]\nmodel_params = [lr_params, dt_params, sv_params]\n\nbayes_score, bayes_time, bayes_hist = [], [], []\nfor m, par in tqdm(zip(models, model_params)):\n    param, score, hist, dt = bayes_tuning(m, atr, target, 10, par, 150)\n    bayes_score.append(score)\n    bayes_time.append(dt)\n    bayes_hist.append(hist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Print bayesian tuning results\nbayes_df = pd.DataFrame(index = model_names)\nbayes_df['Accuracy'] = bayes_score\nbayes_df['Time'] = bayes_time\n\nprint(bayes_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Grid Search Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define function for grid search tuning\ndef grid_tuning(estimator, xdata, ydata, cv, space):\n    \n    start = time.time()\n    \n    #Perform tuning\n    grid = GridSearchCV(estimator = estimator,\n                        param_grid = space,\n                        scoring = 'accuracy',\n                        cv = 10)\n    grid.fit(xdata, ydata)\n    \n    return grid.best_params_, grid.best_score_, time.time() - start","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define hyperparameters spaces for grid seach tuning\nlr_params = {'tol': [1e-5, 1e-3, 1e-2],\n             'C': [0.1, 0.5, 1.0, 2.0],\n             'fit_intercept': [True, False],\n             'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n             'max_iter': [50, 100, 250, 500]\n}\n\ndt_params = {'criterion': ['gini', 'entropy'],\n             'splitter': ['best', 'random'],\n             'max_depth': [3, 10, 25, 40, 50],\n             'min_samples_split': [2, 10, 25, 50, 50],\n             'min_samples_leaf': [1, 10, 25, 50, 50],\n             'max_features': ['auto', 'log2', None]\n}\n\nsv_params = {'C': [0.1, 0.5, 1.0, 2.0],\n             'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n             'degree': [2, 3, 5],\n             'gamma': ['auto', 'scale'],\n             'tol': [1e-5, 1e-3, 1e-2],\n             'max_iter': [-1, 50, 100]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Apply grid seach tuning\nmodels = [LogisticRegression(), DecisionTreeClassifier(), SVC()]\nmodel_params = [lr_params, dt_params, sv_params]\n\ngrid_score, grid_time = [], []\nfor m, par in tqdm(zip(models, model_params)):\n    _, score, dt = grid_tuning(m, atr, target, 10, par)\n    grid_score.append(score)\n    grid_time.append(dt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Print grid search tuning results\ngrid_df = pd.DataFrame(index = model_names)\ngrid_df['Accuracy'] = grid_score\ngrid_df['Time'] = grid_time\n\nprint(grid_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Search Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define function for random search tuning\ndef random_tuning(estimator, xdata, ydata, cv, space, max_iter):\n    \n    start = time.time()\n    \n    #Perform tuning\n    rand = RandomizedSearchCV(estimator = estimator,\n                              param_distributions = space,\n                              n_iter = max_iter,\n                              scoring = 'accuracy',\n                              cv = 10,\n                              random_state = np.random.RandomState(1))\n    rand.fit(xdata, ydata)\n    \n    return rand.best_params_, rand.best_score_, rand.cv_results_['mean_test_score'], time.time() - start","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define hyperparameters spaces for random search tuning\nlr_params = {'tol': list(np.logspace(np.log(1e-5), np.log(1e-2), num = 10, base = 3)),\n             'C': list(np.linspace(0.1, 2.0, 20)),\n             'fit_intercept': [True, False],\n             'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n             'max_iter': list(range(50, 501))\n}\n\ndt_params = {'criterion': ['gini', 'entropy'],\n             'splitter': ['best', 'random'],\n             'max_depth': list(range(3, 51)),\n             'min_samples_split': list(range(2, 50)),\n             'min_samples_leaf': list(range(1, 50)),\n             'max_features': ['auto', 'log2', None]\n}\n\nsv_params = {'C': list(np.linspace(0.1, 2.0, 10)),\n             'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n             'degree': list(range(2, 6)),\n             'gamma': ['auto', 'scale'],\n             'tol': list(np.logspace(np.log(1e-5), np.log(1e-2), num = 10, base = 3)),\n             'max_iter': list(range(-1, 101))\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Apply random seach tuning\nmodels = [LogisticRegression(), DecisionTreeClassifier(), SVC()]\nmodel_params = [lr_params, dt_params, sv_params]\n\nrand_score, rand_time, rand_hist = [], [], []\nfor m, par in tqdm(zip(models, model_params)):\n    _, score, hist, dt = random_tuning(m, atr, target, 10, par, 150)\n    rand_score.append(score)\n    rand_time.append(dt)\n    rand_hist.append(hist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Print random search tuning results\nrand_df = pd.DataFrame(index = model_names)\nrand_df['Accuracy'] = rand_score\nrand_df['Time'] = rand_time\n\nprint(rand_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Compare Tuning Approaches"},{"metadata":{},"cell_type":"markdown","source":"As mentioned before, we want to evaluate the three tuning methods. This is done first comparing the cross-validation accuracy and then the processing time for each approach. The plots in this notebook use the Plotly library. So, we take the time to set up the approppriate imports."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Install plotly\n!pip install plotly","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotly imports\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Compare Accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Compare accuracy\ntuning_acc = pd.DataFrame(index = model_names)\ntuning_acc['Bayes'] = bayes_score\ntuning_acc['Grid'] = grid_score\ntuning_acc['Random'] = rand_score\n\nfig = go.Figure(data = [\n    go.Bar(name = 'Bayes Tuning', x = tuning_acc.index, y = tuning_acc['Bayes']),\n    go.Bar(name = 'Grid Tuning', x = tuning_acc.index, y = tuning_acc['Grid']),\n    go.Bar(name = 'Random Tuning', x = tuning_acc.index, y = tuning_acc['Random'])\n])\n\nfig.update_layout(barmode = 'group', \n                  title = 'Accuracy Comparison',\n                  xaxis_title = 'Estimator',\n                  yaxis_title = 'Cross-validation accuracy (%)',\n                  yaxis = dict(range = [0.75, 0.9]))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the three tuning approaches produce similar accuracy. If we are dealing with Kaggle competitions, however, the slighest improvement is of great importance. On that page, the Bayesian generates the best results for both the DecisionTreeClassifier and SVC. For the LogisticRegression algorithm, the Grid Search and Random Search methods present a nearly neglectable advantage.\n\nNow, let's try to undertand these results. Why would the results present such differences given the hyperparameters space limits were the same for all tuning procedures? First of all, it is easy to understand the Random Search results. As the name goes, the tuning procedure is completely random. Applying it several times for the same hyperparameters space will likely yield a different result every time. \n\nAs for the Grid Search tuning, these results can be explained by the fact only specific values for each hyperparameter are tested. The other methods, on the other hand, test the parameters values within a range. It is, of course, possible to work around this limitation by providing several more values for each hyperparameter. This, however, is not advisable, since the processing time will greatly increase, rendering the procedure too ineficient.\n\nFinally, the Bayesian tuning, given its nature, will eventually find a good, not necessarily the best, result if the hyperparameter space provided is wide and the number of trials is large enough. Of course, these concepts are arbitrary and depend on the problem at hand."},{"metadata":{},"cell_type":"markdown","source":"## Compare Performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Compare performance\ntuning_time = pd.DataFrame(index = model_names)\ntuning_time['Bayes'] = bayes_time\ntuning_time['Grid'] = grid_time\ntuning_time['Random'] = rand_time\n\nfig = go.Figure(data = [\n    go.Bar(name = 'Bayes Tuning', x = tuning_time.index, y = tuning_time['Bayes']),\n    go.Bar(name = 'Grid Tuning', x = tuning_time.index, y = tuning_time['Grid']),\n    go.Bar(name = 'Random Tuning', x = tuning_time.index, y = tuning_time['Random'])\n])\n\nfig.update_layout(barmode = 'group',\n                  title = 'Performance Comparison',\n                  xaxis_title = 'Estimator',\n                  yaxis_title = 'Computation time (sec)')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The performance results, given the accuracy data, show the Grid Search tuning approach takes much more time to find the best set of hyperparameters while producing comparable results. In this case, we are dealing with a small dataset, so the tuning times are not really a matter of concern. However, this can easily become an issue for larger datasets contaning a larger number of attributes.\n\nNow, the comparison between the Bayesian and Random Search tuning procedures shows us the latter is sistematically faster then the former. The explanation here is simple. The Bayesian tuning method takes some extra time to evaluate the next hyperparameters set to be tested. As we are running both methods with the same amount of iterations, this difference accounts exactly to this extra step. "},{"metadata":{},"cell_type":"markdown","source":"## Compare Tuning Progression"},{"metadata":{},"cell_type":"markdown","source":"Now, we want to understand how the Bayesian and Random Search tunings progressed to their final results. In other words, we want to know how many iterations it took for both approaches to get their best accuracy score."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Compare Bayesian and Random\nbayes_best = dict()\nrandom_best = dict()\nfor i,model in enumerate(model_names):\n    dummy = [-x['loss'] for x in bayes_hist[i].results]\n    bayes_best[model] = np.maximum.accumulate(dummy)\n    \n    dummy = [x for x in rand_hist[i]]\n    random_best[model] = np.maximum.accumulate(dummy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Logistic Regression\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x = list(range(150)), y = bayes_best['LogisticRegression'], name = 'Bayes (Hyperopt)'))\nfig.add_trace(go.Scatter(x = list(range(150)), y = random_best['LogisticRegression'], name = 'Random search'))\n\nfig.update_layout(title = 'Logistic Regression Tuning progression',\n                  xaxis_title = 'Iteration',\n                  yaxis_title = 'Cross-validation accuracy (%)')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#DecisionTreeClassifier\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x = list(range(150)), y = bayes_best['DecisionTreeClassifier'], name = 'Bayes (Hyperopt)'))\nfig.add_trace(go.Scatter(x = list(range(150)), y = random_best['DecisionTreeClassifier'], name = 'Random search'))\n\nfig.update_layout(title = 'Decision Tree Tuning progression',\n                  xaxis_title = 'Iteration',\n                  yaxis_title = 'Cross-validation accuracy (%)')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SVC\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x = list(range(150)), y = bayes_best['SVC'], name = 'Bayes (Hyperopt)'))\nfig.add_trace(go.Scatter(x = list(range(150)), y = random_best['SVC'], name = 'Random search'))\n\nfig.update_layout(title = 'SVC Tuning progression',\n                  xaxis_title = 'Iteration',\n                  yaxis_title = 'Cross-validation accuracy (%)')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us now understand the results in the plots above individually:\n\n* Logistic Regression:\n\nThe Random Search tuning rapidly reaches its best accuracy value, while the Bayesin tuning takes much more steps to find its optimal value.\n\n* Decision Tree:\n\nThe Bayesian tuning begins at a smaller accuracy value, but soon improves and reachs its highest values several iterations before the Random Search method.\n\n* SVC:\n\nThe Bayesian method takes more iterations to find its best accuracy value than the Random Search approach. However, it present the superior value at every iteration level."},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"The hyperopt Bayesin tuning presents a good alternative for hyperparameters optimization. It greatly outperforms the Grid Search approach, while producing similar results. Additionally, it presents comparable performance to the Random Search method, speacilly considering it can achieve similar results with a smaller number of iterations. "},{"metadata":{},"cell_type":"markdown","source":"# References"},{"metadata":{},"cell_type":"markdown","source":"1. [An Introductory Example of Bayesian Optimization in Python with Hyperopt](https://towardsdatascience.com/an-introductory-example-of-bayesian-optimization-in-python-with-hyperopt-aae40fff4ff0)\n\n2. [Hyperparameter optimisation with Hyperopt](https://github.com/MBKraus/Hyperopt/blob/master/Hyperopt.ipynb)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}