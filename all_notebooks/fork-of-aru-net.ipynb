{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nfrom sklearn.utils import shuffle\nimport pandas as pd\nimport pickle\nfrom matplotlib.pyplot import MultipleLocator","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:51:34.58953Z","iopub.execute_input":"2021-07-02T08:51:34.589977Z","iopub.status.idle":"2021-07-02T08:51:41.746043Z","shell.execute_reply.started":"2021-07-02T08:51:34.589877Z","shell.execute_reply":"2021-07-02T08:51:41.745121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\ntf.config.experimental_connect_to_cluster(resolver)\n# This is the TPU initialization code that has to be at the beginning.\ntf.tpu.experimental.initialize_tpu_system(resolver)\nprint(\"All devices: \", tf.config.list_logical_devices('TPU'))","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:51:41.750013Z","iopub.execute_input":"2021-07-02T08:51:41.750427Z","iopub.status.idle":"2021-07-02T08:51:47.243724Z","shell.execute_reply.started":"2021-07-02T08:51:41.750399Z","shell.execute_reply":"2021-07-02T08:51:47.242838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"strategy = tf.distribute.TPUStrategy(resolver)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:51:47.245334Z","iopub.execute_input":"2021-07-02T08:51:47.245623Z","iopub.status.idle":"2021-07-02T08:51:47.254708Z","shell.execute_reply.started":"2021-07-02T08:51:47.245596Z","shell.execute_reply":"2021-07-02T08:51:47.253398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"读入数据\"\"\"\ndata_A = pd.read_csv(r'../input/datasets-ymz/train_0.csv')\ndata_B = pd.read_csv(r'../input/datasets-ymz/test_0.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:51:47.256249Z","iopub.execute_input":"2021-07-02T08:51:47.256745Z","iopub.status.idle":"2021-07-02T08:53:45.430813Z","shell.execute_reply.started":"2021-07-02T08:51:47.256715Z","shell.execute_reply":"2021-07-02T08:53:45.429588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = data_A.iloc[:105000,:]\nprint(len(train_data))\n\nvalidation_data = data_A.iloc[105000:,:]\nprint(len(validation_data))\n\ntest_data = data_B.iloc[:,:]\nprint(len(test_data))","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:53:45.436033Z","iopub.execute_input":"2021-07-02T08:53:45.436509Z","iopub.status.idle":"2021-07-02T08:53:45.446039Z","shell.execute_reply.started":"2021-07-02T08:53:45.436454Z","shell.execute_reply":"2021-07-02T08:53:45.44508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_data = pd.read_csv(r'../input/y-datasets/Train_Merge_Data.csv')\n# print(len(train_data))\n# train_dev = pd.read_csv(r'../input/y-datasets/train.csv')\n# print(len(train_dev))\n# validation_data = pd.read_csv(r'../input/y-datasets/Validation_Merge_Data.csv')\n# print(len(validation_data))\n# validation_dev = pd.read_csv(r'../input/y-datasets/validation.csv')\n# print(len(validation_dev))","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:53:45.447475Z","iopub.execute_input":"2021-07-02T08:53:45.447793Z","iopub.status.idle":"2021-07-02T08:53:45.467153Z","shell.execute_reply.started":"2021-07-02T08:53:45.447764Z","shell.execute_reply":"2021-07-02T08:53:45.465887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"加载数据集train_data\"\"\"\nTrain_label_Sbp = train_data.iloc[:,1800]\nTrain_label_Sbp = Train_label_Sbp.values\n\nTrain_label_Dbp = train_data.iloc[:,1801]\nTrain_label_Dbp = Train_label_Dbp.values\n\nTrain_Dev_One = train_data.iloc[:,600:1200]\nTrain_Dev_One = Train_Dev_One.values\n\nTrain_Dev_Two = train_data.iloc[:,1200:1800]\nTrain_Dev_Two = Train_Dev_Two.values\n\nTrain_Data = train_data.iloc[:,:600]\nTrain_Data = Train_Data.values","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:53:45.468736Z","iopub.execute_input":"2021-07-02T08:53:45.469043Z","iopub.status.idle":"2021-07-02T08:53:45.484324Z","shell.execute_reply.started":"2021-07-02T08:53:45.469013Z","shell.execute_reply":"2021-07-02T08:53:45.483124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# \"\"\"加载数据集train_data\"\"\"\n# Train_label_Sbp = train_data.iloc[:,650]\n# Train_label_Sbp = Train_label_Sbp.values\n\n# Train_label_Dbp = train_data.iloc[:,651]\n# Train_label_Dbp = Train_label_Dbp.values\n\n# Train_Dev_One = train_dev.iloc[:,:640]\n# Train_Dev_One = Train_Dev_One.values\n\n# Train_Dev_Two = train_dev.iloc[:,640:]\n# Train_Dev_Two = Train_Dev_Two.values\n\n# Train_Data = train_data.iloc[:,:640]\n# Train_Data = Train_Data.values","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:53:45.487394Z","iopub.execute_input":"2021-07-02T08:53:45.487742Z","iopub.status.idle":"2021-07-02T08:53:45.494538Z","shell.execute_reply.started":"2021-07-02T08:53:45.487685Z","shell.execute_reply":"2021-07-02T08:53:45.493531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# \"\"\"加载数据集validation_data\"\"\"\n# Validation_label_Sbp = validation_data.iloc[:,650]\n# Validation_label_Sbp = Validation_label_Sbp.values\n\n# Validation_label_Dbp = validation_data.iloc[:,651]\n# Validation_label_Dbp = Validation_label_Dbp.values\n\n# Validation_Dev_One = validation_dev.iloc[:,:640]\n# Validation_Dev_One = Validation_Dev_One.values\n\n# Validation_Dev_Two = validation_dev.iloc[:,640:]\n# Validation_Dev_Two = Validation_Dev_Two.values\n\n# Validation_Data = validation_data.iloc[:,:640]\n# Validation_Data = Validation_Data.values","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:53:45.496235Z","iopub.execute_input":"2021-07-02T08:53:45.496528Z","iopub.status.idle":"2021-07-02T08:53:45.515187Z","shell.execute_reply.started":"2021-07-02T08:53:45.496499Z","shell.execute_reply":"2021-07-02T08:53:45.514194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"加载数据集validation_data\"\"\"\nValidation_label_Sbp = validation_data.iloc[:,1800]\nValidation_label_Sbp = Validation_label_Sbp.values\n\nValidation_label_Dbp = validation_data.iloc[:,1801]\nValidation_label_Dbp = Validation_label_Dbp.values\n\nValidation_Dev_One = validation_data.iloc[:,600:1200]\nValidation_Dev_One = Validation_Dev_One.values\n\nValidation_Dev_Two = validation_data.iloc[:,1200:1800]\nValidation_Dev_Two = Validation_Dev_Two.values\n\nValidation_Data = validation_data.iloc[:,:600]\nValidation_Data = Validation_Data.values","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:53:45.517119Z","iopub.execute_input":"2021-07-02T08:53:45.51757Z","iopub.status.idle":"2021-07-02T08:53:45.529514Z","shell.execute_reply.started":"2021-07-02T08:53:45.517523Z","shell.execute_reply":"2021-07-02T08:53:45.52879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"数据格式、尺寸\"\"\"\nprint(\"data_information:\")\nprint(Train_Data.shape)\nprint(Train_Dev_One.shape)\nprint(Train_Dev_Two.shape)\nprint(Train_label_Sbp.shape)\nprint(Train_label_Dbp.shape)\n\nprint(\"data_information:\")\nprint(Validation_Data.shape)\nprint(Validation_Dev_One.shape)\nprint(Validation_Dev_Two.shape)\nprint(Validation_label_Sbp.shape)\nprint(Validation_label_Dbp.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:53:45.530441Z","iopub.execute_input":"2021-07-02T08:53:45.530687Z","iopub.status.idle":"2021-07-02T08:53:45.548438Z","shell.execute_reply.started":"2021-07-02T08:53:45.530662Z","shell.execute_reply":"2021-07-02T08:53:45.547045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"注意力机制模块\"\"\"\n#---------------------------------------------------------------------------------------\n#---------------------------------------------------------------------------------------\n# 注意力机制：Relu6方式\n# 激活函数 relu6\ndef relu6(x):\n    return tf.keras.activations.relu(x, max_value=6)\n#   利用relu函数乘上x模拟sigmoid\ndef hard_swish(x):\n    return x * tf.keras.activations.relu(x + 3.0, max_value=6.0) / 6.0\n\n#---------------------------------------#\n#   通道注意力机制单元\n#   利用两次全连接算出每个通道的比重\n#   可以连接在任意特征层后面\n#---------------------------------------#\n\ndef squeeze(inputs):\n    input_channels = int(inputs.shape[-1])\n    \n    x = layers.GlobalAveragePooling1D()(inputs)\n\n    x = layers.Dense(int(input_channels/4))(x)\n    x = relu6(x)\n\n    x = layers.Dense(input_channels)(x)\n    x = hard_swish(x)\n\n    x = layers.Reshape((1, input_channels))(x)\n    #print(x)\n    #print(inputs)\n    x = layers.Multiply()([inputs, x])\n    return x\n","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:53:45.549929Z","iopub.execute_input":"2021-07-02T08:53:45.550444Z","iopub.status.idle":"2021-07-02T08:53:45.560627Z","shell.execute_reply.started":"2021-07-02T08:53:45.550408Z","shell.execute_reply":"2021-07-02T08:53:45.559423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"下采样\"\"\"\n#---------------------------------------------------------------------------------------\n#---------------------------------------------------------------------------------------\n#基础层—3x3_3x3—下采样——改变卷积层步长的方式\n#ps:目前试验结果中最好的下采样方式\ndef DownSampling(inputs,filters,stride_padding='same',Drop=False):\n    layer = layers.Conv1D(filters,3,padding='same')(inputs)\n    layer = layers.BatchNormalization()(layer)\n    layer = layers.Activation(tf.nn.relu)(layer)\n\n    layer = layers.Conv1D(filters,3,padding='same')(layer)\n    layer = layers.BatchNormalization()(layer)\n    \n    res_layer = layers.Conv1D(filters,1,padding='same')(inputs)\n    res_layer = layers.BatchNormalization()(res_layer)\n    layer = layers.add([layer,res_layer])\n    \n    layer = layers.Activation(tf.nn.relu)(layer)\n    \n    if Drop==True:\n        layer = layers.Dropout(0.25)(layer)\n    \n    pool_inputs = layers.Conv1D(filters,3,padding=stride_padding,strides=2)(layer)\n    pool = layers.BatchNormalization()(pool_inputs)\n    pool = layers.Activation(tf.nn.relu)(pool)\n\n    pool = layers.Conv1D(filters,3,padding='same')(pool)\n    pool = layers.BatchNormalization()(pool)\n    \n    res_pool = layers.Conv1D(filters,1,padding='same')(pool_inputs)\n    res_pool = layers.BatchNormalization()(res_pool)\n    pool = layers.add([pool,res_pool])\n    \n    pool = layers.Activation(tf.nn.relu)(pool)\n    \n    \n    return layer,pool\n","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:53:45.56215Z","iopub.execute_input":"2021-07-02T08:53:45.562501Z","iopub.status.idle":"2021-07-02T08:53:45.57921Z","shell.execute_reply.started":"2021-07-02T08:53:45.562472Z","shell.execute_reply":"2021-07-02T08:53:45.578249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"上采样\"\"\"\n#基础层上采样\ndef UpSampling(inputs,con_input,filters,need_zero=False):\n    \n    \n    up_layer = layers.UpSampling1D(size=2)(inputs)\n    \n    if need_zero==True:\n        up_layer = layers.ZeroPadding1D((0,1))(up_layer)\n    \n    layer = layers.Conv1D(filters,2,padding='same')(up_layer)\n    layer = layers.BatchNormalization()(layer)\n    layer = layers.Activation(tf.nn.relu)(layer)\n    \n    layer = layers.Concatenate(axis=2)([layer,con_input])\n    \n    layer = layers.Conv1D(filters,3,padding='same')(layer)\n    layer = layers.BatchNormalization()(layer)\n    layer = layers.Activation(tf.nn.relu)(layer)\n\n    layer = layers.Conv1D(filters,3,padding='same')(layer)\n    layer = layers.BatchNormalization()(layer)\n    output = layers.Activation(tf.nn.relu)(layer)\n    \n    return output","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:53:45.580802Z","iopub.execute_input":"2021-07-02T08:53:45.581228Z","iopub.status.idle":"2021-07-02T08:53:45.598208Z","shell.execute_reply.started":"2021-07-02T08:53:45.581186Z","shell.execute_reply":"2021-07-02T08:53:45.597092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"上采样结束的卷积操作\"\"\"\ndef FinalConv1D(inputs,filters):\n    layer = layers.Conv1D(filters,3,padding='same')(inputs)\n    layer = layers.BatchNormalization()(layer)\n    layer = layers.Activation(tf.nn.relu)(layer)\n    \n    layer = layers.MaxPooling1D(pool_size=2)(layer)\n    \n    layer = layers.Conv1D(filters,3,padding='same')(layer)\n    layer = layers.BatchNormalization()(layer)\n    layer = layers.Activation(tf.nn.relu)(layer)\n    \n    layer = layers.MaxPooling1D(pool_size=2)(layer)\n    return layer","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:53:45.599372Z","iopub.execute_input":"2021-07-02T08:53:45.599689Z","iopub.status.idle":"2021-07-02T08:53:45.616843Z","shell.execute_reply.started":"2021-07-02T08:53:45.599654Z","shell.execute_reply":"2021-07-02T08:53:45.615813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"上采样结束的卷积操作\"\"\"\ndef Inception_model(inputs,filters,kernel_size=3,stride_padding='same',Drop=False):\n    layer = layers.Conv1D(filters,kernel_size=kernel_size,padding='same')(inputs)\n    layer = layers.BatchNormalization()(layer)\n    layer = layers.Activation(tf.nn.relu)(layer)\n\n    layer = layers.Conv1D(filters,kernel_size=kernel_size,padding='same')(layer)\n    layer = layers.BatchNormalization()(layer)\n    \n    res_layer = layers.Conv1D(filters,1,padding='same')(inputs)\n    res_layer = layers.BatchNormalization()(res_layer)\n    layer = layers.add([layer,res_layer])\n    \n    layer = layers.Activation(tf.nn.relu)(layer)\n    \n    if Drop==True:\n        layer = layers.Dropout(0.25)(layer)\n    \n    pool_inputs = layers.Conv1D(filters,kernel_size=kernel_size,padding=stride_padding,strides=2)(layer)\n    pool = layers.BatchNormalization()(pool_inputs)\n    pool = layers.Activation(tf.nn.relu)(pool)\n\n    pool = layers.Conv1D(filters,kernel_size=kernel_size,padding='same')(pool)\n    pool = layers.BatchNormalization()(pool)\n    \n    res_pool = layers.Conv1D(filters,1,padding='same')(pool_inputs)\n    res_pool = layers.BatchNormalization()(res_pool)\n    pool = layers.add([pool,res_pool])\n    \n    pool = layers.Activation(tf.nn.relu)(pool)\n    return layer,pool","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:53:45.618549Z","iopub.execute_input":"2021-07-02T08:53:45.618998Z","iopub.status.idle":"2021-07-02T08:53:45.631582Z","shell.execute_reply.started":"2021-07-02T08:53:45.618955Z","shell.execute_reply":"2021-07-02T08:53:45.630825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ARU-Net原版\ndef create_model():\n    #双输入模型\n    inputs_1 = keras.Input(shape=(600,1),name='inputs_1')\n    inputs_dev1 = keras.Input(shape=(600,1),name='inputs_dev1')\n    inputs_dev2 = keras.Input(shape=(600,1),name='inputs_dev2')\n    \n    inputs = layers.Concatenate(axis=2)([inputs_1,inputs_dev1,inputs_dev2])\n    \n    #下采样\n    layer1,pool1 = DownSampling(inputs,filters=64)\n    \n    layer2,pool2 = DownSampling(pool1,filters=128)\n        \n    layer3,pool3 = DownSampling(pool2,filters=256)\n    \n    layer4,pool4 = DownSampling(pool3,filters=512,stride_padding='valid',Drop=False)\n    \n    #最后一层不需要Pooling\n#     layer5,pool5 = DownSampling(pool4,filters=1024,Drop=False)\n\n    layer51,pool51 = Inception_model(pool4,filters=1024,kernel_size=1)\n    layer52,pool52 = Inception_model(pool4,filters=1024,kernel_size=3)\n    layer53,pool53 = Inception_model(pool4,filters=1024,kernel_size=5)\n    \n    layer51 = squeeze(layer51)\n    layer52 = squeeze(layer52)\n    layer53 = squeeze(layer53)\n    \n    layer51 = layers.BatchNormalization()(layer51)\n    layer52 = layers.BatchNormalization()(layer52)\n    layer53 = layers.BatchNormalization()(layer53)\n    layer5 = layers.add([layer51,layer52])\n    layer5 = layers.add([layer5,layer53])\n    #上采样\n    \n    #attention\n    layer4 = squeeze(layer4)\n    \n    layer44 = UpSampling(layer5,layer4,512,need_zero=True)\n    \n    #attention\n    layer3 = squeeze(layer3)\n                     \n    layer33 = UpSampling(layer44,layer3,256)\n    \n    #attention\n    layer2 = squeeze(layer2)\n    \n    layer22 = UpSampling(layer33,layer2,128)\n    \n    #attention\n    layer1 = squeeze(layer1)\n    \n    layer11 = UpSampling(layer22,layer1,64)\n    \n    \n    layer = layers.GlobalAveragePooling1D()(layer11)\n\n    outputs_sbp = layers.Dense(1,name='Sbp')(layer)\n    outputs_dbp = layers.Dense(1,name='Dbp')(layer)\n\n    model = keras.Model(inputs=[inputs_1,inputs_dev1,inputs_dev2],outputs=[outputs_sbp,outputs_dbp])\n    #model = keras.Model(inputs=[inputs_1],outputs=[outputs_sbp,outputs_dbp])\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:53:45.633025Z","iopub.execute_input":"2021-07-02T08:53:45.633546Z","iopub.status.idle":"2021-07-02T08:53:45.648556Z","shell.execute_reply.started":"2021-07-02T08:53:45.633512Z","shell.execute_reply":"2021-07-02T08:53:45.647699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"自定义评价指标模块\"\"\"\ndef standard_deviation(y_true, y_pred):\n    u = keras.backend.mean(abs(y_pred-y_true))\n    return keras.backend.sqrt(keras.backend.mean(keras.backend.square(abs(y_pred-y_true) - u)))","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:53:45.650139Z","iopub.execute_input":"2021-07-02T08:53:45.650941Z","iopub.status.idle":"2021-07-02T08:53:45.662721Z","shell.execute_reply.started":"2021-07-02T08:53:45.650893Z","shell.execute_reply":"2021-07-02T08:53:45.661815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"回调函数\"\"\"\n#保存迭代周期内最好的模型\ncheckpoint_filepath = r'./model_struction.h5'\nSave_epochs = 100 #迭代多少层保存一次模型\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    # save_weights_only=True,\n    monitor='val_Sbp_mean_absolute_error',\n    mode='min',\n    save_best_only=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:53:45.66397Z","iopub.execute_input":"2021-07-02T08:53:45.664426Z","iopub.status.idle":"2021-07-02T08:53:45.674472Z","shell.execute_reply.started":"2021-07-02T08:53:45.664387Z","shell.execute_reply":"2021-07-02T08:53:45.673448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"建立模型\"\"\"\nwith strategy.scope():\n    model = create_model()\n    \n    model.compile(loss={'Sbp':\"mse\",'Dbp':\"mse\"}, optimizer=keras.optimizers.Adam(lr=0.0001),metrics=[tf.keras.metrics.MeanAbsoluteError(),standard_deviation])","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:53:45.676005Z","iopub.execute_input":"2021-07-02T08:53:45.676286Z","iopub.status.idle":"2021-07-02T08:53:55.87603Z","shell.execute_reply.started":"2021-07-02T08:53:45.67626Z","shell.execute_reply":"2021-07-02T08:53:55.875256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:53:55.877354Z","iopub.execute_input":"2021-07-02T08:53:55.877922Z","iopub.status.idle":"2021-07-02T08:53:55.977215Z","shell.execute_reply.started":"2021-07-02T08:53:55.87788Z","shell.execute_reply":"2021-07-02T08:53:55.976432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"保存模型结构图片\"\"\"\ntf.keras.utils.plot_model(model, to_file=r'./model_graph.png', show_shapes=True, show_layer_names=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:53:55.9782Z","iopub.execute_input":"2021-07-02T08:53:55.978454Z","iopub.status.idle":"2021-07-02T08:54:00.654169Z","shell.execute_reply.started":"2021-07-02T08:53:55.978429Z","shell.execute_reply":"2021-07-02T08:54:00.652628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit({'inputs_1':Train_Data,'inputs_dev1':Train_Dev_One,'inputs_dev2':Train_Dev_Two},{'Sbp':Train_label_Sbp,'Dbp':Train_label_Dbp},\n                    batch_size=128*8,\n                    epochs=500,\n                    callbacks=model_checkpoint_callback,\n                    validation_data=({'inputs_1':Validation_Data,'inputs_dev1':Validation_Dev_One,'inputs_dev2':Validation_Dev_Two},{'Sbp':Validation_label_Sbp,'Dbp':Validation_label_Dbp})\n                    )","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:54:00.656205Z","iopub.execute_input":"2021-07-02T08:54:00.656537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# history = model.fit({'inputs_1':Train_Data},{'Sbp':Train_label_Sbp,'Dbp':Train_label_Dbp},\n#                     batch_size=128*8,\n#                     epochs=500,\n#                     callbacks=model_checkpoint_callback,\n#                     validation_data=({'inputs_1':Validation_Data},{'Sbp':Validation_label_Sbp,'Dbp':Validation_label_Dbp})\n#                     )","metadata":{"execution":{"iopub.status.busy":"2021-07-02T06:44:15.513697Z","iopub.status.idle":"2021-07-02T06:44:15.514157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(r'./last_model.h5.pickle', 'wb') as file_pi:\n \tpickle.dump(history.history, file_pi)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T06:44:15.514942Z","iopub.status.idle":"2021-07-02T06:44:15.515351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# x=32\n# def create_model():\n    \n#     inputs_1 = keras.Input(shape=(600,1),name='inputs_1')\n#     inputs_dev1 = keras.Input(shape=(600,1),name='inputs_dev1')\n#     inputs_dev2 = keras.Input(shape=(600,1),name='inputs_dev2')\n    \n#     inputs = layers.Concatenate(axis=1)([inputs_1,inputs_dev1,inputs_dev2])\n    \n#     inputs = keras.Input(shape=(1800,1))\n\n#     conv1 = layers.Conv1D(x*2,3,padding='same')(inputs)\n#     conv1 = layers.BatchNormalization()(conv1)\n#     conv1 = layers.Activation(tf.nn.relu)(conv1)\n\n#     conv1 = layers.Conv1D(x*2,3,padding='same')(conv1)\n#     conv1 = layers.BatchNormalization()(conv1)\n#     conv1 = layers.Activation(tf.nn.relu)(conv1)\n\n#     pool1 = layers.MaxPooling1D(pool_size=2)(conv1)\n    \n#     conv2 = layers.Conv1D(x*4,3,padding='same')(pool1)\n#     conv2 = layers.BatchNormalization()(conv2)\n#     conv2 = layers.Activation(tf.nn.relu)(conv2)\n\n#     conv2 = layers.Conv1D(x*4,3,padding='same')(conv2)\n#     conv2 = layers.BatchNormalization()(conv2)\n#     conv2 = layers.Activation(tf.nn.relu)(conv2)\n\n#     pool2 = layers.MaxPooling1D(pool_size=2)(conv2)\n    \n    \n#     conv3 = layers.Conv1D(x*8,3,padding='same')(pool2)\n#     conv3 = layers.BatchNormalization()(conv3)\n#     conv3 = layers.Activation(tf.nn.relu)(conv3)\n\n#     conv3 = layers.Conv1D(x*8,3,padding='same')(conv3)\n#     conv3 = layers.BatchNormalization()(conv3)\n#     conv3 = layers.Activation(tf.nn.relu)(conv3)\n\n#     pool3 = layers.MaxPooling1D(pool_size=2)(conv3)\n    \n    \n#     conv4 = layers.Conv1D(x*16,3,padding='same')(pool3)\n#     conv4 = layers.BatchNormalization()(conv4)\n#     conv4 = layers.Activation(tf.nn.relu)(conv4)\n\n#     conv4 = layers.Conv1D(x*16,3,padding='same')(conv4)\n#     conv4 = layers.BatchNormalization()(conv4)\n#     conv4 = layers.Activation(tf.nn.relu)(conv4)\n    \n#     conv4 = layers.Dropout(0.5)(conv4)\n#     ####\n#     pool4 = layers.MaxPooling1D(pool_size=2)(conv4)\n    \n\n    \n#     conv5 = layers.Conv1D(x*32,3,padding='same')(pool4)\n#     conv5 = layers.BatchNormalization()(conv5)\n#     conv5 = layers.Activation(tf.nn.relu)(conv5)\n\n#     conv5 = layers.Conv1D(x*32,3,padding='same')(conv5)\n#     conv5 = layers.BatchNormalization()(conv5)\n#     conv5 = layers.Activation(tf.nn.relu)(conv5)\n    \n#     conv5 = layers.Dropout(0.5)(conv5)\n    \n    \n#     ####\n#     conv5 = layers.UpSampling1D(size=2)(conv5)\n#     conv5 = layers.ZeroPadding1D((0,1))(conv5)\n    \n#     conv5 = layers.Conv1D(x*16,2,padding='same')(conv5)\n    \n#     up6 = layers.Concatenate(axis=2)([conv5,conv4])\n    \n#     conv6 = layers.Conv1D(x*16,3,padding='same')(up6)\n#     conv6 = layers.BatchNormalization()(conv6)\n#     conv6 = layers.Activation(tf.nn.relu)(conv6)\n\n#     conv6 = layers.Conv1D(x*16,3,padding='same')(conv6)\n#     conv6 = layers.BatchNormalization()(conv6)\n#     conv6 = layers.Activation(tf.nn.relu)(conv6)\n    \n#     ####\n#     conv6 = layers.UpSampling1D(size=2)(conv6)\n    \n#     conv6 = layers.Conv1D(x*8,2,padding='same')(conv6)\n#     conv6 = layers.BatchNormalization()(conv6)\n#     conv6 = layers.Activation(tf.nn.relu)(conv6)\n    \n#     up7 = layers.Concatenate(axis=2)([conv6,conv3])\n\n#     conv7 = layers.Conv1D(x*8,3,padding='same')(up7)\n#     conv7 = layers.BatchNormalization()(conv7)\n#     conv7 = layers.Activation(tf.nn.relu)(conv7)\n\n#     conv7 = layers.Conv1D(x*8,3,padding='same')(conv7)\n#     conv7 = layers.BatchNormalization()(conv7)\n#     conv7 = layers.Activation(tf.nn.relu)(conv7)\n    \n#     ####\n    \n#     conv7 = layers.UpSampling1D(size=2)(conv7)\n    \n#     conv7 = layers.Conv1D(x*4,2,padding='same')(conv7)\n#     conv7 = layers.BatchNormalization()(conv7)\n#     conv7 = layers.Activation(tf.nn.relu)(conv7)\n    \n#     up8 = layers.Concatenate(axis=2)([conv7,conv2])\n\n#     conv8 = layers.Conv1D(x*4,3,padding='same')(up8)\n#     conv8 = layers.BatchNormalization()(conv8)\n#     conv8 = layers.Activation(tf.nn.relu)(conv8)\n\n#     conv8 = layers.Conv1D(x*4,3,padding='same')(conv8)\n#     conv8 = layers.BatchNormalization()(conv8)\n#     conv8 = layers.Activation(tf.nn.relu)(conv8)\n    \n#     ####\n    \n#     conv8 = layers.UpSampling1D(size=2)(conv8)\n    \n#     conv8 = layers.Conv1D(x*2,2,padding='same')(conv8)\n#     conv8 = layers.BatchNormalization()(conv8)\n#     conv8 = layers.Activation(tf.nn.relu)(conv8)\n    \n\n#     up9 = layers.Concatenate(axis=2)([conv8,conv1])\n\n#     conv9 = layers.Conv1D(x*2,3,padding='same')(up9)\n#     conv9 = layers.BatchNormalization()(conv9)\n#     conv9 = layers.Activation(tf.nn.relu)(conv9)\n\n#     conv9 = layers.Conv1D(x*2,3,padding='same')(conv9)\n#     conv9 = layers.BatchNormalization()(conv9)\n#     conv9 = layers.Activation(tf.nn.relu)(conv9)\n\n#     # 论文方法\n#     conv10 = layers.Conv1D(2,1,padding='same')(conv9)\n#     conv10 = layers.BatchNormalization()(conv10)\n#     conv10 = layers.Activation(tf.nn.relu)(conv10)\n    \n#     conv11 = layers.Conv1D(1,1,padding='same')(conv10)\n#     conv11 = layers.BatchNormalization()(conv11)\n#     conv11 = layers.Activation(tf.nn.relu)(conv11)\n    \n#     com_layer = layers.Flatten()(conv11)\n#     #com_layer = layers.GlobalAveragePooling1D()(conv10)\n    \n#     #com_layer = layers.Dense(32,activation='relu',kernel_regularizer=tf.keras.regularizers.L2(0.4))(com_layer)\n    \n#     outputs_sbp = layers.Dense(1,name='Sbp')(com_layer)\n#     outputs_dbp = layers.Dense(1,name='Dbp')(com_layer)\n\n#     model = keras.Model(inputs=[inputs_1,inputs_dev1,inputs_dev2],outputs=[outputs_sbp,outputs_dbp])\n    \n#     return model","metadata":{"execution":{"iopub.status.busy":"2021-06-11T05:43:16.617453Z","iopub.execute_input":"2021-06-11T05:43:16.617959Z","iopub.status.idle":"2021-06-11T05:43:16.661332Z","shell.execute_reply.started":"2021-06-11T05:43:16.617924Z","shell.execute_reply":"2021-06-11T05:43:16.660096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"回调函数\"\"\"\n#保存迭代周期内最好的模型\ncheckpoint_filepath = r'./model_struction_1.h5'\nSave_epochs = 100 #迭代多少层保存一次模型\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    # save_weights_only=True,\n    monitor='val_Sbp_mean_absolute_error',\n    mode='min',\n    save_best_only=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T05:43:16.66287Z","iopub.execute_input":"2021-06-11T05:43:16.663303Z","iopub.status.idle":"2021-06-11T05:43:16.682952Z","shell.execute_reply.started":"2021-06-11T05:43:16.663259Z","shell.execute_reply":"2021-06-11T05:43:16.681813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"建立模型\"\"\"\nwith strategy.scope():\n    model = create_model()\n    \n    model.compile(loss={'Sbp':\"mse\",'Dbp':\"mse\"}, optimizer=keras.optimizers.Adam(lr=0.0001),metrics=[tf.keras.metrics.MeanAbsoluteError(),standard_deviation])","metadata":{"execution":{"iopub.status.busy":"2021-06-11T05:43:16.6843Z","iopub.execute_input":"2021-06-11T05:43:16.684661Z","iopub.status.idle":"2021-06-11T05:43:19.639945Z","shell.execute_reply.started":"2021-06-11T05:43:16.68463Z","shell.execute_reply":"2021-06-11T05:43:19.637929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T05:43:19.641366Z","iopub.status.idle":"2021-06-11T05:43:19.642054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"保存模型结构图片\"\"\"\ntf.keras.utils.plot_model(model, to_file=r'./model_graph_1.png', show_shapes=True, show_layer_names=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-11T05:43:19.643734Z","iopub.status.idle":"2021-06-11T05:43:19.644397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit({'inputs_1':Train_Data,'inputs_dev1':Train_Dev_One,'inputs_dev2':Train_Dev_Two},{'Sbp':Train_label_Sbp,'Dbp':Train_label_Dbp},\n                    batch_size=128*8,\n                    epochs=500,\n                    callbacks=model_checkpoint_callback,\n                    validation_data=({'inputs_1':Validation_Data,'inputs_dev1':Validation_Dev_One,'inputs_dev2':Validation_Dev_Two},{'Sbp':Validation_label_Sbp,'Dbp':Validation_label_Dbp})\n                    )","metadata":{"execution":{"iopub.status.busy":"2021-06-11T05:43:19.645821Z","iopub.status.idle":"2021-06-11T05:43:19.646481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(r'./last_model_1.h5.pickle', 'wb') as file_pi:\n \tpickle.dump(history.history, file_pi)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T05:43:19.647799Z","iopub.status.idle":"2021-06-11T05:43:19.648469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}