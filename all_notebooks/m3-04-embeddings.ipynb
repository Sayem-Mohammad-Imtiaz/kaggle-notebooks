{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Embeddings\n\nEs gibt numerische Daten wie zum Beispiel Alter oder Absatzzahlen, die ein Machine-Learning-Modell direkt verarbeiten kann. Andere Daten sind dagegen kategorisch, wie zum Beispiel Wochentage oder Länder. Um mit diesen kategorischen Daten arbeiten zu können, nutzen wir Embeddings, die kategorische in numerische Werte verwandeln.\n\nIn diesem Notebook wird erklärt, wie man mithilfe der fastai-Bibliothek anhand eines Datensatzes von Covid-19-Infektionszahlen Embeddings entwickeln, tabellarische Daten voraussagen und kategorische Daten visualisieren kann. Anschließend gibt es die Möglichkeit, in der Übung dasselbe mit Absatzzahlen von des amerikanischen Supermarktes Walmart auszuprobieren.","metadata":{}},{"cell_type":"markdown","source":"### Einlesen der Daten\nZunächst werden die benötigten Bibliotheken numpy und pandas importiert. Wir importieren das tabular-Modul von fastai, das wir zur Verarbeitung von tabellarischen Daten verwenden.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport m3utils\nfrom fastai.tabular.all import *","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocessing\nWir nutzen wieder den Covid-Datensatz und verarbeiten ihn wie in den bereits vorangegangenen Einheiten. Neuronale Netze mögen normalisierte Daten, was wir mit ein paar zusätzlichen Transformationen unterstützen.","metadata":{}},{"cell_type":"code","source":"df = m3utils.load_covid_19_data(m3utils.EU_UK_data()['Country/Region'])\ndf['value_trend_slope'] = df['value_trend_slope'] / (df['value_trend']+0.1)\ndf['value_trend'] = np.log(df['value_trend']+1)\ndf['value_daily'] = np.log(df['value_daily']+1)\ndf['Country/Region'] = df['Country/Region'].cat.remove_unused_categories() # damit wir später nicht für alle Länder ohne Daten Embeddings berechnen.\nfor lag in list(range(7, 0, -1)):\n    df['value_daily_lag'+str(lag)] = df.groupby('Country/Region')['value_daily'].shift(lag)\nfor lag in list(range(3, 0, -1)):\n    df['value_trend_lag'+str(lag)] = df.groupby('Country/Region')['value_trend'].shift(lag)\ndf.sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wir möchten Infektionszahlen ('value_daily') vorhersagen und fügen als 'target_value' immer den Wert der nächsten Woche hinzu.","metadata":{}},{"cell_type":"code","source":"df['target_value'] = df.groupby('Country/Region')['value_daily'].shift(-7).astype('float')\ndf.dropna(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Für die Validierung nehmen alle Daten ab April 2021 aus dem Trainingsdatensatz. Wir definieren, welche Spalten kategorisch und welche kontinuierlich (numerisch) sind.\nWeiterhin definieren wir die genutzten Preprocessing-Funktionen. Categorify verändert die als kategorisch definierte Spalten in einen kategorischen Datentyp. FillMissing fügt fehlende Daten in den numerischen Variablen hinzu. Normalize wendet Standardisierung an. TabularPandas ist ein Container für tabellarische Trainings-Daten. Hier wird das zuvor spezifizierte Preprocessing vorgenommen.","metadata":{}},{"cell_type":"code","source":"train_condition = df['date'] <= '2021-03-31'\ntrain_idx = np.where(train_condition)[0]\nvalid_idx = np.where(~train_condition)[0]\n\nsplits = (list(train_idx),list(valid_idx))\n\ncat_names = ['Country/Region', 'weekday']\ncont_names = df.columns.drop(['Country/Region', 'weekday', 'date', 'target_value']).values.tolist()\npreprocessing = [Categorify, FillMissing, Normalize]\n\nto = TabularPandas(df, preprocessing, cat_names, cont_names, splits=splits, y_names='target_value')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(len(to.train), len(to.valid))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Trainingskonfiguration\nWir erstellen ein Neuronales-Netz mit dem `tabular_learner`. Wir definieren mit dem Parameter layers, wie viele Hidden-Layer das neuronale Netz, mit dem die Vorhersage gemacht wird, haben soll und wie viele Neuronen sich in jedem Layer befinden. Als Verlust-Funktion nehmen wir den Mean Squared Error. Mit dem Parameter `y_range` wird der Ausgabe-Bereich des Modells durch eine Sigmoid-Funktion angepasst.","metadata":{}},{"cell_type":"code","source":"(df['target_value'].min(), df['target_value'].max())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn = tabular_learner(to.dataloaders(1024), layers=[200,100], y_range=(0, 13), loss_func=F.mse_loss, n_out=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Als Learning-Rate nehmen wir `0.033`.","metadata":{}},{"cell_type":"code","source":"learn.fit_one_cycle(10, 0.033)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wir wollen in diesem Notebook zeigen, wie die Embeddings trainiert werden und nicht, wie wir gute Vorhersagen mit neuronalen Netzen machen, deshalb geht es hier nach dem Training nicht mit den Vorhersagen weiter.\n\nWenn ihr noch mehr Interesse am Thema Forecasting und neuronale Netzwerke habt empfehlen wir euch das Projekt [GluonTS](https://ts.gluon.ai/) von Amazon oder für alle experimentierfreudigen auch [Pytorch-Forecasting](https://github.com/jdb78/pytorch-forecasting).","metadata":{}},{"cell_type":"markdown","source":"### Visualisierung\nNun möchten wir die Embeddings visualisieren. Dabei bietet sich die Country/Region an. Ob die Länder sich in der Visualisierung entsprechend ihrer geografischen Lage anordnen?\n\nWie sieht jetzt eins der gelernten Embeddings aus?","metadata":{}},{"cell_type":"code","source":"next(learn.model.embeds[0].parameters())[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wir können jetzt die Embeddings mit einer PCA in einen zwei dimensionalen Raum projezieren. Mithilfe einer Embeddings-Matrix werden die Koordinaten der Embeddings ermittelt, dementsprechend wie sie am Ende visualisiert werden sollen.","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\nvar = 'Country/Region'\nvar_index = cat_names.index(var)\nvar_values = list(df[var].astype('category').cat.categories.values)\n\nemb_matrix = to_np(next(learn.model.embeds[var_index].parameters()))\nemb_coordinates = PCA(n_components=2).fit_transform(emb_matrix)\nannotation = np.append('Other', np.array(var_values))\n\nemb_df = pd.DataFrame(emb_coordinates, columns=['X', 'Y'])\nemb_df[var]=annotation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.scatter(emb_df, x='X', y='Y', text='Country/Region')\nfig.update_traces(textposition='top center')\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Aufgabe - Embeddings für den Walmart-Datensatz\nNun sollst du dich am Walmart-Datensatz versuchen. Wir haben ein paar Features vorbereitet. Stelle eine Vorhersage der Absatzzahlen (`target_value`) und visualisiere z.B. die `store_id` oder `item_id`. Du kannst das Training z.B. bis 2015 machen (`df['wm_yr_wk']<1500`). Den Datensatz haben wir für dich schon vorbereitet.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/m5-forecasting-parquet-and-aggregations/weekly_sales_items_top105.csv',\n                dtype={'state_id': 'category', 'store_id': 'category', 'cat_id': 'category', 'dept_id': 'category', 'item_id': 'category'})\ndf['wm_yr_wk'] = df['wm_yr_wk']-10000\ndf['id'] = (df['store_id'].astype('str') + '_' + df['item_id'].astype('str')).astype('category')\ndf['value'] = np.log(df['value']+1)\ndf['value_trend'] = df.groupby(['id'], observed=True)['value'].rolling(window=4).mean().reset_index(level=0, drop=True)\ndf['value_trend_slope'] = df.groupby('id')['value_trend'].diff(4)\nfor lag in list(range(4, 0, -1)):\n    df['value_lag'+str(lag)] = df.groupby('id')['value'].shift(lag)\nfor lag in list(range(4, 0, -1)):\n    df['value_trend_lag'+str(lag)] = df.groupby('id')['value_trend'].shift(lag)\n\ndf['target_value'] = df.groupby('id')['value'].shift(-1).astype('float')\n\ndf.dropna(inplace=True)\ndf.drop(columns=['id'], inplace=True)\n\ndf.sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hier ein TabularPandas erstellen","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hier das neuronale Netzwerk (tabular_learner) erstellen (vorher noch min/max für y_range checken)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainieren (fit_one_cycle)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualisieren","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Musterlösung: 1. Aufgabe - Embeddings für den Walmart-Datensatz\n","metadata":{}},{"cell_type":"code","source":"train_condition = df['wm_yr_wk'] < 1500\ntrain_idx = np.where(train_condition)[0]\nvalid_idx = np.where(~train_condition)[0]\n\nsplits = (list(train_idx),list(valid_idx))\n\ncat_names = ['state_id', 'store_id', 'cat_id', 'dept_id', 'item_id']\ncont_names = df.columns.drop(['state_id', 'store_id', 'cat_id', 'dept_id', 'item_id', 'wm_yr_wk']).values.tolist()\npreprocessing = [Categorify, FillMissing, Normalize]\n\nto = TabularPandas(df, preprocessing, cat_names, cont_names, splits=splits, y_names='target_value')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(df['target_value'].min(), df['target_value'].max())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn = tabular_learner(to.dataloaders(1024), layers=[200,100], y_range=(0, 9), loss_func=F.mse_loss, n_out=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.fit_one_cycle(10, 0.033)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\nvar = 'store_id'\nvar_index = cat_names.index(var)\nvar_values = list(df[var].astype('category').cat.categories.values)\n\nemb_matrix = to_np(next(learn.model.embeds[var_index].parameters()))\nemb_coordinates = PCA(n_components=2).fit_transform(emb_matrix)\nannotation = np.append('Other', np.array(var_values))\n\nemb_df = pd.DataFrame(emb_coordinates, columns=['X', 'Y'])\nemb_df[var]=annotation\n\nfig = px.scatter(emb_df, x='X', y='Y', text=var)\nfig.update_traces(textposition='top center')\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}