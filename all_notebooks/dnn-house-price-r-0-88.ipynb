{"cells":[{"metadata":{},"cell_type":"markdown","source":"# King County House Data\n\n\n\n\nThe dataset consists of house prices from King County an area in the US State of Washington, this data also covers Seattle. The <a target=\"_new\" href=\"https://www.kaggle.com/shivachandel/kc-house-data\">dataset</a> was obtained from <a target=\"_new\" href=\"https://www.kaggle.com/shivachandel/kc-house-data\">Kaggle</a>.\n\n<ol>\n    <li><a target=\"_self\" href=\"#obj\" id=\"index-obj\" style=\"color: black;\">Objective</a></li>\n    <li><a target=\"_self\" href=\"#load-data\" id=\"index-load-data\" style=\"color: black;\">Load Data</a></li>\n    <li><a target=\"_self\" href=\"#var-types\" id=\"index-var-types\" style=\"color: black;\">Identification of variables and data types</a></li>\n    <li><a target=\"_self\" href=\"#metrics\" id=\"index-metrics\" style=\"color: black;\">Analyzing the basic metrics</a></li>\n    <li><a target=\"_self\" href=\"#uni-analysis\" id=\"index-uni-analysis\" style=\"color: black;\">Non-Graphical Univariate Analysis</a></li>\n    <li><a target=\"_self\" href=\"#uni-analysis2\" id=\"index-uni-analysis2\" style=\"color: black;\">Graphical Univariate Analysis</a></li> \n    <li><a target=\"_self\" href=\"#bi-analysis\" id=\"index-bi-analysis\" style=\"color: black;\">Graphical Bivariate Analysis</a></li>\n    <li><a target=\"_self\" href=\"#var-trans\" id=\"index-var-trans\" style=\"color: black;\">Variable transformations</a></li>\n    <li><a target=\"_self\" href=\"#nan-value\" id=\"index-nan-value\" style=\"color: black;\">Missing value treatment</a></li>\n    <li>\n        <a target=\"_self\" href=\"#outlier\" id=\"index-outlier\" style=\"color: black;\">Outlier treatment</a>\n    </li>\n    <li>\n        <a target=\"_self\" href=\"#feat-eng\" id=\"index-feat-eng\" style=\"color: black;\">Feature Engineering</a>\n    </li>\n    <li>\n        <a target=\"_self\" href=\"#model\" id=\"index-model\" style=\"color: black;\">Model - Deep Neural Network</a>\n    </li>     \n    <li>\n        <a target=\"_self\" href=\"#feat-imp\" id=\"index-feat-imp\" style=\"color: black;\">Feature Importance</a>\n    </li>    \n</ol>\n"},{"metadata":{},"cell_type":"markdown","source":"<h3><a target=\"_self\" href=\"#index-obj\" id=\"obj\" style=\"color: black;\">Objective</a></h3>"},{"metadata":{},"cell_type":"markdown","source":"The objective here is to build a model to predict the price of the houses, and check which features play an important role to the model."},{"metadata":{},"cell_type":"markdown","source":"<h3><a target=\"_self\" href=\"#index-load-data\" id=\"load-data\" style=\"color: black;\">Load Data</a></h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nsns.set(palette='RdYlGn')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/kc-house-data/kc_house_data.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><a target=\"_self\" href=\"#index-var-types\" id=\"var-types\" style=\"color: black;\">Identification of variables and data types</a></h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'm going to delete the id and date columns, since they will not help us too much in our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(columns=['id', 'date'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><a target=\"_self\" href=\"#index-metrics\" id=\"metrics\" style=\"color: black;\">Analyzing the basic metrics</a></h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().T.drop(columns=['count'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><a target=\"_self\" href=\"#index-uni-analysis\" id=\"uni-analysis\" style=\"color: black;\">Non-Graphical Univariate Analysis</a></h3>"},{"metadata":{},"cell_type":"markdown","source":"What are the characteristics of the most expensive house?"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sort_values(by='price').tail(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wooow! A house of 7.7 millions dollars?! I should have made an offer! :)"},{"metadata":{},"cell_type":"markdown","source":"What are the characteristics of the oldest house?"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sort_values(by='yr_built').head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A house built in 1900, which has never been renovated. It must probably be falling apart."},{"metadata":{},"cell_type":"markdown","source":"<h3><a target=\"_self\" href=\"#index-uni-analysis2\" id=\"uni-analysis2\" style=\"color: black;\">Graphical Univariate Analysis</a></h3>\n\nLet's check the behaviour of our features."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_vars = ['price', 'long', 'lat', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'sqft_living15', 'sqft_lot15', 'yr_built', 'yr_renovated', 'zipcode']\ncat_vars = ['bedrooms', 'bathrooms', 'floors', 'waterfront', 'view', 'condition', 'grade']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_univariate_panel(vars_name, data, func_plot, n_cols=2):\n    \n    from math import ceil\n    \n    n_rows = ceil(len(vars_name) / n_cols)\n    \n    plt.figure(figsize=(7 * n_cols, 4 * n_rows))\n    for idx, var in enumerate(vars_name, 1):\n        plt.subplot(n_rows, n_cols, idx)\n        func_plot(data[var])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_univariate_panel(num_vars, df, sns.boxplot, 3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the boxplots above, we can see many outliers and the data distributions are distorted. We will deal with these problems later. Next, let's take a look on the discrete features."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_univariate_panel(cat_vars, df, sns.countplot, 3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><a target=\"_self\" href=\"#index-bi-analysis\" id=\"bi-analysis\" style=\"color: black;\">Graphical Bivariate Analysis</a></h3>\n\nNow let's take a look at how each feature behaves compared to the price."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_bivariate_panel(vars_name, var_ref, data, n_cols=3):\n\n    from math import ceil\n    \n    n_rows = ceil(len(vars_name) / n_cols)\n    \n    plt.figure(figsize=(7 * n_cols, 4 * n_rows))\n    for idx, var in enumerate(vars_name, 1):\n        \n        mean = df.groupby(by=var).mean()[[var_ref]].reset_index()\n        std = df.groupby(by=var).std()[var_ref].fillna(0)\n\n        plt.subplot(n_rows, n_cols, idx)        \n        sns.scatterplot(x=var, y=var_ref, data=mean)\n        plt.ylabel(f'Mean {var_ref}')               ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_bivariate_panel(df.drop(columns=['price']).columns, 'price', df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apparently, all features seem to have some relationship with the price. Next, let's see how price is distributed along geographical coordinates."},{"metadata":{"trusted":true},"cell_type":"code","source":"bbox = (\n    (df['long'].min(), df['long'].max(),\n    df['lat'].min(), df['lat'].max())\n)\nbbox","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 8))\nhouse_map = plt.imread('/kaggle/input/mapcity/map.png')\n\nplt.imshow(house_map, zorder=0, extent=bbox, aspect='equal')\nsns.scatterplot(\n    x='long', \n    y='lat', \n    data=df[df['price'] < 2e6], \n    hue='price',    \n    zorder=1, \n    edgecolor=None, \n    alpha=0.2,    \n    palette='hot'\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to the map, the most expensive houses are near Lake Washington, and it is very concentrated on Mercer Island. The next map shows the distribution of houses according to their age."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 8))\nhouse_map = plt.imread('/kaggle/input/mapcity/map.png')\n\nplt.imshow(house_map, zorder=0, extent=bbox, aspect='equal')\nsns.scatterplot(\n    x='long', \n    y='lat', \n    data=df, \n    hue='yr_built',    \n    zorder=1, \n    edgecolor=None, \n    alpha=0.2,    \n    palette='hot'\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, the oldest houses are near Lake Washington, on the side of Seattle city."},{"metadata":{},"cell_type":"markdown","source":"<h3><a target=\"_self\" href=\"#index-var-trans\" id=\"var-trans\" style=\"color: black;\">Variable transformations</a></h3>\n\nSome variables are almost normal distributed, right-skewed, and we can fix it taking the logarithm of each value."},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df = df.copy()\n\nnew_df['price'] = np.log(new_df['price'])\nnew_df['sqft_living'] = np.log(new_df['sqft_living'])\nnew_df['sqft_above'] = np.log(new_df['sqft_above'])\nnew_df['sqft_living15'] = np.log(new_df['sqft_living15'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><a target=\"_self\" href=\"#index-nan-value\" id=\"nan-value\" style=\"color: black;\">Missing value treatment</a></h3>\n\nFortunately, there are no missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Missing values: {new_df.isnull().sum().sum()}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><a target=\"_self\" href=\"#index-outlier\" id=\"outlier\" style=\"color: black;\">Outlier treatment</a></h3>\n\nAs we saw on the past graphs, many variables have outliers. I'm going to use Tukey's fences method to remove them."},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_outliers(data, var_names):\n    ans = data.copy()\n    for var in var_names:\n        var_info = ans[var].describe()        \n        iq_range = var_info['75%'] - var_info['25%']\n        intv_range = (var_info['25%'] - 1.5 * iq_range, var_info['75%'] + 1.5 * iq_range)\n        ans = ans[ans[var].between(*intv_range)]\n    return ans\n\nnew_df = remove_outliers(new_df, num_vars)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_univariate_panel(num_vars, new_df, sns.boxplot, 3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the boxplots above, we can see that there are still outliers, but I will not remove them because we may lose information."},{"metadata":{},"cell_type":"markdown","source":"<h3><a target=\"_self\" href=\"#index-feat-eng\" id=\"feat-eng\" style=\"color: black;\">Feature Engineering</a></h3>\n\nAt that point, we could go ahead and build our model, but we will try to build more resources to capture more information.\n\nThe first thing I wanto to do is exclude the zipcode variable, because it will give me an extra work figuring out how to use this feature properly, and since we have the geographic coordinates, propably those will give us a more accurate information."},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df.drop('zipcode', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The second thing I'm going to do is remap the year renovated variable, because some houses weren't renovated since it was built. My idea is to replace the zero values by the year that the house was built."},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df['yr_renovated'] = new_df[['yr_renovated', 'yr_built']].apply(lambda pair: pair[0] if pair[0] != 0 else pair[1], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The third thing to do is to create square features."},{"metadata":{"trusted":true},"cell_type":"code","source":"def square_features(features, X, degree=2):\n    \n    X_cp = X.copy()\n    X_features = X.columns\n    \n    for feature in features:\n        if feature in X_features:\n            X_cp[feature + f'^{degree}'] = np.power(X_cp[feature], degree)\n    \n    return X_cp\n\nnew_df = square_features(num_vars[1:] + cat_vars, new_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The fourth and final thing to do is to create a deviation variable to capture how the year the house was built fits into your average grade."},{"metadata":{"trusted":true},"cell_type":"code","source":"cat = new_df.groupby('grade')['yr_built']\nnew_df['yr_built_dev_grade'] = cat.transform(lambda x: (x - x.mean()) / x.std())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><a target=\"_self\" href=\"#index-model\" id=\"model\" style=\"color: black;\">Model - Deep Neural Network</a></h3>\n\nTo model this problem, I'll use a deep neural network with eight layers and 36 units per layer."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow import keras\nfrom sklearn.metrics import mean_absolute_error, r2_score, explained_variance_score\n\n\nX = new_df.drop(columns=['price'])\ny = new_df['price']\ny = np.power(np.exp(1), y)\n\n# Splitting the dataset into train and test data\nX_train, X_test, y_train, y_test = train_test_split(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler().fit(X_train)\n\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = keras.Sequential()\n\nn_units = X.shape[1]\nn_layers = 8\n\nfor _ in range(n_layers):\n    model.add(keras.layers.Dense(units=n_units, activation='relu'))\n\nmodel.add(keras.layers.Dense(units=1))\n\nmodel.compile(optimizer='adam', loss='mse')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = model.fit(\n    x=X_train, \n    y=y_train,    \n    verbose=False,\n    validation_data=(X_test, y_test),\n    batch_size=128,\n    epochs=500\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = model.predict(x=X_train)\ny_test_pred = model.predict(x=X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'MAE train set: {mean_absolute_error(y_train_pred, y_train):.2}')\nprint(f'MAE test set: {mean_absolute_error(y_test_pred, y_test):.2}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_df = pd.DataFrame(hist.history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 6))\nx, y = range(1, len(loss_df['loss']) + 1), loss_df['loss']\nstart = 2\nplt.plot(x[start:], y[start:], label='Training Loss')\nx, y = range(1, len(loss_df['val_loss']) + 1), loss_df['val_loss']\nplt.plot(x[start:], y[start:], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('MSE')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see in the learning curves above, the model is not over-dimensioning or under-adjusting the data. It has achived a good balance. Let's compare the some statistical information about the real and predicted price."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred = model.predict(X_test)\n\ncomp_df = pd.DataFrame(\n    data={\n        'Test True Y': y_test,\n        'Test Pred Y': y_test_pred.reshape((-1,))\n    }\n)\n\ncomp_df.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 6))\n\nplt.subplot(1,2,1)\nplt.scatter(\n    x=y_train, \n    y=y_train_pred, \n    alpha=0.3, \n    edgecolor=None\n)\nplt.plot(\n    [y_train.min(), y_train.max()],\n    [y_train.min(), y_train.max()],\n    color='black', \n    linestyle='dashed', \n    label=f'Desired\\nR² train set: {r2_score(y_train, y_train_pred):.2}'\n)\nplt.legend()\nplt.xlabel('True value')\nplt.ylabel('Predicted value')\n\n\nplt.subplot(1,2,2)\nplt.scatter(\n    x=y_test, \n    y=y_test_pred, \n    alpha=0.3, \n    edgecolor=None\n)\nplt.plot(\n    [y_test.min(), y_test.max()],\n    [y_test.min(), y_test.max()],\n    color='black', \n    linestyle='dashed', \n    label=f'Desired\\nR² test set: {r2_score(y_test, y_test_pred):.2}'\n)\nplt.legend()\nplt.xlabel('True value')\nplt.ylabel('Predicted value')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the graphs above, the model is doing a pretty decent job. The R² coefficient in the training set is about 0.9 and 0.88 in the test set."},{"metadata":{},"cell_type":"markdown","source":"<h3><a target=\"_self\" href=\"#index-feat-imp\" id=\"feat-imp\" style=\"color: black;\">Feature Importance</a></h3>\n\nNow let's get an idea which features are more important to our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_importance(model, metric, X, y, features_name, shuffles_per_column=10):\n    \n    n_rows, n_cols = X.shape\n    ref_score = metric(y, model.predict(X)) # Reference score    \n    mean_list, std_list = [], []\n    X_cp = X.copy()\n    \n    for c in range(n_cols):\n        metric_list = []\n        for _ in range(shuffles_per_column):\n            np.random.shuffle(X_cp[:, c])\n            y_pred = model.predict(X_cp)\n            score = metric(y, y_pred)\n            metric_list.append(ref_score - score)\n            X_cp[:, c] = X[:, c]        \n        mean_list.append(np.mean(metric_list))\n        std_list.append(np.std(metric_list))\n    \n    importance_rel = [np.round(100*np.mean(mean)/np.sum(mean_list), 2) for mean in mean_list]\n            \n    return pd.DataFrame({\n        'Feature': features_name,\n        'Importance (%)': importance_rel,\n        'Mean': mean_list,\n        'Std': std_list\n    }).sort_values('Importance (%)', ascending=False).reset_index(drop=True)\n\nans = feature_importance(model, r2_score, X_test, y_test, X.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 8))\nsns.barplot(x='Importance (%)', y='Feature', data=ans)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we would expect, the localization of the house is very important to predict the price, that's why latitude and longitude is on the top of importance. Second, the area available. Third, the year that the house was built and so on."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}