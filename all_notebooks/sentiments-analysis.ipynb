{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"contextual mining sentiments analysis of racist/non-racist using NLP","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-22T12:58:19.097205Z","iopub.execute_input":"2021-07-22T12:58:19.097657Z","iopub.status.idle":"2021-07-22T12:58:19.103508Z","shell.execute_reply.started":"2021-07-22T12:58:19.097609Z","shell.execute_reply":"2021-07-22T12:58:19.102633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset \n\nDataset that I have used contains variety of tweets and our objective is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets.\n\nFormally, given a training sample of tweets and labels, where label '1' denotes the tweet is racist/sexist and label '0' denotes the tweet is not racist/sexist.\n\n# Motivation\n\nHate  speech  is  an  unfortunately  common  occurrence  on  the  Internet.  Often social media sites like Facebook and Twitter face the problem of identifying and censoring  problematic  posts  while weighing the right to freedom of speech. The  importance  of  detecting  and  moderating hate  speech  is  evident  from  the  strong  connection between hate speech and actual hate crimes. Early identification of users promoting  hate  speech  could  enable  outreach  programs that attempt to prevent an escalation from speech to action.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/train.csv')\ndf.head(10)","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2021-07-22T12:58:19.11508Z","iopub.execute_input":"2021-07-22T12:58:19.115604Z","iopub.status.idle":"2021-07-22T12:58:19.299256Z","shell.execute_reply.started":"2021-07-22T12:58:19.115538Z","shell.execute_reply":"2021-07-22T12:58:19.29833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Let's start our solution with some basic exploratory data analysis and try do dig some insights from the dataset.</h3>\n","metadata":{}},{"cell_type":"markdown","source":"First let's divide our dataset into two part, first one contains positive tweets and the other one containing negative tweets i.e. tweets with racist/ sexist remarks","metadata":{}},{"cell_type":"code","source":"train_pos = df[df['label'] == 0]\ntrain_neg = df[df['label'] == 1]","metadata":{"execution":{"iopub.status.busy":"2021-07-22T12:58:19.300544Z","iopub.execute_input":"2021-07-22T12:58:19.300912Z","iopub.status.idle":"2021-07-22T12:58:19.311441Z","shell.execute_reply.started":"2021-07-22T12:58:19.300727Z","shell.execute_reply":"2021-07-22T12:58:19.310329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now lets  clean the tweets from hashtags, mentions and links","metadata":{}},{"cell_type":"code","source":"def clean_word(data):\n    words = \" \".join(data['tweet'])\n    \n    cleaned_words = \" \".join([word for word in words.split() \n                             if 'http' not in word\n                             and not word.startswith('@')\n                             and not word.startswith('#')\n                             and word != 'RT'])\n    return cleaned_words","metadata":{"execution":{"iopub.status.busy":"2021-07-22T12:58:19.312776Z","iopub.execute_input":"2021-07-22T12:58:19.313055Z","iopub.status.idle":"2021-07-22T12:58:19.319345Z","shell.execute_reply.started":"2021-07-22T12:58:19.313002Z","shell.execute_reply":"2021-07-22T12:58:19.318408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pos_clean_words = clean_word(train_pos)\nneg_clean_words = clean_word(train_neg)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T12:58:19.320641Z","iopub.execute_input":"2021-07-22T12:58:19.320857Z","iopub.status.idle":"2021-07-22T12:58:19.58014Z","shell.execute_reply.started":"2021-07-22T12:58:19.320818Z","shell.execute_reply":"2021-07-22T12:58:19.579416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we are ready for a WordCloud visualization which shows only the most emphatic words of the Positive and Negative tweets.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nfrom wordcloud import WordCloud,STOPWORDS","metadata":{"execution":{"iopub.status.busy":"2021-07-22T12:58:19.581455Z","iopub.execute_input":"2021-07-22T12:58:19.581783Z","iopub.status.idle":"2021-07-22T12:58:19.60597Z","shell.execute_reply.started":"2021-07-22T12:58:19.581657Z","shell.execute_reply":"2021-07-22T12:58:19.60514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Stopwords are some of the most frequently occuring words e.g. 'a','an','the'. They does not give any significant information regarding the content and context of text.\n\n[Check out this link for more information.](https://en.wikipedia.org/wiki/Stop_words)","metadata":{}},{"cell_type":"code","source":"def wcloud(cleaned_words):\n    wordcloud = WordCloud(stopwords=STOPWORDS,\n                         background_color='black',\n                         width=3000,\n                          height=2500\n                         ).generate(cleaned_words)\n    return wordcloud","metadata":{"execution":{"iopub.status.busy":"2021-07-22T12:58:19.607352Z","iopub.execute_input":"2021-07-22T12:58:19.607755Z","iopub.status.idle":"2021-07-22T12:58:19.612602Z","shell.execute_reply.started":"2021-07-22T12:58:19.607689Z","shell.execute_reply":"2021-07-22T12:58:19.611537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pos_wcloud = wcloud(pos_clean_words)\nneg_wcloud = wcloud(neg_clean_words)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T12:58:19.614133Z","iopub.execute_input":"2021-07-22T12:58:19.614595Z","iopub.status.idle":"2021-07-22T12:59:13.651674Z","shell.execute_reply.started":"2021-07-22T12:58:19.614528Z","shell.execute_reply":"2021-07-22T12:59:13.650542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Non racist tweets')\n\nplt.figure(1,figsize=(12,12))\nplt.imshow(pos_wcloud)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-22T12:59:13.652902Z","iopub.execute_input":"2021-07-22T12:59:13.653187Z","iopub.status.idle":"2021-07-22T12:59:14.711734Z","shell.execute_reply.started":"2021-07-22T12:59:13.653121Z","shell.execute_reply":"2021-07-22T12:59:14.709575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that for the positive tweets most commonly occuring words are **'amp','happy','day','love',** etc. and most of this word doesn't conveys any racist sentiments","metadata":{}},{"cell_type":"code","source":"print('Racist tweets')\n\nplt.figure(1,figsize=(12,12))\nplt.imshow(neg_wcloud)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-22T12:59:14.713197Z","iopub.execute_input":"2021-07-22T12:59:14.713526Z","iopub.status.idle":"2021-07-22T12:59:15.711851Z","shell.execute_reply.started":"2021-07-22T12:59:14.71346Z","shell.execute_reply":"2021-07-22T12:59:15.710907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the negative tweets has most common occuring words such as 'racist','black','libtard' even 'trump'(:p which is funny though :P)\n\n**One important insight that we can draw from the visualization is that the word 'amp' has a significant presence in positive as well as in negative tweets, hence we have to take care of this as it may confuse our classifier**","metadata":{}},{"cell_type":"markdown","source":"One last thing that we would be exploring is distribution of our dataset","metadata":{}},{"cell_type":"code","source":"df['label'].value_counts(normalize = True).plot.bar()","metadata":{"execution":{"iopub.status.busy":"2021-07-22T12:59:15.713149Z","iopub.execute_input":"2021-07-22T12:59:15.713456Z","iopub.status.idle":"2021-07-22T12:59:16.044443Z","shell.execute_reply.started":"2021-07-22T12:59:15.713395Z","shell.execute_reply":"2021-07-22T12:59:16.043489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see our dataset is highly skewed, we will have to take care of this.\n\nBut we won't be going in much details of on how to deal with skewed data as the main motive of this kernel is to give you a starting point for performing sentiment analysis.\n\n[You can check out this link for more information regarding handling of skewed data](https://medium.com/@TheDataGyan/day-8-data-transformation-skewness-normalization-and-much-more-4c144d370e55)","metadata":{}},{"cell_type":"markdown","source":"Now we done with our exploratory data analysis.\n\n# It's time for us to perform some data preprocessing before we can use it for training our classifier.\n","metadata":{}},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords","metadata":{"execution":{"iopub.status.busy":"2021-07-22T12:59:16.045891Z","iopub.execute_input":"2021-07-22T12:59:16.046441Z","iopub.status.idle":"2021-07-22T12:59:17.678069Z","shell.execute_reply.started":"2021-07-22T12:59:16.04638Z","shell.execute_reply":"2021-07-22T12:59:17.676874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will using [Regular Expression](https://docs.python.org/3/howto/regex.html) for removing special characters eg. '@','#', etc.","metadata":{}},{"cell_type":"code","source":"def clean_tweet_words(tweet):\n    alpha_only = re.sub(\"[^a-zA-Z]\",' ',tweet) #\"[^a-zA-Z]\" this regex will remove any non-alphabetical char as they are not significant\n    words = alpha_only.lower().split()\n    stop = set(stopwords.words('english'))\n    #from the dataframe we can see 'user' word is quite common in the tweets, which is basically used for tagging someone in the tweet\n    #so I will be removing that\n    stop.add('user')\n    sig_words = [word for word in words if not word in stop]\n    return(\" \".join(sig_words))","metadata":{"execution":{"iopub.status.busy":"2021-07-22T12:59:17.679647Z","iopub.execute_input":"2021-07-22T12:59:17.680234Z","iopub.status.idle":"2021-07-22T12:59:17.688049Z","shell.execute_reply.started":"2021-07-22T12:59:17.680167Z","shell.execute_reply":"2021-07-22T12:59:17.687113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['clean_tweet']  = df['tweet'].apply(lambda tweet: clean_tweet_words(tweet))\n\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T12:59:17.689762Z","iopub.execute_input":"2021-07-22T12:59:17.690465Z","iopub.status.idle":"2021-07-22T12:59:23.249872Z","shell.execute_reply.started":"2021-07-22T12:59:17.690399Z","shell.execute_reply":"2021-07-22T12:59:23.249014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now our dataset contains a column with cleaned tweets","metadata":{}},{"cell_type":"markdown","source":"Now I'll be splitting the dataset into train and test set using skelearn's train_test_split\n\n**#suggestion: Rather using train_test_split() you should use [sklearn.model_selection.StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) this will make sure that the distribution in train and test set remains similar**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain,test = train_test_split(df,test_size = 0.2,random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T12:59:23.251499Z","iopub.execute_input":"2021-07-22T12:59:23.251838Z","iopub.status.idle":"2021-07-22T12:59:23.264567Z","shell.execute_reply.started":"2021-07-22T12:59:23.251772Z","shell.execute_reply":"2021-07-22T12:59:23.263911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_clean_tweet = []\nfor tweet in train['clean_tweet']:\n    train_clean_tweet.append(tweet)\ntest_clean_tweet = []\nfor tweet in test['clean_tweet']:\n    test_clean_tweet.append(tweet)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T12:59:23.265947Z","iopub.execute_input":"2021-07-22T12:59:23.266306Z","iopub.status.idle":"2021-07-22T12:59:23.281822Z","shell.execute_reply.started":"2021-07-22T12:59:23.266238Z","shell.execute_reply":"2021-07-22T12:59:23.280818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Until now we are done with the basic preprocessing steps, but one **crucial thing is still left** our data is still in the form of text and we can't feed it directly to classifier.\n\nThere are two approaches to solve this problem:\n\n* Use sklearn CountVectorizer\n* Or we can use sklearn TfidfVectorizer\n\n# CountVectorizer\nCountVectorizer converts the all the text in the document to form [DTM(Document Term Matrix)](https://en.wikipedia.org/wiki/Document-term_matrix)\n\n# TfidfVectorizer\n\nTfidfVectorizer first prepares A DTM and then prepares a [tf-idf matrix](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n\nFrom the exploratory data analysis we have seen words such as 'amp',etc. whose occurrence frequency is quite high in positive as well as in negative tweets, tf-idf penalises such words with high occurring frequency and give importance to less frequently occuring words as they can give more meaning to a sentence.\n\nSo we will be using TfidfVectorizer for our solution\n\nFeel free to do experiments with CountVectorizer.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"execution":{"iopub.status.busy":"2021-07-22T12:59:23.283269Z","iopub.execute_input":"2021-07-22T12:59:23.283615Z","iopub.status.idle":"2021-07-22T12:59:23.294181Z","shell.execute_reply.started":"2021-07-22T12:59:23.28355Z","shell.execute_reply":"2021-07-22T12:59:23.293594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now here's a hack rather instantiating TfidfVectorizer directly and then using .fit_trasform() and .transform() method separately on train and test set we will add this to our ML pipeline so that we don't have to do it every time for train, cross-validation and test sets.\n\nFor the sake of simplicity we will be using [LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html) and [MultinomialNB](hhttps://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) model for our solution as they are believed to deliver a robust performance on text data.\n\nFeel free to experiment with other models and share you results :)\n","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline","metadata":{"execution":{"iopub.status.busy":"2021-07-22T12:59:23.295277Z","iopub.execute_input":"2021-07-22T12:59:23.295777Z","iopub.status.idle":"2021-07-22T12:59:23.312657Z","shell.execute_reply.started":"2021-07-22T12:59:23.29572Z","shell.execute_reply":"2021-07-22T12:59:23.311701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc_pipe = Pipeline([('tfidf',TfidfVectorizer()),('svc', LinearSVC(random_state=0,max_iter=5000))])\nnb_pipe = Pipeline([('tfidf',TfidfVectorizer()),('nb', MultinomialNB())])","metadata":{"execution":{"iopub.status.busy":"2021-07-22T12:59:23.314215Z","iopub.execute_input":"2021-07-22T12:59:23.314761Z","iopub.status.idle":"2021-07-22T12:59:23.324729Z","shell.execute_reply.started":"2021-07-22T12:59:23.314692Z","shell.execute_reply":"2021-07-22T12:59:23.323643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc_pipe.fit(train_clean_tweet,train['label'])\nnb_pipe.fit(train_clean_tweet,train['label'])","metadata":{"execution":{"iopub.status.busy":"2021-07-22T12:59:23.326051Z","iopub.execute_input":"2021-07-22T12:59:23.326352Z","iopub.status.idle":"2021-07-22T12:59:24.436036Z","shell.execute_reply.started":"2021-07-22T12:59:23.326299Z","shell.execute_reply":"2021-07-22T12:59:24.43507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_svc = svc_pipe.predict(test_clean_tweet)\npred_nb = nb_pipe.predict(test_clean_tweet)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T12:59:24.437254Z","iopub.execute_input":"2021-07-22T12:59:24.437535Z","iopub.status.idle":"2021-07-22T12:59:24.652168Z","shell.execute_reply.started":"2021-07-22T12:59:24.437477Z","shell.execute_reply":"2021-07-22T12:59:24.651489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2021-07-22T12:59:24.65324Z","iopub.execute_input":"2021-07-22T12:59:24.653709Z","iopub.status.idle":"2021-07-22T12:59:24.657366Z","shell.execute_reply.started":"2021-07-22T12:59:24.653638Z","shell.execute_reply":"2021-07-22T12:59:24.656598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('SVC')\nprint(accuracy_score(test['label'],pred_svc))\nprint('\\n')\nprint(confusion_matrix(test['label'],pred_svc))\nprint('\\n')","metadata":{"execution":{"iopub.status.busy":"2021-07-22T12:59:24.658471Z","iopub.execute_input":"2021-07-22T12:59:24.658749Z","iopub.status.idle":"2021-07-22T12:59:24.688827Z","shell.execute_reply.started":"2021-07-22T12:59:24.658698Z","shell.execute_reply":"2021-07-22T12:59:24.687908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our classifier is doing good on +ve tweets but some work still needs to be done so as to improve its performance on -ve tweets.","metadata":{}},{"cell_type":"code","source":"print('Naive Bayes Classifier')\nprint(accuracy_score(test['label'],pred_nb))\nprint('\\n')\nprint(confusion_matrix(test['label'],pred_nb))\nprint('\\n')","metadata":{"execution":{"iopub.status.busy":"2021-07-22T12:59:24.690416Z","iopub.execute_input":"2021-07-22T12:59:24.690689Z","iopub.status.idle":"2021-07-22T12:59:24.710472Z","shell.execute_reply.started":"2021-07-22T12:59:24.690638Z","shell.execute_reply":"2021-07-22T12:59:24.708929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our Support Vector Classifer is showing good results, but it can be improved further by doing some feature engineering such as adding a new feature like length of tweet, etc. also switiching to a different model such as Adaboost or Random Forest might give a better result.\n\nSo I leave it on you to explore and learn and most important to have fun!! :)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}