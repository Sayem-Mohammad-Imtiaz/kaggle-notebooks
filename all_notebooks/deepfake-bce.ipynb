{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport cv2\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.transforms import Normalize\n\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !ls /kaggle/input/deepfake-faces/faces_224","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ngpu","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"root = '/kaggle/input/deepfake-faces/faces_224'\n\nmetadata_df = pd.read_csv(\"/kaggle/input/deepfake-faces/metadata.csv\")\nmetadata_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(metadata_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_size = 224\nbatch_size = 64","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata_df.groupby(['label']).count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_path = os.path.join(root, np.random.choice(os.listdir(root)))\nimg = cv2.imread(img_path)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nplt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nclass Unnormalize:\n    \"\"\"Converts an image tensor that was previously Normalize'd\n    back to an image with pixels in the range [0, 1].\"\"\"\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        mean = torch.as_tensor(self.mean, dtype=tensor.dtype, device=tensor.device).view(3, 1, 1)\n        std = torch.as_tensor(self.std, dtype=tensor.dtype, device=tensor.device).view(3, 1, 1)\n        return torch.clamp(tensor*std + mean, 0., 1.)\n\n\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\nnormalize_transform = Normalize(mean, std)\nunnormalize_transform = Unnormalize(mean, std)\n\ndef random_hflip(img, p=0.5):\n    \"\"\"Random horizontal flip.\"\"\"\n    if np.random.random() < p:\n        return cv2.flip(img, 1)\n    else:\n        return img\n    \ndef load_image_and_label(filename, cls, crops_dir, image_size, augment):\n    \"\"\"Loads an image into a tensor. Also returns its label.\"\"\"\n    img = cv2.imread(os.path.join(crops_dir, filename))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    if augment: \n        img = random_hflip(img)\n\n    img = cv2.resize(img, (image_size, image_size))\n\n    img = torch.tensor(img).permute((2, 0, 1)).float().div(255)\n    img = normalize_transform(img)\n\n    target = 1 if cls == \"FAKE\" else 0\n    return img, target\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img, target = load_image_and_label(\"aabuyfvwrh.jpg\", \"FAKE\", root, 224, augment=True)\nimg.shape, target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(unnormalize_transform(img).permute((1, 2, 0)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass VideoDataset(Dataset):\n    \"\"\"Face crops dataset.\n\n    Arguments:\n        crops_dir: base folder for face crops\n        df: Pandas DataFrame with metadata\n        split: if \"train\", applies data augmentation\n        image_size: resizes the image to a square of this size\n        sample_size: evenly samples this many videos from the REAL\n            and FAKE subfolders (None = use all videos)\n        seed: optional random seed for sampling\n    \"\"\"\n    def __init__(self, crops_dir, df, split, image_size, sample_size=None, seed=None):\n        self.crops_dir = crops_dir\n        self.split = split\n        self.image_size = image_size\n        \n        if sample_size is not None:\n            real_df = df[df[\"label\"] == \"REAL\"]\n            fake_df = df[df[\"label\"] == \"FAKE\"]\n            sample_size = np.min(np.array([sample_size, len(real_df), len(fake_df)]))\n            print(\"%s: sampling %d from %d real videos\" % (split, sample_size, len(real_df)))\n            print(\"%s: sampling %d from %d fake videos\" % (split, sample_size, len(fake_df)))\n            real_df = real_df.sample(sample_size, random_state=seed)\n            fake_df = fake_df.sample(sample_size, random_state=seed)\n            self.df = pd.concat([real_df, fake_df])\n        else:\n            self.df = df\n\n        num_real = len(self.df[self.df[\"label\"] == \"REAL\"])\n        num_fake = len(self.df[self.df[\"label\"] == \"FAKE\"])\n        print(\"%s dataset has %d real videos, %d fake videos\" % (split, num_real, num_fake))\n\n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        filename = row[\"videoname\"][:-4] + \".jpg\"\n        cls = row[\"label\"]\n        return load_image_and_label(filename, cls, self.crops_dir, \n                                    self.image_size, self.split == \"train\")\n    def __len__(self):\n        return len(self.df)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = VideoDataset(root, metadata_df, \"val\", image_size, sample_size=1000, seed=1234)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(unnormalize_transform(dataset[0][0]).permute(1, 2, 0))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_splits(crops_dir, metadata_df, frac):\n    # Make a validation split. Sample a percentage of the real videos, \n    # and also grab the corresponding fake videos.\n    real_rows = metadata_df[metadata_df[\"label\"] == \"REAL\"]\n    real_df = real_rows.sample(frac=frac, random_state=666)\n    fake_df = metadata_df[metadata_df[\"original\"].isin(real_df[\"videoname\"])]\n    val_df = pd.concat([real_df, fake_df])\n\n    # The training split is the remaining videos.\n    train_df = metadata_df.loc[~metadata_df.index.isin(val_df.index)]\n\n    return train_df, val_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df, val_df = make_splits(root, metadata_df, frac=0.05)\n\nassert(len(train_df) + len(val_df) == len(metadata_df))\nassert(len(train_df[train_df[\"videoname\"].isin(val_df[\"videoname\"])]) == 0)\n\ndel train_df, val_df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndef create_data_loaders(crops_dir, metadata_df, image_size, batch_size, num_workers):\n    train_df, val_df = make_splits(crops_dir, metadata_df, frac=0.05)\n\n    train_dataset = VideoDataset(crops_dir, train_df, \"train\", image_size, sample_size=10000)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n                              num_workers=num_workers, pin_memory=True)\n\n    val_dataset = VideoDataset(crops_dir, val_df, \"val\", image_size, sample_size=500, seed=1234)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, \n                            num_workers=num_workers, pin_memory=True)\n\n    return train_loader, val_loader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loader, val_loader = create_data_loaders(root, metadata_df, image_size, batch_size, num_workers=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = next(iter(train_loader))\nplt.imshow(unnormalize_transform(X[4]).permute(1, 2, 0))\nprint(y[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = next(iter(val_loader))\nplt.imshow(unnormalize_transform(X[2]).permute(1, 2, 0))\nprint(y[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(net, data_loader, device, silent=False):\n    net.train(False)\n\n    loss = 0\n    total_examples = 0\n\n    with tqdm(total=len(data_loader), desc=\"Evaluation\", leave=False, disable=silent) as pbar:\n        for batch_idx, data in enumerate(data_loader):\n            with torch.no_grad():\n                batch_size = data[0].shape[0]\n                x = data[0].to(device)\n                y_true = data[1].to(device).float()\n\n                y_pred = net(x)\n                y_pred = y_pred.squeeze()\n\n                loss += F.binary_cross_entropy_with_logits(y_pred, y_true).item() * batch_size\n\n            total_examples += batch_size\n            pbar.update()\n\n    loss /= total_examples\n\n    if silent:\n        return loss\n    else:\n        print(\"loss: %.4f\" % (loss))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit(epochs):\n    global history, iteration, epochs_done, lr\n\n    with tqdm(total=len(train_loader), leave=False) as pbar:\n        for epoch in range(epochs):\n            pbar.reset()\n            pbar.set_description(\"Epoch %d\" % (epochs_done + 1))\n            \n            bce_loss = 0\n            total_examples = 0\n\n            net.train(True)\n\n            for batch_idx, data in enumerate(train_loader):\n                batch_size = data[0].shape[0]\n                x = data[0].to(gpu)\n                y_true = data[1].to(gpu).float()\n                \n                optimizer.zero_grad()\n\n                y_pred = net(x)\n                y_pred = y_pred.squeeze()\n                \n                loss = F.binary_cross_entropy_with_logits(y_pred, y_true)\n                loss.backward()\n                optimizer.step()\n                \n                batch_bce = loss.item()\n                bce_loss += batch_bce * batch_size\n                history[\"train_bce\"].append(batch_bce)\n\n                total_examples += batch_size\n                iteration += 1\n                pbar.update()\n\n            bce_loss /= total_examples\n            epochs_done += 1\n\n            print(\"Epoch: %3d, train BCE: %.4f\" % (epochs_done, bce_loss))\n\n            val_bce_loss = evaluate(net, val_loader, device=gpu, silent=True)\n            history[\"val_bce\"].append(val_bce_loss)\n            \n            print(\"              val BCE: %.4f\" % (val_bce_loss))\n\n            # TODO: can do LR annealing here\n            # TODO: can save checkpoint here\n\n            print(\"\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = torch.load(\"resnext50_32x4d-7cdf4587.pth\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torchvision.models as models\n\nclass MyResNeXt(models.resnet.ResNet):\n    def __init__(self, training=True):\n        super(MyResNeXt, self).__init__(block=models.resnet.Bottleneck,\n                                        layers=[3, 4, 6, 3], \n                                        groups=32, \n                                        width_per_group=4)\n\n        self.load_state_dict(checkpoint)\n\n        # Override the existing FC layer with a new one.\n        self.fc = nn.Linear(2048, 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"net = MyResNeXt().to(gpu)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# del checkpoint\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out = net(torch.zeros((10, 3, image_size, image_size)).to(gpu))\nout.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def freeze_until(net, param_name):\n    found_name = False\n    for name, params in net.named_parameters():\n        if name == param_name:\n            found_name = True\n        params.requires_grad = found_name\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[k for k, v in net.named_parameters()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freeze_until(net, \"layer4.0.conv1.weight\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[k for k,v in net.named_parameters() if v.requires_grad]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate(net, val_loader, device=gpu)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = 0.01\nwd = 0.\n\nhistory = { \"train_bce\": [], \"val_bce\": [] }\niteration = 0\nepochs_done = 0\n\noptimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=wd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fit(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(net.state_dict(), \"checkpoint.pth\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checkpoint = torch.load(\"checkpoint.pth\")\n# net.load_state_dict(checkpoint)\n\n# checkpoint = torch.load(\"optimizer-checkpoint.pth\")\n# optimizer.load_state_dict(checkpoint)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate(net, train_loader, device=gpu)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history[\"train_bce\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history[\"val_bce\"])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate(net, val_loader, device=gpu)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}