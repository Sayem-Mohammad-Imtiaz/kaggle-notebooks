{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering & Preprocessing"},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.100084Z","start_time":"2021-01-21T08:01:40.039509Z"},"trusted":true},"cell_type":"code","source":"import pickle\nimport re\nimport sys\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\n\nfrom scipy.stats import boxcox, yeojohnson\nimport nltk\nimport spacy\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction import text\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans, DBSCAN\n\n# Connect tqdm to pandas\ntqdm.pandas()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary of Variables Created\nIn this notebook, I created a significant number of additional features including:\n\n- Box-cox transformation of word count\n- Time variables based on publication date\n        - Day of week, day of month, hour, weekend\n        - Number of articles posted per day\n- Keywords\n        - Donald Trump\n        - Republican / Democrat news\n        - COVID-19\n- Headline / abstract length\n- Whether headline / abstract contains a question mark \n- Categorical features transformed into ordinal features based on average popularity (newsdesk, section, subsection, material)\n- Sentiment based on headline + abstract\n        - Positive, neutral, negative and compound sentiment"},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.104075Z","start_time":"2021-01-21T08:01:40.027Z"},"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/new-york-times-articles-comments-2020/train.csv', converters={'keywords': eval}, parse_dates=['pub_date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/new-york-times-articles-comments-2020/test.csv', converters={'keywords': eval}, parse_dates=['pub_date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().sum()[test.isnull().sum() > 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['keywords'] = test['keywords'].fillna('None')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()[train.isnull().sum() > 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['abstract'] = train['abstract'].fillna('')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word Count\nWe saw previously that the Boxcox transformation seems to work best, so we'll use that going forward. We'll also keep the original word count variable as removing it led to a drop in model accuracy."},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.108074Z","start_time":"2021-01-21T08:01:40.038Z"},"trusted":true},"cell_type":"code","source":"section_avg = train.groupby('section').mean()['word_count']","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.108074Z","start_time":"2021-01-21T08:01:40.041Z"},"trusted":true},"cell_type":"code","source":"train['boxcox_word'] = train['word_count'].apply(lambda x: 0.001 if x == 0 else x)\ntrain['boxcox_word'] = boxcox(train['boxcox_word'])[0]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.109074Z","start_time":"2021-01-21T08:01:40.043Z"},"trusted":true},"cell_type":"code","source":"test['boxcox_word'] = test['word_count'].apply(lambda x: 0.001 if x == 0 else x)\ntest['boxcox_word'] = boxcox(test['boxcox_word'])[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Time Variables"},{"metadata":{},"cell_type":"markdown","source":"Time affects the frequency of published articles, which correspondingly affects the popularity of articles. Articles published at a time where less articles are published are more likely to be more popular -- there's less places for commentators to go."},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.110074Z","start_time":"2021-01-21T08:01:40.048Z"},"trusted":true},"cell_type":"code","source":"train['day_of_month'] = train['pub_date'].apply(lambda x: x.day)\ntrain['day_of_week'] = train['pub_date'].apply(lambda x: x.dayofweek)\ntrain['hour'] = train['pub_date'].apply(lambda x: x.hour)\ntrain['is_weekend'] = train['day_of_week'].apply(lambda x: 1 if x==5 or x==6 else 0)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.110074Z","start_time":"2021-01-21T08:01:40.05Z"},"trusted":true},"cell_type":"code","source":"test['day_of_month'] = test['pub_date'].apply(lambda x: x.day)\ntest['day_of_week'] = test['pub_date'].apply(lambda x: x.dayofweek)\ntest['hour'] = test['pub_date'].apply(lambda x: x.hour)\ntest['is_weekend'] = test['day_of_week'].apply(lambda x: 1 if x==5 or x==6 else 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I also created an additional variable that tracks when an article was published. Articles published between 10PM and 2AM seem to have to have a much higher average popularity."},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.111074Z","start_time":"2021-01-21T08:01:40.052Z"},"trusted":true},"cell_type":"code","source":"train['is_primehour'] = train['hour'].apply(lambda x: 1 if x > 22 else 1 if x < 4 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.corr()['is_popular']['is_primehour']","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.111074Z","start_time":"2021-01-21T08:01:40.054Z"},"trusted":true},"cell_type":"code","source":"test['is_primehour'] = test['hour'].apply(lambda x: 1 if x > 22 else 1 if x < 4 else 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Articles Per Day"},{"metadata":{},"cell_type":"markdown","source":"Similar to our time variables, I created a variable that tracks the number of articles posted in a day. The idea is that the less articles there are, the higher the popularity and vice versa."},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.112074Z","start_time":"2021-01-21T08:01:40.056Z"},"trusted":true},"cell_type":"code","source":"train['group_date'] = train['pub_date'].astype(str).apply(lambda x: x[:10])\ngroup_dates = train['group_date'].value_counts()\ntrain['posts_per_day'] = train['group_date'].apply(lambda x: group_dates[x])","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.112074Z","start_time":"2021-01-21T08:01:40.058Z"},"trusted":true},"cell_type":"code","source":"# More posts in a day correlated with lower popularity\ntrain.corr()['is_popular'][['posts_per_day']]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.113075Z","start_time":"2021-01-21T08:01:40.061Z"},"trusted":true},"cell_type":"code","source":"test['group_date'] = test['pub_date'].astype(str).apply(lambda x: x[:10])\ngroup_dates = test['group_date'].value_counts()\ntest['posts_per_day'] = test['group_date'].apply(lambda x: group_dates[x])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Keywords"},{"metadata":{},"cell_type":"markdown","source":"Having a certain number of keywords seems important -- the ideal number of keywords seems to be between 11 and 16. This could suggest that people are more interested in articles that cover a range of topics, people and organizations."},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.113075Z","start_time":"2021-01-21T08:01:40.064Z"},"trusted":true},"cell_type":"code","source":"train['n_keywords'] = train['keywords'].apply(lambda x: len(x))\ntest['n_keywords'] = test['keywords'].apply(lambda x: len(x) if type(x) is list else 0)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.114075Z","start_time":"2021-01-21T08:01:40.066Z"},"trusted":true},"cell_type":"code","source":"train['ideal_n_keywords'] = train['n_keywords'].apply(lambda x: 1 if x == 1 else 1 if (x > 11 and x < 16) else 0)\ntest['ideal_n_keywords'] = test['n_keywords'].apply(lambda x: 1 if x == 1 else 1 if (x > 11 and x < 16) else 0)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.115076Z","start_time":"2021-01-21T08:01:40.069Z"},"scrolled":false,"trusted":true},"cell_type":"code","source":"train.corr()['is_popular'][['n_keywords', 'ideal_n_keywords']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Trump / Republican / Democrat"},{"metadata":{},"cell_type":"markdown","source":"We saw that Donald Trump and Republican/Democrat keywords are among the most frequent keywords, so we'll create a variable here to keep track of that. Both these features have a significant correlation with popularity."},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.115076Z","start_time":"2021-01-21T08:01:40.071Z"},"trusted":true},"cell_type":"code","source":"train['is_trump'] = train['keywords'].apply(lambda x: 1 if 'Trump, Donald J' in x else 0)\n\ntest['is_trump'] = test['keywords'].apply(lambda x: 0 if type(x) is not list \n                                          else (1 if 'Trump, Donald J' in x else 0))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.116075Z","start_time":"2021-01-21T08:01:40.073Z"},"trusted":true},"cell_type":"code","source":"train['is_party'] = train['keywords'].apply(lambda x: 0 if type(x) is not list \n                                          else (1 if 'Democratic Party' in x \n                                                else 1 if 'Republican Party' in x else 0))\n\ntest['is_party'] = test['keywords'].apply(lambda x: 0 if type(x) is not list \n                                          else (1 if 'Democratic Party' in x \n                                                else 1 if 'Republican Party' in x else 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.corr()['is_popular'][['is_trump', 'is_party']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Race & Ethnicity"},{"metadata":{},"cell_type":"markdown","source":"Race and ethnicity has always been a hot topic in the US, and especially so this year with the death of George Floyd. This has a slight correlation with article popularity."},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.116075Z","start_time":"2021-01-21T08:01:40.075Z"},"trusted":true},"cell_type":"code","source":"train['is_racial'] = train['keywords'].apply(lambda x: 0 if type(x) is not list \n                                          else (1 if 'Black People' in x \n                                                else 1 if 'Race and Ethnicity' in x \n                                                else 1 if 'Discrimination' in x\n                                                else 1 if 'Black Lives Matter Movement' in x\n                                                else 0))\n\ntest['is_racial'] = test['keywords'].apply(lambda x: 0 if type(x) is not list \n                                          else (1 if 'Black People' in x \n                                                else 1 if 'Race and Ethnicity' in x \n                                                else 1 if 'Discrimination' in x\n                                                else 1 if 'Black Lives Matter Movement' in x\n                                                else 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.corr()['is_popular'][['is_racial']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### COVID-19"},{"metadata":{},"cell_type":"markdown","source":"COVID-19 has drastically changed the world as we know it -- it was also the most frequent keyword in our entire dataset. All three features below have a faint correlation with popularity."},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.117076Z","start_time":"2021-01-21T08:01:40.078Z"},"trusted":true},"cell_type":"code","source":"train['is_covid'] = train['keywords'].apply(lambda x: 0 if type(x) is not list \n                                          else (1 if 'Coronavirus (2019-nCoV)' in x \\\n                                                else 1 if 'Coronavirus Risks and Safety Concerns' in x \n                                                else 0))\n\ntest['is_covid'] = test['keywords'].apply(lambda x: 0 if type(x) is not list \n                                          else (1 if 'Coronavirus (2019-nCoV)' in x \\\n                                                else 1 if 'Coronavirus Risks and Safety Concerns' in x else 0))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.118077Z","start_time":"2021-01-21T08:01:40.08Z"},"trusted":true},"cell_type":"code","source":"train['is_epidemic'] = train['keywords'].apply(lambda x: 0 if type(x) is not list \n                                          else (1 if 'Epidemics' in x else 0))\n\ntest['is_epidemic'] = test['keywords'].apply(lambda x: 0 if type(x) is not list \n                                          else (1 if 'Epidemics' in x else 0))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.118077Z","start_time":"2021-01-21T08:01:40.082Z"},"trusted":true},"cell_type":"code","source":"train['is_death'] = train['keywords'].apply(lambda x: 1 if 'Deaths (Fatalities)' in x else 0)\n\ntest['is_death'] = test['keywords'].apply(lambda x: 0 if type(x) is not list else 1 if 'Deaths (Fatalities)' in x else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.corr()['is_popular'][['is_covid', 'is_epidemic', 'is_death']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Question"},{"metadata":{},"cell_type":"markdown","source":"If the headline or abstract contains a question mark, there's a good chance that the article has been written in a way to invite commentary. Alternatively, people might view the question mark as a friendly invitation to comment."},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.119076Z","start_time":"2021-01-21T08:01:40.085Z"},"trusted":true},"cell_type":"code","source":"train['headline_question'] = train['headline'].apply(lambda x: 1 if '?' in x else 0)\ntest['headline_question'] = test['headline'].apply(lambda x: 1 if '?' in x else 0)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.120076Z","start_time":"2021-01-21T08:01:40.088Z"},"trusted":true},"cell_type":"code","source":"train[train['headline_question'] == 1]['is_popular'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.120076Z","start_time":"2021-01-21T08:01:40.09Z"},"trusted":true},"cell_type":"code","source":"train['abs_question'] = train['abstract'].apply(lambda x: 1 if '?' in x else 0)\ntest['abs_question'] = test['abstract'].apply(lambda x: 1 if '?' in x else 0)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.121077Z","start_time":"2021-01-21T08:01:40.091Z"},"trusted":true},"cell_type":"code","source":"train[train['abs_question'] == 1]['is_popular'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.corr()['is_popular'][['headline_question', 'abs_question']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Newsdesk / Section / Subsection / Material"},{"metadata":{},"cell_type":"markdown","source":"An article's newsdesk, section, subsection are likely the most powerful predictors of popularity. Opinion Editorials (OpEds) are much more likely to draw comments because they're likely written in a way to attract attention or controversy. These OpEds tackle recent events and issues, and attempt to formulate viewpoints based on an objective analysis of happenings and conflicting/contrary opinions. NYT Opinion pieces are also always open for comments, which would naturally increase the likelihood of having a popular article.\n\nIt was pretty difficult to decide on how to map / encode these features. There are 60 newsdesks, 42 sections, 67 subsections, and 10 types of material. Performing a one hot encoding on each variable would leave me with over 180 features in total.\n\nThere are several approaches that I tried:\n- One hot encode Newsdesk, Section and Subsection (this returned poor results)\n- Combine Newsdesk, Section and Subsection into a single `NewsType` variable \n    - For example: Foreign newsdesk, World section, Australia Subsection --> #Foreign#World#Australia.\n    - Group similar articles together e.g. #Foreign#World#Australia and #Foreign#World#Asia Pacific. This naturally presents some difficulty though -- how do we decide which sections and subsections to group together? The Australia subsection has more popular articles but only has 46 articles compared to the 327 articles in the Asia Pacific subsection.\n- Create an ordinal interaction feature (number of popular articles * total number of articles) that gives a higher weight to features that have many popular articles and a large number of total articles.\n- Use DBSCAN or other clustering methods to group variables together.\n\nUltimately, I found that <b>taking a simpler approach returned better results</b>. Below, I created a function that groups features according to their average popularity. In short, I created an ordinal feature that places more weight on newsdesks/sections/subsections/materials that have an average popularity of 0.6 and above. Conversely, I placed a low weight on variables that have a average popularity of 0.4 and below.\n\nTo catch unique newsdesks/sections/subsections/materials that are in the test but not in train dataset, I added in an `if` statement that maps these unique sections to 0. This should cause our model to treat them in a neutral way."},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.131079Z","start_time":"2021-01-21T08:01:40.123Z"},"trusted":true},"cell_type":"code","source":"# Combining newsdesks -- the different names reflect interactive articles that will be accounted for later\ntrain['newsdesk'] = train['newsdesk'].apply(lambda x: 'The Upshot' if x=='Upshot' else x)\ntrain['newsdesk'] = train['newsdesk'].apply(lambda x: 'OpEd' if x=='Opinion' else x)\ntrain['newsdesk'] = train['newsdesk'].apply(lambda x: 'AtHome' if x=='At Home' else x)\n\ntest['newsdesk'] = test['newsdesk'].apply(lambda x: 'The Upshot' if x=='Upshot' else x)\ntest['newsdesk'] = test['newsdesk'].apply(lambda x: 'OpEd' if x=='Opinion' else x)\ntest['newsdesk'] = test['newsdesk'].apply(lambda x: 'AtHome' if x=='At Home' else x)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.130079Z","start_time":"2021-01-21T08:01:40.121Z"},"trusted":true},"cell_type":"code","source":"# We have to fill the null values in our subsection\ntrain['subsection'].fillna('N/A', inplace=True)\ntest['subsection'].fillna('N/A', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def map_popularity(col):\n    df = train.groupby(f'{col}').mean().reset_index().sort_values(by='is_popular', ascending=False) \\\n              [[f'{col}', 'is_popular']]\n    df.columns=[f'{col}', 'avg_popularity']\n    \n    pop_5 = df[df['avg_popularity'] >= 0.7][f'{col}'].values\n    pop_4 = df[(df['avg_popularity'] < 0.7) & (df['avg_popularity'] >= 0.6)][f'{col}'].values\n    pop_3 = df[(df['avg_popularity'] < 0.6) & (df['avg_popularity'] >= 0.5)][f'{col}'].values\n    pop_2 = df[(df['avg_popularity'] < 0.5) & (df['avg_popularity'] >= 0.4)][f'{col}'].values\n    pop_1 = df[(df['avg_popularity'] < 0.4) & (df['avg_popularity'] >= 0.3)][f'{col}'].values\n    pop_0 = df[df['avg_popularity'] < 0.3][f'{col}'].values\n    \n    def lambda_fxn(x):\n        if x in pop_5:\n            return 5\n        elif x in pop_4:\n            return 4\n        elif x in pop_3:\n            return 3\n        elif x in pop_2:\n            return 2\n        elif x in pop_1:\n            return 1\n        elif x in pop_0:\n            return -1\n        \n        # To catch news desks/sections/subsections/material in test but not in train\n        else:\n            return 0\n    \n    train[f'{col}_pop'] = train[f'{col}'].apply(lambda_fxn)\n    test[f'{col}_pop'] = test[f'{col}'].apply(lambda_fxn)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.13208Z","start_time":"2021-01-21T08:01:40.129Z"},"trusted":true},"cell_type":"code","source":"map_popularity('newsdesk')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.13308Z","start_time":"2021-01-21T08:01:40.132Z"},"trusted":true},"cell_type":"code","source":"map_popularity('section')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.13308Z","start_time":"2021-01-21T08:01:40.134Z"},"trusted":true},"cell_type":"code","source":"map_popularity('subsection')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"map_popularity('material')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.134079Z","start_time":"2021-01-21T08:01:40.137Z"},"trusted":true},"cell_type":"code","source":"train.loc[0][['headline', 'newsdesk', 'newsdesk_pop', 'section', 'section_pop', 'subsection',\n              'subsection_pop', 'material', 'material_pop']].to_frame().T","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.13508Z","start_time":"2021-01-21T08:01:40.139Z"},"scrolled":true,"trusted":true},"cell_type":"code","source":"train.loc[101][['headline', 'newsdesk', 'newsdesk_pop', 'section', 'section_pop', 'subsection', \n                'subsection_pop', 'material', 'material_pop']].to_frame().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Other Features"},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.121077Z","start_time":"2021-01-21T08:01:40.094Z"},"trusted":true},"cell_type":"code","source":"train['combi_text'] = train['headline'] + '. ' + train['abstract']\ntrain['combi_text2'] = train['combi_text'].str.replace(r'[\\!?.]+[\\.]+','.', regex=True) # remove extra punctuation in headline\n\ntest['combi_text'] = test['headline'] + '. ' + test['abstract']\ntest['combi_text2'] = test['combi_text'].str.replace(r'[\\!?.]+[\\.]+','.', regex=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sentiment"},{"metadata":{},"cell_type":"markdown","source":"Sentiment plays a notable role in determining popularity. People are more likely to comment on articles with headlines that have negative sentiment, and less likely to comment on articles with headlines that have neutral sentiment. Previous [research](https://jonahberger.com/wp-content/uploads/2013/02/ViralityB.pdf) has shown that content that evokes high-arousal positive (awe) or negative (anger or anxiety) emotions tends to be more viral."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['combi_text'][0]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.123077Z","start_time":"2021-01-21T08:01:40.098Z"},"scrolled":true,"trusted":true},"cell_type":"code","source":"# Instantiating sentiment intensity analyzer\nsia = SIA()\nsia.polarity_scores(train['combi_text'][0])","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.123077Z","start_time":"2021-01-21T08:01:40.1Z"},"trusted":true},"cell_type":"code","source":"def get_sentiment(row):\n    sentiment_dict = sia.polarity_scores(row['combi_text'])\n    row['sentiment_pos'] = sentiment_dict['pos']\n    row['sentiment_neu'] = sentiment_dict['neu']\n    row['sentiment_neg'] = sentiment_dict['neg']\n    row['sentiment_compound'] = sentiment_dict['compound']\n    return row","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.124077Z","start_time":"2021-01-21T08:01:40.102Z"},"trusted":true},"cell_type":"code","source":"train = train.progress_apply(get_sentiment, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.125078Z","start_time":"2021-01-21T08:01:40.104Z"},"trusted":true},"cell_type":"code","source":"# Looks like negative articles tend to be more popular\ntrain.corr()['is_popular'][['sentiment_compound', 'sentiment_pos', 'sentiment_neu', 'sentiment_neg']]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.125078Z","start_time":"2021-01-21T08:01:40.106Z"},"trusted":true},"cell_type":"code","source":"test = test.progress_apply(get_sentiment, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Headline / Abstract Length"},{"metadata":{},"cell_type":"markdown","source":"The idea here is that longer headlines and abstracts will lead to less comments -- the easier the headline / abstract is to understand, the more comments the article will attract. We can see this seems to be a factor for the abstract, while headline length doesn't seem to have much of an impact."},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.126078Z","start_time":"2021-01-21T08:01:40.108Z"},"trusted":true},"cell_type":"code","source":"train['headline_len'] = train['headline'].apply(lambda x: len(x))\ntrain['abstract_len'] = train['abstract'].apply(lambda x: len(x))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.126078Z","start_time":"2021-01-21T08:01:40.111Z"},"trusted":true},"cell_type":"code","source":"test['headline_len'] = test['headline'].apply(lambda x: len(x))\ntest['abstract_len'] = test['abstract'].apply(lambda x: len(x))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.127078Z","start_time":"2021-01-21T08:01:40.113Z"},"trusted":true},"cell_type":"code","source":"train['head_abs_len'] = train['headline_len'] + train['abstract_len']\ntest['head_abs_len'] = test['headline_len'] + test['abstract_len']","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.128078Z","start_time":"2021-01-21T08:01:40.117Z"},"scrolled":true,"trusted":true},"cell_type":"code","source":"# Shorter abstracts seem to do better\ntrain.corr()['is_popular'].sort_values(ascending=False)[['abstract_len', 'headline_len']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Interactive Features"},{"metadata":{},"cell_type":"markdown","source":"There are only a few interactive features, but generally I found that including this feature increased my model accuracy."},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.13508Z","start_time":"2021-01-21T08:01:40.141Z"},"trusted":true},"cell_type":"code","source":"train['is_interactive'] = train['material'].apply(lambda x: 1 if x == 'Interactive Feature' else 0)\ntest['is_interactive'] = test['material'].apply(lambda x: 1 if x == 'Interactive Feature' else 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Clustering"},{"metadata":{},"cell_type":"markdown","source":"I also tried out various clustering methods on my data to see if there was a more efficient way of clustering by newsdesk or section, or by headline. Generally, I found that clustering didn't help my model accuracy much. The headlines also seemed clustered pretty close together, which made it difficult for DBSCAN effectively separate them. Using K-means clustering seemed to work slightly better, but the topics ended up looking pretty similar. I excluded clustering from my final model, but it's worth noting that some clusters had a moderate correlation with popularity.\n\nCredit to [Brandon Rose](http://brandonrose.org/clustering) for this part."},{"metadata":{"trusted":true},"cell_type":"code","source":"full_df = pd.concat([train, test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_df['newsdesk'].nunique(), full_df['section'].nunique(), full_df['subsection'].nunique(), full_df['material'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extra_stopwords = [\"ain't\", \"amn't\", \"aren't\", \"can't\", \"could've\", \"couldn't\",\n                    \"daresn't\", \"didn't\", \"doesn't\", \"don't\", \"gonna\", \"gotta\", \n                    \"hadn't\", \"hasn't\", \"haven't\", \"he'd\", \"he'll\", \"he's\", \"how'd\",\n                    \"how'll\", \"how's\", \"I'd\", \"I'll\", \"I'm\", \"I've\", \"isn't\", \"it'd\",\n                    \"it'll\", \"it's\", \"let's\", \"mayn't\", \"may've\", \"mightn't\", \n                    \"might've\", \"mustn't\", \"must've\", \"needn't\", \"o'clock\", \"ol'\",\n                    \"oughtn't\", \"shan't\", \"she'd\", \"she'll\", \"she's\", \"should've\",\n                    \"shouldn't\", \"somebody's\", \"someone's\", \"something's\", \"that'll\",\n                    \"that're\", \"that's\", \"that'd\", \"there'd\", \"there're\", \"there's\", \n                    \"these're\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this's\",\n                    \"those're\", \"tis\", \"twas\", \"twasn't\", \"wasn't\", \"we'd\", \"we'd've\",\n                    \"we'll\", \"we're\", \"we've\", \"weren't\", \"what'd\", \"what'll\", \n                    \"what're\", \"what's\", \"what've\", \"when's\", \"where'd\", \"where're\",\n                    \"where's\", \"where've\", \"which's\", \"who'd\", \"who'd've\", \"who'll\",\n                    \"who're\", \"who's\", \"who've\", \"why'd\", \"why're\", \"why's\", \"won't\",\n                    \"would've\", \"wouldn't\", \"y'all\", \"you'd\", \"you'll\", \"you're\", \n                    \"you've\", \"'s\", \"'d\", \"'m\", \"abov\", \"afterward\", \"ai\", \"alon\", \"alreadi\", \"alway\", \"ani\", \n                     \"anoth\", \"anyon\", \"anyth\", \"anywher\", \"becam\", \"becaus\", \"becom\", \"befor\", \n                     \"besid\", \"ca\", \"cri\", \"dare\", \"describ\", \"did\", \"doe\", \"dure\", \"els\", \n                     \"elsewher\", \"empti\", \"everi\", \"everyon\", \"everyth\", \"everywher\", \"fifti\", \n                     \"forti\", \"gon\", \"got\", \"henc\", \"hereaft\", \"herebi\", \"howev\", \"hundr\", \"inde\", \n                     \"let\", \"ll\", \"mani\", \"meanwhil\", \"moreov\", \"n't\", \"na\", \"need\", \"nobodi\", \"noon\", \n                     \"noth\", \"nowher\", \"ol\", \"onc\", \"onli\", \"otherwis\", \"ought\", \"ourselv\", \"perhap\", \n                     \"pleas\", \"sever\", \"sha\", \"sinc\", \"sincer\", \"sixti\", \"somebodi\", \"someon\", \"someth\", \n                     \"sometim\", \"somewher\", \"ta\", \"themselv\", \"thenc\", \"thereaft\", \"therebi\", \"therefor\", \n                     \"togeth\", \"twelv\", \"twenti\", \"ve\", \"veri\", \"whatev\", \"whenc\", \"whenev\", \n                    \"wherea\", \"whereaft\", \"wherebi\", \"wherev\", \"whi\", \"wo\", \"anywh\", \"el\", \"elsewh\", \"everywh\", \n                    \"ind\", \"otherwi\", \"plea\", \"somewh\", \"yourselv\"]\n\ncustom_stopwords = text.ENGLISH_STOP_WORDS.union(extra_stopwords)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.13608Z","start_time":"2021-01-21T08:01:40.144Z"},"trusted":true},"cell_type":"code","source":"stemmer = SnowballStemmer(\"english\")","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.137081Z","start_time":"2021-01-21T08:01:40.146Z"},"trusted":true},"cell_type":"code","source":"def tokenize_and_stem(text, do_stem=True):\n    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n    \n    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n    filtered_tokens = []\n    for token in tokens:\n        if re.search('[a-zA-Z]', token):\n            filtered_tokens.append(token)\n            \n    # stem filtered tokens\n    stems = [stemmer.stem(t) for t in filtered_tokens]\n    \n    if do_stem:\n        return stems\n    else:\n        return filtered_tokens","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.13808Z","start_time":"2021-01-21T08:01:40.148Z"},"trusted":true},"cell_type":"code","source":"# not super pythonic, no, not at all.\n# use extend so it's a big flat list of vocab\ntotalvocab_stemmed = []\ntotalvocab_tokenized = []\nfor i in tqdm(full_df['combi_text2']):\n    allwords_stemmed = tokenize_and_stem(i)\n    totalvocab_stemmed.extend(allwords_stemmed)\n    \n    allwords_tokenized = tokenize_and_stem(i, False)\n    totalvocab_tokenized.extend(allwords_tokenized)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.13808Z","start_time":"2021-01-21T08:01:40.15Z"},"trusted":true},"cell_type":"code","source":"vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\nvocab_frame.head()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.139081Z","start_time":"2021-01-21T08:01:40.152Z"},"trusted":true},"cell_type":"code","source":"# Define vectorizer parameters\ntfidf_vectorizer = TfidfVectorizer(max_features=20_000,\n                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,2), \n                                   min_df=0.01, stop_words=extra_stopwords)\n\ntfidf_matrix = tfidf_vectorizer.fit_transform(full_df['combi_text2']) #fit the vectorizer to synopses\n\nprint(tfidf_matrix.shape)\n\nterms = tfidf_vectorizer.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.140081Z","start_time":"2021-01-21T08:01:40.154Z"},"trusted":true},"cell_type":"code","source":"num_clusters = 7\ncluster_model = KMeans(n_clusters=num_clusters, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.140081Z","start_time":"2021-01-21T08:01:40.157Z"},"trusted":true},"cell_type":"code","source":"cluster_model.fit(tfidf_matrix)\nclusters = cluster_model.labels_.tolist()\nfull_df['cluster'] = clusters\nfull_df.head()[['headline', 'cluster']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_df['cluster'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"print(\"Top terms per cluster:\")\nprint()\n\n# Sort cluster centers by proximity to centroid\norder_centroids = cluster_model.cluster_centers_.argsort()[:, ::-1] \n\nfor i in range(num_clusters):\n    print(\"Cluster %d words:\" % i, end='')\n\n    for ind in order_centroids[i, :6]: #replace 6 with n words per cluster\n        print(' %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0], end=',')\n    print('\\n')\n\n    print(\"Cluster %d titles:\" % i, end='')\n    print()\n    for title in full_df[full_df['cluster'] == i]['headline'].values.tolist()[:8]:\n        print(' - %s' % title)\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.141081Z","start_time":"2021-01-21T08:01:40.161Z"},"trusted":true},"cell_type":"code","source":"cluster_df = pd.get_dummies(data=full_df, columns=['cluster'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only Cluster 0 has a moderate correlation with popularity -- however we're already accounting for political news with our previous features."},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.142081Z","start_time":"2021-01-21T08:01:40.163Z"},"trusted":true},"cell_type":"code","source":"cluster_df.corr()['is_popular'].sort_values(ascending=False).filter(like='cluster')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Process Test"},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.144082Z","start_time":"2021-01-21T08:01:40.172Z"},"trusted":true},"cell_type":"code","source":"[i for i in train if i not in test]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.145082Z","start_time":"2021-01-21T08:01:40.175Z"},"trusted":true},"cell_type":"code","source":"[i for i in test if i not in train]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.146082Z","start_time":"2021-01-21T08:01:40.177Z"},"trusted":true},"cell_type":"code","source":"train.isnull().sum()[train.isnull().sum() > 0]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.146082Z","start_time":"2021-01-21T08:01:40.179Z"},"trusted":true},"cell_type":"code","source":"test.isnull().sum()[test.isnull().sum() > 0]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.147082Z","start_time":"2021-01-21T08:01:40.182Z"},"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(13,13))\nsns.heatmap(train.corr(), cmap='coolwarm', annot=False, square=True, fmt='.2f', cbar=True)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.150083Z","start_time":"2021-01-21T08:01:40.19Z"},"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,12))\nsns.heatmap(train.corr()[['is_popular']].sort_values(ascending=False, by='is_popular'), \n            cmap='coolwarm', annot=True, vmax=0.8)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.150083Z","start_time":"2021-01-21T08:01:40.191Z"},"trusted":true},"cell_type":"code","source":"train.to_csv('/kaggle/working/train_processed.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2021-01-21T08:01:43.151083Z","start_time":"2021-01-21T08:01:40.194Z"},"trusted":true},"cell_type":"code","source":"test.to_csv('/kaggle/working/test_processed.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}