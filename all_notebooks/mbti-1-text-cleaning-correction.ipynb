{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt # visualize data\n\nfrom sklearn.pipeline import Pipeline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-14T13:32:21.433702Z","iopub.execute_input":"2021-07-14T13:32:21.4341Z","iopub.status.idle":"2021-07-14T13:32:22.544119Z","shell.execute_reply.started":"2021-07-14T13:32:21.434062Z","shell.execute_reply":"2021-07-14T13:32:22.542939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Introduction</h1>\nThis notebook is presenting an approach to solve a text-classification problem with machine learning techniques. The solution is including three steps - data understanding, text pre-processing and classification.<br>\n\nThe purpose is to train a model with our clean text dataset, that will be able to make predictions.","metadata":{}},{"cell_type":"code","source":"## Spoiler Alert!\n## File Configurations\ndata_reduction = False\nskip_early_classification = True\nskip_initial_posts_classification = True\nskip_first_cleaning = True\nskip_clean_posts_classification = True\nskip_second_clean_posts_classification = True\nskip_lemma_posts_classification = True\nskip_lemmatazation = True\n\nsave_data_to_files = False\n\n\n# TPOT configurations\ngenerations = 5\npopulation_size = 5\nconfig_dict='TPOT sparse'\nverbosity = 1\nmemory='auto'","metadata":{"execution":{"iopub.status.busy":"2021-07-14T13:32:22.547393Z","iopub.execute_input":"2021-07-14T13:32:22.547759Z","iopub.status.idle":"2021-07-14T13:32:22.556462Z","shell.execute_reply.started":"2021-07-14T13:32:22.547725Z","shell.execute_reply":"2021-07-14T13:32:22.555362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2><a id = Test></a>Data</h2>","metadata":{}},{"cell_type":"markdown","source":"By the dataset's description we know the following:\n<ul>\n    <li>This data was collected through the <a href=\"http://personalitycafe.com/\">PersonalityCafe</a> forum, as it provides a large selection of people and their MBTI personality type, as well as what they have written.</li>\n    <li>The dataset is consist of : \n        <ul>\n        <li> 2 categorical columns (<code>type</code> and <code>posts</code>) and</li>  \n        <li> 8675 rows, each row represents a user.</li>\n        </ul>\n    </li>  \n</ul>\nSo, each user has a personality <code>type</code> and has posted some <code>posts</code>.<br>","metadata":{}},{"cell_type":"code","source":"dataset=pd.read_csv(\"/kaggle/input/mbti-type/mbti_1.csv\")\ndataset.head(2)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-07-14T13:32:23.606993Z","iopub.execute_input":"2021-07-14T13:32:23.607352Z","iopub.status.idle":"2021-07-14T13:32:25.031481Z","shell.execute_reply.started":"2021-07-14T13:32:23.607318Z","shell.execute_reply":"2021-07-14T13:32:25.030726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-14T13:32:25.032894Z","iopub.execute_input":"2021-07-14T13:32:25.033325Z","iopub.status.idle":"2021-07-14T13:32:25.13484Z","shell.execute_reply.started":"2021-07-14T13:32:25.033293Z","shell.execute_reply":"2021-07-14T13:32:25.133943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's use an module called <code>pandas-profiling</code> to extract some basic insights.","metadata":{}},{"cell_type":"code","source":"from pandas_profiling import ProfileReport","metadata":{"execution":{"iopub.status.busy":"2021-07-12T13:19:01.975774Z","iopub.execute_input":"2021-07-12T13:19:01.976152Z","iopub.status.idle":"2021-07-12T13:19:01.980821Z","shell.execute_reply.started":"2021-07-12T13:19:01.976116Z","shell.execute_reply":"2021-07-12T13:19:01.97973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"profile = ProfileReport(dataset)\nprofile.to_notebook_iframe()","metadata":{"execution":{"iopub.status.busy":"2021-07-12T13:19:02.235874Z","iopub.execute_input":"2021-07-12T13:19:02.236398Z","iopub.status.idle":"2021-07-12T13:19:09.223993Z","shell.execute_reply.started":"2021-07-12T13:19:02.236363Z","shell.execute_reply":"2021-07-12T13:19:09.222768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have the following insights:\n<ul>\n    <li> <code>type</code> column is our target column and it has 16 unique values/classes.</li>\n    <li> there are minority classes and majority classes, so the dataset is imbalanced.</li>\n    <li> There are no missing values</li>\n    <li> There are links that we have to process</li>\n    <li> There are <code>|||</code> that seperate posts in each line.</li>\n    <li> There may be lines with no latin words.</li>\n    \n</ul>","metadata":{}},{"cell_type":"markdown","source":"In the end of the day, we have to solve a classification problem.  I want to know if we train a model with the initial texts, what the accuracy will be. Next, we will do some text-cleanning and then we will be able to compare new models' accuracy with the initial one.<br>\nIt may be too early for this, but let's see what the accuracy of a simple model is.<br>\nObviously, the classification's features are the <code>posts</code>'s text, and target column is the <code>type</code> column.<br>\nSad to say, models can train only with numeric values, so we have to convert both posts and type into numbers. We will transform them with diferrent ways.","metadata":{}},{"cell_type":"markdown","source":"Due to imbalance problem and cpu runtime limit, I remove some instances of majority classes. In the end, we will have no more than 100 instances per class.","metadata":{}},{"cell_type":"code","source":"if data_reduction:\n    data = pd.DataFrame(columns=dataset.columns)\n    data_len =100\n    for personality in dataset[\"type\"].unique():\n        data=pd.concat(  [ data,dataset.loc[dataset['type']==personality][0 : data_len]])\nelse:\n    data=dataset","metadata":{"execution":{"iopub.status.busy":"2021-07-14T13:34:38.070615Z","iopub.execute_input":"2021-07-14T13:34:38.071267Z","iopub.status.idle":"2021-07-14T13:34:38.135566Z","shell.execute_reply.started":"2021-07-14T13:34:38.071218Z","shell.execute_reply":"2021-07-14T13:34:38.134049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"profile = ProfileReport(data)\nprofile.to_notebook_iframe()","metadata":{"execution":{"iopub.status.busy":"2021-07-12T13:19:09.282824Z","iopub.execute_input":"2021-07-12T13:19:09.283356Z","iopub.status.idle":"2021-07-12T13:19:14.565073Z","shell.execute_reply.started":"2021-07-12T13:19:09.283318Z","shell.execute_reply":"2021-07-12T13:19:14.564007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that dataset is more balanced we can continue.","metadata":{}},{"cell_type":"markdown","source":"<h3>Text column's transformation.</h3>\nThere are many ways to do this, I choose TfidfVectorizer from scikit-learn, because it is fast and it tends to improve models' accuracy.\nTfidfVectorizer will transform words to numbers. For each person/row a new row will be showing how frequently each word is used. Finaly a document-word_frequency matrix will be yield.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"execution":{"iopub.status.busy":"2021-07-14T13:32:55.920033Z","iopub.execute_input":"2021-07-14T13:32:55.920395Z","iopub.status.idle":"2021-07-14T13:32:55.986824Z","shell.execute_reply.started":"2021-07-14T13:32:55.920365Z","shell.execute_reply":"2021-07-14T13:32:55.985203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf = TfidfVectorizer(stop_words='english')\nX = tfidf.fit_transform(data['posts'])\nprint(\"The ducument-term matrix has {} rows (documents) and {} columns (total words).\".format(data.shape[0],len(tfidf.get_feature_names())))","metadata":{"execution":{"iopub.status.busy":"2021-07-14T13:34:42.481048Z","iopub.execute_input":"2021-07-14T13:34:42.481469Z","iopub.status.idle":"2021-07-14T13:34:44.613519Z","shell.execute_reply.started":"2021-07-14T13:34:42.481434Z","shell.execute_reply":"2021-07-14T13:34:44.61231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What did <code>TfidfVectorizer</code> return is the following.","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(X.toarray(),index=data.index,columns=tfidf.get_feature_names())","metadata":{"execution":{"iopub.status.busy":"2021-07-08T16:01:54.448909Z","iopub.execute_input":"2021-07-08T16:01:54.449222Z","iopub.status.idle":"2021-07-08T16:02:23.30299Z","shell.execute_reply.started":"2021-07-08T16:01:54.44919Z","shell.execute_reply":"2021-07-08T16:02:23.301726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That's a lot of columns. Let's try to remove words with less than 2 instances throughout all the dataset.","metadata":{}},{"cell_type":"code","source":"tfidf2 = TfidfVectorizer(stop_words='english',min_df=2)\ntfidf2.fit(data['posts'])\n\nprint(\"The ducument-term matrix has {} rows (documents) and {} columns (total words).\".format(data.shape[0],len(tfidf2.get_feature_names())))","metadata":{"execution":{"iopub.status.busy":"2021-07-12T13:32:19.360615Z","iopub.execute_input":"2021-07-12T13:32:19.36096Z","iopub.status.idle":"2021-07-12T13:32:21.32313Z","shell.execute_reply.started":"2021-07-12T13:32:19.360929Z","shell.execute_reply":"2021-07-12T13:32:21.322372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Target column transformation</h3>\nWe will transform personality type column into a new one num_type.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder","metadata":{"execution":{"iopub.status.busy":"2021-07-14T13:34:46.455152Z","iopub.execute_input":"2021-07-14T13:34:46.455835Z","iopub.status.idle":"2021-07-14T13:34:46.460257Z","shell.execute_reply.started":"2021-07-14T13:34:46.455795Z","shell.execute_reply":"2021-07-14T13:34:46.459111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type_le = LabelEncoder()\ndata['num_type']=type_le.fit_transform(data['type'])\ny=data['num_type']","metadata":{"execution":{"iopub.status.busy":"2021-07-14T13:34:46.597769Z","iopub.execute_input":"2021-07-14T13:34:46.598425Z","iopub.status.idle":"2021-07-14T13:34:46.605191Z","shell.execute_reply.started":"2021-07-14T13:34:46.598386Z","shell.execute_reply":"2021-07-14T13:34:46.60422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>First classification try.</h1>","metadata":{}},{"cell_type":"markdown","source":"We will try to messure the accuracy of some models' predictions. In order to do this, we will use TPOT module. ","metadata":{}},{"cell_type":"markdown","source":"Consider TPOT your Data Science Assistant. TPOT is a Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming. More info http://epistasislab.github.io/tpot/ <br>\nTPOTClassifier is what we need. At the moment we do not have to figure out the best model, just a simple accuracy number.\nIn addition to this, dataset is very large and limited RAM is an issue.\nSo, <code>TPOTClassifier</code> with TPOT_sparce configuration is the best for us. <code>TPOT_sparce</code> is a list of preproccesors and estimators that run fast on sparce matrices.\n","metadata":{}},{"cell_type":"markdown","source":"These are the classifiers and transformers TPOTClassifier is able to use (on 'sparce' mode).","metadata":{}},{"cell_type":"code","source":"from tpot import TPOTClassifier\n\nfrom tpot.config.classifier_sparse import  classifier_config_sparse\n[estimator for estimator in classifier_config_sparse ]","metadata":{"execution":{"iopub.status.busy":"2021-07-12T13:20:33.449593Z","iopub.execute_input":"2021-07-12T13:20:33.450041Z","iopub.status.idle":"2021-07-12T13:20:34.844437Z","shell.execute_reply.started":"2021-07-12T13:20:33.450005Z","shell.execute_reply":"2021-07-12T13:20:34.8435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nIn order to split the dataset we will use <code>train_test_split</code> and we will keep 10% for test.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nfrom sklearn.feature_selection import SelectFwe\nfrom sklearn.naive_bayes import MultinomialNB","metadata":{"execution":{"iopub.status.busy":"2021-07-14T13:33:02.26646Z","iopub.execute_input":"2021-07-14T13:33:02.266862Z","iopub.status.idle":"2021-07-14T13:33:02.579158Z","shell.execute_reply.started":"2021-07-14T13:33:02.266828Z","shell.execute_reply":"2021-07-14T13:33:02.578018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.1, random_state=42,shuffle=True)\n\n  \nif skip_early_classification :\n    tpot = Pipeline([('selectfwe', SelectFwe(alpha=0.006)), ('multinomialnb', MultinomialNB(alpha=0.01, fit_prior=False))])\n    tpot.fit(X_train ,y_train)\n    steps = tpot.steps\nelse:\n    tpot = TPOTClassifier(generations=generations,verbosity=verbosity,population_size=population_size,\n                      config_dict=config_dict,n_jobs=-1,random_state=42)\n    tpot.fit(X_train ,y_train)\n    steps = tpot.fitted_pipeline_.steps\n\npure_dataset_score  = tpot.score(X_test,y_test)*100\nprint(\"First classification's accuracy is {:.2f} % and model's steps are: {}\".format(pure_dataset_score,steps))","metadata":{"execution":{"iopub.status.busy":"2021-07-12T13:33:16.380631Z","iopub.execute_input":"2021-07-12T13:33:16.381086Z","iopub.status.idle":"2021-07-12T13:33:16.488251Z","shell.execute_reply.started":"2021-07-12T13:33:16.381049Z","shell.execute_reply":"2021-07-12T13:33:16.486234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Now we can take a closer look at our dataset.</h3>","metadata":{}},{"cell_type":"markdown","source":"<code>posts</code> column contains posts of each person. Each cell is made up of posts separated by <code>|||</code>. How many posts does each line contains?","metadata":{}},{"cell_type":"code","source":"data['#_posts'] = data['posts'].apply(lambda x : len(x.split(\"|||\")))\ndisplay(data['#_posts'].describe())","metadata":{"execution":{"iopub.status.busy":"2021-07-12T13:33:21.608837Z","iopub.execute_input":"2021-07-12T13:33:21.609239Z","iopub.status.idle":"2021-07-12T13:33:21.636186Z","shell.execute_reply.started":"2021-07-12T13:33:21.60919Z","shell.execute_reply":"2021-07-12T13:33:21.634924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"There are {} rows with less than 50 posts. This is the {:.2f} % of the dataset.\\n\".format( len(data[data['#_posts']<50]),len(data[data['#_posts']<50])/data.shape[0]*100 ) )\nprint(\"There are {} rows with less than 40 posts. This is the {:.2f} % of the dataset.\\n\".format( len(data[data['#_posts']<40]),len(data[data['#_posts']<40])/data.shape[0]*100 ) )\ndata[data['#_posts']<50]['#_posts'].sort_values().plot(kind='bar',title=\"Less than 50\",figsize=(15,4)) ; plt.show()\n\nprint(\"\\nThere are {} rows with more than 50 posts. This is the {:.2f} % of the dataset.\\n\".format( len(data[data['#_posts']>50]),len(data[data['#_posts']>50])/data.shape[0]*100 ) )","metadata":{"execution":{"iopub.status.busy":"2021-07-08T16:11:54.121114Z","iopub.execute_input":"2021-07-08T16:11:54.121662Z","iopub.status.idle":"2021-07-08T16:11:54.528589Z","shell.execute_reply.started":"2021-07-08T16:11:54.121619Z","shell.execute_reply":"2021-07-08T16:11:54.527549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.groupby('#_posts').sum().plot(kind='bar',title=\"#_posts variation\",figsize=(15,4)) ; plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T16:11:54.530604Z","iopub.execute_input":"2021-07-08T16:11:54.531056Z","iopub.status.idle":"2021-07-08T16:11:54.891532Z","shell.execute_reply.started":"2021-07-08T16:11:54.531014Z","shell.execute_reply":"2021-07-08T16:11:54.890465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What is beneath of 50 ?","metadata":{}},{"cell_type":"code","source":"tmp = data[data['#_posts']<50].groupby('type').count()[['#_posts']].join( data.groupby('type').count()[['#_posts']],lsuffix=\"<50\",rsuffix='_total' )\ntmp['Contain (%)'] = round(tmp['#_posts<50'] / tmp['#_posts_total'] *100)\ntmp.sort_values('Contain (%)',ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-08T16:11:54.893529Z","iopub.execute_input":"2021-07-08T16:11:54.893918Z","iopub.status.idle":"2021-07-08T16:11:54.926524Z","shell.execute_reply.started":"2021-07-08T16:11:54.89388Z","shell.execute_reply":"2021-07-08T16:11:54.925118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above we see that lines with less than 50 posts compose more than 10% of most classes. So we can not assume them as noise and we can not remove them.\n<br>So, most of the lines contains 50 posts.<br>","metadata":{}},{"cell_type":"markdown","source":"Let's see if there is any **correlation** beetwen number of posts and personality post.","metadata":{}},{"cell_type":"code","source":"data.groupby('type').median()[['#_posts']].join(data.groupby('type').mean()[['#_posts']],lsuffix='_median',rsuffix='_mean')","metadata":{"execution":{"iopub.status.busy":"2021-07-08T16:11:54.928859Z","iopub.execute_input":"2021-07-08T16:11:54.929354Z","iopub.status.idle":"2021-07-08T16:11:54.958309Z","shell.execute_reply.started":"2021-07-08T16:11:54.929302Z","shell.execute_reply":"2021-07-08T16:11:54.957218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.corrcoef(data['#_posts'],data['num_type'])[0][1]","metadata":{"execution":{"iopub.status.busy":"2021-07-08T16:11:54.95993Z","iopub.execute_input":"2021-07-08T16:11:54.960299Z","iopub.status.idle":"2021-07-08T16:11:54.971112Z","shell.execute_reply.started":"2021-07-08T16:11:54.960262Z","shell.execute_reply":"2021-07-08T16:11:54.969791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can not consider that there is any correlation.","metadata":{}},{"cell_type":"code","source":"data['posts']","metadata":{"execution":{"iopub.status.busy":"2021-07-08T16:11:54.972432Z","iopub.execute_input":"2021-07-08T16:11:54.973047Z","iopub.status.idle":"2021-07-08T16:11:54.983345Z","shell.execute_reply.started":"2021-07-08T16:11:54.973009Z","shell.execute_reply":"2021-07-08T16:11:54.982418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If we want to do some classification, we should remove content like urls, contractions and numbers. Also, capitals letters will be tranformed into lower and characters that are repeated more than two times in a row, we will keep only the first two of them (e.x. \"aaaaaand\" will be \"aand\").","metadata":{}},{"cell_type":"markdown","source":"<h1> Text Cleaning</h1>","metadata":{}},{"cell_type":"markdown","source":"Again, we check how many words are included in our dataset.","metadata":{}},{"cell_type":"code","source":"voc = tfidf.vocabulary_\nprint(\"Basic post's vocabulary contains {} words.\".format(len(voc)))","metadata":{"execution":{"iopub.status.busy":"2021-07-14T13:34:53.662736Z","iopub.execute_input":"2021-07-14T13:34:53.663256Z","iopub.status.idle":"2021-07-14T13:34:53.66873Z","shell.execute_reply.started":"2021-07-14T13:34:53.663223Z","shell.execute_reply":"2021-07-14T13:34:53.667774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We import two libraries for text processing. <br>\n<code>re</code> stands for Regular Expresions, it will help in finding spesific character patterns in texts and replace with new ones. For more info <a href=\"https://docs.python.org/3/library/re.html\">here</a> .<br>\n<code>nltk</code> stands for Natural Language Toolkit, NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries. For more info <a href=https://www.nltk.org/>here</a>.","metadata":{}},{"cell_type":"code","source":"import re\n\nimport string\npunctuation = string.punctuation","metadata":{"execution":{"iopub.status.busy":"2021-07-14T13:33:12.96736Z","iopub.execute_input":"2021-07-14T13:33:12.967931Z","iopub.status.idle":"2021-07-14T13:33:13.382232Z","shell.execute_reply.started":"2021-07-14T13:33:12.967896Z","shell.execute_reply":"2021-07-14T13:33:13.380716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from spacy.lang.en import English\nnlp = English()\nfrom spacy.lang.en.stop_words import STOP_WORDS as stopwords","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"givenpatterns =[(r'(\\|\\|\\|)',r' ||| '),\n                (r'http\\S+', r''),\n                (r\"(.)\\1{2,}\",r\"\\1\\1\"),\n                (r'[0-9]\\S+',r''),\n                (r\"'\",r''),\n                (r' {2,}',r' ')]\n\ndef cleaning(text):\n    text = text.lower()\n    \n    text = re.sub(r'(\\|\\|\\|)',r' ||| ',text)\n    text = re.sub(r'http\\S+', r' URL ',text)\n    \n    doc = nlp(text)\n    text = \" \".join([word.text for word in doc if not word.text in stopwords])\n    \n    for token in punctuation.replace(\"'\",'').replace(\"|\",''):\n        text = text.replace(token,' ')\n        \n    for (raw,rep) in givenpatterns:\n        regex = re.compile(raw)\n        text = regex.sub(rep,text)\n        \n    #This is a try to remove non-Latin words.    \n    text =re.sub(r'[^\\x00-\\x7F\\x80-\\xFF\\u0100-\\u017F\\u0180-\\u024F\\u1E00-\\u1EFF]', u'', text)\n    \n    return text","metadata":{"execution":{"iopub.status.busy":"2021-07-14T13:34:07.566356Z","iopub.execute_input":"2021-07-14T13:34:07.566738Z","iopub.status.idle":"2021-07-14T13:34:07.578693Z","shell.execute_reply.started":"2021-07-14T13:34:07.566702Z","shell.execute_reply":"2021-07-14T13:34:07.577185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = data['posts'][0]\ntext","metadata":{"execution":{"iopub.status.busy":"2021-07-14T13:34:58.294201Z","iopub.execute_input":"2021-07-14T13:34:58.294767Z","iopub.status.idle":"2021-07-14T13:34:58.302122Z","shell.execute_reply.started":"2021-07-14T13:34:58.29473Z","shell.execute_reply":"2021-07-14T13:34:58.300801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cleaning(text)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T13:35:00.889571Z","iopub.execute_input":"2021-07-14T13:35:00.889979Z","iopub.status.idle":"2021-07-14T13:35:00.919905Z","shell.execute_reply.started":"2021-07-14T13:35:00.88994Z","shell.execute_reply":"2021-07-14T13:35:00.919072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if skip_first_cleaning:\n    clean_posts = pd.read_csv(\"/kaggle/input/only-df-clean-posts/first_phase_clean_posts.csv\",index_col=0)\n    data=data.join(clean_posts)\nelse:\n    # This is a time consuming process...\n    data['clean_posts']=data['posts'].apply(cleaning)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T13:35:03.20581Z","iopub.execute_input":"2021-07-14T13:35:03.206218Z","iopub.status.idle":"2021-07-14T13:35:04.16581Z","shell.execute_reply.started":"2021-07-14T13:35:03.206181Z","shell.execute_reply":"2021-07-14T13:35:04.164802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T13:35:41.595535Z","iopub.execute_input":"2021-07-14T13:35:41.595934Z","iopub.status.idle":"2021-07-14T13:35:41.610168Z","shell.execute_reply.started":"2021-07-14T13:35:41.595891Z","shell.execute_reply":"2021-07-14T13:35:41.608808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer","metadata":{"execution":{"iopub.status.busy":"2021-07-14T13:35:41.692421Z","iopub.execute_input":"2021-07-14T13:35:41.692809Z","iopub.status.idle":"2021-07-14T13:35:41.69791Z","shell.execute_reply.started":"2021-07-14T13:35:41.692774Z","shell.execute_reply":"2021-07-14T13:35:41.696726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_cv = CountVectorizer().fit(data['clean_posts'])\nclean_voc = clean_cv.vocabulary_\nprint(\"After cleaning, dataset contains {} words. This is {:.2f}% of the initial dataset's total words.\".format(len(clean_voc),len(clean_voc)/len(voc)*100))","metadata":{"execution":{"iopub.status.busy":"2021-07-14T13:35:46.124954Z","iopub.execute_input":"2021-07-14T13:35:46.125357Z","iopub.status.idle":"2021-07-14T13:35:47.227508Z","shell.execute_reply.started":"2021-07-14T13:35:46.125319Z","shell.execute_reply":"2021-07-14T13:35:47.226495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=TfidfVectorizer( stop_words='english').fit_transform(data['clean_posts'])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42,shuffle=True)\n\nif skip_clean_posts_classification :\n\n    tpot = Pipeline([('selectfwe', SelectFwe(alpha=0.006)), ('multinomialnb', MultinomialNB(alpha=0.01, fit_prior=False))])\n    tpot.fit(X_train ,y_train)\n    steps = tpot.steps\nelse:\n    tpot = TPOTClassifier(generations=generations,verbosity=verbosity,\n                              population_size=population_size,config_dict=config_dict,memory=memory,\n                              n_jobs=-1,random_state=42)\n\n    tpot.fit(X_train ,y_train)\n    steps = tpot.fitted_pipeline_.steps\n    \nclean_dataset_score = tpot.score(X_test,y_test)*100\nprint(\"With clean data we have {:.2f} % accuracy. And the best pipeline is: {}\".format(\n                    clean_dataset_score ,steps)  )","metadata":{"execution":{"iopub.status.busy":"2021-07-12T13:33:52.651833Z","iopub.execute_input":"2021-07-12T13:33:52.652277Z","iopub.status.idle":"2021-07-12T13:33:53.931414Z","shell.execute_reply.started":"2021-07-12T13:33:52.652241Z","shell.execute_reply":"2021-07-12T13:33:53.930034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Score without cleaning: \\t{:.2f} %\".format(pure_dataset_score))\nprint(\"Score after first cleaning: \\t{:.2f} %\".format(clean_dataset_score))","metadata":{"execution":{"iopub.status.busy":"2021-07-12T13:33:53.933747Z","iopub.execute_input":"2021-07-12T13:33:53.934195Z","iopub.status.idle":"2021-07-12T13:33:53.944748Z","shell.execute_reply.started":"2021-07-12T13:33:53.93415Z","shell.execute_reply":"2021-07-12T13:33:53.943414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Now we have cleaned up our dataset, let's take a look at vocabulary.<br>\n    Also, we can check each personality type's top words.","metadata":{}},{"cell_type":"code","source":"DTM = pd.DataFrame(clean_cv.transform(data['clean_posts']).toarray(),index=data.index,columns=clean_cv.get_feature_names()).join(data['type'],rsuffix=\"_pers\")\nDTM","metadata":{"execution":{"iopub.status.busy":"2021-07-08T16:18:04.625225Z","iopub.execute_input":"2021-07-08T16:18:04.62572Z","iopub.status.idle":"2021-07-08T16:18:04.837273Z","shell.execute_reply.started":"2021-07-08T16:18:04.625666Z","shell.execute_reply":"2021-07-08T16:18:04.835984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DTM=DTM.join(data['type'],rsuffix=\"_pers\")\nfr= DTM.groupby('type_pers').sum().transpose()\nfr","metadata":{"execution":{"iopub.status.busy":"2021-07-08T16:18:04.83898Z","iopub.execute_input":"2021-07-08T16:18:04.839428Z","iopub.status.idle":"2021-07-08T16:18:04.916408Z","shell.execute_reply.started":"2021-07-08T16:18:04.839371Z","shell.execute_reply":"2021-07-08T16:18:04.915186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wordcloud as wc","metadata":{"execution":{"iopub.status.busy":"2021-07-12T13:34:10.818052Z","iopub.execute_input":"2021-07-12T13:34:10.818439Z","iopub.status.idle":"2021-07-12T13:34:10.871095Z","shell.execute_reply.started":"2021-07-12T13:34:10.818407Z","shell.execute_reply":"2021-07-12T13:34:10.870298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(len(data['type'].unique()), sharex=True, figsize=(20,15))\nfig.patch.set_facecolor('xkcd:tan')\n\nwordcloud = wc.WordCloud(stopwords=None,background_color='white',relative_scaling=1,max_font_size=100 ,normalize_plurals=False)\n\nfor i,pers in enumerate(data['type'].unique(),start=1):\n    plt.subplot(4,4,i)\n    scores = fr[pers].sort_values(ascending=False)[:10]\n    wordcloud.fit_words(scores) \n    plt.imshow(wordcloud,interpolation='bilinear'); plt.title(pers); plt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T16:18:04.973056Z","iopub.execute_input":"2021-07-08T16:18:04.973395Z","iopub.status.idle":"2021-07-08T16:18:09.270513Z","shell.execute_reply.started":"2021-07-08T16:18:04.97336Z","shell.execute_reply":"2021-07-08T16:18:09.26933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for pers in DTM['type_pers'].unique():\n    scores = fr[pers].sort_values(ascending=False)[:10]\n    print(\"###\",pers,\": \",scores.index.values,\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-07-08T16:18:09.272546Z","iopub.execute_input":"2021-07-08T16:18:09.273003Z","iopub.status.idle":"2021-07-08T16:18:09.342256Z","shell.execute_reply.started":"2021-07-08T16:18:09.272962Z","shell.execute_reply":"2021-07-08T16:18:09.341137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, we see that people like discusing personality types, this is expected because dataset comes from a forum for personalities. Moreover, people like discusing their own personality more frequent than the other ones. This is logical but it is wrong to train a model with this dataset. We have to remove all personalities' references.","metadata":{}},{"cell_type":"markdown","source":"<h2 name= \"Text Cleaning\">Text cleaning</h2> (round 2)","metadata":{}},{"cell_type":"code","source":"personalities = [personality.lower() for personality in data['type'].unique()]\n\ndef second_cleaning(text):\n    for preson in personalities:\n        text = re.sub(r's*|'.join(personalities),\"\",text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-07-12T13:34:25.300248Z","iopub.execute_input":"2021-07-12T13:34:25.30074Z","iopub.status.idle":"2021-07-12T13:34:25.308325Z","shell.execute_reply.started":"2021-07-12T13:34:25.300708Z","shell.execute_reply":"2021-07-12T13:34:25.306971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['second_clean_posts'] = data['clean_posts'].apply(second_cleaning)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T13:34:25.48143Z","iopub.execute_input":"2021-07-12T13:34:25.481837Z","iopub.status.idle":"2021-07-12T13:34:29.718713Z","shell.execute_reply.started":"2021-07-12T13:34:25.481803Z","shell.execute_reply":"2021-07-12T13:34:29.717853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"second_clean_cv = CountVectorizer().fit(data['second_clean_posts'])\nsecond_clean_voc = second_clean_cv.vocabulary_\nprint(\"After second cleaning, dataset contains {} words.This is {:.2f}% of the initial dataset's total words.\".format(len(second_clean_voc),len(second_clean_voc)/len(voc)*100))","metadata":{"execution":{"iopub.status.busy":"2021-07-12T13:34:32.644857Z","iopub.execute_input":"2021-07-12T13:34:32.645251Z","iopub.status.idle":"2021-07-12T13:34:33.748028Z","shell.execute_reply.started":"2021-07-12T13:34:32.645179Z","shell.execute_reply":"2021-07-12T13:34:33.747148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=TfidfVectorizer(stop_words='english').fit_transform(data['second_clean_posts'])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42,shuffle=True)\nif skip_second_clean_posts_classification:\n    \n    tpot = Pipeline([('selectfwe', SelectFwe(alpha=0.006)), ('multinomialnb', MultinomialNB(alpha=0.01, fit_prior=False))])\n    tpot.fit(X_train ,y_train)\n    steps = tpot.steps\nelse:\n    \n    tpot = TPOTClassifier(generations=generations,verbosity=verbosity, max_time_mins=500,\n                              population_size=population_size,config_dict=config_dict,memory=memory,\n                              n_jobs=-1,random_state=42)\n    \n    tpot.fit(X_train ,y_train)\n    steps = tpot.fitted_pipeline_.steps\n                          \n\nsecond_clean_dataset_score = tpot.score(X_test,y_test)*100\nprint(\"With double clean data we have {:.2f} % accuracy. And the best pipeline is: {}\".format(second_clean_dataset_score,steps))","metadata":{"execution":{"iopub.status.busy":"2021-07-12T13:34:44.211816Z","iopub.execute_input":"2021-07-12T13:34:44.212189Z","iopub.status.idle":"2021-07-12T13:34:45.374752Z","shell.execute_reply.started":"2021-07-12T13:34:44.212157Z","shell.execute_reply":"2021-07-12T13:34:45.373675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>We can keep cleaning the dataset, but we stop here.</h3>","metadata":{}},{"cell_type":"code","source":"print(\"Score without cleaning: \\t{:.2f} %\".format(pure_dataset_score))\nprint(\"Score after first cleaning: \\t{:.2f} %\".format(clean_dataset_score))\nprint(\"Score after second cleaning: \\t{:.2f} %\".format(second_clean_dataset_score))","metadata":{"execution":{"iopub.status.busy":"2021-07-12T13:34:47.751769Z","iopub.execute_input":"2021-07-12T13:34:47.752151Z","iopub.status.idle":"2021-07-12T13:34:47.759035Z","shell.execute_reply.started":"2021-07-12T13:34:47.752119Z","shell.execute_reply":"2021-07-12T13:34:47.757681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if save_data_to_files:\n    dataset['clean_posts'] = dataset['posts'].apply(cleaning)\n    dataset['second_clean_posts'] = dataset['clean_posts'].apply(second_cleaning)\n    dataset[['type','second_clean_posts']].to_csv(\"second_clean_posts.csv\")\nelse:\n    print(\"Not Saved!\")","metadata":{"execution":{"iopub.status.busy":"2021-07-12T13:35:30.282961Z","iopub.execute_input":"2021-07-12T13:35:30.283337Z","iopub.status.idle":"2021-07-12T13:35:30.291227Z","shell.execute_reply.started":"2021-07-12T13:35:30.283303Z","shell.execute_reply":"2021-07-12T13:35:30.29018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2021-07-14T13:36:03.402763Z","iopub.execute_input":"2021-07-14T13:36:03.403309Z","iopub.status.idle":"2021-07-14T13:36:03.422261Z","shell.execute_reply.started":"2021-07-14T13:36:03.403248Z","shell.execute_reply":"2021-07-14T13:36:03.42092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Lemmatisation</h2>","metadata":{}},{"cell_type":"markdown","source":"Lemmatisation is the process which we transform different inflected forms of a word into the basic word. In order to do this, I use <a href=\"https://spacy.io/\" >spaCy</a> library. <br>\n**spaCy** is a python library for natural language process (nlp). It contains a variety of models, which are trained to extract information from text data in a lot of different languages.<br>\nIn detail, I use the spaCy's model for english language <code>en_core_web_sm</code>.","metadata":{}},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"code","source":"import spacy\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef spacy_lemma(text):\n    doc = nlp(text)\n    return ' '.join([word.lemma_ for word in doc])","metadata":{"execution":{"iopub.status.busy":"2021-07-14T13:36:14.727926Z","iopub.execute_input":"2021-07-14T13:36:14.728352Z","iopub.status.idle":"2021-07-14T13:36:15.823192Z","shell.execute_reply.started":"2021-07-14T13:36:14.728313Z","shell.execute_reply":"2021-07-14T13:36:15.82205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if skip_lemmatazation :\n    data = data.join(pd.read_csv(\"/kaggle/input/only-df-clean-posts/lemmatized_posts.csv\",index_col=0) )\nelse:\n    data['spacy_lemma'] = data['clean_posts'].map(spacy_lemma)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T13:58:29.201311Z","iopub.execute_input":"2021-07-14T13:58:29.201748Z","iopub.status.idle":"2021-07-14T13:58:30.108212Z","shell.execute_reply.started":"2021-07-14T13:58:29.201711Z","shell.execute_reply":"2021-07-14T13:58:30.106986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['spacy_lemma'].head()","metadata":{"execution":{"iopub.status.busy":"2021-07-12T14:01:27.438188Z","iopub.execute_input":"2021-07-12T14:01:27.43862Z","iopub.status.idle":"2021-07-12T14:01:27.447099Z","shell.execute_reply.started":"2021-07-12T14:01:27.438587Z","shell.execute_reply":"2021-07-12T14:01:27.446371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if save_data_to_files:\n    data['spacy_lemma'].to_csv(\"lemmatized_posts.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-12T14:08:51.870872Z","iopub.execute_input":"2021-07-12T14:08:51.871576Z","iopub.status.idle":"2021-07-12T14:08:51.875862Z","shell.execute_reply.started":"2021-07-12T14:08:51.871519Z","shell.execute_reply":"2021-07-12T14:08:51.875023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=TfidfVectorizer( stop_words='english').fit_transform(data['spacy_lemma'])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42,shuffle=True)\nif skip_lemma_posts_classification:\n    \n    tpot = Pipeline([('selectfwe', SelectFwe(alpha=0.006)), ('multinomialnb', MultinomialNB(alpha=0.01, fit_prior=False))])\n    tpot.fit(X_train ,y_train)\n    steps = tpot.steps\nelse:\n    \n    tpot = TPOTClassifier(generations=generations,verbosity=verbosity, max_time_mins=500,\n                              population_size=population_size,config_dict=config_dict,memory=memory,\n                              n_jobs=-1,random_state=42)\n    \n    tpot.fit(X_train ,y_train)\n    steps = tpot.fitted_pipeline_.steps\n                          \n\nlemma_post_dataset_score = tpot.score(X_test,y_test)*100\nprint(\"With lemmatised data we have {:.2f} % accuracy. And the best pipeline is: {}\".format(second_clean_dataset_score,steps))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Score without cleaning: \\t{:.2f} %\".format(pure_dataset_score))\nprint(\"Score after first cleaning: \\t{:.2f} %\".format(clean_dataset_score))\nprint(\"Score after second cleaning: \\t{:.2f} %\".format(second_clean_dataset_score))\nprint(\"Score after lemmatization: \\t{:.2f} %\".format(lemma_post_dataset_score))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I want to remind you that the data which has been used to train and test the models above is reduced. There is still the issue with imbalanced dataset.","metadata":{}}]}