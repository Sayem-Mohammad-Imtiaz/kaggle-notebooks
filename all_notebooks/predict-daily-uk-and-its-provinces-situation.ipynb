{"cells":[{"metadata":{},"cell_type":"markdown","source":"# I. Predict Daily UK and Its Provinces Confirmed Cases\n## 1. Download Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nBASE_URL = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/'\nCONFIRMED = 'time_series_covid19_confirmed_global.csv'\nURL = BASE_URL + CONFIRMED\n\ndata_download = pd.read_csv(URL)\ndata_download = data_download[data_download['Country/Region'] == 'United Kingdom']\nprovince = data_download['Province/State'].values\nprovince = ['Other' if item is np.nan else item for item in province]\ndata_download = data_download.drop(columns=['Province/State', 'Country/Region', 'Lat', 'Long']).values\n\n# Store all the data into store based on province. Finally, add them up to form a total.\nstore = {}\nstore_scaler = {}\nstore_model = {}\n\nfor index,prov in enumerate(province):\n    store[prov] = np.reshape(data_download[index], (-1, 1))\nstore['Total'] = np.reshape(np.sum(data_download, axis=0), (-1, 1))\n\nprint(province)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Training,Test Data Preprocessing and Generation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\ndef _data_norm(data_train, data_test):\n    # This scaler will also be used for future data recovery\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    data_train_norm = scaler.fit_transform(data_train)\n    data_test_norm = scaler.transform(data_test)\n    return data_train_norm, data_test_norm, scaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split data\ndef _data_split(data, test_ratio):\n    data_train, data_test = train_test_split(data, test_size=test_ratio, shuffle=False)\n    return np.reshape(data_train, (-1,1)), np.reshape(data_test, (-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate data based on training_step and prediction_step\ndef _data_generator(data, training_step, prediction_step):\n    data_X = []\n    data_y = []\n    for i in range(len(data) - training_step - prediction_step):\n        data_X.append(data[i:i+training_step])\n        data_y.append(data[i + training_step:i + training_step + prediction_step])\n    return np.array(data_X), np.array(data_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate data by the above methods\ndef data_prepare(region, test_ratio, training_step, prediction_step):\n    if region not in store:\n        raise ValueError('data_prepare():' + str(region) + ' is not a supported region!')\n    data = store[region]\n    train, test         = _data_split(data, test_ratio)\n    train, test, scaler = _data_norm(train, test)\n    X_train, y_train    = _data_generator(train, training_step, prediction_step)\n    X_test,  y_test     = _data_generator(test, training_step, prediction_step)\n    X_train = np.reshape(X_train, (np.shape(X_train)[0], -1, 1))\n    X_test  = np.reshape(X_test, (np.shape(X_test)[0], -1, 1))\n    # Store the current scaler for the region for later data recovery\n    store_scaler[region] = scaler\n    return X_train, y_train, X_test, y_test\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.RNN Gated Recurrent Unit"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\n\ndef GRU(training_step, prediction_step):\n    model = keras.Sequential([\n        keras.layers.GRU(units=256, input_shape=(training_step,1), dropout=0.2, return_sequences=True),\n        keras.layers.GRU(units=128, dropout=0.2, return_sequences=False),\n        keras.layers.Dense(units=64, activation='relu'),\n        keras.layers.Dense(units=prediction_step)\n    ])\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.Training for different regions of the UK"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ratio      = 0.15\ntraining_step   = 5\n# Notice, if prediction_step is not 1, other functions might crash\nprediction_step = 1\n\n# Start training models for all regions\nepochs = 200\n\nfor region in store: \n    print('Start training (' + str(epochs) + ' epochs): ' + region)\n    X_train, y_train, X_test, y_test = data_prepare(region=region, test_ratio=test_ratio, training_step=training_step, prediction_step=prediction_step)\n    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n    model = GRU(training_step, prediction_step)\n    model.fit(X_train,y_train, epochs=epochs, validation_data=(X_test, y_test), callbacks=[early_stopping], verbose=0)\n    store_model[region] = model\n    print('Evaulate on ' + region + ' :' + str(model.evaluate(X_test, y_test)) + '\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ## 5. Predictions of different regions of the UK[](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Used to predict the future data\ndef data_predict_recover(region, prediction_days):\n    model  = store_model[region]\n    scaler = store_scaler[region]\n    data   = scaler.transform(store[region])[-training_step:].tolist()\n    for day in range(prediction_days):\n        pred = model.predict(np.reshape(data[-training_step:], (1, -1, 1)))\n        data.append([float(pred)])\n        if len(data) > prediction_days:\n            data = data[1:]\n    return scaler.inverse_transform(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Doing data predictions\nprediction_days = 10\nstore_predictions = {}\n\nfor region in store:\n    store_predictions[region] = data_predict_recover(region=region, prediction_days=prediction_days)\n\nprint(store_predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.Data Plotting"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nprint('Prediction days ' + str(prediction_days))\n\nplt.subplots(3,4,figsize=(25,20))\nfor i, region in enumerate(store.keys()):\n    data = np.reshape(store[region],(1,-1)).tolist()[0]\n    data_prediction = np.reshape(store_predictions[region], (1,-1)).tolist()[0]\n    plt.subplot(3,4,i+1)\n    plt.plot(list(range(len(data))),data, '-.', label='Current')\n    plt.plot(list(range(len(data), len(data) + prediction_days)),data_prediction, '.', label='Prediction')\n    plt.xlabel('Day')\n    plt.ylabel('Confirmed Cases')\n    plt.title('UK ' + region + ' COVID-19 Confirmed Cases' )\n    plt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# II. Predict Daily UK and Its Provinces Death Cases\n## 1. Download Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nBASE_URL = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/'\nDEATH = 'time_series_covid19_deaths_global.csv'\nURL = BASE_URL + DEATH\n\ndata_download = pd.read_csv(URL)\ndata_download = data_download[data_download['Country/Region'] == 'United Kingdom']\nprovince = data_download['Province/State'].values\nprovince = ['Other' if item is np.nan else item for item in province]\ndata_download = data_download.drop(columns=['Province/State', 'Country/Region', 'Lat', 'Long']).values\n\n# Store all the data into store based on province. Finally, add them up to form a total.\nstore = {}\nstore_scaler = {}\nstore_model = {}\n\nfor index,prov in enumerate(province):\n    store[prov] = np.reshape(data_download[index], (-1, 1))\nstore['Total'] = np.reshape(np.sum(data_download, axis=0), (-1, 1))\n\nprint(province)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.Repeat I.2-5"},{"metadata":{},"cell_type":"markdown","source":"## 3.Data Plotting"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nprint('Prediction days ' + str(prediction_days))\n\nplt.subplots(3,4,figsize=(25,20))\nfor i, region in enumerate(store.keys()):\n    data = np.reshape(store[region],(1,-1)).tolist()[0]\n    data_prediction = np.reshape(store_predictions[region], (1,-1)).tolist()[0]\n    plt.subplot(3,4,i+1)\n    plt.plot(list(range(len(data))),data, '-.', label='Current')\n    plt.plot(list(range(len(data), len(data) + prediction_days)),data_prediction, '.', label='Prediction')\n    plt.xlabel('Day')\n    plt.ylabel('Death Cases')\n    plt.title('UK ' + region + ' COVID-19 Death Cases' )\n    plt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# III. Analysis\n### 1. For both confirmed cases and death cases, the number keeps growing, which means it still will be a really long time until the end of the virus.\n### 2. The growing rates in almost all provinces and total number are decreasing, which is a good news that the daily cases for both confirmed cases and death cases will be less. In other words, we have passes the darkest time.\n### 3. However, we cannot predict any virus rebound from the above data or pictures. If the data doesn't become 0, it is still highly likely that the virus will come back and cause huge infections again. Therefore, we still should be cautious."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}