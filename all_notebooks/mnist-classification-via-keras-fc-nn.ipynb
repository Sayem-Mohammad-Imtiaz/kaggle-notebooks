{"cells":[{"metadata":{"_cell_guid":"822341cf-6221-4fa8-9004-43f4c87f7b25","_kg_hide-input":true,"_kg_hide-output":false,"_uuid":"95c7651db776afe07275c4f4e7895374fe25a7c3","collapsed":true,"trusted":true},"cell_type":"code","source":"### MNIST Digit Classification via KERAS using Fully-Connected Neural Network\n\n##### A simple 2 layer fully-connected feed forward neural network that achieves ~99.998% on training data set and ~97.7 on the test data set.\n\n# Import Numpy, TensorFlow, Keras and vectorized MNIST data\nimport numpy as np\nfrom numpy import array\n#import tensorflow as tf\nimport keras\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, Flatten\nfrom keras.optimizers import SGD, Adam\nfrom keras.utils import to_categorical\n\n# Load the training set:\ntrain_set = pd.read_csv('../input/train.csv')\n# extract the labels:\ntrain_label = train_set.label\n# training features, normalized:\ntrain_feat = np.array(train_set.iloc[:, 1:])/255\n# reshape the column vector to row array\ntrain_label = np.array(train_label).reshape(-1, 1)\n# one hot encode labels:\nencoded_label = to_categorical(train_label)\n\n# Define the Sequential Feed-farward Model\nmodel = Sequential() \nmodel.add(Dense(units=300, input_dim=784, activation='relu'))\nmodel.add(Dense(units=200, activation='relu'))\nmodel.add(Dense(units=100, activation='relu'))\nmodel.add(Dense(units=10))\nmodel.add(Activation('softmax'))\n\n# Initialize and compile:\nkeras.initializers.TruncatedNormal(mean=0.0, stddev=0.1, seed=None)\nmodel.compile(optimizer=SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True), loss='categorical_crossentropy')\n#model.compile(optimizer=Adam(lr=0.001, decay=1e-6), loss='categorical_crossentropy')\n\n# Training\nhistory = model.fit(train_feat, encoded_label, validation_split=0.01,  batch_size=64, epochs=15, verbose=0)\n# Find the indices of the most confident prediction for each item. That tells us the predicted digit for that sample.\npredictions = model.predict(np.array(train_feat)).argmax(axis=1)\nactual = train_label[:,0]\n# Calculate the accuracy, which is the percentage of times the predicated labels matched the actual labels\ntrain_accuracy = np.mean(predictions == actual)\n\n# Print out the result\nprint(\"Train accuracy: \", train_accuracy)\n\n# Plot some figures:\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nplt.semilogy(history.history['loss'])\nplt.semilogy(history.history['val_loss'])\nplt.title('Model Complexity Graph:  Training vs. Validation Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validate'], loc='upper right')\n\n# Load the test set:\ntest_set = pd.read_csv('../input/test.csv')\n# test features:\ntest_feat = np.array(test_set)/255\npredictions = model.predict(np.array(test_feat)).argmax(axis=1)\n\nprint('Done!')\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}