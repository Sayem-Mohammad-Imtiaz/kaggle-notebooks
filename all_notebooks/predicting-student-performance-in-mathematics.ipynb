{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predicting Student Performance in Mathematics\n\nThe obejctive of this project is to build and compare three binary classifiers for predicting student performance in Mathematics, using the data collected from two public schools in Portugal during the school year 2005/06. The dataset was retrieved from the UCI Machine\nLearning Repository (Cortez & Silva, 2008). The descriptive features include 5 numeric, 17 nominal and 10 ordinal features.\nThe target feature, G3, is a numeric variable, which shows the final grade. The values of G3 range from 0 to 20, where 0 represents the lowest grade while 20 represents full marks. For a binary classification, G3 values that are greater or equal to 10 represent \"Pass\", else \"Fail\". \n\n## Outline:\n- [Section 1 (Overview)](#1)\n- [Section 2 (Data Preparation)](#2)  \n- [Section 3 (Hyperparameter Tuning)](#3) \n- [Section 4 (Performance Comparison)](#4) \n- [Section 5 (Summary)](#5) \n- [Section 6 (References)](#6) "},{"metadata":{},"cell_type":"markdown","source":"# Overview <a class=\"anchor\" id=\"1\"></a> \n\n## Methodology\n\nI build three classification models, K-Nearest Neighbors (KNN), Decision Trees (DT) and Naive Bayes (NB), to predict whether students pass or fail in Mathematics.\n\nI start by transforming the dataset. The categorical features are encoded into numerical features and the whole descriptive features are scaled using Min-Max Scaling. The dataset is then partitioned into two parts at a 70:30 ratio for training and test.\n\nThen, given the large number of columns of descriptive features after transformation, applying feature selection could be beneficial before fitting the model. I select the top 10 features by Random Forest Importance and F-Score. Then, I compare the performance of these two feature selection methods and continue with the better one for further model fitting.\n\nAfter the feature selection, I train the models with hyperparameter search in a pipeline with 5-fold repeated stratified cross-validation based on the train data with full features and the same train data but only with the top 10 features selected in the previous stage. \n\nStratification is necessary throughout the model fitting and selection as the binary target classes are imbalanced.\n\nIn the end, I fit the best models identified from the hyperparameter search on the test data with a 5-fold repeated stratified cross-validation and compare the model performance by a paired t-test to determine if these models yield any significant differences. The comparison is initially based on the metric area under curve (AUC), and I integrate other evaluation metrics, such as recall, precision, and F1-score, for a comprehensive and in-depth comparison."},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation <a class=\"anchor\" id=\"2\"></a> \n\n## Loading Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nnp.random.seed(999)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"math = pd.read_csv('../input/matcsv/mat.csv')\nmath.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(math.shape)\nmath.columns.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset contains 395 observations. The numeric target feature \"G3\" has been renamed as \"target\" and transformed into a binary categorical feature with two levels \"pass\" and \"fail\"."},{"metadata":{},"cell_type":"markdown","source":"## Checking for Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"math.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Summary Statistics"},{"metadata":{"trusted":true},"cell_type":"code","source":"math.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoding Categorical Features\n\nIt is necessary to encode all categorical features into numerical features, since Scikit-learn requires all data to be numeric before putting them into the algorithm. \n\nBefore encoding the target feature, the descriptive features and the target feature need to be partitioned. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data = math.drop(columns='target')\ntarget = math['target']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Target Feature\n\nIt is obvious that the target classes are imbalanced. The number of \"pass\" is twice as many as that of \"fail\".\n\nThe positive target feature level \"pass\" is encoded as \"1\". "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(target.value_counts())\ntarget = target.replace({'pass':1,'fail':0})\ntarget.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Categorical Descriptive Features\n\nThere are two types of categorical descriptive features in the dataset.\n\n#### Nominal:\n\n13 out of the 17 nominal features have only two levels, therefore they are simply encoded into a single column of 0 and 1. The remaining 4 features have more than two levels, therefore applying one-hot-encoding is necessary as it can create a binary column for each unique value under these multi-level nominal features.\n\n1. sex: binary - female or male)\n2. school: binary - Gabriel Pereira or Mousinho da Silveira\n3. address: binary - urban or rural\n4. Pstatus: binary - living together or apart\n5. Mjob: 5 levels\n6. Fjob: 5 levels\n7. guardian: 3 levels\n8. famsize: binary - â‰¤ 3 or > 3\n9. reason: 4 levels\n10. schoolsup: binary - yes or no\n11.\tfamsup: binary - yes or no\n12.\tactivities: binary - yes or no\n13.\tpaidclass: binary - yes or no\n14.\tinternet: binary - yes or no\n15.\tnursey: binary - yes or no\n16.\thigher: binary - yes or no\n17.\tromantic: binary - yes or no"},{"metadata":{"trusted":true},"cell_type":"code","source":"nominal_cols = data.columns[data.dtypes==object].tolist()\nnominal_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ordinal:\nThe ordinal categorical features have been encoded into numbers in the original dataset and therefore there is no need to further transform them. The numbers under each ordinal categorical feature are meaningful. For example, under the feature \"Medu\" (mother's education level), 0 is \"none\"; 1 is \"primary education\"; 2 is \"5th to 9th grade\"; 3 is \"secondary education\"; 4 is \"higher education\". The larger the number, the higher the education level.\n\n1. Medu: 0 to 4; the larger the number, the higher the education level\n2. Fedu: 0 to 4; the larger the number, the higher the education level\n3. famrel: 1 to 5; the larger the number, the higher the quality of family relationship\n4. traveltime: 1 to 4; the larger the number, the longer the travel time to school\n5. studytime: 1 to 4; the larger the number, the longer the weekly study time\n6. freetime: 1 to 5; the larger the number, the more free time after school\n7. goout: 1 to 5; the larger the number, the more frequent going out with friends\n8. Walc: 1 to 5; the larger the number, the more weekend alcohol consumption\n9. Dalc: 1 to 5; the larger the number, the more workday alcohol consumption\n10. health: 1 to 5; the larger the number, the healthier"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in nominal_cols:\n    n = len(data[col].unique())\n    if (n == 2):\n        data[col] = pd.get_dummies(data[col], drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.get_dummies(data)\ndata = pd.get_dummies(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After performing one-hot-enconding on those 4 nominal features, the number of columns with descriptive features in the dataset extend from 32 to 45. "},{"metadata":{},"cell_type":"markdown","source":"## Feature Scaling\n\nScaling descriptive features is beneficial as it can normalise the numeric values among different variables within a specific range and can help speed up the processing time in the algorithm. \nMin-Max Scaling is applied to scale the descriptive features between 0 and 1. Each binary feature can be still kept as binary after scaling."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\ndata_unscaled=data.values\ndata_scaled = preprocessing.MinMaxScaler().fit_transform(data_unscaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(data_scaled, columns=data.columns).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Selection & Ranking\n\nThe 1-nearest neighbor classifier is used as a wrapper to compare the performance of feature selection methods: F-Score and Random Forest Importance. I also use stratified 5-fold cross validation with 3 repetitions for assessment."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors=1)\ncv_method = RepeatedStratifiedKFold(n_splits=5, n_repeats=3,random_state=999)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Full Set of Features\nBefore applying the feature selection methods, I assess the perfomance using all descriptive features in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\ncv_results_full = cross_val_score(estimator=clf,\n                             X=data_scaled,\n                             y=target, \n                             cv=cv_method, \n                             scoring='roc_auc')\ncv_results_full.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The AUC score for the full features is very low, at 0.556. There are probably some irrelevant features in the dataset that weaken the performance of the model."},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nnp.random.seed(999)\nmodel_rfi = RandomForestClassifier(n_estimators=100)\nmodel_rfi.fit(data_scaled, target)\nfs_indices_rfi = np.argsort(model_rfi.feature_importances_)[::-1][0:10]\n\nbest_features_rfi = data.columns[fs_indices_rfi].values\nprint(best_features_rfi)\n\nfeature_importances_rfi = model_rfi.feature_importances_[fs_indices_rfi]\nprint(feature_importances_rfi)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_results_rfi = cross_val_score(estimator=clf,\n                             X=data_scaled[:, fs_indices_rfi],\n                             y=target, \n                             cv=cv_method, \n                             scoring='roc_auc')\ncv_results_rfi.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The AUC score for the top 10 features selected by Random Forest Importance is 0.753."},{"metadata":{},"cell_type":"markdown","source":"### F-Score"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import feature_selection as fs\nnp.random.seed(999)\nfs_fit_fscore = fs.SelectKBest(fs.f_classif, k=10)\nfs_fit_fscore.fit_transform(data_scaled, target)\nfs_indices_fscore = np.argsort(fs_fit_fscore.scores_)[::-1][0:10]\nbest_features_fscore = data.columns[fs_indices_fscore].values\nprint(best_features_fscore)\nfeature_importances_fscore = fs_fit_fscore.scores_[fs_indices_fscore]\nprint(feature_importances_fscore)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_results_fscore = cross_val_score(estimator=clf,\n                             X=data_scaled[:, fs_indices_fscore],\n                             y=target, \n                             cv=cv_method, \n                             scoring='roc_auc')\ncv_results_fscore.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The AUC score for the top 10 features selected by F-Score is 0.812."},{"metadata":{},"cell_type":"markdown","source":"### Performance Comparison Using Paired T-Tests"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nprint(stats.ttest_rel(cv_results_full, cv_results_fscore).pvalue.round(3))\nprint(stats.ttest_rel(cv_results_full, cv_results_rfi).pvalue.round(3))\nprint(stats.ttest_rel(cv_results_rfi, cv_results_fscore).pvalue.round(3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The performances after feature selection by both Random Forest Importance and F-Score are statistically better than the performance based on the full features, at 5% level of significance. \nMeanwhile, the difference between F-Score and Random Forest Importance is also statistically significant. Therefore, for the further analysis, I continue with the top 10 features selected by F-Score as shown in the below figure. In this figure, it shows that the importance decreases sharply after the top 2 features. The importance becomes very marginalised till the last feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_ranking = pd.DataFrame({'Feature': best_features_fscore, \n                                'Importance': list(feature_importances_fscore)}, \n                               columns=['Feature', 'Importance'])\nimport seaborn as sns\nsns. barplot(x=\"Feature\",y=\"Importance\",\n            color='blue',data=feature_ranking)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train-Test Splitting\n\nThe dataset is split into train and test at a 70:30 partition ratio by stratification:\n\n* Training (70%): X_train (descriptive), y_train (target)\n* Testing (30%): X_test (desciptive), y_test (target)\n\nMeanwhile, I created X_train_10 and X_test_10, which have the same sample rows as X_train and X_test, but only have the top 10 features selected by F-score from the previous process."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(data_scaled,\n                                                 target.values,\n                                                 test_size=0.3,\n                                                 random_state=999,\n                                                 stratify=target.values)\nprint(X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_10 = pd.DataFrame(X_train, columns=data.columns)\nX_train_10 = X_train_10[best_features_fscore].values\nX_test_10 = pd.DataFrame(X_test, columns=data.columns)\nX_test_10 = X_test_10[best_features_fscore].values\nprint(X_train_10.shape)\nprint(X_test_10.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameter Tuning<a class=\"anchor\" id=\"3\"></a>\n\nIn this section, I train and fine-tune the models based on the 276 rows of training data. I also compare the performance of models with the full features and with the top 10 features.\n\nEach model is evaluated by 5-fold stratified cross-validation with 3 repetitions for hyperparameter tuning."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\ncv_method = RepeatedStratifiedKFold(n_splits = 5,n_repeats=3,random_state=999)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. K-Nearest Neighbors\n\nI use grid search for hyperparameter tuning in a pipeline and train the KNN model with different k-nearest neighbors and distance types."},{"metadata":{"trusted":true},"cell_type":"code","source":"params_knn = {'n_neighbors': [2,3,4,5,6,7,8,9,10,11,12,13,14,15], \n              'p': [1, 2]}\ngs_knn = GridSearchCV(estimator=KNeighborsClassifier(), \n                      param_grid=params_knn, \n                      cv=cv_method,\n                      verbose=1, \n                      scoring='roc_auc')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_knn.fit(X_train, y_train)\nprint(gs_knn.best_params_)\nprint(gs_knn.best_score_)\nknn_best = gs_knn.best_estimator_\nprint(knn_best)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_knn.fit(X_train_10, y_train)\nprint(gs_knn.best_params_)\nprint(gs_knn.best_score_)\nknn_best10 = gs_knn.best_estimator_\nprint(knn_best10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The optimal KNN model based on the full features has a mean AUC score of 0.725 with 14 nearest neighbors and with Manhattan distance.\n* The optimal KNN model based on the top 10 features has a mean AUC score of 0.934 with 14 nearest neighbors and with Manhattan distance.\n\nIn general, the performance of KNN models seems to have improved after feature selection. "},{"metadata":{},"cell_type":"markdown","source":"## 2. Decision Tree\n\nTo find the optimal Decision Tree model, I include different criterion (gini index and entropy), maximum depth and minimum sample split in the grid search."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nparams_dt = {'criterion':['gini','entropy'],\n             'max_depth':[3,4,5,6,7,8,9,10],\n             'min_samples_split':[2,3,4,5]}\n    \ngs_dt = GridSearchCV(estimator=DecisionTreeClassifier(random_state=999), \n                      param_grid=params_dt, \n                      cv=cv_method,\n                      verbose=1, \n                      scoring='roc_auc')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_dt.fit(X_train, y_train)\nprint(gs_dt.best_params_)\nprint(gs_dt.best_score_)\ndt_best = gs_dt.best_estimator_\nprint(dt_best)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_dt.fit(X_train_10, y_train)\nprint(gs_dt.best_params_)\nprint(gs_dt.best_score_)\ndt_best10 = gs_dt.best_estimator_\nprint(dt_best10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The optimal Decision Tree model based on the full features has a mean AUC score of 0.956 using gini index. It has a maximum depth of 3 and minimum split value of 2 samples.\nThe optimal Decision Tree model based on the top 10 features has a mean AUC score of 0.963 using entropy. It also has a maximum depth of 3 and minimum split value of 2 samples.\nIt seems the performance of KNN models does not improve much after feature selection, but I will confirm it with a paired t-tests later."},{"metadata":{},"cell_type":"markdown","source":"## 3. (Gaussian) Naive Bayes\n\nI include different var_smoothing to search for the optimal Gaussian Naive Bayes model, starting with 1 to 10^-9 with 100 different values. Before fitting into the algorithm, I perform a power transformation to ensure that each descriptive feature follows a Gaussian distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PowerTransformer\nX_train_trans = PowerTransformer().fit_transform(X_train)\nX_train_10_trans = PowerTransformer().fit_transform(X_train_10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import RandomizedSearchCV\n\nnp.random.seed(999)\nparams_nb ={'var_smoothing': np.logspace(0,-9,num=100)}\ngs_nb =GridSearchCV(estimator=GaussianNB(),\n                   param_grid=params_nb,\n                   cv=cv_method,\n                   verbose=1,\n                   scoring='roc_auc')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_nb.fit(X_train_trans, y_train)\nprint(gs_nb.best_params_)\nprint(gs_nb.best_score_)\nnb_best = gs_nb.best_estimator_\nprint(nb_best)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_nb.fit(X_train_10_trans, y_train)\nprint(gs_nb.best_params_)\nprint(gs_nb.best_score_)\nnb_best10 = gs_nb.best_estimator_\nprint(nb_best10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The optimal Naive Bayes model based on the full features yields a meal AUC score of 0.889, while the optimal one based on the top 10 features has a mean AUC score of 0.931."},{"metadata":{},"cell_type":"markdown","source":"# Performance Comparison <a class=\"anchor\" id=\"4\"></a> "},{"metadata":{},"cell_type":"markdown","source":"In this section, I fit the optimal models from the above analyses on the test data with 5-fold stratified cross validation and 3 repetitions. Then, I compare the performance of models by paired t-test:\n* DT (full features) vs. DT (top 10 features)\n* KNN (full features) vs. KNN (top 10 features)\n* NB (full features) vs. NB (top 10 features)\n\nFull Features:\n* DT vs. KNN\n* DT vs. NB\n* KNN vs. NB\n\nTop 10 Features:\n* DT vs. KNN\n* DT vs. NB\n* KNN vs. NB"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\ncv_method_ttest = RepeatedStratifiedKFold(n_splits=5,n_repeats=3,\n                                          random_state=999)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_results_knn = cross_val_score(estimator=knn_best,\n                                 X=X_test,\n                                 y=y_test, \n                                 cv=cv_method_ttest, \n                                 scoring='roc_auc')\ncv_results_knn_10 = cross_val_score(estimator=knn_best10,\n                                 X=X_test_10,\n                                 y=y_test, \n                                 cv=cv_method_ttest, \n                                 scoring='roc_auc')\ncv_results_dt = cross_val_score(estimator=dt_best,\n                                X=X_test,\n                                y=y_test, \n                                cv=cv_method_ttest, \n                                scoring='roc_auc')\ncv_results_dt_10 = cross_val_score(estimator=dt_best10,\n                                X=X_test_10,\n                                y=y_test, \n                                cv=cv_method_ttest, \n                                scoring='roc_auc')\ncv_results_nb = cross_val_score(estimator=nb_best,\n                                X=X_test,\n                                y=y_test, \n                                cv=cv_method_ttest, \n                                scoring='roc_auc')\ncv_results_nb_10 = cross_val_score(estimator=nb_best10,\n                                X=X_test_10,\n                                y=y_test, \n                                cv=cv_method_ttest, \n                                scoring='roc_auc')\nprint(\"KNN(full features):\",cv_results_knn.mean())\nprint(\"KNN(top 10 features):\",cv_results_knn_10.mean())\nprint(\"DT(full features):\",cv_results_dt.mean())\nprint(\"DT(top 10 features):\",cv_results_dt_10.mean())\nprint(\"NB(full features):\",cv_results_nb.mean())\nprint(\"NB(top 10 features):\",cv_results_nb_10.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(stats.ttest_rel(cv_results_dt, cv_results_dt_10))\nprint(stats.ttest_rel(cv_results_nb, cv_results_nb_10))\nprint(stats.ttest_rel(cv_results_knn, cv_results_knn_10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above results show that all models perform significantly better with the top 10 features selected by F-Score, than with the full features, at a 5% level of significance."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(stats.ttest_rel(cv_results_dt, cv_results_knn))\nprint(stats.ttest_rel(cv_results_dt, cv_results_nb))\nprint(stats.ttest_rel(cv_results_knn, cv_results_nb))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For models based on the full features, the Decision Tree model performs significantly better than KNN and Naive Bayes models, at a 5% level of significance. KNN and Naive Bayes models perform at similar levels."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(stats.ttest_rel(cv_results_dt_10, cv_results_knn_10))\nprint(stats.ttest_rel(cv_results_dt_10, cv_results_nb_10))\nprint(stats.ttest_rel(cv_results_knn_10, cv_results_nb_10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For models based on the top 10 features selected by F-Score, the KNN model performs significantly better than the Naive Bayes model. Decision Tree performs similarily comparing with either KNN or Naive Bayes models. It is hard to decide the optimal model at this stage with only AUC score available, but based on the above performance comparison, it is clear that all models with the top 10 features perform better on the test data. Therefore, for further evaluation on accuracy, precision, recall, F1 Score and confusion matrix, I only consider models with the top 10 features."},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_knn = gs_knn.predict(X_test_10)\npred_dt = gs_dt.predict(X_test_10)\nX_test_10_trans = PowerTransformer().fit_transform(X_test_10)\npred_nb = gs_nb.predict(X_test_10_trans)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nprint(\"\\nKNN: Confusion matrix\") \nprint(metrics.confusion_matrix(y_test, pred_knn))\nprint(\"\\nKNN: Classification report\") \nprint(metrics.classification_report(y_test, pred_knn))\n\nprint(\"\\nDT: Confusion matrix\") \nprint(metrics.confusion_matrix(y_test, pred_dt))\nprint(\"\\nDT: Classification report\") \nprint(metrics.classification_report(y_test, pred_dt))\n\nprint(\"\\nNB: Confusion matrix\") \nprint(metrics.confusion_matrix(y_test, pred_nb))\nprint(\"\\nNB: Classification report\") \nprint(metrics.classification_report(y_test, pred_nb))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Suppose the school wants to predict students who are likely to fail the Mathematics course in order to give these students more support in advance. Then the recall of \"0\" (fail) is an important metric to consider in this case. The Decision Tree model is the most optimal one.\nHowever, if the school wants to correctly predict students who are likely to pass in order to select good students to attend Mathematics competitions, the recall of \"1\" (pass) should be emphasized. Therefore, the KNN model can be the most optimal one.\nIn general, the Decision Tree model can be regarded as the best model in terms of the F1-score as this score is a weighted harmonic mean of precision and recall."},{"metadata":{},"cell_type":"markdown","source":"# Summary <a class=\"anchor\" id=\"5\"></a> \n\nThe Decision Tree model based on the top 10 features selected by F-Score is the best model for prediction in this analysis, under a limited hyperparameter tuning and feature selection approach. Although there is not enough statiscal evidence to show that this Descision Tree model performs better than the other KNN and Naive Bayes models, at a 5% level of significance, it yields the highest AUC score (0.916) and F1 score (0.9). In general, all models with only the top 10 features selected by F-Score perform significantly better than those with the full features, at a 5% level of significance."},{"metadata":{},"cell_type":"markdown","source":"# References <a class=\"anchor\" id=\"6\"></a> \n* P. Cortez and A. Silva. Using Data Mining to Predict Secondary School Student Performance. In A. Brito and J. Teixeira Eds., Proceedings of 5th FUture BUsiness TEChnology Conference (FUBUTEC 2008) pp. 5-12, Porto, Portugal, April, 2008, EUROSIS, ISBN 978-9077381-39-7. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}