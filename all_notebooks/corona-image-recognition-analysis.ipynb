{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Thank you for Kaggle to provide this dataset.\n\nThe purpose of my work is to provide a machine learning model that can predict if a person has corona from their Xray. This can assist doctors when they diagnose corona.\n\nThe dataset includes Xray images of patients with Corona from multiple sources: Virus such as COVID 19 and SARS, bacteria such as Streptococcus, and stress smoking such as ARDS.\n\nI get a high cross-validation result: 0.96 accuracy from the time I run the model. But applying the model with highest cross validation result to the test data only results in 0.75 accuracy","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Import Initial Library","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport os\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load the data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/coronahack-chest-xraydataset/Chest_xray_Corona_Metadata.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## View the data\nFrom Viewing the data, I find that there are 2 types of Dataset: TRAIN and TEST, so I split them into df_train and df_test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting train and test data\ndf_train = df[df[\"Dataset_type\"] == \"TRAIN\"]\ndf_test = df[df[\"Dataset_type\"] == \"TEST\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis/Data cleaning\nIts important to understand the data :) \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Missing values\nFrom looking at the missing value graph, we see that most of the virus name is missing, and some of the virus category is missing.\n\nThere are 1576 Missing values on Virus type (Label 1 Virus Category). That just means the person is normal. We can fill the data on Label 1 Virus Category with None. We can also set Label_2_Virus_category to None\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import missingno\nmissingno.matrix(df, figsize = (30,10))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum() #check for number of null values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Label_1_Virus_category']= df['Label_1_Virus_category'].fillna(\"None\")\ndf_train['Label_1_Virus_category']= df_train['Label_1_Virus_category'].fillna(\"None\")\ndf_test['Label_1_Virus_category']=df_test['Label_1_Virus_category'].fillna(\"None\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['Label_1_Virus_category'] == \"None\"][\"Label_2_Virus_category\"] = \"None\"\ndf_train[df_train['Label_1_Virus_category'] == \"None\"][\"Label_2_Virus_category\"] = \"None\"\ndf_test[df_test['Label_1_Virus_category'] == \"None\"][\"Label_2_Virus_category\"] = \"None\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check for values in each column\nSO we have 1576 normal patients to compare with 4334 Corona Patients.\n\nWe dont know most of the source of Corona, only a few labels are provided in Label_2_Virus_category.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Label.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Dataset_type.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train.Label_2_Virus_category.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_test.Label_2_Virus_category.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train.Label_1_Virus_category.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_test.Label_1_Virus_category.value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load images\nIn short, I load images and then put them into numpy array to put in the CNN model.\nI rescale the image, which can create some potential problems when dealing with test data.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#get test and train dir\ntest_img_dir = '/kaggle/input/coronahack-chest-xraydataset/Coronahack-Chest-XRay-Dataset/Coronahack-Chest-XRay-Dataset/test'\ntrain_img_dir = '/kaggle/input/coronahack-chest-xraydataset/Coronahack-Chest-XRay-Dataset/Coronahack-Chest-XRay-Dataset/train'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Here I am loading all the names of different image\nimage_train = os.listdir(train_img_dir)\nimage_train = sorted(image_train)\nimage_train\ndf_train = df_train.sort_values(\"X_ray_image_name\")\n\nimage_test = os.listdir(test_img_dir)\nimage_test = sorted(image_test)\nimage_test\ndf_test = df_test.sort_values(\"X_ray_image_name\")\n\ntrain_images_name = df_train[\"X_ray_image_name\"]\ntest_images_name= df_test[\"X_ray_image_name\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now I am building the numpy array for train images\nimport cv2\nTrainImages = []\nfor i in image_train:\n    if i in train_images_name.values:\n        img = cv2.imread(train_img_dir+'/'+i)\n        img = cv2.resize(img, (200,200)) #if I dont rescale to (200,200), the memory cannot take it. Also, its good to have all the images in the same size.\n        TrainImages.append(img)\nTrainImages= np.array(TrainImages)\nTrainImages.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#I build the numpy array for test images\nTestImages = []\nfor i in image_test:\n    if i in test_images_name.values:\n        img = cv2.imread(test_img_dir+'/'+i)\n        img = cv2.resize(img, (200,200))    \n        TestImages.append(img)\nTestImages= np.array(TestImages)\nTestImages.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets view some of the images, it looks like the image is still fine (I dont know how X ray image works)\nimport matplotlib.pyplot as plt\nplt.figure()\nplt.imshow(TrainImages[0])\nplt.colorbar()\nplt.grid(False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets view some of the images, it looks like te image is still fine (I dont know how X ray image works)\nplt.figure()\nplt.imshow(TrainImages[1])\nplt.colorbar()\nplt.grid(False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing\nWe create numpy arrays with 1 as Pnemonia and 0 as normal\nWe also scale the image to 0 and 1.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Create dummy labels","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create train and test labels for neural network models\ntrain_labels = df_train[\"Label\"] == \"Pnemonia\"\ntrain_labels = np.array(train_labels).astype(int)\ntest_labels = df_test[\"Label\"] == \"Pnemonia\"\ntest_labels = np.array(test_labels).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Augmentation\nActually, I received lower validation accuracy when doing data augmentation. My theory is because that the X-ray is pretty standard so augment it hurts the prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#from keras.preprocessing.image import ImageDataGenerator \n#aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15, width_shift_range=0.2, \n#                         height_shift_range=0.2, shear_range=0.15, \n#                         horizontal_flip=True, fill_mode=\"nearest\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scaleimage","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Scaling the image\nTrainImages = TrainImages/255\nTestImages = TestImages/255","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split train and test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\nTrainImages, train_labels, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Delete the df to release some memory\n\nBasically, Kaggle has a 13gb RAM limit, so I delete some dataframes to safe RAM.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"list = [df, df_train, df_test, train_img_dir, test_img_dir, TrainImages, train_labels, img, train_images_name, test_images_name, image_test, image_train]\ndel list\nimport gc\ngc.collect()\ndf = pd.DataFrame()\ndf_train= pd.DataFrame()\ndf_test=pd.DataFrame()\ntrain_img_dir= []\ntest_img_dir = []\nTrainImages= []\ntrain_labels= []\nimg = []\ntest_images_name = []\ntest_images_name = []\nimg_train = []\nimg_test = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build the model\nFor the model, similar to other image recognition project, I use CNN.\n\nBy changing different Dense nodes, I find that 30 dense nodes yields better accuracy than (50,100,150) nodes.\n\nFrom some previous experience with image recognition, I only use one Dense hidden layer\n\nI try to use 2 Convolution layers followed by Maxpooling and find that it works the best\n\nI also find that 1 epoch works the best.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow import keras\n#Initialize the model\nmodel = keras.Sequential()\n#Convolutional layers\nmodel.add(keras.layers.Conv2D(filters = 32, kernel_size = (3, 3),activation='relu', input_shape= (200,200,3)))\nmodel.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(keras.layers.Conv2D(filters = 64, kernel_size = (3, 3),activation='relu', input_shape= (200,200,3)))\nmodel.add(keras.layers.MaxPooling2D(pool_size = 2, strides=2))\nmodel.add(keras.layers.Dropout(0.5))\n\n#Dense layers\nmodel.add(keras.layers.Flatten())\n\n\nmodel.add(keras.layers.Dense(30, activation='relu'))\nmodel.add(keras.layers.Dropout(0.5))\nmodel.add(keras.layers.Dense(2, activation='softmax'))\n\n#Choose compiler\nmodel.compile(optimizer = 'adam', \n              loss='sparse_categorical_crossentropy',\n              metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#What the model looks like\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train and Evaluate the model\nbest CV score I can get is about 0.97","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Here I found out that my data augmentation wasnt helping.\n#history = model.fit_generator(aug.flow(X_train, y_train, batch_size= 32),\n#                    epochs = 6, validation_data= (X_test, y_test))\nmodel.fit(X_train, y_train, epochs = 5,validation_data=(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Use the model on the test data\nI get about 0.74 accuracy on test model, which is a huge loss of accuracy from validation. I am not sure why this is the case.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(TestImages, test_labels)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}