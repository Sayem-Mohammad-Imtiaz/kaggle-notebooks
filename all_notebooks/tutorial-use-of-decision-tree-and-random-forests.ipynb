{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For this notebook project, we will explore a loan data  which connects people who need money (borrowers) with people who have money (investors). We try to predict whether an investor will invest in people who showed a profile of borrowers with some features.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1.Import Libraries and Data\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport cufflinks as cf\ncf.go_offline()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"../input/loan_data.csv\")\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.Exploratory Data Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info() \n#There 9578 rows and 14 columns in our dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here are what the columns represent:\n* credit.policy: 1 if the customer meets the credit underwriting criteria of LendingClub.com, and 0 otherwise.\n* purpose: The purpose of the loan (takes values \"credit_card\", \"debt_consolidation\", \"educational\", \"major_purchase\", \"small_business\", and \"all_other\").\n* int.rate: The interest rate of the loan, as a proportion (a rate of 11% would be stored as 0.11). Borrowers judged by LendingClub.com to be more risky are assigned higher interest rates.\n* installment: The monthly installments owed by the borrower if the loan is funded.\n* log.annual.inc: The natural log of the self-reported annual income of the borrower.\n* dti: The debt-to-income ratio of the borrower (amount of debt divided by annual income).\n* fico: The FICO credit score of the borrower.\n* days.with.cr.line: The number of days the borrower has had a credit line.\n* revol.bal: The borrower's revolving balance (amount unpaid at the end of the credit card billing cycle).\n* revol.util: The borrower's revolving line utilization rate (the amount of the credit line used relative to total credit available).\n* inq.last.6mths: The borrower's number of inquiries by creditors in the last 6 months.\n* delinq.2yrs: The number of times the borrower had been 30+ days past due on a payment in the past 2 years.\n* pub.rec: The borrower's number of derogatory public records (bankruptcy filings, tax liens, or judgments).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe(include=\"all\") \n#Here we get overall statistical information about the data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*here wreate a histogram of two FICO distributions on top of each other, one for each credit.policy outcome","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\ndf[df[\"credit.policy\"] == 1][\"fico\"].hist(color=\"blue\",bins=50,label=\"Credit Policy = 1\",alpha=0.4)\ndf[df[\"credit.policy\"] == 0][\"fico\"].hist(color=\"red\",bins=50,label=\"Credit Policy = 0\",alpha=0.4)\nplt.legend()\n#here we make two different histogram one for those who have credit policy 1 score and the other for those who have 0 score\n# we compare their relative fico credit scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*This figure shows that people who have lower FICO score tends to have a credit policy of 0,\nthis means that they do not meet the criteria of borrowing money\n\n*People who have 660 or fewer fico socre do not meet the criteria ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df[\"credit.policy\"] == 1][\"fico\"].iplot(kind=\"hist\",bins=24,colors=\"blue\")\ndf[df[\"credit.policy\"] == 0][\"fico\"].iplot(kind=\"hist\",bins=24,colors=\"orange\")\n#Here we do the same histogram with iplot library because it is interactive\n# this means that when we click somewhere we can get exact score ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now we will create a similar figure, except this time select by the not.fully.paid column as our target column\n\n*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\ndf[df[\"not.fully.paid\"] ==1][\"fico\"].hist(label=\"not fully paid = 1\",alpha=0.6,color=\"blue\",bins=30)\ndf[df[\"not.fully.paid\"] ==0][\"fico\"].hist(label=\"not fully paid = 0\",alpha=0.6,color=\"red\",bins=30)\nplt.xlabel(\"FICO\")\nplt.title(\"The FICO credit score of the borrower\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*The figure shows that the majority of people pay these loans\n\n*The loans fully paid or not fully paid has almost the same distribution, but those not fully paid has lower fico scores","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"*Here we create a countplot using seaborn showing the counts of loans by purpose, with the color hue defined by not.fully.paid. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.countplot(x=\"purpose\",hue=\"not.fully.paid\", data=df, palette=\"Set1\")\nplt.title(\"The Counts of Loans by Purpose\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Based on countplot of purpose of loan, we can say that debt consolidation is the most popular reason for loan\n\n*Secondly the ratio of fully paid and not fully paid is almost similar across different purposes of loan","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Here we will see the trend between FICO score and interest rate with the following jointplot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(x=\"fico\",y=\"int.rate\",data=df,color=\"green\",space=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*The figure above shows that the more fico score increase, the lower interest rate people have better credit get and vice versa","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Here we create the following lmplots to see whether the trend between not.fully.paid and credit policy columns. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot(x=\"fico\",y=\"int.rate\",data=df,palette=\"Set1\",hue=\"credit.policy\",col=\"not.fully.paid\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*col parameter of lmplot  gives us the possiblity to create more than one plot according to the number of items inside defined column\n\n*for example in the column \"not fully paid\", we have just two value, so we get two separate plots because we assigned \"not fully paid\" as col parameter\n\n*Here we get more detailed version of the previous plot and get more complex relationship between columns in a single plot","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 3. Feature Engineering:\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info() # we look again the overall information about the data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*We need to dela with categorical columns \n\n*We have  a categorical column as **purpose** column \n\n*This means we need to transform the values in this column by using dummy variables so sklearn will be able to understand them. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_feature=[\"purpose\"]\nfinal_data= pd.get_dummies(df,columns=cat_feature,drop_first=True)\nfinal_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"get_dummies(data, prefix=None, prefix_sep='_', dummy_na=False, columns=None, sparse=False, drop_first=False, dtype=None) -> 'DataFrame'\n    Convert categorical variable into dummy/indicator variables.\n\nIt turns a categorical variable into a series of zeros and ones, which makes them a lot easier to quantify and compare.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_data.head()\n#Now all of the features in the data has been tranformed into 0 and 1 by adding a new column for each of them","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Splitting the Data and Training Decision Three Model\n\n*Now its time to split our data into a training set and a testing set before applying the algorithm\n\n*we use sklearn to split our data into a training set and a testing set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X=final_data.drop(\"not.fully.paid\",axis=1) # All of the columns except from the target column has assigned as the X\ny=final_data[\"not.fully.paid\"] # \"not.fully.paid\" column has been assigned as the target column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3) # Here we split our data as training and test dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndtree=DecisionTreeClassifier()\ndtree.fit(X_train,y_train) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Here we create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 5. Predictions and Evaluation of Decision Tree Model\n\n**We will make predictions from the test set and create a classification report and a confusion matrix to compare the results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions=dtree.predict(X_test)\n#df_pred=pd.DataFrame(predictions)\nplt.figure(figsize=(15,10))\nsns.countplot(predictions,palette=\"Set1\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*According to classification report; \n            Precision:True Positive/Total Predicted Positivemacro average of precision(averaging the unweighted mean per label) 0.54(%85 for 0's and %22 for 1's);weighted average (averaging the support-weighted mean per label) is 0.75\n            \n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to confusion matrix; \n\n            -True Negatives: 2019\n            -False Positive:398\n            -False Negative:345\n            -True Positive:112","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"*The results are not good, so we will try Random Forest Model and compare the results with Decision Tree Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 6. Training the Random Forest model\n\n*Now its time to train our new model\n\n* we will create an instance of the RandomForestClassifier class and fit it to our training data from the previous step","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier(n_estimators=300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc.fit(X_train,y_train) # We make th new algorithm fit with the training set\nrfc_predictions=rfc.predict(X_test) #We make the algorith to predict y test values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Predictions and Evaluation\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Now we will create a classification report from the results. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,rfc_predictions))\nprint(5*\"\\n\")\nprint(confusion_matrix(y_test,rfc_predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*When we compare the results from both of the models we use, Random Forest model performs better than Decision Tree Model\n\n*However, when it comes to the resuts for target column=1 of recall and f1 score, Decision Tree Model performs far better than the other one\n\n*Therefore, before choosing an algorithm we have to keep in mind our priorities and pros and cons of different ML models","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Show the Confusion Matrix for the predictions.**","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"**What performed better the random forest or the decision tree?**","execution_count":null},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Great Job!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}