{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1><center>Data Science And Cardiovascular diseases (CVDs)</center></h1>\n<img src=\"https://villa-medica.com/wp-content/uploads/2017/07/villa-medica-cardiovascular-infographic-1-compressor.jpg\" alt=\"drawing\" width=\"600\" height=\"400\"/>\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# What are cardiovascular diseases?\n\n\nCardiovascular diseases (CVDs) are a group of disorders of the heart and blood vessels and they include:\n>* Coronary heart disease – disease of the blood vessels supplying the heart muscle\n>* Cerebrovascular disease – disease of the blood vessels supplying the brain\n>* Peripheral arterial disease – disease of blood vessels supplying the arms and legs\n>* Rheumatic heart disease – damage to the heart muscle and heart valves from rheumatic fever, caused by streptococcal bacteria\n>* Congenital heart disease – malformations of heart structure existing at birth\n>* Deep vein thrombosis and pulmonary embolism – blood clots in the leg veins, which can dislodge and move to the heart and lungs\n\n<img  src=\"https://www.heart.org/-/media/images/health-topics/consumer-healthcare/istock529114056.jpg?la=en&hash=94CE873BB4478FEEAE058CB5FDEE2CB0A53A2C85\" alt=\"FDR\" width=\"600\" height=\"400\"/>\n\n\nAccording to World Health Organisation, \n>CVDs are the number 1 cause of death globally i.e More people die annually from CVDs than from any other cause.\nAn estimated 17.9 million people died from CVDs in 2016, representing 31% of all global deaths. Of these deaths, 85% are due to heart attack and stroke\n\nFrom the above facts and figures, we can easily conclude that Cardiovascular disease (CVD) accounts for the majority of death and hospitalization, health care expenditures and loss of productivity in developed country\n\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Role of Data Science\n\nAI, also known as machine intelligence, can be defined as a branch of computer science that mimics the human mind process. <br>\nOne of the most popular technologies of AI is called machine learning, which enables algorithms to understand and learn data. <br>\n>These trending technolologies are applied in cardiovascular medicine including :\n>* Crecision medicine\n>* Clinical prediction\n>* Cardiac imaging analysis\n>* Intelligent robots\n\nIn this Dataset named \"Cardiovascular Disease dataset\", We'll try to analyze and gather the insights of dataset provided by [Svetlana Ulianova](https://www.kaggle.com/sulianova) and predict the possibility of a person having Cardiovascular disease based on various parameters specified in this dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Life Cycle of a Data Science Project\n\n\n<img  src=\"https://mk0analyticsindf35n9.kinstacdn.com/wp-content/uploads/2020/04/Screenshot-2020-04-15-at-10.08.12-AM-768x630.png\" alt=\"FDR\" width=\"600\" height=\"400\"/>\n\nHey! Don't forget about the main aspects of any Data Science Project. Through this Kernel, Let's Dive into every layer of a Data Science Project.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Importing Necessary Libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.simplefilter('ignore')\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os#Walking through directores\n\nimport plotly.graph_objects as go # Generate Graphs\nfrom plotly.subplots import make_subplots #To Create Subplots\n\nfrom sklearn import decomposition #pca\nfrom sklearn.preprocessing import StandardScaler # Standardization ((X - X_mean)/X_std)\n\nfrom sklearn.neighbors import KNeighborsClassifier #KNN Model\nfrom sklearn.ensemble import RandomForestClassifier #RandomForest Model\nfrom sklearn.linear_model import LogisticRegression #Logistic Model\n\nfrom sklearn.model_selection import train_test_split # Splitting into train and test\n\nfrom sklearn.model_selection import GridSearchCV# Hyperparameter Tuning\nfrom sklearn.model_selection import cross_val_score#cross validation score\n\nfrom sklearn.metrics import classification_report # text report showing the main classification metrics\nfrom sklearn.metrics import confusion_matrix #to get confusion_matirx \n\npd.set_option('display.max_columns', None)#Setting Max Columns Display to Max inorder to get glance of all features in dataframe","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's Dive into our CSV File to get a glance of what we are dealing with","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"missing_values = ['?', '--', ' ', 'NA', 'N/A', '-'] #Sometimes Missing Values are't in form of NaN\ndf = pd.read_csv('../input/cardiovascular-disease-dataset/cardio_train.csv', delimiter = ';', na_values = missing_values)\nprint('There are Total {} datapoints in the dataset with {} Features listed as {}:'.format(df.shape[0], df.shape[1], df.columns.values))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">So, this Dataset contains the following Features :\n* id :> It's just the Id no of the row. Not revelant \n* age :> It's the age of a person in Days\n* gender :> It's the gender of the person  \n* height :> It's the height of the person in cm\n* weight :> It's the weight of the person in kg\n* ap_hi :> It's the Systolic blood pressure i.e. Pressure exerted when Blood is ejected in arteries. Normal value : 120mmhg or Below\n* ap_low :> It's the Diastolic blood pressure i.e. Pressure exerted when Blood exerts between arteries and heartbeats. Normal Value : 80mmhg or Below\n* cholesterol :> It's the Cholestreol value (Cholesterol is a type of fat found in your blood) of your blood. In Adults, 200 mg/dL is desired with 200 and 239 mg/dL as Boderline High. In Children, 170 mg/dL is desired with 170 and 199 mg/dL as Boderline High\n* gluc :> It's the Glucose Level. They're less than 100 mg/dL after not eating (fasting) for at least 8 hours. And they're less than 140 mg/dL 2 hours after eating. For most people without diabetes, blood sugar levels before meals hover around 70 to 80 mg/dL\n* smoke :> It contain Binary Values stating whether Person is a Smoker or not i.e.  {0 : 'Not a Smoker', 1 : 'Smoker'} \n* alco :> It contain Binary Values stating whether Person is an alchoalic or not i.e.  {0 : 'Not a Alchoalic', 1 : 'Alchoalic'} \n* active :> It contain Binary Values stating whether Person is involved in physical activites or not i.e.  {0 : 'Not involved in Physical Activites', 1 : 'involved in physical activites'} \n* cardio :> It's our Target Value Binary Values stating whether Person has Cardiovascular diseases (CVDs) or Not i.e.  {0 : 'Not Have CVD', 1 : 'Have CVD'} ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data Analysis \n\n\n<img  src=\"https://miro.medium.com/max/1400/1*PKXC0FeXQc5LVmqhJ8HnVg.png\" alt=\"FDR\" width=\"600\" height=\"400\"/>\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":">In our Data Analysis, We will try to analyze to find out the below stuffs :\n* Missing / Duplicate Values \n* All the Continuous Values\n* Distribution of the Numerical Values\n* Categorial Values / Discrete Values\n* Cardinality of Categorial Values / Discrete Values\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Missing / Duplicate Values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_with_null = [features for feature in df.columns if df[feature].isnull().sum()>0]\nif features_with_null:\n    print('Features with Null Values {}'.format(features_with_null))\nelse:\n    print('Dataset contains no Null Values')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we have total datapoints as 70000, we can even conclude from above data that we have no Null Values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We doesn't require column name 'id'. Better to remove it","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(columns=['id'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"duplicate_sum = df.duplicated().sum()\nif duplicate_sum:\n    print('Duplicates Rows in Dataset are : {}'.format(duplicate_sum))\nelse:\n    print('Dataset contains no Duplicate Values')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Guess we have some duplicate rows. Let's have a small lookup over duplicated rows","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"duplicated = df[df.duplicated(keep=False)]\nduplicated = duplicated.sort_values(by=['gender', 'height', 'weight'], ascending= False)\nduplicated.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Duplicate Rows doesn't contribue to our prediction. Rather they just increase the training size. It's usual to get rid of duplicates from our dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop_duplicates(keep = 'first', inplace = True)\nprint('Total {} datapoints remaining with {} features'.format(df.shape[0], df.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Continuous Values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Keeping a basic concept in my mind, if a column contains continuous values then it will have good quantity of Unique Values. <br>\nTaking 25 as that threshold :\n> df[feature].unique())>25\n\nwill do our work","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Continuous_features = [feature for feature in df.columns if len(df[feature].unique())>25]\nprint('Continuous Values are : {}'.format(Continuous_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[Continuous_features].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distribution of the Numerical Values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Before diving into distribution on the Continuous Variables, let's have some glance on basic stastical stuffs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[Continuous_features].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above observations enlightens some keypoints that can make anyone curios about this dataset:\n>* Minimum and Maximun Height sounds faulty. Although the smallest and tallest human ever known were of 54.64 cm and 251.46 cm respectively but here it just look like some kind of error \n>* Same conclusions can be made for minimum weight as described here\n>* Systolic blood pressure and Diastolic blood pressure can't be negative\n>* If Systolic blood pressure and Diastolic blood pressure are more than 180 and 120mmHg respectively, it's an emergency case. But here we are dealing with 16020mmHg and 11000mmHg repectively which looks quite fishy \n\nUmm so what does it mean?<br>\nAhh ! I see. Outliers","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## What are Outlier ?\n\nAn Outlier is a data point that differs significantly from other observations. <br>\nAn outlier may be due to variability in the measurement or it may indicate experimental error. <br>\nAn outlier can cause serious problems in statistical analyses and prediction.\n\n<img  src=\"https://miro.medium.com/max/1400/1*TbUF_HTQ6jOhO8EoPnmekQ.jpeg\" alt=\"FDR\" width=\"600\" height=\"400\"/>\n\nThere are various ways to detect outliers like :\n>* Using Scatter Plot\n>* Using Z - Test\n>* Using IQR Interquartile Range\n>* Box Plot\n\nSo, What do we do ? Like we always remove Outliers ? \n>Depends. In some cases even outliers are linked to some important features that you don't wanna lose\n\nWe'll get our solution in a second. Just bare with me <br>\nIn this Kernel, I'll use Box - Plot and IQR Interquartile Range to detect Outliers","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Box Plot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\n\nfig.add_trace(go.Box(x=df['height'], name = 'Height', boxpoints='outliers',))\nfig.add_trace(go.Box(x=df['weight'], name = 'Weight', boxpoints='outliers',))\n\nfig.update_layout(title_text=\"Box Plot for Weight and Height with Outliers\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe lot's of outliers ( Points below Lower Fence and Above Upper Fence ) <br>\nTo explain more about Outliers and result of this Box Plot, I'll use IQR Interquartile Range","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## IQR Interquartile Range\n\n<img  src=\"https://miro.medium.com/max/1400/1*2c21SkzJMf3frPXPAR_gZA.png\" alt=\"FDR\" width=\"600\" height=\"400\"/>\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Interquartile range gives another measure of variability. <br>\nIt equally divides the distribution into four equal parts called quartiles : \n>* First 25% is 1st quartile (Q1), i.e. 25 % of the data in the distribution will the less than Q1\n>* Middle one is 2nd quartile (Q2) i.e. 50 % of the data in the distribution will the less than Q2\n>* Last one is 3rd quartile (Q3) i.e. 75 % of the data in the distribution will the less than Q3\n\nThe interquartile range is the distance between the third and the first quartile i.e. :\n\n> IQR = Q3- Q1\n\nAs a rule of thumb, observations can be qualified as outliers when they lie more than 1.5 IQR below the first quartile or 1.5 IQR above the third quartile. <br>\n>Outliers = Q1 – 1.5 * IQR\n<br>or<br>\n>Outliers = Q3 + 1.5 * IQR\n\n<img  src=\"https://i2.wp.com/makemeanalyst.com/wp-content/uploads/2017/05/IQR-1.png?resize=431%2C460\" alt=\"FDR\" width=\"600\" height=\"400\"/>\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfig = make_subplots(rows=1, cols=2, subplot_titles=(\"Height Distribution\", \"Weight Distribution\"))\n\ntrace0 = go.Histogram(x=df['height'], name = 'Height')\ntrace1 = go.Histogram(x=df['weight'], name = 'Weight')\n\n\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\n\nfig.update_xaxes(title_text=\"Height\", row=1, col=1)\nfig.update_yaxes(title_text=\"Total Count\", row=1, col=1)\n\nfig.update_xaxes(title_text=\"Weight\", row=1, col=2)\nfig.update_yaxes(title_text=\"Total Count\", row=1, col=2)\n\nfig.update_layout(title_text=\"Histograph\", height=700)\n\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feel free to refer the concepts explained above to understand this distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def outliers(df_out, drop = False):\n    for each_feature in df_out.columns:\n        feature_data = df_out[each_feature]\n        Q1 = np.percentile(feature_data, 25.) # 25th percentile of the data of the given feature\n        Q3 = np.percentile(feature_data, 75.) # 75th percentile of the data of the given feature\n        IQR = Q3-Q1 #Interquartile Range\n        outlier_step = IQR * 1.5 #That's we were talking about above\n        outliers = feature_data[~((feature_data >= Q1 - outlier_step) & (feature_data <= Q3 + outlier_step))].index.tolist()  \n        print('For the feature {}, No of Outliers is {}'.format(each_feature, len(outliers)))\noutliers(df[['height', 'weight']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well That's good quality of data that we'll gonna lose.\nSo, Rather than of using this problem Let's explore other techniques to hangle this problem","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"One way is Transforming variables. <br>\nTransforming variables can also eliminate outliers. Natural log of a value reduces the variation caused by extreme values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"outline_free_df = df.copy()\noutline_free_df[['height', 'weight']] = np.log(outline_free_df[['height', 'weight']])\noutliers(outline_free_df[['height', 'weight']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Without losing data we are able to reduce good amount of outliers. <br>\nNow, We can define 0.005 and 0.995 Quantile range to handle the remaining outliers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"outline_free_df = outline_free_df[(outline_free_df['weight'] > outline_free_df['weight'].quantile(0.005)) & (outline_free_df['weight'] < outline_free_df['weight'].quantile(0.995))]\noutline_free_df = outline_free_df[(outline_free_df['height'] > outline_free_df['height'].quantile(0.005)) & (outline_free_df['height'] < outline_free_df['height'].quantile(0.995))]\noutliers(outline_free_df[['height', 'weight']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Handling outliners cost us {} datapoints'.format(len(df)-len(outline_free_df)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We finally get rid of all the outliers from height and pretty good amount from weight. <br>\nMaintaining the momentum why not let's get rid of ap_hi and ap_lo values that are negative","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"outline_free_df = outline_free_df[outline_free_df['ap_lo']>=0]\noutline_free_df = outline_free_df[outline_free_df['ap_hi']>=0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we researched, Substracting Diastolic blood pressure from Systolic blood pressure gives Pulse Pressure that can't be negative <br>\nTherefore, ap_hi > ap_lo","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('There are total {} observations where ap_hi < ap_lo'.format(len(outline_free_df[outline_free_df['ap_hi'] < outline_free_df['ap_lo']])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_data = outline_free_df[outline_free_df['ap_hi'] >= outline_free_df['ap_lo']].reset_index(drop=True)\nprint('Total observations preserved : {}'.format(len(cleaned_data)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you're not a doctor or not aware of domain that much then it's a good practice to google and look-up for some domain knowlegde like i did and realized the highest pressure recorded in an individual was 370/360.<br>\n\n<img  src=\"https://pbs.twimg.com/media/DOkphshW4AA-PuQ?format=jpg&name=medium\" alt=\"FDR\" width=\"600\" height=\"400\"/>\n\nBaically I'm feeling safe to assume some threshold let say, 250 and 200 for ap_hi and ap_lo respectively as upperbound. <br>\nValues after that will be dropped as outliers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('As per our assumptions we have total {} outliers'.format(len(cleaned_data[(cleaned_data[\"ap_hi\"]>250) | (cleaned_data[\"ap_lo\"]>200)])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_data = cleaned_data[(cleaned_data[\"ap_hi\"]<=250) & (cleaned_data[\"ap_lo\"]<=200)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total {} datapoints remaining with {} features'.format(cleaned_data.shape[0], cleaned_data.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have Age in Days. Let's do some wrangling and and Age of person in Years","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_data['age'] = cleaned_data['age'].div(365).apply(lambda x: int(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Histogram(x=cleaned_data['age'], name = 'Age'))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well i must say we got some unusual kind of distribution by rounding of the Age data when was reduced to years <br>\nOnly takeaway is :\n>* Most of the people in dataset belongs to 49 to 60 age group. (We haven't round off the age i.e. One won't complete his current age unless his birthday arives ( if it makes sense )  \n>* We have an outlier with age as 29 (smallest value) but let's ignore it.\n>* Maximun age in our Dataset is 64\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We just converted age from days to year. <br>\nLet's make sure we didn't created any duplicacy ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"duplicate_sum = cleaned_data.duplicated().sum()\nif duplicate_sum:\n    print('Duplicates Rows in Dataset are : {}'.format(duplicate_sum))\nelse:\n    print('Dataset contains no Duplicate Values')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interesting! We just converted our age from days to year and we got huge amount of duplicacy. That was unexpected <br>\nLet's have a look over duplicate rows to get more insights about this cause","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"duplicated = cleaned_data[cleaned_data.duplicated(keep=False)]\nduplicated = duplicated.sort_values(by=['gender', 'height', 'weight'], ascending= False)\nduplicated.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay. To address this problem, let me take an example. Suppose a person aged 20 is haivng his birth day on 7th June and it's May right now. <br>\nWe divided age with 365 to order to get age years. Thus keeping my age as 20 ignoring the fact he's gonna 21 within 30 days. <br>\nConverting Age from days to Year results in loss of important data. <br>\nRight now, We have two options:\n\n> * Keeping Age in Days\n> * Dropping Duplicate rows (keeping age in year)\n\nInorder to build a generalized predictive model, it's better drop these rows. <br>\nSlight change in days of a person's age should not be taken into consideration for determing CVD, provided we have other features that we'll be far valueable than this.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_data.drop_duplicates(keep = 'first', inplace = True)\nprint('Total {} datapoints remaining with {} features'.format(cleaned_data.shape[0], df.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"| Preprocessing    |      Total Datapoints      |  Datapoints Lost |\n|----------|:-------------:|------:|\n| Duplicates Removed |  69976 | 24  |\n| Outliner Removed |  68346 | 1630  |\n| Negative Values Removed |    68338   |   8 |\n| ap_hi < ap_lo Removed | 67156 |    1182 |\n| Removed via Threshold  | 67118 |    38 |\n| Duplicates Removed (Age - Days :> Year)  | 63938 |    3180 |\n    ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure(data=[go.Bar(x = cleaned_data[cleaned_data['cardio'] == 0]['age'].value_counts().index.to_list(), \n                             y =cleaned_data[cleaned_data['cardio'] == 0]['age'].value_counts().values, name = 'Non CVD'),\n                      go.Bar(x = cleaned_data[cleaned_data['cardio'] == 1]['age'].value_counts().index.to_list(), \n                             y =cleaned_data[cleaned_data['cardio'] == 1]['age'].value_counts().values, name = 'CVD')]\n               )\n\nfig.update_layout(barmode='group', xaxis_tickangle=-45, title_text=\"Distribution of Age groups grouped by Target Value\", \n                  yaxis=dict(\n        title='Total Count',\n        titlefont_size=16,\n        tickfont_size=14,\n    ),     xaxis=dict(\n        title='Age',\n        titlefont_size=16,\n        tickfont_size=14,\n    ))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure([go.Pie(labels=cleaned_data[cleaned_data['cardio'] == 0]['age'].value_counts().index.to_list(),values=cleaned_data[cleaned_data['cardio'] == 0]['age'].value_counts().values)])\nfig.update_layout(title_text=\"Distribution of Age group for Non CVD\", template=\"plotly_white\")\nfig.data[0].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[0].marker.line.width = 2\nfig.update_traces(textposition='inside')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure([go.Pie(labels=cleaned_data[cleaned_data['cardio'] == 1]['age'].value_counts().index.to_list(),values=cleaned_data[cleaned_data['cardio'] == 1]['age'].value_counts().values)])\nfig.update_layout(title_text=\"Distribution of Age group for CVD\", template=\"plotly_white\")\nfig.data[0].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[0].marker.line.width = 2\nfig.update_traces(textposition='inside')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* From Pie charts it's clear that no age group dominates Non CVD or CVD portion i.e. Age groups are uniformly distributed\n* But if you examine bar graph carefully, you get an interesting pattern:\n> * People with low age groups have lower chance of having CVD\n> * After reaching a peak point which is in this case is age group of 53 plot shows a delince shape for Non CVD and Increasing cases of CVD\n> * This clearly shows People with higher age groups have more chance of having CVD","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfig = make_subplots(rows=2, cols=2, subplot_titles=(\"Height Distribution for CVD Population\", \"Height Distribution for non CVD Population\", \"Weight Distribution for CVD Population\", \"Weight Distribution for non CVD Population\"))\n\ntrace0 = go.Histogram(x=np.exp(cleaned_data[cleaned_data['cardio'] == 0]['height']), name = 'Non CVD')\ntrace1 = go.Histogram(x=np.exp(cleaned_data[cleaned_data['cardio'] == 1]['height']), name = 'CVD')\n\ntrace2 = go.Histogram(x=np.exp(cleaned_data[cleaned_data['cardio'] == 0]['weight']), name = 'Non CVD')\ntrace3 = go.Histogram(x=np.exp(cleaned_data[cleaned_data['cardio'] == 1]['weight']), name = 'CVD')\n\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig.append_trace(trace2, 2, 1)\nfig.append_trace(trace3, 2, 2)\n\nfig.update_xaxes(title_text=\"Height\", row=1, col=1)\nfig.update_yaxes(title_text=\"Total Count\", row=1, col=1)\n\nfig.update_xaxes(title_text=\"Height\", row=1, col=2)\nfig.update_yaxes(title_text=\"Total Count\", row=1, col=2)\n\nfig.update_xaxes(title_text=\"Weight\", row=2, col=1)\nfig.update_yaxes(title_text=\"Total Count\", row=2, col=1)\n\nfig.update_xaxes(title_text=\"Weight\", row=2, col=2)\nfig.update_yaxes(title_text=\"Total Count\", row=2, col=2)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Features like Weight and Height are well distribuited for Non - CVD and CVD Population <br>\nNothing much for takeaway","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure(data=[go.Bar(x = cleaned_data[cleaned_data['cardio'] == 0]['ap_hi'].value_counts().index.to_list(), \n                             y =cleaned_data[cleaned_data['cardio'] == 0]['ap_hi'].value_counts().values, name = 'Non CVD'),\n                      go.Bar(x = cleaned_data[cleaned_data['cardio'] == 1]['ap_hi'].value_counts().index.to_list(), \n                             y =cleaned_data[cleaned_data['cardio'] == 1]['ap_hi'].value_counts().values, name = 'CVD')]\n               )\n\nfig.update_layout(barmode='group', xaxis_tickangle=-45, title_text=\"Distribution of Systolic blood pressure Values grouped by Target Value\", \n                  yaxis=dict(\n        title='Total Count',\n        titlefont_size=16,\n        tickfont_size=14,\n    ),     xaxis=dict(\n        title='Systolic Blood Pressure Values',\n        titlefont_size=16,\n        tickfont_size=14,\n    ))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure([go.Pie(labels=cleaned_data[cleaned_data['cardio'] == 0]['ap_hi'].value_counts().index.to_list(),values=cleaned_data[cleaned_data['cardio'] == 0]['ap_hi'].value_counts().values)])\nfig.update_layout(title_text=\"Distribution of Systolic blood pressure values for Non CVD\", template=\"plotly_white\")\nfig.data[0].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[0].marker.line.width = 2\nfig.update_traces(textposition='inside')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure([go.Pie(labels=cleaned_data[cleaned_data['cardio'] == 1]['ap_hi'].value_counts().index.to_list(),values=cleaned_data[cleaned_data['cardio'] == 1]['ap_hi'].value_counts().values)])\nfig.update_layout(title_text=\"Distribution of Systolic blood pressure values for CVD\", template=\"plotly_white\")\nfig.data[0].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[0].marker.line.width = 2\nfig.update_traces(textposition='inside')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By looking over the graph, We can conclude that :\n> * Our Population has highest no of people having 120 mmHg Systolic Blood Pressure \n> * If a person is not having CVD, then There's more likely (48.7 %) that he / she has 120 mmHg Systolic Blood Pressure \n> * In case of CVD, We can't claim such strong assumption since no Pressure value is dominating the destribution ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure(data=[go.Bar(x = cleaned_data[cleaned_data['cardio'] == 0]['ap_lo'].value_counts().index.to_list(), \n                             y =cleaned_data[cleaned_data['cardio'] == 0]['ap_lo'].value_counts().values, name = 'Non CVD'),\n                      go.Bar(x = cleaned_data[cleaned_data['cardio'] == 1]['ap_lo'].value_counts().index.to_list(), \n                             y =cleaned_data[cleaned_data['cardio'] == 1]['ap_lo'].value_counts().values, name = 'CVD')]\n               )\n\nfig.update_layout(barmode='group', xaxis_tickangle=-45, title_text=\"Distribution of Diastolic blood pressure Values grouped by Target Value\", \n        yaxis=dict(\n        title='Total Count',\n        titlefont_size=16,\n        tickfont_size=14,\n    ),     xaxis=dict(\n        title='Diastolic Blood Pressure Values',\n        titlefont_size=16,\n        tickfont_size=14,\n    ))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure([go.Pie(labels=cleaned_data[cleaned_data['cardio'] == 0]['ap_lo'].value_counts().index.to_list(),values=cleaned_data[cleaned_data['cardio'] == 0]['ap_lo'].value_counts().values)])\nfig.update_layout(title_text=\"Distribution of Daistolic blood pressure values for Non CVD\", template=\"plotly_white\")\nfig.data[0].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[0].marker.line.width = 2\nfig.update_traces(textposition='inside')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure([go.Pie(labels=cleaned_data[cleaned_data['cardio'] == 1]['ap_lo'].value_counts().index.to_list(),values=cleaned_data[cleaned_data['cardio'] == 1]['ap_lo'].value_counts().values)])\nfig.update_layout(title_text=\"Distribution of Daistolic blood pressure values for CVD\", template=\"plotly_white\")\nfig.data[0].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[0].marker.line.width = 2\nfig.update_traces(textposition='inside')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By looking over the graph, We can conclude that :\n\n> * Our Population has highest no of people having 80 mmHg Daistolic Blood Pressure\n> * If a person is not having CVD, then There's more likely (55.3 %) that he / she has 120 mmHg Daistolic Blood Pressure  \n> * If a person is having CVD, then There's more likely (42.5 %) that he / she has 120 mmHg Systolic Blood Pressure with second mostly likely case (31.9 %) of having 90mmHg Daistolic Blood Pressure ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Categorial Values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Categorial_features = [feature for feature in cleaned_data.columns if len(cleaned_data[feature].unique())<25]\nprint('Categorial Values are : {}'.format(Categorial_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for each_feature in Categorial_features:\n    print('No of Categorial Values in Feature {} is {} as {}'.format(each_feature, len(cleaned_data[each_feature].unique()), cleaned_data[each_feature].unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After going through the description provided by the Distributer, Following information is gathered :\n>* gender has 2 categorial values as { 1 : Women, 2 : Male } \n>* cholesterol has 3 categorial values as { 1: Normal, 2: Above Normal, 3: Well Above Normal }\n>* gluc has 3 categorial values as { 1: Normal, 2: Above Normal, 3: Well Above Normal }\n>* Remaining Categories are just binary values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's check counts of features in different relations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure([go.Pie(labels=['Not Having CVD', 'Having CVD'],values=cleaned_data['cardio'].value_counts().values)])\nfig.update_layout(title_text=\"Pie chart of Target Variable\", template=\"plotly_white\")\nfig.data[0].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[0].marker.line.width = 2\nfig.update_traces(hole=.4,)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly it's a balanced dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfig = make_subplots(rows=2, cols=3,subplot_titles=(\"Alchoal Distribution\", \"Gender Distribution\", \"Choslesterol Distribution\", \"Glucose Distribution\", \"Smoking Distribution\", \"Fitness Distribution\"), specs=[[{'type':'domain'}, {'type':'domain'}, {'type':'domain'}], [{'type':'domain'}, {'type':'domain'}, {'type':'domain'}]])\n\nfig.add_trace(go.Pie(labels=['Non Alchoalic', 'Alchoalic'],values=cleaned_data['alco'].value_counts().values, name = 'Alchoal Status'), 1, 1)\nfig.add_trace(go.Pie(labels=['Female', 'Male'],values=cleaned_data['gender'].value_counts().values, name = 'Gender Status'), 1, 2)\n\nfig.add_trace(go.Pie(labels=['Normal', 'Above Normal', 'Well Above Normal'],values=cleaned_data['cholesterol'].value_counts().values, name = 'Cholesterol Level Status'), 1, 3)\nfig.add_trace(go.Pie(labels=['Normal', 'Above Normal', 'Well Above Normal'],values=cleaned_data['gluc'].value_counts().values, name = 'Glucose Level Status'), 2, 1)\n\nfig.add_trace(go.Pie(labels=['Non Smoker', 'Smoker'],values=cleaned_data['smoke'].value_counts().values, name = 'Smoking Status'), 2, 2)\nfig.add_trace(go.Pie(labels=['Not Involved in Physical Activites', 'Involved in Physical Activites'],values=cleaned_data['active'].value_counts().values, name = 'Fitness Status'), 2, 3)\n\nfig.update_traces(hole=.4,)\nfig.update_layout(title_text=\"Distribution of Various Categorial Values\")\n\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Categorial Values are some kind of biased towards a particular value. I don't think we'll get some usefull insights from ploting their distribution in CVD and Non CVD <br>\nFeel free to have a close look on the chart to get better insights of distrubution of Categorial Features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## PCA ?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Principal component analysis (PCA) is a technique for reducing the dimensionality of such datasets, increasing interpretability but at the same time minimizing information loss.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In other words, we can convert out 12D dataset (because of 12 features) in 2D in order to visualize it <br>\nIs't it cool?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"target_value = cleaned_data['cardio']\ncleaned_data_for_pca = cleaned_data.drop(['cardio'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well We always standardize so as to get mean of whole dataset as 0 and Variance (StD) as 1 <br>\nWorking of PCA is beyond the scope. Feel free to refer google to explore the maths behind it <br>\nJust now enjoy this visualization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_data = StandardScaler().fit_transform(cleaned_data_for_pca)\n\npca = decomposition.PCA()\npca.n_components = 2\npca_data = pca.fit_transform(scaled_data)\n\npca_data = np.vstack((pca_data.T, target_value)).T\npca_df = pd.DataFrame(data = pca_data, columns = ('first', 'second', 'label'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure(data=go.Scattergl(\n    x = pca_df['first'], \n    y = pca_df['second'],\n    mode='markers',\n    marker_color=pca_df['label']\n))\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, That's our whole dataset in 2D. Though visually it seems they kind of forms some clusters, they still have lots disturbance.<br>\nPCA is quite old technique and have more algorithms like TSNE and UMap that can give better visualization but that's not our focus here. <br>\nDon't forget we have lots of things yet to do. We need to Classify man :D ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering\n\n<img  src=\"https://img-a.udemycdn.com/course/750x422/1304050_ee0f_8.jpg\" alt=\"FDR\" width=\"600\" height=\"400\"/>\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Here we will going to :\n> * Derive new features for better insights\n> * Feature Selection\n> * Feature Scaling","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##  New features \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We all whenever we have height and weight, we can calculate BMI. It seems better to get another Feature BMI. Who knows we may get some better insights\n\n<img  src=\"https://secretsofhealthyeating.com/wp-content/uploads/2018/09/BMI-Formula-768x377.jpg\" alt=\"FDR\" width=\"600\" height=\"400\"/>\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"But don't forget we have converted our weight and height via natural log function to reduce outliers <br>\nIn order to calculate BMI, We'll calculate inverse of log and then we'll apply the formula of BMI <br>\n> The inverse of log is exp","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def BMI(data):\n    return np.exp(data['weight']) / (np.exp(data['height'])/100)**2 \n \ncleaned_data['bmi'] = cleaned_data.apply(BMI, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similarly There's a relation bewteen Systolic Diastolic blood pressure that derives pulse rate","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def pulse(data):\n    return np.subtract(data['ap_hi'], data['ap_lo'])\n \ncleaned_data['pulse'] = cleaned_data.apply(pulse, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Selection","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Having irrelevant features in your data can decrease the accuracy of the models and make your model learn based on irrelevant features. <br>\nSo, We do feature selection ( automatically or manually ) to select good data\n\n<img  src=\"https://miro.medium.com/max/694/0*gz5XuPZfN0wAi66I\" alt=\"FDR\" width=\"600\" height=\"400\"/>\n\nWe have various techniques for feature selection. Here ill stuck to more tradational way i.e. using correlational matrix","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we have some new feature, let's build up a heat map to get to know about correlation between variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.rcParams['figure.figsize'] = (20, 15) \nsns.heatmap(cleaned_data.corr(), annot = True, linewidths=.5, cmap=\"YlGnBu\")\nplt.title('Corelation Between Features', fontsize = 30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"| Features with Strong Correlation with Target Value  |      Correlation with Target Value      |  Strong Correlation with another feature |\n|----------|:-------------:|------:|\n| age |    0.24   |   ap_hi ( 0.21 ) |\n| ap_hi |  0.43 | pulse ( 0.82 ) ap_lo ( 0.72 ) weight ( 0.26 ) age ( 0.21 )|\n| ap_lo |  0.33 | ap_hi ( 0.72 ) bmi ( 0.23 ) weight ( 0.24 )|\n| cholesterol | 0.21 |    gluc ( 0.44 ) |\n| pulse  | 0.33 |    ap_hi ( 0.82 ) |","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Basically, you would like to have features with good amount of correlation with target feature and low correlation with any independent feature <br>\nIn simple words, high correlation within independent features means both wanna represent same thing and even if you drop one of the feature, you won't lose much quality data \n\n>* ap_hi has the correlated with quite features but since it has the maximum correlation with Target Value, We can't afford to ignore it <br>\n>* Similar case is with pulse and ap_lo\n\n>* Unfortunately our newly formed feature 'bmi' doesn't have good correlation with target + corelates with ap_lo\n>* Similar case is with weight\n\n>* 'gender' is the least correlated feature + adding correlation with smoke and height \n\n>* Features like height, smoke, alco, active have quite low amount of correlation with target feature\n\nPreserving Good data, We'll gonna drop 'bmi',  'weight',  'gluc',  'gender',  'height',  'smoke',  'alco',  'active'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = cleaned_data.drop(['cardio', 'bmi', 'weight', 'gluc', 'gender', 'smoke', 'alco', 'active'], axis =1)\nY = cleaned_data['cardio']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Scaling","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We have multiple feature with various scales. We don't want our model to give priority to smaller / bigger values just because of difference in scale. <br>\nHence, Feature Scaling is performed during the data pre-processing to handle highly varying magnitudes or values or units <br>\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Here We gonna use Standardization formula i.e. Substracting mean of feature followed by division by standard deviation in order to set feature's mean to 0 and std to 1\n\n<img  src=\"https://365datascience.com/wp-content/uploads/2018/10/image4-9.jpg\" alt=\"FDR\" width=\"600\" height=\"400\"/>\n\nWe have a nice function from Sklearn library that will let us achieve our goal","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nstandard_X = scaler.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling\n\nTime to do some Machine Learning\n\n<img  src=\"https://miro.medium.com/max/624/1*9AWKjTkBm-Tr6UKnQmzSsA.png\" alt=\"FDR\" width=\"600\" height=\"400\"/>\n\nFirst of all, Let's divide our dataset into Training and Testing Datapoints","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(standard_X, Y, test_size=0.2, random_state=42, shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As per our observations from our analysis, we'll gonna train this dataset over few models and compare them for our best use","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Since we have many hyperparameters that we are required to tune inoder to get best out of model, we'll gonna perform some hyperparamters tuning <br>\nWe have some tradational techniques like gridsearch and randomsearch as well as techniques based on Bayesian optimization like hyperopt <br>\nHere, We will use GridSearch <br>\n\n<img  src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQh9a3whJq8CKeHVk8_3DMA32oOTfO8Ourl3KctJUdVE030Q_4x&usqp=CAU\" alt=\"FDR\" width=\"600\" height=\"400\"/>\n\nIn simple language, GridSearch tries all possible cominations of parameters given to train model inorder to select best parameters\nFeel free to research more about these techniques. <br>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## K-Nearest Neighbors","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> * Datapoint is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors\n\n<img  src=\"https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1531424125/KNN_final_a1mrv9.png\" alt=\"FDR\" width=\"600\" height=\"400\"/>\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'n_neighbors':list(range(0, 51)),\n          'weights':['uniform', 'distance'],\n          'p':[1,2]}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> * n_neighbors :> That's our 'K' <br>\n> * weights :> Uniform gives same weight to all points while in Distance, closer neighbors of a query point will have a greater influence than neighbors which are further away <br>\n> * p:> if 1, use manhattan_distance to calculate distance , if 2 use euclidean_distance <br>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Grid Search takes lot of time. So, I'm gonna comment that section out <br>\nDon't worry ! I already ran it and recevied the Best Hyper Parameters for KNN at the time of this kernel creation that i'll gonna print out <br>\nFeel free to experiment with parameters and re-run the grid search by yourself to obtain new Hyper - Parameters. <br>\nWho knows they may give better results than defined by mine :) !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"knn = KNeighborsClassifier()\nknn_grid_cv = GridSearchCV(knn, param_grid=params, cv=10) \nknn_grid_cv.fit(X_train, y_train)\nprint(\"Best Hyper Parameters:\\n\",knn_grid_cv.best_params_)\"\"\"\n\nprint(\"Best Hyper Parameters: {'n_neighbors': 50, 'p': 1, 'weights': 'uniform'}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now time to train knn model with these parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=50, p=1, weights='uniform')\nknn.fit(X_train, y_train) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similarly I'll train few more potential model that may perform well on this dataset and will compare in the end, selecting the best model for our use - case","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Random Forest","execution_count":null},{"metadata":{},"cell_type":"markdown","source":">* Consist of combination of  Multiple Decision Trees <br>\n>* Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model’s prediction <br>\n>* Based on Bagging Technique\n\n<img  src=\"https://miro.medium.com/max/592/1*i0o8mjFfCn-uD79-F1Cqkw.png\" alt=\"FDR\" width=\"600\" height=\"400\"/>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"params = { \n    'n_estimators': [10, 50, 100, 150, 200, 300, 400, 500],\n    'max_depth' : [10,20,30,40,50],\n    'criterion' : ['entropy','gini']\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> * n_estimators :> No of Decision Trees to be used\n> * max_depth :> Depth of Each Tree\n> * criterion :> Measure the quality of a split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"'''rfc_gridcv = RandomForestClassifier(random_state=42)\nrfc_gridcv = GridSearchCV(estimator=rfc_gridcv, param_grid=params, cv= 10, n_jobs = -1)\nrfc_gridcv.fit(X_train, y_train)\nprint(\"Best Hyper Parameters:\\n\",rfc_gridcv.best_params_)'''\n\nprint(\"Best Hyper Parameters:{'criterion': 'entropy', 'max_depth': 10, 'n_estimators': 100}\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now time to train rfc model (random forest) with these parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(random_state=42, n_estimators=100, max_depth= 10, criterion = 'entropy')\nrfc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":">*  Uses a Logistic Function to Model a Categorical Dependent variable\n\n<img  src=\"https://miro.medium.com/max/2400/1*RqXFpiNGwdiKBWyLJc_E7g.png\" alt=\"FDR\" width=\"600\" height=\"400\"/>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"params_for_l1 = { \n    'C' :  np.logspace(0, 4, 10),\n    'solver' : ['liblinear', 'saga']\n}\n\nparams_for_l2 = { \n    'C' :  np.logspace(0, 4, 10),\n    'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n}\n\nparams_for_elasticnet = { \n    'C' :  np.logspace(0, 4, 10),\n    'l1_ratio' : np.arange (0.1, 1.0, 0.1),\n    'solver' : ['saga']\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> * C :> Defines Strength of regularization ( smaller values specify stronger regularization )\n> * penalty :> Used to specify the norm used in the penalization\n> * solver :> Algorithm to use in the optimization problem. Different Solver supports different penalty. Hence we cane 3 cases here\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"'''logreg_with_l1_gridcv = LogisticRegression(penalty = 'l1')\nlogreg_with_l1_gridcv = GridSearchCV(estimator=logreg_with_l1_gridcv, param_grid=params_for_l1, cv= 10, n_jobs = -1)\nlogreg_with_l1_gridcv.fit(X_train, y_train)\nprint(\"Best Hyper Parameters:\\n\",logreg_with_l1_gridcv.best_params_)'''\n\nprint(\"Best Hyper Parameters:{'C': 166.81005372000593, 'solver': 'saga'}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg_with_l1 = LogisticRegression(penalty = 'l1', C = 166.81005372000593, solver = 'saga')\nlogreg_with_l1.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''logreg_with_l2_gridcv = LogisticRegression(penalty = 'l2')\nlogreg_with_l2_gridcv = GridSearchCV(estimator=logreg_with_l2_gridcv, param_grid=params_for_l2, cv= 10, n_jobs = -1)\nlogreg_with_l2_gridcv.fit(X_train, y_train)\nprint(\"Best Hyper Parameters:\\n\",logreg_with_l2_gridcv.best_params_)'''\n\nprint(\"Best Hyper Parameters:{'C': 1.0, 'solver': 'liblinear'}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg_with_l2 = LogisticRegression(penalty = 'l2', C = 1.0, solver = 'liblinear')\nlogreg_with_l2.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''logreg_with_elasticnet_gridcv = LogisticRegression(penalty = 'elasticnet')\nlogreg_with_elasticnet_gridcv = GridSearchCV(estimator=logreg_with_elasticnet_gridcv, param_grid=params_for_elasticnet, cv= 10, n_jobs = -1)\nlogreg_with_elasticnet_gridcv.fit(X_train, y_train)\nprint(\"Best Hyper Parameters:\\n\",logreg_with_elasticnet_gridcv.best_params_)'''\n\n\nprint(\"Best Hyper Parameters:{'C': 1291.5496650148827, 'l1_ratio': 0.6, 'solver': 'saga'}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg_with_elasticnet = LogisticRegression(penalty = 'elasticnet', C = 1291.5496650148827, l1_ratio =  0.6, solver = 'saga')\nlogreg_with_elasticnet.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Evaluation\n\nTime to evaluate to find the best model for our use-case\n\n<img  src=\"https://www.datavedas.com/wp-content/uploads/2018/05/4-MODEL-EVALUATION-AND-VALIDATION-S-1.jpg\" alt=\"FDR\" width=\"600\" height=\"400\"/>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Here We will use :\n> *  cross_val_score :> Evaluate a Score by Cross-Validation \n> * classification_report :> Text Report showing the Main Classification Metrics. We'll gonna draw Confussion Matrix too","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"If you are wondering what's Cross - Validation is the it's :\n<br>\n<img  src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" alt=\"FDR\" width=\"600\" height=\"400\"/>\n<br>\nBasically, dataset is divided in k equal parts. Then one of that part is used as test part and other for training. <br>\nAnd this step is repeated until every K-fold serve as the test set.\n\n# Why Cross Validation?\n\n> * Well, if we use train_test_split, then we get our accuracy according to the split of the data. \n> * If we change re-split in different order, then we'll get another new accuracy ( you can try yourself too in your kernels just by changing random_state defined in train_test_split (sklearn)\n> * In order to get an exact idea of the accuracy, we use Cross Validation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Confusion Matrix\n\nHave a look at this picture to have a better understanding of confusion matrix\n\n<img  src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/1200px-Precisionrecall.svg.png\" alt=\"FDR\" width=\"600\" height=\"400\"/>\n\n\nIn our case :\n> * True Positive :  No of Cases Predicted with CVD and Actually had CVD\n> * True Negative : No of Cases Predicted with No CVD and Actually had No CVD\n> * False Positive : No of Cases Predicted with CVD and Actually had No CVD\n> * False Negative : No of Cases Predicted with No CVD and Actually had  CVD","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Evaluation for KNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = cross_val_score(knn, X_train, y_train, cv=10)\nprint('KNN Model gives an average accuracy of {0:.2f} % with minimun of {1:.2f} % and maximum of {2:.2f} % accuracy'.format(scores.mean() * 100, scores.min() * 100, scores.max() * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_hat = knn.predict(X_test)\nprint(classification_report(y_test, Y_hat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (5, 5) \nsns.heatmap(confusion_matrix(y_test, Y_hat), annot = True, linewidths=.5, cmap=\"YlGnBu\")\nplt.title('Corelation Between Features')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('True Positive Cases : {}'.format(confusion_matrix(y_test, Y_hat)[1][1]))\nprint('True Negative Cases : {}'.format(confusion_matrix(y_test, Y_hat)[0][0]))\nprint('False Positive Cases : {}'.format(confusion_matrix(y_test, Y_hat)[0][1]))\nprint('False Negative Cases : {}'.format(confusion_matrix(y_test, Y_hat)[1][0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation for Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = cross_val_score(rfc, X_train, y_train, cv=10)\nprint('Random Forest Model gives an average accuracy of {0:.2f} % with minimun of {1:.2f} % and maximum of {2:.2f} % accuracy'.format(scores.mean() * 100, scores.min() * 100, scores.max() * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_hat = rfc.predict(X_test)\nprint(classification_report(y_test, Y_hat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (5, 5) \nsns.heatmap(confusion_matrix(y_test, Y_hat), annot = True, linewidths=.5, cmap=\"YlGnBu\")\nplt.title('Corelation Between Features')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('True Positive Cases : {}'.format(confusion_matrix(y_test, Y_hat)[1][1]))\nprint('True Negative Cases : {}'.format(confusion_matrix(y_test, Y_hat)[0][0]))\nprint('False Positive Cases : {}'.format(confusion_matrix(y_test, Y_hat)[0][1]))\nprint('False Negative Cases : {}'.format(confusion_matrix(y_test, Y_hat)[1][0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation for Logistic Model with L1 Penalty","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = cross_val_score(logreg_with_l1, X_train, y_train, cv=10)\nprint('Logistic Model with L1 Penalty gives an average accuracy of {0:.2f} % with minimun of {1:.2f} % and maximum of {2:.2f} % accuracy'.format(scores.mean() * 100, scores.min() * 100, scores.max() * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_hat = logreg_with_l1.predict(X_test)\nprint(classification_report(y_test, Y_hat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (5, 5) \nsns.heatmap(confusion_matrix(y_test, Y_hat), annot = True, linewidths=.5, cmap=\"YlGnBu\")\nplt.title('Corelation Between Features')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('True Positive Cases : {}'.format(confusion_matrix(y_test, Y_hat)[1][1]))\nprint('True Negative Cases : {}'.format(confusion_matrix(y_test, Y_hat)[0][0]))\nprint('False Positive Cases : {}'.format(confusion_matrix(y_test, Y_hat)[0][1]))\nprint('False Negative Cases : {}'.format(confusion_matrix(y_test, Y_hat)[1][0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation for Logistic Model with L2 Penalty","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = cross_val_score(logreg_with_l2, X_train, y_train, cv=10)\nprint('Logistic Model with L2 Penalty gives an average accuracy of {0:.2f} % with minimun of {1:.2f} % and maximum of {2:.2f} % accuracy'.format(scores.mean() * 100, scores.min() * 100, scores.max() * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_hat = logreg_with_l2.predict(X_test)\nprint(classification_report(y_test, Y_hat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (5, 5) \nsns.heatmap(confusion_matrix(y_test, Y_hat), annot = True, linewidths=.5, cmap=\"YlGnBu\")\nplt.title('Corelation Between Features')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('True Positive Cases : {}'.format(confusion_matrix(y_test, Y_hat)[1][1]))\nprint('True Negative Cases : {}'.format(confusion_matrix(y_test, Y_hat)[0][0]))\nprint('False Positive Cases : {}'.format(confusion_matrix(y_test, Y_hat)[0][1]))\nprint('False Negative Cases : {}'.format(confusion_matrix(y_test, Y_hat)[1][0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation for Logistic Model with Elasticnet Penalty","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = cross_val_score(logreg_with_elasticnet, X_train, y_train, cv=10)\nprint('Logistic Model with Elasticnet Penalty gives an average accuracy of {0:.2f} % with minimun of {1:.2f} % and maximum of {2:.2f} % accuracy'.format(scores.mean() * 100, scores.min() * 100, scores.max() * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_hat = logreg_with_elasticnet.predict(X_test)\nprint(classification_report(y_test, Y_hat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (5, 5) \nsns.heatmap(confusion_matrix(y_test, Y_hat), annot = True, linewidths=.5, cmap=\"YlGnBu\")\nplt.title('Corelation Between Features')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('True Positive Cases : {}'.format(confusion_matrix(y_test, Y_hat)[1][1]))\nprint('True Negative Cases : {}'.format(confusion_matrix(y_test, Y_hat)[0][0]))\nprint('False Positive Cases : {}'.format(confusion_matrix(y_test, Y_hat)[0][1]))\nprint('False Negative Cases : {}'.format(confusion_matrix(y_test, Y_hat)[1][0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> * We trained KNN, RandomForest and Logistic Regression with different penality and reveived almost similar results. \n> * Here We dealing with a problem of a disease impacting a human health. So, We can't put someone's health in danger with our reckless decision \n> * We need to have an accurate model. \n> * In such cases, it's not a big threat if our model predict a Non CVD case as CVD (False Positive) because medical staff can later examine him/her.\n> * But We can't risk of predicting a CVD case as Non CVD (False Negative) because in that case we'll be jeopardizing with that person's life.\n\n## Considering False Negative as a major factor, We will choose Random Forest as our model for the probelm since it's having the fine accuracy with Lower False Negative Cases.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# End Notes\n\n> * With this we finish our analysis on this dataset. <br>\n> * I would again like to thanks [Svetlana Ulianova](https://www.kaggle.com/sulianova) for providing this dataset. <br>\n> * We analysed the dataset, explored about different Datascience Pipelines and in the end trained a predictive model. <br>\n\nIt was a nice learning experience and I hope Readers of this Kernel aquired some usefull information from this analysis. <br> <br>\nIt's my first analysis I even did and it was fun. If you like this Kernel, the way I presented insights, the analysis etc then feel free to Upvote this Kernel. It'll boost my spirits, keeping me motivated to create more Kernels like this one <br>\nAnd Everyone can't not perfect at their first attempt. I'm always open for learning. If you want to correct something, advice, wanna share new strageties or techniques then feel free to comment them out. I'll love to hear some great advices / feedback from the community. <br> <br>\nHappy Kaggling :D !","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}