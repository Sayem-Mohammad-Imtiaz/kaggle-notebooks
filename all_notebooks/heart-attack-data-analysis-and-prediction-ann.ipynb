{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **HEART ATTACK ANALYSIS AND PREDICTION**","metadata":{}},{"cell_type":"markdown","source":"![](https://source.wustl.edu/wp-content/uploads/2019/02/HeartImage-760x594.jpg)","metadata":{}},{"cell_type":"markdown","source":"# Overview","metadata":{}},{"cell_type":"markdown","source":"Heart attack or myocardial infarction according to Wikipedia,commonly occurs when blood flow decreases or stops to a part of the heart, causing damage to the heart muscle. The most common symptom is chest pain or discomfort which may travel into the shoulder, arm, back, neck or jaw.According to a medical survey in USA, every year about 647,000 people die of heart attack making it the leading cause of death. According to the Centers for Disease Control and Prevention (CDC) approximately every 40 seconds an American will have a heart attack.And the scenario almost remains same in countries like India. \nThrough the analysis and visualisations in this notebook we would try to go to the rockbottom of this problem and try to figure out what are the features that determines the causes of Heart Attack. \nThe judgements produced are absolutely dependent on the information provided in the data.","metadata":{}},{"cell_type":"markdown","source":"# How will we proceed ?","metadata":{}},{"cell_type":"markdown","source":"1. **Understanding the Data**\n\n2. **EDA**\n\n3. **Model Building**\n\n4. **Model Performance**\n\n5. **Inference**\n","metadata":{}},{"cell_type":"markdown","source":"# **UNDERSTANDING THE DATA**","metadata":{}},{"cell_type":"markdown","source":"# Including Required Packages ","metadata":{}},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**READING THE DATA**","metadata":{}},{"cell_type":"code","source":"df= pd.read_csv('../input/heart-attack-analysis-prediction-dataset/heart.csv')\ndf.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So we know that there are 14 features that has been included in the dataset needed to determine Heart Attack","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**DESCRIPTION OF THE DATASET**","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let Us Know if We Have any missing values**","metadata":{}},{"cell_type":"code","source":"features_with_na=[features for features in df.columns if df[features].isnull().sum()>1]\n## 2- step print the feature name and the percentage of missing values\nfeatures_with_na","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great!!! We don't have to handle the cases for missing values !! ","metadata":{}},{"cell_type":"markdown","source":"# **EDA**","metadata":{}},{"cell_type":"markdown","source":"**Number of Numerical Variables**","metadata":{}},{"cell_type":"code","source":"numerical_features = [feature for feature in df.columns if df[feature].dtypes != 'O']\nlen(numerical_features),df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wow!! We got to know all of the features are numerical variables ! ","metadata":{}},{"cell_type":"markdown","source":"**We need to know the number of discrete variables, Let us find it out !**","metadata":{}},{"cell_type":"code","source":"discrete_feature=[feature for feature in numerical_features if len(df[feature].unique())<25]\nprint(\"Discrete Variables Count: {}\".format(len(discrete_feature)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"discrete_feature","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**LET US FIND OUT THE RELATION BETWEEN EACH OF THE DISCRETE FEATURE AND OUTPUT**","metadata":{}},{"cell_type":"code","source":"for feature in discrete_feature:\n    data=df.copy()\n    data.groupby(feature)['output'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('output')\n    plt.title(feature)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now let's deal with the Continuous Variables**","metadata":{}},{"cell_type":"code","source":"continuous_feature=[feature for feature in numerical_features if feature not in discrete_feature]\nprint(\"Continuous feature Count {}\".format(len(continuous_feature)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for feature in continuous_feature:\n    data=df.copy()\n    data[feature].hist(bins=25)\n    plt.xlabel(feature)\n    plt.ylabel(\"Count\")\n    plt.title(feature)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,8))\nsns.heatmap(df.corr(),annot=True,ax=ax)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Results against the Age**","metadata":{}},{"cell_type":"code","source":"sns.displot(x='age', hue='output', data=df, alpha=0.6)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"attack = df[df['output']==1]\nsns.displot(attack.age, kind='kde')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(attack.age, kind='ecdf')\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ranges = [0, 30, 40, 50, 60, 70, np.inf]\nlabels = ['0-30', '30-40', '40-50', '50-60', '60-70', '70+']\n\nattack['age'] = pd.cut(attack['age'], bins=ranges, labels=labels)\nattack['age'].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(attack.age)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**WE SEE THAT AGES BETWEEN 50-60 ARE THE MOST PRONE TO HEART ATTACKS**","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(8, 5))\nsns.countplot(x='sex', hue='age', data=attack, ax=ax)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"attack = df[df['output'] == 1]\nsns.displot(x='age', kind='kde', hue='sex', data=attack)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**WE NOTICE THAT MALE HAVE A HIGHER TENDENCY TO HAVE HEART ATTACK**","metadata":{}},{"cell_type":"code","source":"male_attack=attack[attack['sex']==1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(male_attack['age'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for feature in continuous_feature:\n    data=df.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature]=np.log(data[feature])\n        data.boxplot(column=feature)\n        plt.ylabel(feature)\n        plt.title(feature)\n        plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import  BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**PREPARING THE DATASET FOR MODEL**","metadata":{}},{"cell_type":"code","source":"#Creating a copy\ndata= df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nscaler = StandardScaler()\n\n# define the columns to be encoded and scaled\ncategorical_vars = ['sex','exng','caa','cp','fbs','restecg','slp','thall']\ncontinuous_vars = [\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\n\n# encoding the categorical columns\ndata = pd.get_dummies(data, columns = categorical_vars, drop_first = True)\n\nX = data.drop(['output'],axis=1)\ny = data[['output']]\n\ndata[continuous_vars] = scaler.fit_transform(X[continuous_vars])\n\n# defining the features and target\nX = data.drop(['output'],axis=1)\ny = data[['output']]\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Models**","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"lr = LogisticRegression(random_state=42)\n\nknn = KNeighborsClassifier()\npara_knn = {'n_neighbors':np.arange(1, 50)}\n\ngrid_knn = GridSearchCV(knn, param_grid=para_knn, cv=5)\n\ndt = DecisionTreeClassifier()\npara_dt = {'criterion':['gini','entropy'],'max_depth':np.arange(1, 50), 'min_samples_leaf':[1,2,4,5,10,20,30,40,80,100]}\ngrid_dt = GridSearchCV(dt, param_grid=para_dt, cv=5)\n\nrf = RandomForestClassifier()\n\n# Define the dictionary 'params_rf'\nparams_rf = {\n    'n_estimators':[100, 350, 500],\n    'min_samples_leaf':[2, 10, 30]\n}\ngrid_rf = GridSearchCV(rf, param_grid=params_rf, cv=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt = DecisionTreeClassifier(criterion='gini', max_depth=9, min_samples_leaf=10, random_state=42)\nknn = KNeighborsClassifier(n_neighbors=3)\nrf = RandomForestClassifier(n_estimators=500, min_samples_leaf=2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the list classifiers\nclassifiers = [('Logistic Regression', lr), ('K Nearest Neighbours', knn), ('Classification Tree', dt), ('Random Forest', rf)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for clf_name, clf in classifiers:    \n \n    # Fit clf to the training set\n    clf.fit(X_train, y_train)    \n   \n    # Predict y_pred\n    y_pred = clf.predict(X_test)\n    \n    # Calculate accuracy\n    accuracy = accuracy_score(y_pred, y_test) \n   \n    # Evaluate clf's accuracy on the test set\n    print('{:s} : {:.3f}'.format(clf_name, accuracy))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**WE SEE THAT LOGISTIC REGRESSION PERFORMS THE BEST**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n\nada = AdaBoostClassifier(base_estimator=rf, n_estimators=100, random_state=1)\n\nada.fit(X_train, y_train)\n\ny_pred = ada.predict(X_test)\n\naccuracy_score(y_pred, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importances = pd.Series(data=rf.feature_importances_,\n                        index= X_train.columns)\n\n# Sort importances\nimportances_sorted = importances.sort_values()\n\n# Draw a horizontal barplot of importances_sorted\nplt.figure(figsize=(10, 10))\nimportances_sorted.plot(kind='bar',color='orange')\nplt.title('Features Importances')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NEURAL NETWORK APPROACH","metadata":{}},{"cell_type":"markdown","source":"**IMPORTING THE NECESSARY LIBRARIES**","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Dense,Dropout,Flatten\nfrom tensorflow.keras.layers import MaxPooling2D,GlobalAveragePooling2D,BatchNormalization,Activation\nfrom tensorflow import keras\nimport tensorflow as tf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodel = tf.keras.Sequential()\nmodel.add(Dense(1024, input_dim=22, activation= \"relu\"))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(512, activation= \"relu\"))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(128, activation= \"relu\"))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(32, activation= \"relu\"))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1))\nmodel.summary() #Print model Summary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss= \"binary_crossentropy\" , optimizer=\"adam\", metrics=[\"accuracy\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Performance = model.fit(X_train, y_train, validation_split =0.1,epochs=30)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(X_test,y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_dpi = 50 # dots per inch .. (resolution)\nplt.figure(figsize=(400/my_dpi, 400/my_dpi), dpi = my_dpi)\nplt.plot(Performance.history['accuracy'], label='train accuracy')\nplt.plot(Performance.history['val_accuracy'], label='val accuracy')\nplt.legend()\nplt.show()\nplt.savefig('AccVal_acc')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"markdown","source":"The accuracy of the following models are \n1. Logistic Regression : 0.871\n2. K Nearest Neighbours : 0.742\n3. Classification Tree : 0.742\n4. Random Forest : 0.839\n5. Adaboost Classifier:0.806\n6. ANN : 0.780\n","metadata":{}},{"cell_type":"markdown","source":"# Acknowledgements\n","metadata":{}},{"cell_type":"markdown","source":"[Sarthak Bobde](http://https://www.kaggle.com/sarthakbobde/heart-attack-analysis-and-classifier)\n[Jędrzej Dudzicz](http://https://www.kaggle.com/jedrzejdudzicz/heart-attack-analysis-prediciton)","metadata":{}}]}