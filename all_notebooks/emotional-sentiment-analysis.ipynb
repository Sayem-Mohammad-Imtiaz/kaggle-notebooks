{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/eeg-brainwave-dataset-feeling-emotions/emotions.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\ny = le.fit_transform(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nimport xgboost as xgb\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report\nfrom sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\nfrom sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('label', axis = 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using Correlation to remove features which are highly correlated\ncorrelated_features = set()\ncorrelation_matrix = X.corr()\ncorrelation_matrix\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(correlation_matrix.columns)):\n    for j in range(i):\n        if abs(correlation_matrix.iloc[i, j]) > 0.9:\n            colname = correlation_matrix.columns[i]\n            correlated_features.add(colname)\n\n#Total number of correlated features\nprint(len(correlated_features))\n#Printing features that are correlated\nprint(correlated_features)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Droping columns that are correlated\nX.drop(labels=correlated_features, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape\n# We are left with 632 columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_array = np.array(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_array, y, test_size = 0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#PCA\npca_result = PCA(n_components=25)\npca_result.fit_transform(X)\nfor index, var in enumerate(pca_result.explained_variance_ratio_):\n    print(\"Explained Variance ratio by Principal Component \", (index+1), \" : \", var)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Voting Classifier Ensemble Technique\nclf1 = LogisticRegression(multi_class='multinomial', random_state=1)\nclf2 = RandomForestClassifier(n_estimators=50, random_state=1)\nclf3 = GaussianNB()\n\neclf1 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\neclf1 = eclf1.fit(X_train, y_train)\nvc_eclf1_y_pred = eclf1.predict(X_test)\nprint(\"Accuracy VC_eclf1:\",accuracy_score(y_test,vc_eclf1_y_pred))\n\n\neclf2 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],voting='soft')\neclf2 = eclf2.fit(X, y)\nvc_eclf2_y_pred = eclf2.predict(X_test)\nprint(\"Accuracy VC_eclf2:\",accuracy_score(y_test,vc_eclf2_y_pred))\n\neclf3 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],voting='soft', weights=[2,1,1],flatten_transform=True)\neclf3 = eclf3.fit(X, y)\nvc_eclf3_y_pred = eclf3.predict(X_test)\nprint(\"Accuracy VC_eclf3:\",accuracy_score(y_test,vc_eclf3_y_pred))\n\neclf4 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],voting='soft', weights=[1,2,1],flatten_transform=True)\neclf4 = eclf4.fit(X, y)\nvc_eclf4_y_pred = eclf4.predict(X_test)\nprint(\"Accuracy VC_eclf4:\",accuracy_score(y_test,vc_eclf4_y_pred))\n\neclf5 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],voting='soft', weights=[1,1,2],flatten_transform=True)\neclf5 = eclf5.fit(X, y)\nvc_eclf5_y_pred = eclf5.predict(X_test)\nprint(\"Accuracy VC_eclf5:\",accuracy_score(y_test,vc_eclf5_y_pred))\n\n#Random forest is giving us the maximum accuracy from pool of Logistic Regression, Random Forest and Naive Bayes.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Pipelines Creation\n    ## 1. Data Preprocessing by using Standard Scaler\n    ## 2. Reduce Dimension using PCA\n    ## 3. Apply  Classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Logistic Regression Pipeline with PCA\npipeline_lr=Pipeline([('scalar1',StandardScaler()),\n                     ('pca1',PCA(n_components=25)),\n                     ('lr_classifier',LogisticRegression(random_state=0))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Linear Support Vector Classifier Pipeline with PCA\npipeline_svc_pca=Pipeline([('scalar2',StandardScaler()),\n                     ('pca2',PCA(n_components=25)),\n                     ('svm_cl', LinearSVC())])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Random Forest Pipeline with PCA\npipeline_randomforest=Pipeline([('scalar3',StandardScaler()),\n                     ('pca3',PCA(n_components=25)),\n                     ('rf_classifier',RandomForestClassifier())])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Random Forest Pipeline without PCA\npipeline_randomforest_pca=Pipeline([('scalar3',StandardScaler()),\n                     ('rf_classifier',RandomForestClassifier())])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Linear Support Vector Classifier without PCA\nsvm_c = Pipeline(steps=[('scaler',StandardScaler()),\n                             ('svm_cl', LinearSVC())])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#XGBoost Pipeline with PCA\npl_xgb_pca = Pipeline(steps=\n                  [('pca4', PCA(n_components=25)) ,('xgboost', xgb.XGBClassifier(objective='multi:softmax'))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#XGBoost Pipeline without PCA\npl_xgb = Pipeline(steps=\n                  [('xgboost', xgb.XGBClassifier(objective='multi:softmax'))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipelines = [pipeline_lr, pipeline_svc_pca, pipeline_randomforest, pipeline_randomforest_pca,  svm_c, pl_xgb_pca, pl_xgb ]\n\nbest_accuracy=0.0\nbest_classifier=0\nbest_pipeline=\"\"\n\npipe_dict = {0: 'Logistic Regression with PCA', 1: 'Support Vector with PCA', 2: 'RandomForest with PCA', 3: 'RandomForest without PCA', 4: 'Support Vector without PCA', 5: 'XGBoost with PCA', 6: 'XGBoost without PCA'}\n\n# Fit the pipelines\nfor pipe in pipelines:\n    pipe.fit(X_train, y_train)\n\nfor i,model in enumerate(pipelines):\n    print(\"{} Test Accuracy: {}\".format(pipe_dict[i],model.score(X_test,y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,model in enumerate(pipelines):\n    if model.score(X_test,y_test)>best_accuracy:\n        best_accuracy=model.score(X_test,y_test)\n        best_pipeline=model\n        best_classifier=i\nprint('Classifier with best accuracy: {}'.format(pipe_dict[best_classifier]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"algo = []\naccuracy = []\nfor i,model in enumerate(pipelines):\n    algo.append(pipe_dict[i])\n    accuracy.append((model.score(X_test,y_test))* 100)\naccuracy_df = pd.DataFrame(list(zip(algo,accuracy)), index  = [0,1,2,3,4,5,6], \n                                              columns =['Algorithm', 'Accuracy']) \nplt.figure(figsize=(16,6))\nsns.barplot(x=\"Algorithm\", y=\"Accuracy\", data=accuracy_df)\nplt.title('Mean Accuracy for different Algorithms', fontsize=16)\nplt.ylabel('Accuracy', fontsize=10)\nplt.xlabel('Algorithm', fontsize=10)\nplt.xticks(rotation='horizontal')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}