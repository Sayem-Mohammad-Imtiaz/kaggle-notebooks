{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Netwok Intrusion Detection System**\n\n# Analysis of the two recent dataset CICIDS2017 and CICIDS2018.\n \n# **The analysis is for the FYP(NIDS) from University of Engineering And Technology Mardan**\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing the libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# > Analysis of the fisrt csv file of both the datasets\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df2017 = pd.read_csv('../input/cicids2017/Friday-WorkingHours-Afternoon-DDos.csv')\ndf2018 = pd.read_csv('../input/ids-intrusion-csv/02-14-2018.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# First To see the number of column in both datasets\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp, No_Col_2017 = df2017.shape\nprint(f'There are {No_Col_2017} columns in CICIDS2017')\ntemp, No_Col_2018 = df2018.shape\nprint(f'There are {No_Col_2018} columns in CICIDS2018')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lets see the fisrt 10 rows of each dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df2017.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2018.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ***To see the column of both dataset samples***"},{"metadata":{},"cell_type":"markdown","source":"# CICIDS2017 columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df2017.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CICIDS2018 columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df2018.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# To see the detail information of features of the datasets samples"},{"metadata":{},"cell_type":"markdown","source":"# CICIDS2017 features"},{"metadata":{"trusted":true},"cell_type":"code","source":"df2017.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CICIDS2018 features"},{"metadata":{"trusted":true},"cell_type":"code","source":"df2018.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comparing the features "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_col_2017 = df2017.columns.tolist()\ndf_col_2018 = df2018.columns.tolist()\ni = 0\nprint(\"CICIDS2017   CICIDS2018\")\nwhile i < 80:\n    if i < 79:\n        print(f'{df_col_2017[i]}\\t\\t\\t\\t{df_col_2018[i]}')\n    else:\n        print(f'\\t\\t\\t\\t{df_col_2018[i]}')\n    i = i + 1\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We will do analysis of features in depth but before that we will see which types of attacks are there in both dataset and what are the frequencies of it"},{"metadata":{},"cell_type":"markdown","source":"# Here we will combine all the portions of dataset in to one datafram"},{"metadata":{},"cell_type":"markdown","source":"# 1. CICIDS2017"},{"metadata":{"trusted":true},"cell_type":"code","source":"d0_2017 = df2017 #the first row portion is already imported so we will just copy that\nd1_2017= pd.read_csv('../input/cicids2017/Friday-WorkingHours-Afternoon-PortScan.csv')\nd2_2017= pd.read_csv('../input/cicids2017/Friday-WorkingHours-Morning.csv')\nd3_2017= pd.read_csv('../input/cicids2017/Monday-WorkingHours.csv')\nd4_2017= pd.read_csv('../input/cicids2017/Thursday-WorkingHours-Afternoon-Infilteration.csv')\nd5_2017= pd.read_csv('../input/cicids2017/Thursday-WorkingHours-Morning-WebAttacks.csv')\nd6_2017= pd.read_csv('../input/cicids2017/Tuesday-WorkingHours.csv')\nd7_2017= pd.read_csv('../input/cicids2017/Wednesday-workingHours.csv')\n\n# And now cmbining the datasets\ndf2017 = pd.concat([d0_2017, d1_2017, d2_2017, d3_2017, d4_2017, d5_2017, d6_2017, d7_2017], ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# No of rows and columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df2017.shape ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. CICIDS2018"},{"metadata":{"trusted":true},"cell_type":"code","source":"# d0_2018 = df2018 #the first row portion is already imported so we will just copy that\n# dtypes_of_df2018 = df2018.dtypes.to_dict()\n# d1_2018= pd.read_csv('../input/ids-intrusion-csv/02-15-2018.csv', error_bad_lines=False, index_col=False, dtype=dtypes_of_df2018)\n# d2_2018= pd.read_csv('../input/ids-intrusion-csv/02-16-2018.csv', error_bad_lines=False, index_col=False, dtype=dtypes_of_df2018)\n# d3_2018= pd.read_csv('../input/ids-intrusion-csv/02-20-2018.csv', error_bad_lines=False, index_col=False, dtype=dtypes_of_df2018)\n# d4_2018= pd.read_csv('../input/ids-intrusion-csv/02-21-2018.csv', error_bad_lines=False, index_col=False, dtype=dtypes_of_df2018)\n# d5_2018= pd.read_csv('../input/ids-intrusion-csv/02-22-2018.csv', error_bad_lines=False, index_col=False, dtype=dtypes_of_df2018)\n# d6_2018= pd.read_csv('../input/ids-intrusion-csv/02-23-2018.csv', error_bad_lines=False, index_col=False, dtype=dtypes_of_df2018)\n# d7_2018= pd.read_csv('../input/ids-intrusion-csv/02-28-2018.csv', error_bad_lines=False, index_col=False, dtype=dtypes_of_df2018)\n# d8_2018= pd.read_csv('../input/ids-intrusion-csv/03-01-2018.csv', error_bad_lines=False, index_col=False, dtype=dtypes_of_df2018)\n# d9_2018= pd.read_csv('../input/ids-intrusion-csv/03-02-2018.csv', error_bad_lines=False, index_col=False, dtype=dtypes_of_df2018)\n\n# # And now cmbining the datasets\n# df2018 = pd.concat([d0_2018, d1_2018, d2_2018, d3_2018, d4_2018, d5_2018, d6_2018, d7_2018, d8_2018, d9_2018], ignore_index=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}