{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"**Factorization Machines**\n\nThe purpose of this notebook is to illustrate a lighter version of Factorization Machines (inspired by the LightFM paper). We use Tensorflow 2.X here, but the same can be replicated using other deep learning frameworks as well.\n\nThis notebook is a continuation of my previous notebook where we explored three alternate ways of doing Matrix Factorization. You can refer to that notebook here:\nhttps://www.kaggle.com/sandy1112/book-reco-mf-using-svd-als-and-deep-learning\n\nOne of the shortcomings of MF based approaches is that even if you have other features, you cannot use them in an MF set up. This is where hybrid set up like Factorization Machines come into play. The original Factorization Machines paper can be reviewed here and a typical data structure for FM is shown below:\nhttps://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf\n\nIn this notebook, we will try and create a lighter version of FM inspired by the LightFM paper, that can be found here:\nhttps://arxiv.org/pdf/1507.08439.pdf\n\nOther Credits:\n* https://www.kaggle.com/rajmehra03/cf-based-recsys-by-low-rank-matrix-factorization\n* https://github.com/jeffheaton/t81_558_deep_learning"},{"metadata":{},"cell_type":"markdown","source":"This is an ongoing work. I will add more details to it. Please upvote if you find it helpful.\n\nAlso, the results are not 100% replicable because I have not seeded everything. so results may vary from one run to the other. However, the core essence of the notebook remains the same."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from IPython.display import Image,display\nImage(\"../input/fm-diag/FM-diag.PNG\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport glob\nfrom scipy.sparse.linalg import svds\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nimport matplotlib.image as mpimgimport \nimport sys\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")\nfrom statistics import mean\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom random import shuffle  \nfrom zipfile import ZipFile\n##Deep Learning specific stuff\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense , Concatenate,Add\nfrom tensorflow.keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.utils import model_to_dot\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.layers import Dropout, Flatten,Activation,Input,Embedding\nfrom tensorflow.keras.layers import  BatchNormalization\nfrom tensorflow.keras.layers import dot\nfrom tensorflow.keras.models import Model\n\n\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"book_rating = pd.DataFrame()\nfor file in glob.glob(\"../input/goodreads-book-datasets-10m/book*.csv\"):\n    df = pd.read_csv(file)\n    if book_rating.empty:\n        book_rating = df\n    else:\n        book_rating.append(df, ignore_index=True)\n\n        \nuser_rating = pd.DataFrame()\nfor file in glob.glob(\"../input/goodreads-book-datasets-10m/user_rating*.csv\"):\n    df = pd.read_csv(file)\n    if user_rating.empty:\n        user_rating = df\n    else:\n        user_rating.append(df, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"book_rating.shape,user_rating.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"book_rating.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a lot of additional meta data for books available here. Let's try and prepare few columns that we can use."},{"metadata":{"trusted":true},"cell_type":"code","source":"book_rating['Num_1star_rating']=book_rating['RatingDist1'].str.split('\\:').str[-1].str.strip()\nbook_rating['Num_2star_rating']=book_rating['RatingDist2'].str.split('\\:').str[-1].str.strip()\nbook_rating['Num_3star_rating']=book_rating['RatingDist3'].str.split('\\:').str[-1].str.strip()\nbook_rating['Num_4star_rating']=book_rating['RatingDist4'].str.split('\\:').str[-1].str.strip()\nbook_rating['Num_5star_rating']=book_rating['RatingDist5'].str.split('\\:').str[-1].str.strip()\nbook_rating['Num_1star_rating'] = book_rating['Num_1star_rating'].astype(int)\nbook_rating['Num_2star_rating'] = book_rating['Num_2star_rating'].astype(int)\nbook_rating['Num_3star_rating'] = book_rating['Num_3star_rating'].astype(int)\nbook_rating['Num_4star_rating'] = book_rating['Num_4star_rating'].astype(int)\nbook_rating['Num_5star_rating'] = book_rating['Num_5star_rating'].astype(int)\nbook_rating['Total_rating_count']= book_rating['Num_1star_rating']+book_rating['Num_2star_rating']+book_rating['Num_3star_rating']+book_rating['Num_4star_rating']+book_rating['Num_5star_rating']\nbook_rating['Pct_1Star']=book_rating['Num_1star_rating']/book_rating['Total_rating_count']\nbook_rating['Pct_2Star']=book_rating['Num_2star_rating']/book_rating['Total_rating_count']\nbook_rating['Pct_3Star']=book_rating['Num_3star_rating']/book_rating['Total_rating_count']\nbook_rating['Pct_4Star']=book_rating['Num_4star_rating']/book_rating['Total_rating_count']\nbook_rating['Pct_5Star']=book_rating['Num_5star_rating']/book_rating['Total_rating_count']\nbook_rating.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"book_rating_df = book_rating[['Name','pagesNumber','PublishMonth','PublishDay',\\\n                              'CountsOfReview','Num_1star_rating','Num_2star_rating',\\\n                              'Num_3star_rating','Num_4star_rating','Num_5star_rating']]\nscaling_cols = ['pagesNumber','PublishMonth','PublishDay',\\\n                              'CountsOfReview','Num_1star_rating','Num_2star_rating',\\\n                              'Num_3star_rating','Num_4star_rating','Num_5star_rating']\ndef mix_max_scaler(df):\n    result = df.copy()\n    for feature_name in scaling_cols:\n        max_value = df[feature_name].max()\n        min_value = df[feature_name].min()\n        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n    return result\nbook_rating_scaled = mix_max_scaler(book_rating)\nbook_rating_df = book_rating_scaled[['Name','pagesNumber','PublishMonth','PublishDay',\\\n                              'CountsOfReview','Num_1star_rating','Num_2star_rating',\\\n                              'Num_3star_rating','Num_4star_rating','Num_5star_rating']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Let's create Book_id that we can use\nbook_id_0 = book_rating_df[['Name']]\nbook_id_1 = user_rating[['Name']]\nbook_id = pd.concat([book_id_0,book_id_1],axis=0,ignore_index=True)\nbook_id.rename(columns={ book_id.columns[0]: \"Name\" }, inplace = True)\nbook_id.drop_duplicates(inplace=True)\nbook_id['Book_Id']=book_id.index.values\nbook_id.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_rating.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_rating = pd.merge(user_rating,book_id, on='Name', how='left')\nbook_rating_df = pd.merge(book_rating_df,book_id, on='Name', how='left')\nuser_rating.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_rating['Rating'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = preprocessing.LabelEncoder()\nuser_rating['Rating_numeric'] = le.fit_transform(user_rating.Rating.values)\nbook_rating_numeric = book_rating_df[['Book_Id','pagesNumber','PublishMonth','PublishDay',\\\n                              'CountsOfReview','Num_1star_rating','Num_2star_rating',\\\n                              'Num_3star_rating','Num_4star_rating','Num_5star_rating']]\nuser_rating = pd.merge(user_rating,book_rating_numeric, on='Book_Id', how='left')\nuser_rating.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_rating.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"users = user_rating.ID.unique()\nbooks = user_rating.Book_Id.unique()\n\nuserid2idx = {o:i for i,o in enumerate(users)}\nbookid2idx = {o:i for i,o in enumerate(books)}\nuser_rating['ID'] = user_rating['ID'].apply(lambda x: userid2idx[x])\nuser_rating['Book_Id'] = user_rating['Book_Id'].apply(lambda x: bookid2idx[x])\ny=user_rating['Rating_numeric']\nX=user_rating.drop(['Rating_numeric'],axis=1)\n####\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n#split = np.random.rand(len(user_rating)) < 0.8\n#train = user_rating[split]\n#valid = user_rating[~split]\nprint(X_train.shape , X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"exogenous_train = np.array(X_train[['pagesNumber','PublishMonth','PublishDay',\\\n                              'CountsOfReview','Num_1star_rating','Num_2star_rating',\\\n                              'Num_3star_rating','Num_4star_rating','Num_5star_rating']])\nexogenous_valid = np.array(X_test[['pagesNumber','PublishMonth','PublishDay',\\\n                              'CountsOfReview','Num_1star_rating','Num_2star_rating',\\\n                              'Num_3star_rating','Num_4star_rating','Num_5star_rating']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Some background on using deep neural network to recreate the LightFM**\n\nBefore we jump into creating the network to mimic Factorization machine, let's understand the building blocks in a bit more details. The most crucial piece in this is the Embedding layer. The piece that we are referring to is 'tf.keras.layers.Embedding'.\n\nPeople most often use Embedding layer in case of sequences, but that doesn't always have to be the case. Here the first argument is input_dim, i.e. input dimension size. In our case when we create embedding for users the input dimension is the number of unique users, which we encode into a lower numbered vectors (it's almost similar to dimension reduction execrcise). In our case there isn't anything for input_length as that is useful for sequences (like in NLP use cases). Think of the embedding layer as a look up table. Let's understand this in bit more details."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nembedding_layer = Embedding(input_dim=10, output_dim=4, input_length=2)\nmodel.add(embedding_layer)\nmodel.compile('adam', 'mse')\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For this neural network, which is just an embedding layer, the input is a vector of size 2. These two inputs are integer numbers from 0 to 9 (corresponding to the requested input_dim quantity of 10 values). Looking at the summary above, we see that the embedding layer has 40 parameters. This value comes from the embedded lookup table that contains four amounts (output_dim) for each of the 10 (input_dim) possible integer values for the two inputs. The output is 2 (input_length) length 4 (output_dim) vectors, resulting in a total output size of 8, which corresponds to the Output Shape given in the summary above.\n\nNow, let us query the neural network with two rows. The input is two integer values, as was specified when we created the neural network."},{"metadata":{"trusted":true},"cell_type":"code","source":"input_data = np.array([\n    [1,2]\n])\n\npred = model.predict(input_data)\n\nprint(input_data.shape)\nprint(pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we see two length-4 vectors that Tensorflow looked up for each of the input integers. Recall that Python arrays are zero-based. Tensorflow replaced the value of 1 with the second row of the 10 x 4 lookup matrix. Similarly, Tensorflow replaced the value of 2 by the third row of the lookup matrix. The following code displays the lookup matrix in its entirety. The embedding layer performs no mathematical operations other than inserting the correct row from the lookup table."},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_layer.get_weights()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The values above are random parameters that Tensorflow generated as starting points. In our case the network trains these embeddings to learn values that are able to minimize the loss function. In our case we create embeddings for users and books, and then combine them using dot product (i.e. tf.keras.layers.dot). We also add in additional feature embeddings (can be user biases, books biases or other exogenous biases) to the dot product and then try to minimize mean squared error to replicate the actual ratings.\n\nPlease note that this implementation doesn't go in depth about implementing custom loss functions that could help in Learning to Rank type of scenarios"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Image(\"../input/lightfm-pred/lightfm_rating_pred.PNG\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(dropout,latent_factors):\n    n_books=len(user_rating['Book_Id'].unique())\n    n_users=len(user_rating['ID'].unique())\n    n_latent_factors=latent_factors  # hyperparamter to deal with. \n    user_input=Input(shape=(1,),name='user_input',dtype='int64')\n    user_embedding=Embedding(n_users,n_latent_factors,name='user_embedding')(user_input)\n    user_vec =Flatten(name='FlattenUsers')(user_embedding)\n    user_vec=Dropout(dropout)(user_vec)\n    book_input=Input(shape=(1,),name='book_input',dtype='int64')\n    book_embedding=Embedding(n_books,n_latent_factors,name='book_embedding')(book_input)\n    book_vec=Flatten(name='FlattenBooks')(book_embedding)\n    book_vec=Dropout(dropout)(book_vec)\n    sim=dot([user_vec,book_vec],name='Similarity-Dot-Product',axes=1)\n    ###Exogenous Features input\n    exog_input = Input(shape=(9,),name='exogenous_input',dtype='float64')\n    exog_embedding = Embedding(9,20,name='exog_embedding')(exog_input)\n    #exog_embedding = Dense(65,activation='relu',name='exog_Dense')(exog_input)\n    exog_vec =Flatten(name='FlattenExog')(exog_embedding)   \n    ##############\n    nn_inp = Add(dtype='float64',name='Combine_inputs')([sim,exog_vec])\n    nn_inp=Dense(128,activation='relu')(nn_inp)\n    nn_inp=Dropout(dropout)(nn_inp)\n    nn_inp=Dense(64,activation='relu')(nn_inp)\n    nn_inp=BatchNormalization()(nn_inp)\n    nn_output=Dense(1,activation='relu')(nn_inp)\n    nn_model =Model([user_input, book_input,exog_input],nn_output)\n    return nn_model\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nn_model = build_model(0.4,65)\nnn_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nn_model.compile(optimizer=Adam(lr=1e-4),loss='mse')\nbatch_size=128\nepochs=15\nHistory = nn_model.fit([X_train.ID,X_train.Book_Id,exogenous_train],y_train, batch_size=batch_size,\n                              epochs =epochs, validation_data = ([X_test.ID,X_test.Book_Id,exogenous_valid],y_test),\n                              verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(History.history['loss'] , 'g')\nplt.plot(History.history['val_loss'] , 'b')\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.grid(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = nn_model.predict([X_test.ID,X_test.Book_Id,exogenous_valid])\navp = (preds,y_test)\ndf_id = pd.DataFrame(np.array(X_test.ID))\ndf_Book_id = pd.DataFrame(np.array(X_test.Book_Id))\ndf_actual_rating = pd.DataFrame(np.array(y_test))\ndf_preds = pd.DataFrame(preds)\ndfList = [df_id, df_Book_id,df_actual_rating,df_preds]  # List of your dataframes\navp = pd.concat(dfList,ignore_index=True,axis=1)\n#new_df = pd.concat([new_df,df_preds],ignore_index=True,axis=1)\navp.rename(columns={ avp.columns[0]: \"ID\" }, inplace = True)\navp.rename(columns={ avp.columns[1]: \"Book_Id\" }, inplace = True)\navp.rename(columns={ avp.columns[2]: \"Rating_numeric\" }, inplace = True)\navp.rename(columns={ avp.columns[3]: \"Pred_Rating\" }, inplace = True)\navp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avp['Pred_Rating'].max(),avp['Pred_Rating'].min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_overlap(UserId,top_recos_to_check):\n    samp_cust = avp[avp['ID']==UserId][['ID','Rating_numeric','Book_Id']]\n    samp_cust.sort_values(by='Rating_numeric', ascending=False, inplace=True)\n    available_actual_ratings = samp_cust.shape[0]\n    rows_to_fetch = min(available_actual_ratings,top_recos_to_check)\n    preds_df_sampcust = avp[avp['ID']==UserId][['ID','Pred_Rating','Book_Id']]\n    preds_df_sampcust.sort_values(by='Pred_Rating', ascending=False, inplace=True)\n    actual_rating = samp_cust.iloc[0:rows_to_fetch,:]\n    pred_rating = preds_df_sampcust.iloc[0:rows_to_fetch,:]\n    overlap = pd.Series(list(set(actual_rating.Book_Id).intersection(set(pred_rating.Book_Id))))\n    pct_overlap = (len(overlap)/rows_to_fetch)*100\n    #print(\"Percentage of overlap in top\"+str(top_recos_to_check)+\" for User ID - \"+str(UserId)+\" : \"+str(pct_overlap))\n    return pct_overlap","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_user_list = avp.ID.unique().tolist()\noverlap_summary={}\ntop_recos_to_check =10\nfor users in test_user_list:\n    overlap_summary[users]= check_overlap(users,top_recos_to_check)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_summary = sorted(overlap_summary.items(), key=lambda x: x[1], reverse=True)\nmax_overlap = np.array(list(overlap_summary.values())).max()\nmin_overlap = np.array(list(overlap_summary.values())).min()\nmean_overlap = np.array(list(overlap_summary.values())).mean()\nprint(\"Max overlap in top\" +str(top_recos_to_check)+ \" books \"+str(max_overlap))\nprint(\"Min overlap in top \"+str(top_recos_to_check)+ \" books \"+str(min_overlap))\nprint(\"Average overlap in top \"+str(top_recos_to_check)+ \" books \"+str(mean_overlap))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like we were able to do atleast as well as MF factors. The performance can further increase with better quality of data and playing around with hyper parameters"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}