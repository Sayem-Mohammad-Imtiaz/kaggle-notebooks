{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nCouple of days ago, I worked on my first data set: Titani survivability prediction. As a result, I officially dipped my toes in data science and python. I will continue to dip my toes, and perhaps, my feet with this project. The data in this project is all numeric with one column being binary information. The columns define various voice chararcteristics and the label column indicates whether it is male or female. A quick peak tells me that the data is clean and usable with no null values - perfect for a newbie. \n\nThere are 3168 observations.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom pandas import read_csv\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\n\nvdf = read_csv('/kaggle/input/voicegender/voice.csv')\nprint(vdf.shape)\nvdf.info()\n#NO null data, all numeric except label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The label containes male and female entries, which we categorize and attach label as follows:\n\n$$ y = \\begin{cases} 0 &\\mbox{if } \\text{label = 'male'}\\\\ \n                     1 & \\mbox{if } \\text{label = 'female'}\n        \\end{cases}.$$\n\nOnce, converted to a numeric label ($y$), we drop them from the dataframe ($x$).","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"print(vdf['label'].unique())\nvdf[\"label\"] = vdf[\"label\"].astype('category')\ny = vdf[\"label\"].cat.codes #save label code as y variabl\n\n#drop label from dataframe\nx = vdf.drop(['label'],axis=1)\nfeatures = x.columns.tolist() #save all the features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x[features].round(2).describe().transpose())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Is data in standard format?\n\nNo, data is not centered and scaled - identified by non-zero means. We can do that as follows:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_mean = x.mean()\nfeature_std = x.std()\n#center and scale the data\nx = (x - feature_mean)/feature_std\nprint(x[features].round(2).describe().transpose())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Is there any linear dependency between columns?\n\n\nNext, we check for linearly dependent columns. We can check the rank of a matrix, if rank < num_features, then we have colinearity. Here, the rank is 17 < 20, therefore we can remove highly correlated columns and hopefully, we can get a full column rank matrix. We check for correlation heat map, where correlation is more than 0.9. The heatmap below shows that features dfrange, meanfun, kurt, Q25, and median have high correlation values and should be dropped from the dataframe. \n\nAs a result, we have 15 features and a full column rank matrix. I found the code to drop linearly dependent columns [here](https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy.linalg import matrix_rank\nprint(matrix_rank(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_corr = 0.9 #largest acceptable correlation value\ncorr_matrix = x.corr().abs() #get absolute values for correlation\n#work with upper triangular matrix, corr_matrix is symmetric\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nsns.heatmap(upper>max_corr); #check for high collinearity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop columns/features\nto_drop = [column for column in upper.columns if any(upper[column] > max_corr)]\nx.drop(to_drop, axis=1, inplace=True)\nprint('Drop features: ', to_drop)\nprint('Rank: ', matrix_rank(x), '\\nShape: ', x.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check the new correlation matrix\ncorr_matrix = x.corr().abs();\nsns.heatmap(corr_matrix);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Is data balanced?\n\nYes, observations have 50/50 split across male and female labels.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=y); #equal counts of male and female data\nplt.xticks(np.arange(2), ('Male','Female'));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cross-validation and Model training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\n#split into training and testing data\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2,shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **SVM**\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n#create classifier objects.\nsvm = SVC(kernel='linear')\n#fit the model\nsvm.fit(x_train,y_train)\n#perform cross validation\nscores = cross_val_score(svm,x,y)#get cross validation score\n#do prediction\ny_pred = svm.predict(x_test)\nprint(\"SVM training accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), 100*scores.std()))\nprint(\"SVM prediction accuracy: %0.2f\" % accuracy_score(y_test, y_pred))\n#check confusion matrix\nconfusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\nsns.heatmap(confusion_matrix, annot=True, cmap=\"Greens\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Logistic Regression**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nLR = LogisticRegression(max_iter=200)\nLR.fit(x_train, y_train)\nscores = cross_val_score(LR,x,y)\ny_pred = svm.predict(x_test)\nprint(\"LR training accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), 100*scores.std()))\nprint(\"LR prediction accuracy: %0.2f\" % accuracy_score(y_test, y_pred))\nconfusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\nsns.heatmap(confusion_matrix, annot=True, cmap=\"Greens\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Random Forest**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nRF = RandomForestClassifier(max_depth=10)\nRF.fit(x_train, y_train)\ny_pred = RF.predict(x_test)\nscores = cross_val_score(RF,x,y)\nprint(\"RF training accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), 100*scores.std()))\nprint(\"RF prediction accuracy: %0.2f\" % accuracy_score(y_test, y_pred))\nconfusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\nsns.heatmap(confusion_matrix, annot=True, cmap=\"Greens\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Neural Network**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\n\nNN = MLPClassifier(random_state = 100,max_iter=500)\nNN.fit(x_train, y_train);\nscores = cross_val_score(NN,x,y)\ny_pred = NN.predict(x_test)\nprint(\"NN training accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\nprint(\"NN prediction accuracy: %0.2f\" % accuracy_score(y_test, y_pred))\nconfusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\nsns.heatmap(confusion_matrix, annot=True, cmap=\"Greens\");","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}