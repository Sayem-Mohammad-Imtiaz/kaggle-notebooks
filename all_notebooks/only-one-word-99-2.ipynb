{"cells":[{"metadata":{},"cell_type":"markdown","source":"<img src='https://conteudo.imguol.com.br/c/parceiros/10/2020/06/30/fake-news-1593567030729_v2_900x506.jpg' style='height:400px'>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">&nbsp;Summary:</h1>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#1\" role=\"tab\" aria-controls=\"profile\">1. Introduction<span class=\"badge badge-primary badge-pill\">1</span></a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#2\" role=\"tab\" aria-controls=\"messages\">2. Twitter Users<span class=\"badge badge-primary badge-pill\">2</span></a>\n   <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#3\" role=\"tab\" aria-controls=\"messages\">3. News Structure<span class=\"badge badge-primary badge-pill\">3</span></a>\n   <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#4\" role=\"tab\" aria-controls=\"messages\">4. Words Relevance<span class=\"badge badge-primary badge-pill\">4</span></a>    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#5\" role=\"tab\" aria-controls=\"messages\">5. Conclusion<span class=\"badge badge-primary badge-pill\">5</span></a>\n</div>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* thank you \"preston fan\" for this summary table. His profile https://www.kaggle.com/prestonfan","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n<font size=\"+3\" color=\"black\"><b>1 - Introduction</b></font><br><a id=\"1\"></a>\n<br> \n\n*  I looked at the other kernels created and I was very curious why everyone has such a high score, so I suspect that the present dataset is skewed so that results to obtain such good results without needing a more robust classification model\n\n* Based on that and to add to future work on kaggle i started i wanted to raise the main features so i can confirm my hypothesis that the dataset is skewed","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"fake = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/Fake.csv')\ntrue = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/True.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fake['text'][0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* A first characteristic that I found curious was the fact that fake news has many mentions of quotes from twitter users, to see the differentiation between true and false news, I raised the amount of quotes in false and true news","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n<font size=\"+3\" color=\"black\"><b>2 - Twitter users mentions</b></font><br><a id=\"2\"></a>\n<br> ","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import re\ndef count_twitters_user(df):\n    twitter_username_re = re.compile(r'@([A-Za-z0-9_]+)')\n    count = 0\n    list_ = []\n    for text in df['text']:\n        count += len(re.findall(twitter_username_re, text))\n    return count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import plotly.express as px\nimport plotly.graph_objects as go\n\ntwitter_users_fake_count = count_twitters_user(fake)\ntwitter_users_true_count = count_twitters_user(true)\nfig = go.Figure()\nfig.add_trace(go.Bar(x=['Fake', 'True'],\n    y=[twitter_users_fake_count, twitter_users_true_count],\n    name='Twitter user name Pattern',\n    marker_color='indianred')\n)\nfig.update_layout({\n        'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n        'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n        'title': 'unique hashtags mentions in twitters',\n})\n#fig = px.bar(y=[twitter_users_fake_count, twitter_users_true_count], x=['Fake', 'True'], title='Twitter user name Pattern')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Regarding the real news, fake news has a much greater presence of @ twitter, the sources from which the real news was collected must be those types of forums where the news mix with twitters. So this may be one of the first features that can reinforce the fact that the dataset is biased.\n\n* But perhaps still just the fact that mentions with @ of users would not be enough to affirm that the data is not suitable for identification of fake news, since any preprocessing that removes @ would be able to decrease the bias. So I went in search of another aspect like the text size, to see if there is a very discrepant difference between the texts.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"from tqdm import tqdm\ndef text_size(df):\n    sizes = []\n    for text in tqdm(df['text']):\n        len_ = len(text.split())\n        sizes.append(len_)\n    return np.array(sizes)\n\nfake_size = text_size(fake)\ntrue_size = text_size(true)\nfake['len'] = fake_size\ntrue['len'] = true_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"true.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fake['is_fake'] = 1\ntrue['is_fake'] = 0\nconcat = pd.concat([fake, true])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n<font size=\"+3\" color=\"black\"><b>3 - News Structure</b></font><br><a id=\"3\"></a>\n<br> ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"concat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fake_ = concat[concat['is_fake']==1]\ntrue_ = concat[concat['is_fake']==0]\nfig = go.Figure()\nfig.add_trace(go.Box(y=list(fake_['len']), name='Fake',\n                marker_color = 'indianred'))\nfig.add_trace(go.Box(y=list(true_['len']), name = 'Real',\n                marker_color = 'lightseagreen'))\n\nfig.update_layout({\n        'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n        'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n        'title': 'Box plot',\n})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Fake news in general has a lot more tokens than real ones, which is kind of weird assuming real news has a tendency to bring details about events to inform the reader, however as noted the fact that fakes news is a mix of twitters and news this may justify the fact that they have more words.\n\n* But would that only demonstrate the bias of the dataset? because this can be resolved given that we would only work with fake news that was less than or equal to the news with more tokens, so this problem could be managed hypothetically.\n\n* Perhaps something simple is not being observed, the presence of duplicate news !!! If this is something frequent in the data when the dataset is split, we would have samples both in the training and in the test, so it is possible to present the same samples for the model","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from hashlib import sha256\nfrom tqdm import tqdm\nlist_ = [ ]\nfor text in tqdm(concat['text']):\n    hash_ = sha256(text.encode('utf-8')).hexdigest()\n    list_.append(hash_)\nconcat['hash'] = list_\nt = concat.groupby(['hash']).size().reset_index(name='count')\nduplicate = t[t['count']>1]\nprint('there are ',duplicate.shape[0], 'duplicate texts')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There are actually duplicate news in the dataset but the ratio is not enough to allow the models to be as accurate, but it already contributes to the bias.\n\n* but let's go deeper into the data, there must be some aspect of writing for the words that allow a clear differentiation, so I will observe the presence of the tokens in the true and false news","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from tqdm import tqdm\ndef unique_tokens(df):\n    unique_tokens = set()\n    for text in tqdm(df['text']):\n        splited = text.split()\n        for token in splited:\n            unique_tokens.add(token)\n    return unique_tokens\n\nunique_tokens_fake = unique_tokens(fake)\nunique_tokens_true = unique_tokens(true)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"twitter_users_fake_count = count_twitters_user(fake)\ntwitter_users_true_count = count_twitters_user(true)\nfig = px.bar(y=[len(unique_tokens_fake), len(unique_tokens_true)], x=['Fake', 'True'], title='Unique tokens')\nfig.update_layout({\n        'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n        'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Fake news has a lot more different tokens than the real ones, this was to be expected assuming that inside the fake there are twitters where people use, abbreviations , slang word and language addictions in non-formal writing.\n\n* I believe it is interesting to observe the occurrence of words that do not exist in the English language to see the difference between true and false news.","execution_count":null},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install pyenchant\n!apt-get install libenchant1c2a -y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import enchant\ndef check_if_exist(list_):\n    d = enchant.DictWithPWL(\"en_US\", \"vocab.txt\")\n    count = 0\n    for token in tqdm(list_):\n        if not d.check(token) and not d.check(token.capitalize()):\n            count+=1\n    return count\ncount_fake = check_if_exist(unique_tokens_fake)\ncount_true = check_if_exist(unique_tokens_true)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nfig = make_subplots(rows=1, cols=2, specs=[[{\"type\": \"pie\"}, {\"type\": \"pie\"}]])\n\nfig.append_trace(go.Pie(values=[count_fake, len(unique_tokens_fake)-count_fake], \n                        labels=['Non exist', 'exist'], hole=.7, \n                        title='Fake News'), row=1, col=1)\n\nfig.append_trace(go.Pie(values=[count_true, len(unique_tokens_true)-count_true], \n                        labels=['Non exist', 'exist'], hole=.7, \n                        title='Real News'), row=1, col=2)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* More than 70% of the words in the news fakes were not found in the dictionary used for verification, it is important to make it clear that it is not a perfect dictionary but that it already brings this section that many words are really misspelled\n\n* but so far everything that has been done has been in the data without any preprocessing, so let's apply a preprocessing that clears some characters and normalizes the text so that we can compare again","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import nltk\nimport re\ntqdm.pandas()\ndef preprocess(df):\n    stopwords = nltk.corpus.stopwords.words('english')\n    df['text_pre'] = df['text']\n    df['text_pre'] = df['text_pre'].progress_apply(lambda x : x.lower())\n    df['text_pre'] = df['text_pre'].progress_apply(lambda x : x.split(\" \"))\n    df['text_pre'] = df['text_pre'].progress_apply(lambda x : [item for item in x if item not in stopwords])\n    df['text_pre'] = df['text_pre'].progress_apply(lambda x : \" \".join(x))\n#    df['text_pre'] = df['text_pre'].str.replace('@[^\\s]+', \"\")\n    df['text_pre'] = df['text_pre'].str.replace('https?:\\/\\/.*[\\r\\n]*', '')\n    df['text_pre'] = df['text_pre'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n    df['text_pre'] = df['text_pre'].str.replace('\\d+', '')\n    df['text_pre'] = df['text_pre'].str.replace('[^\\w\\s]', '')\n    return df\n\nfake = preprocess(fake)\ntrue = preprocess(true)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def unique_tokens2(df):\n    unique_tokens = set()\n    for text in tqdm(df['text_pre']):\n        splited = text.split()\n        for token in splited:\n            unique_tokens.add(token)\n    return unique_tokens\n\nunique_tokens_fake2 = unique_tokens2(fake)\nunique_tokens_true2 = unique_tokens2(true)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Bar(y=[len(unique_tokens_fake2), len(unique_tokens_true2)], \n                         x=['Fake', 'True'], \n                        marker_color='lightsalmon'\n))\nfig.update_layout({\n        'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n        'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\ncount_fake = check_if_exist(unique_tokens_fake2)\ncount_true = check_if_exist(unique_tokens_true2)\nfig = make_subplots(rows=1, cols=2, specs=[[{\"type\": \"pie\"}, {\"type\": \"pie\"}]])\n\nfig.append_trace(go.Pie(values=[count_fake, len(unique_tokens_fake2)-count_fake], \n                        labels=['Non exist', 'exist'], hole=.7, \n                        title='Fake News'), row=1, col=1)\n\nfig.append_trace(go.Pie(values=[count_true, len(unique_tokens_true2)-count_true], \n                        labels=['Non exist', 'exist'], hole=.7, \n                        title='Real News'), row=1, col=2)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* After applying the preprocessing, it only showed the difference in the aspect of writing false news in relation to the real ones\n\n* Since the words are so impactful let's see if there are any that stand out in relation to the others, raising the most relevant words of each type of news","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n<font size=\"+3\" color=\"black\"><b>4 - Words Relevance</b></font><br><a id=\"4\"></a>\n<br> ","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ndef get_top_n_words(corpus, n=None):\n    vec = CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\nlist_fake = get_top_n_words(fake['text_pre'], 25)\nlist_true = get_top_n_words(true['text_pre'], 25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"new_list_words = [ seq[0] for seq in list_fake ]\nnew_list_values = [ seq[1] for seq in list_fake ]\n\nfig = go.Figure()\nfig.add_trace(go.Bar(y=new_list_values, \n                         x=new_list_words, \n                        marker_color='lightsalmon'\n))\nfig.update_layout({\n        'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n        'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n        'title': 'Fake news frequency words'\n})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"new_list_words = [ seq[0] for seq in list_true ]\nnew_list_values = [ seq[1] for seq in list_true ]\n\nfig = go.Figure()\nfig.add_trace(go.Bar(y=new_list_values, \n                         x=new_list_words, \n                        marker_color='lightsalmon'\n))\nfig.update_layout({\n        'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n        'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n        'title': 'Real news Frequency words'\n})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The most frequent words of both types of news is not very different, only the frequency is not enough to differentiate and demonstrate the bias of the data it is necessary to establish some order of importance, and for that I will use the chi2 hypothesis test to raise the most relevant words in the dataset\n\n* but first we will see to do this in relation to the words that are misspelled to evaluate their impact on the news","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_wrong_tokens(list_):\n    d = enchant.DictWithPWL(\"en_US\", \"vocab.txt\")\n    tokens = set()\n    for token in tqdm(list_):\n        if not d.check(token) and not d.check(token.capitalize()):\n            tokens.add(token)\n    return tokens\n\ndef get_top_n_words2(corpus, n=None, vocabulary=None):\n    vec = CountVectorizer(vocabulary=vocabulary).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\nwrong = get_wrong_tokens(unique_tokens_true2)\nwrong_true = get_top_n_words2(true['text_pre'], n=100, vocabulary=wrong)\nwrong = get_wrong_tokens(unique_tokens_fake2)\nwrong_fake = get_top_n_words2(fake['text_pre'], n=100, vocabulary=wrong)\n\nnew_list_words = [ seq[0] for seq in wrong_true ]\nnew_list_values = [ seq[1] for seq in wrong_true ]\n\nfig = go.Figure()\nfig.add_trace(go.Bar(y=new_list_values, \n                         x=new_list_words, \n                        marker_color='lightsalmon'\n))\nfig.update_layout({\n        'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n        'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n        'title': 'Real chi2'\n})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"new_list_words = [ seq[0] for seq in wrong_fake ]\nnew_list_values = [ seq[1] for seq in wrong_fake ]\nfig = go.Figure()\nfig.add_trace(go.Bar(y=new_list_values, \n                         x=new_list_words, \n                        marker_color='lightsalmon'\n))\nfig.update_layout({\n        'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n        'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n        'title': 'Fake chi2'\n})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The words raised in general are acronyms that are not present in the dictionary and words that are incorrect after preprocessing\n\n* Now we will analyze chi2 in all dataset to see which words impact the data more in relation to its relevance","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"concat2 = pd.concat([fake, true])\nconcat2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_selection import chi2\n\nvectorizer = CountVectorizer()\nconcat2 = pd.concat([fake, true])\nX = vectorizer.fit_transform(concat2['text_pre'])\nchi2score = chi2(X,concat2['is_fake'])[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"wscores = dict(zip(vectorizer.get_feature_names(), chi2score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"dict_ = {k: v for k, v in sorted(wscores.items(), key=lambda item: item[1], reverse=True)}\nkeys = list(dict_.keys())\nvalues = list(dict_.values())\nfig = go.Figure()\nfig.add_trace(go.Bar(y=list(values[0:50]), \n                         x=list(keys[0:50]), \n                        marker_color='lightsalmon'\n))\nfig.update_layout({\n        'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n        'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* \"said\" e \"reuters\" are words that have a great prominence in the data set\n* To better understand how the contexts of true and false news are, I will do the modeling by topics to understand if these words of greater prominence are present in well-defined topics","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import NMF, LatentDirichletAllocation\ndef topics(model, feature_names, no_top_words):\n    dict_ = {}\n    for topic_idx, topic in enumerate(model.components_):\n        dict_[topic_idx] = [feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]\n    return dict_\nlda = LatentDirichletAllocation(random_state=42).fit(X)\ntopic_all = topics(lda, vectorizer.get_feature_names(), 15)\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"vectorizer_fake = CountVectorizer()\nvectorizer_true = CountVectorizer()\n\nX_fake = vectorizer_fake.fit_transform(fake['text_pre'])\nX_true = vectorizer_true.fit_transform(true['text_pre'])\n\nlda_fake = LatentDirichletAllocation(random_state=42, n_components=5).fit(X_fake)\nlda_true = LatentDirichletAllocation(random_state=42, n_components=5).fit(X_true)\n\ntopic_true = topics(lda_true, vectorizer_true.get_feature_names(), 15)\ntopic_fake = topics(lda_fake, vectorizer_fake.get_feature_names(), 15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS \nimport matplotlib.pyplot as plt \n  \ndef plot_clouds(dict_, title):\n    for topic, words in zip(dict_.keys(), dict_.values()):\n        cloud = \" \".join(words)\n        wordcloud = WordCloud(width = 800, height = 800, \n                        background_color ='white',  \n                        min_font_size = 10).generate(cloud) \n  \n        plt.figure(figsize = (4, 8), facecolor = None) \n        plt.imshow(wordcloud) \n        plt.axis(\"off\") \n        plt.tight_layout(pad = 0) \n        plt.title(title + ' Topics '+ str(topic))\n        plt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### Fake new topics","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plot_clouds(topic_fake, 'Fake news Topics')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Real news topics","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plot_clouds(topic_true, 'Real news Topics')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The contexts are relatively similar, it is interesting how the topics created do not portray misspelled words or twitter users, demonstrating that at least the subjects covered can be useful for an eventual identification of fake news\n\n* to analyze the words and their impact on the classifier, I will select every 10% of the total of features and evaluate if reducing the number of words the classifier loses performance","execution_count":null},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"import spacy\nfrom spacy import displacy\nnlp = spacy.load(\"en_core_web_sm\")\ndef get_ent(df):\n    vocab = set()\n    for text in tqdm(df['text']):\n        doc = nlp(text)\n        for ent in doc.ents:\n            vocab.add(ent.text)\n    return vocab\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"from sklearn.datasets import load_digits\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n\nvect = TfidfVectorizer()\nX = vect.fit_transform(concat2['text_pre'])\ny = concat2['is_fake']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def select(X, y): \n    dict_ = {}\n    for i in tqdm(range(1, 11)):\n        value = X.shape[1] * i * 0.1\n        X_new = SelectKBest(chi2, k=int(value)).fit_transform(X, y)\n        X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.33, random_state=42)\n        clf = LogisticRegression()\n        model = clf.fit(X_train, y_train)\n        predict = model.predict(X_test)\n        score = accuracy_score(y_test, predict)\n        dict_[str(int(value))] = score\n    return dict_\n\ndict_ = select(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = px.line(x=list(dict_.keys()), y=list(dict_.values()))\nfig.update_layout({\n        'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n        'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Even with only 10% of the features, the classifier already shows a result of 98%, so there must be a set of words that make identification easy, to perform a more crunchy test I will only raise one feature according to chi2 and observe how a classifier based on this word behaves\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* the selected word was 'reuters', I will make a classifier based on whether the reuters is present is real news if it is not false","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"vect = TfidfVectorizer()\nX = vect.fit_transform(concat2['text_pre'])\ny = concat2['is_fake']\n\nnew_feature = [] \ns = SelectKBest(chi2, k=1)\nX_new = s.fit_transform(X, y)\nmask = s.get_support()\nfor bool, feature in zip(mask, vect.get_feature_names()):\n    if bool:\n        new_feature.append(feature)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"new_feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"result = []\nfor text in concat2['text_pre']:\n    if 'reuters' in text:\n        result.append(0)\n    else:\n        result.append(1)\naccuracy_score(concat2['is_fake'], result)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* now lets see whats happen if remove \"reuters\" word","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"concat2['text_pred_less_reuters'] = concat2['text_pre'].apply(lambda x : x.replace('reuters', ''))\n\nvect = TfidfVectorizer()\nX = vect.fit_transform(concat2['text_pred_less_reuters'])\ny = concat2['is_fake']\n\nnew_feature = [] \ns = SelectKBest(chi2, k=1)\nX_new = s.fit_transform(X, y)\nmask = s.get_support()\nfor bool, feature in zip(mask, vect.get_feature_names()):\n    if bool:\n        new_feature.append(feature)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"new_feature","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"result = []\nfor text in concat2['text_pre']:\n    if 'said' in text:\n        result.append(0)\n    else:\n        result.append(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"accuracy_score(concat2['is_fake'], result)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* the word 'said' alone reaches 70% accuracy, showing how biased the fake news data is","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n<font size=\"+3\" color=\"black\"><b>5 - Conclusion</b></font><br><a id=\"5\"></a>\n<br> \n* The dataset has many features that point to bias towards fake news, apparently a poorly structured dataset that does not allow you to raise more consistent information in relation to real characteristics that fake news can present\n\n* The fact that fake news is mixed with twitters points out a lack of care for the data, generating a dataset that is extremely simple to be able to accept high results without much engineering.\n\n* Resulting in the end the use of a single word can allow almost 100% what is true or false news","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}