{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nInput dataset\n\nCheck the amount of data and features:"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/Churn_Modelling.csv')\nprint(\"amount of data:\",train_data.shape[0])\nprint(\"amount of feature\",train_data.shape[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check 'class imbalance':\n\n1. The graph shows that negative case(y=0) is much more than positive case. So 'Precision-recall curve' might be a more exact way to evaluate the result of algorithme, rather than 'accuracy'.\n2. For the learning model, we can change our threshold from 0.5 to the real radio of two case or we can use random forest or xgboost algorithme(collective learning)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['Exited'].astype(int).plot.hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Delete some features:\n\nWe can drop feature 'Surname', 'RowNumber', 'CustomerId'\n\nObviously, these feature can not take any influence on the final result"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_data.drop(['Surname','RowNumber','CustomerId'], axis=1)\ntrain_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check missing data:\n\nFortunately, there isn't any missing data."},{"metadata":{"trusted":true},"cell_type":"code","source":"count_miss_val = train_data.isnull().sum()\ncount_miss_val.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check the non-value feature:\n\nIt turns out there are two label-features, 'geography', 'gender'\n\nApparently, all the label-features are nominal"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.dtypes.value_counts()\ntrain_data.select_dtypes(include = ['object']).apply(pd.Series.nunique, axis = 0)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the feature 'Gender', which have 2 optional values, we can use method 'labelEncoder' to fit it as 0-1 value\n\nFor the feature Geography', which have 3 optional values, we convert categorical variable into dummy variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\nle.fit(train_data['Gender'])\ntrain_data['Gender'] = le.transform(train_data['Gender'])\ntrain_data = pd.get_dummies(train_data)\nprint(\"amount of training data: %d, amount of features:%d\"% (train_data.shape[0],train_data.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Now we can check grossly the distribution of data."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['EstimatedSalary'].describe()\ntrain_data['Balance'].describe()\ntrain_data['Age'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can calculate the correlation between 'Exited' and other features, sort the result and figure out the feature has high correlation to 'Exited'\n\nIt turns out feature 'Age', 'Geography_Germany ', 'Balance' have correlation positive with 'Exited' but 'IsActiveMember' have correlation negative."},{"metadata":{"trusted":true},"cell_type":"code","source":"correlations = train_data.corr()['Exited'].sort_values()\ncorrelations","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As a feature that has the highest correlation to feature 'Extied', we can study on 'Age' feature more deeply.\n\nFirstly, we check the distribution of value 'Age'"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(train_data['Age'], edgecolor = 'k', bins = 25)\nplt.title('Age of Client')\nplt.xlabel('Age (years)')\nplt.ylabel('Count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After the kernel density estimation, it turns out that for the old people maybe more likely classified to class 'exited'"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 6))\nsns.kdeplot(train_data.loc[train_data['Exited'] == 0, 'Age'], label = 'Exited = 0')\nsns.kdeplot(train_data.loc[train_data['Exited'] == 1, 'Age'], label = 'Exited = 1')\nplt.xlabel('Age')\nplt.ylim(0, 0.06)\nplt.ylabel('Density')\nplt.title('KDE Distribution of Ages')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Secondly, we regroup our data by age of clients"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[\"Age\"].describe()\nage_data = train_data[['Exited', 'Age']]\nage_data['Age'] = pd.cut(age_data['Age'], bins = np.linspace(18, 93,num = 15))\nage_groups = age_data.groupby('Age').mean()\nage_groups","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can check which group is most likely to be classifed as 'Exited'\n\nIt turns out that clients between 45 and 60 are most likely to exit (possibility more than 50%)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,4))\nplt.bar(age_groups.index.astype(str), 100 * age_groups['Exited'])\nplt.xticks(rotation = 45)\nplt.xlabel('Age Group (years)')\nplt.ylabel('Exited (%)')\nplt.title('Exited by Age Group')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can discover the relation between different features by 'Correlation Heatmap'\n\nIt shows that feature 'Gemmany' is highly related with feature 'balance'"},{"metadata":{"trusted":true},"cell_type":"code","source":"ext_data = train_data[['Exited', 'Age', 'Geography_Germany', 'IsActiveMember', 'Balance']]\next_data_corrs = ext_data.corr()\nplt.figure(figsize = (7,7))\nsns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.6, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In addition, we can also create some polynomial features. But created features seem to have not better relations with ours target. So, it's not very necessary to add some polynomial features into ours dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\nptrain_data = train_data[['IsActiveMember','Gender','Geography_France','Balance','Geography_Germany','Age']]\nptarget = train_data['Exited']\npoly_transformer = PolynomialFeatures(degree = 3)\npoly_transformer.fit(ptrain_data)\nptrain_data = poly_transformer.transform(ptrain_data)\npoly_features_names = poly_transformer.get_feature_names(input_features = ['IsActiveMember','Gender','Geography_France','Balance','Geography_Germany','Age'])\npoly_features = pd.DataFrame(ptrain_data, columns = poly_features_names) \npoly_features['TARGET'] = target\npoly_corrs = poly_features.corr()['TARGET'].sort_values()\nplt.figure(figsize = (10, 50))\npoly_corrs.plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a tool to adjust dataset automatically, but the result haven't any change relative to our original dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"import featuretools as ft\nauto_train_data = train_data.copy()\nes = ft.EntitySet(id = 'train_data') \nes = es.entity_from_dataframe(entity_id = 'train_data', dataframe = auto_train_data, index = 'SK_ID_CURR')\nauto_train_data, features = ft.dfs(entityset = es,target_entity='train_data',verbose=True)\nauto_train_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Input preprocessing models"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler, Imputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split our data as two separated group, training set and test set\n\nTurn our training data under standerd scale"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = train_data['Exited']\ntrain = train_data.drop(['Exited'], axis = 1)\nX_std = StandardScaler().fit_transform(train)\nX_std_train,X_std_test,y_train, y_test = train_test_split(X_std, target, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Input learning models"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost.sklearn import XGBClassifier\nfrom lightgbm.sklearn import LGBMClassifier\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I create some functions to evaluate our models"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc,confusion_matrix\ndef model_metrics(clf, X_train, X_test, y_train, y_test):    \n    y_train_pred = clf.predict(X_train)    \n    y_test_pred = clf.predict(X_test)        \n    y_train_prob = clf.predict_proba(X_train)[:,1]    \n    y_test_prob = clf.predict_proba(X_test)[:,1]\n    y_scores = clf.predict_proba(X_test)[:,-1]\n    print(classification_report(y_train,y_train_pred, target_names=['non-exited','exited']))\n    print('Accurancy:')    \n    print('Train set: ','%.4f'%accuracy_score(y_train,y_train_pred), end=' ')    \n    print('Test set: ','%.4f'%accuracy_score(y_test,y_test_pred),end=' \\n\\n')\n    acu_curve(y_test,y_scores)\ndef acu_curve(y_test,y_scores):\n    fpr,tpr,threshold = roc_curve(y_test,y_scores) \n    roc_auc = auc(fpr,tpr) \n    plt.figure()\n    lw = 2\n    plt.figure(figsize=(10,10))\n    plt.plot(fpr, tpr, color='darkorange',\n             lw=lw, label='ROC curve (area = %0.3f)' % roc_auc) \n    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fit five models: logistic regression, support vector machine, decision tree, xgboost, lightgboost, "},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(X_std_train,y_train)\nsvm = SVC(gamma='scale',probability=True)\nsvm.fit(X_std_train,y_train)\ny_scores = svm.fit(X_std_train,y_train).decision_function(X_std_test)\ntree = DecisionTreeClassifier()\ntree.fit(X_std_train,y_train)\nxgb = XGBClassifier()\nxgb.fit(X_std_train,y_train)\nlgbm = LGBMClassifier()\nlgbm = lgbm.fit(X_std_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compare the abilities of different models. We can take surface of curve ROS as most important indice.\nObviously, the model xgbooster have the best quality."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Logist regression\")\nmodel_metrics(lr,X_std_train,X_std_test,y_train,y_test)\nprint(\"Decision tree\")\nmodel_metrics(tree,X_std_train,X_std_test,y_train,y_test)\nprint(\"Svm\")\nmodel_metrics(svm,X_std_train,X_std_test,y_train,y_test)\nprint(\"Xgb\")\nmodel_metrics(xgb,X_std_train,X_std_test,y_train,y_test)\nprint(\"Lgbm\")\nmodel_metrics(lgbm,X_std_train,X_std_test,y_train,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then, we can use GridSearchCV to adjust parameters like 'learning_rate', 'max_depth', 'min_child_weight'\n'gamma', 'subsample', 'colsample_bytree'."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nparam_test1 = {\n 'max_depth':range(3,10,3),\n 'min_child_weight':range(1,6,3)\n}\ngsearch1 = GridSearchCV(estimator = XGBClassifier(  learning_rate =0.1, n_estimators=100, max_depth=5,\nmin_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', nthread=4,  scale_pos_weight=1, seed=27), \n param_grid = param_test1,     scoring='roc_auc', n_jobs=4, iid=False, cv=2)\ngsearch1.fit(X_std_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, after the grid search, we can determine the optimum parameters and fit the best model"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgc = XGBClassifier(  learning_rate =0.1, n_estimators=100, max_depth=4, min_child_weight=7, reg_alpha=0.001, reg_lambda = 0,\n                    gamma=0, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', nthread=4,  scale_pos_weight=1, seed=27)\n\nxgc.fit(X_std_train,y_train)\ny_scores = xgc.predict_proba(X_std_test)[:,-1]\nfpr,tpr,threshold = roc_curve(y_test,y_scores) \nroc_auc = auc(fpr,tpr) \nprint('Train set: ','%.4f'%accuracy_score(y_train, xgc.predict(X_std_train)   ), end=' ')    \nprint('Test set: ','%.4f'%accuracy_score(y_test, xgc.predict(X_std_test) ),end=' \\n\\n')\nprint(roc_auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sb\nconfusionMatrix = confusion_matrix(y_test,xgc.predict(X_std_test))\nsb.heatmap(confusionMatrix,annot=True,fmt='d')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}