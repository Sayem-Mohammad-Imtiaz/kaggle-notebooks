{"cells":[{"metadata":{"_uuid":"f9f7c1894996877b1a1ee515b5ef80ab35d90f32"},"cell_type":"markdown","source":"[](http://) #** Introduction** \nIn this kernel we will learn what is the supervised algorithm and how to supervised algorithm use in the kernel. \n\n1. [EDA (Exploratory Data Analysis)](#1)\n1. [MACHINE LEARNING](#2)\n    1. [KNN (K-Nearest Neighbour) Classification](#3)\n    1. [Support Vector Machine( SVM) Classification](#4)\n    1. [Naive Bayes Classification](#5)\n    1. [Decision Tree Classification](#6)\n    1. [Random Forest Classification](#7)\n    1. [Confusion Matrix, Raports & Visualizations](#8)\n    1. [Classification Raports](#10)\n    1. [Logistic Regression Classification](#11)\n   \n    \n1. [Conclusion](#13)  "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# plotly library\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b6fdcba664dcd48c44290a5c34414cc353735f2"},"cell_type":"markdown","source":"<a id=\"1\"></a> \n## EDA(Exploratory Data Analysis)"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0127eb8390637afc4cd036e7d096ed9d7dcf6bfb"},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfe52cb56d1fc1605c270cac43629f37ef5d53d2"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef08a5efa15898f3e94a3f716ab1a0d3ed24b2a3"},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"193a97f31b3f6afdbf05fc12468c5c0d4d896e45"},"cell_type":"code","source":"# I drop the unnecessery columns for my prediction\ndata = data.drop(['Unnamed: 32', 'id'], axis=1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94f0af6cf9bf2c764969e3d5500056f97dc00ce3"},"cell_type":"code","source":"data['diagnosis'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25e04121fe4e45970440b7f6ab72612e7d31d9f3"},"cell_type":"code","source":"color_list = ['red' if i == 'M' else 'blue' for i in data.loc[:,'diagnosis']]\npd.plotting.scatter_matrix(data.iloc[:, 7:13],\n                                       c=color_list,\n                                       figsize= [10,15],\n                                       diagonal='hist',\n                                       alpha=0.5,\n                                       s = 200,\n                                       marker = '.',\n                                       edgecolor= \"black\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10b5593f6520f9fe1da5edd001d4ccefa955f86a"},"cell_type":"code","source":"data['diagnosis'] = [1 if x=='M' else 0 for x in  data['diagnosis']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1414c1ca969e397cf6a3b732c12200f26f514e8"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15205cbf2a777381ab6a54d755bf336e50f787d8"},"cell_type":"code","source":"#Choosing x and y values\n\n#x is our features except diagnosis (classification columns)\n#y is diagnosis\nx_data = data.iloc[:,1:]\ny = data['diagnosis']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a32452faacc08fe905219e629e8e0f9c2e235ac"},"cell_type":"code","source":"# Normalization\nx = (x_data - np.min(x_data) / (np.max(x_data) - np.min(x_data)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc4ae2dff3c42f22a1b21d1c292e612ad4a08645"},"cell_type":"code","source":"x.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d09dca2a060af771a1149a88a0fb224e5559281"},"cell_type":"markdown","source":"<a id=\"2\"></a>\n## MACHINE LEARNING"},{"metadata":{"trusted":true,"_uuid":"b43e2f77865a5bb3f076920436585c55f3c2de93"},"cell_type":"markdown","source":"<a id=\"3\"></a>\n### KNN (K-Nearest Neighbour) Classification\n<a href=\"https://ibb.co/z8cN8yM\"><img src=\"https://i.ibb.co/KNZmNMP/10.jpg\" alt=\"10\" border=\"0\"></a>\n<a href=\"https://ibb.co/bPxnJwM\"><img src=\"https://i.ibb.co/xDyQH9t/11.jpg\" alt=\"11\" border=\"0\"></a><br /><a target='_blank' href='http://www.statewideinventory.org/ford-0-60-times/'>2012 ford taurus 0 60</a><br />"},{"metadata":{"trusted":true,"_uuid":"a47d242ff966c99e4890fd27d64141e60d9c544a"},"cell_type":"code","source":"# train test split\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 1) # 0.3 means 30% of data is splitted for testing. Remaining 70% is used to train our data\nprint('x_train shape : ', x_train.shape)\nprint('y_train shape : ', y_train.shape)\nprint('x_test shape : ', x_test.shape)\nprint('y_test shape : ', y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7cb394e517fcba10a3a65ff662948124ad39f614"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(x_train,y_train) # to train our data\npredicted_values = knn.predict(x_test)\ncorrect_values = np.array(y_test) # just to make them array\nprint('KNN (with k=3) accuracy is: ',knn.score(x_test,y_test)) # accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ccb966b7d6eba1f98e370335be75c855cc976494"},"cell_type":"code","source":"# find best n value for knn\nbest_neig= range(1,25) \ntrain_accuracy_list =[]\ntest_accuracy_list =[]\n\nfor each in best_neig:\n    knn = KNeighborsClassifier(n_neighbors =each)\n    knn.fit(x_train,  y_train)\n    train_accuracy_list.append( knn.score(x_train, y_train))    \n    test_accuracy_list.append( knn.score(x_test, y_test))    \n    \n        \nprint( 'best k for Knn : {} , best accuracy : {}'.format(test_accuracy_list.index(np.max(test_accuracy_list))+1, np.max(test_accuracy_list)))\nplt.figure(figsize=[13,8])\nplt.plot(best_neig, train_accuracy_list,label = 'Train Accuracy')\nplt.plot(best_neig, test_accuracy_list,label = 'Test Accuracy')\nplt.title('Neighbors vs accuracy ')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid()\nplt.xticks(best_neig)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50cbc8c9dcfd376d3dccd169af4ecfa4df2526b8"},"cell_type":"markdown","source":"<a id=\"3\"></a>\n## Support Vector Machine( SVM) Classification\n* Use for both classification or regression challenges\n* It is mostly useful in non-linear separation problem.\n\n<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/jbVJpgs/12.jpg\" alt=\"12\" border=\"0\"></a>"},{"metadata":{"trusted":true,"_uuid":"ea7f1a2ef970aed5ec2c3903d39b54674eca6a58"},"cell_type":"code","source":"#SVM\nfrom sklearn.svm import SVC\nsvm = SVC(random_state = 1,gamma='auto')\nsvm.fit(x_train,y_train)\nprint(\"accuracy of svm: \",svm.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a20378af11e081181d62c3f2be41420912739b2"},"cell_type":"markdown","source":"<a id=\"5\"></a>\n### Naive Bayes\n\n\n<a href=\"https://ibb.co/HG9T2gT\"><img src=\"https://i.ibb.co/440RfNR/13.jpg\" alt=\"13\" border=\"0\"></a>"},{"metadata":{"trusted":true,"_uuid":"5a55833454ed220d33426bfc4b951717eea9cac5"},"cell_type":"code","source":"#Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\nprint(\"accuracy of naive bayes: \",nb.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc80cfc57922cb7d0887821a10b4bdd5a730e4d9"},"cell_type":"markdown","source":"<a id=\"6\"></a>\n### Decision Tree Classification\n* A decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome.\n* It partitions the tree in recursively manner call recursive partitioning.\n* This flowchart-like structure helps you in decision making.\n\n<a href=\"https://ibb.co/T0kxbsq\"><img src=\"https://i.ibb.co/qgd3WvF/14.jpg\" alt=\"14\" border=\"0\"></a><br /><a target='_blank' href='https://poetandpoem.com/meaning-bonolota-sen-jibanananda-das'>jibanananda das kobita in bengali</a><br />"},{"metadata":{"trusted":true,"_uuid":"9bd8ce456cb3df3cef1f98304fa7e9580c36e62f"},"cell_type":"code","source":"#Decision Tree Classification\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\nprint(\"accuracy of Decision Tree Classification: \", dt.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"973676b6038396de75f57232ad6e1f846502e2eb"},"cell_type":"markdown","source":"<a id=\"7\"></a>\n### Random Forest Classification\n* It consists of a combination of more than one decision tree cllassifications."},{"metadata":{"trusted":true,"_uuid":"9cc43f8b23905483263eff26e2a2d8ab3ea1169c"},"cell_type":"code","source":"#Random Forest Classcification\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100 ,random_state=1)\nrf.fit(x_train,y_train)\nprint(\"accuracy of Random Forest Classicifation: \",rf.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05b70d04c2135f6703c7d50adff19eaca291cc3c"},"cell_type":"code","source":"score_list1=[]\nfor i in range(100,501,50):\n    rf2=RandomForestClassifier(n_estimators=i,random_state=1)\n    rf2.fit(x_train,y_train)\n    score_list1.append(rf2.score(x_test,y_test))\nplt.figure(figsize=(10,10))\nplt.plot(range(100,501,50),score_list1)\nplt.xlabel(\"number of estimators\")\nplt.ylabel(\"accuracy\")\nplt.grid()\nplt.show()\n\nprint(\"Maximum value of accuracy is {} \\nwhen n_estimators= {}.\".format(max(score_list1),(1+score_list1.index(max(score_list1)))*100))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24218325142cd965622f2b11cdc0acafde4c2eac"},"cell_type":"markdown","source":"<a id=\"8\"></a>\n### Confusion Matrix, Raports & Visualizations\nConfusion matrix gives the number of true and false predicitons in our classificaiton. It is more reliable than accuracy. \nHere,\n\n* y_pred: results that we predict.\n* y_test: our real values.\n* TP: I predicted True and the label True\n* TN: I predicted False and the label False\n* FP: I predicted True but the labes False \n* FN: I predicten False but the labels True "},{"metadata":{"trusted":true,"_uuid":"3d73779674586ed508532fb98205d8f368f46b9a"},"cell_type":"code","source":"#Confusion matrix of RFC\ny_pred=rf.predict(x_test)\ny_true=y_test\n\n#cm\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_true,y_pred)\n\n#cm visualization\nf,ax=plt.subplots(figsize=(8,8))\nsns.heatmap(cm,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"predicted value\")\nplt.ylabel(\"real value\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17b0ad2f741358b5d1ce14384ad84362ceb58e9d"},"cell_type":"markdown","source":"We know that our accuracy is 0.9590, which is the best result, when number of estimators is 200 . But here we predicted\n\n* TP: 57\n* TN: 106\n* FP: 2\n* FN:6\n"},{"metadata":{"trusted":true,"_uuid":"b0fa071091c137c7f1b7c2768275fe7ca9b6ac45"},"cell_type":"code","source":"##Confusion matrix of KNN\ny_pred1=knn.predict(x_test)\ny_true=y_test\n#cm\ncm1=confusion_matrix(y_true,y_pred1)\n\n#cm visualization\nf,ax=plt.subplots(figsize=(8,8))\nsns.heatmap(cm1,annot=True,linewidths=0.5,linecolor=\"blue\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"predicted value\")\nplt.ylabel(\"real value\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc268419e4ad1c18017a8cc95ddfadb438aebd5b"},"cell_type":"markdown","source":"We know that our accuracy is 0.9356, which is the best result, . But here we predicted\n\n* TP: 104\n* TN: 4\n* FP: 11\n* FN:52"},{"metadata":{"trusted":true,"_uuid":"cd715746ccc5b03d373b16ca2f1c4b20b46feccf"},"cell_type":"code","source":"#Confusion matrix of Decision Tree Classf.\ny_pred2=dt.predict(x_test)\ny_true=y_test\n#cm\ncm2=confusion_matrix(y_true,y_pred2)\n\n#cm visualization\nf,ax=plt.subplots(figsize=(8,8))\nsns.heatmap(cm2,annot=True,linewidths=0.5,linecolor=\"green\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"predicted value\")\nplt.ylabel(\"real value\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a8513caf92cf9850ec83893add43272056e8703"},"cell_type":"markdown","source":"We know that our accuracy is 0.9649, which is the best result. But here we predicted\n\n* TP: 108\n* TN: 0\n* FP: 6\n* FN:57"},{"metadata":{"trusted":true,"_uuid":"239339ddda3efbb0438fbd670034e466958180cb"},"cell_type":"code","source":"y_pred3=svm.predict(x_test)\ny_true=y_test\n#cm\ncm3=confusion_matrix(y_true,y_pred3)\n\n#cm visualization\nf,ax=plt.subplots(figsize=(8,8))\nsns.heatmap(cm3,annot=True,linewidths=0.5,linecolor=\"green\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"predicted value\")\nplt.ylabel(\"real value\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc29c506769dde3f41743fc621fbb58049fb3e29"},"cell_type":"markdown","source":"We know that our accuracy is 0.9649, which is the best result. But here we predicted\n\n* TP: 108\n* TN: 0\n* FP: 63\n* FN:0"},{"metadata":{"trusted":true,"_uuid":"43d80609d14b187c15987a97d9500dbabf411c55"},"cell_type":"code","source":"y_pred4=nb.predict(x_test)\ny_true=y_test\n#cm\ncm4=confusion_matrix(y_true,y_pred4)\n\n#cm visualization\nf,ax=plt.subplots(figsize=(8,8))\nsns.heatmap(cm4,annot=True,linewidths=0.5,linecolor=\"green\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"predicted value\")\nplt.ylabel(\"real value\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df14fa07afa0b030fe0b8dfc58d4c47fa789bc12"},"cell_type":"markdown","source":"We know that our accuracy is 0.9473, which is the best result. But here we predicted\n\n* TP: 104\n* TN: 4\n* FP: 5\n* FN:58"},{"metadata":{"trusted":true,"_uuid":"d73a13aac4203d92e135a11dfab2536ac22e0b13"},"cell_type":"code","source":"dictionary={\"model\":[\"KNN\",\"SVM\",\"NB\",\"DT\",\"RF\"],\"score\":[knn.score(x_test,y_test),svm.score(x_test,y_test),nb.score(x_test,y_test),dt.score(x_test,y_test),rf.score(x_test,y_test)]}\ndf1=pd.DataFrame(dictionary)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4b2e780c105f02f98f3871deb6677394c75e96c"},"cell_type":"code","source":"#sort the values of data \nnew_index5=df1.score.sort_values(ascending=False).index.values\nsorted_data5=df1.reindex(new_index5)\n\n# create trace1 \ntrace1 = go.Bar(\n                x = sorted_data5.model,\n                y = sorted_data5.score,\n                name = \"score\",\n                marker = dict(color = 'rgba(200, 125, 200, 0.5)',\n                             line=dict(color='rgb(0,0,0)',width=1.5)),\n                text = sorted_data5.model)\ndat = [trace1]\nlayout = go.Layout(barmode = \"group\",title= 'Scores of Classifications')\nfig = go.Figure(data = dat, layout = layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50dc6d2925baf4915d925d95f0b0509ad1e761f3"},"cell_type":"markdown","source":"<a id=\"10\"></a>\n### Classification Report"},{"metadata":{"trusted":true,"_uuid":"f86d4e845bdbad39d2251da01d84333f0ab32c20"},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint('Classification report: \\n',classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b2146b0fd7dc1b451f68c6a9f1631f8a611aaa59"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"61642e9c052f2249235a90deda43254747404d30"},"cell_type":"markdown","source":"<a id=\"11\"></a>\n### Logistic Regression"},{"metadata":{"trusted":true,"_uuid":"e6407d07df03744d982751f3c90dcf52fbaf68a4"},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 1)\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(x_train,y_train)\ny_pred = logreg.predict(x_test)\nprint('logistic regression score: ', logreg.score(x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76f50e6825497fa30c2d569fe578d12227761336"},"cell_type":"markdown","source":"<a id=\"13\"></a>\n## Conclusion\n* It seems that Decision Tree Classification is more effective which can also be seen from accuracy scores"},{"metadata":{"_uuid":"01d2aedde71f9920536d5fcea6b8dd7f1ae438a6"},"cell_type":"markdown","source":"Reference :\n* https://www.kaggle.com/sibelkcansu/machine-learning-classification,\n* https://medium.com/@hashinclude/knn-kth-nearest-neighbour-algorit-4b83c6fa31bf,\n* https://www.slideshare.net/tilanigunawardena/k-nearest-neighbors,\n* https://www.slideshare.net/gladysCJ/lesson-71-naive-bayes-classifier,\n* https://www.datacamp.com/community/tutorials/decision-tree-classification-python,\n* https://medium.com/machine-learning-bites/machine-learning-decision-tree-classifier-9eb67cad263e"},{"metadata":{"trusted":true,"_uuid":"0ddd851f2e822dc7cbb497a0f578c80e74f44076"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72811718121d2ebec124d789c87b9dfa4b50179c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}