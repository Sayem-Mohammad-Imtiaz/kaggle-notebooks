{"cells":[{"metadata":{},"cell_type":"markdown","source":"\n## Introduction\n\nI imported the clean biorxiv file from the data conversion and cleaning kernel by xhlulu. https://www.kaggle.com/xhlulu/cord-19-eda-parse-json-and-generate-clean-csv.\n\n## Word2Vec\nWord2Vec is a method to represent words in a numerical - vector format such that words that are closely related to each other are close to each other in numeric vector space. This method was developed by Thomas Mikolov in 2013 at Google.\n\nEach word in the corpus is modeled against surrounding words, in such a way that the surrounding words get maximum probabilities of occurence. The mapping that allows this to happen , becomes the word2vec representation of the word. The number of surrounding words can be chosen through a model parameter called \"window size\". The length of the vector representation is chosen using the parameter 'size'.\n\nIn this notebook, the library gensim is used to construct the word2vec models\n\n## Reading Comprehension\n\nReading comprehension is a way to answer questions with respect to the given text. This is same as the English tests we used to get back in school, where a paragraph would be given about a certain subject and related questions are asked.\n\nOne of the naive ways to answer the questions was to look at the question and to find the paragraph/sentence that closely resembled the question semantically. We are going to do that here, using word2vec representations\n\n## Library Load\n\nIn the following code snippet, we look at the cleaned csv data and take a random sample of 300 articles for the sake of memory."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd #\nimport numpy as np\nimport os\nimport re\nimport gensim\nimport spacy\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\n\nbiorxiv = pd.read_csv(\"/kaggle/input/clean-csv/biorxiv_clean.csv\")\nbiorxiv.shape\nbiorxiv.head()\n\nbiorxiv = biorxiv[['paper_id','title','text']].dropna().drop_duplicates()\npmc = pd.read_csv('/kaggle/input/clean-csv-new/clean_pmc.csv')\npmc = pmc[['paper_id','title','text']].dropna().drop_duplicates()\n\nbiorxiv = pd.concat([biorxiv,pmc]).drop_duplicates()\n\nbiorxiv = biorxiv.sample(n=300)\n\nbiorxiv.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sentence breakdown\n\nAs we look for answers in sentences,we break down each article into its sentence constituents."},{"metadata":{"trusted":true},"cell_type":"code","source":"biorxiv_split = pd.concat([pd.Series(row['paper_id'], row['text'].split('.')) for _, row in biorxiv.iterrows()]).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"biorxiv_split.columns = ['sentences','paper_id']\nbiorxiv_split = biorxiv_split.replace('\\n','', regex=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To ease text processing for english words, the spacy's english module library is used. This helps in tackling tokenization"},{"metadata":{"trusted":true},"cell_type":"code","source":"! python -m spacy download en_core_web_sm\nimport spacy\nimport en_core_web_sm\nnlp = en_core_web_sm.load()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before feeding the data into the word2vector (skip-gram) model, the text data is converted to a list object that is passed. The following code snippet removes stopwords, punctuations and stems words so as to remove noise."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nstemmer = SnowballStemmer(\"english\")\n\ndef text_clean_tokenize(article_data):\n    \n    review_lines = list()\n\n    lines = article_data['text'].values.astype(str).tolist()\n\n    for line in lines:\n        tokens = word_tokenize(line)\n        tokens = [w.lower() for w in tokens]\n        table = str.maketrans('','',string.punctuation)\n        stripped = [w.translate(table) for w in tokens]\n        # remove remaining tokens that are not alphabetic\n        words = [word for word in stripped if word.isalpha()]\n        stop_words = set(stopwords.words('english'))\n        words = [w for w in words if not w in stop_words]\n        words = [stemmer.stem(w) for w in words]\n\n        review_lines.append(words)\n    return(review_lines)\n    \n    \nreview_lines = text_clean_tokenize(biorxiv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The resulting list is then passed to the `gensim.models.Word2Vec()` function. Each word is represented by a vector that is 1000 elements long.And at a time, four words surrounding the context word is used to train the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"model =  gensim.models.Word2Vec(sentences = review_lines,\n                               size=1000,\n                               window=2,\n                               workers=4,\n                               min_count=2,\n                               seed=42,\n                               iter= 50)\n\nmodel.save(\"word2vec.model\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After the numeric vector representation of each word is obtained, these are used to create numeric representations of papers, sentence-wise. For each paper, the word2vec representations of each constituent words is found and the word2vec representation of each sentence is found by averaging."},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nnlp = en_core_web_sm.load()\ndef tokenize(sent):\n    doc = nlp.tokenizer(sent)\n    return [token.lower_ for token in doc if not token.is_punct]\n\nnew_df = (biorxiv_split['sentences'].apply(tokenize).apply(pd.Series))\n\nnew_df = new_df.stack()\nnew_df = (new_df.reset_index(level=0)\n                .set_index('level_0')\n                .rename(columns={0: 'word'}))\n\nnew_df = new_df.join(biorxiv_split,how='left')\n\nnew_df = new_df[['word','paper_id','sentences']]\nword_list = list(model.wv.vocab)\nvectors = model.wv[word_list]\nvectors_df = pd.DataFrame(vectors)\nvectors_df['word'] = word_list\nmerged_frame = pd.merge(vectors_df, new_df, on='word')\nmerged_frame_rolled_up = merged_frame.drop('word',axis=1).groupby(['paper_id','sentences']).mean().reset_index()\ndel merged_frame\ndel new_df\ndel vectors","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Questions\nWe get the list of questions as mentioned by the providers of this dataset. The questions are stored in a dataframe format."},{"metadata":{"trusted":true},"cell_type":"code","source":"questions = {\n    'questions' : [\"What is known about transmission, incubation, and environmental stability of COVID?\",\n                \"What do we know about COVID risk factors?\",\"What do we know about virus genetics, origin, and evolution of COVID?\",\"What do we know about vaccines and therapeutics for COVID?\"]\n}\nquestions = pd.DataFrame(questions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After we get the questions, each question is converted to its word2vec representation."},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df = (questions['questions'].apply(tokenize).apply(pd.Series))\n\nnew_df = new_df.stack()\nnew_df = (new_df.reset_index(level=0)\n                .set_index('level_0')\n                .rename(columns={0: 'word'}))\n\nnew_df = new_df.join(questions,how='left')\n\nnew_df = new_df[['word','questions']]\nword_list = list(model.wv.vocab)\nvectors = model.wv[word_list]\nvectors_df = pd.DataFrame(vectors)\nvectors_df['word'] = word_list\nmerged_frame = pd.merge(vectors_df, new_df, on='word')\nquestion2vec = merged_frame.drop('word',axis=1).groupby(['questions']).mean().reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For each question, the cosine similarity is calculated against all the sentences. The sentences with top 10 scores are printed as the answers."},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy import dot\nfrom numpy.linalg import norm\n\n\nfor i in range(len(question2vec)):\n    tmp = question2vec.iloc[[i]]\n    tmp = tmp.drop('questions',axis=1)\n    a = np.array(tmp.values)\n    list_of_scores = []\n    for j in range(len(merged_frame_rolled_up)):\n        tmp_ = merged_frame_rolled_up.iloc[[j]]\n        tmp_ = tmp_.drop(['paper_id','sentences'],axis=1)\n        b = np.array(tmp_.values)\n        b = b.T\n        cos_sim = dot(a, b)/(norm(a)*norm(b))\n        list_of_scores.append(float(cos_sim))\n    df_answer = pd.DataFrame()\n    df_answer['sentence'] = merged_frame_rolled_up['sentences'].tolist()\n    df_answer['scores'] = list_of_scores\n    df_answer['question'] = question2vec.iloc[i]['questions']\n    df_answer.sort_values(by='scores',ascending=False,inplace=True)\n    print('---------------------------- \\n')\n    print('\\n Answers for question: \\n')\n    print(question2vec.iloc[i]['questions'])\n    print(df_answer.head(10)['sentence'].values)\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Changes\n* Using more advanced techniques to imitate a Q & A bot.\n* Using Decoder & Encoder methodologies to get results in form of an answer. \n* Getting a single paragraph answer with sentences that are connected to each other.\n\nThanks for reading!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}