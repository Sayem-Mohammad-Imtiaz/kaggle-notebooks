{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install & import libraries\n\n!pip install efficientnet_pytorch\n!pip install \"../input/keras-application/Keras_Applications-1.0.8-py3-none-any.whl\"\n!pip install \"../input/efficientnet111/efficientnet-1.1.1-py3-none-any.whl\"\n!pip install \"../input/pycocotools/pycocotools-2.0-cp37-cp37m-linux_x86_64.whl\"\n!pip install \"../input/hpapytorchzoozip/pytorch_zoo-master\"\n!pip install \"../input/tfexplainforoffline/tf_explain-0.2.1-py3-none-any.whl\"\n\nimport numpy as np \nimport pandas as pd\nimport os, gc, cv2\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as nnf\nimport sklearn\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport gc\n\nfrom tqdm.notebook import tqdm\n\nfrom efficientnet_pytorch import EfficientNet","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":1.838023,"end_time":"2021-01-31T14:33:44.653299","exception":false,"start_time":"2021-01-31T14:33:42.815276","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if images can be loaded \n# images\nred = np.load('../input/segmented-train/red/000004_red.npy')\nblue = np.load('../input/segmented-train/blue/000004_blue.npy')\nyellow = np.load('../input/segmented-train/yellow/000004_yellow.npy')\ngreen = np.load('../input/segmented-train/green/000004_green.npy')\n\nplt.imshow(np.stack((red, blue, yellow), axis=2))\n\n# labels\ndf_labels = pd.read_csv('../input/segmented-train/labels.csv')\ndf_labels.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set hyper parameters\nNUM_CL = 19\n\nBATCH = 16\nEPOCHS = 3 # max 5\n\nLR = 0.0001\nIM_SIZE = 256\n\n# Rotating flag\n# True: train a pretrained model with additonal multi-label dataset\n# False: train a model with single-label dataset \nROT = True \n\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nPATH = '/kaggle/input/'\n\nTRAIN_DIR = PATH + 'segmented-train/'\nTRAIN_multi_DIR = PATH + 'segmented-multi-60000/'    ","metadata":{"papermill":{"duration":0.428918,"end_time":"2021-01-31T14:33:45.098207","exception":false,"start_time":"2021-01-31T14:33:44.669289","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Helper functions***","metadata":{}},{"cell_type":"code","source":"# Extend the size of an image by padding\ndef add_margin(image, size):\n    '''\n    Extend the size of an image by padding \n    Height * Width * Channel -> size * size * Channel\n    '''\n    H, W, C = image.shape\n    pad_H1 = (size - H)//2\n    pad_H2 = pad_H1 + (size - H)%2\n    pad_W1 = (size - W)//2\n    pad_W2 = pad_W1 + (size - W)%2\n    \n    return np.pad(image,[(pad_H1, pad_H2),(pad_W1, pad_W2),(0,0)], 'constant')\n\n# Make the image square by padding\ndef resize_to_square(image):\n    '''\n    Extend the size of an image by padding \n    Height * Width * Channel -> max(H, W) * max(H, W) * Channel\n    '''\n    H, W, C = image.shape\n    size = max(H, W)    \n    return add_margin(image, size)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_labels = pd.read_csv('../input/segmented-train/labels.csv', index_col=0)\nlist_IDs = df_labels.index.tolist()\n\n\ndf_labels_multi = pd.read_csv('../input/segmented-multi-60000/labels.csv', index_col=0)\nlist_IDs_multi = df_labels_multi.index.tolist()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataset class for cell-level classification\n# For training dataset\nclass GetData_single_cell(Dataset):\n    def __init__(self, path, list_IDs, df_labels, img_size, Transform='None'):\n        self.path = path\n        self.list_IDs = list_IDs\n        self.labels = df_labels\n        self.img_size = img_size        \n        self.transform = Transform\n        \n    def __len__(self):\n        return len(self.list_IDs)    \n    \n    def __getitem__(self, index):\n        ID = self.list_IDs[index]   \n                        \n        red = np.load(self.path + \"red/\" + str(ID).zfill(6) + '_red.npy')\n        blue = np.load(self.path + \"blue/\" + str(ID).zfill(6) + '_blue.npy')\n        yellow = np.load(self.path + \"yellow/\" + str(ID).zfill(6) + '_yellow.npy')\n        green = np.load(self.path + \"green/\" + str(ID).zfill(6) + '_green.npy')\n        \n        img = np.dstack((red, blue, yellow, green))\n\n        img = resize_to_square(img)\n        img = cv2.resize(img, (self.img_size, self.img_size)) \n        X = img/255.\n        X = np.transpose(X, (2, 0, 1))\n\n        y = self.labels.loc[ID]\n        return X, torch.tensor(y, dtype=torch.float)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GetData_single_cell_180(Dataset):\n    def __init__(self, path, list_IDs, df_labels, img_size, Transform='None'):\n        self.path = path\n        self.list_IDs = list_IDs\n        self.labels = df_labels\n        self.img_size = img_size        \n        self.transform = Transform\n        \n    def __len__(self):\n        return len(self.list_IDs)    \n    \n    def __getitem__(self, index):\n        ID = self.list_IDs[index]   \n                        \n        red = np.rot90(np.load(self.path + \"red/\" + str(ID).zfill(6) + '_red.npy'),k=2)\n        blue = np.rot90(np.load(self.path + \"blue/\" + str(ID).zfill(6) + '_blue.npy'),k=2)\n        yellow = np.rot90(np.load(self.path + \"yellow/\" + str(ID).zfill(6) + '_yellow.npy'),k=2)\n        green = np.rot90(np.load(self.path + \"green/\" + str(ID).zfill(6) + '_green.npy'),k=2)\n        \n        img = np.dstack((red, blue, yellow, green))\n\n        img = resize_to_square(img)\n        img = cv2.resize(img, (self.img_size, self.img_size)) \n        X = img/255.\n        X = np.transpose(X, (2, 0, 1))\n\n        y = self.labels.loc[ID]\n        return X, torch.tensor(y, dtype=torch.float)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the dataset into training data and validation data\nnum_train = round(len(list_IDs) * 0.8)\nnum_valid = len(list_IDs) - num_train\nprint(num_train, num_valid)\n\nnum_train_180 = 42583 # The same number as the number of weakly labeled dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainset = GetData_single_cell(path=TRAIN_DIR, list_IDs=list_IDs[:num_train], df_labels=df_labels,img_size=IM_SIZE, Transform=None)\ntrainset_180 = GetData_single_cell_180(path=TRAIN_DIR, list_IDs=list_IDs[:num_train_180], df_labels=df_labels,img_size=IM_SIZE, Transform=None)\n\nif ROT:\n    trainloader = DataLoader(trainset_180, batch_size=BATCH, shuffle=True)\nelse:\n    trainloader = DataLoader(trainset, batch_size=BATCH, shuffle=True)","metadata":{"papermill":{"duration":0.017976,"end_time":"2021-01-31T14:33:45.29814","exception":false,"start_time":"2021-01-31T14:33:45.280164","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = EfficientNet.from_pretrained('efficientnet-b0',num_classes=NUM_CL, in_channels=4)\n\nif ROT:\n    model.load_state_dict(torch.load('../input/b0-3epochs/state_dict.pth'))\n\nmodel = model.to(DEVICE)\n\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=LR)\n","metadata":{"papermill":{"duration":5.378172,"end_time":"2021-01-31T14:33:50.771526","exception":false,"start_time":"2021-01-31T14:33:45.393354","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TRAIN","metadata":{}},{"cell_type":"code","source":"%%time\n# Reference: https://www.kaggle.com/ateplyuk/hpa-pytorch-starter-code\n\nfor epoch in range(EPOCHS):\n    tr_loss = 0.0\n\n    model = model.train()\n\n    for i, (images, labels) in enumerate(trainloader):        \n        images = images.to(DEVICE)\n        labels = labels.to(DEVICE)       \n        logits = model(images.float())       \n        loss = criterion(logits, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        tr_loss += loss.detach().item()\n\n        del images, labels\n        gc.collect()\n    \n    model.eval()\n    print('Epoch: %d | Loss: %.4f'%(epoch, tr_loss / i))","metadata":{"papermill":{"duration":22.3456,"end_time":"2021-01-31T14:34:13.127298","exception":false,"start_time":"2021-01-31T14:33:50.781698","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model for train dataset\ntrainloader_eval = DataLoader(trainset, batch_size=BATCH, shuffle=False)\n\nwith torch.no_grad():\n    model.eval()\n    for i, (images, labels) in enumerate(trainloader_eval):\n        images = images.to(DEVICE)\n        labels = labels.to(DEVICE)       \n        logits = model(images.float())   \n        prob = nnf.softmax(logits, dim=1)\n        \n        if i == 0:\n            np_prob = np.array(prob.cpu())\n            np_preds = np_prob.max(axis=1,keepdims=1) == np_prob\n            np_labels = np.array(labels.cpu())\n        else:\n            np_prob = np.vstack([np_prob, np.array(prob.cpu())])\n            np_preds = np.vstack([np_preds, np.array(prob.cpu()).max(axis=1,keepdims=1) == np.array(prob.cpu())])\n            np_labels = np.vstack([np_labels, np.array(labels.cpu())])\n        \n        del images, labels\n        gc.collect()\n            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# multi-class evaluation\n# https://vitalflux.com/micro-average-macro-average-scoring-metrics-multi-class-classification-python/\ny_true = np.argmax(np_labels, axis = 1)\ny_pred = np.argmax(np_preds, axis = 1)\n\nacc = sklearn.metrics.accuracy_score(y_true, y_pred)\nprecision = sklearn.metrics.precision_score(y_true, y_pred, average='micro')\nf1 = sklearn.metrics.f1_score(y_true, y_pred, average='micro')\n\nprint(sklearn.metrics.classification_report(y_true, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Output csv file\npd_labels = pd.DataFrame(np_labels.astype(np.bool).astype(int))\npd_prob = pd.DataFrame(np_prob)\n\npd_labels.to_csv(\"labels_train.csv\", index=True)\npd_prob.to_csv(\"prob_train.csv\", index=True)\n\n# Save the trained model\ntorch.save(model.state_dict(), 'state_dict.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# VALIDATION","metadata":{}},{"cell_type":"code","source":"validset = GetData_single_cell(path=TRAIN_DIR, list_IDs=list_IDs[num_train:], df_labels=df_labels,img_size=IM_SIZE, Transform=None)\nvalidloader = DataLoader(validset, batch_size=BATCH, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model for validation dataset\n\nwith torch.no_grad():\n    model.eval()\n    for i, (images, labels) in enumerate(validloader):\n        images = images.to(DEVICE)\n        labels = labels.to(DEVICE)       \n        logits = model(images.float())   \n        prob = nnf.softmax(logits, dim=1)\n\n        if i == 0:\n            np_prob = np.array(prob.cpu())\n            np_preds = np_prob.max(axis=1,keepdims=1) == np_prob\n            np_labels = np.array(labels.cpu())\n        else:\n            np_prob = np.vstack([np_prob, np.array(prob.cpu())])\n            np_preds = np.vstack([np_preds, np.array(prob.cpu()).max(axis=1,keepdims=1) == np.array(prob.cpu())])\n            np_labels = np.vstack([np_labels, np.array(labels.cpu())])\n\n        del images, labels\n        gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# multi-class evaluation\n# https://vitalflux.com/micro-average-macro-average-scoring-metrics-multi-class-classification-python/\ny_true = np.argmax(np_labels, axis = 1)\ny_pred = np.argmax(np_preds, axis = 1)\n\nacc = sklearn.metrics.accuracy_score(y_true, y_pred)\nprecision = sklearn.metrics.precision_score(y_true, y_pred, average='micro')\nf1 = sklearn.metrics.f1_score(y_true, y_pred, average='micro')\n\nprint(sklearn.metrics.classification_report(y_true, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Output csv file\npd_labels = pd.DataFrame(np_labels.astype(np.bool).astype(int))\npd_prob = pd.DataFrame(np_prob)\n\npd_labels.to_csv(\"labels_valid.csv\", index=True)\npd_prob.to_csv(\"prob_valid.csv\", index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}