{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Natural Language Processing is the use of machines to manipulate natural language. Here, we focus on written language, or in simpler words: text.\n\n# Don't Panic! Hitchhiker's Guide to NLP but now it's spaCy v3\n## Upgrading to spaCy v3\n\n## _Any semblance of order or coherence is purely accidental_\n\nHumans are the only known species to have developed written languages. Yet, children don't learn to read and write on their own. This is to highlight the complexity of text processing and NLP.\n\nThe study of natural language processing has been around for more than 50 years. The famous Turing test for General Artificial Intelligence is based on machine comprehension. The field has grown with linguistics and the computational techniques both.\n\nSome Applications of NLP\n\n*  Sentiment Analysis on Social Media\n*  Automated Customer Service\n*  Chatbots, such as that of Uber, Intercom\n","metadata":{}},{"cell_type":"markdown","source":"\n## About spaCy\n\nspaCy is a free open-source library for Natural Language Processing in Python. \n\nIt features NER, POS tagging, dependency parsing, word vectors and more. The name spaCy comes from spaces + Cython. This is because spaCy started off as an industrial grade solution for tokenization - and eventually expanding to other challenges. Cython allows spaCy to be incredibly fast as compared to other solutions like NLTK. \n\nIt has trainable, or in other words customizable and extendable models for most of these tasks - while providing some really good models out of the box. ","metadata":{}},{"cell_type":"code","source":"!pip3 install spacy --upgrade\n!python -m spacy download en_core_web_lg\n!pip3 install textacy","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\nimport random\nfrom collections import Counter #for counting\nimport seaborn as sns #for visualization\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport pandas as pd\nplt.style.use('seaborn')\nsns.set(font_scale=2)\nimport json\ndef pretty_print(pp_object):\n    print(json.dumps(pp_object, indent=2))\n    \nfrom IPython.display import Markdown, display\ndef printmd(string, color=None):\n    colorstr = \"<span style='color:{}'>{}</span>\".format(color, string)\n    display(Markdown(colorstr))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp = spacy.load('en_core_web_lg')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_path = Path(\"../input\")\ntweet_path = data_path/\"all-djtrum-tweets/all_djt_tweets.csv\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets = pd.read_csv(tweet_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data\nWe explore some tweets from President Donald Trump\n\n## What's In Here?\nIn this kernel, we will learn how to use spaCy in Python to generate questions and answers from *any free text*. We will learn about named entitiy recognition, dependency parsing, part of speech tagging, and more!\n\n1. [Named Entity Recognition](#Named-Entity-Recognition-aka-NER),  visualization with `displacy` and **redacting names automatically without a dictionary**!\n2. [Part of Speech Tagging](#Part-of-Speech-Tagging), and exploring what Trump says with *word clouds*!\n3. [Using Linguistic annotations with spaCy Match](#Using-Linguistic-annotations-with-spaCy-Match)\n4. Dependency Parsing, for [**Automatic Question and Answer Generation**](#Automatic-Question-and-Answer-Generation)","metadata":{}},{"cell_type":"markdown","source":"# Named Entity Recognition aka NER\n\n> spaCy can recognise various types of named entities in a document, by asking the model for a prediction. Because models are statistical and strongly depend on the examples they were trained on, this doesn't always work perfectly and might need some tuning later, depending on your use case.\n> \n>  -- from the amazing [spaCy docs](https://spacy.io/usage/linguistic-features#section-named-entities)\n\n## Entities Explained\n\n| Type | \tDescription|\n|:---|:---\n| PERSON |\tPeople, including fictional. |\n| NORP | Nationalities or religious or political groups.| \n| FAC|  \tBuildings, airports, highways, bridges, etc.| \n| ORG|  \tCompanies, agencies, institutions, etc.| \n| GPE|  \tCountries, cities, states.| \n| LOC|  \tNon-GPE locations, mountain ranges, bodies of water.| \n| PRODUCT|  \tObjects, vehicles, foods, etc. (Not services.)| \n| EVENT|  \tNamed hurricanes, battles, wars, sports events, etc.| \n| WORK_OF_ART|  \tTitles of books, songs, etc.| \n| LAW|  \tNamed documents made into laws.| |\n| LANGUAGE|  \tAny named language.| \n| DATE|  \tAbsolute or relative dates or periods.| \n| TIME|  \tTimes smaller than a day.| \n| PERCENT|  \tPercentage, including \"%\".| \n| MONEY|  \tMonetary values, including unit.| \n| QUANTITY|  \tMeasurements, as of weight or distance.| \n| ORDINAL|  \t\"first\", \"second\", etc.| \n| CARDINAL|  \tNumerals that do not fall under another type.| \n\nLet's look at some examples of above in real world sentences. We will also use the `spacy.explain()` on all entities for one example - to build a quick mental model of how these things work.","metadata":{}},{"cell_type":"code","source":"def explain_text_entities(text):\n    doc = nlp(text)\n    for ent in doc.ents:\n        print(f'Entity: {ent}, Label: {ent.label_}, {spacy.explain(ent.label_)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"explain_text_entities(tweets['text'][9])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's continue exploring NER for some more examples, with different entities: ","metadata":{}},{"cell_type":"code","source":"one_sentence = tweets['text'][0]\ndoc = nlp(one_sentence)\nspacy.displacy.render(doc, style='ent',jupyter=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"one_sentence = tweets['text'][240]\ndoc = nlp(one_sentence)\nspacy.displacy.render(doc, style='ent',jupyter=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"one_sentence = tweets['text'][300]\ndoc = nlp(one_sentence)\nspacy.displacy.render(doc, style='ent',jupyter=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"one_sentence = tweets['text'][450]\ndoc = nlp(one_sentence)\nspacy.displacy.render(doc, style='ent',jupyter=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Redacting Names\n\nOne simple use case for NER is to automatically redact names. This is important and quite useful. \n\nFor example, \n\n- to ensure that your company data actually complies with GDPR \n- when journalists wants to publish a large set of documents while still hiding the identity of their sources\n\nWe do this redaction by following broad steps:\n\n```markdown\n1. find all PERSON names\n2. replace these by a filler like [\"REDACTED\"]\n```","metadata":{}},{"cell_type":"code","source":"def redact_names(text):\n    doc = nlp(text)\n    redacted_sentence = []\n    with doc.retokenize() as retokenizer:\n        for ent in doc.ents:\n            retokenizer.merge(ent)\n    for token in doc:\n        if token.ent_type_ == \"PERSON\":\n            redacted_sentence.append(\"[REDACTED]\")\n        else:\n            redacted_sentence.append(token.text)\n    return \" \".join(redacted_sentence)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"printmd(\"**Before**\", color=\"blue\")\none_sentence = tweets['text'][450]\ndoc = nlp(one_sentence)\nspacy.displacy.render(doc, style='ent',jupyter=True)\nprintmd(\"**After**\", color=\"blue\")\none_sentence = redact_names(tweets['text'][450])\ndoc = nlp(one_sentence)\nspacy.displacy.render(doc, style='ent',jupyter=True)\n\nprintmd(\"Notice that `Obama W.H.` was removed\", color=\"#6290c8\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Part-of-Speech Tagging\n\nSometimes, we want to quickly pull out keywords, or keyphrases from a larger body of text. This helps us mentally paint a picture of what this text is about. This is particularly helpful in analysis of texts like long emails or essays.\n\nAs a quick hack, we can pull out all relevant \"nouns\". This is because most keywords are in fact nouns of some form.\n\n### Noun Chunks\nWe need noun chunks. Noun chunks are noun phrases - not a single word, but a short phrase which describes the noun. For example, \"the blue skies\" or \"the worldâ€™s largest conglomerate\".\n\nTo get the noun chunks in a document, simply iterate over doc.noun_chunks:\n","metadata":{}},{"cell_type":"code","source":"example_text = tweets['text'][9]\ndoc = nlp(example_text)\nspacy.displacy.render(doc, style='ent', jupyter=True)\n\nfor idx, sentence in enumerate(doc.sents):\n    for noun in sentence.noun_chunks:\n        print(f\"sentence {idx+1} has noun chunk '{noun}'\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You might notice that Part-of-Speech tagging is different from our NER results. In this particular example, `Stock Market` is not an entity, but definitely a noun. \n\nWhat are the \"Parts of Speech that we can pull out of such sentences? ","metadata":{}},{"cell_type":"code","source":"one_sentence = tweets['text'][300]\ndoc = nlp(one_sentence)\nspacy.displacy.render(doc, style='ent', jupyter=True)\n\nfor token in doc:\n    print(token, token.pos_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# What does Trump talk about? \n\nIt might be interesting to explore what does Trump even talk about? Is it always them 'Angry Dems'? Or is he a narcissist with too many mentions of The President and the USA? \n\nOne way to explore this would be to mine out all the entities and noun chunks from all his tweets! Let's go ahead and do that with amazing ease using spaCy","metadata":{}},{"cell_type":"code","source":"text = tweets['text'].str.cat(sep=' ')\n# spaCy enforces a max limit of 1000000 characters for NER and similar use cases.\n# Since `text` might be longer than that, we will slice it off here\nmax_length = 1000000-1\ntext = text[:max_length]\n\n# removing URLs and '&amp' substrings using regex\nimport re\nurl_reg  = r'[a-z]*[:.]+\\S+'\ntext   = re.sub(url_reg, '', text)\nnoise_reg = r'\\&amp'\ntext   = re.sub(noise_reg, '', text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"doc = nlp(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"items_of_interest = list(doc.noun_chunks)\n# each element in this list is spaCy's inbuilt `Span`, which is not useful for us\nitems_of_interest = [str(x) for x in items_of_interest]\n# so we've converted it to string","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_nouns = pd.DataFrame(items_of_interest, columns=[\"TrumpSays\"])\nplt.figure(figsize=(5,4))\nsns.countplot(y=\"TrumpSays\",\n             data=df_nouns,\n             order=df_nouns[\"TrumpSays\"].value_counts().iloc[:10].index)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hmm, this is interesting in stating he uses \"I\" a lot more than \"we\" and \"We\", put together, but not much beyond that. \nWhat topics does he talk about these filler words? \n\n**Let's remove these filler words and try again!**","metadata":{}},{"cell_type":"code","source":"trump_topics = []\nfor token in doc:\n    if (not token.is_stop) and (token.pos_ == \"NOUN\") and (len(str(token))>2):\n        trump_topics.append(token)\n        \ntrump_topics = [str(x) for x in trump_topics]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_nouns = pd.DataFrame(trump_topics, columns=[\"Trump Topics\"])\ndf_nouns\nplt.figure(figsize=(5,4))\nsns.countplot(y=\"Trump Topics\",\n             data=df_nouns,\n             order=df_nouns[\"Trump Topics\"].value_counts().iloc[:10].index)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is still not very insightul! Let's investigate the entities from Trump tweets instead?\n\n## Exploring Entities","metadata":{}},{"cell_type":"code","source":"trump_topics = []\nfor ent in doc.ents:\n    if ent.label_ not in [\"PERCENT\", \"CARDINAL\", \"DATE\"]:\n#         print(ent.text,ent.label_)\n        trump_topics.append(ent.text.strip())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_ttopics = pd.DataFrame(trump_topics, columns=[\"Trump Nouns\"])\nplt.figure(figsize=(5,4))\nsns.countplot(y=\"Trump Nouns\",\n             data=df_ttopics,\n             order=df_ttopics[\"Trump Nouns\"].value_counts().iloc[1:11].index)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Wow! Trump is really obsessed with Democrats, himself and Hillary. ","metadata":{}},{"cell_type":"code","source":"from spacy.lang.en.stop_words import STOP_WORDS\nfrom wordcloud import WordCloud\nplt.figure(figsize=(10,5))\nwordcloud = WordCloud(background_color=\"white\",\n                      stopwords = STOP_WORDS,\n                      max_words=45,\n                      max_font_size=30,\n                      random_state=42\n                     ).generate(str(trump_topics))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using Linguistic annotations with spaCy Match\n\n> Based on the [Rule Matching docs at spaCy](https://spacy.io/usage/linguistic-features#section-rule-based-matching)\n\nWe want to find out what Trump is saying about \n\n1. Himself e.g. \"I am rich\". \n2. Russia\n3. Democrats\n\nWe want to start off by finding _adjectives following_ \"Democrats are\" or \"Democrats were\". \n\nThis is obviously a very rudimentary solution, but it'll be fast, and a great way get an idea for what's in your data. Our pattern looks like this:\n\n```bash\n[{'LOWER': 'Russia'}, {'LEMMA': 'be'}, {'POS': 'ADV', 'OP': '*'}, {'POS': 'ADJ'}]\n```\n\nThis translates to a token whose lowercase form matches \"democrats\" (like Democrats, democrats or DEMoCrats), followed by a token with the lemma \"be\" (for example, is, was, or 's), followed by an optional adverb, followed by an adjective. \n\nThe optional adverb makes sure you won't miss adjectives with intensifiers, like \"pretty awful\" or \"very nice\".\n\nThis kind of adjective mining can then be used as features to do _aspect-based sentiment analysis_, which is finding sentiment with respect to specific entities or words. ","metadata":{}},{"cell_type":"code","source":"from spacy.matcher import Matcher\n# doc = nlp(text)\nmatcher = Matcher(nlp.vocab)\nmatched_sents = [] # collect data of matched sentences to be visualized\n\ndef collect_sents(matcher, doc, i, matches, label='MATCH'):\n    \"\"\"\n    Function to help reformat data for displacy visualization\n    \"\"\"\n    match_id, start, end = matches[i]\n    span = doc[start : end]  # matched span\n    sent = span.sent  # sentence containing matched span\n    \n    # append mock entity for match in displaCy style to matched_sents\n    \n    if doc.vocab.strings[match_id] == 'DEMOCRATS':  # don't forget to get string!\n        match_ents = [{'start': span.start_char - sent.start_char,\n                   'end': span.end_char - sent.start_char,\n                   'label': 'DEMOCRATS'}]\n        matched_sents.append({'text': sent.text, 'ents': match_ents })\n    elif doc.vocab.strings[match_id] == 'RUSSIA':  # don't forget to get string!\n        match_ents = [{'start': span.start_char - sent.start_char,\n               'end': span.end_char - sent.start_char,\n               'label': 'RUSSIA'}]\n        matched_sents.append({'text': sent.text, 'ents': match_ents })\n    elif doc.vocab.strings[match_id] == 'I':  # don't forget to get string!\n        match_ents = [{'start': span.start_char - sent.start_char,\n               'end': span.end_char - sent.start_char,\n               'label': 'NARC'}]\n        matched_sents.append({'text': sent.text, 'ents': match_ents })","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # declare different patterns\nrussia_pattern = [[{'LOWER': 'russia'}, {'LEMMA': 'be'}, {'POS': 'ADV', 'OP': '*'},\n           {'POS': 'ADJ'}]]\ndemocrats_pattern = [[{'LOWER': 'democrats'}, {'LEMMA': 'be'}, {'POS': 'ADV', 'OP': '*'},\n           {'POS': 'ADJ'}]]\ni_pattern = [[{'LOWER': 'i'}, {'LEMMA': 'be'}, {'POS': 'ADV', 'OP': '*'},\n           {'POS': 'ADJ'}]]\ndemocrats_pattern","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmatcher.add('DEMOCRATS', democrats_pattern, on_match=collect_sents)  # add pattern\nmatcher.add('RUSSIA', russia_pattern, on_match=collect_sents) # add pattern\nmatcher.add('I', i_pattern, on_match=collect_sents)  # add pattern\nmatches = matcher(doc)\n\nspacy.displacy.render(matched_sents, style='ent', manual=True, jupyter=True,  options = {'colors': {'NARC': '#6290c8', 'RUSSIA': '#cc2936', 'DEMOCRATS':'#f2cd5d'}})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let's preview 2-3 elements used in the displaCy visualization above. This is what the list of dictionaries looks like: ","metadata":{}},{"cell_type":"code","source":"pretty_print(matched_sents[:3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Automatic Question and Answer Generation\n\n> ### When asked to produce The Ultimate Question, Deep Thought says that it cannot...\n>\n> -- Douglas Adams\n\n### The Challenge\n\nCan you automatically convert a sentence to a question?\n\nFor instance, \n```bash\nMartin Luther King Jr. was a civil rights activist and skilled orator\n``` \n\nto \n\n```js\nWho was Martin Luther King Jr.?\n```\n\nNotice that when we convert a sentence to a question, the answer might not be in the original sentence anymore. To me, the answer to that question might be something different and that's fine. We are not aiming for _correct_ answers here.\n\n## Question Generation using Dependency Parsing\n\n\nDependency parsing analyzes the grammatical structure of a sentence. It establishes a \"tree\" like structure between a \"root\" word and those that are related to it by branches of some manner. ","metadata":{}},{"cell_type":"code","source":"example_text = tweets['text'][180]\ndoc = nlp(example_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"options = {'compact': True, 'bg': '#09a3d5',\n           'color': 'white', 'font': 'Trebuchet MS'}\nspacy.displacy.render(doc, jupyter=True, style='dep', options=options)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can understand these relationship as a parent-child format as well, looking at one word at a time","metadata":{}},{"cell_type":"code","source":"for token in doc:\n    print(token.text, token.dep_, token.head.text, token.head.pos_,\n          [child for child in token.children])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Adding some explainer text in the output itself:","metadata":{}},{"cell_type":"code","source":"for token in doc:\n    print(f\"token: {token.text},\\t dep: {token.dep_},\\t head: {token.head.text},\\t pos: {token.head.pos_},\\\n    ,\\t children: {[child for child in token.children]}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To generate our questions, let's actually use these two ideas:\n- Subject of Verb\n- Object of Verb\n","metadata":{}},{"cell_type":"code","source":"from textacy.spacier import utils as spacy_utils\n\ndef para_to_ques(eg_text):\n    \"\"\"\n    Generates a few simple questions by slot filling pieces from sentences\n    \"\"\"\n    doc = nlp(eg_text)\n    results = []\n    for sentence in doc.sents:\n        root = sentence.root\n        ask_about = spacy_utils.get_subjects_of_verb(root)\n        answers = spacy_utils.get_objects_of_verb(root)\n        if len(ask_about) > 0 and len(answers) > 0:\n            if root.lemma_ == \"be\":\n                question = f'What {root} {ask_about[0]}?'\n            else:\n                question = f'What does {ask_about[0]} {root.lemma_}?'\n            results.append({'question':question, 'answers':answers})\n    return results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_text = tweets['text'][180]\ndoc = nlp(example_text)\nspacy.displacy.render(doc, style='ent', jupyter=True)\nprint(para_to_ques(example_text))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now let's see if we can extended it to French","metadata":{}},{"cell_type":"code","source":"!python -m spacy download fr_core_news_lg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp = spacy.load('fr_core_news_lg')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweet_path = data_path/\"french-twitter-sentiment-analysis/french_tweets.csv\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets = pd.read_csv(tweet_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Entities French","metadata":{}},{"cell_type":"code","source":"one_sentence = tweets['text'][1476]\ndoc = nlp(one_sentence)\nspacy.displacy.render(doc, style='ent',jupyter=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Redacting Names for French Tweets","metadata":{}},{"cell_type":"code","source":"def redact_names(text):\n    doc = nlp(text)\n    redacted_sentence = []\n    with doc.retokenize() as retokenizer:\n        for ent in doc.ents:\n            retokenizer.merge(ent)\n    for token in doc:\n        if token.ent_type_ == \"PER\":\n            redacted_sentence.append(\"[supprimÃ©]\")\n        else:\n            redacted_sentence.append(token.text)\n    return \" \".join(redacted_sentence)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"printmd(\"**Before**\", color=\"blue\")\none_sentence = tweets['text'][1476]\ndoc = nlp(one_sentence)\nspacy.displacy.render(doc, style='ent',jupyter=True)\nprintmd(\"**After**\", color=\"blue\")\none_sentence = redact_names(tweets['text'][1476])\ndoc = nlp(one_sentence)\nspacy.displacy.render(doc, style='ent',jupyter=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## POS Tagging","metadata":{}},{"cell_type":"code","source":"one_sentence = tweets['text'][300]\ndoc = nlp(one_sentence)\nspacy.displacy.render(doc, style='ent', jupyter=True)\n\nfor token in doc:\n    print(token, token.pos_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = tweets['text'].str.cat(sep=' ')\n# spaCy enforces a max limit of 1000000 characters for NER and similar use cases.\n# Since `text` might be longer than that, we will slice it off here\nmax_length = 1000000-1\ntext = text[:max_length]\n\n# removing URLs and '&amp' substrings using regex\nimport re\nurl_reg  = r'[a-z]*[:.]+\\S+'\ntext   = re.sub(url_reg, '', text)\nnoise_reg = r'\\&amp'\ntext   = re.sub(noise_reg, '', text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"doc = nlp(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploring Entities for French Tweets","metadata":{}},{"cell_type":"code","source":"from spacy.lang.en.stop_words import STOP_WORDS\nfrom wordcloud import WordCloud\n\nfrench_tweet_topics = []    \nfor token in doc:\n    if (not token.is_stop) and (len(str(token))>2):\n        french_tweet_topics.append(token)\n        \nfrench_tweet_topics = [str(x) for x in french_tweet_topics]\n\ndf_nouns = pd.DataFrame(french_tweet_topics, columns=[\"French Topics\"])\ndf_nouns\nplt.figure(figsize=(5,4))\nsns.countplot(y=\"French Topics\",\n             data=df_nouns,\n             order=df_nouns[\"French Topics\"].value_counts().iloc[:10].index)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nwordcloud = WordCloud(background_color=\"white\",\n                      stopwords = STOP_WORDS,\n                      max_words=45,\n                      max_font_size=30,\n                      random_state=42\n                     ).generate(str(french_tweet_topics))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For simply hinting at the power of `textacy` and `spaCy`, I have written only two simple rules to create questions - by adding more, with more nuanced examples, we can generate a large number of specific questions _and_ answers. \n\nYou can find a throrough and **Complete guide to question formation in English on [StackExchange here](https://ell.stackexchange.com/a/1198)**","metadata":{}},{"cell_type":"markdown","source":"# What's did we really see here?\n\n## Don't Panic! Nothing is really beyond spaCy\n\n![](https://i.kym-cdn.com/photos/images/newsfeed/001/022/354/081.jpeg)\n\n1. [Named Entity Recognition](#Named-Entity-Recognition-aka-NER), the different entities and it's visualization with `displacy`\n2. [Part of Speech Tagging](#Part-of-Speech-Tagging), and exploring what Trump says with *word clouds*!\n3. [Using Linguistic annotations with spaCy Match](#Using-Linguistic-annotations-with-spaCy-Match)\n4. Dependency Parsing, for [**Automatic Question and Answer Generation**](#Automatic-Question-and-Answer-Generation)\n\n# Bookmark this with [http://bit.ly/spacykernel](http://bit.ly/spacykernel)","metadata":{}}]}