{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\nimport collections\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport nltk\nimport gensim\nimport sklearn.feature_extraction.text as sktext\nimport re\n","metadata":{"id":"b1MwAYcWa4Xl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom nltk.tokenize import word_tokenize\n\nSTOPWORDS = set(stopwords.words('english'))","metadata":{"id":"KuXrRhlAX_cK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/million-headlines/abcnews-date-text.csv',nrows=50000)\ndata.head()","metadata":{"id":"Jl9yeXaCVrVQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dataset info\n\ntmp = []\nfor col in data.columns:\n    tmp.append([col, type(data[col][0]), data[col].isnull().sum(), data[col].nunique()])\n\ndf = pd.DataFrame(data=tmp, columns=['column_name','Datatype', 'null_count', 'unique_count'])\ndf.insert(2,'non_null_count', len(data)-df['null_count'])\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['word_count'] = data['headline_text'].apply(lambda x: len(str(x).split()))\ndata['unique_word_count'] = data['headline_text'].apply(lambda x: len(set(str(x).split())))\ndata['stop_word_count'] = data['headline_text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\ndata['mean_word_length'] = data['headline_text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ndata['char_count'] = data['headline_text'].apply(lambda x: len(str(x)))\ndata['punctuation_count'] = data['headline_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"METAFEATURES = ['word_count', 'unique_word_count', 'stop_word_count', 'mean_word_length',\n                'char_count', 'punctuation_count']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p =data[METAFEATURES].hist(figsize = (20,30), grid=False, bins=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_top_ngrams(data, n=None, N=100):\n    vec = sktext.CountVectorizer(ngram_range=(n, n)).fit(data)\n    bag_of_words = vec.transform(data)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) \n                  for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n\n    return pd.DataFrame(words_freq[:N])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Unigrams\ndata_unigrams = generate_top_ngrams(data['headline_text'],n=1)\n\nfig, axis = plt.subplots(ncols=1, nrows=1, figsize=(20, 40), dpi=100)\nsns.barplot(y=data_unigrams[0], x=data_unigrams[1])\nplt.title('Top 100 unigrams in news headlines')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Bigrams\ndata_bigrams = generate_top_ngrams(data['headline_text'], n=2)\n\nfig, axis = plt.subplots(ncols=1, nrows=1, figsize=(20, 40), dpi=100)\nsns.barplot(y=data_bigrams[0], x=data_bigrams[1])\nplt.title('Top 100 bigrams in news headlines')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Trigrams\ndata_trigrams = generate_top_ngrams(data['headline_text'], n=3)\n\nfig, axis = plt.subplots(ncols=1, nrows=1, figsize=(20, 40), dpi=100)\nsns.barplot(y=data_trigrams[0], x=data_trigrams[1])\nplt.title('Top 100 bigrams in news headlines')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus=[]\ncorpus = [w for s in data['headline_text'].str.split().values.tolist() for w in s]\n","metadata":{"id":"qMFEMO-vXkF4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"counter = collections.Counter(corpus)\nmost_common_words = counter.most_common()\n\nx, y= [], []\nfor word,count in most_common_words[:50]:\n    if (word not in STOPWORDS):\n        x.append(word)\n        y.append(count)\n\nfig, axis = plt.subplots(ncols=1, nrows=1, figsize=(20, 10), dpi=100)\nsns.barplot(x=y,y=x)\nplt.title('Most common words in corpus (except stopwords)')\nplt.show()","metadata":{"id":"DyXpdSyZX0Yd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x, y= [], []\nfor word,count in most_common_words[:50]:\n    if (word in STOPWORDS):\n        x.append(word)\n        y.append(count)\n        \nfig, axis = plt.subplots(ncols=1, nrows=1, figsize=(20, 10), dpi=100)\nsns.barplot(x=y,y=x)\nplt.title('Most common stop words in corpus')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from wordcloud import WordCloud\n\nwordcloud = WordCloud(\n    background_color='white',\n    stopwords=set(STOPWORDS),\n    max_words=100,\n    max_font_size=100, \n    scale=3,\n    random_state=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordcloud=wordcloud.generate(str(corpus))\nfig = plt.figure(1, figsize=(12, 12))\nplt.axis('off')\nplt.imshow(wordcloud)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _preprocess_text(text):\n    corpus=[]\n    stem=PorterStemmer()\n    lem=WordNetLemmatizer()\n    for news in text:\n        words=[w for w in word_tokenize(news) if (w not in STOPWORDS)]\n        words=[lem.lemmatize(w) for w in words if len(w)>2]\n        corpus.append(words)\n    return corpus\n\ncorpus_processed=_preprocess_text(corpus)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordcloud=wordcloud.generate(str(corpus_processed))\nfig = plt.figure(1, figsize=(12, 12))\nplt.axis('off')\nplt.imshow(wordcloud)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}