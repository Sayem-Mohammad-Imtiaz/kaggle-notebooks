{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n%config Completer.use_jedi = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget https://files.consumerfinance.gov/ccdb/complaints.csv.zip\n!unzip ./complaints.csv.zip\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport transformers\n#!pip install Tez\n#import tez\nimport torch.nn as nn\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom sklearn import metrics\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BERTDataset(Dataset):\n    def __init__(self, texts, targets, max_len = 64):\n        self.texts = texts\n        self.targets = targets\n        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n                        \"bert-base-uncased\", do_lower_case = True\n        )\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        inputs = self.tokenizer.encode_plus(\n                text,\n                None,\n                add_special_tokens = True,\n                max_length = self.max_len,\n                padding = \"max_length\",\n                truncation = True\n        )\n        resp = {\n            \"ids\" : torch.tensor(inputs[\"input_ids\"], dtype=torch.long),\n            \"mask\" : torch.tensor(inputs[\"attention_mask\"], dtype=torch.long),\n            \"token_type_ids\" : torch.tensor(inputs[\"token_type_ids\"], dtype=torch.long),\n            \"targets\" : torch.tensor(self.targets[\"idx\"], dtype = torch.float)\n        }\n        return resp\n    \n    \nclass TextModel(tez.Model):\n    def __init__(self, num_classes, num_train_steps):\n        super().__init__()\n        self.bert = transformers.BertModel.from_pretrained(\n                    \"bert-base-uncased\", return_dict = False\n        )\n        self.bert_drop = nn.Dropout(0.3)\n        self.out = nn.Linear(768, num_classes)\n        self.num_train_steps = num_train_steps\n        self.optim_ = self.fetch_optimizer()\n        \n    def fetch_optimizer(self):\n        opt = AdamW(self.parameters(), lr = 1e-4)\n        \n    def fetch_scheduler(self):\n        sch = get_linear_schedule_with_warmup(\n            AdamW(self.parameters(), lr = 1e-4), num_warmup_steps = 0,num_training_steps = self.num_train_steps\n        )\n        return sch\n    \n    \n    def loss(self, outputs, targets):\n        return nn.BCEWithLogitsLoss()(outputs, targets.view(-1,1))\n    \n    def monitor_metrics(self, outputs, targets):\n        outputs = torch.sigmoid(outputs).cpu().detach().numpy() >= 0.5\n        targets = targets.cpu().detach().numpy()\n        return {\n            \"accuracy\" : metrics.accruacy_score(targets, outputs)\n        }\n    \n    def forward(self, ids, mark, token_type_ids, targets = None):\n        _, x = self.bert(ids, attention_mask = mask, token_type_ids = token_type_ids)\n        x = self.bert_drop(x)\n        x = self.out(x)\n        if(targets is not None):\n            loss = self.loss(outputs , targets)\n            metrics = self.monitor_metrics(x , targets)\n            return x, loss, met\n        return x, 0, {}\n    \n\n    def train_model(out1,out2):\n        \n        \n        train_dataset = BERTDataset(out1.Title.values, out1.label.values)\n        valid_dataset = BERTDataset(out2.Title.values, out2.label.values)\n        \n        n_train_steps = int(len(out1) / 32 * 10)\n        \n        model = TextModel(num_classes = 1, num_train_steps = n_train_steps, )\n        \n        ess = tez.callbacks.EarlyStopping(monitor = \"valid_loss\", patience = 3, model_path = \"model.bin\")\n    \n        model.fit(\n                train_dataset,\n                valid_dataset = valid_dataset,\n                device = \"cuda\",\n                epochs = 10,\n                train_bs = 32,\n                callbacks = [ess]\n        )\n    \n    \n\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!wget https://raw.githubusercontent.com/susanli2016/NLP-with-Python/master/data/title_conference.csv\n!wget https://raw.githubusercontent.com/susanli2016/NLP-with-Python/master/data/title_conference.csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/200000-jeopardy-questions/JEOPARDY_CSV.csv').fillna(\"none\")\n#df['Conference'].value_counts()\n# possible_label = df[\" Value\"].unique()\n# label_dict = {}\n# for index, possible_label in enumerate(possible_label):\n#     label_dict[possible_label] = index\n\n# df['label'] = df[\" Value\"].replace(label_dict)\n# #df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.countplot(x=' Round',data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(df[df[' Value'] == 'None'].index,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['ValueNum'] = df[' Value'].apply(\n    lambda value: int(value.replace('$', '').replace(',','')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['ValueNum'].unique()\ndf['ValueNum'].nunique()\ndef binning(value):\n    if value < 1000:\n        return np.round(value, -2)\n    elif value < 10000:\n        return np.round(value, -3)\n    else:\n        return np.round(value, -4)\n\ndf['ValueBins'] = df['ValueNum'].apply(binning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndef remove_tags(string):\n    result = re.sub('<.*?>','',string)\n    return result\ndf['with_out_tags']=df[' Question'].apply(lambda cw : remove_tags(cw))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df = df[[\"with_out_tags\",\"ValueBins\"]].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df['ValueBins'] = new_df['ValueBins'].astype('category')\n# Assigning numerical values and storing in another column\nnew_df['labels'] = new_df['ValueBins'].cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df.labels.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df = new_df[[\"with_out_tags\",\"labels\"]].copy()\nfinal_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_Df['ValueBins'].value_counts()\npossible_label = new_Df[\"ValueBins\"].unique()\nlabel_dict = {}\nfor index, possible_label in enumerate(possible_label):\n    label_dict[possible_label] = index\n\nnew_Df['label'] = new_Df[\"ValueBins\"].replace(label_dict)\nnew_Df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_Df[\"label\"].unique()\nfinal_df = new_Df[[\"with_out_tags\", \"label\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\n#new_Df = df[['with_out_tags','ValueBins']]\n#added some parameters\nkf = KFold(n_splits = 5, shuffle = True, random_state = 2)\nresult = next(kf.split(final_df), None)\n\n\n\n\ntrain = final_df.iloc[result[0]]\ntest =  final_df.iloc[result[1]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.reset_index(inplace = True, drop = True)\ntest.reset_index(inplace = True, drop = True) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fin_df = final_df.sample(n = 50000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(fin_df, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(final_df.with_out_tags.values, final_df.labels.values, test_size=0.33,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = train_test_split(final_df, test_size=0.2, random_state=2019)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from simpletransformers.classification import ClassificationModel, ClassificationArgs\nimport pandas as pd\nimport logging\n\n\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)\n\n\nmodel_type = \"Out\"\nmodel_name = \"roberta\"\n\ntrain.columns = [\"text\", \"labels\"]\ntrain_args = {\n    'output_dir': f'{model_type}-{model_name}-outputs',\n\n    'max_seq_length': 128,\n    'num_train_epochs': 5,\n    'train_batch_size': 16,\n    'eval_batch_size': 32,\n    'gradient_accumulation_steps': 1,\n    'learning_rate': 5e-5,\n    'save_steps': -1,\n\n    'wandb_project': 'Wysa',\n    'wandb_kwargs': {'name': f'{model_type}-{model_name}'},\n    'evaluate_during_training': True,\n    'evaluate_during_training_steps': 1000,\n    'reprocess_input_data': True,\n    \"save_model_every_epoch\": False,\n    'overwrite_output_dir': True,\n    'no_cache': True,\n    'no_save' : True,\n\n    'use_early_stopping': True,\n    'early_stopping_patience': 3,\n    'manual_seed': 4,\n}\n\n\n\n\ntest.columns = [\"text\", \"labels\"]\n\n\n# Optional model configuration\n#model_args = ClassificationArgs(num_train_epochs=8)\n\n# Create a ClassificationModel\nmodel = ClassificationModel(\n'roberta', 'roberta-base',\n    num_labels=20,\n    #args=model_args,\n    args=train_args\n) \n\n#model = ClassificationModel(model_type, model_name, num_labels=4, args=train_args)\n# Train the model\nmodel.train_model(train,eval_df=test)\n\n# Evaluate the model\n#result, model_outputs, wrong_predictions = model.eval_model(test)\n\n# Make predictions with the model\n#predictions, raw_outputs = model.predict([\"Sam was a Wizard\"])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"du - h ./final-roberta-outputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"du -h final-roberta-outputs/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls -ltr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_best = ClassificationModel('roberta', 'outputs/best_model/', num_labels=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score, accuracy_score\n\n\ndef f1_multiclass(labels, preds):\n    return f1_score(labels, preds, average='micro')\n    \n#result, model_outputs, wrong_predictions = model.eval_model(eval_df, f1=f1_multiclass, acc=accuracy_score)\n\nresult, model_outputs, wrong_predictions = model.eval_model(test, f1=f1_multiclass, acc=accuracy_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install simpletransformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!wget https://s3.amazonaws.com/fast-ai-nlp/ag_news_csv.tgz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bow = CountVectorizer(stop_words='english', max_features=2000)\nbow.fit(train['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = bow.transform(train[\"text\"])\nX_test = bow.transform(test[\"text\"])\n\ny_train = train.labels\ny_test = test.labels\n\nprint(\"Shape of X_train:\", X_train.shape)\nprint(\"Shape of X_test:\", X_test.shape)\nprint(\"Shape of y_train:\", y_train.shape)\nprint(\"Shape of y_test:\", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(solver='saga', multi_class='multinomial', max_iter=200)\nlr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = lr.predict(X_test)\n\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}