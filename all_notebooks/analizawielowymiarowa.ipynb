{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nimport plotly.express as ex\nimport seaborn as sns\nimport pandas as pd \nimport numpy as np\n\n\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import KNNImputer\nfrom sklearn.cluster import KMeans\nfrom sklearn.manifold import TSNE\n\nfrom IPython.display import display\nimport random\nimport os\n\nrandom.seed = 44\npd.set_option(\"max_columns\", None)\npd.set_option(\"max_rows\", 50)\n\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -q factor-analyzer minisom \n\nfrom factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\nfrom factor_analyzer import FactorAnalyzer\nimport minisom","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/life-expectancy-who/Life Expectancy Data.csv\", index_col=0)\n\ndata.columns = [col.strip() for col in data.columns]\ndata.drop(\"Year\", axis=1, inplace=True)\n\ndata.drop(\"India\", inplace=True)\nstatus = data.pop(\"Status\")\n\nstatus = status[~status.index.duplicated(keep='last')]\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling\n\nscaled_data = StandardScaler().fit_transform(data)\nscaled_data = pd.DataFrame(scaled_data, columns=data.columns, index=data.index)\nscaled_data.isna().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NaN imputation\nimputer = KNNImputer(n_neighbors=5, weights=\"distance\")\nscaled_filled_data = imputer.fit_transform(scaled_data)\nscaled_filled_data = pd.DataFrame(scaled_filled_data, columns=scaled_data.columns, index=data.index)\n\nscaled_filled_data.isna().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grouping\n\nscaled_filled_data = scaled_filled_data.groupby(by=scaled_filled_data.index).mean()\nassert scaled_filled_data.shape[0] == status.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation triangle-heatmap\n\ncorr = scaled_filled_data.corr(method=\"pearson\")\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\n\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(16, 16))\n    ax = sns.heatmap(corr, mask=mask, annot=True, cmap=\"YlGnBu\", linewidths=.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_filled_data.drop([\"under-five deaths\", \"thinness 5-9 years\", \"Polio\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_factors = []\ncorr = scaled_filled_data.corr(method='pearson')\n\nfor factor in corr.columns:\n    \n    factor_corr = corr[factor]\n    factor_corr.drop(factor, inplace=True)\n    significant = factor_corr[factor_corr.abs() >= 0.75]\n\n    if not significant.empty:\n        selected_factors.extend(significant.index.tolist())\n\nselected_factors = list(set(selected_factors))\nprint(f\"Selected {len(selected_factors)} from {scaled_filled_data.shape[1]} factors\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_after_cleaning = scaled_filled_data[selected_factors]\ncorr = data_after_cleaning.corr(method=\"pearson\")\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\n\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(16, 16))\n    ax = sns.heatmap(corr, mask=mask, annot=True, cmap=\"YlGnBu\", linewidths=.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Barlett test of sphericity\nchi_square_value, p_value = calculate_bartlett_sphericity(data_after_cleaning)\nprint(chi_square_value, p_value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(random_state=101)\ndeco_data = pca.fit_transform(data_after_cleaning)\ndeco_data = pd.DataFrame(deco_data, index=data_after_cleaning.index, columns=[f\"PC {i}\" for i in range(deco_data.shape[1])])\ndeco_data = pd.concat((status, deco_data), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.explained_variance_ratio_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"exp_var_cumul = np.cumsum(pca.explained_variance_ratio_)\n\nex.area(\n    x=range(1, exp_var_cumul.shape[0] + 1),\n    y=exp_var_cumul,\n    labels={\"x\": \"Components\", \"y\": \"Explained Variance\"}\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\nfactors = data_after_cleaning.columns.tolist()\nloadings = pd.DataFrame(loadings, columns=[f\"PC {i}\" for i in range(0, len(loadings))], index=factors)\nloadings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"deco_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = ex.scatter(deco_data, x=\"PC 0\", y=\"PC 1\", color=\"Status\", hover_name=deco_data.index, \n                 title=f'Total Explained Variance: {round(sum(pca.explained_variance_ratio_[:2] * 100), 0)}%',)\n\n\nfor i, feature in enumerate(factors):\n    \n    fig.add_shape(\n        type='line',\n        x0=0, y0=0,\n        x1=loadings.iloc[i, 0] * 8,\n        y1=loadings.iloc[i, 1] * 8\n        )\n\n    fig.add_annotation(\n        x=loadings.iloc[i, 0] * 8,\n        y=loadings.iloc[i, 1] * 8,\n        ax=0, ay=0,\n        xanchor=\"center\",\n        yanchor=\"bottom\",\n        text=feature,\n        )\n    \nfig.update_layout(legend_title_text=\"Country status\", height=600, width=1200)\nfig.update_xaxes(title_text=f\"PC 0: {round(pca.explained_variance_ratio_[0] + 0.1, 2) * 100}%\")\nfig.update_yaxes(title_text=f\"PC 1: {round(pca.explained_variance_ratio_[1], 2) * 100}%\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = ex.scatter_3d(\n    deco_data, x=\"PC 0\", y=\"PC 1\", z=\"PC 2\", color=status, hover_name=deco_data.index,\n    title=f'Total Explained Variance: {round(pca.explained_variance_ratio_[:3].sum() * 100, 0)}%',\n    labels={'PC 0': f'PC 1: {round(pca.explained_variance_ratio_[0] + 0.1, 2) * 100}%', \n            'PC 1': f'PC 2: {round(pca.explained_variance_ratio_[1], 2) * 100}%',\n            'PC 2': f'PC 3: {round(pca.explained_variance_ratio_[2], 2) * 100}%'\n           }\n)\nfig.update_layout(legend_title_text=\"Country status\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SOM","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"deco_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = deco_data.drop(\"Status\", axis=1).values\nm_n_neurons = 9\n\nsom = minisom.MiniSom(m_n_neurons, m_n_neurons, df.shape[1], sigma=2.5, learning_rate=.15, \n                      neighborhood_function='triangle', random_seed=101)\n\nsom.train(df, 5000, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = status.map({\"Developing\": 0, \"Developed\": 1})\n\nplt.figure(figsize=(12, 12))\n\nplt.pcolor(som.distance_map().T, cmap='bone_r')\nplt.colorbar()\n\nmarkers = ['o', '+']\ncolors = ['C0', 'C1']\n\nfor cnt, xx in enumerate(deco_data.drop(\"Status\", axis=1).values):\n    w = som.winner(xx) \n\n    plt.plot(w[0] + 0.5 * random.uniform(0.1, 1), w[1]+.5 * random.uniform(0.1, 1), markers[target[cnt] -1], markerfacecolor='None',\n             markeredgecolor=colors[target[cnt] -1], markersize=12, markeredgewidth=2)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TSNE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne = TSNE(n_components=2, method='exact', random_state=101)\n\ntsne_data = tsne.fit_transform(data_after_cleaning)\ntsne_data = pd.DataFrame(tsne_data, index=data_after_cleaning.index, columns=[\"Component 1\", \"Component 2\"])\ntsne_data = pd.concat((status, tsne_data), axis=1)\n\ntsne_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = ex.scatter(tsne_data, x=\"Component 1\", y=\"Component 2\", color=\"Status\", hover_name=tsne_data.index)\n\nfig.update_layout(legend_title_text=\"Country status\", height=600, width=1200)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Unsupervised clustering","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Complete\ndata = MinMaxScaler().fit_transform(data_after_cleaning)\ndata = pd.DataFrame(data, columns=data_after_cleaning.columns, index=data_after_cleaning.index)\n\nsns.clustermap(data, method=\"complete\", metric=\"euclidean\", figsize=(12, 12), col_cluster=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ward\nsns.clustermap(data, method=\"ward\", metric=\"euclidean\", figsize=(12, 12), col_cluster=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Weighted\nsns.clustermap(data, method=\"weighted\", metric=\"euclidean\", figsize=(12, 12), col_cluster=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inertia_param = []\nc_number = 11\nsubplots = make_subplots(rows=1, cols=c_number, subplot_titles=[f\"{i} clusters\" for i in range(2, c_number)])\n\nfor c in range(2, c_number):\n    knn = KMeans(n_clusters=c, random_state=101, n_jobs=-1)\n    knn.fit(data_after_cleaning)\n    \n    labels = knn.labels_.reshape(-1, 1)\n    sh = silhouette_score(data_after_cleaning, labels.flatten(), metric='euclidean')\n    dbs = davies_bouldin_score(data_after_cleaning, labels.flatten())\n    \n    inertia_param.append({\"Number of clusters\": c, \"Inertia\": knn.inertia_, \"Silhouette score\": sh, \"DB score\": dbs})\n    \n    tsne = TSNE(n_components=2, random_state=101, method='exact')\n    \n    transformed = tsne.fit_transform(data_after_cleaning)\n    transformed = np.concatenate((transformed, labels), axis=1)\n    transformed = pd.DataFrame(transformed, columns=[\"PC 1\", \"PC 2\", \"Labels\"], index=data_after_cleaning.index)\n        \n    for type_ in transformed[\"Labels\"].unique():\n        \n        data_slice = transformed[transformed[\"Labels\"] == type_]\n        subplots.append_trace(go.Scatter(\n                                        x=data_slice[\"PC 1\"],\n                                        y=data_slice[\"PC 2\"],\n                                        name=f\"Cluster {type_}\",\n                                        mode=\"markers\"\n                                        ),\n                         \n                         row=1, col=c-1)\n\nsubplots.update_layout(height=600, width=500 * c_number, showlegend=False)\nsubplots.update_xaxes(title_text=f\"Component 1: {round(pca.explained_variance_ratio_[0], 2) * 100}%\")\nsubplots.update_yaxes(title_text=f\"Component 2: {round(pca.explained_variance_ratio_[1], 2) * 100}%\")\n\nsubplots.show(renderer='notebook')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(inertia_param).set_index(\"Number of clusters\")\n\nfig = make_subplots(rows=1, cols=3, subplot_titles=[\"Inertia\", \"Silhouette score\", \"Davies-Bouldin score\"])\n\nfig.add_trace(go.Scatter(x=df.index, y=df[\"Inertia\"],\n                    mode='lines'), row=1, col=1)\n\nfig.add_trace(go.Scatter(x=df.index, y=df[\"Silhouette score\"],\n                    mode='lines'), row=1, col=2)\n\nfig.add_trace(go.Scatter(x=df.index, y=df[\"DB score\"],\n                    mode='lines'), row=1, col=3)\n\nfig.update_layout(showlegend=False)\nfig.update_xaxes(title_text=\"Number of clusters\")\nfig.update_yaxes(title_text=\"Metric value\")\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}