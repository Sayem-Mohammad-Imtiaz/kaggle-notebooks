{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport seaborn as sns\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport matplotlib\n%matplotlib inline\n\n# Import the 3 dimensionality reduction methods\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/heart.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the labels to a Pandas series target\ntarget = train['target']\n# Drop the label feature\ntrain = train.drop(\"target\",axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"YY = train.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Standardize the data\nfrom sklearn.preprocessing import StandardScaler\nX = train.values\nX_std = StandardScaler().fit_transform(X)\n\n# Calculating Eigenvectors and eigenvalues of Cov matirx\nmean_vec = np.mean(X_std, axis=0)\ncov_mat = np.cov(X_std.T)\neig_vals, eig_vecs = np.linalg.eig(cov_mat)\n# Create a list of (eigenvalue, eigenvector) tuples\neig_pairs = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]\n\n# Sort the eigenvalue, eigenvector pair from high to low\neig_pairs.sort(key = lambda x: x[0], reverse= True)\n\n# Calculation of Explained Variance from the eigenvalues\ntot = sum(eig_vals)\nvar_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)] # Individual explained variance\ncum_var_exp = np.cumsum(var_exp) # Cumulative explained variance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trace1 = go.Scatter(\n    x=list(range(13)),\n    y= cum_var_exp,\n    mode='lines+markers',\n    name=\"'Cumulative Explained Variance'\",\n#     hoverinfo= cum_var_exp,\n    line=dict(\n        shape='spline',\n        color = 'goldenrod'\n    )\n)\ntrace2 = go.Scatter(\n    x=list(range(13)),\n    y= var_exp,\n    mode='lines+markers',\n    name=\"'Individual Explained Variance'\",\n#     hoverinfo= var_exp,\n    line=dict(\n        shape='linear',\n        color = 'black'\n    )\n)\nfig = tls.make_subplots(insets=[{'cell': (1,1), 'l': 0.7, 'b': 0.5}],print_grid=True)\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2,1,1)\nfig.layout.title = 'Explained Variance plots - Full and Zoomed-in'\nfig.layout.xaxis = dict(range=[0, 80], title = 'Feature columns')\nfig.layout.yaxis = dict(range=[0, 60], title = 'Explained Variance')\n# fig['data'] = []\n# fig['data'].append(go.Scatter(x= list(range(784)) , y=cum_var_exp, xaxis='x2', yaxis='y2', name = 'Cumulative Explained Variance'))\n# fig['data'].append(go.Scatter(x= list(range(784)) , y=cum_var_exp, xaxis='x2', yaxis='y2', name = 'Cumulative Explained Variance'))\n\n# fig['data'] = go.Scatter(x= list(range(784)) , y=cum_var_exp, xaxis='x2', yaxis='y2', name = 'Cumulative Explained Variance')]\n# fig['data'] += [go.Scatter(x=list(range(784)), y=var_exp, xaxis='x2', yaxis='y2',name = 'Individual Explained Variance')]\n\n# # fig['data'] = data\n# # fig['layout'] = layout\n# # fig['data'] += data2\n# # fig['layout'] += layout2\n# py.iplot(fig, filename='inset example')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Invoke SKlearn's PCA method\nn_components = 3\npca = PCA(n_components=n_components).fit(train.values)\n\n#eigenvalues = pca.components_.reshape(n_components, 28, 28)\n\n# Extracting the PCA components ( eignevalues )\n#eigenvalues = pca.components_.reshape(n_components, 28, 28)\neigenvalues = pca.components_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Delete our earlier created X object\ndel X\n# Taking only the first N rows to speed things up\nX= train[:300].values\ndel train\n# Standardising the values\nX_std = StandardScaler().fit_transform(X)\n\n# Call the PCA method with 5 components. \npca = PCA(n_components=3)\npca.fit(X_std)\nX_5d = pca.transform(X_std)\n\n# For cluster coloring in our Plotly plots, remember to also restrict the target values \nTarget = target[:300]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trace0 = go.Scatter(\n    x = X_5d[:,0],\n    y = X_5d[:,1],\n#     name = Target,\n#     hoveron = Target,\n    mode = 'markers',\n    text = Target,\n    showlegend = False,\n    marker = dict(\n        size = 8,\n        color = Target,\n        colorscale ='Jet',\n        showscale = False,\n        line = dict(\n            width = 2,\n            color = 'rgb(255, 255, 255)'\n        ),\n        opacity = 0.8\n    )\n)\ndata = [trace0]\n\nlayout = go.Layout(\n    title= 'Principal Component Analysis (PCA)',\n    hovermode= 'closest',\n    xaxis= dict(\n         title= 'First Principal Component',\n        ticklen= 5,\n        zeroline= False,\n        gridwidth= 2,\n    ),\n    yaxis=dict(\n        title= 'Second Principal Component',\n        ticklen= 5,\n        gridwidth= 2,\n    ),\n    showlegend= True\n)\n\n\nfig = dict(data=data, layout=layout)\npy.iplot(fig, filename='styled-scatter')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans # KMeans clustering \n# Set a KMeans clustering with 9 components ( 9 chosen sneakily ;) as hopefully we get back our 9 class labels)\nkmeans = KMeans(n_clusters=9)\n# Compute cluster centers and predict cluster indices\nX_clustered = kmeans.fit_predict(X_5d)\n\ntrace_Kmeans = go.Scatter(x=X_5d[:, 0], y= X_5d[:, 1], mode=\"markers\",\n                    showlegend=False,\n                    marker=dict(\n                            size=8,\n                            color = X_clustered,\n                            colorscale = 'Portland',\n                            showscale=False, \n                            line = dict(\n            width = 2,\n            color = 'rgb(255, 255, 255)'\n        )\n                   ))\n\nlayout = go.Layout(\n    title= 'KMeans Clustering',\n    hovermode= 'closest',\n    xaxis= dict(\n         title= 'First Principal Component',\n        ticklen= 5,\n        zeroline= False,\n        gridwidth= 2,\n    ),\n    yaxis=dict(\n        title= 'Second Principal Component',\n        ticklen= 5,\n        gridwidth= 2,\n    ),\n    showlegend= True\n)\n\ndata = [trace_Kmeans]\nfig1 = dict(data=data, layout= layout)\n# fig1.append_trace(contour_list)\npy.iplot(fig1, filename=\"svm\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}