{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"#Library imports\nimport numpy as np\nimport pandas as pd\n#Ploting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.figure_factory as ff\n#Model\nfrom sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n#Metrics\nfrom sklearn.metrics import accuracy_score, plot_confusion_matrix\nfrom sklearn.metrics import f1_score, make_scorer, roc_curve, auc\nfrom scipy.stats import uniform\nfrom scipy.stats import randint\n#dataset\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#saving the dataset as df\ndf = pd.read_csv(\"/kaggle/input/heart-attack-analysis-prediction-dataset/heart.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Pre prossesing the data**","metadata":{}},{"cell_type":"code","source":"#Let's see if there is any \"missing\" data\ndf.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Features Distribution","metadata":{}},{"cell_type":"code","source":"#Let's plot the variables distribution\n\n#Starting with age\nfig = plt.figure(figsize=(25, 25))\nfig = plt.subplot(4,3,1)\nsns.kdeplot(df['age'],label=' Age')\nsns.despine()\nplt.legend()\n\n#Now for the sex\nfig = plt.subplot(4, 3, 2)\ndf['sex'].replace([0,1],['Women','Men']).value_counts().plot(kind='barh', label=' Entries')\nsns.despine()\nplt.legend()\n\n#Distribution of different types of chest pain\nfig = plt.subplot(4, 3, 3)\ncp = df['cp'].replace([0, 1, 2, 3],['Typical','Atypical','Non-anginal','Asymptomatic']).value_counts().plot(kind='barh', label=' Chest Pain entries')\nsns.despine()\nplt.legend()\n\n#Resting blood pressure\nfig = plt.subplot(4, 3, 4)\nsns.histplot(x = df['trtbps'], kde=True, label=' Blood pressure')\nplt.legend()\nsns.despine()\n\n#Cholestoral\nfig = plt.subplot(4, 3, 5)\nsns.histplot(x = df['chol'], kde = True, label = 'Cholestoral')\nplt.legend()\n\n#Blood sugar while fasting\nfig = plt.subplot(4, 3, 6)\nsns.countplot(x=df['fbs'].replace([0, 1],['No', 'Yes']))\nplt.title('Is blood sugar > 120 mg/dl?')\nsns.despine()\n\n#Resting eletrocardiographic results\nfig = plt.subplot(4, 3, 7)\nsns.countplot(y=df['restecg'].replace([0, 1, 2], ['Normal', 'ST-T wave abnormality', \n                                                  'left ventricular hypertrophy']))\nsns.despine()\nplt.title('Resting eletrocardiographic results')\n\n#Maximum heart rate achieved\nfig = plt.subplot(4, 3, 8)\nsns.histplot(x = df['thalachh'], kde=True)\nsns.despine()\nplt.title('Maximum heart rate achieved')\n\n#Distribution of induced angina\nfig = plt.subplot(4, 3, 9)\nsns.countplot(x = df['exng'].replace([0, 1], ['No','Yes']))\nplt.title('Does angina occur on exercise?')\nsns.despine()\n\n#Old peak\nfig = plt.subplot(4, 3, 10)\nsns.kdeplot(x = df['oldpeak'], label='St depression')\n#Observation: an st depression is a finding on an electrocardiogram\nplt.title('ST depression induced by exercise relative to rest')\nsns.despine()\n\n#Slope of the peak exercise\nfig = plt.subplot(4, 3, 11)\nsns.countplot(x = df['slp'].replace([0, 1, 2], ['Unsloping', 'Flat', 'Donwsloping']))\nplt.title('At the peak of an exercise, how does the ST segment slope?')\nsns.despine()\n\n#Number of major vessels\nfig = plt.subplot(4, 3, 12)\nsns.countplot(x = df['caa'])\nplt.title('Number of major vessels')\nsns.despine()\n#I don't get why in the original paper there are only 3 vessels and here are 5, but let's continue\n\n#The thall column is also strange in the dataset (3 entries, and here are 4), I won't be using it.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From here, some valuable information can be observed. \n\n- In this dataset, it's more probable to find men, between 50 to 65 years old. \n\n- Most of the recorded abdominal pain is typical anginal and non-anginal, and don't occur during exercise. \n\n- The typical blood pressure is between 120mg/dl to 140mg/dl, although, while fasting, the pressure reduces to lower than 120mg/dl. \n\n- The cholestoral levels are usually between 200 and 300. \n\n- While resting, the eletrocardiographic results are normal or with a ST-T wave abnormality. While resting the ST depression is normally between 0 and 2.\n\n- The maximum heart rate is more distributed in 140 to 180.\n\nLet's try looking for some correlations.\n","metadata":{}},{"cell_type":"markdown","source":"### Pearson's correlation heatmap","metadata":{}},{"cell_type":"code","source":"#The pearson's r correlation heatmap\nplt.figure(figsize=(12,12))\nsns.heatmap(df.corr(), vmin=-1, cmap=\"Blues\", annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The output column refers to the chance of heart attack. Looking at the last line (or column) in the heatmap, the variable with more \"color\" has more correlation with the output. Of course, correlation is not causality, but it's at least good to know that:\n- In the dataset, there is an observed slightly positive correlation between chest pain and maximum heart rate to a greater risk of heart attack.(If that wasn't obvious)\n\n- In contrary, there is a slightly negative correlation between age and heart attack. (The younger the lower the risk) Sex and heart attack. (In the dataset, men appear more having higher risk).","metadata":{}},{"cell_type":"markdown","source":"### Comparison in a single feature","metadata":{}},{"cell_type":"code","source":"# Making a distribution comparison between outputs\natk = df[df['output']==1]['age'].fillna(0.0).astype(float)\n#Creates a mask and apply 0 to any number not inside output = 1 \natk_no = df[df['output']==0]['age'].fillna(0.0).astype(float)\n#Creates another mask and apply 0 to any number not inside output = 0\nfi = ff.create_distplot([atk, atk_no], ['Risk cases','No risk cases'], bin_size=0.65, curve_type='normal'\n                        ,colors =  ['crimson','black'])\nfi.update_layout(\n    title=\"Heart Attack distibution over age\",\n    xaxis_title=\"Age\")\nfi.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that the data collected more cases of heart attack at youger ages. Also, the risk of heart attack starts to decline in the 50 years mark. If we were to apply a hipothesis test here, we would be able to make some inferences, but I'll leave it for later.\nLet's apply the same idea for some other cases.","metadata":{}},{"cell_type":"code","source":"# Same idea, but for maximum heart rate\natk = df[df['output']==1]['thalachh'].fillna(0.0).astype(float)\natk_no = df[df['output']==0]['thalachh'].fillna(0.0).astype(float)\nfi = ff.create_distplot([atk, atk_no], ['Risk cases','No risk cases'], bin_size=0.65, curve_type='normal'\n                        ,colors =  ['crimson','black'])\nfi.update_layout(\n    title=\"Heart Attack distibution over maximum heart rate\",\n    xaxis_title=\"Heart rate\")\nfi.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that the risk of heart attack increases just as the maximum heart rate increases, which makes some logical sense.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"### **Processing** ","metadata":{}},{"cell_type":"markdown","source":"There is not many values to alocate between train and test in the dataset. In this case I will use the stratified K fold cross validation method, in order to preserve information.","metadata":{}},{"cell_type":"code","source":"#Let's start by selecting some random information to cross validate later.\ndf_out = df['output'].to_numpy()\ndf_feats = df.drop(columns='output').to_numpy()\n#For this, I'll use the scikit learn library \ncv = StratifiedKFold(n_splits = 10)\n#I'll get the values of the cv\nfor train_index, test_index in cv.split(df_feats, df_out):\n    xtrain, xtest = df_feats[train_index], df_feats[test_index]\n    ytrain, ytest = df_out[train_index], df_out[test_index]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# K nearest neighbor","metadata":{}},{"cell_type":"code","source":"#Let's see how the nearest neighbor performs\nknn = KNeighborsClassifier(n_neighbors= 5)\nknn = knn.fit(xtrain, ytrain)\nknnpred = knn.predict(xtest)\nprint(f'The acccuracy of the KNN is {accuracy_score(ytest, knnpred):.4}')\nprint(f'The F1 score of the KNN is {f1_score(ytest, knnpred):.4}')\nprint(f'There are {len(ytest)} values to test.')\nprint('Its confusion matrix is:')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(knn, xtest, ytest, colorbar=False,\n                     cmap='Blues')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I don't plan on getting the KNN to actually perform better here, I just wanted to see how well it would perform in a \"simplistic way\". That said, the model is actually performing pretty well considering that it was very simple to apply, but it can do better.","metadata":{}},{"cell_type":"markdown","source":"## Random Forest with search on hyper parameters","metadata":{}},{"cell_type":"code","source":"#Let's start with some basics\nforest = RandomForestClassifier()\n#For the search, F1 score will be interesting to compare precision and recall metrics\nf1 = make_scorer(f1_score)\n#Now it's just some code for the random forest params it will look into\nparams = dict(n_estimators = randint(100, 200), bootstrap = [True, False], criterion = ['gini', 'entropy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code for the search\nsearch = RandomizedSearchCV(forest, params, n_iter = 5,\n                            cv = cv, scoring = f1, random_state = 15)\nsearch.fit(df_feats,df_out)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The best params are\nprint(f'Best score: {search.best_score_}\\nBest paramns: {search.best_params_}\\nBest estimator: {search.best_estimator_}\\nNumber of splits:{search.n_splits_}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Metrics**","metadata":{}},{"cell_type":"code","source":"#Let's use the params and apply it on the model now\nforest = RandomForestClassifier(n_estimators = 171, criterion = 'entropy',\n                               bootstrap = True, random_state=15)\n\n#Now for the model\nforest.fit(xtrain, ytrain)\nforestpred = forest.predict(xtest)\nprint(f\"The model's accuracy score is {accuracy_score(ytest, forestpred):.4}\")\nprint(f\"The model's f1 score is {f1_score(ytest, forestpred):.4}\")\nprint(f'There are {len(ytest)} values to test.')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(forest, xtest, ytest, colorbar=False,\n                     cmap='Blues')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model has a better accuracy and F1 score. By it's confusion matrix it looks like the model has better classification power over the true positive risk, but still has some problems to classify as seem by the false positives and negatives. Looking at the AUC, we can see if the model is doing the best it can.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#Ploting a AUC to look for possible scenarios\nfpr, tpr, threshold = roc_curve(ytest, forestpred)\nroc_auc = auc(fpr, tpr)\n\nplt.figure(figsize=(6.5,6.5))\nplt.title('Receiver Operating Characteristic',fontsize=20)\nplt.plot(fpr, tpr, 'b', label = f'AUC = {roc_auc:0.2f}', lw=2)\nplt.legend(loc = 'lower right', fontsize=15)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate', fontsize=15)\nplt.xlabel('False Positive Rate', fontsize=15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}