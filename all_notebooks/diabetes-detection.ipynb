{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom dateutil import parser\n%matplotlib inline\n\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nimport pickle\nfrom lightgbm import LGBMClassifier\nprint('Library Loaded')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/diabetes/diabetes.csv')#,engine='python')\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Correlation matrix\ncorrmat = df.corr()\nfig = plt.figure(figsize = (12, 12))\n\nsns.heatmap(corrmat, vmax = 1, square = True,annot=True,vmin=-1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.hist(figsize=(12,12))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df,hue='Outcome')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for unbalance \ndf.Outcome.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"# rows in dataframe {0}\".format(len(df)))\nprint(\"-------------------------------------------\")\nprint(\"# rows missing Glucose: {0}\".format(len(df.loc[df.Glucose == 0 ])))\nprint(\"# rows missing BloodPressure: {0}\".format(len(df.loc[df.BloodPressure == 0 ])))\nprint(\"# rows missing SkinThickness: {0}\".format(len(df.loc[df.SkinThickness == 0 ])))\nprint(\"# rows missing insulin: {0}\".format(len(df.loc[df.Insulin == 0 ])))\nprint(\"# rows missing bmi: {0}\".format(len(df.loc[df.BMI == 0 ])))\nprint(\"# rows missing Age: {0}\".format(len(df.loc[df.Age == 0 ])))\nprint(\"# rows missing Pregnancies: {0}\".format(len(df.loc[df.Pregnancies == 0 ])))\nprint(\"# rows missing DiabetesPedigreeFunction: {0}\".format(len(df.loc[df.DiabetesPedigreeFunction == 0 ])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('Outcome',axis=1) # predictor feature coloumns\ny = df.Outcome\n\n\nX_train , X_test , y_train , y_test = train_test_split(X, y, test_size = 0.20, random_state = 10)\n\nprint('Training Set :',len(X_train))\nprint('Test Set :',len(X_test))\nprint('Training labels :',len(y_train))\nprint('Test Labels :',len(y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.preprocessing import Imputer\nfrom sklearn.impute import SimpleImputer\n#impute with mean all 0 readings\n\nfill = SimpleImputer(missing_values = 0 , strategy =\"mean\")\n\nX_train = fill.fit_transform(X_train)\nX_test = fill.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Building and Evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training Set :',len(X_train))\nprint('Test Set :',len(X_test))\nprint('Training labels :',len(y_train))\nprint('Test Labels :',len(y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def FitModel(X_train,y_train,X_test,y_test,algo_name,algorithm,gridSearchParams,cv):\n    np.random.seed(10)\n   \n    \n    grid = GridSearchCV(\n        estimator=algorithm,\n        param_grid=gridSearchParams,\n        cv=cv, scoring='accuracy', verbose=1, n_jobs=-1)\n    \n    \n    grid_result = grid.fit(X_train, y_train)\n    best_params = grid_result.best_params_\n    pred = grid_result.predict(X_test)\n    cm = confusion_matrix(y_test, pred)\n   # metrics =grid_result.gr\n    print(pred)\n    #pickle.dump(grid_result,open(algo_name,'wb'))\n   \n    print('Best Params :',best_params)\n    print('Classification Report :',classification_report(y_test,pred))\n    print('Accuracy Score : ' + str(accuracy_score(y_test,pred)))\n    print('Confusion Matrix : \\n', cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create regularization penalty space\npenalty = ['l1', 'l2']\n\n# Create regularization hyperparameter space\nC = np.logspace(0, 4, 10)\n\n# Create hyperparameter options\nhyperparameters = dict(C=C, penalty=penalty)\n\nFitModel(X_train,y_train,X_test,y_test,'LogisticRegression',LogisticRegression(),hyperparameters,cv=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XgBoost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"param ={\n            'n_estimators': [100, 500, 1000,1500, 2000],\n            'max_depth' :[2,3,4,5,6,7],\n    'learning_rate':np.arange(0.01,0.1,0.01).tolist()\n           \n        }\n\nFitModel(X_train,y_train,X_test,y_test,'XGBoost',XGBClassifier(),param,cv=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"param ={\n            'n_estimators': [100, 500, 1000,1500, 2000],\n           \n        }\nFitModel(X_train,y_train,X_test,y_test,'Random Forest',RandomForestClassifier(),param,cv=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SVC","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"param ={\n            'C': [0.1, 1, 100, 1000],\n            'gamma': [0.0001, 0.001, 0.005, 0.1, 1, 3, 5]\n        }\nFitModel(X_train,y_train,X_test,y_test,'SVC',SVC(),param,cv=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Balancing the Dataset - Over Sampling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nsm =SMOTE(random_state=42)\nX_res_OS , Y_res_OS = sm.fit_resample(X,y)\npd.Series(Y_res_OS).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train , X_test , y_train , y_test = train_test_split(X_res_OS, Y_res_OS, test_size = 0.20, random_state = 10)\n\nprint('Training Set :',len(X_train))\nprint('Test Set :',len(X_test))\nprint('Training labels :',len(y_train))\nprint('Test Labels :',len(y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfill = SimpleImputer(missing_values = 0 , strategy =\"mean\")\n\nX_train = fill.fit_transform(X_train)\nX_test = fill.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training Set :',len(X_train))\nprint('Test Set :',len(X_test))\nprint('Training labels :',len(y_train))\nprint('Test Labels :',len(y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression - After Over sampling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create regularization penalty space\npenalty = ['l1', 'l2']\n\n# Create regularization hyperparameter space\nC = np.logspace(0, 4, 10)\n\n# Create hyperparameter options\nhyperparameters = dict(C=C, penalty=penalty)\n\nFitModel(X_train,y_train,X_test,y_test,'LogisticRegression',LogisticRegression(),hyperparameters,cv=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XgBoost  - After Over sampling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"param ={\n            'n_estimators': [100, 500, 1000,1500, 2000],\n            'max_depth' :[2,3,4,5,6,7],\n    'learning_rate':np.arange(0.01,0.1,0.01).tolist()\n           \n        }\n\nFitModel(X_train,y_train,X_test,y_test,'XGBoost',XGBClassifier(),param,cv=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest  - After Over sampling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"param ={\n            'n_estimators': [100, 500, 1000,1500, 2000],\n           \n        }\nFitModel(X_train,y_train,X_test,y_test,'Random Forest',RandomForestClassifier(),param,cv=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SVC  - After Over sampling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"param ={\n            'C': [0.1, 1, 100, 1000],\n            'gamma': [0.0001, 0.001, 0.005, 0.1, 1, 3, 5]\n        }\nFitModel(X_train,y_train,X_test,y_test,'SVC',SVC(),param,cv=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}