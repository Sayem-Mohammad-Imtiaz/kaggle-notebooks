{"cells":[{"metadata":{"_uuid":"9151942070d0010fefeeff9db9f02124a9576abb","_cell_guid":"3ad086f0-7f56-41de-9772-6a663407011e"},"cell_type":"markdown","source":" # Spam or Ham? \nThe goal of the project is to obtain a model that successfully will label a SMS to be spam or not. The method of obtaining that model will be the following:\n\n  * Download dataset and set up environment\n  * Explore data to gain domain knowledge for feature engineering \n  * Establish benchmark model\n  * Preprocess data and create features\n  * Implement at least three different supervised learning models and evaluate performance\n  * Choose best performing model and perform Grid Search \n  * Evaluate model versus benchmark model\n  * Evaluate results\n\n\n### Metrics\nA common metric for evaluation performance in binary classifications is \n$accuracy  = \\frac{correct\\: predictions}{total\\: predictions}$. The metric does however fall short when the dataset is heavily skewed like it is in this case. Just predicting that all SMS text messages would achieve a accuracy of 75\\% on the dataset and in a real life scenario when the overwhelming majority of text messages are not spam it would probably achieve an accuracy close to 100\\%. The model would however be completely useless, despite the high accuracy. \n\n\nTo solve this problem we introduce the Precision and Recall measurement. The\n$precision = \\frac{true\\: positives}{true\\: positives\\ + \\ false \\: positives}$ metric explains how many of messages classified as spam that actually were spam. The\n$recall = \\frac{true\\: positives}{true\\: positives\\ + \\ false\\: negatives}$ metric explains how many of the total spam messages the algorithm was able to correctly identify.\n\nWhat metric out of the two to use is determined on a case to case basis. When it comes to spam, most people are probably okay with receiving a spam message from time to time but are probably not okay with missing an important text message because it was labeled as spam. Therefore, we will evaluate our model with en emphasis on precision. Luckily though, there are another metric called\n$F_\\beta = (1+ \\beta^2) * \\frac{ \\cdot precision\\cdot recall}{\\beta^2*precision+ recall}$ that takes into account both the precision and the recall. Which of the two metrics to weigh higher than the other is determined by $\\beta$, a $\\beta$ lower than $1$ put emphasis on recall and higher than $1$ weighs precision higher. In this project $\\beta = 1.5$ will be used.\n## 1. Import Dataset\n"},{"metadata":{"_uuid":"0750966154f3d3308a6ac35b57adaa3f887d9cea","collapsed":true,"_cell_guid":"279d2664-4585-4cd4-a6f7-ba953bd8b566","trusted":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input dat\n#a files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\ndata = pd.read_csv(\"../input/spam.csv\",encoding='latin-1')\n\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ba4852e9a82795b1f78b0e53ff5770582494f1f","_cell_guid":"2e4b7766-44f7-4856-8269-5bd85bda8c6d"},"cell_type":"markdown","source":"Lets inspect the first 5 objects of the data"},{"metadata":{"_uuid":"6c4d8f40861957f90176d9a55b28286a106f29cb","collapsed":true,"_cell_guid":"ae36f471-6a2b-4aae-aa37-284f1599b71a","trusted":false},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce50ffafee8368196f567a5be9bed2e213867287","_cell_guid":"f5e87676-5724-4313-9db8-9c06d6669aaa"},"cell_type":"markdown","source":"As we can see above there are 3 columns created in the csv import process that contains no information, let's remove those and rename the columns appropriately "},{"metadata":{"_uuid":"d9a9ccf994537effcb10acfdf19270fd1259ea29","collapsed":true,"_cell_guid":"9f169bb0-9c68-43e1-9f23-477fb202aa9d","trusted":false},"cell_type":"code","source":"data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\ndata = data.rename(columns={\"v1\":\"label\", \"v2\":\"sms\"})\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b36dc1fca4dd6d4bc77ede71ff7310b8cc9908c1","_cell_guid":"b5fb2a4a-6867-4a18-a355-5342ba0ac7ef"},"cell_type":"markdown","source":"Lets change the label to be easier for a the computer to process, let 0 resemble ham and 1 spam  "},{"metadata":{"_uuid":"7b8471366f87a6afd22e8b9134716e7cbd3683db","collapsed":true,"_cell_guid":"950aba1b-6588-4db5-ab58-2d4e186c4585","trusted":false},"cell_type":"code","source":"data['y'] = data.label.map({'ham': 0, 'spam': 1})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b3fb3ffe8936a60c5220f1e34ef59ef52fbc522","collapsed":true,"_cell_guid":"f21a1f25-65ec-406f-b291-cbf42ab6d672","trusted":false},"cell_type":"code","source":"print(data.shape)\nprint(data.label.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3a3bb103432564ab2e8e4700628f22c9dcb4653","collapsed":true,"_cell_guid":"70ad1300-376f-42ff-bf98-7187d5869a5a"},"cell_type":"markdown","source":"## Data Exploration \n\nThe dataset provided consists of 5572 total text messages. Every data point consists of two features, the label and the text message itself. The messages is labeled as either spam or ham.  There are a total of 4825 text messages labeled ham and 747 labeled as spam.\n\n\n#### Text Length \nLet's start by looking if there is any difference in the length of the two types of text messages. Since spam a lot of times contains long messages about different deals and things that you can win, intuitively I feel like the spams SMS should be longer on average \n"},{"metadata":{"_uuid":"36c160662808beadf580a364701c2e18aa736537","collapsed":true,"_cell_guid":"118d9625-0f84-4aeb-abc1-9fe0220a3914","trusted":false},"cell_type":"code","source":"# Add a column for the lenght of the SMS\ndata['length'] = data.sms.str.len()\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cae85d0439449db236a11af6cf8d5677fdd34cea","collapsed":true,"_cell_guid":"93d0f604-6b09-4118-b115-6eac3150af1f","trusted":false},"cell_type":"code","source":"spam = data[data['label'] == 'spam']\nham = data[data['label'] == 'ham']\nprint(\"Data for the spam:\")\nprint(spam.length.describe())\n\nprint(\"\\nData for the ham:\")\nprint(ham.length.describe())\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42bcef61df7cb9ec5d79dfefadbd3739227e1f65","_cell_guid":"64468477-b3b3-425a-855a-43396e37370f"},"cell_type":"markdown","source":"As suspected the spam messages seems to be longer on average, in fact the average length of a ham SMS is almost half of that of a Spam SMS. \nThe standard deviation of the the ham SMS length is a lot larger though and the max of the ham SMS is over 4 times that of spam. \nSince we are seeing such clear differences in the SMS length we should definately try to use the length as a feature to try to improve the model later on. "},{"metadata":{"_uuid":"078eeb2b5422607947f64fb26fd6f1a809fdb74b","collapsed":true,"_cell_guid":"b369f613-fce0-433c-be8a-bc676b3b3868","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30b20a6881b286e6a97aa314e37958d51f448ebe","_cell_guid":"8bc28970-fa94-451b-97d7-46b503111608"},"cell_type":"markdown","source":"#### Word usage "},{"metadata":{"_uuid":"2f7d7d77b144b466d274ff9ba90a457eef8c6685","_cell_guid":"11c8ed4c-84fb-4423-a268-45fe22f569d2"},"cell_type":"markdown","source":"Let's look at word usage. One nice way of doing this is constructing a word cloud which is a nice graphical representation of the most used words in a large corpus of words\n\nWe are also gonna see if there seems to any indicator if a SMS is spam or ham depending on the spelling. When it comes to email a lot of the spam will contain misspellings and wierd grammar and can therefore be a good way of determining if an email is spam or not. My first thought was that it might be the same for SMS but then I realized that when I text it's usually with friends and I use a lot of slang and definately do not care that much about my spelling. However I think it would be an interesting thing to look a bit more into. "},{"metadata":{"_uuid":"d3bbabf1e7456d2334d569e348a566356b00272b","collapsed":true,"_cell_guid":"6840e2de-f2da-451d-a2c1-ec8cddc51dde","trusted":false},"cell_type":"code","source":"# Import neccesary libraries \nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem import SnowballStemmer\nfrom wordcloud import WordCloud \nimport matplotlib.pyplot as plt\n\n\n#nltk.download('stopwords')\n#stemmer = SnowballStemmer('english')\nword_set = set(nltk.corpus.words.words()) # a set containing all english words to determine  \nstop_words = set(stopwords.words(\"english\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"890e6f1a70dabd6e3e2c9dbce55192ecce346f67","collapsed":true,"_cell_guid":"f77fc7d2-9b18-4d26-8a51-144ada67f00f","trusted":false},"cell_type":"code","source":"# strings to store long strings of word for creating \nham_wordlist = ''\ntotal_ham_words = 0\ntotal_misspelled_ham = 0\n\nspam_wordlist = ''\ntotal_spam_words = 0\ntotal_misspelled_spam = 0\n\n# tokenize and remove non alphanumerical \ntknzr = RegexpTokenizer(r'\\w+')\nfor text in ham.sms:\n    tokens = tknzr.tokenize(text.lower())\n    # Remove all word\n    for word in tokens:\n        total_ham_words += 1 # increment total words for every word\n        if word not in stop_words: # only save words that are not in stop words\n            ham_wordlist = ham_wordlist + ' ' + word  \n        if word not in word_set:\n            total_misspelled_ham += 1 # count the total of misspelled words \n\n    \nfor text in spam.sms:\n    tokens = tknzr.tokenize(text.lower())\n    # Remove all word\n    for word in tokens:\n        total_spam_words += 1 # increment total words for every word\n        if word not in stop_words: # only save words that are not in stop words\n            spam_wordlist = spam_wordlist + ' ' + word \n        if word not in word_set:\n            total_misspelled_spam += 1 # count the total of misspelled words \n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"045d6dfdb55a08d9eb6326eeae3e229856ee4ac2","collapsed":true,"_cell_guid":"a24810bc-5bf0-40d8-830f-cd9e34a95267","trusted":false},"cell_type":"code","source":"spam_wordcloud = WordCloud(background_color=\"lightgrey\", width=600, height=400).generate(spam_wordlist)\nham_wordcloud = WordCloud(background_color=\"lightgrey\", width=600, height=400).generate(ham_wordlist)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c5509dd8208e82df6580a02de7ed36bab91bed0","collapsed":true,"_cell_guid":"8b4d07cf-12ec-4d3c-8028-7363eea63b5f","trusted":false},"cell_type":"code","source":"# Ham wordcloud\nplt.figure( figsize=(10,8))\nplt.imshow(ham_wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7b6d7cd835464315a3348d802e1e0c2c441fa3f","collapsed":true,"_cell_guid":"109d26eb-d113-44cf-a3ed-f4138c96d631","trusted":false},"cell_type":"code","source":"# Spam wordcloud\n\nplt.figure( figsize=(10,8))\nplt.imshow(spam_wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"342932c4054ec71158f1dcbb0b2ad903fdde1c02","_cell_guid":"f485b01f-3265-412d-bc07-c0e48a2cf37e"},"cell_type":"markdown","source":"As seen in the wordclouds above the most used words differ a lot from the two classes. While the most used words for records labeled ham does not seem to follow any certain pattern, the SMS labeled spam certainly does. The most used word is free, which is used in for example a lot of gambling advertisements. "},{"metadata":{"_uuid":"d05a30e4ba71e85dfbcd57dccae611c8ddcb0f55","collapsed":true,"_cell_guid":"b8748cf5-9983-479b-836f-26ae230fae86","trusted":false},"cell_type":"code","source":"print('HAM \\n total words: {} \\n total misspells: {} \\n %: {}'.format(total_ham_words,\n                                                                     total_misspelled_ham, \n                                                                     total_misspelled_ham*100 / total_ham_words))\n\nprint('SPAM \\n total words: {} \\n total misspells: {} \\n %: {}'.format(total_spam_words,\n                                                                     total_misspelled_spam, \n                                                                     total_misspelled_spam*100 / total_spam_words))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e1cf31325c5ee39eebfb81d666f9e3f89597c7e","collapsed":true,"_cell_guid":"480fe078-ad5a-46cd-a2a8-023127612b13","trusted":false},"cell_type":"code","source":"# Function to calculate the number of misspells in each message\ndef calculate_misspells(x):\n    #print(x)\n    tokens = tknzr.tokenize(x.lower())\n    #print(tokens)\n    corr_spelled = [word for word in tokens if word in word_set]\n    if len(tokens) == 0:\n        return 0\n    return len(corr_spelled)/len(tokens)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38f4197ce14aadfd18b7b9a85acb8176a7491ace","_cell_guid":"2ebe4a27-6bb0-475e-baaf-ea91d71fca49"},"cell_type":"markdown","source":"When looking at all words written in text messages that were labeled as Ham the percentage of misspelled words were $16.5\\%$. For the text messages labeled Spam the same number was $35.5 \\%$ \n"},{"metadata":{"_uuid":"8849097129bf3e38bda9a493cca49e958aabee65","collapsed":true,"_cell_guid":"603f3e3e-601b-42b8-9ab1-afbbe71aec72","trusted":false},"cell_type":"code","source":"data['misspells'] = data.sms.apply(calculate_misspells)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c4da64ab57fcf06b83ef4bf63597de1d87a4c0e","collapsed":true,"_cell_guid":"ccf2311a-88c7-4e9e-9cb4-c77b026fbb5a","trusted":false},"cell_type":"code","source":"spam = data[data['label'] == 'spam']\nham = data[data['label'] == 'ham']\nprint(\"Data for the spam:\")\nprint(spam.misspells.describe())\n\nprint(\"\\nData for the ham:\")\nprint(ham.misspells.describe())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"934ce389915176f2f2ff48b764594f86f779a92e","_cell_guid":"9c85dd80-89c4-42e3-8fb5-8395dcecf8c6"},"cell_type":"markdown","source":"When looking at the misspellings on a record to record basis the statistics  where $1.0$ notes a text message that contain no errors and $0.0$ notes a message where all words were misspelled. Even though the result is not as clear as in the case of text length, the possibility of using misspellings as a feature should be investigated. "},{"metadata":{"_uuid":"d72e746b0283d3eb4262f45bf2857e5be2a56139","collapsed":true,"_cell_guid":"9312e63c-790d-4206-a45f-a85ec2d461ee","trusted":false},"cell_type":"code","source":"data_ready = data.drop([ \"label\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3473733a5c1ffc07cbd772da245838642aad3b7","collapsed":true,"_cell_guid":"4d5eac21-f6b7-4de6-a131-4fbbee4603a1","trusted":false},"cell_type":"code","source":"data_ready.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56ec3d682d13730aa76e907c97547065bc5a90d2","collapsed":true,"_cell_guid":"0ea1035d-21f1-4323-a64b-990bc37f4a48","trusted":false},"cell_type":"code","source":"# Create a function to remove all stopwords from the \ndef remove_stopwords(x):\n    #print(x)\n    tokens = tknzr.tokenize(x.lower())\n    #print(tokens)\n    stop_removed = [word for word in tokens if word not in stop_words]\n    \n    return \" \".join(stop_removed)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ae0beb565714bf10186c54c2a00e44570604339","collapsed":true,"_cell_guid":"776fdd19-f0c1-4167-b91d-65a16262f690","trusted":false},"cell_type":"code","source":"data_ready.sms = data.sms.apply(remove_stopwords)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4383123b92b29747d54da9c437511d5575c70b85","collapsed":true,"_cell_guid":"be45dbc1-77d7-4e6e-bf17-f825f68a5141","trusted":false},"cell_type":"code","source":"data_x = data_ready.drop(['y'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b85c71c7699f931e0466c2ec241239f9e747839","collapsed":true,"_cell_guid":"281bc61b-6ff9-4f00-9380-c77a2d186ea9","trusted":false},"cell_type":"code","source":"data_x.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46879c56da74c6337e6e6570e34bf81c2ac77b75","_cell_guid":"ab2db2e6-36f6-4db4-9e96-2df7741ff174"},"cell_type":"markdown","source":"### Tf-idf\nTf-idf is a numerical statistic that reflect how important a word is to a document in a corpus. It will be used to extract features from the text messages into a feature vector. The idea is to treat each document as a bag of word while retaining the information about the occurrences of each word. \n\nTf-idf consits of two statistics, tf and idf. Tf is the term frequency, and is basically just the raw count of a term in a document $$ tf_{i,j} = \\frac{n_{i,j}}{\\sum_k n_{k,j}} $$ \nIdf stands for inverse document frequency and measures how much information a word provides. Basically a word that is used a lot will not contain as much information as a word that is used less. Idf is calculated as $idf_i = \\mbox{log} \\frac{|D|}{|{d : t_i \\in d}|} $. The Tf-idf is thereafter calculated as $\\mbox{tf-idf}_{t,d} = (1 +\\log \\mbox{tf}_{t,d}) \\cdot \\log \\frac{N}{\\mbox{df}_t}$\n\n\n### Naive Bayes\nNaive Bayes is a simple supervised learning method based on applying Bayes' theorem. Basically the classifier works by assigning a record the class that has the highest probability of being true given the features, finding the $C_k$ with the highest probability $p(C_k | x_i,...,x_n)$ where $C_k$ represents all possible classes and $x_i$ a feature. Since the above formula is infisible to calculate if the number of features are large enough. Bayes theorem is therefore used to rewrite the problem to be possible to solve, $p(C_k | x_i,...,x_n) = \\frac{p(C_k)p(\\mathbf{x}|C_k)}{p(\\mathbf{x})}$. \n\nTo be able to be used as a classifier the above formula is rewritten and then a decision rule is added. Most usually the decision rle used is to pick the hypothesis that is most probable, know as MAP. This gives us the following formula ${\\displaystyle {\\hat {y}}={\\underset {k\\in \\{1,\\dots ,K\\}}{\\operatorname {argmax} }}\\ p(C_{k})\\displaystyle \\prod _{i=1}^{n}p(x_{i}\\mid C_{k}).}$ for the Naive Bayes Classifier\n\\\\ \\\\\nNaive Bayes have been shown too work really well in binary classification cases and a lot of the early spam detectors were implemented using naive bayes.\n\n### Train test split\nThe data will be split into a training set and a test set to evaluate the performance of the model. If this was not done the we would have no idea of knowing if the model is actually working or just overfitting to the data. \n\n### Benchmark\nSince the data is heavily skeewed a simple benchmark will be to just classify everything as ham (all predictions are 0). The naive predictor achives an Accuracy of $0.87$ and $F_{1.5}$ score of $0.48$"},{"metadata":{"_uuid":"ead0aab5c1e0f8d068e3e11cf329fb8b239f61f9","collapsed":true,"_cell_guid":"8147f312-009a-4e7e-96e9-c197f17eb242","trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(data_x,data[\"y\"], test_size = 0.2, random_state = 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23a6d9a55290bc9422547c6f018c8e0c4120f591","collapsed":true,"_cell_guid":"e544726a-51be-4f79-85c3-f8b95a672c07","trusted":false},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntransvector = TfidfVectorizer()\n\ntfidf1 = transvector.fit_transform(X_train.sms)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"833cf2f14b4e1b764b32063a550572077b71601c","collapsed":true,"_cell_guid":"986aad66-07e9-4262-8920-df0d40fd5eda","trusted":false},"cell_type":"code","source":"X_train_df = tfidf1.todense()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c12453984c8f8dd9658af84777962feb8281db3f","collapsed":true,"_cell_guid":"05790138-57ae-4e6f-8fc4-af64e6392273","trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\n# Initialize a scaler, then apply it to the features\n# The misspells are already scaled between 0-1 but I had to include it due to some wierd error \n# when I tried to only minmax the length feature. \nscaler = MinMaxScaler() \nX_train[['length', 'misspells']] = scaler.fit_transform(X_train[['length', 'misspells']])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a2ece9b63d713bc0cfc3e298924a246874808d1","collapsed":true,"_cell_guid":"44acacf8-5452-45fc-91de-19663eb14bf1","trusted":false},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c893588928466e475738ae2aa29ae2337a0921e9","collapsed":true,"_cell_guid":"30416dac-ecc6-429e-a49a-b967f3c82404","trusted":false},"cell_type":"code","source":"# Convert the Pandas dataframe so that it is a Numpy matrix to concatinate with the tfidf features\nX_train[['length', 'misspells']].as_matrix()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc0f1dfbc3d0fc11dcd566c0f90ba9c8b0ffb2bd","collapsed":true,"_cell_guid":"83f235fd-bb16-4ff2-917c-7e6d922476df","trusted":false},"cell_type":"code","source":"X_train_final = np.concatenate((X_train_df , X_train[['length', 'misspells']].as_matrix()), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88945a21ea95f523a3fb59deb4c2980ac4f1f24a","collapsed":true,"_cell_guid":"e5dc7dfc-7a95-4365-92b0-40d0471add33","trusted":false},"cell_type":"code","source":"# Transform test set\ntfidf_test = transvector.transform(X_test.sms)\nX_test_df = tfidf_test.todense()\nX_test[['length', 'misspells']] = scaler.transform(X_test[['length', 'misspells']])\nX_test_final = np.concatenate((X_test_df , X_test[['length', 'misspells']].as_matrix()), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fabdb3b736d367dd66447bf5606b60541f0001dc","collapsed":true,"_cell_guid":"d58a08f1-503d-43fe-beaa-41a5188c019d","trusted":false},"cell_type":"code","source":"# Try using both naive bayes models \nprediction = dict()\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\ngnb = GaussianNB()\nclf = MultinomialNB()\ngnb.fit(X_train_final,y_train)\nclf.fit(X_train_final,y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f121778d24ff5c57f3028c0d4a7be54f08d60b3","collapsed":true,"_cell_guid":"c0084503-bd00-4298-b93e-b294f77b56a2","trusted":false},"cell_type":"code","source":"prediction[\"gaussian\"] = gnb.predict(X_test_final)\nprediction[\"multinom\"] = clf.predict(X_test_final)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b51bcca70e1f9cc45fef5b7df1161b18e1315bf6","collapsed":true,"_cell_guid":"4a1492fa-3972-41e5-a27c-c9e273d84b6e","trusted":false},"cell_type":"code","source":"# Compare models \nprint(\"F-score Gaussian, F-score Multinom, Accuracy Gaussian, Accuracy Multinom\")\nfrom sklearn.metrics import fbeta_score, accuracy_score\nprint(fbeta_score( y_test, prediction[\"gaussian\"], average='macro', beta=1.5))\nprint(fbeta_score( y_test, prediction[\"multinom\"], average='macro', beta=1.5))\nprint(accuracy_score( y_test, prediction[\"gaussian\"]))\nprint(accuracy_score( y_test, prediction[\"multinom\"]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82a781c3fe652c24fc41b06049f707481b522d4b","collapsed":true,"_cell_guid":"f6edb137-e51e-427a-b493-8bc6e46f8e86","trusted":false},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86d259b6da9807f942987c9d1c5383306aee4815","collapsed":true,"_cell_guid":"6611c407-8fac-4a7f-941b-e742c8472483","trusted":false},"cell_type":"code","source":"# Perform Grid Search on multinomial\nparam_grid = {'alpha': [0, 0.5, 1, 2,5,10], 'fit_prior': [True, False]}\nmultinom = MultinomialNB()\nscorer = make_scorer(fbeta_score, beta=1.5)\nclf = GridSearchCV(multinom, param_grid, scoring=scorer)\nclf.fit(X_train_final,y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"caf253e66bdea29fb6684979783273c7a154c041","collapsed":true,"_cell_guid":"cecb705a-6eea-4383-a8b0-13b7ddc04884","trusted":false},"cell_type":"code","source":"best_clf = clf.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74a6b6bb1e53d7e0bf615d9abf8c4a4e0d2a111b","collapsed":true,"_cell_guid":"885254ec-9f4b-4c35-be9d-d90641e26a9e","trusted":false},"cell_type":"code","source":"best_predictions = best_clf.predict(X_test_final)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ddc137ddea8050406786be1ea63409711c14f04","collapsed":true,"_cell_guid":"ee1129a3-8b44-400f-bf13-51ebf1573a98","trusted":false},"cell_type":"code","source":"print(\"Best model F-beta and Accuracy:\")\nprint(fbeta_score( y_test, best_predictions, average='macro', beta=1.5))\nprint(accuracy_score( y_test, best_predictions))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15fec154ba85997be24cdbab50e694d71f1c9801","collapsed":true,"_cell_guid":"4f566397-5d80-475d-bbfb-734440c365f0","trusted":false},"cell_type":"code","source":"#Benchmark model: \nprint(\"Benchmark model metrics on complete set and test set:\")\n#whole dataset\nprint(fbeta_score( data.y, np.zeros_like(data.y), average='macro', beta=1.5))\nprint(accuracy_score( data.y, np.zeros_like(data.y)))\n#test set\nprint(fbeta_score( y_test, np.zeros_like(y_test), average='macro', beta=1.5))\nprint(accuracy_score( y_test, np.zeros_like(y_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b24d315ab3a495bac7c8bce3ff21cde597a95723","collapsed":true,"_cell_guid":"e1300dfc-c4a3-4021-b4ea-3b83f0bf7c15","trusted":false},"cell_type":"code","source":"# Missclassified as spam\nX_test[y_test < best_predictions ]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f592b70e5bb079397075542c041c812f48b2a7ac","collapsed":true,"_cell_guid":"6bbc0980-b498-486f-8952-1a4342ade058","trusted":false},"cell_type":"code","source":"# Missclassified as ham\nX_test[y_test > best_predictions]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2169fb5c5c053e82bcb6ce0825ff523b0e2d30d2","collapsed":true,"_cell_guid":"74828084-8872-475b-b6bc-9db20ace08f4","trusted":false},"cell_type":"code","source":"from sklearn.model_selection import *","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e0bba02c8f33eb85deef2f360becacd61b15129","collapsed":true,"_cell_guid":"c5034f33-90e9-471c-af77-f0b04b4cb5af","trusted":false},"cell_type":"code","source":"# Testing the robustness of the model\n\n\nkf = KFold(n_splits=5)\nkf.get_n_splits(data_x)\nprint(\"Scores of the different folds\")\nfor train_index, test_index in kf.split(data_x):\n    X_train, X_test = data_x.iloc[train_index], data_x.iloc[test_index]\n    y_train, y_test = data.y.iloc[train_index], data.y.iloc[test_index]\n    \n    \n    tfidf_train = transvector.fit_transform(X_train.sms)\n    X_train_df = tfidf_train.todense()\n    X_train[['length', 'misspells']] = scaler.transform(X_train[['length', 'misspells']])\n    X_train_final = np.concatenate((X_train_df , X_train[['length', 'misspells']].as_matrix()), axis=1)\n\n    tfidf_test = transvector.transform(X_test.sms)\n    X_test_df = tfidf_test.todense()\n    X_test[['length', 'misspells']] = scaler.transform(X_test[['length', 'misspells']])\n    X_test_final = np.concatenate((X_test_df , X_test[['length', 'misspells']].as_matrix()), axis=1)\n    \n    clf = MultinomialNB(alpha=0.5, fit_prior=False)\n    clf.fit(X_train_final,y_train)\n    \n    predictions = clf.predict(X_test_final)\n    \n    print(\"fbeta:\")\n    print(fbeta_score( y_test, predictions, average='macro', beta=1.5))\n    print(\"accuracy:\")\n    print(accuracy_score( y_test, predictions))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bea640b638da2d7633aba87ccbca88c164220384","collapsed":true,"_cell_guid":"545b2340-30f7-4d38-ab1a-b61a7213bcb3","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}