{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ti = pd.read_csv(\"../input/travel-insurance/travel insurance.csv\")\ndf_ti.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ti.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_ti = df_ti.copy()\nclean_ti.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#assign to category to save memory usage from 5+mb to 2+mb\ncategorical_col = ['Agency', 'Agency Type', 'Distribution Channel', 'Product Name', 'Destination','Gender']\n# clean_ti.drop(['Gender'], axis=1, inplace=True)\nclean_ti['Gender'] = clean_ti['Gender'].fillna(\"not_disclosed\")\nclean_ti[categorical_col] = clean_ti[categorical_col].astype('category') \nclean_ti.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#look unique value for each columns\nfor col in categorical_col:\n    uniq = len(clean_ti[col].unique())\n    print(f'{col} :{uniq} Categories')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n# encode the labels, converting them from strings to integers\nle = LabelEncoder()\nlabels = clean_ti['Claim']\nlabels = le.fit_transform(clean_ti['Claim'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_ti.describe()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"clean_ti.hist(figsize=(20,10), grid = False, layout=(3,2), bins = 10);","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"clean_ti[clean_ti[\"Duration\"] <0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_ti[clean_ti[\"Age\"] >100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_ti.loc[clean_ti['Duration'] < 0, 'Duration'] = 49.317\nclean_ti.loc[clean_ti['Age'] > 100, 'Age'] = 39.97","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_ti.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#numerical columns\ntest_ti = clean_ti.copy()\ntest_ti['Claim2'] = labels\n\nplt.title(\"Pearson Correlation for Numerical Feature\")\nsns.heatmap(test_ti.corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ti.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nsource :https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9\n        https://www.kaggle.com/ayangupta/predict-the-claim\n\"\"\"\nimport scipy.stats as ss\nimport numpy as np\n\ndef cramers_v(x, y):\n    confusion_matrix = pd.crosstab(x,y)\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n    rcorr = r-((r-1)**2)/(n-1)\n    kcorr = k-((k-1)**2)/(n-1)\n    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))\n\ncategorical=['Agency', 'Agency Type', 'Distribution Channel', 'Product Name',  'Destination','Gender','Claim']\ncramers=pd.DataFrame({i:[cramers_v(clean_ti[i],clean_ti[j]) for j in categorical] for i in categorical})\ncramers['column']=[i for i in categorical if i not in ['memberid']]\ncramers.set_index('column',inplace=True)\n\n#categorical correlation heatmap\nplt.figure(figsize=(10,7))\nplt.title(\"Cramer's V Chi-Squared\")\nsns.heatmap(cramers,annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"product_claim = pd.crosstab(clean_ti['Product Name'],clean_ti['Claim'],margins=True)\nproduct_claim.drop(index=['All'],inplace=True)\n\nplt.figure(figsize=(10, 7))\nsns.barplot(product_claim.index, product_claim.Yes.values)\nplt.xticks(rotation=90)\nplt.title(\"Claim:Yes Per Product Name\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 7))\nsns.barplot(product_claim.index, product_claim.No.values)\nplt.xticks(rotation=90)\nplt.title(\"Claim:No Per Product Name\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#target columns\nsns.countplot(clean_ti['Claim'])\nplt.title(\"Target Label Distribution\")\nplt.grid(axis='y')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reducing Target No due to severe imbalance\nrandom_no = clean_ti[clean_ti['Claim']=='No'].sample(frac=1)\nn_to_drop = len(random_no) - 10000\n\nclean_reduce = clean_ti.drop(axis=0, index=random_no.index[:n_to_drop])\n\n#target columns\nsns.countplot(clean_reduce['Claim'])\nplt.title(\"Target Label Distribution After Reducing\")\nplt.grid(axis='y')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom collections import Counter\n\n\n#split label and features\n#One Hot Encoding for categorical data\nX = clean_reduce.drop(columns=['Claim'])\nX = pd.get_dummies(X, columns=categorical_col).values\n# y = clean_ti['Claim'].replace(labels).values\ny = clean_reduce['Claim'].replace({'No':0, 'Yes':1}).values\nprint(f'Datasets Features Size {X.shape}')\n\n#X, y without Oversampling\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)\nprint('Traininng shape %s' % Counter(y_train))\nprint('Testing shape %s' % Counter(y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\n#with SMOTE\nsm = SMOTE(random_state=42)\nX_smote, y_smote = sm.fit_resample(X_train, y_train)\nprint('Resampled dataset shape %s' % Counter(y_smote))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling Machine Learning"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingClassifier \nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBClassifier\n\n\ndef model_check(models, X_train, y_train):\n    for name, model in models.items():\n        score = cross_val_score(model, X_train, y_train, cv=3, scoring='f1', n_jobs=-1)\n        print(f'{name} F1 score : {np.mean(score)}')\n\nmodels = {'random_forest':RandomForestClassifier(), \n          'logistic_reg':LogisticRegression(), \n          'XGB':XGBClassifier(), \n          'GB':GradientBoostingClassifier()}\n\nprint(\"Without SMOTE\")\nmodel_check(models, X_train, y_train)\nprint()\n\nprint(\"With SMOTE\")\nmodel_check(models, X_smote, y_smote)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Parameter Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n\nmodel = RandomForestClassifier(n_jobs=-1, verbose=1)\nparam_dist = {'n_estimators':[300, 400, 500, 600], 'max_depth':[5,6,7,8]}\n\nrandom = RandomizedSearchCV(model, param_dist, random_state=0, scoring='f1', n_jobs=-1, cv=3, verbose=1)\nsearch = random.fit(X_smote, y_smote)\nprint('BEST PARAM', search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(search.cv_results_).sort_values(by='rank_test_score')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\nmodel = RandomForestClassifier(n_estimators=400, max_depth=8)\nscore = cross_val_score(model,  X_smote, y_smote, cv=5, scoring='f1', n_jobs=-1)\n\nprint(f'Model F1 score : {np.mean(score)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_smote, y_smote)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import plot_confusion_matrix, classification_report, f1_score\n\n#using test_set\nplot_confusion_matrix(model, X_test, y_test)\nprint(classification_report(y_test, model.predict(X_test)))\nprint(f1_score(y_test, model.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import joblib\n\n# joblib.dump(model, 'model_insurance_RF.pkl') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Deep Learning Model (ANN)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing\nimport keras.backend as K","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting for validation in training process\nX_smote_train, X_val, y_smote_train, y_val = train_test_split(X_smote, y_smote, \n                                                              test_size=0.2, \n                                                              random_state=0)\n\n#Standardization is very useful for deeplearning model to learn\nscaler = StandardScaler()\nX_smote_scaled = scaler.fit_transform(X_smote_train)\nX_val_scaled = scaler.transform(X_val)\nX_test_scaled = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 1024\n\n#calling dataset\ndef data_to_tensor(X, y, batch_size, shuffle=True):\n  ds = tf.data.Dataset.from_tensor_slices((X, y))\n  if shuffle:\n    ds = ds.shuffle(buffer_size=len(X))\n  ds = ds.batch(batch_size)\n  ds = ds.prefetch(batch_size)\n  return ds\n\n#datasets per batch\ntrain_ds = data_to_tensor(X_smote_train, y_smote_train, batch_size=batch_size)\nval_ds = data_to_tensor(X_val, y_val, batch_size=batch_size)\n\n#model\ndef Model_ANN():\n    model = tf.keras.Sequential([\n            layers.Dense(128, activation='relu', input_shape=(X_smote_train.shape[1],)),\n            layers.Dense(128, activation='relu'),\n            layers.Dropout(.5),\n            layers.Dense(64, activation='relu'),\n            layers.Dropout(.3),\n            layers.Dense(32, activation='relu'),\n            layers.Dropout(.3),\n            layers.Dense(1, activation='sigmoid')])\n    \n    optim =tf.keras.optimizers.Adam(learning_rate=1e-3) \n\n    model.compile(optimizer=optim,\n                  loss=tf.keras.losses.BinaryCrossentropy(),\n                  metrics=['AUC'])\n    \n    return model\n\nmodel_ann = Model_ANN()\nmodel_ann.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def get_f1(y_true, y_pred): #taken from old keras source code\n#     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n#     possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n#     predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n#     precision = true_positives / (predicted_positives + K.epsilon())\n#     recall = true_positives / (possible_positives + K.epsilon())\n#     f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n#     return f1_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.utils.plot_model(model_ann, show_shapes=True, rankdir=\"TB\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 1000\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_auc', \n    verbose=1,\n    patience=10,\n    mode='max',\n    restore_best_weights=True)\n\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_auc', \n                                                 factor=0.1, patience=10, \n                                                 verbose=0, mode='auto',\n                                                 min_delta=0.0001)\n\nhistory = model_ann.fit(train_ds, epochs=EPOCHS, \n                  validation_data=val_ds, \n                  callbacks=[early_stopping], verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], linestyle=\"--\", label='Val Loss')\nplt.legend()\nplt.title(\"Training Loss\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['auc'], label='Training AUC')\nplt.plot(history.history['val_auc'], linestyle=\"--\", label='Val AUC')\nplt.legend()\nplt.title(\"Training AUC\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\ny_predict = model_ann.predict(X_test_scaled)>0.5\n\nsns.heatmap(confusion_matrix(y_test, y_predict), annot=True, fmt=\"d\")\nprint(classification_report(y_test, y_predict))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}