{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:purple;\">1. Make Necessary Imports</h1>","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport re\nimport string\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\nfrom sklearn.utils.multiclass import unique_labels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:purple;\">2. Understand Data</h1>\n\nLet us load and perform a quick EDA on the data to futher understand it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/hotel-review/train.csv\")\ntest_df = pd.read_csv(\"../input/hotel-review/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print five rows of the training data\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print datatype of columns\ntrain_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#display count, uniqiue count and the most frequent value in each column\ntrain_df.describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Display percentage of distribution of data between the two target classes\n\nhappy_percent = train_df['Is_Response'].value_counts()['happy']/train_df['Is_Response'].count()\nnot_happy_percent = train_df['Is_Response'].value_counts()['not happy']/train_df['Is_Response'].count()\nprint(f'Happy: {happy_percent*100}%\\nNot Happy: {not_happy_percent*100}%')\n\nsns.countplot(train_df['Is_Response'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:purple;\">3. Preprocess Data</h1>\n\nWe will be only taking into account the description column for fitting the model. Other columns such as userid, browser used and device used do not seem relevant to the task of sentiment analysis, thus we are dropping all the unnecessary columns. \n\nAlso, we will be cleaning the text by removing unncessary characters, numbers and white spaces.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.drop(columns=['User_ID', 'Browser_Used', 'Device_Used'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_clean(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = re.sub('[\"\"''_]', '', text)\n    text = re.sub('\\n', '', text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decontract_text(text):\n    \"\"\"\n    Decontract text\n    \"\"\"\n    # specific\n    text = re.sub(r\"won\\'t\", \"will not\", text)\n    text = re.sub(r\"can\\'t\", \"can not\", text)\n    text = re.sub(r\"won\\’t\", \"will not\", text)\n    text = re.sub(r\"can\\’t\", \"can not\", text)\n    text = re.sub(r\"\\'t've\", \" not have\", text)\n    text = re.sub(r\"\\'d've\", \" would have\", text)\n    text = re.sub(r\"\\'clock\", \"f the clock\", text)\n    text = re.sub(r\"\\'cause\", \" because\", text)\n\n    # general\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'t\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m\", \" am\", text)\n\n    text = re.sub(r\"n\\’t\", \" not\", text)\n    text = re.sub(r\"\\’re\", \" are\", text)\n    text = re.sub(r\"\\’s\", \" is\", text)\n    text = re.sub(r\"\\’d\", \" would\", text)\n    text = re.sub(r\"\\’ll\", \" will\", text)\n    text = re.sub(r\"\\’t\", \" not\", text)\n    text = re.sub(r\"\\’ve\", \" have\", text)\n    text = re.sub(r\"\\’m\", \" am\", text)\n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['cleaned_description'] = train_df['Description'].apply(lambda x: decontract_text(x))\ntrain_df['cleaned_description'] = train_df['cleaned_description'].apply(lambda x: text_clean(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Original Description:\\n', train_df['Description'][0])\nprint('\\n\\nCleaned Description:\\n', train_df['cleaned_description'][0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will perform an 80-20 split on the training data in order to obtain our training and testing dataset required for fitting the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x, y = train_df['cleaned_description'], train_df['Is_Response']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y,\n                                                    test_size=0.1,\n                                                    random_state=42)\n\nprint(f'x_train: {len(x_train)}')\nprint(f'x_test: {len(x_test)}')\nprint(f'y_train: {len(y_train)}')\nprint(f'y_test: {len(y_test)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:purple;\">4. Model</h1>\n\nWe will be using a [tfid vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) for extracting the features by converting the cleaned text to a matrix of TF-IDF features. For the classification, we use logistic regression. Finally, we create a model pipeline by combining the vectorizer and the classifier.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tvec = TfidfVectorizer()\nclf = LogisticRegression(solver='lbfgs', max_iter=1000)\n\nmodel = Pipeline([('vectorizer', tvec), ('classifier', clf)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:purple;\">5. Evaluation</h1>\n\nWe evaluate the model against the testing dataset. We compute the accuracy, precision and recall. Also, we plot a confusion matrix to get a better understanding about the model's performance.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(x_test)\n\nprint(f'Accurcy: {accuracy_score(y_pred, y_test)}')\nprint(f'Precision: {precision_score(y_pred, y_test, average=\"weighted\")}')\nprint(f'Recall: {recall_score(y_pred, y_test, average=\"weighted\")}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_confusion_matrix(confusion_matrix, class_names, figsize = (8,4), fontsize=12, model='clf'):\n    \"\"\"\n    Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix,\n    as a seaborn heatmap. \n    \"\"\"\n    df_cm = pd.DataFrame(\n        confusion_matrix, index=class_names, columns=class_names, \n    )\n    fig, ax = plt.subplots(1, 1, figsize=figsize)\n    heatmap = sns.heatmap(df_cm, annot=True, ax=ax, fmt=\"d\", cmap=plt.cm.Oranges)   \n    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_mat = confusion_matrix(y_test, y_pred)\nuniq_labels = unique_labels(y_test, y_pred)\n\nprint_confusion_matrix(conf_mat, uniq_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"color:red;\">WORK IN PROGRESS</h2>\n\nThis notebook is a starter notebook. I will further update this notebook using other sentiment analysis approaches such as TextBlob and VADER.\n\n---\n\n<h2 style=\"color:red;\"> If you liked it, please upvote!</h2>","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}