{"cells":[{"metadata":{"_uuid":"63fa02cc6f3a2593013a53dcd20bdd9c54533efb"},"cell_type":"markdown","source":"# 1. Introduction\n\n**Kernel objectives: **\n1. Build and train CNN classifier to recognize Pheumonia on X-Rays image. \n1. Explore CNN's internal view of the image. Push a sample through it's layers and visualize the output of each layer."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"#####################################\n# Libraries\n#####################################\n# Common libs\nimport pandas as pd\nimport numpy as np\nimport sys\nimport os\nimport os.path\nimport random\nfrom pathlib import Path\n\n# Image processing\nimport imageio\nimport skimage\nimport skimage.io\nimport skimage.transform\n#from skimage.transform import rescale, resize, downscale_local_mean\n\n# Charts\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nimport seaborn as sns\n\n\n# ML\nimport scipy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\n#from sklearn.preprocessing import OneHotEncoder\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\nfrom keras.utils import to_categorical\nimport tensorflow\n\n#####################################\n# Settings\n#####################################\nplt.style.use('ggplot')\n# Set random seed to make results reproducable\nnp.random.seed(42)\ntensorflow.set_random_seed(42)\nos.environ['PYTHONHASHSEED'] = '42'\n\n# Global variables\nimg_folder='../input/all/All/'\nimg_width=100\nimg_height=100\nimg_channels=1","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"},"cell_type":"markdown","source":"# 2. Read and explore data\n## 2.1 Read from csv"},{"metadata":{"_uuid":"5d69ff10797c793abb7c5a05594f8d3463769995","scrolled":true,"trusted":true},"cell_type":"code","source":"# Read ground truth labels\ndata = pd.read_csv('../input/all/All/GTruth.csv')\n\ndata['cat']=data['Ground_Truth'].astype('category').cat.rename_categories(['Healthy', 'Pneumonia'])\n#data['img']=data['Id'].apply(read_img)\nplt.show()\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1419a5e71da60223177d900fd0e432e46bfa67a0"},"cell_type":"markdown","source":"## 2.2 Explore distribution of samples by features."},{"metadata":{"_uuid":"66c7d150a5af4da47f98ac1962e1ad571ab657e8","scrolled":false,"trusted":true},"cell_type":"code","source":"# Show count by category in barplot\ndata['cat'].value_counts().plot(kind='bar',title='Pheumonia counts')\nplt.ylabel('Count')\nplt.xticks(rotation=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"61f753dd7f829e386db0a7069f8912d25dde8018"},"cell_type":"markdown","source":"The data is unbalanced, we have more images with pneumonia. Rebalancing is needed before training the model."},{"metadata":{"_uuid":"d750593ed655926232fabd8653ebadde62b19b31"},"cell_type":"markdown","source":"## 2.3 Look at images\nLoad several images to have a look at them."},{"metadata":{"_uuid":"cb4156649e8ebd2d06fd0b08d44ddbe6adeb6f9f","scrolled":true,"trusted":true},"cell_type":"code","source":"# Draw 4 columns per category\ncols=4\n\n# Draw samples for each category: healthy, pneumonia\nfor cat in data['cat'].cat.categories:\n    # Plot with ncols for this category\n    f, axs = plt.subplots(1,cols, figsize=(12,3))\n    cat_sample = data[data['cat']==cat]['Id'].sample(cols)\n    i=0\n    # Draw columns\n    for fid in cat_sample.values:\n        file = img_folder+str(fid)+'.jpeg'\n        im=imageio.imread(file)\n        axs[i].imshow(im)\n        # Hide grid lines came from matplotlib style\n        axs[i].grid(False)\n        i+=1\n    plt.suptitle(cat)\n    plt.tight_layout()\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79fb4788f90b3603c8b71472f44647b808bab25e"},"cell_type":"markdown","source":"# 3. Data preparation\n\n\n## 3.1 Split to train, validation and test.\n\n"},{"metadata":{"trusted":true,"_uuid":"82c3ca22ef480595b8f1fbca2ad81cd2fb6cec6e"},"cell_type":"code","source":"# Split to train_data, val_data, test_data\ntrain_data, test_data = train_test_split(data)\ntrain_data, val_data = train_test_split(train_data, test_size=0.1)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3226d731d5224e92819f2833815b42cf9d4546d"},"cell_type":"markdown","source":"## 3.2. Balance train data\nUpsample healthy items up to Pneumonia count. "},{"metadata":{"trusted":true,"_uuid":"c325c7bd7831c0710f8194c99e72794e859493bf"},"cell_type":"code","source":"# # ncat_bal items per category after balanced\nncat_bal = train_data['cat'].value_counts().max()\n# # Pandas construction to up/downresample and get ncat_bal items in each category\ntrain_data_bal = train_data.groupby('cat', as_index=False).apply(lambda g: g.sample(ncat_bal, replace=True)).reset_index(drop=True)\n\n# Plot balancing results\nf, axs = plt.subplots(1,2, figsize=(8,4))\n\n# Before\nax = train_data['cat'].value_counts().plot(kind='bar', ax=axs[0])\nax.set_title('Before balancing')\nax.set_ylabel('Count')\n\n# After\nax = train_data_bal['cat'].value_counts().plot(kind='bar', ax=axs[1])\nax.set_title('After balancing')\nax.set_ylabel('Count')\n\nplt.tight_layout()\nplt.show()\n\n# Train data is balanced data from now\ntrain_data = train_data_bal\n\n# # ncat_bal items per category after balanced\nncat_bal = val_data['cat'].value_counts().max()\n# # Pandas construction to up/downresample and get ncat_bal items in each category\nval_data = val_data.groupby('cat', as_index=False).apply(lambda g: g.sample(ncat_bal, replace=True)).reset_index(drop=True)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9650143e38ccda561450d08d241cb7ce51c89e8"},"cell_type":"markdown","source":"## 3.3 Load images\nNow we know which items are in train, validation and test. We can load images for augmentation.\n\n"},{"metadata":{"trusted":true,"_uuid":"75467b09ff2b87a346c92b03c249b653c35ab1f5"},"cell_type":"code","source":"def read_img(fileid):\n    \"\"\"\n    Read and resize img, adjust channels. \n    Caution: This function is not independent, it uses global vars: img_folder, img_channels\n    @param file: file id, int\n    \"\"\"\n    img = skimage.io.imread(img_folder + str(fileid) + '.jpeg')\n    img = skimage.transform.resize(img, (img_width, img_height), mode='reflect')\n    # A few image are grey, duplicate them for to have 3 alpha channels.\n    if(len(img.shape) < 3):\n        img = np.dstack([img, img, img])\n    return img\n                        \n# Train data\ntrain_X = np.stack(train_data['Id'].apply(read_img))\nval_X = np.stack(val_data['Id'].apply(read_img))\ntest_X = np.stack(test_data['Id'].apply(read_img))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75562696d7604f2eef1aaed4566978ea39c37d84"},"cell_type":"markdown","source":"## 3.4 Data augmentation\nImageDataGenerator used for random transformations on input image."},{"metadata":{"trusted":true,"_uuid":"ee3db431212d736a1f4658e56cb78fff1619d360"},"cell_type":"code","source":" # Data augmentation - a little bit rotate, zoom and shift input images.\ngenerator = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=180,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)\n\ngenerator.fit(train_X)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b1dc79237d325355ede9ec15ae6f778bfc3f24b"},"cell_type":"markdown","source":"## 3.5 Labels one hot encoding"},{"metadata":{"trusted":true,"_uuid":"ad462c138a529b912225b37018ae15a8f60cec80"},"cell_type":"code","source":"train_y = pd.get_dummies(train_data['Ground_Truth'], drop_first=False)\nval_y = pd.get_dummies(val_data['Ground_Truth'], drop_first=False)\ntest_y = pd.get_dummies(test_data['Ground_Truth'], drop_first=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"727bf43412f8b6eedc3673571a78c13e9c5f8fd0"},"cell_type":"markdown","source":"# 4. Prepare and train CNN model\n\n\n## 4.1 Build the model"},{"metadata":{"_uuid":"053d12032f6d113094617826f5f78bba8555c115","trusted":true},"cell_type":"code","source":"\n# Build CNN model\nmodel=Sequential()\n\n# Convolutional layers\nmodel.add(Conv2D(32, kernel_size=3, input_shape=(img_width, img_height,3), activation='relu', padding='same'))\nmodel.add(MaxPool2D(2))\nmodel.add(Conv2D(64, kernel_size=3, activation='relu', padding='same'))\nmodel.add(MaxPool2D(2))\nmodel.add(Conv2D(128, kernel_size=3, activation='relu', padding='same'))\n\nmodel.add(Flatten())\n\n#Dense layers\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(train_y.columns.size, activation='softmax'))\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n\n#Convolutional layers\n# model.add(Conv2D(64, kernel_size=3, input_shape=(img_width, img_height,3), activation='relu', padding='same'))\n# model.add(MaxPool2D(2))\n# model.add(Dropout(0.2))\n# model.add(Conv2D(128, kernel_size=3, activation='relu', padding='same'))\n# model.add(MaxPool2D(2))\n# model.add(Dropout(0.2))\n# model.add(Conv2D(256, kernel_size=3, activation='relu', padding='same'))\n# model.add(BatchNormalization())\n# model.add(Conv2D(256, kernel_size=3, activation='relu', padding='same'))\n# model.add(BatchNormalization())\n# model.add(Conv2D(256, kernel_size=3, activation='relu', padding='same'))\n# model.add(Flatten())\n\n# # Dense layers\n# model.add(Dense(500, activation='relu'))\n# model.add(Dropout(0.2))\n# model.add(Dense(200, activation='relu'))\n# model.add(Dropout(0.2))\n# model.add(Dense(train_y.columns.size, activation='softmax'))\n\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"974ce7e280699fa07f13d4b69fee1d18835e7d39"},"cell_type":"markdown","source":"## 4.2 Train the model\n"},{"metadata":{"trusted":true,"_uuid":"a39a3a1739d63cdf18f3d2f04227c8e594981730"},"cell_type":"code","source":"weights_file='best_weights.h5'\n\n# We'll stop training if no improvement after some epochs\nearlystopper = EarlyStopping(monitor='loss', patience=10, verbose=1)\n\n# Low, avg and high score training will be saved here\n# Save the best model during the traning\ncheckpointer = ModelCheckpoint(weights_file\n    ,monitor='loss'\n    ,verbose=1\n    ,save_best_only=True\n    ,save_weights_only=True)\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=5, min_lr=0.001)\n\n# Train\ntraining = model.fit_generator(generator.flow(train_X,train_y, batch_size=60)\n                                ,epochs=100\n                                ,validation_data=[val_X, val_y]\n                                ,steps_per_epoch=100\n                                ,callbacks=[earlystopper, checkpointer, reduce_lr])\n# Load best weights saved\nmodel.load_weights(weights_file)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42a271ffe8d1c2ea1ce9b305273b2483add30028"},"cell_type":"markdown","source":"## 4.3 Model evaluation\n### 4.3.1 Compare training and validation loss, accuracy"},{"metadata":{"trusted":true,"_uuid":"d657c2d471841ff5da385ba62bac635afef565a1"},"cell_type":"code","source":"## Trained model analysis and evaluation\nf, axs = plt.subplots(1,2, figsize=(10,3))\naxs=axs.flatten()\nax = axs[0]\nax.plot(training.history['loss'], label=\"Loss\")\nax.plot(training.history['val_loss'], label=\"Validation loss\")\nax.set_title('Train/validation loss')\nax.set_xlabel('Epoch')\nax.set_ylabel('Loss')\nax.legend()\nax.yaxis.set_major_locator(MaxNLocator(integer=True))\n\n# Accuracy\nax = axs[1]\nax.plot(training.history['acc'], label=\"Accuracy\")\nax.plot(training.history['val_acc'], label=\"Validation accuracy\")\nax.set_title('Train/validation accuracy')\nax.set_xlabel('Epoch')\nax.set_ylabel('Accuracy')\nax.legend()\nax.yaxis.set_major_locator(MaxNLocator(integer=True))\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b5464782f260da2bce8a5f54979c92cc4002863"},"cell_type":"markdown","source":"Validation metrics jumps up and down too much. It is a **point of improvement.**"},{"metadata":{"_uuid":"ac639c24adf884c806ce1ab985fe7db7e9aab26a"},"cell_type":"markdown","source":"### 4.3.2 Evaluate results by category\n"},{"metadata":{"_uuid":"b096e119c62a41dc227ad9f12131ca0c28dbed8b","trusted":true},"cell_type":"code","source":"# Prepare groung truth/predicted data for evaluation\npred_onehot = model.predict(test_X)\npred = np.argmax(pred_onehot, axis=1)\ngtruth = np.argmax(test_y.values, axis=1)\n\nf, axs = plt.subplots(1,2, figsize=(12,4))\n# F1 score p\nm = metrics.f1_score(gtruth, pred, average=None)\n#m = metrics.precision_score(test_truth, test_pred, average=None)\nax = sns.barplot(test_data['cat'].cat.categories,m, ax=axs[0])\nax.set_title(\"F1 score\")\n\n# sklearn.metrics.confusion_matrix result: y - true labels, x = predicted labels\ncm = metrics.confusion_matrix(gtruth, pred)\n# Normalize\ncm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nconf_matrix = pd.DataFrame(cm\n                           ,index = test_data['cat'].cat.categories\n                           ,columns = test_data['cat'].cat.categories)\n# Visualize confusion matrix\nax = sns.heatmap(conf_matrix, annot=True, ax=axs[1])\nax.set_title(\"Pneumonia confusion matrix\")\nax.set_xlabel(\"Predicted\")\nax.set_ylabel(\"Ground truth\")\nplt.show()\n\n# Print classification report\nprint(metrics.classification_report(gtruth,pred, target_names = test_data['cat'].cat.categories))\nprint('Accuracy: %s' % metrics.accuracy_score(gtruth, pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f8c5a518bdacedb3de0b24baeec02c4f77a733d9"},"cell_type":"markdown","source":"# 5. Explore CNN's subconscious (under development)\nPush random images through convolutional layers and visualize image representation inside CNN:\n\n1. Visualize pneumonia X-Rays inside CNN\n1. Visualize healthy X-Rays inside CNN\n1. Explore the difference"},{"metadata":{"_uuid":"c75a17f623e78f677dfae5ada6d87f7cfd8d1d30"},"cell_type":"markdown","source":"\n\n## 5.1 Class with Conv2D layers visualization logic."},{"metadata":{"_uuid":"0556df0097ce5c7585e8e27517ef8bd47f4d436f","trusted":true},"cell_type":"code","source":"class CnnVisualizer:\n    \"\"\"\n    Visualization. How do images look inside CNN.\n    \"\"\"\n    # Common function for visualization of kernels\n    def visualize_layer_kernels(self, img, conv_layer, title):\n        \"\"\"\n        Displays how input sample image looks after convolution by each kernel\n        :param img: Sample image array\n        :param conv_layer: Layer of Conv2D type\n        :param title: Text to display on the top \n        \"\"\"\n        # Extract kernels from given layer\n        weights1 = conv_layer.get_weights()\n        kernels = weights1[0]\n        kernels_num = min(kernels.shape[3], 5)\n\n        # Each row contains 3 images: kernel, input image, output image\n        f, ax = plt.subplots(kernels_num, 3, figsize=(7, kernels_num*2))\n        for a in ax.flatten(): a.grid(False)\n\n        for i in range(0, kernels_num):\n            # Get kernel from the layer and draw it\n            kernel=kernels[:,:,:3,i]\n            ax[i][0].imshow((kernel * 255).astype(np.uint8), vmin=0, vmax=255)\n            ax[i][0].set_title(\"Kernel %d\" % i, fontsize = 9)\n\n            # Get and draw sample image from test data\n            ax[i][1].imshow((img * 255).astype(np.uint8), vmin=0, vmax=255)\n            ax[i][1].set_title(\"Before\", fontsize=8)\n\n            # Filtered image - apply convolution\n            img_filt = scipy.ndimage.filters.convolve(img, kernel)\n            ax[i][2].imshow((img_filt * 255).astype(np.uint8), vmin=0, vmax=255)\n            ax[i][2].set_title(\"After\", fontsize=8)\n\n        plt.suptitle(title)\n        plt.tight_layout()\n        plt.subplots_adjust(top=0.93)\n        plt.show()     \n        \n    def convolve_layer(self, img, layer):\n        \"\"\"\n        Convolve images through all layer filters\n        \"\"\"\n        # Extract kernels from given layer\n        weights = layer.get_weights()\n        kernels = weights[0]\n        # Pass the image through all kernels in the layer\n        res_img = img\n        for kernel in kernels:\n            # Filtered image - apply convolution\n            kernel_img = scipy.ndimage.filters.convolve(img, kernel)  \n            res_img = res_img + kernel_img\n        return res_img / (len(kernels) + 1)\n        \n    def visualize_layer(self, img, layer, title):\n        \"\"\"\n        Displays how input sample looks after given Conv2D layer\n        @param img: img to display\n        @param layer: Conv2D layer to process img\n        @param title: text to display\n        \"\"\"\n        # Apply layer's filters to the image\n        res_img = self.convolve_layer(img, layer)\n        \n        f, axs = plt.subplots(1, 2, figsize=(7, 4))\n        for ax in axs.flatten(): ax.grid(False)\n\n        # Get and draw sample image from test data\n        axs[0].imshow((img * 255).astype(np.uint8), vmin=0, vmax=255)\n        axs[0].set_title(\"Before\", fontsize=8)\n        \n        # After\n        axs[1].imshow((res_img * 255).astype(np.uint8), vmin=0, vmax=255)\n        axs[1].set_title(\"After\", fontsize=8)\n        \n        plt.suptitle('Layer: %s' % title)\n        plt.tight_layout()\n        plt.show()  \n        \n        return(res_img)\n    \n    def visualize_layers(self, img, model):\n        \"\"\"\n        Push input image through each layer with visualization.\n        \"\"\"\n        # Get Conv2D layers from the model\n        layers = list(filter(lambda l : isinstance(l, Conv2D), model.layers))\n        \n        # Filter input image layer by layer sequentually and display the output\n        res_img = img\n        for l in layers:\n            res_img = self.visualize_layer(res_img, l, l.name)\n        return(res_img)\n            \n\n# Visualizer class instance\ncnn_vis = CnnVisualizer()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7cc6bad0c3f8da4e4868aa8a2f7d5cc8c830726c"},"cell_type":"markdown","source":"## 6.1 Visualize convolutions in Layers\nTake random images and push through convolution layers in the CNN:\n\n### 6.1.1 Pneumonia visualization\nTake random image contains Pheumonia diagnosis and push it through the model\n"},{"metadata":{"_uuid":"ca71ec8bff6ef29197449d18f24bc08dd1112e30","trusted":true},"cell_type":"code","source":"# Get random image with Pneumonia\npneumonia_data = test_data[test_data['cat'] == 'Pneumonia']\nidx = random.randint(0,len(pneumonia_data)-1)\n#idx = random.randint(0,len(test_X)-1)\n# Pneumonia input image\np_in_img = test_X[idx,:,:,:]\n\n# Visualize\np_out_img = cnn_vis.visualize_layers(p_in_img, model)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a0713fbf626f460d07ec6cc406956d1d661f3510"},"cell_type":"markdown","source":"### 6.1.2 Healthy sample visualization\nTake random image with normal health status, push it through the model and compare with Pheumonia above"},{"metadata":{"trusted":true,"_uuid":"3c170d6b655fa4c778d5ea079f24cde6520710a0"},"cell_type":"code","source":"# Get random image with Pneumonia\npneumonia_data = test_data[test_data['cat'] == 'Healthy']\nidx = random.randint(0,len(pneumonia_data)-1)\n#idx = random.randint(0,len(test_X)-1)\n# Healthy input image\nh_in_img = test_X[idx,:,:,:]\n\n# Visualize\nh_out_img = cnn_vis.visualize_layers(h_in_img, model)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b07b345fc062182d7cd04d03c8709a0f6ffbcfe4"},"cell_type":"markdown","source":"## 6.2 Analysis of what we have now.\n\nAfter second Conv2D layer the images translated to something not readable by human.\nI am also not able to see the difference between Pheumonia and Health images in CNN internal representation. But model's prediction accuracy is about 0.9, so this output should contain something valued.\n\nLet's analyze distribution:\n"},{"metadata":{"trusted":true,"_uuid":"2ccbf040f7e64a96632876bdc972cfc8aedbe134"},"cell_type":"code","source":"f, axs = plt.subplots(1,2, figsize=(12,4))\n\n# Input distribution\nsns.distplot(p_in_img.flatten(), label=\"Pneumonia\", ax=axs[0])\nsns.distplot(h_in_img.flatten(), label=\"Healthy\", ax=axs[0])\naxs[0].legend()\naxs[0].set_title('Input')\n\n# Otput distribution\nsns.distplot(p_out_img.flatten(), label=\"Pneumonia\", ax=axs[1])\nsns.distplot(h_out_img.flatten(), label=\"Healthy\", ax=axs[1])\naxs[1].legend()\naxs[1].set_title(\"Output\")\nplt.suptitle(\"Distribution\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e7a01473ca64985c962c751b16ce66668b55b99"},"cell_type":"markdown","source":"Not sure the output should contain such numbers. \n\n**ToDo:** check my code for bugs\n\n**Will be continued...**"},{"metadata":{"_uuid":"7c944484e8be57d3a0b7fe4db9b21099dd888ebe"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}