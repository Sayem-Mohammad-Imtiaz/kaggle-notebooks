{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra - data manipulation\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom collections import OrderedDict\n\npd.options.display.max_columns = 50\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing Datasets","metadata":{}},{"cell_type":"code","source":"airlines = pd.read_csv(\"../input/flight-delays/airlines.csv\")\nairports = pd.read_csv(\"../input/flight-delays/airports.csv\")\nflights = pd.read_csv(\"../input/flight-delays/flights.csv\")","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"flights.info()","metadata":{"_cell_guid":"","_uuid":"","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"flights.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DATA CLEANING","metadata":{}},{"cell_type":"markdown","source":"**Reduce down the dataset from 5819079 to 469969 rows by only working on the month January**","metadata":{}},{"cell_type":"code","source":"flights = flights[flights['MONTH'] == 1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"flights.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime,warnings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"flights['DATE'] = pd.to_datetime(flights[['YEAR','MONTH', 'DAY']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Merge the take-off hour with the flight date**","metadata":{}},{"cell_type":"code","source":"def format_hour(chain):\n    if pd.isnull(chain):\n        return np.nan\n    else:\n        if chain == 2400: chain = 0\n        chain = \"{0:04d}\".format(int(chain))\n        hour = datetime.time(int(chain[0:2]), int(chain[2:4]))\n        return hour","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Combine two columns of the dataframe to create a datetime format","metadata":{}},{"cell_type":"code","source":"def combine_date_hour(x):\n    if pd.isnull(x[0]) or pd.isnull(x[1]):\n        return np.nan\n    else:\n        return datetime.datetime.combine(x[0],x[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_flight_time(df, col):    \n    list = []\n    for index, cols in df[['DATE', col]].iterrows():    \n        if pd.isnull(cols[1]):\n            list.append(np.nan)\n        elif float(cols[1]) == 2400:\n            cols[0] += datetime.timedelta(days=1)\n            cols[1] = datetime.time(0,0)\n            list.append(combine_date_hour(cols))\n        else:\n            cols[1] = format_hour(cols[1])\n            list.append(combine_date_hour(cols))\n    return pd.Series(list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"flights['SCHEDULED_DEPARTURE'] = create_flight_time(flights, 'SCHEDULED_DEPARTURE')\nflights['DEPARTURE_TIME'] = flights['DEPARTURE_TIME'].apply(format_hour)\nflights['SCHEDULED_ARRIVAL'] = flights['SCHEDULED_ARRIVAL'].apply(format_hour)\nflights['ARRIVAL_TIME'] = flights['ARRIVAL_TIME'].apply(format_hour)\n#__________________________________________________________________________\nflights.loc[:5, ['SCHEDULED_DEPARTURE', 'SCHEDULED_ARRIVAL', 'DEPARTURE_TIME',\n             'ARRIVAL_TIME', 'DEPARTURE_DELAY', 'ARRIVAL_DELAY']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"variables_to_remove = ['TAXI_OUT', 'TAXI_IN', 'WHEELS_ON', 'WHEELS_OFF', 'YEAR', \n                       'MONTH','DAY','DAY_OF_WEEK','DATE', 'AIR_SYSTEM_DELAY',\n                       'SECURITY_DELAY', 'AIRLINE_DELAY', 'LATE_AIRCRAFT_DELAY',\n                       'WEATHER_DELAY', 'DIVERTED', 'CANCELLED', 'CANCELLATION_REASON',\n                       'FLIGHT_NUMBER', 'TAIL_NUMBER', 'AIR_TIME']\nflights.drop(variables_to_remove, axis = 1, inplace = True)\nflights = flights[['AIRLINE', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT',\n        'SCHEDULED_DEPARTURE', 'DEPARTURE_TIME', 'DEPARTURE_DELAY',\n        'SCHEDULED_ARRIVAL', 'ARRIVAL_TIME', 'ARRIVAL_DELAY',\n        'SCHEDULED_TIME', 'ELAPSED_TIME']]\nflights[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_flights = flights.isnull().sum(axis=0).reset_index()\nmissing_flights.columns = ['variable', 'missing values']\nmissing_flights['filling factor (%)']=(flights.shape[0]-missing_flights['missing values'])/flights.shape[0]*100\nmissing_flights.sort_values('filling factor (%)').reset_index(drop = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"flights.dropna(inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"flights.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"flights.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"No. of airports: {}\".format(len(flights['ORIGIN_AIRPORT'].unique())))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"airports","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"airlines","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"abbr_companies = airlines.set_index('IATA_CODE')['AIRLINE'].to_dict()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_stats(group):\n    return {'min': group.min(), 'max': group.max(),\n             'mean': group.mean()}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"global_stats = flights['DEPARTURE_DELAY'].groupby(flights['AIRLINE']).apply(get_stats).unstack()\nglobal_stats = global_stats.sort_values('mean')\nglobal_stats\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\norigin_nb = dict()\nfor carrier in abbr_companies.keys():\n    list_origin_airport = flights[flights['AIRLINE'] == carrier]['ORIGIN_AIRPORT'].unique()\n    origin_nb[carrier] = len(list_origin_airport)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt \ntest_df = pd.DataFrame.from_dict(origin_nb, orient='index')\ntest_df.rename(columns = {0:'count'}, inplace = True)\nax = test_df.plot(kind='bar', figsize = (8,3))\nlabels = [abbr_companies[item.get_text()] for item in ax.get_xticklabels()]\nax.set_xticklabels(labels)\nplt.ylabel('Number of destiantion airports', fontsize=14, weight = 'bold', labelpad=12)\nplt.setp(ax.get_xticklabels(), fontsize=11, ha = 'right', rotation = 80)\nax.legend().set_visible(False)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"identify_airport = airports.set_index('IATA_CODE')['CITY'].to_dict()\nlatitude_airport = airports.set_index('IATA_CODE')['LATITUDE'].to_dict()\nlongitude_airport = airports.set_index('IATA_CODE')['LONGITUDE'].to_dict()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"MEAN DELAY PER ARIPORT","metadata":{}},{"cell_type":"code","source":"airport_mean_delays = pd.DataFrame(pd.Series(flights['ORIGIN_AIRPORT'].unique()))\nairport_mean_delays.set_index(0, drop = True, inplace = True)\n\nfor carrier in abbr_companies.keys():\n    flights1 = flights[flights['AIRLINE'] == carrier]\n    test = flights1['DEPARTURE_DELAY'].groupby(flights['ORIGIN_AIRPORT']).apply(get_stats).unstack()\n    airport_mean_delays[carrier] = test.loc[:, 'mean']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nfig = plt.figure(1, figsize=(8,8))\n\nax = fig.add_subplot(1,2,1)\nsubset = airport_mean_delays.iloc[:50,:].rename(columns = abbr_companies)\nsubset = subset.rename(index = identify_airport)\nmask = subset.isnull()\nsns.heatmap(subset, linewidths=0.01, cmap=\"Accent\", mask=mask, vmin = 0, vmax = 35)\nplt.setp(ax.get_xticklabels(), fontsize=10, rotation = 85) ;\nax.yaxis.label.set_visible(False)\n\nax = fig.add_subplot(1,2,2)    \nsubset = airport_mean_delays.iloc[50:100,:].rename(columns = abbr_companies)\nsubset = subset.rename(index = identify_airport)\nfig.text(0.5, 1.02, \"Delays: impact of the origin airport\", ha='center', fontsize = 18)\nmask = subset.isnull()\nsns.heatmap(subset, linewidths=0.01, cmap=\"Accent\", mask=mask, vmin = 0, vmax = 35)\nplt.setp(ax.get_xticklabels(), fontsize=10, rotation = 85) ;\nax.yaxis.label.set_visible(False)\n\nplt.tight_layout()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can deduce from these observations that there is a high variability in average delays, both between the different airports but also between the different airlines. This is important because it implies that in order to accurately model the delays, it will be necessary to adopt a model that is specific to the company and the home airport .","metadata":{}},{"cell_type":"markdown","source":"We will find the average delay per airport per airline","metadata":{}},{"cell_type":"code","source":"#subset of the dataframe\ncarrier = 'AA'\nflights1 = flights[flights['AIRLINE']==carrier][['ORIGIN_AIRPORT','DESTINATION_AIRPORT','DEPARTURE_DELAY']]\n\n#  routes and list the delays for each of them\ntrajet = dict()\nfor ind, col in flights1.iterrows():\n    if pd.isnull(col['DEPARTURE_DELAY']): continue\n    route = str(col['ORIGIN_AIRPORT'])+'-'+str(col['DESTINATION_AIRPORT'])\n    if route in trajet.keys():\n        trajet[route].append(col['DEPARTURE_DELAY'])\n    else:\n        trajet[route] = [col['DEPARTURE_DELAY']]\n        \n# transpose the dictionary in a list to sort the routes by origins        \nlist_trajet = []\nfor key, value in trajet.items():\n    list_trajet.append([key, value])\nlist_trajet.sort()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"average delay on the various paths A → B, as well as the standard deviation","metadata":{}},{"cell_type":"code","source":"import scipy\nfrom scipy.optimize import curve_fit\nmean_val = [] ; std_val = [] ; x_label = []\n\ni = 0\nfor route, list_retards in list_trajet:\n    #_____________________________________________\n    # set the labels as the airport from origin\n    index = route.split('-')[0]\n    x_label.append(identify_airport[index])\n    #______________________________________________________________________________\n    # put a threshold on delays to prevent that high values take too much weight\n    trajet2 = [min(90, s) for s in list_retards]\n    #________________________________________\n    # compute mean and standard deviations\n    mean_val.append(scipy.mean(trajet2))\n    std_val.append(scipy.std(trajet2))\n    i += 1\n#________________\n# Plot the graph\nfig, ax = plt.subplots(figsize=(10,4))\nstd_min = [ min(15 + mean_val[i], s) for i,s in enumerate(std_val)] \nax.errorbar(list(range(i)), mean_val, yerr = [std_min, std_val], fmt='o') \nax.set_title('Mean route delays for \"{}\"'.format(abbr_companies[carrier]),\n             fontsize=14, weight = 'bold')\nplt.ylabel('Mean delay at origin (minutes)', fontsize=14, weight = 'bold', labelpad=12)\n#___________________________________________________\n# define the x,y range and positions of the ticks\nimin, imax = 145, 230\nplt.xlim(imin, imax) ; plt.ylim(-20, 45)\nlist_ticks = [imin]\nfor j in range(imin+1,imax):\n    if x_label[j] == x_label[j-1]: continue\n    list_ticks.append(j)\n#_____________________________\n# and set the tick parameters  \nax.set_xticks(list_ticks)\nax.set_xticklabels([x_label[int(x)] for x in ax.get_xticks()], rotation = 90, fontsize = 8)\nplt.setp(ax.get_yticklabels(), fontsize=12, rotation = 0)\nax.tick_params(axis='y', which='major', pad=15)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The error bars associated with the different paths correspond to the standard deviations. In this example, it can be seen that for a given airport of origin, delays will fluctuate depending on the destination. We see, for example, that here the greatest variations are obtained for New York or Miami where the initial average delays vary between 0 and ∼ 20 minutes.","metadata":{}},{"cell_type":"markdown","source":"# DATA MODELLING","metadata":{}},{"cell_type":"code","source":"df_train = flights[flights['SCHEDULED_DEPARTURE'].apply(lambda x:x.date()) < datetime.date(2015, 1, 23)]\ndf_test  = flights[flights['SCHEDULED_DEPARTURE'].apply(lambda x:x.date()) > datetime.date(2015, 1, 23)]\nflights = df_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Delay for one airport for one airline","metadata":{}},{"cell_type":"code","source":"def get_flight_delays(df, carrier, id_airport, extrem_values = False):\n    df2 = flights[(flights['AIRLINE'] == carrier) & (flights['ORIGIN_AIRPORT'] == id_airport)]\n    \n    # remove extreme values before fitting\n    if extrem_values:\n        df2['DEPARTURE_DELAY'] = df2['DEPARTURE_DELAY'].apply(lambda x:x if x < 60 else np.nan)\n        df2.dropna(how = 'any')\n    \n    # Conversion: date + hour -> hour\n    df2.sort_values('SCHEDULED_DEPARTURE', inplace = True)\n    df2['hour_depart'] =  df2['SCHEDULED_DEPARTURE'].apply(lambda x:x.time())\n    \n    # grouping of flights by departure time and averaging\n    test2 = df2['DEPARTURE_DELAY'].groupby(df2['hour_depart']).apply(get_stats).unstack()\n    test2.reset_index(inplace=True)\n    \n    \n    # hour to second conversion\n    fct = lambda x:x.hour*3600+x.minute*60+x.second\n    test2.reset_index(inplace=True)\n    test2['hour_depart_min'] = test2['hour_depart'].apply(fct)\n    return test2\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Delay for one airline multiple airports","metadata":{}},{"cell_type":"code","source":"def get_merged_delays(flights, carrier):\n    list_airports = flights[flights['AIRLINE'] == carrier]['ORIGIN_AIRPORT'].unique()\n    i = 0\n    list_columns = ['AIRPORT_ID', 'hour_depart_min', 'mean']\n    for id_airport in list_airports:\n        test2 = get_flight_delays(flights, carrier, id_airport, True)\n        test2.loc[:, 'AIRPORT_ID'] = id_airport\n        test2 = test2[list_columns]\n        test2.dropna(how = 'any', inplace = True)\n        if i == 0:\n            merged_df = test2.copy()\n        else:\n            merged_df = pd.concat([merged_df, test2], ignore_index = True)\n        i += 1    \n    return merged_df\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"last_expr\"\ncarrier = 'AA'\nmerged_df = get_merged_delays(flights, carrier)\nmerged_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabel_encoder = LabelEncoder()\ninteger_encoded = label_encoder.fit_transform(merged_df['AIRPORT_ID'])\n\n# correspondance between the codes and tags of the airports\nzipped = zip(integer_encoded, merged_df['AIRPORT_ID'])\nlabel_airports = list(set(list(zipped)))\nlabel_airports.sort(key = lambda x:x[0])\nlabel_airports[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"onehot_encoder = OneHotEncoder(sparse=False)\ninteger_encoded = integer_encoded.reshape(len(integer_encoded), 1)\nonehot_encoded = onehot_encoder.fit_transform(integer_encoded)\nb = np.array(merged_df['hour_depart_min'])\nb = b.reshape(len(b),1)\nX = np.hstack((onehot_encoded, b))\nY = np.array(merged_df['mean'])\nY = Y.reshape(len(Y), 1)\nprint(X.shape, Y.shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LINEAR REGRESSION","metadata":{}},{"cell_type":"markdown","source":"The matrices X and Y thus created can be used to perform a linear regression:","metadata":{}},{"cell_type":"code","source":"from sklearn import metrics, linear_model #modelling\nlm = linear_model.LinearRegression()\nmodel = lm.fit(X,Y)\npredictions = pd.DataFrame(lm.predict(X))\nprint(predictions.head(50))\nprint(\"MSE =\", metrics.mean_squared_error(predictions, Y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"number of predictions where the differences with real values is greater than 15 minutes","metadata":{}},{"cell_type":"code","source":"icount = 0\nfor i, val in enumerate(Y):\n    if abs(val-predictions[i]) > 15: icount += 1\n'{:.2f}%'.format(icount / len(predictions) * 100)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# POLYNOMIAL REGRESSION","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures, StandardScaler\npoly = PolynomialFeatures(degree = 2)\nregr = linear_model.LinearRegression()\nX_ = poly.fit_transform(X)\nregr.fit(X_, Y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = pd.DataFrame(regr.predict(X_))\nprint(result.head(50))\nprint(\"MSE =\", metrics.mean_squared_error(result, Y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that a polynomial fit improves slightly the MSE score. In practice, the percentage of values where the difference between predictions and real delays is greater than 15 minutes is:","metadata":{}},{"cell_type":"code","source":"\n\nicount = 0\nfor i, val in enumerate(Y):\n    if abs(val-result[i]) > 15: icount += 1\n'{:.2f}%'.format(icount / len(result) * 100)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\nTwo models were fit and tested on the training set.\n\nSETTING THE FREE PARAMETERS\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nX_train.shape\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"poly = PolynomialFeatures(degree = 2)\nregr = linear_model.LinearRegression()\nX_ = poly.fit_transform(X_train)\nregr.fit(X_, Y_train)\nresult = pd.DataFrame(regr.predict(X_))\nprint(result.head(50))\nscore = metrics.mean_squared_error(result, Y_train)\nprint(\"Mean squared error = \", score)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"TESTING THE TEST SET","metadata":{}},{"cell_type":"code","source":"X_ = poly.fit_transform(X_test)\nresult = pd.DataFrame(regr.predict(X_))\nprint(result.head(50))\nscore = metrics.mean_squared_error(result, Y_test)\nprint(\"Mean squared error = \", score)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we see that the fit is particularly bad with a MSE > 500, which means that the fit performs poorly when generalyzing to other data. Now let's examine in detail the reasons why we have such a bad score. Below, I examing all the terms of the MSE calculation and identify the largest terms:","metadata":{}},{"cell_type":"code","source":"sum = 0\nfor values in zip(result, Y_test):\n    add = (float(values[0]) - float(values[1]))**2\n    sum += add\n    if add > 10**4:\n        print(\"{:<.1f} {:<.1f} {:<.1f}\".format(add, float(values[0]), float(values[1])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that some predictions show very large errors. In practice, this can be explained by the fact that during the separation in train and test sets, data with no equivalent in the training set was put in the test data.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Ridge\nridgereg = Ridge(alpha=0.3,normalize=True)\npoly = PolynomialFeatures(degree = 2)\nX_ = poly.fit_transform(X_train)\nridgereg.fit(X_, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_ = poly.fit_transform(X_test)\nresult = pd.DataFrame(ridgereg.predict(X_))\nprint(result.head(50))\nscore = metrics.mean_squared_error(result, Y_test)\nprint(\"Mean squared error = \", score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_min = 10000\nfor pol_order in range(1, 3):\n    for alpha in range(0, 20, 2):\n        ridgereg = Ridge(alpha = alpha/10, normalize=True)\n        poly = PolynomialFeatures(degree = pol_order)\n        regr = linear_model.LinearRegression()\n        X_ = poly.fit_transform(X_train)\n        ridgereg.fit(X_, Y_train)        \n        X_ = poly.fit_transform(X_test)\n        result = ridgereg.predict(X_)\n        score = metrics.mean_squared_error(result, Y_test)        \n        if score < score_min:\n            score_min = score\n            parameters = [alpha/10, pol_order]\n        print(\"n={} alpha={} , MSE = {:<0.5}\".format(pol_order, alpha, score))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ridgereg = Ridge(alpha = parameters[0], normalize=True)\npoly = PolynomialFeatures(degree = parameters[1])\nX_ = poly.fit_transform(X)\nridgereg.fit(X_, Y)\nresult = ridgereg.predict(X_)\nscore = metrics.mean_squared_error(result, Y)        \nprint(score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TESTING","metadata":{}},{"cell_type":"code","source":"carrier = 'AA'\nmerged_df_test = get_merged_delays(df_test, carrier)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_conversion = dict()\nfor s in label_airports:\n    label_conversion[s[1]] = s[0]\n\nmerged_df_test['AIRPORT_ID'].replace(label_conversion, inplace = True)\n\nfor index, label in label_airports:\n    temp = merged_df_test['AIRPORT_ID'] == index\n    temp = temp.apply(lambda x:1.0 if x else 0.0)\n    if index == 0:\n        matrix = np.array(temp)\n    else:\n        matrix = np.vstack((matrix, temp))\nmatrix = matrix.T\n\nb = np.array(merged_df_test['hour_depart_min'])\nb = b.reshape(len(b),1)\nX_test = np.hstack((matrix, b))\nY_test = np.array(merged_df_test['mean'])\nY_test = Y_test.reshape(len(Y_test), 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_ = poly.fit_transform(X_test)\nresult = ridgereg.predict(X_)\nscore = metrics.mean_squared_error(result, Y_test)\n'MSE = {:.2f}'.format(score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\nThis MSE score is equivalent to an average delay of:\n","metadata":{}},{"cell_type":"code","source":"'Ecart = {:.2f} min'.format(np.sqrt(score))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, the average difference in our predictions and actual delay is about 7.23 min","metadata":{}},{"cell_type":"markdown","source":"We trained and tested the model nearly 4 times and successfully reduced the MSE Score from 62.98 to 52.27.Given the time, we reduced the data but if we utilize more training data, the MSE Score can further be minimized.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}