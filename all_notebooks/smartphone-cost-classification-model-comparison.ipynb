{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Aided by the article at https://www.datatechnotes.com/2020/07/classification-example-with-linearsvm-in-python.html\n\n# TODO:\n# Import required libraries/packages\n# Get the training and testing data \n#   Get Dataset paths\n#   Read data into a dataframe\n#   Find target\n#   Remove rows with missing targets\n#   Separate predictors from target\n# Preprocess the data\n#   Check for categorical data -> drop or OneHotEncode/Label them\n#   Check for missing numerical values -> drop or Impute them\n# Set apart validation set from training data\n# Inspect data\n# Train model\n# Test model/make predictions\n\n# Compare with other models","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_file_path = '../input/mobile-price-classification/train.csv'\ntest_file_path = '../input/mobile-price-classification/test.csv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_full = pd.read_csv(train_file_path)\nX_test_full = pd.read_csv(test_file_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_full.columns)\n\n# price_range is the target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_full.dropna(axis=0, subset=['price_range'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = X_full['price_range']\nX_full.drop(['price_range'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X_full\nX_test = X_test_full","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X, y,\n                                                     train_size=0.8,\n                                                     test_size=0.2,\n                                                     random_state=0,\n                                                     stratify=y)\n# note stratify=y is used for when your data's classes are imbalanced","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X.describe())\nprint('-----------------------------------------------------------------')\nprint(X.head())\nprint('-----------------------------------------------------------------')\nprint(X.shape)\nprint('-----------------------------------------------------------------')\ny.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# All parameters are default except dual was set to False to avoid a converging error.\n# A possible alternative could be to scale or normalize the training data.\nlsvc = LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                 intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n                 multi_class='ovr', random_state=None, tol=0.0001,\n                 verbose=0)\n\n# Train the model and see its score.\nlsvc.fit(X_train, y_train)\nscore = lsvc.score(X_train, y_train)\nprint(f'Score: {score}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply cross validation training and see the score\ncv_scores = cross_val_score(lsvc, X_train, y_train, cv=10)\nprint(f'Average Cross Validation Score: {cv_scores.mean()}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 0 is low cost\n# 1 is medium cost\n# 2 is high cost\n# 3 is very high cost\n\n# Categorize an average spec smart phone cost:\n# battery (mAh) = 1239\n# has bluetooth or not (1 or 0) = 0\n# microprocessor clock speed (GHz) = 1.5\n# has dual sim support or not (1 or 0) = 1\n# front camera Mpx = 4\n# has 4G or not (1 or 0) = 1\n# internal memory (Gbytes) = 32\n# mobile depth in cm = 0.5\n# weight of mobile phone (grams) = 140\n# number of cores = 5\n# primary camera Mpx = 10\n# Pixel Resolution Height = 645\n# Pixel Resolution Width  = 1251\n# RAM (Mbytes) = 2124\n# Screen Height of mobile in cm = 12.3\n# Screen Width of mobile in cm = 5.8\n# longest time that a single battery charge will last when you are talking (imputed) = 11\n# Has 3G or not (1 or 0) = 1\n# Has touch screen or not (1 or 0) = 1\n# Has wifi or no (1 or 0) = 1\n\n\n# we'll need to reshape this array since it is only one sample\naverage_sample = np.array([1239, 0, 1.5, 1, 4, 1, 32, 0.5, 140, 5, 10, 645, 1251, 2124, 12.3, 5.8, 11, 1, 1, 1])\n\ncost_category = lsvc.predict(average_sample.reshape(1, -1))\nprint(f'Cost Category of a smart phone with average specs: {cost_category}')\n\n# Conclusion\n# I think this dataset is simply outdated. Even if you were to use sample specs in the\n# prediction from 2017 (the year of the last dataset update), the model will still\n# be a little inaccurate because the dataset is not representative. For instance,\n# the max battery life of smart phones was 1999 which is well below the battery\n# life of very costly phones at that time. But then again, nowhere does it explain when\n# the data was recorded","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Aided by the article at https://medium.com/analytics-vidhya/evaluating-a-random-forest-model-9d165595ad56\n# Lets try a RandomForestClassifier model\nrfc = RandomForestClassifier(random_state=0)\nrfc.fit(X_train, y_train)\ny_pred = rfc.predict(X_valid)\nscore = accuracy_score(y_valid, y_pred)\nprint(f'Score: {score}')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# accuracy is not a great measure of classifier performance when the classes are imbalanced\n# View confusion matrix for test data and predictions\n# the diagnal is the number of correctly predicted instances of a class (col 1 is class 0, col 2 -> class 1, etc)\nconfusion_matrix(y_valid, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using the confusion matrix and the classification report gives us a good understanding of how\n# well the model is predicting the instances correctly and what classes it is struggling to identify\nprint(classification_report(y_valid, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets compare the above models with Extreme Gradient Boosting\nfrom xgboost import XGBClassifier\n\nxgb = XGBClassifier(n_estimators=1000, learning_rate=0.05, n_jobs=4, random_state=0)\nxgb.fit(X_train, y_train,\n       early_stopping_rounds=5,\n       eval_set=[(X_valid, y_valid)],\n       verbose=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_preds = xgb.predict(X_valid)\nscore = accuracy_score(y_valid, y_preds)\nprint(f'Model Accuracy Score: {score}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_valid, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_valid, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now lets build a vanilla Neural Network in PyTorch\n# Aided by the guides:\n#   https://curiousily.com/posts/build-your-first-neural-network-with-pytorch/\n#   https://www.youtube.com/watch?v=Jy4wM2X21u0&t=185s\n\n# Objectives:\n#   Preprocess CSV files and convert the data to Tensors\n#   Build a Neural Network model with PyTorch\n#   Use a loss function and an optimizer to train the model\n#   Evaluate the model\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim                 # optimization algorithms like gradient descent\nimport torch.nn.functional as F             # for activation funtions\nimport torchvision.transforms as transforms # transformations to perform on dataset\n\nfrom sklearn.preprocessing import StandardScaler\n\nseed = 0\nnp.random.seed(seed)\ntorch.manual_seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create fully connected Neural Network\nclass NN(nn.Module):\n    def __init__(self, n_features, num_classes):\n        super(NN, self).__init__()\n        # two hidden layers 25 to 15 nodes\n        self.fc1 = nn.Linear(n_features, 25) \n        self.fc2 = nn.Linear(25, 30)            \n        self.fc3 = nn.Linear(30, num_classes)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = torch.sigmoid(self.fc3(x))\n        return x\n    \n# test it out\n\n# model = NN(20, 4)\n# x = torch.randn(100, 20)\n# print(model(x).shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the data\ntrain_file_path = '../input/mobile-price-classification/train.csv'\ntest_file_path = '../input/mobile-price-classification/test.csv'\n\nX_full = pd.read_csv(train_file_path)\nX_full.dropna(axis=0, subset=['price_range'], inplace=True)\n\ny = X_full['price_range']\nX_full.drop(['price_range'], axis=1, inplace=True)\nX = X_full\n\n\n# note stratify=y is used for when your data's classes are imbalanced\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                     train_size=0.8,\n                                                     test_size=0.2,\n                                                     random_state=seed,\n                                                     stratify=y)\n\n\n# Normalize data here after train_test_split\nscaler = StandardScaler()\nX_train = pd.DataFrame(scaler.fit_transform(X_train))\nX_test = pd.DataFrame(scaler.transform(X_test))\n\n\n\n\n# Convert data to tensors\nX_train = torch.from_numpy(X_train.to_numpy()).float()\ny_train = torch.squeeze(torch.from_numpy(y_train.to_numpy()).long())\n\nX_test = torch.from_numpy(X_test.to_numpy()).float()\ny_test = torch.squeeze(torch.from_numpy(y_test.to_numpy()).long())\n\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyper-parameters\nn_features = X_train.shape[1]\nnum_classes = 4\nlearning_rate = 0.009\nnum_epochs = 1200","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize network\nmodel = NN(n_features=n_features, num_classes=num_classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set device (CPU or GPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nX_train = X_train.to(device)\ny_train = y_train.to(device)\n\nX_test = X_test.to(device)\ny_test = y_test.to(device)\n\nmodel = model.to(device)\n\ncriterion = criterion.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the model\nfor epoch in range(num_epochs):\n    \n    # forward\n    y_pred = model(X_train)\n    train_loss = criterion(y_pred, y_train)\n    \n    # backward\n    optimizer.zero_grad()\n    train_loss.backward()\n    \n    # gradient descent or adam step\n    optimizer.step()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check accuracy on training and test\ndef check_accuracy(X_train, y_train, model):\n    num_correct = 0\n    num_samples = 0\n    model.eval()\n    \n    with torch.no_grad():\n        scores = model(X_train)\n        _, predictions = scores.max(1)\n        num_correct += (predictions == y_train).sum()\n        num_samples += predictions.size(0)\n        print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n        \n    model.train()\n\n\ncheck_accuracy(X_train, y_train, model)\ncheck_accuracy(X_test, y_test, model)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}