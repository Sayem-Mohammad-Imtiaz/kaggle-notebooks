{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":false},"cell_type":"code","source":"# These two bad boys will help us plot our data and understand it a bit graphically:\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Importing some tools to preprocess the data:\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# Lets import some algorithms now:\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, r2_score\n\n# Importing tools form Keras library in order to build Neural Network:\nfrom keras.losses import binary_crossentropy\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.metrics import Accuracy\nfrom keras.optimizers import RMSprop, SGD, Adam\nfrom keras.optimizers.schedules import ExponentialDecay \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reading the data:\n","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"data = pd.read_csv(\"../input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv\", index_col='customerID')\ndata.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets check the data for any missing values because the plotting tools such as seaborn and matplotlib don't exactly like it when you have even a single one(i.e it gives an error).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No missing values! :D","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's visualize it a little bit now, checking for the proportion of males and females:","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"plt.figure(figsize=(7,7))\nsns.set_context(\"poster\", font_scale=0.7)\nsns.set_palette(['pink', 'skyblue'])\nsns.countplot(data['gender'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How many senior citizens are there though compared to the non senior citizens?","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nsns.set_context(\"poster\", font_scale=0.7)\nsns.set_palette(['k', 'darkgrey'])\nsns.countplot(data['SeniorCitizen'])\nplt.xticks([0,1], ['Not a SeniorCitizen', 'SeniorCitizen'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Way more non-senior citizens than senior citizens here.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Here's the little code to make a list of all categorical columns:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"c = (data.dtypes == 'object')\ncatcol = list(c[c].index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since I am trying to find the correlation between the features in this dataset I will have to convert the categorical values into numerical values using the LabelEncoder.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"encdata = data.copy()\nenc = LabelEncoder()\ncolumns = data.columns\nfor col in catcol:\n    encdata[col] = enc.fit_transform(encdata[col])\n    \nencdata = pd.DataFrame(encdata, columns=columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally plotting the HeatMap:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(9,9))\nsns.heatmap(encdata.corr(), cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(7,7))\nsns.set_context(\"poster\", font_scale=0.7)\nsns.set_palette(['pink', 'skyblue'])\nsns.scatterplot(data=data, x='TotalCharges', y='tenure', hue='Churn')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(7,7))\nsns.set_context(\"poster\", font_scale=0.7)\nsns.set_palette(['pink', 'skyblue'])\nsns.scatterplot(data=data, x='MonthlyCharges', y='tenure', hue='Churn')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see from the data above that there is little correlation with the elements and with the naked eye, we wouldn't be able to conclude anything in my perspective. (feel free to correct me)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Lets redo the whole preprocessing once again for my satisfaction:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets use the Churn as a label and predict on it:\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data['Churn']\n\nenc = LabelEncoder()\ny = enc.fit_transform(y)\n\ndata.drop(['Churn', 'customerID'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Defining the new list of categorical features since we dropped a couple of features:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"c = (data.dtypes == 'object')\ncatcol = list(c[c].index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Label encoding the data:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in catcol:\n    data[col] = enc.fit_transform(data[col])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting the data into training and test sets now:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain, xtest, ytrain, ytest = train_test_split(data, y, train_size=0.95, test_size=0.05)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets predict on the data using Decision Tree Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"DecModel = DecisionTreeClassifier()\n\nDecModel.fit(xtrain, ytrain)\n\nDecPreds = DecModel.predict(xtest)\n\naccuracy_score(DecPreds, ytest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets predict on the data using Random Forest Classifier:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"DecModel = RandomForestClassifier(n_estimators=1500)\n\nDecModel.fit(xtrain, ytrain)\n\nDecPreds = DecModel.predict(xtest)\n\naccuracy_score(DecPreds, ytest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Defining a Neural Network for binary classification now:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def neuralnet(xtrain, xtest, ytrain, ytest):\n    NModel = Sequential([\n    Dense(128, input_shape=(19,), activation='relu'),\n    Dense(240, activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(1, activation='sigmoid')\n    ])\n    \n    adam = Adam(learning_rate=0.007)\n    \n    NModel.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n    \n    Fit = NModel.fit(xtrain, ytrain, epochs=50, validation_data=(xtest, ytest))\n    return Fit","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now its time to use Standard Scaler:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scale = StandardScaler()\n\nscaledtrain = scale.fit_transform(xtrain)\nscaledtest = scale.transform(xtest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets train a neural network on the scaled data:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"neuralnet(scaledtrain, scaledtest, ytrain, ytest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thank you for going through this notebook! I hope this helped you.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}