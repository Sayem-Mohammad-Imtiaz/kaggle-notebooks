{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# To handle datetime data-type\nimport time, warnings\nimport datetime as dt\n\n#visualizations\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import scatter_matrix\n%matplotlib inline\nimport seaborn as sns #easy to use, awesome\n\n# !!! add this to your first cell at the END of YOUR WORK !!!\n# if you are sure your code is correct, it is beneficial to ignore warnings\nwarnings.filterwarnings(\"ignore\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# after adding data to Kaggle Input directory, import it to notebook\nretail_df = pd.read_csv('/kaggle/input/onlineretail/OnlineRetail.csv',encoding=\"ISO-8859-1\",dtype={'CustomerID': str,'InvoiceID': str})\nretail_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating a new column \"Amount\" by multiplying \"Quantity\" and \"Unit Price\"\nretail_df[\"Amount\"]= retail_df[\"Quantity\"]*retail_df[\"UnitPrice\"]\nretail_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"retail_df.dtypes #to see data types","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"retail_df.describe(exclude=\"number\") #a brief summary for columns whose data types are NOT NUMBER","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"retail_df.describe() #a brief summary for columns whose data types are NUMBER","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"retail_df.shape # #of rows, #of columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convertin a column's datatype to datetime\nretail_df[\"InvoiceDate\"] = pd.to_datetime(retail_df[\"InvoiceDate\"])\nretail_df.dtypes\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"retail_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#I would like to make analysis for only one country but which one? Let's see the proportions\n\nretail_df.Country.value_counts(normalize=True) #this code counts all values in Country column (and see proportions with the parameter \"normalize=True\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# proportion of UK customers\n\nretail_df[retail_df[\"Country\"]==\"United Kingdom\"].CustomerID.nunique() / retail_df.CustomerID.nunique() #90% of the customers are from UK","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#forming a sub-sample which includes only the customers from UK\n\nretail_uk = retail_df[retail_df['Country']=='United Kingdom']\n\n#check the shape\nretail_uk.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove canceled orders\nretail_uk = retail_uk[retail_uk['Quantity']>0]\nretail_uk.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove rows where customerID are NA since we are going to do CUSTOMER SEGMENTATION\nretail_uk.dropna(subset=['CustomerID'],how='all',inplace=True) # check out the documentation for dropping rows\n# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html\n\n\nretail_uk.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We would like to restrict the data to one full year because it's better to use a metric per Months or Years in RFM\nretail_uk['InvoiceDate'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"retail_uk['InvoiceDate'].min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I am taking the last one year in the dataset\nretail_uk = retail_uk[retail_uk['InvoiceDate']>= \"2010-12-09\"]\nretail_uk.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"retail_uk['InvoiceDate'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"retail_uk['InvoiceDate'].min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Average total quantity ordered by a customer\nnp.mean(retail_uk.groupby(\"CustomerID\").Quantity.sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Average total amount spent by a customer\nnp.mean(retail_uk.groupby(\"CustomerID\").Amount.sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Average quantity in a row\n# Note that each row represent a purchased (by a customer) product in an invoice\n# In other words, primary key for this data set is __InvoiceNo-StockCode-CustomerID__\nretail_uk.Quantity.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Average amount in a row\nretail_uk.Amount.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of unique products sold between 2010-12-09 and 2011-12-09\nretail_uk.StockCode.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Summary..\")\n#exploring the unique values of each attribute\nprint(\"Number of invoices: \", retail_uk['InvoiceNo'].nunique())\nprint(\"Number of products bought: \",retail_uk['StockCode'].nunique())\nprint(\"Number of customers:\", retail_uk['CustomerID'].nunique() )\nprint(\"Percentage of customers NA: \", round(retail_uk['CustomerID'].isnull().sum() * 100 / len(retail_uk),2),\"%\" )\nprint(\"Average quantity of product purchased by a customer: \", round(np.mean(retail_uk.groupby(\"CustomerID\").Quantity.sum()), 0))\nprint(\"Average revenue generated per customer: \", round(np.mean(retail_uk.groupby(\"CustomerID\").Amount.sum()), 2))\nprint(\"Average product quantity sold per transaction: \", round(retail_uk.Quantity.mean(), 0))\nprint(\"Average revenue generated per transaction: \", round(retail_uk.Amount.mean(), 2) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"retail_uk.describe(exclude='number')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"retail_uk.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for NaN's to see if dataset is ready to go\n\nretail_uk.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To calculate recency, we need a reference\n#The difference (in days) between NOW and date of invoice will give us recency. Range of recency will be (0, 365)\nnow = dt.date(2011,12,9)\nprint(now)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a new column called date which contains the date of invoice only\nretail_uk['date'] = pd.DatetimeIndex(retail_uk['InvoiceDate']).date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"retail_uk.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CREATE RECENCY DATAFRAME\n#group by customers and check the last date of purchase\nrecency_df = retail_uk.groupby(by='CustomerID', as_index=False)['date'].max()\nrecency_df.columns = ['CustomerID','LastPurshaceDate']\nrecency_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculate recency\nrecency_df['Recency'] = recency_df['LastPurshaceDate'].apply(lambda x: (now - x).days)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recency_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop LastPurchaseDate as we don't need it anymore\nrecency_df.drop('LastPurshaceDate',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CREATE FREQUENCY DATAFRAME\n# drop duplicates\nretail_uk_copy = retail_uk\nretail_uk_copy.drop_duplicates(subset=['InvoiceNo', 'CustomerID'], inplace=True) \n\n#What we do is here is a bit complex compared to previous codes. I would like to calculate the number of invoices for each induvidual customer.\n#Since dataset involves so many duplicates of ['InvoiceNo', 'CustomerID'], we are keeping just one.\n#Call \"retail_uk.head()\" and look these two columns to understand better\n\n#Calculate frequency of purchases\nfrequency_df = retail_uk_copy.groupby(by=['CustomerID'], as_index=False)['InvoiceNo'].count()\nfrequency_df.columns = ['CustomerID','Frequency']\nfrequency_df.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"retail_uk_copy.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Amount spent by each individual customer\nmonetary_df = retail_uk.groupby(by='CustomerID',as_index=False)['Amount'].sum()\nmonetary_df.columns = ['CustomerID','Monetary']\nmonetary_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RFM TABLE\n\n#merge recency dataframe with frequency dataframe, resulting a temporary dataframe\ntemp_df = recency_df.merge(frequency_df,on='CustomerID')\ntemp_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#merge with monetary dataframe to get a table with the 3 columns\nrfm_df = temp_df.merge(monetary_df,on='CustomerID')\n#check the head\nrfm_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make 0's 1, I do not want to have 0's in Recency column\nrfm_df['Recency'] = rfm_df[\"Recency\"] + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfm_df.describe()\n#As seen, we have 3863 customer with R-F-M values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#outlier treatment: the algorithm we are going to use is vulnerable to outliers which are very very manipulative\n#if you do not exclude outliers, the results will be heavily influenced\n\n#outlier treatment for recency\nQ1 = rfm_df.Recency.quantile(0.25)\nQ3 = rfm_df.Recency.quantile(0.75)\nIQR = Q3 - Q1\nrfm_df = rfm_df[(rfm_df.Recency >= (Q1 - 1.5*IQR)) & (rfm_df.Recency <= (Q3 + 1.5*IQR))]\nrfm_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#outlier treatment for frequency\nQ1 = rfm_df.Frequency.quantile(0.25)\nQ3 = rfm_df.Frequency.quantile(0.75)\nIQR = Q3 - Q1\nrfm_df = rfm_df[(rfm_df.Frequency >= (Q1 - 1.5*IQR)) & (rfm_df.Frequency <= (Q3 + 1.5*IQR))]\nrfm_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#outlier treatment for monetary\nQ1 = rfm_df.Monetary.quantile(0.25)\nQ3 = rfm_df.Monetary.quantile(0.75)\nIQR = Q3 - Q1\nrfm_df = rfm_df[(rfm_df.Monetary >= (Q1 - 1.5*IQR)) & (rfm_df.Monetary <= (Q3 + 1.5*IQR))]\nrfm_df.describe()\n\n#After excluding outliers, 3147 customers are left","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing modules for k-Means\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler #I am going to scale variables manually but it is possible to use one of the scalers here\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom scipy.cluster.hierarchy import linkage, dendrogram, cut_tree","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#scaling is crucial step to make k-Means clustering, scaling range could differ\n\n#scaling Recency values to the range (0,1)\nrfm_df[\"R\"]= (rfm_df[\"Recency\"]-rfm_df[\"Recency\"].min())/(rfm_df[\"Recency\"].max()-rfm_df[\"Recency\"].min())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#scaling Frequency values to the range (0,1)\nrfm_df[\"F\"]= (rfm_df[\"Frequency\"]-rfm_df[\"Frequency\"].min())/(rfm_df[\"Frequency\"].max()-rfm_df[\"Frequency\"].min())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#scaling Monetary values to the range (0,1)\nrfm_df[\"M\"]= (rfm_df[\"Monetary\"]-rfm_df[\"Monetary\"].min())/(rfm_df[\"Monetary\"].max()-rfm_df[\"Monetary\"].min())\nrfm_df.head()\n\n#In this analysis, the value of the customer is inversely proportional to the Recency value and directly proportional to the Frequency and Monetary values.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now, we formed a new DataFrame, which RFM analysis can be made\n\ndf = rfm_df\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Recency\"].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Frequency\"].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Monetary\"].hist()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"fig, (ax0, ax1, ax2) = plt.subplots(nrows= 1, ncols= 3, sharey=True, figsize=(15, 6))\n\nsns.distplot(df[\"Recency\"], ax=ax0, kde=False, color='b')\nax0.set(xlabel=\"Recency\", ylabel=\"Number of customers\")\n\nsns.distplot(df[\"Frequency\"], ax=ax1, kde=False, color='r')\nax1.set(xlabel=\"Frequency\")\n\nsns.distplot(df[\"Monetary\"], ax=ax2, kde=False, color='g')\nax2.set(xlabel=\"Monetary\")\n\n\nfig.savefig(\"Histograms of Attributes\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#resetting index for future concatenate purposes\n#pd.concat() needs same indexes to concatenate pandas objects, otherwise some rows will be lost\n\ndf.reset_index(drop=True, inplace=True)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#HOPKIN's STATISTICS\n#tells how much data is suitable to cluster\n\nfrom sklearn.neighbors import NearestNeighbors\nfrom random import sample\nfrom numpy.random import uniform\nimport numpy as np\nfrom math import isnan\n \ndef hopkins(X):\n    d = X.shape[1]\n    #d = len(vars) # columns\n    n = len(X) # rows\n    m = int(0.1 * n) \n    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)\n \n    rand_X = sample(range(0, n, 1), m)\n \n    ujd = []\n    wjd = []\n    for j in range(0, m):\n        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)\n        ujd.append(u_dist[0][1])\n        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)\n        wjd.append(w_dist[0][1])\n \n    H = sum(ujd) / (sum(ujd) + sum(wjd))\n    if isnan(H):\n        print(ujd, wjd)\n        H = 0\n \n    return H","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hopkins(df[[\"R\",\"F\",\"M\"]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#forming a new DF which includes only scaled R,F,M scores\n\ndf_1 = df[[\"CustomerID\",\"R\",\"F\",\"M\"]]\ndf_1.set_index(\"CustomerID\", inplace=True)\ndf_1.head(20)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#DENDROGRAM\n\nd = linkage(df_1, method='complete')\ndendrogram(d,\n          leaf_rotation=90,\n          leaf_font_size=5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#From dendrogram, it can be said that the possible numbers of clusters are 3,4,5,6 \n\n#First Clustering\n\nmodel = KMeans(n_clusters= 4, init= 'random', max_iter= 500, tol= 1e-10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(df_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.cluster_centers_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.inertia_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.n_iter_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#FIND OPTIMUM \"K\" for k-Means\n\nfrom sklearn.metrics import silhouette_score\nsse_ = []\nssd = []\ncentroids_from_kmeans = []\niterations = []\n\nfor k in range(2, 15):\n    kmeans = KMeans(n_clusters= k, init= 'random', n_init= 100, max_iter= 500, tol= 1e-10).fit(df_1)\n    sse_.append([k, silhouette_score(df_1, kmeans.labels_)])\n    ssd.append([k, kmeans.inertia_])\n    centroids_from_kmeans.append(kmeans.cluster_centers_)\n    iterations.append([k, kmeans.n_iter_])\n\nprint(\"Silhouette Score for each K : \\n\", sse_)\nprint(\"Inertia for each K : \\n\", ssd)\nprint(\"Number of iterations for each K : \", iterations)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(pd.DataFrame(sse_)[0], pd.DataFrame(sse_)[1], label= \"Silhouette Score\")\nplt.title(\"Silhouette Scores for Varying Number of Clusters\")\nplt.xlabel(\"# of clusters\")\nplt.ylabel(\"Silhouette Score\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(pd.DataFrame(ssd)[0], pd.DataFrame(ssd)[1], label= \"Inertia\")\nplt.title(\"Inertia of K-means Clustering Results\")\nplt.xlabel(\"# of clusters\")\nplt.ylabel(\"Inertia\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#FIND OPTIMUM \"K\" for k-Means\n\nsse_ward = []\n\nfor k in range(2, 15):\n    h_cluster = AgglomerativeClustering(n_clusters= k)\n    h_cluster.fit(df_1)\n    sse_ward.append([k, silhouette_score(df_1, h_cluster.labels_)])\n\nprint(sse_ward)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(pd.DataFrame(sse_ward)[0], pd.DataFrame(sse_ward)[1], label= \"Silhouette Score\")\nplt.title(\"Silhouette Scores for Varying Number of Clusters\")\nplt.xlabel(\"# of clusters\")\nplt.ylabel(\"Silhouette Score\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#since low inertia and high silhouette score is desirable, let \"K\" (number of clusters) equals 3\n#this is also in parallel with what dendrogram suggests\n\n\n#Hierarchical Cluster Analyses to find initial seeds for k-means\n\nclustering= AgglomerativeClustering(n_clusters= 3) #linkage=\"ward\", by default\nclustering.fit(df_1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Just checking, Agglomerative clustering results\n\nlabels_v0= pd.DataFrame(clustering.labels_)\nlabels_v0\nRFM_0= pd.concat([df, labels_v0], axis=1)\nRFM_0.columns= [\"CustomerID\", \"Recency\", \"Frequency\", \"Monetary\", \"R\", \"F\", \"M\", \"Cluster\"]\nRFM_0[\"Cluster\"] = RFM_0[\"Cluster\"] +1\nRFM_ward= RFM_0\nRFM_ward.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RFM_ward.groupby(\"Cluster\").CustomerID.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Clusters_R= pd.DataFrame(RFM_ward.groupby(\"Cluster\").R.mean())\nClusters_R","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Clusters_F= pd.DataFrame(RFM_ward.groupby(\"Cluster\").F.mean())\nClusters_F","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Clusters_M= pd.DataFrame(RFM_ward.groupby(\"Cluster\").M.mean())\nClusters_M","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#initial seeds for k-means\n\nclusters_ward= pd.concat([Clusters_R, Clusters_F, Clusters_M], axis=1)\ninitial_seeds= clusters_ward.to_numpy() #converting to numpy array\nprint(initial_seeds.dtype)\nprint(initial_seeds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inertias_in_each_iteration= []\ncentroids= []\nnumber_of_iterations = []\n\nfor i in range(20):\n    model= KMeans(n_clusters= 3, init= initial_seeds, max_iter= 500, tol= 1e-10)\n    model.fit(df_1)\n    inertias_in_each_iteration.append(model.inertia_)\n    centroids.append(model.cluster_centers_)\n    number_of_iterations.append(model.n_iter_)\n\nprint(inertias_in_each_iteration)\nprint(centroids[-1])\nprint(number_of_iterations)\nlabels= model.labels_\nlabels= pd.Series(labels)\nprint(silhouette_score(df_1, labels))\n\n#compare this silhouette value with the silhouette value obtained from k-means with 'random' initial seeds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inertias_in_each_iteration= []\ncentroids= []\nnumber_of_iterations = []\n\nfor i in range(300):\n    model= KMeans(n_clusters= 3, init= 'random', n_init= 10, max_iter= 500, tol= 1e-10)\n    model.fit(df_1)\n    inertias_in_each_iteration.append(model.inertia_)\n    centroids.append(model.cluster_centers_)\n    number_of_iterations.append(model.n_iter_)\n\nprint(np.mean(inertias_in_each_iteration))\nprint(centroids[-1])\nprint(np.mean(number_of_iterations))\nlabels= model.labels_\nlabels= pd.Series(labels)\nprint(silhouette_score(df_1, labels))\n\n#compare this silhouette value with the silhouette value obtained from k-means with 'random' initial seeds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#resulting dataframe showing customer R,F,M values and which cluster the customer belongs to\n\nRFM_1 = pd.concat([df, pd.Series(labels)], axis= 1)\nRFM_1.columns= [\"CustomerID\", \"Recency\", \"Frequency\", \"Monetary\", \"R\", \"F\", \"M\", \"Cluster\"]\nRFM_1[\"Cluster\"] = RFM_1[\"Cluster\"] +1\nRFM_1.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RFM_1.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RFM_1.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of customers in each cluster\n\nRFM_1.groupby(\"Cluster\").CustomerID.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Clusters_Recency= pd.DataFrame(RFM_1.groupby(\"Cluster\").Recency.mean())\nClusters_Recency","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Clusters_Frequency= pd.DataFrame(RFM_1.groupby(\"Cluster\").Frequency.mean())\nClusters_Frequency","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Clusters_Monetary= pd.DataFrame(RFM_1.groupby(\"Cluster\").Monetary.mean())\nClusters_Monetary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Clusters_R= pd.DataFrame(RFM_1.groupby(\"Cluster\").R.mean())\nClusters_R","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Clusters_F= pd.DataFrame(RFM_1.groupby(\"Cluster\").F.mean())\nClusters_F","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Clusters_M= pd.DataFrame(RFM_1.groupby(\"Cluster\").M.mean())\nClusters_M","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cluster centroids\n\nclusters_1= pd.concat([Clusters_Recency, Clusters_Frequency, Clusters_Monetary, Clusters_R, Clusters_F, Clusters_M], axis=1)\nclusters_1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating 4 new dataframe (1 for each cluster)\n\ncluster_1= RFM_1[RFM_1[\"Cluster\"]==1]\ncluster_2= RFM_1[RFM_1[\"Cluster\"]==2]\ncluster_3= RFM_1[RFM_1[\"Cluster\"]==3]\ncluster_1.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g= sns.catplot(x=\"Cluster\", y=\"Recency\", data= RFM_1, kind=\"box\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g= sns.catplot(x=\"Cluster\", y=\"Recency\", data= RFM_1, kind=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g= sns.catplot(x=\"Cluster\", y=\"Frequency\", data= RFM_1, kind=\"box\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g= sns.catplot(x=\"Cluster\", y=\"Frequency\", data= RFM_1, kind=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g= sns.catplot(x=\"Cluster\", y=\"Monetary\", data= RFM_1, kind=\"box\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g= sns.catplot(x=\"Cluster\", y=\"Monetary\", data= RFM_1, kind=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.relplot(x=\"Recency\", y=\"Frequency\",\n           data=RFM_1, kind=\"scatter\",\n           hue=\"Cluster\", style=\"Cluster\", alpha=0.7)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.relplot(x=\"Recency\", y=\"Monetary\",\n           data=RFM_1, kind=\"scatter\",\n           hue=\"Cluster\", style=\"Cluster\", alpha=0.7)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.relplot(x=\"Frequency\", y=\"Monetary\",\n           data=RFM_1, kind=\"scatter\",\n           hue=\"Cluster\", style=\"Cluster\", alpha=0.7)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(RFM_1, vars=[\"Recency\", \"Frequency\", \"Monetary\"], hue=\"Cluster\",\n             diag_kind= 'auto', diag_kws={'alpha': 0.4}, corner=False,\n            palette=\"husl\", plot_kws={'alpha': 0.4},  height=3, aspect=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RFM_1.Recency.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RFM_1.Frequency.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RFM_1.Monetary.mean()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}