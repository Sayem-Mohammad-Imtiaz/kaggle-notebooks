{"cells":[{"metadata":{"id":"HP9U_AO8stAq"},"cell_type":"markdown","source":"# Introduction to Logistic Regression\n### This acts as a good benchmark to start understanding Logistic Regression\n### If you find this interesting, do Upvote the kernel!","execution_count":null},{"metadata":{"id":"t5WlU9mlAtTt"},"cell_type":"markdown","source":"# Background and data exploration","execution_count":null},{"metadata":{"id":"dYeVd9eptE31"},"cell_type":"markdown","source":"Each biopsy sample in the dataset is labeled with an ID number and whether or not the tumor it came from is malignant (M) or benign (B). Each sample also has 10 different features associated with it, some of which are described above. Remember that each feature value for a given biopsy sample is a real-valued number.","execution_count":null},{"metadata":{"id":"zTLBQUJZJpEU","trusted":true},"cell_type":"code","source":"# Load the data\nimport pandas as pd\nfrom sklearn import metrics\ndata = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')\ndata['diagnosis'].replace({'M':1, 'B':0}, inplace = True)\ndata.to_csv('data.csv')\ndel data","execution_count":null,"outputs":[]},{"metadata":{"id":"LSpUGH4NvaUk"},"cell_type":"markdown","source":"## Loading our annotated dataset\n\nThe first step in building our breast cancer tumor classification model is to load in the dataset we'll use to \"teach\" (or \"train\") our model.","execution_count":null},{"metadata":{"id":"LmyUG5GrvdPf","trusted":true},"cell_type":"code","source":"# First, import helpful Python tools for loading/navigating data\nimport os             # Good for navigating your computer's files \nimport numpy as np    # Great for lists (arrays) of numbers\nimport pandas as pd   # Great for tables (google spreadsheets, microsoft excel, csv)","execution_count":null,"outputs":[]},{"metadata":{"id":"5Cu2tGYpv5AZ","trusted":true},"cell_type":"code","source":"data_path  = 'data.csv'","execution_count":null,"outputs":[]},{"metadata":{"id":"TE3BWCCJv8TZ","trusted":true},"cell_type":"code","source":"# Use the 'pd.read_csv('file')' function to read in read our data and store it in a variable called 'dataframe'\ndataframe = pd.read_csv(data_path)","execution_count":null,"outputs":[]},{"metadata":{"id":"YKx4euGqwHpS"},"cell_type":"markdown","source":" ## Looking at our dataset\n \n A key step in machine learning (and coding in general!) is to view the structure and dimensions of our new dataframe, which stores all our training data from the tumor biopsies. We want to confirm that the size of our table is correct, check out the features present, and get a more visual sense of what it looks like overall.","execution_count":null},{"metadata":{"id":"eUBEd-10v-Ro","trusted":true},"cell_type":"code","source":"# Let's look at just a few of the biopsy sample features to start by subsetting our 'dataframe'\ndataframe = dataframe[['diagnosis','radius_mean','area_mean', 'radius_se', 'area_se', 'smoothness_mean','smoothness_se']]","execution_count":null,"outputs":[]},{"metadata":{"id":"44xNblKYVRHh"},"cell_type":"markdown","source":"You can think of dataframes like Google or Microsoft Excel spreadsheets (large tables with row/column headers) \n\n**Use the 'head()' method to show the first five rows of the table and their corresponding column headers (our 7 biopsy features!)**","execution_count":null},{"metadata":{"id":"sKRxaP1qweIz","outputId":"bc27002a-9fc3-458b-88a9-ac8acff37e06","trusted":true},"cell_type":"code","source":"dataframe.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"tilXEPhVwkjk"},"cell_type":"markdown","source":"\n* $diagnosis$: Whether or not the tumor was diagnosed as malignant (M) or benign (B).\n* $radius$_$mean$: The radius data feature, averaged across cells in that particular biopsy\n* $area$_$mean$: The area data feature, averaged across cells in that particular biopsy\n* $radius$_$se$: The standard error of the radius data feature for cells in that particular biopsy\n* $area$_$se$: The standard error of the area data feature for cells in that particular biopsy\n* $smoothness$_$mean$: The smoothness feature, averaged across cells in that particular biopsy\n* $smoothness$_$se$: The standard error of the smoothness data feature for cells in that particular biopsy\n\nRecall that the term mean refers to taking an average (summing the values for each cell and dividing by the total number of cells observed in that biopsy). Additionally, standarded error gives a sense of the standard deviation (how much variance there is between cells in that biopsy for that feature). ","execution_count":null},{"metadata":{"id":"0-SoD9EQw5Ie","outputId":"0f331e1d-2a59-4150-cb2d-954305bb3e64","trusted":true},"cell_type":"code","source":"# Next, we'll use the 'info' method to see the data types of each column\ndataframe.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"LP0-9c4VxsJ7","outputId":"6612de37-fa48-4d60-f945-363160054589","trusted":true},"cell_type":"code","source":"# First, we'll import some handy data visualization tools\nimport seaborn as sns\nimport matplotlib.pyplot as plt ","execution_count":null,"outputs":[]},{"metadata":{"id":"IULVFMLhyLwN","outputId":"a8fcf2e8-f763-44a2-b07f-cc38424f54c6","trusted":true},"cell_type":"code","source":"# To see how well mean radius correlates with diagnosis, we'll plot the data\n# separated based on diagnosis category on the x-axis and have the points' y-value\n# be its mean radius value\n\nsns.catplot(x = 'diagnosis', y = 'radius_mean', data = dataframe)","execution_count":null,"outputs":[]},{"metadata":{"id":"sFG1rBi8zq0n"},"cell_type":"markdown","source":"Next, we might want to check just how well mean radius can be used to classify, or separate, the datapoints in either category\nLet's pick a boundary value for the radius mean and see how well it separates the data","execution_count":null},{"metadata":{"id":"1oYBTJsvyaC6","outputId":"80f1ad3c-bab8-4bfe-f8bb-4e716aa65b25","trusted":true},"cell_type":"code","source":"boundary = 10\nsns.scatterplot(x = 'radius_mean', y = 'diagnosis', data = dataframe)\nplt.plot([boundary, boundary], [0, 1], 'g', linewidth = 6)","execution_count":null,"outputs":[]},{"metadata":{"id":"6ZmcQ_vGWZnR"},"cell_type":"markdown","source":"Using a boundary value, we can build a boundary classifier function. This function will take in a boundary value of our choosing and then classify the data points based on whether or not they are above or below the boundary\n\n#### Building the boundary classifier\nHere we build the function that takes in a target boundary (value of radius mean). Write a function to implement a boundary classifier. Think about what the return 'type' of this classifier might be.","execution_count":null},{"metadata":{"id":"kBttJRoNYYdJ"},"cell_type":"markdown","source":"The code below chooses a boundary and runs it for us. ","execution_count":null},{"metadata":{"id":"0PiRMX-xKjiA","outputId":"3d60b850-7f03-45ed-b365-3488a53adf99","trusted":true},"cell_type":"code","source":"def boundary_classifier(target_boundary,x):\n  result = []\n  for i in x:\n    if i > target_boundary:\n      result.append(1)\n    else:\n      result.append(0)\n  return result\n     \nchosen_boundary = 15\ny_pred = boundary_classifier(chosen_boundary, dataframe['radius_mean'])\ndataframe['predicted'] = y_pred\ny_true = dataframe['diagnosis']\nsns.scatterplot(x = 'radius_mean', y = 'diagnosis', hue = 'predicted', data = dataframe)\nplt.plot([chosen_boundary, chosen_boundary], [0, 1], 'g', linewidth = 6)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"Ro6toAITfrp1","outputId":"a66cbdc2-10b3-4515-ba3a-a8a746c17c4d","trusted":true},"cell_type":"code","source":"accuracy = metrics.accuracy_score(y_true,y_pred)\naccuracy","execution_count":null,"outputs":[]},{"metadata":{"id":"YwOdbfoiL9pz"},"cell_type":"markdown","source":"**True positive rate (TPR)**: Sometimes called sensitvity, the TPR is the proportion of actual positives that are correctly identified as such. An analogy would be the percentage of sick people who are correctly identified as having the disease in some population.\n\n**True negative rate (TNR)**: Sometimes called specificity, the TNR is the proportion of actual negatives that are correctly identified as such. An analogy would be the the percentage of healthy people who are correctly identified as not having the disease in some population.\n\n**False positive rate (FPR)**: The FPR is the proportion of actual negatives that are incorrectly identified as positives. An analogy would be the percentage of healthy people who are incorrectly identified as having the disease.\n\n**False negative rate (FNR)**: The FPR is the proportion of actual positives that are incorrectly identified as negatives. An analogy would be the percentage of sick people who are incorrectly identified as healthy.\n\nA key insight is that there is a tradeoff when trying to reduce the different types of errors. For instance, if we want to increase our TPR (thus decrease our FNR by correctly identifying more sick people), our improvements will have to increase the number of people we guess to be sick. However, such an improvement will decrease our TNR (thus inrease our FPR by guessing more healty people are sick). \n\nSometimes, one type of error is worse than the others for a given problem. Other times, however, we must strike an acceptable balance between the two.","execution_count":null},{"metadata":{"id":"Koc7oPy8AASf"},"cell_type":"markdown","source":"![alt text](https://drive.google.com/uc?export=view&id=1S4S2MBM86D74C-Q0aPPwHzbU8iUveLKq)","execution_count":null},{"metadata":{"id":"EcfsPpup9ljK","outputId":"ab921689-9e98-4c75-ba6d-a4a52132cf03","trusted":true},"cell_type":"code","source":"# Import the metrics class\nfrom sklearn import metrics\n\n# Create the Confusion Matrix\ny_test = dataframe['diagnosis']\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n\n# Visualizing the Confusion Matrix\nclass_names = [0,1] # Our diagnosis categories\n\nfig, ax = plt.subplots()\n# Setting up and visualizing the plot (do not worry about the code below!)\ntick_marks = np.arange(len(class_names)) \nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g') # Creating heatmap\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y = 1.1)\nplt.ylabel('Actual diagnosis')\nplt.xlabel('Predicted diagnosis')","execution_count":null,"outputs":[]},{"metadata":{"id":"0xqCb1zElUUo","trusted":true},"cell_type":"code","source":"# YOUR CODE HERE:  \ndef model_stats(y_test, y_pred):\n  print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred))\n  print(\"Precision: \", metrics.precision_score(y_test, y_pred))\n  print(\"Recall: \", metrics.recall_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"tymoFFLaJzMc","outputId":"bdc3940e-d2eb-4007-9bcf-360ffaca6702","trusted":true},"cell_type":"code","source":"model_stats(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"id":"sHhZeAbdupMl"},"cell_type":"markdown","source":"# Finding a better separation with logistic regression\n","execution_count":null},{"metadata":{"id":"ZEq3DYrh1nPO","trusted":true},"cell_type":"code","source":"# Let's pull our handy linear fitter from our 'prediction' toolbox: sklearn!\nfrom sklearn import linear_model","execution_count":null,"outputs":[]},{"metadata":{"id":"krQ6dJo-5yek","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, test_df = train_test_split(dataframe, test_size = 0.4, random_state = 1)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"wDYAAl6xgPYO","outputId":"9d4a62a8-fcc8-4c56-8e1f-e4343f071819","trusted":true},"cell_type":"code","source":"print('\\n\\nTraining dataframe has %d rows'%train_df.shape[0])\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"EMornfXPhzli","outputId":"e6526406-756a-4ba0-b0e0-5cf111182e20","trusted":true},"cell_type":"code","source":"print('\\n\\nTesting dataframe has %d rows'%test_df.shape[0])\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"chesGAaKNVON","outputId":"168dd378-ef19-48c7-891d-664b707a45ea","trusted":true},"cell_type":"code","source":"input_labels = ['radius_mean']\noutput_label = 'diagnosis'\n\n\nx_train = train_df[input_labels]\nprint('Our x variables')\nprint(x_train.head())\nprint('\\n\\n')\n\ny_train = train_df[output_label]\nprint('Our y variable:')\nprint(y_train.head())","execution_count":null,"outputs":[]},{"metadata":{"id":"kj1LIpVT6SwY","trusted":true},"cell_type":"code","source":"# Here, we create a 'reg' object that handles the line fitting for us!\nclass_rm = linear_model.LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"id":"NfV0t3bqjtum","outputId":"e04b8559-059c-431d-dfc0-b705efb41ff9","trusted":true},"cell_type":"code","source":"class_rm = linear_model.LogisticRegression()\nclass_rm.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"WaBE6yg6kF2N","trusted":true},"cell_type":"code","source":"x_test = test_df[input_labels]","execution_count":null,"outputs":[]},{"metadata":{"id":"0Xcz9OW6kcAI","trusted":true},"cell_type":"code","source":"y_test = test_df[output_label].values.squeeze()","execution_count":null,"outputs":[]},{"metadata":{"id":"_59Wk6q46XVq","trusted":true},"cell_type":"code","source":"y_pred = class_rm.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"EjwNluOxOtoC","outputId":"a475b2d6-dc3a-4699-c241-52a937dd19e1","trusted":true},"cell_type":"code","source":"print(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"id":"ensyfit5lP1U"},"cell_type":"markdown","source":"Run the code below to visualize the results","execution_count":null},{"metadata":{"id":"GUSOLlz8lJu-","outputId":"690544f0-1082-4228-dd83-49dff95d1893","trusted":true},"cell_type":"code","source":"y_pred = y_pred.squeeze()\nx_test_view = x_test[input_labels].values.squeeze()\nsns.scatterplot(x = x_test_view, y = y_pred, hue = y_test)\nplt.xlabel('Radius')\nplt.ylabel('Predicted')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"id":"x_DJgPAHlaVQ"},"cell_type":"markdown","source":"### Finally, let's re-evalute the recall, accuracy, and precision for the model by calling the functions we created.","execution_count":null},{"metadata":{"id":"BKz5TfiOTHDl","outputId":"e1b30d2c-61cd-4cfc-df83-df5caf549ecf","trusted":true},"cell_type":"code","source":"model_stats(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"id":"V0tu5u7Y7CVL","outputId":"8cd1d341-a3e5-4c2c-c97b-c1370b78ea21","trusted":true},"cell_type":"code","source":"# Let's visualize the probabilities for `x_test`\ny_prob = class_rm.predict_proba(x_test)\nsns.scatterplot(x = x_test_view, y = y_prob[:,1], hue = y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"4768savLF9kQ"},"cell_type":"markdown","source":"## Visualization: linear vs. logistic regression\n\n","execution_count":null},{"metadata":{"id":"nVZQExC5Qhbs"},"cell_type":"markdown","source":"This plot shows the graphical representations described above. As you can see, the linear model can yield predicted values outside the [0,1] range because it is a continuous linear function. \n\nOn the other hand, the logistic model stays within our bounds. You can see that the logistic model gives a \"line\" with curvy ends in the [0,1] range, which is the best approximation for a line that will also always respect these boundaries. \n\n**Confusingly, the biggest difference between linear and logistic regression is that linear regression is used for regression problems (predicting the value of continuous variables) while logistic regression is used for classification problems!**\n\n*Linear Regression:*\n\n![Linear Regression](https://i.stack.imgur.com/kW8YP.png)\n\n*Logistic Regression:*\n\n![Logistic Regression](https://techdifferences.com/wp-content/uploads/2018/01/graph-logistic-regression.jpg)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}