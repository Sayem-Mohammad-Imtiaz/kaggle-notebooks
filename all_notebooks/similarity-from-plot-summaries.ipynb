{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-11T14:05:16.058928Z","iopub.execute_input":"2021-08-11T14:05:16.059586Z","iopub.status.idle":"2021-08-11T14:05:16.125941Z","shell.execute_reply.started":"2021-08-11T14:05:16.059466Z","shell.execute_reply":"2021-08-11T14:05:16.124475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load the dataset","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/movies-similarity/movies.csv')\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:05:54.480926Z","iopub.execute_input":"2021-08-11T14:05:54.481472Z","iopub.status.idle":"2021-08-11T14:05:54.698991Z","shell.execute_reply.started":"2021-08-11T14:05:54.481426Z","shell.execute_reply":"2021-08-11T14:05:54.697746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Basic data exploration","metadata":{}},{"cell_type":"code","source":"# Check the shape of the dataset\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:07:54.777712Z","iopub.execute_input":"2021-08-11T14:07:54.77831Z","iopub.status.idle":"2021-08-11T14:07:54.785766Z","shell.execute_reply.started":"2021-08-11T14:07:54.778255Z","shell.execute_reply":"2021-08-11T14:07:54.784289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for duplicates\ndata.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:07:29.186859Z","iopub.execute_input":"2021-08-11T14:07:29.187491Z","iopub.status.idle":"2021-08-11T14:07:29.200893Z","shell.execute_reply.started":"2021-08-11T14:07:29.187403Z","shell.execute_reply":"2021-08-11T14:07:29.199728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for missing values\ndata.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:08:16.746385Z","iopub.execute_input":"2021-08-11T14:08:16.746773Z","iopub.status.idle":"2021-08-11T14:08:16.756579Z","shell.execute_reply.started":"2021-08-11T14:08:16.746743Z","shell.execute_reply":"2021-08-11T14:08:16.755144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that we have 10 missing summaries in IMDb plot column.\n\nTo resolve this, we could create a new column called \"Plot\" and append \"Wiki_Plot\" with \"Imdb_Plot\"","metadata":{}},{"cell_type":"code","source":"# Create a new column \"Plot\"\ndata[\"plot\"] = data[\"wiki_plot\"].astype(str) + \"\\n\" + data[\"imdb_plot\"].astype(str)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:11:08.227368Z","iopub.execute_input":"2021-08-11T14:11:08.227795Z","iopub.status.idle":"2021-08-11T14:11:08.268039Z","shell.execute_reply.started":"2021-08-11T14:11:08.227761Z","shell.execute_reply":"2021-08-11T14:11:08.267004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for missing values again\ndata.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:18:07.056738Z","iopub.execute_input":"2021-08-11T14:18:07.058102Z","iopub.status.idle":"2021-08-11T14:18:07.092687Z","shell.execute_reply.started":"2021-08-11T14:18:07.058033Z","shell.execute_reply":"2021-08-11T14:18:07.090969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, our new column \"plot\" doesn't have any missing value. We're going to use this column for our further processing.","metadata":{}},{"cell_type":"markdown","source":"## Use NLP techniques to convert \"plot\" into numerical vectors","metadata":{}},{"cell_type":"markdown","source":"### Tokenization\n\n_Tokenization is the process by which we break down articles into individual sentences or words, as per the requirement. It is required to enable machines to understand context between 2 articles, if they are similar or far apart._\nFor example, consider a sentence \"This is a sentence\", if we try to match this whole sentence in the document, we might not find any matching sentence, but if we break the sentence into words and try find match again with words, it might match using different words in the document.","metadata":{}},{"cell_type":"markdown","source":"To apply tokenization, we need a python library called `nltk`. ","metadata":{}},{"cell_type":"code","source":"# Natural Language Tool Kit - required for textual processing\nimport nltk\n# Used for Stemming purpose\nfrom nltk.stem import SnowballStemmer\n# Tfidf vectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Regular expression processing\nimport re","metadata":{"execution":{"iopub.status.busy":"2021-08-11T15:11:10.028329Z","iopub.execute_input":"2021-08-11T15:11:10.028722Z","iopub.status.idle":"2021-08-11T15:11:10.034243Z","shell.execute_reply.started":"2021-08-11T15:11:10.028682Z","shell.execute_reply":"2021-08-11T15:11:10.032586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenize first plot i.e. data.plot[0]\n\nsentences = [sent for sent in nltk.sent_tokenize(data['plot'][0])]\nprint(len(sentences))\nprint(sentences[:5])","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:28:31.926077Z","iopub.execute_input":"2021-08-11T14:28:31.926566Z","iopub.status.idle":"2021-08-11T14:28:31.947384Z","shell.execute_reply.started":"2021-08-11T14:28:31.92653Z","shell.execute_reply":"2021-08-11T14:28:31.946192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenize first sentence into words\nwords = [word for word in nltk.word_tokenize(sentences[0])]\nprint(len(words))\nprint(words)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:29:36.900661Z","iopub.execute_input":"2021-08-11T14:29:36.901111Z","iopub.status.idle":"2021-08-11T14:29:36.909094Z","shell.execute_reply.started":"2021-08-11T14:29:36.901072Z","shell.execute_reply":"2021-08-11T14:29:36.907177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can observe from above example, that we have words with punctuations or even numbers which doesn't make sense in our current analysis of similarity, we can filter these words using Regular expressions.","metadata":{}},{"cell_type":"code","source":"words_filtered = [word for word in words if re.search(\"[a-zA-Z]\", word)]\nprint(len(words_filtered))\nprint(words_filtered)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:34:21.786533Z","iopub.execute_input":"2021-08-11T14:34:21.787098Z","iopub.status.idle":"2021-08-11T14:34:21.794188Z","shell.execute_reply.started":"2021-08-11T14:34:21.787064Z","shell.execute_reply":"2021-08-11T14:34:21.792569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Stemming\n\n_It is the process of reducing infected words to their word stem. This is required to reduce complexity while processing different words._\n\nConsider an example: \"This is a sentence from a collection of sentences.\". Here `sentence` and `sentences` are word of same stem but in different form. In our textual processing, most of the times, we just need to work with the stem word and not different forms of same stem. Therefore, using Stemming process, we can bring `sentence` and `sentences` to stem word i.e. `sentence`.\n\nFor Stemming, `nltk` provides different Stemmers, but we're going to use `SnowballStemmer`.\n\n","metadata":{}},{"cell_type":"code","source":"stemmer = SnowballStemmer('english')\n\ntokens = [stemmer.stem(token) for token in words_filtered]\n\nprint(len(tokens))\nprint(words_filtered)\nprint(tokens)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:44:27.901936Z","iopub.execute_input":"2021-08-11T14:44:27.902304Z","iopub.status.idle":"2021-08-11T14:44:27.9096Z","shell.execute_reply.started":"2021-08-11T14:44:27.902275Z","shell.execute_reply":"2021-08-11T14:44:27.908491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can observe that, words are converted to base form, like, `only` => `onli`, `wedding` => `wed`, etc.\n\n_Note: `Stemming` can convert to stem words which might not be present in the dictionary and it might not make sense, (Example: `Only` => `onli`). If we need sensical stem word, then there's a process called `Lemmatization` which takes care of dictionary words, but it is slower compared to `Stemming`_","metadata":{}},{"cell_type":"code","source":"a = \"This is a sentence. This is another sentence.\"\ns = nltk.sent_tokenize(a)\nprint(type(s))\nprint(s)\nt = [nltk.word_tokenize(s_) for s_ in s]\nprint(type(t))\nprint(t)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T14:54:25.579341Z","iopub.execute_input":"2021-08-11T14:54:25.579741Z","iopub.status.idle":"2021-08-11T14:54:25.588542Z","shell.execute_reply.started":"2021-08-11T14:54:25.579709Z","shell.execute_reply":"2021-08-11T14:54:25.587155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a function which takes text and apply \"Tokenization\" and \"Stemming\" and returns processed text.\ndef tokenize_and_stem(text):\n    # Tokenize text into sentences\n    sentences = nltk.sent_tokenize(text)\n    \n    # Create an empty list to contain all tokens (words)\n    tokens = set()\n    for sent in sentences:\n        # Tokenize sentence into words provided each word contains atleast one alphabetical character\n        words = {stemmer.stem(word) for word in nltk.word_tokenize(sent) if re.search('[a-zA-Z]', word)}\n        tokens = tokens.union(words)\n    \n    return tokens","metadata":{"execution":{"iopub.status.busy":"2021-08-11T15:00:28.221232Z","iopub.execute_input":"2021-08-11T15:00:28.22164Z","iopub.status.idle":"2021-08-11T15:00:28.228359Z","shell.execute_reply.started":"2021-08-11T15:00:28.221607Z","shell.execute_reply":"2021-08-11T15:00:28.22657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test above function\nres = tokenize_and_stem(\"Today (May 19, 2016) is his only daughter's wedding.\")\nprint(res)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T15:00:29.324664Z","iopub.execute_input":"2021-08-11T15:00:29.325351Z","iopub.status.idle":"2021-08-11T15:00:29.33101Z","shell.execute_reply.started":"2021-08-11T15:00:29.325311Z","shell.execute_reply":"2021-08-11T15:00:29.330239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Convert tokens into numerical vectors\n\nTo apply any Machine Learning algorithm, we are required to convert text tokens into numerical vectors. We're going to use `TfIdfVectorizer` i.e. `Term Frequency Inverse Document Frequency Vectorizer`. This converts each token into a number within the context of whole document.\n\n$$\n\\text{Term frequency} = \\frac{\\text{No. of repititions of word in the sentence}}{\\text{No. of words in the sentence}}\n$$\n\n$$\n\\text{Inverse Document frequency} = log(\\frac{\\text{No. of sentences}}{\\text{No. of sentences containing the word}})\n$$\n\n$$\n\\text{Final number} = \\text{T.F.} \\times \\text{I.D.F.}\n$$","metadata":{}},{"cell_type":"code","source":"help(TfidfVectorizer)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T15:12:25.545774Z","iopub.execute_input":"2021-08-11T15:12:25.546335Z","iopub.status.idle":"2021-08-11T15:12:25.55986Z","shell.execute_reply.started":"2021-08-11T15:12:25.5463Z","shell.execute_reply":"2021-08-11T15:12:25.558263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate TfidfVectorizer with stopwords, tokenizer for efficient processing of text\ntfidf_vect = TfidfVectorizer(tokenizer=tokenize_and_stem, \n                             stop_words='english', \n                             ngram_range=(1, 3), \n                             max_df=0.8, \n                             min_df=0.2, \n                             max_features=200000, \n                             use_idf=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T15:15:10.79454Z","iopub.execute_input":"2021-08-11T15:15:10.794931Z","iopub.status.idle":"2021-08-11T15:15:10.800845Z","shell.execute_reply.started":"2021-08-11T15:15:10.7949Z","shell.execute_reply":"2021-08-11T15:15:10.799487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`stopwords` are those words in a given text which do not contribute considerably towards the meaning of the sentence and are generally grammatical filler words. For example, in the sentence _'Dorothy Gale lives with her dog Toto on the farm of her Aunt Em and Uncle Henry'_, we could drop the words 'her' and 'the', and still have a similar overall meaning to the sentence. Thus, 'her' and 'the' are stopwords and can be conveniently dropped from the sentence. On setting the stopwords to 'english', we direct the vectorizer to drop all stopwords from a pre-defined list of English language stopwords present in the nltk module.\n\n`ngram_range` defines the length of the ngrams to be formed while vectorizing the text.","metadata":{}},{"cell_type":"markdown","source":"Once we create a TF-IDF Vectorizer object, we must fit the text to it and then transform the text to produce the corresponding numeric form of the data which the computer will be able to understand and derive meaning from. To do this, we use the `fit_transform()` method of the `TfidfVectorizer` object.","metadata":{}},{"cell_type":"code","source":"tfidf_mat = tfidf_vect.fit_transform(data['plot'])\n\nprint(tfidf_mat.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T15:19:36.868157Z","iopub.execute_input":"2021-08-11T15:19:36.868576Z","iopub.status.idle":"2021-08-11T15:19:48.586571Z","shell.execute_reply.started":"2021-08-11T15:19:36.868538Z","shell.execute_reply":"2021-08-11T15:19:48.585068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tfidf_mat[0])","metadata":{"execution":{"iopub.status.busy":"2021-08-11T15:21:20.170513Z","iopub.execute_input":"2021-08-11T15:21:20.171085Z","iopub.status.idle":"2021-08-11T15:21:20.17849Z","shell.execute_reply.started":"2021-08-11T15:21:20.171034Z","shell.execute_reply":"2021-08-11T15:21:20.177619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, as we have converted our text (plot summaries) into numerical vectors, we can apply Clustering algorithm to cluster similar items together. \n\n`Clustering` is the method of grouping together a number of items such that they exhibit similar properties. According to the measure of similarity desired, a given sample of items can have one or more clusters.\n\nWe're going to use `KMeans` clustering. `KMeans` is the algorithm in which the given sample is divided into `K` clusters where each cluster is denoted by the mean of all the items lying in that cluster.","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom scipy.cluster.hierarchy import linkage, dendrogram","metadata":{"execution":{"iopub.status.busy":"2021-08-11T15:44:28.296991Z","iopub.execute_input":"2021-08-11T15:44:28.297694Z","iopub.status.idle":"2021-08-11T15:44:28.332172Z","shell.execute_reply.started":"2021-08-11T15:44:28.297651Z","shell.execute_reply":"2021-08-11T15:44:28.331059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# KMeans object\nkm = KMeans(n_clusters = 5)\n\n# Fit the KMeans object with the tfidf_mat\nkm.fit(tfidf_mat)\n\nclusters = km.labels_.tolist()\n\n# Create a cluster column in the dataframe\ndata['cluster'] = clusters\n\ndata['cluster'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T15:29:22.005255Z","iopub.execute_input":"2021-08-11T15:29:22.005649Z","iopub.status.idle":"2021-08-11T15:29:22.121201Z","shell.execute_reply.started":"2021-08-11T15:29:22.005617Z","shell.execute_reply":"2021-08-11T15:29:22.120211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display 5 movie titles for each cluster\ndata_clustered = data.groupby('cluster')\n\nfor c, data_ in data_clustered:\n    print('Cluster:',c)\n    print(data_['title'][:5])\n    print('-'*10)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T15:34:48.628812Z","iopub.execute_input":"2021-08-11T15:34:48.629225Z","iopub.status.idle":"2021-08-11T15:34:48.645051Z","shell.execute_reply.started":"2021-08-11T15:34:48.629188Z","shell.execute_reply":"2021-08-11T15:34:48.64411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate cosing similarity distance between the tfidf matrix\nsimilarity_distance = 1 - cosine_similarity(tfidf_mat)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T15:37:06.705925Z","iopub.execute_input":"2021-08-11T15:37:06.706352Z","iopub.status.idle":"2021-08-11T15:37:06.71907Z","shell.execute_reply.started":"2021-08-11T15:37:06.706319Z","shell.execute_reply":"2021-08-11T15:37:06.716977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This gives distance between each vector therefore 100x100 matrix\nsimilarity_distance.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-11T15:37:24.557831Z","iopub.execute_input":"2021-08-11T15:37:24.558373Z","iopub.status.idle":"2021-08-11T15:37:24.567397Z","shell.execute_reply.started":"2021-08-11T15:37:24.558328Z","shell.execute_reply":"2021-08-11T15:37:24.566087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can visualize the similar items in the form of `dendrogram`. Dendrograms help visualize the results of hierarchical clustering, which is an alternative to k-means clustering. Two pairs of movies at the same level of hierarchical clustering are expected to have similar strength of similarity between the corresponding pairs of movies.","metadata":{}},{"cell_type":"code","source":"# Create mergings matrix\nmergings = linkage(similarity_distance, method='complete')\n\n# Plot the dendrogram, using title as label\ndendrogram_ = dendrogram(mergings,\n                        labels = [x for x in data['title']],\n                        leaf_rotation = 90,\n                        leaf_font_size = 16,)\n\n# Adjust the plot\nfig = plt.gcf()\n_ = [lbl.set_color('r') for lbl in plt.gca().get_xmajorticklabels()]\nfig.set_size_inches(108, 21)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T15:50:12.389732Z","iopub.execute_input":"2021-08-11T15:50:12.390338Z","iopub.status.idle":"2021-08-11T15:50:17.457018Z","shell.execute_reply.started":"2021-08-11T15:50:12.390285Z","shell.execute_reply":"2021-08-11T15:50:17.456134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}