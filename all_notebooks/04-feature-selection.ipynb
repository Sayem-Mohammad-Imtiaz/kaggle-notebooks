{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Lab 5\n## Feature Selection\nEnda McCarthy - 19159269\n\nOctober 2019\n\nLab 5 builds on top of Lab 4 by introducing feature selection into the process of training and comparing predictive models (i.e. classification and numeric prediction models). \n\nIn general, the more features/attributes a dataset has (with a fixed number of examples), the more difficult it might be to train an accurate predictive model. For datasets with a large number of features, it is almost always necessary to select the most important features and train the model only on them.\n\nThe goal of Lab 5 is to understand how to evaluate a model trained with a subset of features without overestimating its accuracy. It also introduces scikit-learn model-training pipelines and implements feature-selection methods within such pipelines."},{"metadata":{},"cell_type":"markdown","source":" "},{"metadata":{},"cell_type":"markdown","source":"## 1 - Preparation \n\n### a) Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statistics\n\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler, MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest, f_regression, SelectPercentile, RFE\n\nsns.set(style=\"darkgrid\")\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### b) Load and prepare dataset"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/winequality_red.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.countplot(df[\"quality\"], palette=\"muted\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this we can see the following:\n- there are 1599 instances in the dataset\n- all of the attributes are numeric\n- the `quality` attribute is a whole number score between 1-10\n- all of the instances have scores between 3-8, with the majority between 5-6\n- there are quite a few attributes here, we can use feature selection to find the most relevant ones for predicting `quality`\n\nCheck for any missing values:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.apply(lambda x: sum(x.isnull()), axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks good, no missing values.\n\nNext we can define our target attribute and set the rest to our predictors."},{"metadata":{"trusted":true},"cell_type":"code","source":"# target attribute\ntarget_attribute_name = 'quality'\ntarget = df[target_attribute_name]\n\n# predictor attributes\npredictors = df.drop(target_attribute_name, axis=1).values\n\n# predictor attributes names\npredictors_col_names = list(df.drop(target_attribute_name, axis=1).columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We then need to convert the `quality` attribute from numerical to categorical. We can do this using the following function."},{"metadata":{"trusted":true},"cell_type":"code","source":"labelencoder = LabelEncoder()\ntarget = labelencoder.fit_transform(target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now our target has will have 6 categories for each score between 3-8 (there are no scores outside this range).\n\n__Note:__ If we wanted, we could abstract our quality scores into fewer categories (good, okay, poor) to make the final predictions a bit easier.\nThe following code would replace the two code blocks above this:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#        quality = df[\"quality\"].values\n#        category = []\n#        for number in quality:\n#            if number > 6:\n#                category.append(\"Good\")\n#            elif number > 3:\n#                category.append(\"Okay\")\n#            else:\n#                category.append(\"Poor\")\n#        category = pd.DataFrame(data=category, columns=[\"category\"])\n#        data = pd.concat([df, category], axis=1)\n#        data.drop(columns=\"quality\", axis=1, inplace=True)\n#\n#        # target attribute\n#        target_attribute_name = 'category'\n#        target = data[target_attribute_name]\n#\n#        # predictor attributes\n#        predictors = data.drop(target_attribute_name, axis=1).values\n#\n#        labelencoder = LabelEncoder()\n#        target = labelencoder.fit_transform(target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we split the data set into training (80%) and test (20%) datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare independent stratified data sets for training and test of the final model\npredictors_train, predictors_test, target_train, target_test = train_test_split(predictors, \n                                                                                target, \n                                                                                test_size=0.20, \n                                                                                shuffle=True, \n                                                                                stratify=target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scale all predictor values to the range [0, 1]. \n\nThe target attribute is now treated as a categorical attribute so we do not need to scale it.\n\nNote that the MinMaxScaler is applied separately to the training and the testing datasets. \nThis is to ensure that this transformation when performed on the testing dataset is not influenced by the training dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"min_max_scaler = MinMaxScaler()\npredictors_train = min_max_scaler.fit_transform(predictors_train)\npredictors_test = min_max_scaler.fit_transform(predictors_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" "},{"metadata":{},"cell_type":"markdown","source":"## 2 - Feature Selection\n\n#### a) Apply RFE with SVM for selecting the best features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a base classifier used to evaluate a subset of attributes\nestimatorSVM = svm.SVR(kernel=\"linear\")\n# create the RFE model and select 3 relevant attributes\nselectorSVM = RFE(estimatorSVM, 3)\nselectorSVM = selectorSVM.fit(predictors_train, target_train)\n# summarize the selection of the attributes\nprint(selectorSVM.support_)\nprint(selectorSVM.ranking_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the relevant attributes are True and are ranked as 1, while the remaining attributes are all False and are ranked from 2 downwards.\n\n#### 2. Apply RFE with Logistic Regression for selecting the best features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a base classifier used to evaluate a subset of attributes\nestimatorLR = LogisticRegression(solver='lbfgs', multi_class='auto')\n# create the RFE model and select 3 relevant attributes\nselectorLR = RFE(estimatorLR, 3)\nselectorLR = selectorLR.fit(predictors_train, target_train)\n# summarize the selection of the attributes\nprint(selectorLR.support_)\nprint(selectorLR.ranking_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" "},{"metadata":{},"cell_type":"markdown","source":"## 3 - Evaluate on the Test Dataset\n\nApply the selectors to prepare training data sets with only the selected features.\n\n__Note:__ The same selectors are applied to the test data set. However, it is important that the test data set was not used by (it's invisible to) the selectors."},{"metadata":{"trusted":true},"cell_type":"code","source":"# select only relevant features from training and test seperately - SVM\npredictors_train_SVMselected = selectorSVM.transform(predictors_train)\npredictors_test_SVMselected = selectorSVM.transform(predictors_test)\n\n# select only relevant features from training and test seperately - LR\npredictors_train_LRselected = selectorLR.transform(predictors_train)\npredictors_test_LRselected = selectorLR.transform(predictors_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Train and evaluate SVM classifiers with both the selected features and all features \n\nHere we train three models:\n* model1 - with the features selected by SVM\n* model2 - with the features selected by Logistic Regression\n* model3 - with all features (i.e. without feature selection)\n\nWe use an SVM classifier for all 3 models. \n\nBasically, we are using an SVM classifier to train a dataset with only the relevant features (according to each RFE model)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create SVM classifier\nclassifier = svm.SVC(gamma='scale')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run final classifier with only features selected using RFE with SVM\nmodel1 = classifier.fit(predictors_train_SVMselected, target_train)\naccuracy1 = model1.score(predictors_test_SVMselected, target_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run final classifier with only features selected using RFE with LR\nmodel2 = classifier.fit(predictors_train_LRselected, target_train)\naccuracy2 = model2.score(predictors_test_LRselected, target_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run final classifier with all the features\nmodel3 = classifier.fit(predictors_train, target_train)\naccuracy3 = model3.score(predictors_test, target_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Model 1 Accuracy = %.4f\" % (accuracy1))\nprint(\"Model 2 Accuracy = %.4f\" % (accuracy2))\nprint(\"Model 3 Accuracy = %.4f\" % (accuracy3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion\n\nThe results above do not vary much and it is likely that each time we run it we will have a different model as the most accurate.\n\nTo get more accurate results, accounting for the variance in the results, it is better to run the whole experiment multiple times and measure the __variance__ in the results. Then pick the model that gives the best results."},{"metadata":{},"cell_type":"markdown","source":" "},{"metadata":{},"cell_type":"markdown","source":"## 4 - Iteration\n\nWe will use iteration to repeat the experiment multiple times, each time with a different percentage of random test data selected from the dataset. The splits we will use are:\n- 15%\n- 20%\n- 25%"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create base classifiers\nestimatorSVM = svm.SVR(kernel=\"linear\")\nestimatorLR = LogisticRegression(solver='lbfgs', multi_class='auto')\n\n# create RFE model for both classifiers to find 3 best features\nselectorSVM = RFE(estimatorSVM, 3)\nselectorLR = RFE(estimatorLR, 3)\n\n# create SVM classifier for final evaluation\nclassifier = svm.SVC(gamma='scale')\n\n# store the results from loop in a dataframe \nresults_df = pd.DataFrame(columns=('score', 'split', 'model'))\n\n# create list of differant % test splits (15%, 20% and 25%)\ntest_sizes = [0.15, 0.20, 0.25]\n\n# list of model numbers\nmodel = [1, 2, 3]\n\n# counter\nrow = 0\n\nfor i in range(len(test_sizes)):\n    for j in range(20):\n        predictors_train, predictors_test, target_train, target_test = train_test_split(predictors, \n                                                                                        target, \n                                                                                        test_size=test_sizes[i], \n                                                                                        shuffle=True, \n                                                                                        stratify=target)\n\n        # scale predictors\n        predictors_train = min_max_scaler.fit_transform(predictors_train)\n        predictors_test = min_max_scaler.fit_transform(predictors_test)\n\n        # use RFE models on data to identify best features\n        selectorSVM = selectorSVM.fit(predictors_train, target_train)\n        selectorLR = selectorLR.fit(predictors_train, target_train)\n\n        # select only relevant features from training and test seperately - SVM\n        predictors_train_SVMselected = selectorSVM.transform(predictors_train)\n        predictors_test_SVMselected = selectorSVM.transform(predictors_test)\n\n        # select only relevant features from training and test seperately - LR\n        predictors_train_LRselected = selectorLR.transform(predictors_train)\n        predictors_test_LRselected = selectorLR.transform(predictors_test)\n\n        # run final classifier with only with features selected using RFE with SVM\n        model1 = classifier.fit(predictors_train_SVMselected, target_train)\n        model1_score = model1.score(predictors_test_SVMselected, target_test)\n        results_df.loc[row] = [model1_score, (test_sizes[i]*100), model[0]]\n        row+=1\n\n        # run final classifier with only with features selected using RFE with LR\n        model2 = classifier.fit(predictors_train_LRselected, target_train)\n        model2_score = model2.score(predictors_test_LRselected, target_test)\n        results_df.loc[row] = [model2_score, (test_sizes[i]*100), model[1]]\n        row+=1\n\n        # run final classifier with all features\n        model3 = classifier.fit(predictors_train, target_train)\n        model3_score = model3.score(predictors_test, target_test)\n        results_df.loc[row] = [model3_score, (test_sizes[i]*100), model[2]]\n        row+=1\n            ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can boxplot the 3 models for each test split and compare them."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14, 8))\nsns.boxplot(x=\"split\", y=\"score\", hue=\"model\", data=results_df, width = .4, palette=\"Set3\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also look at the variance of each example."},{"metadata":{"trusted":true},"cell_type":"code","source":"variance_df = pd.DataFrame(index=[0], columns=('0.15 split, model 1', '0.15 split, model 2', '0.15 split, model 3',\n                                               '0.20 split, model 1', '0.20 split, model 2', '0.20 split, model 3',\n                                               '0.25 split, model 1', '0.25 split, model 2', '0.25 split, model 3'))\n\nfor i in test_sizes:\n    for j in model:\n        variance = np.var(list(results_df.score[(results_df['model'] == j) & (results_df['split'] == (i*100))]))\n        variance_df.at[0, '%.2f split, model %d' % (i,j)] = variance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14, 8))\nax = sns.barplot(data=variance_df, palette=\"Set3\")\nfor item in ax.get_xticklabels():\n    item.set_rotation(60)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From examining the above information we can conclude that:\n- all 9 models have similar median accuracies\n- model 1 with a split of 25% test data has the lowest variance\n- model 3 with a split of 20% test data has the highest variance\n- __model 1 (features selected using SVM) with a split of 25%__ is the best performing\n\nHowever, even though we ran this through a loop multiple times, if we were to run it again we may have different conclusions. \n\nThis means that there is very little difference in using the best 3 attributes (selected using either SVM or Logistic Regression) or by using all the attributes to train a classification model for this dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy2_20 = statistics.mean(list(results_df.score[(results_df['model'] == 1) & (results_df['split'] == (25))]))\nprint(\"Model 2 with 0.20 Split Accuracy = %.4f\" % (accuracy2_20))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. This performs the same as earlier on where we had a split of 20%."},{"metadata":{},"cell_type":"markdown","source":"## 5 - Pipelines\n\nWe can use pipelines to extract the best combination of features to train our model. This will help us to avoid overfitting.\n\nWe will set up the data from the beginning."},{"metadata":{"trusted":true},"cell_type":"code","source":"# target attribute\ntarget_attribute_name = 'quality'\ntarget = df[target_attribute_name]\nlabelencoder = LabelEncoder()\ntarget = labelencoder.fit_transform(target)\n\n# predictor attributes\npredictors = df.drop(target_attribute_name, axis=1).values\n\n# predictor attributes names\npredictors_col_names = list(df.drop(target_attribute_name, axis=1).columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare independent stratified data sets for training and test of the final model\nX_train, X_test, y_train, y_test = train_test_split(predictors, \n                                                    target, \n                                                    test_size=0.25, \n                                                    shuffle=True, \n                                                    stratify=target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set up pipeline\n    # step 1 - scale the data\n    # step 2 - select the best features to use (reduce dimensionality)\n    # step 3 - use a learning algorithm on the selected features\n# note: we will add more options to each step in the next block)\npipe = Pipeline([('scaler', MinMaxScaler()),\n                 ('reduce_dim', SelectPercentile(f_regression)),\n                 ('regressor', svm.SVC(gamma='scale'))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# as stated above we can add more options for each step\n# first we add options for scaling\nscalers_to_test = [StandardScaler(), RobustScaler(), MinMaxScaler()]\n# next we add options for learning algorithms\nregressors_to_test = [svm.SVC(gamma='scale'), LogisticRegression(solver='lbfgs', multi_class='auto')]\n# then we can vary the number of selected features from 1-11 for each variation\nn_features_to_test = np.arange(1, 12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# params will be passed in alongside our pipeline\n# we have two sets of params here, each with a different method of selectinf features:\n    # first we use the SelectPercentile method (where percentile is the number of attributes)\n    # then we use the SelectKBest method (where k is the number of attributes)\nparams = [{'scaler': scalers_to_test,\n           'reduce_dim': [SelectPercentile(f_regression)],\n           'reduce_dim__percentile': n_features_to_test,\n           'regressor': regressors_to_test},\n\n          {'scaler': scalers_to_test,\n           'reduce_dim': [SelectKBest(f_regression)],\n           'reduce_dim__k': n_features_to_test,\n           'regressor': regressors_to_test}]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# then we can train our final model using the pipeline and params (cross-validation is used)\ngridsearch = GridSearchCV(pipe, params, cv=5, verbose=1, iid=False).fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can see which params were chosen as the best performing\ngridsearch.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can see which features were selected from the pipeline\nfinal_pipeline = gridsearch.best_estimator_\nfinal_classifier = final_pipeline.named_steps['regressor']\nmask = final_pipeline.named_steps['reduce_dim'].get_support()\nfeature_names = df.drop(target_attribute_name, axis=1).columns\nselected_features = feature_names[final_pipeline.named_steps['reduce_dim'].get_support()].tolist()\nselected_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# and finally we can see the accuracy of the final model\nprint('Final score is: ', gridsearch.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the following from these results:\n- best performing scaler is StandardScaler\n- best performing feature selector is SelectKBest\n- best performing learning algorithm is SVC\n- ideal number of features for learning is 11\n\nThis gives us a total accuracy of 58% for our model."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}