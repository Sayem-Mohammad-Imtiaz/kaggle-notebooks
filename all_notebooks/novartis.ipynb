{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Importing the required libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\nimport shap\nimport xgboost","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing the required train and test dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train=pd.read_csv(\"../input/novartis-data/Train.csv\")\ndf_test=pd.read_csv(\"../input/novartis-data/Test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Looking for missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature X_12 has missing 182 values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature X_12 is also missing in test dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,18))\ndf_train.iloc[:,2:-1].boxplot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Looking at all the features it is evident that there is a high presence of outliers in all the features , incident ID and Date has been removed from the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df_train.iloc[:,2:-1].corr()\ncorr.style.background_gradient(cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Looking at the plot it is evident that X_2 and X_3 are highly correlated, and X_12 and X_10 are highly correlated.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import KNNImputer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using KNN imputer to impute missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"k=int(round(len(df_train)**0.5,0))\nif k%2==0:\n    k=k+1\nk","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Identifying square root of the number of observations and making it odd","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"imputer = KNNImputer(n_neighbors=k)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df_train.iloc[:,2:-1].values\nY=df_train.iloc[:,-1].values\nx_test=df_test.iloc[:,2:].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Using selected features for classification.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X=imputer.fit_transform(X)\nx_test=imputer.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Imputing the dataset with KNN impute for test as well as train\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Importing Standard Scaler for pre processing of Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sc=StandardScaler()\nX=sc.fit_transform(X)\nx_test=sc.transform(x_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### Scaling all the features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"np.corrcoef(X[:,9],X[:,11])\nnp.corrcoef(X[:,1],X[:,2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Since the correlation is high we will drop the features X_3 and X_12","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_reformed=np.delete(X,(2,11),axis=1)\nx_test_reformed=np.delete(x_test,(2,11),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Trying to reduce the number of variables using clustering if we can find clusters and then we can classify","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"iner=[]\ncount=[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1,8):\n    kmeans=KMeans(n_clusters=i)\n    kmeans.fit(X_reformed)\n    inertia=kmeans.inertia_\n    count.append(i)\n    iner.append(inertia)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Looking for ideal number of clusters,since the ideal number of clusters cannot be determined that is why we sack this idea.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"count=np.array(count)\niner=np.array(iner)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(count,iner)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nexplained_variance=[]\ncount1=[]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Trying to reduce the number of features using a scree plot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pca=PCA(n_components=10)\npca.fit(X_reformed)\nexplained_variance1=pca.explained_variance_ratio_\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(explained_variance1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Since the number of components cannot be determined using a scree plot, we have to move to classification algorithms","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lets look at the balance of the training set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"p=sum(Y)/len(Y)\nprint(p)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Since it is highly imbalanced we have to resample the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler\nros = RandomOverSampler(random_state=0)\nX_resampled, Y_resampled = ros.fit_resample(X_reformed, Y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Checking the balance again","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"p=sum(Y_resampled)/len(Y_resampled)\nprint(p)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Now the dataset has been balanced","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.fit(X_resampled,Y_resampled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=lr.predict(X_resampled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, auc, confusion_matrix,f1_score, roc_curve, roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(Y_resampled,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Creating the confusion matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\nscores = cross_val_score(lr,X_resampled, Y_resampled, cv=10, scoring='recall')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### The recall is 87% which can be improved.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = metrics.roc_curve(Y_resampled, y)\nroc_auc = metrics.auc(fpr, tpr)\ndisplay = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,estimator_name='logistic')\ndisplay.plot()  \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The AUC curve shows 88% coverage.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Going for Randomized Search to find optimum paprameters for Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"param= {\n    'bootstrap': [True],\n    'max_depth': [3,4,5,6,7,8],\n    'max_features': [5,6,7,8,9,10],\n    'min_samples_leaf': [5,6,7,8,9,10],\n    'min_samples_split': [20,25,50],\n    'n_estimators': [500,1000],\n    'criterion':[\"gini\",\"entropy\"]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random=RandomizedSearchCV(estimator=RandomForestClassifier(),param_distributions=param,n_iter=10,cv=3,n_jobs=-1)\nrandom.fit(X_resampled,Y_resampled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search=random.fit(X_resampled,Y_resampled)\nsearch.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### It is clear that n_estimators should be greater that 500 trees","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y=random.predict(X_resampled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = cross_val_score(random,X_resampled, Y_resampled, cv=10, scoring='recall',n_jobs=-1)\nscores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Cross Validation Score is 97% which can be further improved","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = metrics.roc_curve(Y_resampled, y)\nroc_auc = metrics.auc(fpr, tpr)\ndisplay = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,estimator_name='randomforest')\ndisplay.plot()  \nplt.show()  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n#### Using Ada boost Classifier since we know that n_estimators should be greater than 500, therefore using 1000 trees","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ada=AdaBoostClassifier(n_estimators=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ada.fit(X_resampled,Y_resampled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = cross_val_score(ada,X_resampled, Y_resampled, cv=10, scoring='recall',n_jobs=-1)\nscores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Best recall till now using Adaboost Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y=ada.predict(X_resampled)\nfpr, tpr, thresholds = metrics.roc_curve(Y_resampled, y)\nroc_auc = metrics.auc(fpr, tpr)\ndisplay = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,estimator_name='Adaboost')\ndisplay.plot()  \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Using Gradient Boosting algorithm to check if it is greater than ada boost.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gbc=GradientBoostingClassifier(n_estimators=1000)\ngbc.fit(X_resampled,Y_resampled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = cross_val_score(gbc,X_resampled, Y_resampled, cv=10, scoring='recall',n_jobs=-1)\nscores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Since the average score of Adaboost Classifier is greater than Gradient Boost Classifier we will go for Adaboost Classifier.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y=ada.predict(x_test_reformed)\nxcv={\"INCIDENT_ID\":df_test.iloc[:,0],\"MULTIPLE_OFFENSE\":y}\nsample=pd.DataFrame(xcv)\nsample.to_csv(\"Sample.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using adaboost to predict test case","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}