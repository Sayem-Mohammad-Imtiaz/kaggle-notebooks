{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n# Load the transfer learning tweet dataset\ndf = pd.read_csv('../input/twitterdata/finalSentimentdata2.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking if there any NAN value or not\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# droping the unnecesary colmns \ndf.drop(['Unnamed: 0'], axis='columns', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['sentiment'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(10,5))\nsns.countplot(df['sentiment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.preprocessing import LabelEncoder\nscaler=LabelEncoder()\ndf['sentiment']=scaler.fit_transform(df['sentiment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#1 for fear\n#3 for sad\n#0 for anger\n#2 for joy\ndf['sentiment'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'][:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#making the copy of original datasets\nmessage=df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"message.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"message.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's take a single paragraph for example message['text'][0] and check how the cleaning is going on after that it will be done in whole datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"message['text'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"para=''''agree the poor in india are treated badly their poors \nseek a living in singapore and are treated like citizens they \nare given free medical treatment given food daily sim cards\nto call home to tell their family that they are fine if covid \n19 case treated foc in hospitals'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# stemming and cleaning the text"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nimport re\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\ncorpus = []\n\nreview=re.sub(r'http\\S+',' ',para) #removing all the link releted text\nreview = re.sub('[^a-zA-Z]', ' ', review)# removing all the element except a-z and A-Z\nreview = review.lower()#lowering the text\nreview = review.split()\n\n#removing all the stopwords and then stemming the text \nreview = [ps.stem(word) for word in review if not word in stopwords.words('english')]\nreview = ' '.join(review)\ncorpus.append(review)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Its seems that 80% cleaning is done by stemming now let's check it my lemmatizing"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nlem=WordNetLemmatizer()\n\ncorpus_lem=[]\nreview=re.sub(r'http\\S+',' ',para)#removing all the link releted text\nreview = re.sub('[^a-zA-Z]', ' ', review)# removing all the element except a-z and A-Z\nreview = review.lower()#lowering the text\nreview = review.split()\n\n#removing all the stopwords and then lemmatizing the text \nreview=[lem.lemmatize(word) for word in review if word not in set(stopwords.words('english'))]\nreview=' '.join(review)\ncorpus_lem.append(review)\n    \ncorpus_lem","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now lets apply this in whole datasets for cleaning the text."},{"metadata":{"trusted":true},"cell_type":"code","source":"#stemming and cleaning\ncorpus_stem = []\nfor i in range(0, len(message)):\n    review=re.sub(r'http\\S+',' ',message['text'][i])\n    review = re.sub('[^a-zA-Z]', ' ', review)\n    review = review.lower()\n    review = review.split()\n    \n    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n    review = ' '.join(review)\n    corpus_stem.append(review)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(message)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2=pd.DataFrame(df['sentiment'],index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2['stemming_text']=corpus_stem","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.head(4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Most commonly used Anger words "},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom wordcloud import WordCloud\nanger_text = df2[df2['sentiment'] == 0]\nall_words = ' '.join([text for text in anger_text.stemming_text])\nwordcloud = WordCloud(width= 1000, height= 800,\n                          max_font_size = 120,\n                          collocations = False).generate(all_words)\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title(\"Most Used anger words\", fontsize=20)\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Most commonly used Fear words"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom wordcloud import WordCloud\nfear_text = df2[df2['sentiment'] == 1]\nall_words = ' '.join([text for text in fear_text.stemming_text])\nwordcloud = WordCloud(width= 1000, height= 800,\n                          max_font_size = 120,\n                          collocations = False).generate(all_words)\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title(\"Most Used fear words\", fontsize=20)\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Most commonly used Joy words"},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\njoy_text = df2[df2['sentiment'] == 2]\nall_words = ' '.join([text for text in joy_text.stemming_text])\nwordcloud = WordCloud(width= 1000, height= 800,\n                          max_font_size = 120,\n                          collocations = False).generate(all_words)\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title(\"Most Used joy words\", fontsize=20)\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Most commonly used Sad words"},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nsad_text = df2[df2['sentiment'] == 3]\nall_words = ' '.join([text for text in sad_text.stemming_text])\nwordcloud = WordCloud(width= 1000, height= 800,\n                          max_font_size = 120,\n                          collocations = False).generate(all_words)\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title(\"Most Used sad words\", fontsize=20)\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import tokenize\ntoken_space = tokenize.WhitespaceTokenizer()\ndef counter(text, column_text, quantity):\n    all_words = ' '.join([text for text in text[column_text]])\n    token_phrase = token_space.tokenize(all_words)\n    frequency = nltk.FreqDist(token_phrase)\n    df_frequency = pd.DataFrame({\"Word\": list(frequency.keys()),\n                                   \"Frequency\": list(frequency.values())})\n    df_frequency = df_frequency.nlargest(columns = \"Frequency\", n = quantity)\n    plt.figure(figsize=(15,8))\n    ax = sns.barplot(data = df_frequency, x = \"Word\", y = \"Frequency\", color = 'yellow')\n    ax.set(ylabel = \"Count\")\n    plt.xticks(rotation='vertical')\n    plt.show()\n    \n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#frequency of most anger words\ncounter(df2[df2['sentiment'] == 0], 'stemming_text', 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#frequency of most fear words\ncounter(df2[df2['sentiment'] == 1], 'stemming_text', 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#frequency of most joy words\ncounter(df2[df2['sentiment'] == 2], 'stemming_text', 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#frequency of most sad words\ncounter(df2[df2['sentiment'] == 3], 'stemming_text', 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lammetizing and cleaning\n\ncorpus_lemmetize = []\nfor i in range(0, len(message)):\n    review=re.sub(r'http\\S+',' ',message['text'][i])\n    review = re.sub('[^a-zA-Z]', ' ', review)\n    review = review.lower()\n    review = review.split()\n    \n    review = [lem.lemmatize(word) for word in review if word not in set(stopwords.words('english'))]\n    review = ' '.join(review)\n    corpus_lemmetize.append(review)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus_lemmetize[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(corpus_lemmetize)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#countvectoriser with stemming\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nCV=CountVectorizer(max_features=5000, ngram_range=(1,3))\nx_stem=CV.fit_transform(corpus_stem).toarray()\nx_stem","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_stem.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#countvectorizer with lemmetizing\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nCV=CountVectorizer(max_features=5000)\nx_lem=CV.fit_transform(corpus_lemmetize).toarray()\nx_lem","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_lem.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TF-IDF for stemming\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntf_stem=TfidfVectorizer()\nx_tf_stem=tf_stem.fit_transform(corpus_stem)\nprint(x_tf_stem.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TF-IDF for lemmatizing\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntf_stem=TfidfVectorizer()\nx_tf_lem=tf_stem.fit_transform(corpus_lemmetize)\nprint(x_tf_lem.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=df['sentiment']\ny[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Making the model by using Countvectorizer and Stemming"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test= train_test_split(x_stem,y,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nmodel=MultinomialNB().fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ny_pred=model.predict(X_test)\ncm=confusion_matrix(y_test,y_pred)\ncm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.figure(figsize=(10,5))\nsns.heatmap(cm, annot=True, fmt='d')\nplt.xlabel('Predicted')\nplt.ylabel('True')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making the model by using Lemmetizing and Countvectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test= train_test_split(x_lem,y,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nmodel=MultinomialNB().fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ny_pred=model.predict(X_test)\ncm=confusion_matrix(y_test,y_pred)\ncm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.heatmap(cm, annot=True, fmt='d')\nplt.xlabel('Predicted')\nplt.ylabel('True')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making the model by using TF-IDF and stemming"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test= train_test_split(x_tf_stem,y,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nmodel=MultinomialNB().fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ny_pred=model.predict(X_test)\ncm=confusion_matrix(y_test,y_pred)\ncm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.heatmap(cm, annot=True, fmt='d')\nplt.xlabel('Predicted')\nplt.ylabel('True')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making the model by using TF-IDF and lemmetizing"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test= train_test_split(x_tf_lem,y,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_tf_lem.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nmodel=MultinomialNB().fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ny_pred=model.predict(X_test)\ncm=confusion_matrix(y_test,y_pred)\ncm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.heatmap(cm, annot=True, fmt='d')\nplt.xlabel('Predicted')\nplt.ylabel('True')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's check the accuracy after applying hyperparameter in MultinomialNB"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test= train_test_split(x_stem,y,test_size=0.2)\n\nclassifier=MultinomialNB(alpha=0.1)\n\nfrom sklearn import metrics\nimport numpy as np\nprevious_score=0\nfor alpha in np.arange(0,1,0.1):\n    sub_classifier=MultinomialNB(alpha=alpha)\n    sub_classifier.fit(X_train,y_train)\n    y_pred=sub_classifier.predict(X_test)\n    score = metrics.accuracy_score(y_test, y_pred)\n    if score>previous_score:\n        classifier=sub_classifier\n    print(\"Alpha: {}, Score : {}\".format(alpha,score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# when alpha=0.6 its gives the maximumn accuracy of 0.6747572815533981"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import  DecisionTreeClassifier\nfrom sklearn.svm import SVC\n\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.metrics import classification_report,confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now let's check with other classifier algorithm like DecisionTreeClassifier, RandamForestClassifier, SVM, LogisticRegression by using GridSearchCV and crossvalidation"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_params = {\n    'svm': {\n        'model': SVC(gamma='auto'),\n        'params' : {\n            'C': [1,10,20,25,30,40],\n            'kernel': ['rbf','linear']\n        }  \n    },\n    'random_forest': {\n        'model': RandomForestClassifier(),\n        'params' : {\n            'n_estimators': [1,5,10,15,20,25,30]\n        }\n    },\n    'logistic_regression' : {\n        'model': LogisticRegression(solver='liblinear'),\n        'params': {\n            'C': [1,5,10,15,20,25]\n        }\n    },\n    'decision_tree': {\n        'model': DecisionTreeClassifier(),\n        'params':{\n            'criterion':['gini','entropy']\n        }\n    }\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = []\n\nX_train, X_test, y_train, y_test= train_test_split(x_stem,y,test_size=0.2)\n\nfor model_name, mp in model_params.items():\n    clf =  GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n    clf.fit(X_train,y_train)\n    scores.append({\n        'model': model_name,\n        'best_score': clf.best_score_,\n        'best_params': clf.best_params_\n    })\n    \ndf_score = pd.DataFrame(scores,columns=['model','best_score','best_params'])\ndf_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hence, It can conclude that Logistic Regression with the accuracy of 0.69"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}