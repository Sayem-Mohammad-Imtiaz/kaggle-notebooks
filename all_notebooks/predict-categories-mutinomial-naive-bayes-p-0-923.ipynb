{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"dc5e1bcf-92a1-f918-b585-2100b64e4124"},"source":"# Predict document category using Mutinomial Naive Bayes\n\nConsider a vocabulary of N words which we label with numbers {1,...,N}. We choose to discard the order in which words appear in documents and consider documents as bags of words. Furthermore we assume a conditional independence of words in documents of a given category. This means that the probability of a word appearing in a document depends only on the category of the document, but not on the other words in the document. Being the \"naive\" assumption of the Naive Bayes algorithm, it has been observed to work very well in practice.\n\nWe choose to model word probabilities in each category by a discrete probability distribution. We assume that each word in a document is a result of independent random choice from a vocabulary according to category-specific probabilities {p_c(1),...,p_c(N)}.  The probabilities are the unknown parameters of the learning model. We will use the training data to estimate them.\n\nDue to the conditional independence assumption the likelihood of a document of a given category is computed as product of probabilities of each word in the document. Let (j1,...,jn) be a list of distinct words in a document and let (fj1,...,fjn) be a corresponding count of each word. Than a likelihood of a document is a probability of getting this outcome in sequence of independent draws from a discrete probability distribution. This probability is given by multinomial probability distribution. \n\nReferences: \n \nhttps://en.wikipedia.org/wiki/Multinomial_distribution\n  \nhttps://en.wikipedia.org/wiki/Naive_Bayes_classifier#Multinomial_naive_Bayes"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aa9f58e4-952e-9c3c-f14b-83a294afb0e0"},"outputs":[],"source":"%matplotlib notebook\nfrom matplotlib import pyplot as plt\n                                                               \nimport re                                       # standard Python REGEXP module\n\n\nfrom collections import defaultdict             # defaultdict is like a dict with a default\n                                                # value generated for missing keys\n\n\nimport numpy as np                              # NUMPY is a popular math package for Python\n\nimport csv                                      # standard Python package for comma-separated files,\n                                                # very useful for data import \n\nimport nltk                                     # NLTK is Natural Language Tool Kit for Python "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fd3aa9c4-e495-614e-4149-add3b22dbeb2"},"outputs":[],"source":"class MultinomialNaiveBayesClassifier(object):\n    \"\"\"Naive Bayes learning model for text classification\n    \"\"\"\n    \n    def __init__(self, nfeatures, nclasses, reg=1.0):\n        \"\"\"Constructor\n        \"\"\"\n        self.nclasses = nclasses \n        self.nfeatures = nfeatures\n        \n        self.fstat = np.zeros((nclasses, nfeatures))         # Table of word counts for each class.\n                                                             # We use data from this table to estimate a\n                                                             # probability of a word in a document given\n                                                             # the document class.\n        \n        self.cstat = np.zeros(nclasses)                      # Table of document counts by category.\n                                                             # We use data from this table to estimate prior\n                                                             # probability distribution of categories in general\n                                                             # population of documents\n        \n        self.reg = reg                                       # Regularization parameter defines a uniform apriory\n                                                             # probability distribution for words. \n        \n    def update(self, document, class_):\n        \"\"\"Update model by one example\n        \n        Input:\n            document - list of pairs [(word1, count1), (word2, count2), ...]\n            class_ - numerical label of document category\n        \"\"\"\n        \n        for (feature, count) in document:\n            self.fstat[class_, feature] += count             # Updates are extremely simple: just update\n            self.cstat[class_] += 1                          # the counts using words from a given document \n                                                             # and its category.\n    \n    def predict(self, document):\n        \"\"\"Compute probabilities of all categories for input document\n        \n        Input:\n            document - list of pairs [(word1, count1), (word2, count2), ...]\n        \"\"\"\n        \n        # Start with apriory probabilities of categories\n        score = np.log(self.cstat + 1)\n        \n        # Update by a regularized likelihood of each word \n        for (feature, count) in document:\n            score += count * np.log(self.fstat[:, feature] + self.reg) \\\n                   - count * np.log(np.sum(self.fstat, axis=1) + self.reg*self.nfeatures)\n        \n        # Transform from logarithms to actual probabilities:\n        prob = np.exp(score - np.min(score))                  # substract constant to avoid underflow!\n        \n        # Normalize probibilities to one\n        return prob / np.sum(prob)\n        \n    def estimates(self):\n        \"\"\"Get normalized estimates of class conditional probabilities\n        \"\"\"\n        norm = np.sum(self.fstat, axis=1) + self.reg*self.nfeatures\n        return (self.fstat + self.reg) / np.kron(norm, np.ones((self.nfeatures, 1))).transpose()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f1c1e794-14d8-d603-d22b-2b0033a0b593"},"outputs":[],"source":"def load(filename):\n    \"\"\"Load and preprocess dataset from file\n    \"\"\"\n    \n    stemmer = nltk.stem.PorterStemmer()                             # Stemmer instance\n    \n    pattern_remove_non_aplanumeric = re.compile('\\W+')              # Pattern to remove all non-alphanumeric symbols\n    \n    documents = []\n    categories = defaultdict(int)\n    words = defaultdict(int)\n    \n    with open(filename) as fid:\n        reader = csv.reader(fid)                                    # Initialize CSV reader object\n        \n        header = next(reader)                                  # Read file header line\n        \n        count = 0\n        for line in reader:                                                     # Read dataset line by line:\n            category = line[4]                                                  # * category is in 5th column\n            document = line[1].strip()                                          # * document line\n            document = re.sub(pattern_remove_non_aplanumeric, ' ', document)    # * remove non-alphanumeric symbols\n            document = [str.lower(w) for w in document.split()]                 # * split into words by whitespace\n            document = [stemmer.stem(w) for w in document]                      # * replace words by stems\n            document = [w for w in document if len(w) > 1]                      # * filter single character words\n            \n            categories[category] += 1                                           # compute categories count\n            for w in document:        \n                words[w] += 1                                                   # compute words count\n            \n            documents.append((category, document))                              # add document to output\n    \n    return categories, words, documents "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e1920c42-78bd-e2f6-5ae7-63f05c40d8c8"},"outputs":[],"source":"# Load data\ncategories, words, documents = load('../input/uci-news-aggregator.csv')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"26682954-e743-a7e4-1953-62e404b91ca3"},"outputs":[],"source":"# Examine statistics of the dataset\nprint('CATEGORIES: {:d}'.format(len(categories)))\nprint(' '.join('{}={}'.format(c, categories[c]) for c in sorted(categories, key=categories.get)))\nprint() \n\nprint('WORDS: {} \\n'.format(len(words)))\nthreshold = 50\nword_stats = {j: 0 for j in range(1, threshold + 1)}\nfor w in words:\n    count = words[w]\n    count = count if count < 50 else 50\n    word_stats[count] += 1\nprint('WORDS FREQUENCIES:') \nprint(' '.join('{}={}'.format(c, word_stats[c]) for c in range(1, threshold + 1)))\nprint()\n\ntop = 50\nprint ('TOP {} WORDS:'.format(top))\nprint (' '.join('{}={}'.format(w, words[w]) for w in sorted(words, key=words.get, reverse=True)[:top]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f1b98f40-dbde-5e8b-0a4b-bf1fe74fd9ba"},"outputs":[],"source":"# Collect classes\nclasses_ = dict()\nclass_id = 0\nfor c in sorted(categories, key=categories.get, reverse=True):\n    classes_[c] = class_id\n    class_id += 1\n\n# Collect vocabulary, filtering out rare words\nvocabulary = dict()\nmin_frequency = 2\nword_id = 0\n\nfor w in sorted(words, key=words.get, reverse=True): \n    if words[w] < min_frequency:\n        break\n\n    vocabulary[w] = word_id\n    word_id += 1\n\n# Encode documents as bag of words from vocabulary\nfor j, d in enumerate(documents):\n    class_ = classes_[d[0]]\n    document = [vocabulary[w] for w in d[1] if w in vocabulary]\n    unique, counts = np.unique(document, return_counts=True)\n    documents[j] = (class_, np.asarray((unique, counts)).T)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"be65d239-0fe8-b42a-4e5f-b185cb5d2452"},"outputs":[],"source":"# Shuffle data at random\nN = len(documents)\nrnd = np.random.RandomState(seed=1923)\nrnd.shuffle(documents)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"84415b65-dff9-60dd-c4ae-5294ccc397be"},"outputs":[],"source":"# Split dataset into train, validation and test sets\ntrain_set_idx = range(int(N * 0.8))\nprint ('Train: ', min(train_set_idx), max(train_set_idx))\n\nvalidation_set_idx = range(int(N * 0.8), int(N * 0.82))\nprint ('Validation: ', min(validation_set_idx), max(validation_set_idx))\n\ntest_set_idx = range(int(N * 0.82), N)\nprint ('Train: ', min(test_set_idx), max(test_set_idx))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ffa7338d-b535-a50e-43a1-797875ea8188"},"outputs":[],"source":"# Define test metrics\ndef metrics(documents_idx):\n    total = 0\n    correct = 0\n    entropy = 0\n    for k in documents_idx:\n        total += 1\n        (class_, document) = documents[k]\n        probabilities = classifier.predict(document)\n        prediction = np.argmax(probabilities)\n        entropy += - np.log(probabilities[class_])\n        if prediction == class_:\n            correct += 1\n\n    precision = float(correct) / total\n    entropy = entropy / total\n    \n    return precision, entropy"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d1fa113f-7ec1-36a0-a30d-76df6c94011b"},"outputs":[],"source":"# Train classifier\nclassifier = MultinomialNaiveBayesClassifier(nclasses=len(classes_), nfeatures=len(vocabulary))\nlearn_curves = []\n\nfor j in train_set_idx:\n    (class_, document) = documents[j]\n    classifier.update(document, class_)\n    \n    if j % 10000 == 0:\n        precision, entropy = metrics(validation_set_idx)\n        learn_curves.append((j, precision, entropy))\n        print ('iter={:d}, precision={:f}, entropy={:f}'.format(j, precision, entropy))\n        \nprint ('Training complete')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"62a4c9fe-9841-7ff4-d1a5-cf1bcf3fb19b"},"outputs":[],"source":"# Compute final score\nprint ('Test: precision={:f}, entropy={:f}'.format(*metrics(test_set_idx)))"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}