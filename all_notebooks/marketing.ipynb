{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"file=pd.read_csv(\"../input/direct-marketing/DirectMarketing.csv\",sep=',')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file.shape\nfile.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load the data and check for NA in the file.\nfile.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Column History has 303 NA's. Let us now check the values in that column\nfile['History'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file['History'].value_counts(normalize = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Filter all the rows with NA, to check if there are any anomalies or to identify the way to replace the NA.\nfile[file.isnull().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fill the NA in column \"History\" with the help of column 'Age'. Group the columns Age and History and identify the value \n#which occurs the most for each category in Age. Replace the NA in column 'History' based on the values obtained after \n#grouping the data\n\nfile['History'] = file.groupby(['Age'], sort=False)['History'].apply(lambda x: x.fillna(x.mode().iloc[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Univariate Analysis\nimport matplotlib.pyplot as plt\nplt.figure(1)\nplt.subplot(2,3,1)\nfile['Age'].value_counts(normalize=True).plot.bar(figsize=(20,10), title= 'Age') \nplt.subplot(2,3,2)\nfile['Gender'].value_counts(normalize=True).plot.bar(title= 'Gender') \nplt.subplot(2,3,3)\nfile['Married'].value_counts(normalize=True).plot.bar(title= 'Married')\nplt.subplot(2,3,4)\nfile['OwnHome'].value_counts(normalize=True).plot.bar(title= 'Own Home')\nplt.subplot(2,3,5)\nfile['Location'].value_counts(normalize=True).plot.bar(title= 'Location')\nplt.subplot(2,3,6)\nfile['History'].value_counts(normalize=True).plot.bar(title= 'Histoy') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\nplt.figure(1)\nplt.subplot(121)\nsns.distplot(file['Salary'])\nplt.subplot(122)\nfile['Salary'].plot.box(figsize = (16,5))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1)\nplt.subplot(121)\nsns.distplot(file['Children'])\nplt.subplot(122)\nfile['Children'].plot.box(figsize = (16,5))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1)\nplt.subplot(121)\nsns.distplot(file['Catalogs'])\nplt.subplot(122)\nfile['Catalogs'].plot.box(figsize = (16,5))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1)\nplt.subplot(121)\nsns.distplot(file['AmountSpent'])\nplt.subplot(122)\nfile['AmountSpent'].plot.box(figsize = (16,5))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Bivariate Analysis\n#Age\n\n#Let us now view the column 'Age' in detail to identify the amount spent details.\n\npd.pivot_table(file, index = ['Age','History'], values = 'AmountSpent', aggfunc=np.sum)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.pivot_table(file, index = ['Age','History'], values = 'AmountSpent', aggfunc=[np.mean,len])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Gender\n\n#Gender is used for analysis of Amount spent. There is not much difference on the average spent by gender. \n#However, on further analysis, we could see that the number of customers with Low History is more in the female than male and \n#High History is more in male that female.\npd.pivot_table(file, index = ['Gender', 'History'], values = 'AmountSpent', aggfunc=np.sum)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.pivot_table(file, index = ['Gender', 'History'], values = 'AmountSpent', aggfunc=[np.mean,len])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Own home\n#1.Customers with own home are more in category High and their average spending is high too. \n#2.Average spending is in same level for Low and Medium categories.\n\npd.pivot_table(file, index = ['OwnHome', 'History'], values = 'AmountSpent', aggfunc=np.sum)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.pivot_table(file, index = ['OwnHome', 'History'], values = 'AmountSpent', aggfunc=[np.mean,len])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Married\n#1.Married people are more in category High \n#2.Single customers are more in category Low.\n\npd.pivot_table(file, index = ['Married', 'History'], values = 'AmountSpent', aggfunc=np.sum)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.pivot_table(file, index = ['Married', 'History'], values = 'AmountSpent', aggfunc=[np.mean,len])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Location\n#1.Customers who are in close by location are more in number. \n#2.Average spending is high with respect to customers who are in far locations\n\npd.pivot_table(file, index = ['Location', 'History'], values = 'AmountSpent', aggfunc=np.sum)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.pivot_table(file, index = ['Location', 'History'], values = 'AmountSpent', aggfunc=[np.mean,len])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Preprocessing data\n\n#Convert all the categorical columns to numerical columns using labelencoder\n\nfrom sklearn.preprocessing import LabelEncoder\n\ncat_cols = ['Age','Gender', 'OwnHome','Married','Location','History']\n\nfor c in cat_cols:\n    file[c]= LabelEncoder().fit_transform(file[c])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Relationship between Amount spent and other fields.\n\n#1.Salary = 0.7\n#2.Catalogs = 0.5\n#3.History = -0.5\n#4.Married = -0.5\n#5.Own Home = -0.4\n#6.Age = -0.4\n\nt = file.corr()\n\nsns.heatmap(t, cmap='coolwarm', annot=True, fmt=\".1f\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split the data into train and test\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\n\nX= file.drop('AmountSpent', axis =1)\ny = file['AmountSpent']\n\nscale = StandardScaler().fit_transform(X)\nX = pd.DataFrame(scale, columns=X.columns)\n\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.2)\n\ndef model_details(model,name,X_train = X_train,y_train = y_train,X_test = X_test,y_test = y_test):\n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    table = pd.DataFrame({'Model' :[name],\n                          'RMSE' :[np.sqrt(mean_squared_error(y_test,y_pred))],\n                          })\n\n    return table","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Model Building\n\nfrom sklearn.linear_model import LinearRegression\nmodel_LR = LinearRegression()\nmodel_LR.fit(X_train,y_train)\n\nmodel_LR_predict = model_LR.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\npoly_reg = PolynomialFeatures(degree = 2)\n\nX_train_poly_2 = poly_reg.fit_transform(X_train)\nX_test_poly_2 = poly_reg.fit_transform(X_test)\nmodel_PR_2 = LinearRegression()\nmodel_PR_2.fit(X_train_poly_2,y_train)\n\nmodel_PR_2_predict = model_PR_2.predict(X_test_poly_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\npoly_reg = PolynomialFeatures(degree = 3)\n\nX_train_poly_3 = poly_reg.fit_transform(X_train)\nX_test_poly_3 = poly_reg.fit_transform(X_test)\nmodel_PR_3 = LinearRegression()\nmodel_PR_3.fit(X_train_poly_3,y_train)\n\nmodel_PR_3_predict = model_PR_3.predict(X_test_poly_3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVR\nmodel_SVR = SVR(kernel = 'rbf')\n\nmodel_SVR.fit(X_train, y_train)\nmodel_SVR_predict = model_SVR.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeRegressor\n\nmodel_DT = DecisionTreeRegressor()\nmodel_DT.fit(X_train, y_train)\n\nmodel_DT_predict = model_DT.predict(X_test)\n\nimportance = model_DT.feature_importances_\nfeat_imp = pd.DataFrame({'Columns': X.columns,\n                         'Imp' : importance})\nfeat_imp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nmodel_RF = RandomForestRegressor(n_estimators = 100, random_state = 111)\nmodel_RF.fit(X_train, y_train)\n\nmodel_RF_predict = model_RF.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_1 = model_details(model_LR, 'Linear Regression')\nmodel_2 = model_details(model_PR_2, 'Polynomial Regression (2)',X_train = X_train_poly_2, X_test = X_test_poly_2)\nmodel_3 = model_details(model_PR_3, 'Polynomial Regression (3)',X_train = X_train_poly_3, X_test = X_test_poly_3)\nmodel_4 = model_details(model_SVR, 'SVR')\nmodel_5 = model_details(model_DT, 'Decision Tree')\nmodel_6 = model_details(model_RF, 'Random Forest')\n\nfinal_table = pd.concat([model_1,model_2,model_3, model_4, model_5, model_6], axis =0)\nfinal_table","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}