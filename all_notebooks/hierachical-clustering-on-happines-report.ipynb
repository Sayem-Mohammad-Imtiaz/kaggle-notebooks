{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Hierachical Clustering On Happines Report\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n1. [Introduction and Data Import](#0)<br>\n2. [Feature Selection](#1)<br>\n3. [Clustering using Scipy](#2)\n4. [Visualizing ](#3)\n5. [Clustering using scikit-learn](#4)\n6. [Visualizing ](#5)\n<hr>"},{"metadata":{},"cell_type":"markdown","source":"# Introduction and Data Import <a id=\"0\"></a>"},{"metadata":{},"cell_type":"markdown","source":"Importing necessary libraries."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom scipy import ndimage \nfrom scipy.cluster import hierarchy \nfrom scipy.spatial import distance_matrix \nfrom matplotlib import pyplot as plt \nfrom sklearn import manifold, datasets \nfrom sklearn.cluster import AgglomerativeClustering \nfrom sklearn.datasets.samples_generator import make_blobs \n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/world-happiness/2015.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature selection <a id=\"1\"></a>"},{"metadata":{},"cell_type":"markdown","source":"Lets select our feature set:"},{"metadata":{"trusted":true},"cell_type":"code","source":"featureset = df[[\"Standard Error\",\"Economy (GDP per Capita)\",\"Family\",\"Health (Life Expectancy)\",\"Freedom\",\"Trust (Government Corruption)\",\"Generosity\",\"Dystopia Residual\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Normalization<br>\nNow we can normalize the feature set. MinMaxScaler transforms features by scaling each feature to a given range. It is by default (0, 1). That is, this estimator scales and translates each feature individually such that it is between zero and one."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nx = featureset.values #returns a numpy array\nmin_max_scaler = MinMaxScaler()\nfeature_mtx = min_max_scaler.fit_transform(x)\nfeature_mtx [0:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clustering using Scipy <a id=\"2\"></a>"},{"metadata":{},"cell_type":"markdown","source":"In this part we use Scipy package to cluster the dataset:<br>\nFirst, we calculate the distance matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy\nleng = feature_mtx.shape[0]\nD = scipy.zeros([leng,leng])\nfor i in range(leng):\n    for j in range(leng):\n        D[i,j] = scipy.spatial.distance.euclidean(feature_mtx[i], feature_mtx[j])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pylab\nimport scipy.cluster.hierarchy\nZ = hierarchy.linkage(D, 'complete')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Essentially, Hierarchical clustering does not require a pre-specified number of clusters. However, in some applications we want a partition of disjoint clusters just as in flat clustering. So you can use a cutting line:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.cluster.hierarchy import fcluster\nmax_d = 3\nclusters = fcluster(Z, max_d, criterion='distance')\nclusters","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also, you can determine the number of clusters directly:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.cluster.hierarchy import fcluster\nk = 5\nclusters = fcluster(Z, k, criterion='maxclust')\nclusters","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing <a id=\"3\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = pylab.figure(figsize=(20,200))\ndef llf(id):\n    return '[%s ,%s,%s ]' % (df['Country'][id], df['Region'][id],int(float(df['Happiness Rank'][id])) )\n    \ndendro = hierarchy.dendrogram(Z,  leaf_label_func=llf, leaf_rotation=0, leaf_font_size =12, orientation = 'right')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clustering using scikit-learn <a id=\"4\"></a>"},{"metadata":{},"cell_type":"markdown","source":"Lets redo it again, but this time using scikit-learn package:"},{"metadata":{"trusted":true},"cell_type":"code","source":"dist_matrix = distance_matrix(feature_mtx,feature_mtx) \nprint(dist_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we can use the 'AgglomerativeClustering' function from scikit-learn library to cluster the dataset. The AgglomerativeClustering performs a hierarchical clustering using a bottom up approach. The linkage criteria determines the metric used for the merge strategy:\n\n- Ward minimizes the sum of squared differences within all clusters. It is a variance-minimizing approach and in this sense is similar to the k-means objective function but tackled with an agglomerative hierarchical approach.\n- Maximum or complete linkage minimizes the maximum distance between observations of pairs of clusters.\n- Average linkage minimizes the average of the distances between all observations of pairs of clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"agglom = AgglomerativeClustering(n_clusters = 3, linkage = 'complete')\nagglom.fit(feature_mtx)\nagglom.labels_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And, we can add a new field to our dataframe to show the cluster of each row:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['cluster_'] = agglom.labels_\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing <a id=\"5\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.cm as cm\nn_clusters = max(agglom.labels_)+1\ncolors = cm.rainbow(np.linspace(0, 1, n_clusters))\ncluster_labels = list(range(0, n_clusters))\n\n\nplt.figure(figsize=(15,15))\n\nfor color, label in zip(colors, cluster_labels):\n    subset = df[df.cluster_ == label]\n    for i in subset.index:\n            plt.text(subset[\"Happiness Score\"][i], subset.Region[i],str(subset.Country[i]), rotation=25) \n    plt.scatter(subset[\"Happiness Score\"], subset.Region,  c=color, label='cluster'+str(label),alpha=0.5)\n#    plt.scatter(subset.horsepow, subset.mpg)\nplt.legend()\nplt.title('Clusters')\nplt.xlabel('Happiness Score')\nplt.ylabel('Region')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Obviously the countries with most happiness score are red, with less happiness score are purple and the others are light blue.<br>\nThere is a big 'but' beacuse USA and few other countries with high happiness score are light blue and few countries with fewer scores are red. Isn't it intersting?"},{"metadata":{},"cell_type":"markdown","source":"Now we can look at the characterestics of each cluster:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby(['cluster_','Region'])['cluster_'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"agg_reg = df.groupby(['cluster_','Region'])['Happiness Score','Economy (GDP per Capita)','Freedom','Health (Life Expectancy)'].mean()\nagg_reg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for label in cluster_labels:\n    subset=agg_reg.loc[(label,),]\n    print(subset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,15))\nfor color, label in zip(colors, cluster_labels):\n    subset = agg_reg.loc[(label,),]\n    for i in subset.index:\n        plt.text(subset.loc[i][0], subset.loc[i][2], 'Region='+str(i) + ', Health='+str(subset.loc[i][3]))\n    plt.scatter(subset[\"Happiness Score\"], subset[\"Freedom\"], c=color, label='cluster'+str(label))\nplt.legend()\nplt.title('Clusters')\nplt.xlabel('horsepow')\nplt.ylabel('mpg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Thank you for sahring your time to take a look at my kernel."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Please leave a comment if you like it or if you think the kernel needs improvement"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}