{"cells":[{"metadata":{"tags":[],"cell_id":"00000-eb35a5b7-fcb4-4d59-9249-8ca098bcb35f","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"## Pima Indians Diabetes Modeling\nIn the [previous notebook](https://www.kaggle.com/michaelchen1116/pima-indians-diabetes-eda): \n\n* We conducted exploratory data analysis on the `diabetes` dataset through the process of **univariate analysis**, **bivariate analysis**, **missingness analysis**. \n* We identified zero entries as missing values in different columns and imputed those values using the existing data from those columns. \n* We selected columns that best explain the target variables and have low residuals relative to their counterpart (either original or bins that we created by cutting original columns).\n\nThis notebook contains the modeling step for the diabetes analysis. It presents the process of preprocessing, creating models, parameter tuning, and comparing models using different evaluation metrics."},{"metadata":{"tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"e9d52541","execution_millis":2431,"cell_id":"00001-351c5b9a-4ced-48f6-9f7a-1403d9f34838","execution_start":1616109455027,"deepnote_cell_type":"code","trusted":true},"cell_type":"code","source":"# data analysis, wrangling and preprocessing\nimport numpy as np\nimport pandas as pd\nimport random\n\n# data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# statistical modeling\nimport statsmodels.api as sm\n\n# modeling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n\n# hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV\n\n# model evaluation\nfrom sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, confusion_matrix ","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"00002-6ea90ae9-e22b-4974-993e-647985318ce2","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"### Loading Data\nWe start by loading the dataset from the previous anaysis into Pandas DataFrame."},{"metadata":{"tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"2f91a08c","execution_millis":23,"cell_id":"00003-b4b60c67-1d74-4389-bfee-aa8fa94d7b46","execution_start":1616109457461,"deepnote_cell_type":"code","trusted":true},"cell_type":"code","source":"diabetes = pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')\ndiabetes.columns","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"00004-0c9a0ffc-9625-49d2-9d27-22d7b1faa1b7","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"Here is a brief description of each column:\n* `Pregnancies`: number of times pregnant\n* `Glucose`: blood plasma glucose concentration a 2 hours in an oral glucose tolerance test\n* `BloodPressure` : Diastolic blood pressure with a unit of  mm Hg\n* `SkinThickness` : triceps skin fold thickness with a unit of mm\n* `Insulin` : 2-Hour serum insulin with a unit of mu U/ml\n* `BMI` : body mass index, which is calculated by weight in kg/(height in m)^2\n* `DiabetesPedigreeFunction` : a synthesis of the diabetes mellitus history in relatives \nand the genetic relationship of those relatives to the subject\n* `Age` : age of the patient in years\n* `Outcome` : whether the patient has diabetes or not, which is our target variable that we are trying \nto predict"},{"metadata":{"tags":[],"cell_id":"00005-8a2d8154-ec6d-4a63-9069-d15f2ec3d57a","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"### Modeling approach\nWe plan to use **logistic regression**, **random forest**, and **gradient boosting** to model whether a given patient in the dataset is classified as having diabetes or not. We also plan to go through the process of hyperparameter tuning for each model using GridSearch to obtain the best result before we compare the results from different models using our evaluation metrics."},{"metadata":{"tags":[],"cell_id":"00006-15ec43c7-f751-43e5-8c4c-2731529040bc","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"#### Preprocessing for modeling\nWe will go through the process for the completeness of the analysis. Here are several things we have to do to reach the resulting dataset from the the [previous notebook](https://www.kaggle.com/michaelchen1116/pima-indians-diabetes-eda):\n* Replace zeros by imputing existing values in that column randomly. \n* Next, we will cut some columns we selected in the previous notebook into bins since the original columns has either larger residual or lower R-square."},{"metadata":{"tags":[],"cell_id":"00007-9a519be5-875b-4059-91dc-a2dd7a95fedd","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"We identify zero values in `BloodPressure`, `Insulin`, and `SkinThickness` as missing values. As a result, we decided to handle them by imputing existing values in that column randomly. "},{"metadata":{"tags":[],"cell_id":"00007-babe9887-ddf6-4a34-a05b-b85e4934c36b","deepnote_to_be_reexecuted":false,"source_hash":"c1a0ca7","execution_millis":236,"execution_start":1616109457477,"deepnote_cell_type":"code","trusted":true},"cell_type":"code","source":"random.seed(0)\ndiabetes['BloodPressure']=diabetes['BloodPressure'].replace(0, random.choice(diabetes[diabetes.BloodPressure != 0 ][\"BloodPressure\"].tolist()))\ndiabetes['SkinThickness']=diabetes['SkinThickness'].replace(0, random.choice(diabetes[diabetes.SkinThickness != 0 ][\"SkinThickness\"].tolist()))\ndiabetes['Insulin']=diabetes['Insulin'].replace(0, random.choice(diabetes[diabetes.Insulin != 0 ][\"Insulin\"].tolist()))","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"00009-b7f149b5-2f8c-4df8-ac38-b992460af301","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"We will cut these columns into bins since the original columns has either larger residual or lower R-square:\n* `Insulin`\n* `BMI`\n* `DiabetesPedigreeFunction`"},{"metadata":{"tags":[],"cell_id":"00008-c87877d5-a497-4293-81ed-1b0ed6cf4c06","deepnote_to_be_reexecuted":true,"source_hash":"65a5a4a9","execution_millis":7,"deepnote_cell_type":"code","trusted":true},"cell_type":"code","source":"diabetes['Insulin_group'] = pd.cut(diabetes['Insulin'], bins = 3, labels=False)\ndiabetes['BMI_group'] = pd.cut(diabetes['BMI'], bins = 5, labels=False)\ndiabetes['pedigree_group'] = pd.cut(diabetes['DiabetesPedigreeFunction'], bins = 3, labels=False)","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"00011-34912a78-878d-45f4-b182-428e7b9912eb","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"Only select columns we chose to put into the model. The process of feature selection can be found in the [previous notebook](https://www.kaggle.com/michaelchen1116/pima-indians-diabetes-eda):"},{"metadata":{"tags":[],"cell_id":"00011-84adbe12-0ac6-400f-83e4-89b4745e3c90","deepnote_to_be_reexecuted":true,"source_hash":"fb8a2caa","execution_millis":1,"deepnote_cell_type":"code","trusted":true},"cell_type":"code","source":"diabetes = diabetes[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',\n       'Insulin_group', 'BMI_group', 'pedigree_group', 'Outcome']]","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"00008-25d9fbba-7de5-457d-b3f4-2fa0bbf15ab5","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"We want to use the heatmap to see if there are any features that are highly correlated with other features. If there is such feature, we have to drop them since they provide redundant information for our model."},{"metadata":{"tags":[],"deepnote_to_be_reexecuted":true,"source_hash":"2ef604a4","execution_millis":418,"cell_id":"00007-38a6102a-e000-45a5-9cc4-ecb9cebbe703","deepnote_cell_type":"code","trusted":true},"cell_type":"code","source":"sns.heatmap(diabetes.iloc[:,:-1].corr())\nplt.xticks(rotation=45)\nplt.title(\"Heapmap of Selected Feautures\")","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"00008-b462f828-ae0e-4386-b62c-c9b3937b4232","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"There are not two columns that are significantly correlated with each other. We can proceed to the modeling step.\n\nTo avoid duplicate code, we decided to create `X` (independent variables) and `y` (dependent variable) in advance and splitting the dataset into training and testing sets using `train_test_split` for the purpose of getting more accurate results for the model evaluation process. "},{"metadata":{"tags":[],"deepnote_to_be_reexecuted":true,"source_hash":"cefab8db","execution_millis":11,"cell_id":"00009-243968e7-eacd-49aa-bdee-ce4e802c243a","deepnote_cell_type":"code","trusted":true},"cell_type":"code","source":"X = diabetes.drop(columns=['Outcome'])\nX = sm.add_constant(X)\ny = diabetes['Outcome']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"00010-1a7019c5-1a71-4f29-ba60-7cf8e8621124","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"#### Logistic Regression\nLogistic regression is perfect for a binary classification problem like this one where we try to predict whether the patient has diabetes or not. It predicts the probability of each outcome and we can round it to 0 and 1. \n\nHere is a brief description of the parameters we are trying to tune:\n* **Penalty**: the type of regularization between 'L1' and 'L2'. 'L1' is the Lasso regression, which adds a \"squared-magnitude\" of coefficient as penalty term in the loss function, while 'L2' is the Ridge regression, which adds “absolute value of magnitude” of coefficient as penalty term to the loss function.\n* **C**: the trade-off parameter of logistic regression that determines the strength of the regularization."},{"metadata":{"tags":[],"deepnote_to_be_reexecuted":true,"source_hash":"e5b12591","execution_millis":1610,"output_cleared":true,"cell_id":"00011-6e3a874c-73c0-4afb-9bbb-8d2e92b91585","deepnote_cell_type":"code","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"log_reg = LogisticRegression()\n\npenalty = ['l1','l2']\nC = np.linspace(0, 5, 10)\nhyperparameters = dict(C=C, penalty=penalty)\n# Create Grid Search using 5-fold cross validation\nclf = GridSearchCV(log_reg, hyperparameters, cv = 5, verbose=0)\nbest_logic_model = clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"deepnote_to_be_reexecuted":true,"source_hash":"726f0562","execution_millis":10,"cell_id":"00012-889bc5ea-108b-4a28-8022-5bd25429bf9a","deepnote_cell_type":"code","trusted":true},"cell_type":"code","source":"print('Best Penalty:', best_logic_model.best_estimator_.get_params()['penalty'])\nprint('Best C:', best_logic_model.best_estimator_.get_params()['C'])","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"00022-346eda8b-f269-4643-bfce-9fcbbea98545","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"To avoid redundant code, we created a function called `model_performance` that generates the number of true negatives, true positives, false negatives, and false positives. It has two parameters: `name` (the name of the model) and `prediction` (the prediction made by the given model). It creates a dictionary with the metrics we need to build a table later on to compare different models."},{"metadata":{"tags":[],"deepnote_to_be_reexecuted":true,"source_hash":"b6265e3a","execution_millis":18,"cell_id":"00013-2430a1a5-95b4-4e10-b5c0-f8f8320c67e5","deepnote_cell_type":"code","trusted":true},"cell_type":"code","source":"new_dict = {}\ndef model_performance(name, prediction):\n    TN = confusion_matrix(y_test, prediction)[1,1]\n    TP = confusion_matrix(y_test, prediction)[0,0]\n    FN = confusion_matrix(y_test, prediction)[1,0]\n    FP = confusion_matrix(y_test, prediction)[0,1]\n    new_dict[name] = [TN, TP, FN, FP]\n    return new_dict","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"00021-e3657d4b-db5f-46f9-affd-28a7d108195d","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"We use the `model_performance` function that we just created to get the metrics."},{"metadata":{"tags":[],"cell_id":"00021-d353fb73-a99b-429c-b748-205ebd4fb48e","deepnote_to_be_reexecuted":true,"source_hash":"ed25aad2","execution_millis":15,"deepnote_cell_type":"code","trusted":true},"cell_type":"code","source":"# performing predictions on the test datdaset \nyhat = best_logic_model.predict(X_test) \nprediction = list(map(round, yhat))\n# getting model performance data for model evalation and comparison \nmodel_performance('Logistic', prediction)","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"00014-bccd73f3-db86-4b23-b0ef-e7b813a9ada0","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"#### Random Forest\nRandom forest is a collection of decision trees and each tree is independent of others. The final call depends on the majority of the decision. \n\nHere is a brief description of the parameters we are trying to tune:\n* **n_estimators**: number of trees in the forest\n* **max_depth**: the depth of each tree in the forest\n* **min_samples_split**: the minimum number of samples required to split an internal node"},{"metadata":{"tags":[],"deepnote_to_be_reexecuted":true,"source_hash":"d173eec0","execution_millis":93436,"cell_id":"00015-551226b3-463f-4d0b-ab53-2b175274aeb9","deepnote_cell_type":"code","trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier()\n\nn_estimators = [16,32,64,128]\nmax_depth = np.linspace(1, 10, 10, endpoint=True)\nmin_samples_split = np.linspace(0.1, 1.0, 10, endpoint=True)\nhyperparameters = dict(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split)\n# Create Grid Search using 3-fold cross validation\nclf = GridSearchCV(rf, hyperparameters, cv = 3, verbose=0)\nbest_rf_model = clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"deepnote_to_be_reexecuted":true,"source_hash":"83f2b313","execution_millis":8,"cell_id":"00016-6de6baaa-7fa0-4fc3-acaa-0d3182f910d1","deepnote_cell_type":"code","trusted":true},"cell_type":"code","source":"print('Best n_estimator:', best_rf_model.best_estimator_.get_params()['n_estimators'])\nprint('Best max_depth:', best_rf_model.best_estimator_.get_params()['max_depth'])\nprint('Best min_samples_split:', best_rf_model.best_estimator_.get_params()['min_samples_split'])","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"00026-9583f7bb-885b-46a2-9318-177b566c4218","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"We want to see what are some features that play an important part in deciding how to classify patients in this dataset into two distinct groups. "},{"metadata":{"tags":[],"deepnote_to_be_reexecuted":true,"source_hash":"30ce8ab5","execution_millis":183,"cell_id":"00017-432fa1e1-ee03-4db8-9d59-0fbe16d8b4af","deepnote_cell_type":"code","trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=16, max_depth=8, min_samples_split=0.1)\nrf_model = rf.fit(X_train, y_train)\nfeature_imp = pd.Series(rf_model.feature_importances_,index=X.columns).sort_values(ascending=False)\nsns.barplot(x=feature_imp, y=feature_imp.index)\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"00026-b5fba91a-a9ed-4a30-a2bb-b60aefad9d5d","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"As we can see in the chart above, `Glucose` plays a significant role comparing with other features in deciding where to split the tree into different branches followed by `Pregnancies` and `BMI_group`. This makes sense because according to [this source](https://www.niddk.nih.gov/health-information/diabetes/overview/what-is-diabetes), diabetes normally occurs glucose, also called blood sugar, is too high. Also, `BMI_group` and `Pregnancies` both reflect the weight of a patient, which is a crucial factor when determining whether a patient has diabetes or not.\n\nIt is surprising to us that Insulin doesn't play much of a role here because it helps glucose from food get into your cells to be used for energy. The reason might be that we cut the original `Insulin` column into three evenly distributed bins instead of cutting them into bins according to medical ranges that define different levels of insulin."},{"metadata":{"tags":[],"cell_id":"00029-4ccda33a-e519-4a4f-9b3e-aae72b304216","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"Performing predictions on the test dataset and using the function we created earlier to get model performance data for model evaluation and comparison."},{"metadata":{"tags":[],"deepnote_to_be_reexecuted":true,"source_hash":"18acc69c","execution_millis":21,"cell_id":"00018-6e46123a-4a2e-4451-a679-e903235bf64e","deepnote_cell_type":"code","trusted":true},"cell_type":"code","source":"yhat = best_rf_model.predict(X_test) \nprediction = list(map(round, yhat)) \nmodel_performance('Random Forest', prediction)","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"00019-e88fd12b-0427-4240-9c14-51b75705eb3e","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"#### Gradient Boost Model\nGradient boosting relies on the intuition that the best possible next model, when combined with the previous model, minimizes the overall prediction error.\n\nHere is a brief description of the parameters we are trying to tune:\n* **learning_rate**: the step size of optimization algorithm moving towards the objective\n* **n_estimators**: number of trees in the forest\n* **max_depth**: the depth of each tree in the forest\n* **min_samples_split**: the minimum number of samples required to split an internal node"},{"metadata":{"tags":[],"deepnote_to_be_reexecuted":true,"source_hash":"abc7bf67","execution_millis":35098,"cell_id":"00020-959a6927-eb9f-4219-ae45-919ba54c4dbd","deepnote_cell_type":"code","trusted":true},"cell_type":"code","source":"gb = GradientBoostingClassifier()\n\nlearning_rate = [1, 0.5, 0.25, 0.1]\nn_estimators = [1,2,4,8,16,32,64]\nmax_depth = np.linspace(1, 5, 5, endpoint=True)\nmin_samples_split = np.linspace(0.1, 0.5, 5, endpoint=True)\nhyperparameters = dict(learning_rate=learning_rate,n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split)\n# Create Grid Search using 3-fold cross validation\nclf = GridSearchCV(gb, hyperparameters, cv = 3, verbose=0)\nbest_gb_model = clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"deepnote_to_be_reexecuted":true,"source_hash":"58536b9a","execution_millis":10,"cell_id":"00021-4183d17a-cd5f-49d4-ac4a-7660a5b96ce4","deepnote_cell_type":"code","trusted":true},"cell_type":"code","source":"print('Best learning_rate:', best_gb_model.best_estimator_.get_params()['learning_rate'])\nprint('Best n_estimator:', best_gb_model.best_estimator_.get_params()['n_estimators'])\nprint('Best max_depth:', best_gb_model.best_estimator_.get_params()['max_depth'])\nprint('Best min_samples_split:', best_gb_model.best_estimator_.get_params()['min_samples_split'])","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"00034-b32d6130-a334-47ea-a1dc-90a3bf15d7d6","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"Performing predictions on the test dataset and using the function we created earlier to get model performance data for model evaluation and comparison."},{"metadata":{"tags":[],"deepnote_to_be_reexecuted":true,"source_hash":"3749c87f","execution_millis":9,"cell_id":"00022-78c33915-beba-47db-b002-cbb89f062877","deepnote_cell_type":"code","trusted":true},"cell_type":"code","source":"yhat = best_gb_model.predict(X_test) \nprediction = list(map(round, yhat))\nmodel_performance('Gradient Boosting', prediction)","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"00023-3e758818-ad08-4e77-aeda-aa0265c819cb","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"#### Model performance comparison\nSince this is a classification problem, we decided to use corresponding metrics to compare the model performance (listed as follows).\n* **accuracy**: the proportion of predictions that are correct\n* **sensitivity** (true positive rate): the proportion of true observations that are correctly predicted by the model as being true.\n* **specificity** (true negative rate): the proportion of false observations that are correctly predicted by the model as being false."},{"metadata":{"tags":[],"cell_id":"00037-b756f8bf-e23b-40fa-bda9-3ba39f29a9b1","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"To avoid repetitive code, we decided to create a funcction that calculates those different metrics that we just mentioned. "},{"metadata":{"tags":[],"deepnote_to_be_reexecuted":true,"source_hash":"8f9155cd","execution_millis":0,"cell_id":"00024-096378ef-ff62-44a0-80b7-45e54f0557c0","deepnote_cell_type":"code","trusted":true},"cell_type":"code","source":"def evaluation_metric(aDict):\n    for name in aDict:\n        TN = aDict[name][0]\n        TP = aDict[name][1]\n        FN = aDict[name][2]\n        FP = aDict[name][3]\n        accuracy = np.round((TN+TP)/ (TN+TP+FN+FP), 4)\n        # sensitivity is calculated by the true positives over the sum of the true positives and false negatives\n        sensitivity = np.round((TP)/ (TP+FN), 4)\n        # specificity is calculated by the true negatives over the sum of the true negatives and the false positives\n        specificity = np.round((TN)/ (TN+FP), 4)\n        aDict[name].append(accuracy)\n        aDict[name].append(sensitivity)\n        aDict[name].append(specificity)\n    return aDict","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"00039-a91b4fa3-b433-410b-9807-8f7d1b455374","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"##### Create a table using the dictionary\nThe table includes performance data and evaluation metrics for each model.\n* First, we count the number of test cases in each model\n* Then, we want to transform the numbers in the confusion matrix into percentages."},{"metadata":{"tags":[],"cell_id":"00042-39650d0d-c8ad-4e2b-84b5-6c87aba17bef","deepnote_to_be_reexecuted":true,"source_hash":"bd0b7b91","execution_millis":49,"deepnote_cell_type":"code","trusted":true},"cell_type":"code","source":"# count the sum of test cases\nsum_list = []\nfor aList in new_dict.values():\n    aSum = 0\n    for i in np.arange(4):\n        aSum += aList[i]\n    sum_list.append(aSum)\nsum_list","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"00043-ea69ce43-3f7e-4323-ad95-79a6290cbd62","deepnote_to_be_reexecuted":true,"source_hash":"3eecb9f0","execution_millis":2,"deepnote_cell_type":"code","trusted":true},"cell_type":"code","source":"for aList in new_dict.values():\n    for j in np.arange(4):\n        aList[j] = np.round(aList[j]/254, 3)","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"deepnote_to_be_reexecuted":true,"source_hash":"d462d638","execution_millis":49,"cell_id":"00025-7763c7f8-4d43-45b3-adda-8305601375f7","deepnote_cell_type":"code","trusted":true},"cell_type":"code","source":"model_comparison = pd.DataFrame.from_dict(evaluation_metric(new_dict))\nmodel_comparison = model_comparison.rename(index={0: \"True Negative\", 1: \"True Positive\", 2: \"False Negative\",3:'False Positive',\n    4:'Accuracy',5:'Sensitivity',6:'Specificity'})\nmodel_comparison","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"00026-4b629483-bdbb-4a4b-9487-d4f8a97beafb","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"According to the table above, we can see that logistic regression seems to have the most accuracy in terms of predicting whether a patient in this dataset is classified as having diabetes or not. There is not much of a difference in model performance after the process of parameters tuning.\n\n**Logistic Regression** algorithm has the highest true positive rate. This means that **Logistic Regression** is the most useful in correctly predicting whether a patient should be classified as having diabetes for this dataset.\n\n**Logistic Regression** has the highest sensitivity. This means that Logistic Regression correctly generates a positive result for people who have diabetes more often than other models. Also, we have to note that a model with highly sensitive will flag almost everyone who has the disease and not generate many false-negative results. In this case, around 78% does not raise this red flag of being too high. \n\n**Logistic Regression** has the highest specificity. This means that Logistic Regression correctly generates a negative result for people who don't have diabetes. Similar to sensitivity, a high specificity will raise a red flag because it means the model will classify everyone in the dataset as not having diabetes. In our Logistic Regression model, around 70% of specificity does not raise this red flag. \n\nNormally, we will choose the model that correctly identifying patients without diabetes was more important than correctly identifying patients with diabetes. Ane we will choose the model that correctly identifying patients with diabetes was more important than correctly identifying patients without diabetes. In this case, since **Logistic Regression** has the highest sensitivity and specificity at the same time. There is no doubt that it is our best model when we try to predict whether patients in this dataset have diabetes or not."},{"metadata":{"tags":[],"cell_id":"00042-cc6c3e53-06e2-4297-a056-6419d0d56810","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"##### Functions that help us generate the ROC-AUC curve \n* `no_skill_prediction` generates a plot with a line that shows when the true positive equals the false positive rate.\n* `graph_roc_auc` generates ROC-AUC curve for a given model"},{"metadata":{"tags":[],"deepnote_to_be_reexecuted":true,"source_hash":"7dec76c3","execution_millis":39,"cell_id":"00027-0816718a-2f8c-4bb9-8b72-0a2a1257d10f","deepnote_cell_type":"code","trusted":true},"cell_type":"code","source":"def no_skill_prediction():\n    ns_probs = [0 for _ in range(len(y_test))]\n    ns_auc = roc_auc_score(y_test, ns_probs)\n    print('No Skill: ROC AUC=%.3f' % (ns_auc))\n    ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n    plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n\ndef graph_roc_auc(model, name):\n    # predict probabilities\n    lr_probs = model.predict_proba(X_test)\n    lr_probs = lr_probs[:, 1]\n    # calculate score\n    lr_auc = roc_auc_score(y_test, lr_probs)\n    # print score\n    print(name + ': ROC AUC=%.3f' % (lr_auc))\n    # calculate roc curves\n    lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n    # plot the roc curve for the model\n    plt.plot(lr_fpr, lr_tpr, marker='.', label=name)\n    # axis labels\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    # show the legend\n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"00044-5a0e12b3-a488-4e8e-9307-8a3885e7563f","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"##### Produce the ROC AUC curves for all three models on the same graph\nWe generate ROC AUC curves for all three models on the same graph using the function we created earlier."},{"metadata":{"tags":[],"cell_id":"00037-84f459b4-fc7f-4027-9e64-798591172811","deepnote_to_be_reexecuted":true,"source_hash":"1dc5c919","execution_millis":259,"deepnote_cell_type":"code","trusted":true},"cell_type":"code","source":"no_skill_prediction()\nmodel = [best_logic_model, best_rf_model, best_gb_model]\nname = ['Logistic','Random Forest','Gradient Boosting']\nfor i in range(len(model)):\n    graph_roc_auc(model[i], name[i])\nplt.title('ROC AUC comparison')","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"cell_id":"00046-f268bff6-2a82-4cc5-b5bb-84da1f1a4d67","deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"As shown in the ROC AUC curve above, the y-axis is the true positive rate (sensitivity), while the x-axis is the false positive rate (1- specificity). The true positive rate tells us the proportion of patients in this dataset that identified as having diabetes were correctly classified. The false-positive rate tells us the proportion of patients in this dataset that identified as not having diabetes were correctly classified and are false positive. \n\nThe line for each model as shown in orange, green, and red illustrates the trade-off between we correctly classified all of the patients in this dataset that have diabetes and incorrectly classified all of the patients in this dataset that don't have diabetes.\n\nThe area under the curve (AUC) makes it easier to compare one ROC curve to another. They were shown above the graph. The graph and the AUC confirm that we should choose logistic regression. "},{"metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"},"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=fd4a356b-76ac-4ad6-9eb0-790688a80ec2' target=\"_blank\">\n<img style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}