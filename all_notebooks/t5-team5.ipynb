{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Layer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/facebookresearch/detr.git\nos.chdir('detr')\n!git checkout a54b77800eb8e64e3ad0d8237789fcbf2f8350c5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir('/kaggle/input/models-1')\nfrom models1.face_recognition.model_small import create_model\nfrom models1.face_recognition.align import AlignDlib\nfrom models1.triplet_loss import TripletLossLayer\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir('/kaggle/input/lfw-preprocessor')\nfrom lfw_preprocessor import LfwDataGenerator","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import argparse\nimport random\nfrom pathlib import Path\n\nimport torch\nimport torchvision.transforms as T\nimport matplotlib.pyplot as plt\nimport PIL.Image\n\nos.chdir('/kaggle/input/detr-1/detr')\nimport util.misc as utils\nfrom models import build_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir('/kaggle/working/detr')\nfrom main import get_args_parser","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir('/kaggle/input/competition')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\n\n#Dictionary of IDs (int) and names (string)\nidict = {}\nidict2 = {}\nwith open(\"person_id_name_mapping.csv\", mode = 'r') as ids:\n  tdict = csv.DictReader(ids)\n  for r in tdict:\n    idict[r[\"person_name\"]] =  int(r[\"person_id\"])\n    idict2[int(r[\"person_id\"])] = r[\"person_name\"]\n\nprint(idict)\nprint(idict2)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"in_a = Input(shape = (96, 96, 3), name = \"img_a\")\nin_p = Input(shape = (96, 96, 3), name = \"img_p\")\nin_n = Input(shape = (96, 96, 3), name = \"img_n\")\n\nmodel_sm = create_model()\n\nemb_a = model_sm(in_a)\nemb_p = model_sm(in_p)\nemb_n = model_sm(in_n)\n\ntriplet_loss_layer = TripletLossLayer(alpha = 0.2, name = 'triplet_loss_layer')([emb_a, emb_p, emb_n])\n\nmodel = Model([in_a, in_p, in_n], triplet_loss_layer)\n\nos.chdir('/kaggle/input/lfw-checkpoints')\nmodel.load_weights('ckpts/epoch010_loss2.664.hdf5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model = model.layers[3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class IdentityMetadata():\n    def __init__(self, base, name, file):\n        # dataset base directory\n        self.base = base\n        # identity name\n        self.name = name\n        # image file name\n        self.file = file\n\n    def __repr__(self):\n        return self.image_path()\n\n    def image_path(self):\n        return os.path.join(self.base, self.name, self.file) \n    \ndef load_metadata(path, upper_limit):\n    metadata = []\n    count = 0\n    for i in sorted(os.listdir(path)):\n        if count == upper_limit: \n            break\n            \n        for f in sorted(os.listdir(os.path.join(path, i))):\n            if count == upper_limit: \n                break\n                \n            count += 1\n            # Check file extension. Allow only jpg/jpeg' files.\n            ext = os.path.splitext(f)[1]\n            if ext == '.jpg' or ext == '.jpeg':\n                metadata.append(IdentityMetadata(path, i, f))\n    return np.array(metadata)\n\ndef load_image(path):\n    img = cv2.imread(path, 1)\n    # OpenCV loads images with color channels\n    # in BGR order. So we need to reverse them\n    return img[...,::-1]\n    \ndef align_image(img):\n        os.chdir('/kaggle/input/models-1')\n        alignment = AlignDlib('models1/landmarks.dat')\n        bb = alignment.getLargestFaceBoundingBox(img)\n        if bb is None:\n            return cv2.resize(img, (96,96))\n        else:\n            return alignment.align(96, \n                                   img, \n                                   bb,\n                                   landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir('/kaggle/input/fire-pictures')\nmetadata = load_metadata('data/fire_data', 144)\n\nos.chdir('/kaggle/input/models-1')\nalignment = AlignDlib('models1/landmarks.dat')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir('/kaggle/input/fire-pictures')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings = np.empty((metadata.shape[0], 128))\nids1 = []\nfor i, m in enumerate(metadata):\n    print(m.image_path())\n    img = load_image(m.image_path())\n    img = align_image(img)\n    print('test')\n    os.chdir('/kaggle/input/fire-pictures')\n    img = img.astype('float32')\n    img = img / 255.0\n    img = np.expand_dims(img, axis = 0)\n    name = m.image_path().split('/')[2].replace('_', ' ')\n    embeddings[i] = base_model.predict(img)\n    if(idict.get(name)) :  \n      ids1.append(idict[name])\n    else:\n      ids1.append(-1)\n    \n    if i%1 == 0: print(i, end = \" \")\n    print(ids1[i])\n    if i == len(metadata) - 1: print('done')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def distance(emb1, emb2):\n    return np.sum(np.square(emb1 - emb2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def knockknock(pth):\n    mini = 100\n    idd = -1    \n    for i in range(43):\n      d = distance2(i * 3, pth)\n      if d < mini and ids1[i * 3] > -1:\n          idd = ids1[i * 3]\n          mini = d\n\n    return idd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def distance2(img1, img2):\n  return distance(embeddings[img1], predictor(img2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predictor(pth):\n  img = load_image(pth)\n  img = align_image(img)\n  img = img.astype('float32')\n  img = img / 255.0\n  img = np.expand_dims(img, axis = 0)\n  return base_model.predict(img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def match(pth): \n  return idict2[knockknock(pth)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parser = argparse.ArgumentParser(description='DETR args parser', parents=[get_args_parser()])\nargs = parser.parse_args(args=[])\nos.chdir('/kaggle/input/models-1/models1')\nargs.resume = 'checkpoint.pth'\nargs.device = 'cpu'\n\nif args.output_dir:\n  Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n\nargs.distributed = False\n\nprint(args)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model, criterion, postprocessors = build_model(args)\n\ndevice = torch.device(args.device)\nmodel.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_dir = Path(args.output_dir)\nif args.resume:\n  if args.resume.startswith('https'):\n    checkpoint = torch.hub.load_state_dict_from_url(args.resume, map_location='cpu', check_hash=True)\n  else:\n    checkpoint = torch.load(args.resume, map_location='cpu')\n  \n  model.load_state_dict(checkpoint['model'], strict=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# COCO classes\nCLASSES = [\n   'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n   'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n   'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n   'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',\n   'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n   'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n   'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n   'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n   'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n   'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n   'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n   'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n   'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n   'toothbrush'\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# colors for visualization\nCOLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n\n# standard PyTorch mean-std input image normalization\ntransform = T.Compose([\n    T.ToTensor(),\n    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# for output bounding box post-processing\ndef box_cxcywh_to_xyxy(x):\n    x_c, y_c, w, h = x.unbind(1)\n    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n    return torch.stack(b, dim=1)\n\ndef rescale_bboxes(out_bbox, size):\n    img_w, img_h = size\n    b = box_cxcywh_to_xyxy(out_bbox)\n    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n    return b","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def detect(im, model, transform):\n  img = transform(im).unsqueeze(0)\n  assert img.shape[-2] <= 1600 and img.shape[-1] <= 1600, 'demo model only supports images up to 1600 pixels on each side'\n  outputs = model(img)\n  probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n  keep = probas.max(-1).values > 0.7\n  bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n  return probas[keep], bboxes_scaled","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_lite(pil_img, prob, reslts, classes = ['person']):\n  plt.figure(figsize=(16,10))\n  plt.imshow(pil_img)\n  ax = plt.gca()\n  for p, (text, (xmin, ymin, xmax, ymax)), c in zip(prob, reslts, COLORS * 100):\n    cl = p.argmax()\n    if CLASSES[cl] not in classes:\n      continue\n    ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color=c, linewidth=3))\n    \n    ax.text(xmin, ymin, text, fontsize=15, bbox=dict(facecolor='yellow', alpha=0.5))\n  plt.axis('off')\n  plt.show\n  plt.close","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def detect_and_draw(ipth):\n  dimg = PIL.Image.open(ipth)\n  prob, boxs = detect(dimg, model, transform)\n  outs = []\n\n  if not os.path.isdir(\"/kaggle/working\"):\n    os.mkdir(\"/kaggle/working\") \n    \n  for i,j in enumerate(boxs.tolist()):\n    copped = dimg.crop(j)\n    cpth = \"/kaggle/working/IMG_\" + str(i) + \".jpg\"\n    copped.save(cpth)\n    outs.append([cpth, j])\n\n  # Replace f with the function that takes in a file path and returns the name of the person in the image\n  # like the filepath will be the cropped face (by itself) saved as a .jpg, so the function just has to tell who it is\n\n  # i.e. if the function is called \"make_name\" then replace\n  #      the current definition of f with\n  #      f = make_name\n\n  # Currently it does nothing\n\n  f = match\n  outs = [(f(path), dims) for path,dims in outs]\n\n  return dimg, prob, outs\n  #return outs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir('/kaggle/input/competition')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img, prob, result_a = detect_and_draw('a.jpg')\nplot_lite(img, prob, result_a)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir('/kaggle/input/competition')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img, prob, result_b = detect_and_draw('b.jpg')\nplot_lite(img, prob, result_b)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir('/kaggle/input/competition')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img, prob, result_c = detect_and_draw('c.jpg')\nplot_lite(img, prob, result_c)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir('/kaggle/input/competition')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img, prob, result_d = detect_and_draw('d.jpg')\nplot_lite(img, prob, result_d)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def csv_rite(fles):\n  heads = [\"id\", \"xmin\", \"xmax\", \"ymin\", \"ymax\"]\n\n  with open('/kaggle/working/copped.csv', 'w', encoding='UTF8') as f:\n    writer = csv.writer(f)\n    writer.writerow(heads)\n\n    for rslt in fles:\n      for ln in rslt[1]:\n        k = [rslt[0] + \"_\" + str(idict[ln[0]])]\n        k.extend(list(map(int, ln[1])))\n        x = k[2]\n        k[2] = k[3]\n        k[3] = x\n        print(k)\n        writer.writerow(k)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"csv_rite([[\"a\", result_a]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"csv_rite([[\"a\", result_a], [\"b\", result_b], [\"c\", result_c], [\"d\", result_d]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def csv_rite2(ourdata, sampledata):\n  odict = {}\n  with open(sampledata, mode = 'r') as smpl:\n    tdict = csv.DictReader(smpl)\n    for r in tdict:\n      odict[r[\"id\"]] =  [0, 0, 0, 0]\n\n  with open(ourdata, mode = 'r') as smpl:\n    tdict = csv.DictReader(smpl)\n    for r in tdict:\n      if odict.get(r[\"id\"]):\n        odict[r[\"id\"]] =  [r[\"xmin\"], r[\"xmax\"], r[\"ymin\"], r[\"ymax\"]]\n\n  heads = [\"id\", \"xmin\", \"xmax\", \"ymin\", \"ymax\"]\n\n  print(odict)\n\n  with open('/kaggle/working/output.csv', mode='w', encoding='UTF8') as f:\n    writer = csv.writer(f)\n    writer.writerow(heads)\n\n    for (pids, crds) in odict.items():\n      k = [pids]\n      k.extend(crds)\n      #print(k)\n      writer.writerow(k)\n ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir('/kaggle')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.getcwd()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -a","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"csv_rite2(\"working/copped.csv\", \"input/sample-submission/kaggle_sample_submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}