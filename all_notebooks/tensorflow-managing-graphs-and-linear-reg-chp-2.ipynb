{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"**TensorFlow chapter 2 :**<br>\n**In this chapter we will cover three topic's.**\n* Managing Graphs.\n* Lifecycle of a Node value.\n* Linear Regression with TensorFlow \n\n<center><h1> Managing Graphs</h1></center>\n\nAny **Node** we create is automatically added to the **default graph**."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\n'''creating a variable'''\nx = tf.Variable(1 , name = 'x')\n'''checking whether the variable is stored as node value in the default graph.'''\nx.graph is tf.get_default_graph()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58d371dd323d14f7c1c5c10242afda0d8225ef35"},"cell_type":"markdown","source":"So we know that a variable is stored in a default graph as a node.  <br>\nIn most of the cases this is fine  , but sometimes we may **want to manage multiple independent graphs**.<br>\nWe can do this by creating a **new graph** and **temporarily** making it the **default graph** inside a **with** block , like so :"},{"metadata":{"trusted":true,"_uuid":"9a7157aa25af706b175f93e747395ce9215afd73"},"cell_type":"code","source":"'''creating a temporary graph'''\ngraph = tf.Graph()\nwith graph.as_default():\n    '''Creating a variable in temporary graph.'''\n    x1 = tf.Variable(2 , name = 'x1')\n    \nx1.graph is graph ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c8744a4977204688118abb571085effe82867001"},"cell_type":"code","source":"x1.graph is tf.get_default_graph()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b375432500559090f7b896a5ba4f8c4f109b835c"},"cell_type":"markdown","source":"In jupyter it is common to run the same commands more than once while we are experimenting . As a result , we may end up with a default graph containing many containing many duplicate nodes. One solution is to restart the jupyter kernel , but a more convinent solution is to just reset the default graph by running **tf.reset_default_graph()**."},{"metadata":{"_uuid":"40ccbeba3ae93668638f12eed74f968a64fa5530"},"cell_type":"markdown","source":"<center> <h1> Lifecycle of a <b>NODE VALUE</b> </h1> </center>\n\nWhen we evaluate a node , **TensorFlow automatically determines the set of nodes that it depends on and it evaluates these nodes first.**<br>\nFor example :"},{"metadata":{"trusted":true,"_uuid":"d618aa67b0364e7d22ac953cf13d48b70592c58c"},"cell_type":"code","source":"'''first lets reset the graph'''\ntf.reset_default_graph()\n\n'''variables'''\nw = tf.constant(2)\nx = w + 3\ny = x + 5\nz = x * 3\n\n'''initializing a session'''\nwith tf.Session() as session:\n    print(y.eval()) #10\n    print(z.eval()) #15","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"464984e4ad5d6abb5a63b993f1a239bd549b9d16"},"cell_type":"markdown","source":"* First , the code will define a very simple graph. (**Construction phase**)\n* Then it starts the session \n* Then it runs the graph to evaluate **y**.  (**Execution phase**).\n     * TensorFlow automatically detects that **y is dependent on x** and **x is dependent on w**.\n     * First it will evaluate **w** , then **x**, then **y** .\n     * And returns the value of **y**.\n* Finally the code runs to the graph to evaluate **z**\n     * TensorFlow detects that **z is dependent on x  and x is depented on w**\n     * It will first evaluate **w** , then **x** , then **z** ,and returns the value of **z**.\n* If we notice that TensorFlow does not reuses the result of previous evaluation **w** and **x**. Inshort , the preceding code evaluates **w** and **x** twice.\n\n<br>\nAll **node values** are dropped between graph runs , except variable values , which are maintained by the session across the graph runs.<br>\nA Variable starts its life when its initializer is run , and it ends when the session is closed.\n<br>\nIf we want to evaluate **y and z** effeciently , without evaluating **w** and **x** twice as we did in the previous code , we must ask TensorFlow to evaluate both **y** and **x** in just one graph run , Example\n"},{"metadata":{"trusted":true,"_uuid":"b3a39f011954542e58362a046196b15b550031ba"},"cell_type":"code","source":"with tf.Session() as session :\n    '''Commanding TensorFlow to evaluate y and z , without evaluating w and x twice'''\n    y_val , z_val = session.run([y , z ])\n    print(y_val)\n    print(z_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac23df8fc544a32106307aea046e56937d3331e0"},"cell_type":"markdown","source":"<center><h1><b>Linear Regression</b> with TensorFlow</h1></center>\n\n* **TensorFlow operations (ops)** can take any number of of inputs and produce any number of outputs . For example , the addition and multiplication  ops each take two inputs and produce one output. Constant and variables take no input , they are called as **source ops**\n* The inputs and outputs are multidimensional arrays, called as **TENSORS** (hence the name tensor flow).\n* Just like NumPy arrays , **tensors** have a type and a shape. In fact , the Python API tensors are simply represented by **NumPy ndarrays**. We know that numpy ndarrays typically contain floats , but we can also use them to carry **strings** . In the examples so far , the **tensors** just contained a single scalar value.\n* Now we will write a code which manipulates **2D arrays** to perform **Linear Regression** on the **Boston House pricing dataset**."},{"metadata":{"trusted":true,"_uuid":"f8eb2b4daef91f20aa5da682ba337ec81e35e896"},"cell_type":"code","source":"'''importing the Boston House pricing dataset'''\nfrom sklearn.datasets import load_boston\nhousing = load_boston()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e936a7394544adcca36ca2f971595e6790720ccb"},"cell_type":"markdown","source":"1. **Now lets add a bias input feature ($x_{0}$ =   1)  to all training instances.** <br>\nnote : - Formula of **Linear Regression : <h2> $h(x) = \\theta_{0} + x_{1}*\\theta_{1}  + ...... + x_{n} * \\theta_{n} $ </h2>, where $\\theta_{0}$ is the bias value or intercept , $x_{1}...x{n}$ are the independent variables , and $\\theta_{1}...\\theta_{n}$ are the feature parameters or slope . Formula in matrix format is $\\theta^T\\cdotp x$** <br> \n"},{"metadata":{"trusted":true,"_uuid":"18bde58a385296b67162cb2898468a6f577fbe81"},"cell_type":"code","source":"import numpy as np\n\nm , n = housing.data.shape\n'''adding or concating bias input feature'''\nhousing_plus_bias = np.c_[np.ones( (m , 1) ) , housing.data]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4641f89463d6008928001d4172a555474dc437ce"},"cell_type":"markdown","source":"Creating two **TensorFlow constant nodes , X and y**<br>\n* **X** =  independent feature matrix.\n* **y** = dependent feature vectore."},{"metadata":{"trusted":true,"_uuid":"1592fe6f4bf55c7e0ae5e317854bc9d2cb1b9d76"},"cell_type":"code","source":"X = tf.constant(housing_plus_bias , dtype = tf.float32 , name = \"X\")\ny = tf.constant(housing.target.reshape(-1 , 1) , dtype = tf.float32 , name = \"y\")\n\n'''even create X transpose which will be usefull for theta evaluation.'''\nXT = tf.transpose(X)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5fb26e5006590c76122b715909994a5b054c70a7"},"cell_type":"markdown","source":"* Now that we have **X** and **y** , it is time to fit the parameter  $\\theta$ .<br>\n* Here we will use **Normal Equation** to find the optimized value of $\\theta$.\n* Formula for **Normal Equation**  : <h2>$\\theta = (X^T \\cdotp X)^{-1} \\cdotp X^T \\cdotp y$ </h2>\n* Here we will use **Matrix Operation** provide by TensorFlow eg , matmul() , matrix_inverse() . "},{"metadata":{"trusted":true,"_uuid":"6a256dcb78efcd75b0f250501465076657a64ef6"},"cell_type":"code","source":"'''This line of code does not performs any kin of computaion , instead it creates a nodes\nin the graph'''\ntheta = tf.matmul( tf.matmul( tf.matrix_inverse( tf.matmul(XT , X ) ) , XT ) , y)\n\n'''Initializing session'''\nwith tf.Session() as session:\n    '''evaluating theta'''\n    theta_value = theta.eval()\nprint(theta_value)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0512d704cb26e2e08caed01b4c4c4f92723639db"},"cell_type":"markdown","source":"**Below are the parameters ($\\theta$) learned by our model.**<br>\n* The main benefit of this **code versus computing the Normal Equation** directly using **NumPy** is that TensorFlow will automatically run this on your **GPU** card if you have one.\n<br>\n<br>\n* **In the next chapter we will implement Gradient Descent to fit the parameter $\\theta$  manually and by using an Optimizer (tf.train.Gradient).**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}