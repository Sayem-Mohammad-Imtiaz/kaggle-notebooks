{"cells":[{"metadata":{"id":"BVE3_mzgJW5X"},"cell_type":"markdown","source":"#**Using Predictive Analysis To Predict Diagnosis of a Breast Tumor**\n-By Mohd.Shoaib Nadaf\n\n## 1. Identify the problem\nBreast cancer is the most common malignancy among women, accounting for nearly 1 in 3 cancers diagnosed among women in the United States, and it is the second leading cause of cancer death among women. Breast Cancer occurs as a results of abnormal growth of cells in the breast tissue, commonly referred to as a Tumor. A tumor does not mean cancer - tumors can be benign (not cancerous), pre-malignant (pre-cancerous), or malignant (cancerous). Tests such as MRI, mammogram, ultrasound and biopsy are commonly used to diagnose breast cancer performed.\n\n## 1.1 Expected outcome\nGiven breast cancer results from breast fine needle aspiration (FNA) test (is a quick and simple procedure to perform, which removes some fluid or cells from a breast lesion or cyst (a lump, sore or swelling) with a fine needle similar to a blood sample needle). Since this build a model that can classify a breast cancer tumor using two training classification:\n\n1= Malignant (Cancerous) - Present\n0= Benign (Not Cancerous) -Absent\n\n## 1.2 Objective\nSince the labels in the data are discrete, the predication falls into two categories, (i.e. Malignant or benign). In machine learning this is a classification problem.\n\nThus, the goal is to classify whether the breast cancer is benign or malignant and predict the recurrence and non-recurrence of malignant cases after a certain period. To achieve this we have used machine learning classification methods to fit a function that can predict the discrete class of new input.\n\n## 1.3 Identify data sources\nThe Breast Cancer datasets is available machine learning repository maintained by the University of California, Irvine. The dataset contains 569 samples of malignant and benign tumor cells.\n\nThe first two columns in the dataset store the unique ID numbers of the samples and the corresponding diagnosis (M=malignant, B=benign), respectively.\nThe columns 3-32 contain 30 real-value features that have been computed from digitized images of the cell nuclei, which can be used to build a model to predict whether a tumor is benign or malignant.\n\n\n\n"},{"metadata":{"id":"S1o3NWfZKi7L"},"cell_type":"markdown","source":"**Getting Started: Load libraries and set options**\n"},{"metadata":{"id":"__nSKATvgrUW","trusted":true},"cell_type":"code","source":"#Breast cancer Detection\n#importing library\nimport pandas as pd \nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","execution_count":null,"outputs":[]},{"metadata":{"id":"5zo2nIZ4LVQB"},"cell_type":"markdown","source":"**Load Dataset**\n"},{"metadata":{"id":"fFY7YEBdhK_d","outputId":"424976c7-ca15-4653-c7fc-9a543717f588","trusted":true},"cell_type":"code","source":"#Load the data\n#from google.colab import files \n#uploaded = files.upload()\ndf = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')\ndf.head(7);","execution_count":null,"outputs":[]},{"metadata":{"id":"zaWQMvYZLuh8"},"cell_type":"markdown","source":"**Inspecting the data**"},{"metadata":{"id":"ZLqj7CBBlOJT","outputId":"7b453943-4331-4d40-c8c9-ed6075af08d7","trusted":true},"cell_type":"code","source":"print(df.head())\n","execution_count":null,"outputs":[]},{"metadata":{"id":"K-H2gMGfL35N"},"cell_type":"markdown","source":"**Data Cleaning:**"},{"metadata":{"id":"Z3YtZDmGnPSX","outputId":"64ebee7c-146a-4e69-a203-c2e350e1eb46","trusted":true},"cell_type":"code","source":"#Count the number of rows and columns \nprint(\"(rows,cols)\",df.shape,\"rows means no of patients\" )\n","execution_count":null,"outputs":[]},{"metadata":{"id":"90Wpf3Bym0kk","outputId":"1bcec03f-3267-4125-df15-74143af2fa9c","trusted":true},"cell_type":"code","source":"#Count the number of empty values in each column (NAN,NaN,na)\ndf.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"sW0o_U4wn3yD"},"cell_type":"markdown","source":"We Found here , Unnamed col has 569 Na values so we will drop the column \n"},{"metadata":{"id":"DhXRguTKoBza","trusted":true},"cell_type":"code","source":"#drop col\ndf = df.dropna(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"9eEQJMrDoTrZ","outputId":"7f43e8b3-b1b3-46b4-b59c-28e89ac3e1d2","trusted":true},"cell_type":"code","source":"#count the No of rows and cols\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"zAz1g038qC91","outputId":"55ad90ff-7df0-46f5-99a9-fda54b598536","trusted":true},"cell_type":"code","source":"#count the number of Malognant - M and Benign - B \ndf['diagnosis'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"D74BkJn9qVdN","outputId":"4d0609bf-5766-4633-f401-313c7e24ed8e","trusted":true},"cell_type":"code","source":"#visualize the count \nsns.countplot(df['diagnosis'],label = 'count')","execution_count":null,"outputs":[]},{"metadata":{"id":"YbuL0dPqrF-q","outputId":"d0c91e5d-8116-467d-d3b5-2ae8d94dd944","trusted":true},"cell_type":"code","source":"#data type of df\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{"id":"GwQahEBbMzg0"},"cell_type":"markdown","source":"From the results above, diagnosis is a categorical variable, because it represents a fix number of possible values (i.e, Malignant, of Benign. The machine learning algorithms wants numbers, and not strings, as their inputs so we need some method of coding to convert them."},{"metadata":{"id":"G8CO4BO3rPMj","outputId":"2864dab0-304b-4531-b3ea-129ad57829b7","trusted":true},"cell_type":"code","source":"#Encoding the catagorial data values \nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder_Y = LabelEncoder()\nlabelencoder_Y.fit_transform(df.iloc[:,1].values)\ndf.iloc[:,1]","execution_count":null,"outputs":[]},{"metadata":{"id":"jWA-n-RSNAP_"},"cell_type":"markdown","source":"## **Exploratory Data Analysis**\n\n## Objectives of Data Exploration\n**Exploratory data analysis (EDA)** is a very important step which takes place after feature engineering and acquiring data and it should be done before any modeling. This is because it is very important for a data scientist to be able to understand the nature of the data without making assumptions. The results of data exploration can be extremely useful in grasping the structure of the data, the distribution of the values, and the presence of extreme values and interrelationships within the data set.\n\n### The purpose of EDA is:\n\nto use summary statistics and visualizations to better understand data, *find clues about the tendencies of the data, its quality and to formulate assumptions and the hypothesis of our analysis\nFor data preprocessing to be successful, it is essential to have an overall picture of your data Basic statistical descriptions can be used to identify properties of the data and highlight which data values should be treated as noise or outliers.**\n\nNext step is to explore the data. There are two approached used to examine the data using:\n\n**Descriptive statistics** is the process of condensing key characteristics of the data set into simple numeric metrics. Some of the common metrics used are mean, standard deviation, and correlation.\n\n**Visualization** is the process of projecting the data, or parts of it, into Cartesian space or into abstract images. In the data mining process, data exploration is leveraged in many different steps including preprocessing, modeling, and interpretation of results."},{"metadata":{"id":"hHLY6e8eNn_4","outputId":"7123bc55-3c37-4069-c058-78a84fdba381","trusted":true},"cell_type":"code","source":"#basic descriptive statistics\ndf.describe()\ndf.skew()","execution_count":null,"outputs":[]},{"metadata":{"id":"8Zo2fES4OEAd"},"cell_type":"markdown","source":"The skew result show a positive (right) or negative (left) skew. Values closer to zero show less skew. From the graphs, we can see that radius_mean, perimeter_mean, area_mean, concavity_mean and concave_points_mean are useful in predicting cancer type due to the distinct grouping between malignant and benign cancer types in these features. We can also see that area_worst and perimeter_worst are also quite useful."},{"metadata":{"id":"nmh13yBSPK1J"},"cell_type":"markdown","source":"## **Unimodal Data Visualizations**\nOne of the main goals of visualizing the data here is to observe which features are most helpful in predicting malignant or benign cancer. The other is to see general trends that may aid us in model selection and hyper parameter selection.\n\nApply 3 techniques that you can use to understand each attribute of your dataset independently.\n\n- Histograms.\n- Density Plots.\n- Box and Whisker Plots."},{"metadata":{"id":"2UCAmUS8OFkL","trusted":true},"cell_type":"code","source":"sns.set_style(\"white\")\nsns.set_context({\"figure.figsize\": (10, 8)})","execution_count":null,"outputs":[]},{"metadata":{"id":"YZduJRZCPbiN"},"cell_type":"markdown","source":"## Visualise distribution of data via histograms\nHistograms are commonly used to visualize numerical variables. A histogram is similar to a bar graph after the values of the variable are grouped (binned) into a finite number of intervals (bins).\n\nHistograms group data into bins and provide you a count of the number of observations in each bin. From the shape of the bins you can quickly get a feeling for whether an attribute is Gaussian, skewed or even has an exponential distribution. It can also help you see possible outliers.\n\n**Separate columns into smaller dataframes to perform visualization**"},{"metadata":{"id":"1INknyoYPYLZ","trusted":true},"cell_type":"code","source":"data_id_diag=df.loc[:,[\"id\",\"diagnosis\"]]\ndata_diag=df.loc[:,[\"diagnosis\"]]\n\n#For a merge + slice:\ndata_mean=df.iloc[:,1:11]\ndata_se=df.iloc[:,11:22]\ndata_worst=df.iloc[:,23:]","execution_count":null,"outputs":[]},{"metadata":{"id":"AEx-yM7RQibP"},"cell_type":"markdown","source":"## Histogram the \"_mean\" suffix designition"},{"metadata":{"id":"0REdXudoQlDO","outputId":"52597d35-ee21-4cbb-9177-00462ca09f80","trusted":true},"cell_type":"code","source":"hist_mean=data_mean.hist(bins=10, figsize=(15, 10),grid=False,)","execution_count":null,"outputs":[]},{"metadata":{"id":"w-jZjsKQQnIJ"},"cell_type":"markdown","source":"## Histogram the \"_se\" suffix designition"},{"metadata":{"id":"5WAPEicmQu64","outputId":"a82e9df8-cfbb-4ea3-fd80-60e365e4bdc5","trusted":true},"cell_type":"code","source":"hist_se=data_se.hist(bins=10, figsize=(15, 10),grid=False,)","execution_count":null,"outputs":[]},{"metadata":{"id":"6BbxwSjjQzHZ"},"cell_type":"markdown","source":"## Histogram the \"_worst\" suffix designition"},{"metadata":{"id":"7U5I0noBQ0CL","outputId":"dce7be58-ccc9-4f69-baba-834bae754b50","trusted":true},"cell_type":"code","source":"hist_worst=data_worst.hist(bins=10, figsize=(15, 10),grid=False,)","execution_count":null,"outputs":[]},{"metadata":{"id":"bReQmcUiRG4u"},"cell_type":"markdown","source":"**Observation**\n\nWe can see that perhaps the attributes concavity,and concavity_point may have an exponential distribution ( ). We can also see that perhaps the texture and smooth and symmetry attributes may have a Gaussian or nearly Gaussian distribution. This is interesting because many machine learning techniques assume a Gaussian univariate distribution on the input variables."},{"metadata":{"id":"xfvUsvDpRISc"},"cell_type":"markdown","source":"## Visualize distribution of data via density plots\n**Density plots \"_mean\" suffix designition**"},{"metadata":{"id":"i0SNuKlrRaKc"},"cell_type":"markdown","source":"## Density plots \"_mean\" suffix designition"},{"metadata":{"id":"3NPUeE51ROZI","outputId":"eedb8928-4d37-4811-a8ea-58bc73b9ca75","trusted":true},"cell_type":"code","source":"#Density Plots\nplt = data_mean.plot(kind= 'density', subplots=True, layout=(4,3), sharex=False, \n                     sharey=False,fontsize=12, figsize=(15,10))","execution_count":null,"outputs":[]},{"metadata":{"id":"GR992xzMRirH"},"cell_type":"markdown","source":"##Density plots \"_se\" suffix designition"},{"metadata":{"id":"t9Z-s8JSRonJ","outputId":"3a523b5c-1291-40e7-d571-f6f91f4c9cc5","trusted":true},"cell_type":"code","source":"#Density Plots\nplt = data_se.plot(kind= 'density', subplots=True, layout=(4,3), sharex=False, \n                     sharey=False,fontsize=12, figsize=(15,10))","execution_count":null,"outputs":[]},{"metadata":{"id":"mnJ2x3EmRnjN"},"cell_type":"markdown","source":"##Density plots \"_worst\" suffix designition"},{"metadata":{"id":"NTnnLM-RR0Oe","outputId":"d1afc4bf-297d-4ee9-e9ea-80dac7fe710a","trusted":true},"cell_type":"code","source":"#Density Plots\nplt = data_worst.plot(kind= 'density', subplots=True, layout=(4,3), sharex=False, \n                    sharey=False,fontsize=12, figsize=(15,10))\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"IktHwjs2UujN"},"cell_type":"markdown","source":"**Observation**\n\nWe can see that perhaps the attributes perimeter,radius, area, concavity,ompactness may have an exponential distribution ( ). We can also see that perhaps the texture and smooth and symmetry attributes may have a Gaussian or nearly Gaussian distribution. This is interesting because many machine learning techniques assume a Gaussian univariate distribution on the input variables."},{"metadata":{"id":"6bKXqTbeVA1k"},"cell_type":"markdown","source":"## **Visualise distribution of data via box plots**\n## Box plot \"_mean\" suffix designition"},{"metadata":{"id":"4Sua8lH6VU3e","outputId":"e705fbe2-a006-4043-e8fa-6b5321fef820","trusted":true},"cell_type":"code","source":"# box and whisker plots\nplt=data_mean.plot(kind= 'box' , subplots=True, layout=(4,4), sharex=False, sharey=False,fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{"id":"cISn8VEUVOlY"},"cell_type":"markdown","source":"## Box plot \"_se\" suffix designition"},{"metadata":{"id":"lP17Pa6SVaRT","outputId":"09158f44-1b1a-4655-bfd3-dff420d85b1e","trusted":true},"cell_type":"code","source":"# box and whisker plots\nplt=data_se.plot(kind= 'box' , subplots=True, layout=(4,4), sharex=False, sharey=False,fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{"id":"zVzRvBhCVQ8I"},"cell_type":"markdown","source":"## Box plot \"_worst\" suffix designition"},{"metadata":{"id":"fOZtz6IsVeQi","outputId":"8344f545-e8b1-420f-f355-e63af5ebc924","trusted":true},"cell_type":"code","source":"# box and whisker plots\nplt=data_worst.plot(kind= 'box' , subplots=True, layout=(4,4), sharex=False, sharey=False,fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{"id":"PKuIptS6VnBQ"},"cell_type":"markdown","source":"**Observation**\n\nWe can see that perhaps the attributes perimeter,radius, area, concavity,ompactness may have an exponential distribution ( ). We can also see that perhaps the texture and smooth and symmetry attributes may have a Gaussian or nearly Gaussian distribution. This is interesting because many machine learning techniques assume a Gaussian univariate distribution on the input variables."},{"metadata":{"id":"Hb96rmqhVtH3"},"cell_type":"markdown","source":"## **Multimodal Data Visualizations**"},{"metadata":{"id":"b7SV3fnMWeGe"},"cell_type":"markdown","source":"**Scatter Plot**"},{"metadata":{"id":"qmk58h9gWkv_","outputId":"b4a48cf6-9f80-41a8-fb6f-8de6c35b6a35","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nsns.set_style(\"white\")\n\n# Compute the correlation matrix\ncorr = data_mean.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\ndf, ax = plt.subplots(figsize=(8, 8))\nplt.title('Breast Cancer Feature Correlation')\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(260, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, vmax=1.2, square='square', cmap=cmap, mask=mask, \n            ax=ax,annot=True, fmt='.2g',linewidths=2)","execution_count":null,"outputs":[]},{"metadata":{"id":"afKLrYaOW0w7"},"cell_type":"markdown","source":"**Observation:**\n\nWe can see strong positive relationship exists with mean values paramaters between 1-0.75;.\n\n- The mean area of the tissue nucleus has a strong positive correlation with mean values of radius and parameter;\n\n- Some paramters are moderately positive corrlated (r between 0.5-0.75)are concavity and area, concavity and perimeter etc\n\n- Likewise, we see some strong negative correlation between fractal_dimension with radius, texture, parameter mean values."},{"metadata":{"id":"1pKZFaaUWhVK"},"cell_type":"markdown","source":"**Corelation Matrix**"},{"metadata":{"id":"wH3it-wr20bz","outputId":"37cfec4f-98f3-4dd6-cb29-24fd27369e59","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')\ndf = df.dropna(axis=1)\n#Encoding the catagorial data values \nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder_Y = LabelEncoder()\nlabelencoder_Y.fit_transform(df.iloc[:,1].values)\ndf.iloc[:,1]\n\n#create pair plot \n#index 1 to 6\nimport seaborn as sns\nsns.pairplot(df.iloc[:,1:8],hue='diagnosis')","execution_count":null,"outputs":[]},{"metadata":{"id":"bdMqjSEwWBXh"},"cell_type":"markdown","source":"**Summary**\n\n- Mean values of cell radius, perimeter, area, compactness, concavity and concave points can be used in classification of the cancer. Larger values of these parameters tends to show a correlation with malignant tumors.\n\n- mean values of texture, smoothness, symmetry or fractual dimension does not show a particular preference of one diagnosis over the other.\n\n- In any of the histograms there are no noticeable large outliers that warrants further cleanup."},{"metadata":{"id":"3KF_Ru0o3vhA","outputId":"372f5db9-4ed5-4959-c6af-1531bd79f48a","trusted":true},"cell_type":"code","source":"#coorelation in cols\ndf.iloc[:,2:].corr()","execution_count":null,"outputs":[]},{"metadata":{"id":"mcbdemjIMYVH","trusted":true},"cell_type":"code","source":"#split the dataset into independent x and dependent y \n\nX = df.iloc[:,2:31].values\nY = df.iloc[:,1].values","execution_count":null,"outputs":[]},{"metadata":{"id":"P1HW2yNvNUf9","trusted":true},"cell_type":"code","source":"#split dataset 75% into trainning and 25% into testing\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test= train_test_split(X,Y,test_size= 0.25 , random_state =0) ","execution_count":null,"outputs":[]},{"metadata":{"id":"d0wmPY6VYphx","outputId":"7dc77648-715c-49aa-9e38-b8f69d8c6822","trusted":true},"cell_type":"code","source":"# feature scaling / data normaization \n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)\n\nX_train","execution_count":null,"outputs":[]},{"metadata":{"id":"EyqKjs_ubia4","trusted":true},"cell_type":"code","source":"# create function\ndef models(X_train , Y_train):\n  #logistic regression \n\n  from sklearn.linear_model import LogisticRegression\n  log = LogisticRegression(random_state = 0)\n  log.fit(X_train,Y_train)\n\n  #Decison Treee\n\n  from sklearn.tree import DecisionTreeClassifier\n  tree = DecisionTreeClassifier(criterion = 'entropy' ,random_state = 0 )\n  tree.fit(X_train,Y_train)\n\n  #Random Forest\n  from sklearn.ensemble import RandomForestClassifier\n  forest =  RandomForestClassifier(n_estimators = 10 , criterion = 'entropy', random_state = 0)\n  forest.fit(X_train,Y_train)\n  #Print the models \n  print('[0] logistic Regression ', log.score(X_train,Y_train))\n  print('[1] Decision Tree ', tree.score(X_train,Y_train))\n  print('[2] Random Forest ', forest.score(X_train,Y_train))\n  return log,tree,forest","execution_count":null,"outputs":[]},{"metadata":{"id":"EGa6BdbA8tlS","outputId":"1cc7c5aa-4b3a-427d-a1ce-0c8e8a238db5","trusted":true},"cell_type":"code","source":"#getting all models \nmodel = models(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"hM4uVanpqj8u","outputId":"f655d679-63fa-47ff-cc4f-cf16bce8c5b3","trusted":true},"cell_type":"code","source":"#test model accuracy on test data on confusion matrix \n\nfrom sklearn.metrics import confusion_matrix\nfor i in range(len(model)):\n  print('model:',i)\n  cm = confusion_matrix(Y_test,model[0].predict(X_test))\n\n  TP = cm[0][0]\n  TN = cm[1][1]\n  FN = cm[1][0]\n  FP = cm[0][1]\n  print(cm)\n  print('Testing accurancy : ', (TP+TN)/(TP+TN+FN+FP))\n  print()","execution_count":null,"outputs":[]},{"metadata":{"id":"p67IrV_-rMTm"},"cell_type":"markdown","source":"[[True positive False postive]\n\n [False negative True negative]] "},{"metadata":{"id":"D3k-HlG8stjN","outputId":"886470e3-b2c9-4cfa-8b78-673d6f97a929","trusted":true},"cell_type":"code","source":"#show another way to get matrics of the model \n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\n\nfor i in range(len(model)):\n  print('model :',i)\n  print(classification_report(Y_test,model[i].predict(X_test)))\n  print(accuracy_score(Y_test,model[i].predict(X_test)))\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"TPUBqFdRt0p6","outputId":"6872c275-eee9-469c-86db-3a1ac1763728","trusted":true},"cell_type":"code","source":"#print the prediction of Random forest classifier model \npred = model[2].predict(X_test)\nprint(pred)\nprint()\nprint(Y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"TMDRJwu8kWyx"},"cell_type":"markdown","source":"## Saving the Output into CSV file \n- column1 = Actual\n- column2 = predicted "},{"metadata":{"id":"Qtp24bMabbM4","outputId":"dd79a2cd-18af-4e48-bf2b-0b8caabec461","trusted":true},"cell_type":"code","source":"op = pd.DataFrame(Y_test,pred)\n\n#save the Op dataframe into csv\nop_file = op.to_csv('pred_op.csv')\n\nop","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}