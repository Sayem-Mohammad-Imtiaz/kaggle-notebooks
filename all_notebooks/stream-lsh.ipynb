{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* > # EE226 - Coding 2\n## Streaming algorithm & Locality Sensitive Hashing","metadata":{}},{"cell_type":"markdown","source":"### Streaming: DGIM","metadata":{}},{"cell_type":"markdown","source":"DGIM is an efficient algorithm in processing large streams. When it's infeasible to store the flowing binary stream, DGIM can estimate the number of 1-bits in the window. In this coding, you're given the *stream_data.txt* (binary stream), and you need to implement the DGIM algorithm to count the number of 1-bits. Write code and ask the problems below.","metadata":{}},{"cell_type":"markdown","source":"### Your task","metadata":{}},{"cell_type":"markdown","source":"1. Set the window size to 1000, and count the number of 1-bits in the current window.","metadata":{}},{"cell_type":"code","source":"# Your code here, you can add cells if necessary\n#读入数据并去掉空格转化为int列表格式\nimport time\nfrom math import *\n\nBucket = {}        # dictionary type to store buckets as following form: { key:2^i, value:[timestamp1,timestamp2...] ...}\nwindow_size = 1000    \nstoptimestamp = 1000  # the end location for searching, if 2000, it means we only search the first 2000 bits in the file\nsame_buckets_num = 2  # the max number of the buckets with the same size, if overpass, we do update/merge\n\n\nkeylist = []          # store the power of 2, within window_size\nfor i in range(int(log(window_size,2))+1):\n    key = int(pow(2,i))\n    keylist.append(key)\n    Bucket[key] = []     # create empty Bucket\n\n    \n\ndef DGIM(data,Bucket,keylist,window_size,same_buckets_num,stoptimestamp):\n    '''\n    param: data: the list with the 0-1 sequence to search\n    param: Bucket: the container for buckets\n    param: keylist: the list with the bucket size\n    param: window_size: the length for the sum of the current buckets' sizes\n    param: same_buckets_num: the max number of the buckets with the same size, if overpass, we do update/merge\n    param: stoptimestamp: the end location for searching\n    '''\n    start_time = time.time()\n    cnt = 0\n    timestamp = 0\n    \n    for i in range(stoptimestamp):\n        timestamp = (timestamp + 1) % window_size     # for each bit in, timestamp++, and we mod it by window_size in case \n                                                      # the stoptimestamp overpass window_size.\n        for key in Bucket:\n            for eachstamp in Bucket[key]:\n                if eachstamp == timestamp:            # if the stoptimestamp overpass window_size, the timestamps may\n                    Bucket[key].remove(eachstamp)  # yield conflicts, we check the same timestamps and remove in order\n                                                      # to avoid the confliction.\n                    \n        if data[i] == '1':\n            Bucket[1].append(timestamp)\n            for key in keylist:                              # check the buckets size\n                if len(Bucket[key]) > same_buckets_num:   # if overpass the max number, we do merge oepration\n                    Bucket[key].pop(0)\n                    tmpstamp = Bucket[key].pop(0)\n                    if key != keylist[-1]:\n                        Bucket[key*2].append(tmpstamp)\n                    else:\n                        Bucket[key].pop(0)\n                else:\n                    break\n    firststamp = 0                                 # find the first timestamp \n    for key in keylist:\n        if len(Bucket[key]) > 0:\n            firststamp = Bucket[key][0]    \n        for tmpstamp in Bucket[key]:\n            print(\"size of bucket: {}, with the timestamp: {}\".format(key,tmpstamp))\n    for key in keylist:\n        for tmpstamp in Bucket[key]:\n            if tmpstamp != firststamp:            # not the firststamp, we add all keys\n                cnt += key\n            else:\n                cnt += 0.5*key                    # the firststamp, we add half\n            \n    end_time = time.time()\n    return cnt,end_time-start_time\n    \nwith open('../input/coding2/stream_data.txt','r') as f:\n    data = f.read().split('\\t')\n    res, cost_time = DGIM(data,Bucket,keylist,window_size,same_buckets_num,stoptimestamp)\n    print(\"Estimated number of 1s in the last {} bits of all {} bits: {} with the costed time: {}\".format(window_size,stoptimestamp,res,cost_time))    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Write a function that accurately counts the number of 1-bits in the current window, and compare the difference between its running time and space and the DGIM algorithm.","metadata":{}},{"cell_type":"code","source":"# Your code here, you can add cells if necessary\nwindow_size = 1000\nstoptimestamp = 4000\n\n\ndef bruteforce(data,window_size,stoptimestamp):\n    '''\n    param: data: the list with the 0-1 sequence to search\n    param: window_size: the length for the sum of the current buckets' sizes\n    param: stoptimestamp: the end location for searching\n    '''\n    start_time = time.time()\n    cnt = 0\n    \n    for i in range(stoptimestamp-window_size,stoptimestamp):\n        if data[i] == '1':\n            cnt += 1\n    \n    end_time = time.time()\n    return cnt, end_time-start_time \n        \n\nwith open('../input/coding2/stream_data.txt','r') as f:\n    data = f.read().split('\\t')\n    res, cost_time = bruteforce(data,window_size,stoptimestamp)\n    print(\"Exact number of 1s in the last {} bits of all {} bits: {} with the costed time: {}\".format(window_size,stoptimestamp,res,cost_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Locality Sensitive Hashing","metadata":{}},{"cell_type":"markdown","source":"The locality sensitive hashing (LSH) algorithm is efficient in near-duplicate document detection. In this coding, you're given the *docs_for_lsh.csv*, where the documents are processed into set of k-shingles (k = 8, 9, 10). *docs_for_lsh.csv* contains 201 columns, where column 'doc_id' represents the unique id of each document, and from column '0' to column '199', each column represents a unique shingle. If a document contains a shingle ordered with **i**, then the corresponding row will have value 1 in column **'i'**, otherwise it's 0. You need to implement the LSH algorithm and ask the problems below.","metadata":{}},{"cell_type":"markdown","source":"### Your task","metadata":{}},{"cell_type":"markdown","source":"Use minhash algoirthm to create signature of each document, and find 'the most similar' documents under Jaccard similarity. \nParameters you need to determine:\n1) Length of signature (number of distinct minhash functions) *n*. Recommanded value: n > 20.\n\n2) Number of bands that divide the signature matrix *b*. Recommanded value: b > n // 10.","metadata":{}},{"cell_type":"code","source":"# Your code here, you can add cells if necessary\nimport numpy as np\nimport csv\nimport random\n\ntime = 0\ndata = []\nwith open('../input/coding2/docs_for_lsh.csv') as f:\n#with open('../input/newtest/test/test.csv') as f:\n    # effective row data length 1000000, id from column 0-199(up to 200)\n    csvmap = csv.reader(f)\n    for row in csvmap:\n        time += 1\n        if time == 1:               # pass the first row (store column id)\n            pass\n        else:\n            data.append(row[1:])    # pass the first column (store doc id)\n\ndata = np.array(data)\ndata = data.T                       # for there is 1000000 documents and shingles are 200, we need to do tranverse\nprint(data.shape)\nprint(data)\nprint('Data preprocessing over!')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Problem: For document 0 (the one with id '0'), list the **30** most similar document ids (except document 0 itself). You can valid your results with the [sklearn.metrics.jaccard_score()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html) function.\n\nTips: You can adjust your parameters to hash the documents with similarity *s > 0.8* into the same bucket.","metadata":{}},{"cell_type":"code","source":"# Your code here, you can add cells if necessary\ndef Hash(data, b, r):\n    '''\n    param: data: the ndarray type, for shingles and documents\n    param: b: the number of the bands in signature matrix\n    param: r: the number of rows in one single band in the signature matrix, b*r stands for the length of signature\n    '''\n    n = b*r\n    signature = []\n    \n    for i in range(n):                          \n        trans = []                        \n        signal_signature = []                  \n        for num in range(1,data.shape[0]+1):\n            trans.append(num)\n        \n        random.shuffle(trans)            \n      \n       \n        for j in range(data.shape[1]):\n            for k in range(data.shape[0]):\n                index = trans.index(k+1)  \n                \n                if data[index][j] == '1':\n                    signal_signature.append(k+1)\n                    break\n                else:\n                    pass\n                \n        signature.append(signal_signature)\n    return np.array(signature)\n\nb = 10\nr = 5\nres_signature = Hash(data,b,r)\nprint(res_signature)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}