{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\n\nfrom sklearn.metrics import roc_auc_score\n\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here in the first section we will try to visualize and explain what PCA is and how do we actually reduce the number of columns in a dataset known popularly as 'curse of dimensionality'. THen we will also visualize a high dimension dataset using t-SNE to visually see if the data is actually separable or not. And lastly will use PCA and then t-SNE to visualize the data.\n\nIn the second section we will use a normal approach and build a model using XGBoostClassifier.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/anomaly-detection/Participants_Data_WH18/Train.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/anomaly-detection/Participants_Data_WH18/Test.csv\")\ntest.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"Class\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(train[\"Class\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in train.columns[:3]:\n    plt.hist(train[i])\n    plt.title(i)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Resampling the Data\n\nHere we will first remove a test set from our original Dataset and then  resample our train dataset using SMOTE. This will give us a Datset which is untouched and on which we can test our final model. We will also create a validation dataset from our training to use cross validation techniques.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Sm = SMOTE()\nX = train.drop(\"Class\", axis = 1)\ny = train[\"Class\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting the original Dataset to create a Test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.1, stratify = y)\nX_train.shape, X_valid.shape, y_train.shape, y_valid.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Resampling the Training Dataset (oversampling is used) to create a balance in the train set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, y_train = Sm.fit_resample(X_train, y_train)\nx_train.shape, y_train.shape, y_train.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_re, x_valid_re, y_train_re, y_valid_re = train_test_split(x_train, y_train, test_size = 0.1, stratify = y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training a simple Classifier using Random Forest and Catboost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nimport catboost\nfrom sklearn.metrics import auc, roc_curve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def metric(preds, target):\n    fpr, tpr, thresholds = roc_curve(target, preds)\n    return auc(fpr, tpr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Rf = RandomForestClassifier()\nmodel_Rf = Rf.fit(x_train_re, y_train_re)\npreds = model_Rf.predict(x_valid_re)\nprint(metric(preds, y_valid_re))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PCA for Dimensionality Reduction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Principal Component Analysis is that technique which is used to reduce the dimensionality of the Dataset. In short it is used to reduce the number of columns in a dataset. No we dont just throw away the columns directly but do this reduction in number of columns systematically using some Math behind it which is taking out the principal component axes using Eigen values and Eigen vectors. These axes which we get are actually very good at explaining the entire variance in the dataset by not loosing much of the dataset information. \n\nIn short these axes or principal components helps us to represent our high dimensional data with equivalent information on a lower dimension space. PCA is a very popular technique in Dimensionality reduction. We would try to use PCA on our Dataset and also try to use T-sne algorithm to visualize our reduced dataset.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"For this we will use our original Dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=3, random_state=52)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_result = pca.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pca.explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_copy = train.copy()\ntrain_copy['pca-one'] = pca_result[:,0]\ntrain_copy['pca-two'] = pca_result[:,1] \ntrain_copy['pca-three'] = pca_result[:,2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rndperm = np.random.permutation(train_copy.shape[0])\nplt.figure(figsize=(10,8))\nsns.scatterplot(\n    x=\"pca-one\", y=\"pca-two\",\n    hue=\"Class\",\n    palette=sns.color_palette(\"hls\", 2),\n    data= train_copy.loc[rndperm,:],\n    legend=\"full\",\n    alpha=0.3\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3D version of the same plot:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = plt.figure(figsize=(10,8)).gca(projection='3d')\nax.scatter(\n    xs=train_copy.loc[rndperm,:][\"pca-one\"], \n    ys=train_copy.loc[rndperm,:][\"pca-two\"], \n    zs=train_copy.loc[rndperm,:][\"pca-three\"], \n    c=train_copy.loc[rndperm,:][\"Class\"], \n    cmap='tab10'\n)\nax.set_xlabel('pca-one')\nax.set_ylabel('pca-two')\nax.set_zlabel('pca-three')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# t-SNE","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\ntime_start = time.time()\ntsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\ntsne_results = tsne.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_copy['tsne-2d-one'] = tsne_results[:,0]\ntrain_copy['tsne-2d-two'] = tsne_results[:,1]\nplt.figure(figsize=(10,8))\nsns.scatterplot(\n    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n    hue=\"Class\",\n    palette=sns.color_palette(\"hls\", 2),\n    data=train_copy,\n    legend=\"full\",\n    alpha=0.3\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using PCA and t-SNE together\n\nWe will now use the reduced dimensions from the PCA to visualize the data using t-SNE.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_50 = PCA(n_components=50)\npca_result_50 = pca_50.fit_transform(X)\nprint('Cumulative explained variation for 50 principal components: {}'.format(np.sum(pca_50.explained_variance_ratio_)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"time_start = time.time()\ntsne = TSNE(n_components=2, verbose=0, perplexity=40, n_iter=1000)\ntsne_pca_results = tsne.fit_transform(pca_result_50)\nprint('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_copy['tsne-pca50-one'] = tsne_pca_results[:,0]\ntrain_copy['tsne-pca50-two'] = tsne_pca_results[:,1]\nplt.figure(figsize=(16,4))\nax1 = plt.subplot(1, 3, 1)\nsns.scatterplot(\n    x=\"pca-one\", y=\"pca-two\",\n    hue=\"Class\",\n    palette=sns.color_palette(\"hls\", 2),\n    data=train_copy,\n    legend=\"full\",\n    alpha=0.3,\n    ax=ax1\n)\nax2 = plt.subplot(1, 3, 2)\nsns.scatterplot(\n    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n    hue=\"Class\",\n    palette=sns.color_palette(\"hls\", 2),\n    data=train_copy,\n    legend=\"full\",\n    alpha=0.3,\n    ax=ax2\n)\nax3 = plt.subplot(1, 3, 3)\nsns.scatterplot(\n    x=\"tsne-pca50-one\", y=\"tsne-pca50-two\",\n    hue=\"Class\",\n    palette=sns.color_palette(\"hls\", 2),\n    data=train_copy,\n    legend=\"full\",\n    alpha=0.3,\n    ax=ax3\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Normal Approach","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/anomaly-detection/Participants_Data_WH18/Train.csv')\ntrain.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"lets check if there are any duplicate columns present in the Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.T.drop_duplicates().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets check if the number of unique values in the dataset is only 1 and if that is the case just delete it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = 20,6\nplt.subplot(131)\nsns.boxplot(train[\"Class\"], train[\"feature_1\"])\nplt.subplot(132)\nsns.boxplot(train[\"Class\"], train[\"feature_2\"])\nplt.subplot(133)\nsns.boxplot(train[\"Class\"], train[\"feature_3\"])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame((train == 0).astype(int).sum(axis=0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_zero = df[df[0]>1761].index\n\ntrain.drop(all_zero,axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Building the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop(\"Class\", axis = 1)\ny = train[\"Class\"]\n\nX_train, X_valid , y_train, y_valid = train_test_split(X, y, test_size = 0.2, stratify = y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = XGBClassifier(silent=True,\n                      booster = 'gbtree',\n                      scale_pos_weight=5,\n                      learning_rate=0.01,  \n                      colsample_bytree = 0.7,\n                      subsample = 0.5,\n                      max_delta_step = 3,\n                      reg_lambda = 2,\n                     objective='binary:logistic',\n                      \n                      n_estimators=818, \n                      max_depth=8,\n                     )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\neval_set = [(X_valid, y_valid)]\neval_metric = [\"logloss\"]\nmodel.fit(X_train, y_train,early_stopping_rounds=50, eval_metric=eval_metric, eval_set=eval_set)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Evaluating the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict_proba(X_valid)[:, -1]\n\nscore = roc_auc_score(y_valid, predictions)\nscore","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the PCA to remove some columns from the Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_3 = PCA(n_components=3)\npca_result_3 = pca_3.fit_transform(X)\nprint('Cumulative explained variation for 50 principal components: {}'.format(np.sum(pca_50.explained_variance_ratio_)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_3 = pd.DataFrame(pca_result_3, columns=[\"pca1\", 'pca2', 'pca3'])\n\ndf_3.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_valid , y_train, y_valid = train_test_split(df_3, y, test_size = 0.2, stratify = y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\neval_set = [(X_valid, y_valid)]\neval_metric = [\"logloss\"]\nmodel.fit(X_train, y_train,early_stopping_rounds=50, eval_metric=eval_metric, eval_set=eval_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict_proba(X_valid)[:, -1]\n\nscore2 = roc_auc_score(y_valid, predictions)\nscore2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}