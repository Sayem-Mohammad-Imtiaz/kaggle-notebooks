{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Customer segmentation basics "},{"metadata":{},"cell_type":"markdown","source":"![pic](https://acquire.io/wp-content/uploads/2016/09/25-Awesome-Customer-Service-Tips-You-Must-Employ-Updated%E2%80%A9.png)"},{"metadata":{},"cell_type":"markdown","source":"Customer segmentation (or market segmentation) is the process of dividing customers into groups based on common characteristics so companies can market to each group effectively and appropriately. All customers share the common need of your product or service, but beyond that, there are distinct demographic differences (i.e., age, gender) and they tend to have additional socio-economic, lifestyle, or other behavioral differences that can be useful to the organization.\nIn this notebook we're going to split customers into segments according to their age and income.\n\nLet's go!"},{"metadata":{},"cell_type":"markdown","source":"* [Data overview and preparation](#section-one)\n* [EDA](#section-two)\n    - [Spending score and Annual income](#subsection-one)\n    - [Spending score and Age](#subsection-two)\n* [Customers segmentation with K-means](#section-three)\n    - [Spending score and Annual income](#subsection-two-one)\n    - [Spending score and Age](#subsection-two-two)\n* [Customers segmentation with DBCSAN](#section-four)\n    - [Spending score and Annual income](#subsection-three-one)\n    - [Spending score and Age](#subsection-three-two)\n* [Results](#section-five)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import KElbowVisualizer\n\ndf=pd.read_csv('../input/customer-segmentation-tutorial-in-python/Mall_Customers.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First of all, quick data overview for better understanding...."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n# Data overview and preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-two\"></a>\n# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"colnames_numerics_only = df.select_dtypes(include=np.number).columns.tolist()\ncolnames_numerics_only","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1,figsize=(15,6))\nn=0\nfor x in colnames_numerics_only:\n    n+=1\n    plt.subplot(1,4,n)\n    plt.subplots_adjust(hspace=0.5,wspace=0.5)\n    sns.distplot(df[x],bins=20)\n    plt.title('Distplot of {}'.format(x))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1,figsize=(15,10))\nn=0\nfor x in colnames_numerics_only:\n    for y in colnames_numerics_only:\n        n+=1\n        plt.subplot(4,4,n)\n        plt.subplots_adjust(hspace=0.5,wspace=0.5)\n        sns.regplot(x=x,y=y,data=df)\n        plt.ylabel(y.split()[0]+''+y.split()[1] if len(y.split())>1 else y)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-one\"></a>\n## Spending score and Annual income"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nplt.scatter(df[\"Spending Score (1-100)\"], df[\"Annual Income (k$)\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.scatterplot(data=df, x=\"Spending Score (1-100)\", y=\"Annual Income (k$)\", hue=\"Gender\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-two\"></a>\n## Spending score and Age"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nplt.scatter(df[\"Spending Score (1-100)\"], df[\"Age\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.scatterplot(data=df, x=\"Spending Score (1-100)\", y=\"Age\", hue=\"Gender\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(7,5))\nax = sns.boxplot(x=\"Gender\", y=\"Spending Score (1-100)\", data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-three\"></a>\n# Customers segmentation with K-means"},{"metadata":{},"cell_type":"markdown","source":"Firstly, taking Spending score and Annual income.\nLooking at the scatter plot, it seems like 5 groups  of customes. However, we should prove it. There are two most used methods to determine optimal clusters amount - Elbow method and Silhouette method. BIC is also used in some cases.  Let's look at all of them!"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-two-one\"></a>\n## Spending score and annual income"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_income_score = df.iloc[:, [False, False, False, True, True]].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndf_income_score_scaled=scaler.fit_transform(df_income_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_income_score_scaled","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nElbow method helps to select the optimal number of clusters by fitting the model with a range of values for K.If the line chart resembles an arm, then the “elbow” (the point of inflection on the curve) is a good indication that the underlying model fits best at that point. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"distortions = []\nK = range(1,10)\nfor k in K:\n    kmeanModel = KMeans(n_clusters=k)\n    kmeanModel.fit(df_income_score_scaled)\n    distortions.append(kmeanModel.inertia_)\n    \nplt.figure(figsize=(10,8))\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method showing the optimal k')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Elbow method shows 5 is optimal. But what about silhouette method? By the way, the Elbow Method and the Silhouette Method are not like alternatives to each other for finding the optimal amount of clusters. Rather they are instruments for using together for a more confident decision.\n\nThe silhouette value measures how similar a point is to its own cluster (cohesion) compared to other clusters (separation). It's values aer within [-1;1]. Optimal value is a peak."},{"metadata":{"trusted":true},"cell_type":"code","source":"s_scores = []\nclusters = [2,3,4,5,6,7,8,9,10]\nclusters_inertia = []\n\nfor n in clusters:\n    KM_est = KMeans(n_clusters=n, init='k-means++').fit(df_income_score_scaled)\n    clusters_inertia.append(KM_est.inertia_)   \n    silhouette_avg = silhouette_score(df_income_score_scaled, KM_est.labels_)\n    s_scores.append(silhouette_avg)\n    \n    \nfig, ax = plt.subplots(figsize=(12,5))\nax = sns.lineplot(clusters, s_scores, marker='o', ax=ax)\n\nax.set_xlabel(\"Number of clusters\")\nax.set_ylabel(\"Silhouette score\")\nplt.title('The Silhouette Method showing the optimal k')\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, let's take a look at Bayesian information criterion (BIC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.mixture import GaussianMixture\n\ngm_bic= []\ngm_score=[]\nfor i in range(2,12):\n    gm = GaussianMixture(n_components=i,n_init=10,tol=1e-3,max_iter=1000).fit(df_income_score_scaled)\n    gm_bic.append(-gm.bic(df_income_score_scaled))\n    gm_score.append(gm.score(df_income_score_scaled))\n    \nplt.figure(figsize=(7,4))\nplt.title(\"The Gaussian Mixture model BIC\",fontsize=16)\nplt.scatter(x=[i for i in range(2,12)],y=np.log(gm_bic),s=150,edgecolor='k')\nplt.grid(True)\nplt.xlabel(\"Number of clusters\",fontsize=14)\nplt.ylabel(\"Log of Gaussian mixture BIC score\",fontsize=15)\nplt.xticks([i for i in range(2,12)],fontsize=14)\nplt.yticks(fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here again optimal value (peak) is 5. Great! Going straight to K-means model..."},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeanModel = KMeans(n_clusters=5,init='k-means++',max_iter=300,n_init=10,random_state=0)\ny_kmeans= kmeanModel.fit_predict(df_income_score)\nplt.figure(figsize=(8,8))\nplt.scatter(df_income_score[y_kmeans == 0, 0], df_income_score[y_kmeans == 0, 1], s = 100, c = 'g', label = 'Cluster 1')\nplt.scatter(df_income_score[y_kmeans == 1, 0], df_income_score[y_kmeans == 1, 1], s = 100, c = 'b', label = 'Cluster 2')\nplt.scatter(df_income_score[y_kmeans == 2, 0], df_income_score[y_kmeans == 2, 1], s = 100, c = 'r', label = 'Cluster 3')\nplt.scatter(df_income_score[y_kmeans == 3, 0], df_income_score[y_kmeans == 3, 1], s = 100, c = 'burlywood', label = 'Cluster 4')\nplt.scatter(df_income_score[y_kmeans == 4, 0], df_income_score[y_kmeans == 4, 1], s = 100, c = 'green', label = 'Cluster 5')\nplt.scatter(kmeanModel.cluster_centers_[:, 0], kmeanModel.cluster_centers_[:, 1], s = 200, c = 'black', label = 'Centroids')\nplt.title('Clusters of customers')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Groups could be described as: \n* Blue - low annual income and high spending score (careless)\n* Red - high  income and high spending score (target)\n* Light green - medium indome and medium spending score (standart)\n* Dark green - high icome and low spending rate (careful)\n* Beige - low income and low spending score (sensible)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-two-two\"></a>\n## Spending and age"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_age_score = df.iloc[:, [False, False, True, False, True]].values\n\nscaler = MinMaxScaler()\ndf_age_score_scaled=scaler.fit_transform(df_age_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distortions = []\nK = range(1,10)\nfor k in K:\n    kmeanModel = KMeans(n_clusters=k)\n    kmeanModel.fit(df_age_score_scaled)\n    distortions.append(kmeanModel.inertia_)\n    \nplt.figure(figsize=(10,8))\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method showing the optimal k')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s_scores = []\nclusters = [2,3,4,5,6,7,8,9,10]\nclusters_inertia = []\n\nfor n in clusters:\n    KM_est = KMeans(n_clusters=n, init='k-means++').fit(df_age_score_scaled)\n    clusters_inertia.append(KM_est.inertia_)   \n    silhouette_avg = silhouette_score(df_age_score_scaled, KM_est.labels_)\n    s_scores.append(silhouette_avg)\n    \n    \nfig, ax = plt.subplots(figsize=(12,5))\nax = sns.lineplot(clusters, s_scores, marker='o', ax=ax)\n\nax.set_xlabel(\"Number of clusters\")\nax.set_ylabel(\"Silhouette score\")\nplt.title('The Silhouette Method showing the optimal k')\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Optimal value here is 6. Let's try!"},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeanModelAge = KMeans(n_clusters=6,init='k-means++',max_iter=300,n_init=10,random_state=0)\ny_kmeansAge= kmeanModelAge.fit_predict(df_age_score)\nplt.figure(figsize=(8,8))\nplt.scatter(df_age_score[y_kmeansAge == 0, 0], df_age_score[y_kmeansAge == 0, 1], s = 100, c = 'g', label = 'Cluster 1')\nplt.scatter(df_age_score[y_kmeansAge == 1, 0], df_age_score[y_kmeansAge == 1, 1], s = 100, c = 'b', label = 'Cluster 2')\nplt.scatter(df_age_score[y_kmeansAge == 2, 0], df_age_score[y_kmeansAge == 2, 1], s = 100, c = 'grey', label = 'Cluster 3')\nplt.scatter(df_age_score[y_kmeansAge == 3, 0], df_age_score[y_kmeansAge == 3, 1], s = 100, c = 'burlywood', label = 'Cluster 4')\nplt.scatter(df_age_score[y_kmeansAge == 4, 0], df_age_score[y_kmeansAge == 4, 1], s = 100, c = 'green', label = 'Cluster 5')\nplt.scatter(df_age_score[y_kmeansAge == 5, 0], df_age_score[y_kmeansAge == 5, 1], s = 100, c = 'red', label = 'Cluster 6')\nplt.scatter(kmeanModelAge.cluster_centers_[:, 0], kmeanModelAge.cluster_centers_[:, 1], s = 200, c = 'black', label = 'Centroids')\nplt.title('Clusters of customers')\nplt.xlabel('Age')\nplt.ylabel('Spending Score (1-100)')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Groups could be described as:\n\n* Blue - middle age and medium spendings\n* Red - young and low spending\n* Light green - young and medium spending score\n* Dark green - the elderly with low spendings\n* Beige - the elderly with medium spendings \n* Grey - young that spend a lot (seems to be target group)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-four\"></a>\n# Customer segmentation with DBCSAN "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-three-one\"></a>\n## Spending score and annual income"},{"metadata":{},"cell_type":"markdown","source":"Density-Based Clustering refers to unsupervised learning methods that identify distinctive groups/clusters in the data, based on the idea that a cluster in data space is a contiguous region of high point density, separated from other such clusters by contiguous regions of low point density. Simply,the  main idea of DBSCAN algorithm is to locate regions of high density that are separated from one another by regions of low density."},{"metadata":{},"cell_type":"markdown","source":"There are two parameters we need to set:\n* Eps, ε - distance,radius around each point\n\nThe higher eps is, more elements will be included in the particular group and the less density of this group will be.\n\n* MinPts – minimum number of data points that should be around that point within that radius\n\nThe more minPts is,the more outliers potentially could be, the more detached points will be exluded from clusters.\n\nIt should be said, selecting parameters isn't easy, it needs time and some iterations. The reason is that parameters are unique for each dataset and particular task. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_income_score = df.iloc[:, [False, False, False, True, True]]\ndf_norm = scaler.fit_transform(df_income_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_income_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here I've tried range of eps but selected 0.09 which gives adequate number of clusters. By the way, default value is 0.5 but here we have poinnts with much more higher density.\nAs for the min_samples, it seems logical to use approximately 10 or more as totally there are 200 customers in the dataset, however it leads to huge amount of poinnts considered as outliers and unadequate result. I decreased it slightly and decided to use the value of 5. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import DBSCAN\n\n\nDBS_clustering = DBSCAN(eps=0.09, min_samples=5).fit(df_norm)\nDBSCAN_clustered = df_norm.copy()\nlabels = DBS_clustering.labels_\nlabels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise_ = list(labels).count(-1)\nn_clusters_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncolors = ['g','r','b','y','burlywood','green', 'm', 'c']\nplt.figure(figsize=(8,8))\nfor i in range(0 ,n_clusters_ - 1):\n    plt.scatter(df_norm[labels == i, 0], df_norm[labels == i, 1], s = 100, c = colors[i], label = 'Cluster ' + str(i + 1))\nplt.scatter(df_norm[labels == -1, 0], df_norm[labels == -1, 1], s = 50, c = 'black', label = 'Outliers')    \nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we have 5 clusters that look different from what we had using K-means. In terms of customer segmentation and marketing strategies, black outliers here should rather be interpeted as actual customers, but this is how the algorithm works ;-)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-three-two\"></a>\n## Spending score and age"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_age_score = df.iloc[:, [False, False, True, False, True]]\ndf_norm_age = scaler.fit_transform(df_age_score)\n\nDBS_clustering = DBSCAN(eps=0.08, min_samples=5).fit(df_norm_age)\nDBSCAN_clustered = df_norm_age.copy()\nlabels = DBS_clustering.labels_\nlabels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_clusters_age = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise_ = list(labels).count(-1)\nn_clusters_age","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colors = ['g','r','b','y','burlywood','green', 'm', 'c']\nplt.figure(figsize=(8,8))\nfor i in range(0 ,n_clusters_age - 1):\n    plt.scatter(df_norm_age[labels == i, 0], df_norm_age[labels == i, 1], s = 100, c = colors[i], label = 'Cluster ' + str(i + 1))\nplt.scatter(df_norm_age[labels == -1, 0], df_norm_age[labels == -1, 1], s = 50, c = 'black', label = 'Outliers')    \nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here there are 6 clusters with some black outliers. Well, the result also differs from K-means one. However, cluster 1 and 4 look reasonable and logic. Probably, it'd be better to unite some of the clusters 2,3,5,6 because they look to small to represent whole category of customers. "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-five\"></a>\n# Results"},{"metadata":{},"cell_type":"markdown","source":"All in all, we've successfuly found several groups that show the spending score of customers depending on their age or annual income.These groups could be applied in marketing in order to optimize the companies of attraction and retention as well as in strategic management and other business areas. Having the results of two algorithms it looks like K-means performs better for this need than  DBSCAN in this particular task. However, this theory could be proven only after application of our results and testing.  \n\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}