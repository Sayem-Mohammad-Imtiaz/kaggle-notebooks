{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom wordcloud import WordCloud, STOPWORDS\nfrom PIL import Image\nimport requests\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import RandomizedSearchCV\n\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Import dataset\ndataset = pd.read_csv(\"../input/sms-spam-collection-dataset/spam.csv\", encoding = \"latin-1\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop unnecessary columns\ndataset = dataset.drop(columns = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rename our 2 columns to make them more readable and meaningfull\ndataset = dataset.rename(columns = {\"v1\" : \"target\", \"v2\" : \"sms\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[\"length\"] = dataset[\"sms\"].str.len()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How many spams and how many hams?\nplt.figure(figsize=(10,5))\nsns.countplot(data = dataset, x=\"target\")\nprint(dataset[\"target\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Spam mails tend to have more lengthy messages!\nplt.figure(figsize=(15,7))\nplt.xlim(0,200)\nsns.distplot(dataset.loc[dataset[\"target\"] == \"ham\"][\"length\"], \n                     kde_kws={\"label\": \"Ham\"}, bins = 100)\nsns.distplot(dataset.loc[dataset[\"target\"] == \"spam\"][\"length\"], \n                     kde_kws={\"label\": \"Spam\"}, bins = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create copy dataset to manipulate\nmanip_dataset = dataset.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"manip_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport nltk\nnltk.download('stopwords') #download non relevant words\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer # Stemming is taking the root of every word (containing what it means)\nsms = [] # will contain all the different sms cleaned\nfor i in range(0, len(dataset)):\n    string = re.sub(\"[^a-zA-Z]\", \" \", manip_dataset[\"sms\"][i]) # replaces anything NOT in a-z or A-Z by a space, in the variable \n    string = string.lower()\n    string = string.split()\n    stemmer = SnowballStemmer(\"english\")\n    all_stopwords = stopwords.words(\"english\")\n    #if the word is not in the stopwords vocabulary then go ahead with the word iter and stem it\n    string = [stemmer.stem(word) for word in string if not word in set(all_stopwords)]\n    string = ' '.join(string) # joins the words again with a space in between them\n    sms.append(string) # add the cleaned sms to our sms list\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# See the first 5 stemmed messages\nsms[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Iterate through the list and replace the old texts with the new cleaned texts\nfor i in range(0,len(sms)):\n    manip_dataset[\"sms\"][i] = sms[i] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"manip_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating feature for the length of the \"cleaned\" messages\nmanip_dataset[\"after_length\"] = manip_dataset[\"sms\"].str.len()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"manip_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Length distributions more discrete in initial length, so i will not use the after_length attr\nfig, ax =plt.subplots(1,2,figsize=(25,5))\nax[0].set_xlim([0, 200])\nax[1].set_xlim([0, 200])\n\n\nsns.distplot(manip_dataset.loc[manip_dataset[\"target\"] == \"ham\"][\"after_length\"], \n                     kde_kws={\"label\": \"Ham\"}, bins = 100, ax = ax[0])\nsns.distplot(manip_dataset.loc[manip_dataset[\"target\"] == \"spam\"][\"after_length\"], \n                     kde_kws={\"label\": \"Spam\"}, bins = 100, ax = ax[0])\n\nsns.distplot(manip_dataset.loc[manip_dataset[\"target\"] == \"ham\"][\"length\"], \n                     kde_kws={\"label\": \"Ham\"}, bins = 100, ax = ax[1])\nsns.distplot(manip_dataset.loc[manip_dataset[\"target\"] == \"spam\"][\"length\"], \n                     kde_kws={\"label\": \"Spam\"}, bins = 100, ax = ax[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"manip_dataset = manip_dataset.drop(columns = [\"after_length\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"manip_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reindexing our columns\nmanip_dataset = manip_dataset.reindex(columns = [\"sms\", \"length\", \"target\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"manip_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding the target values\ntarget_encoder = LabelEncoder()\nmanip_dataset[\"target\"] = target_encoder.fit_transform(manip_dataset[\"target\"])\nmanip_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Taking the spam stemmed words to put them in a word cloud\nspam_words = \"\"\nfor val in manip_dataset.loc[manip_dataset[\"target\"] == 1][\"sms\"]: \n    val = str(val)\n    tokens = val.split()\n    spam_words += \" \".join(tokens)+\" \"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Taking the spam stemmed words to put them in a word cloud\nham_words = \"\"\nfor val in manip_dataset.loc[manip_dataset[\"target\"] == 0][\"sms\"]: \n    val = str(val)\n    tokens = val.split()\n    ham_words += \" \".join(tokens)+\" \"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Downloading the pic to use and defining our spam_word cloud\npic = np.array(Image.open(requests.get('http://www.clker.com/cliparts/O/i/x/Y/q/P/yellow-house-hi.png',stream=True).raw))\nspam_wordcloud = WordCloud(width = 800, height = 800,\n                      background_color ='white', mask = pic, \n                      min_font_size = 10).generate(spam_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining our ham_word cloud\nham_wordcloud = WordCloud(width = 800, height = 800,\n                      background_color ='white', mask = pic, \n                      min_font_size = 10).generate(ham_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Displaying the word cloud of most frequent stemmed spam sms messages words.\nplt.figure(figsize = (8, 8), facecolor = 'white', edgecolor='blue') \nplt.imshow(spam_wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Displaying the word cloud of most frequent stemmed ham sms messages words.\nplt.figure(figsize = (8, 8), facecolor = 'white', edgecolor='blue') \nplt.imshow(ham_wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Since i'm new and haven't done a word cloud before, I found this implementation to apply here. If you want you can check it out!\nhttps://medium.com/@harinisureshla/wordclouds-basics-of-nlp-5b60be226414"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = manip_dataset.drop(columns = [\"target\"])\ny = manip_dataset[\"target\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features = 4001) # the one at the end is because i'll use the last one for the length feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create our vectors for our bag-of-words model\nX_sms = cv.fit_transform(sms).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assign the last value of each vector to the length feature\nfor i in range(0,len(X_sms)):\n    X_sms[i][-1] = X[\"length\"][i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display first five vectors\nX_sms[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split our data to train/test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_sms, y, test_size=0.20, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to print metrics\ndef print_metrics(y_test,y_pred):\n    print(\"The confusion matrix is : \\n\", confusion_matrix(y_test, y_pred), \"\\n\")\n    print(\"The accuracy score is : \\n\",accuracy_score(y_test, y_pred), \"\\n\")\n    print(\"The precision is : \\n\",precision_score(y_test,y_pred), \"\\n\")\n    print(\"The recall is : \\n\",recall_score(y_test,y_pred), \"\\n\")\n    print(\"The f1 score is : \\n\",f1_score(y_test,y_pred), \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create function running models and printing scores\ndef run_model(model, X_train, y_train, X_test, y_test):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print_metrics(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create function running randomized search on model given in the parameters\ndef run_grid_model(model, grid, X_train, y_train, X_test, y_test):\n    rf_random = RandomizedSearchCV(estimator = model, param_distributions = grid, n_iter = 100, cv = 2, verbose=2, random_state=42, n_jobs = -1)\n    # Fit the random search model\n    rf_random.fit(X_train, y_train)\n    print(f\"Best params of the randomized CV-2 model is : {rf_random.best_params_} \\n-----------------------------------\")\n    y_pred = rf_random.predict(X_test)\n    print_metrics(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nrun_model(GaussianNB(),X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nrun_model(AdaBoostClassifier(random_state = 42),X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nrun_model(GradientBoostingClassifier(random_state = 42),X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrun_model(RandomForestClassifier(random_state = 42),X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### As we can see by the above metrics our best model, RandomForest Classifier, detected spam sms messages with accuracy 97,9%.\n#### Also we can see that all the spams we detected, were indeed ACTUALLY spams (precision = 100%) but we missed approximately 15% (recall = 84.66%) of all the spam messages."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the random grid\nrandom_grid = {'n_estimators': [int(x) for x in np.linspace(200, 2000, num = 7)],\n               'max_features': ['auto', 'sqrt'],\n               'max_depth': [int(x) for x in np.linspace(5, 100, num= 7)],\n               'min_samples_split': [2, 5, 10],\n               'min_samples_leaf': [1, 2, 4],\n               'bootstrap': [True, False]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Perform RandomizedSearch on our random forest classifier\nrun_grid_model(RandomForestClassifier(random_state = 42),random_grid, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### After RandomizedSearch on our RandomForestClassifier which performed better with default parameters, we managed to reach 98.2% accuracy and increase the recall (86.6%). Precision stayed the same at 100%"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}