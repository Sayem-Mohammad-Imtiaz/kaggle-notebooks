{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport sqlite3\nimport datetime\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.manifold import TSNE\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load feature and target matrices from part 1","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"X = pd.read_csv('/kaggle/input/predicting-real-time-delay-status-part-1/X.csv', index_col=0)\ny = pd.read_csv('/kaggle/input/predicting-real-time-delay-status-part-1/y.csv', index_col=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Section 5: Pre-pocessing feature and target matrix","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 5.1 Partial pairplot of feature matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"col_to_keep = ['last_status','avg_station_same', 'avg_station_opp', 'avg_sys']\npair_plot_df = X[col_to_keep]\nsns.pairplot(pair_plot_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.2 Split train and test sets","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Split train and test sets with test size = 20%.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=200)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Release the RAM used by X and y.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = []\ny = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.3 Standardize feature matrix","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Standardize training feature matrix.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"standardize = StandardScaler()\nX_train = standardize.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use the same settings to standardize test set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = standardize.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.4 Apply PCA","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Apply PCA and first set number of compenents to original dimension. Plot cummulative variance over number of components.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=51)\npca.fit(X_train)\npc_vs_variance = np.cumsum(pca.explained_variance_ratio_)\n\nfig, ax = plt.subplots(figsize=[8, 8])\nplt.plot(pc_vs_variance)\nax.set_xlabel('Num of components')\nax.set_ylabel('Cummulative variance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pc_vs_variance[46])\nprint('According to the plot, n = 47 should be enough to capture 100% of variance')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Set n_compnents to 47 and re-apply PCA to training set. Then, use the same projection to transform test set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=47)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Section 6: ML models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 6.1 Linear regression","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Apply linear regression and print out mean squared error and r2 on both training and test sets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LinearRegression()\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)\nprint('mean squared error on train sets:', mean_squared_error(y_train, lr.predict(X_train)))\nprint('mean squared error on test sets:', mean_squared_error(y_test, y_pred))\nprint('r2 on train sets:', r2_score(y_train, lr.predict(X_train)))\nprint('r2 on test sets:', r2_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.2 Neural Network Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 6.2.1 K-fold validation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Use K-fold validation to validate different models. First initialize K-fold validation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ny_train_np = np.array(y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model 1: Intial model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Come up with an initial model:\n* Number of hidden layers: 3\n* Number of nodes each layer: 40\n* Activation function type: relu\n* Loss function: Mean squared error\n* Epochs: 10\n* Mini-batch size: 128","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cvscores = []\n\nfor train, test in kfold.split(X_train, y_train_np):\n  model = Sequential()\n  model.add(Dense(40, input_dim=47, kernel_initializer='normal', activation='relu'))\n  model.add(Dense(40, kernel_initializer='normal', activation='relu'))\n  model.add(Dense(40, kernel_initializer='normal', activation='relu'))\n  model.add(Dense(1, kernel_initializer='normal'))    \n  # Compile model\n  model.compile(loss='mse', optimizer='adam')\n  print('-------------')\n  # Fit the model\n  model.fit(X_train[train], y_train_np[train], epochs=10, batch_size=128)\n  print('-------------')\n  # evaluate the model\n  scores = model.evaluate(X_train[test], y_train_np[test])\n  cvscores.append(scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Print out validation score.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('loss: %.4f' % np.mean(cvscores), '(+/-%.3f)' % np.std(cvscores))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model 2: w/ less hidden layer","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Test second model:\n* **Number of hidden layers: 2 (only difference between 1 & 2)**\n* Number of nodes each layer: 40\n* Activation function type: relu\n* Loss function: Mean squared error\n* Epochs: 10\n* Mini-batch size: 128","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cvscores2 = []\n\nfor train, test in kfold.split(X_train, y_train_np):\n  model = Sequential()\n  model.add(Dense(40, input_dim=47, kernel_initializer='normal', activation='relu'))\n  model.add(Dense(40, kernel_initializer='normal', activation='relu'))\n  model.add(Dense(1, kernel_initializer='normal'))    \n  # Compile model\n  model.compile(loss='mse', optimizer='adam')\n  # Fit the model\n  print('-------------')\n  model.fit(X_train[train], y_train_np[train], epochs=10, batch_size=128)\n\t# evaluate the model\n  print('-------------')\n  scores = model.evaluate(X_train[test], y_train_np[test])\n  cvscores2.append(scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Print out validation score.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('loss: %.4f' % np.mean(cvscores2), '(+/-%.3f)' % np.std(cvscores2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results show that Model 2 is better than Model 1.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Model 3: Further simplify the model. Decrease the neurons of each hidden layer to 30.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Test second model:\n* Number of hidden layers: 2 \n* **Number of nodes each layer: 30 (only difference between 1 & 2)**\n* Activation function type: relu\n* Loss function: Mean squared error\n* Epochs: 10\n* Mini-batch size: 128","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cvscores3 = []\n\nfor train, test in kfold.split(X_train, y_train_np):\n  model = Sequential()\n  model.add(Dense(30, input_dim=47, kernel_initializer='normal', activation='relu'))\n  model.add(Dense(30, kernel_initializer='normal', activation='relu'))\n  model.add(Dense(1, kernel_initializer='normal'))    \n  # Compile model\n  model.compile(loss='mse', optimizer='adam')\n  # Fit the model\n  print('-------------')\n  model.fit(X_train[train], y_train_np[train], epochs=10, batch_size=128)\n\t# evaluate the model\n  print('-------------')\n  scores = model.evaluate(X_train[test], y_train_np[test])\n  cvscores3.append(scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Print out validation score.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('loss: %.4f' % np.mean(cvscores3), '(+/-%.3f)' % np.std(cvscores3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results show that Model 2 is better than Model 3.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 6.2.2 Neural Network Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Based on the the cross-validation results, I chose the model 2 for the final model of neural network.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Re-build model 2 and train with the whole training set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(40, input_dim=47, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(40, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(1, kernel_initializer='normal'))    \n# Compile model\nmodel.compile(loss='mse', optimizer='adam')\n# Fit the model\nmodel.fit(X_train, y_train_np, epochs=10, batch_size=128)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Predict based on test set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test)\ny_pred_train = model.predict(X_train)\n\nprint('mean squared error on train sets:', mean_squared_error(y_train, y_pred_train))\nprint('mean squared error on test sets:', mean_squared_error(y_test, y_pred))\nprint('r2 on train sets:', r2_score(y_train, y_pred_train))\nprint('r2 on test sets:', r2_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Section 7: Conclusion and Discussion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 7.1 Model comparison","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Based on the mean squared error and r2 scores, the Neural Network model is better than Linear regression results. I also tried random forest regressor in Google colab, which has much better hardware than my laptop, and it shows slightly worse result compared with the neural network but still better than the linear regression.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 7.2 Description of challenges/ Obstacles faced","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The first challenge I came across is from the limit of kaggle notebook. The RAM limit is 14 GB, which is clearly not enough for such a huge dataset.\n\nTo test in a safer way, I am personally more likely to use the function .copy() to make sure that I will not ruin my original dataframe. However, it would directly double the usage of RAM. I have used the RAM up several times before going into model section. And eventually, it ended up with spliting into two separat notebooks.\n\nAfter making sure all the processing is correct, I removed the .copy() function and re-run program to release RAM. However, in the model section, the fit process also takes a lot of memory. My notebook crashed once with Random Forest Regressor (even in Google colab with 25-G RAM) and crashed more than 5 times with neural network.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The second challenge is the time. At the very beginning, I used a lot of apply function to process the data, but noticed that it occupied a lot of time. I re-wrote the program by replacing apply functions with built-in pandas methods, and it worked pretty well.\n\nHowever, there are still some time-consuming process which are inevitable like GridSearchCV (10 hours + in Google Colab), training of Random Forest Regressor (30 minutes + in Google Colab), training of Nerual Network (20 minutes + in Google Colab). So, if I have more time, I could further improve my work.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 7.3 Potential Next Steps/ Future Direction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"As for future work, the first thing I want to mention is that all the models here can be further improved if more time is given to tune hyperparameters and test more models with cross-validation.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"And I am pretty satisfied with the current performance of models described in this notebook, since the mean squared error is around 1.4 in minutes. So I think it could be potentially applied to the SEPTA system to predict delay time.\n\nIn order to do so, the models need to ba adjusted to receive real-time data, re-run machine learning program, and give out real-time prediction. This would require stream processing.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}