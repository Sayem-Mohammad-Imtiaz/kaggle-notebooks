{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# creating a reuseable function that will help us in ploting our barplots for analysis\n\ndef BarPlot(df, Variable, plotSize):\n    fig, axs = plt.subplots(figsize = plotSize)\n    plt.xticks(rotation = 45)\n    ax = sns.countplot(x=Variable, data=df)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}'.format(height/len(df) * 100),\n                ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating a reuseable function that will help us in Bivariate for analysis\ndef CountPlot(df,Variable,title,plotsize,hue=None):\n    plt.figure(figsize=plotsize)\n    plt.xticks(rotation=90)\n    plt.title(title)\n    sns.countplot(data = df, x=Variable, order=df[Variable].value_counts().index,hue = hue)\n    plt.show()\n    \n    convertcount=df.pivot_table(values='Lead Number',index=Variable,columns='Converted', aggfunc='count').fillna(0)\n    convertcount[\"Conversion(%)\"] =round(convertcount[1]/(convertcount[0]+convertcount[1]),2)*100\n    return print(convertcount.sort_values(ascending=False,by=\"Conversion(%)\"))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 1: Reading and Understanding the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing Libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport re\nfrom scipy import stats \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Importing RFE and LogisticRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nimport statsmodels.api as sm  \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_recall_curve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading File\ndf= pd.read_csv(\"../input/lead-scoring-x-online-education/Leads X Education.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting Original onversion rate for the data set\norgConversionRate = round(100*(sum(df['Converted'])/len(df['Converted'].index)), 2)\nprint(\"The conversion rate of leads is: \",orgConversionRate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking shape of dataframe\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking columns name\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking columns type in dataframe\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking attributes for continuous variables\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\nThe shape of the dataset is 9240x37\n\nOriginal conversion rate of company X is 38.54%.\n\nLarge number of 'Select' values present for Lead Profile and City in the dataset. These values correspond to the user having   not made any selection.\n\nThere are 7 numerical columns and 30 categorical columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# AS Value  select represent that User has not selecte any values for that, Hence it can be converted to Null\n# so that it can be treated as Null\ndf = df.replace('Select', np.nan)\ndf.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking if any duplicate value in Lead Number and Prospect ID\nprint(sum(df.duplicated(subset= 'Lead Number'))!=0)\nprint(sum(df.duplicated(subset= 'Prospect ID'))!=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\nNo Duplicates value for Prospect ID and Lead Number"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking Null Values\nprint(df.isnull().sum(axis=0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking column-wise null percentages here\nprint(round(100*(df.isnull().sum()/len(df)).sort_values(ascending= False), 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation:\nThere are some columns with over 50% of null values.\n"},{"metadata":{},"cell_type":"markdown","source":"## Step 2: Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Droping columns having null percentage >50%\ndf = df.drop(df.columns[df.apply(lambda col: col.isnull().sum()/len(df) > 0.70)], axis=1)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Checking number of unique values per column \ndf.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\nThere are a lot of columns with 1 or two unique values.\n\nBelow are the columns that have only one unique value, since they won't have anything to contribute to the model significantly,  we will remove these columns.\n\n- Get updates on DM Content\n-  Update me on Supply Chain Content\n- I agree to pay the amount through cheque\n- Receive More Updates About Our Courses\n- Magazine\n\nNote: There are no null values in these columns as seen from the null values table above."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(df.columns [df.apply(lambda col: col.nunique()==1)], axis=1)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### We can also drop the column 'Prospect ID' as we already have an identifying column with unique values: 'Lead Number'\ndf = df.drop('Prospect ID', axis=1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Checking all columns individually having null values in order to decide to impute/drop the column\nAs we can see some of the columns have substantial number of null or missing values. If we drop all these columns we will lose a lot of information so instead of dropping them, for some of the feature variables we will create a new value as 'Unknown' "},{"metadata":{},"cell_type":"markdown","source":"#### Asymmetrique Activity/Profile Index Activity and Score"},{"metadata":{},"cell_type":"markdown","source":"\n\nLet us now look at the following columns: Asymmetrique Activity Index, Asymmetrique Profile Index,Asymmetrique Profile Score and Asymmetrique Activity Score.\n\nWe know from the data dictionary that these are scores assigned to a customer based on their activity and profile Via X- Education employee after calling to Lead."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nplt.subplot(2,2,1)\nsns.countplot(df['Asymmetrique Activity Index'])\n\nplt.subplot(2,2,2)\nsns.boxplot(df['Asymmetrique Activity Score'])\n\nplt.subplot(2,2,3)\nsns.countplot(df['Asymmetrique Profile Index'])\n\nplt.subplot(2,2,4)\nsns.boxplot(df['Asymmetrique Profile Score'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation\nThere is a lot of variation in the data and the number of null values is also very high at 45.65% and its assigned to leas aster calling them via Employee of X-education. Therefore we will drop these columns.\n\nLet us now drop all the 4 columns: **Asymmetrique Activity Index, Asymmetrique Activity Score, Asymmetrique Profile Index and Asymmetrique Profile Score**."},{"metadata":{"trusted":true},"cell_type":"code","source":"colsToDrop = ['Asymmetrique Activity Index','Asymmetrique Activity Score','Asymmetrique Profile Index','Asymmetrique Profile Score']\ndf =df.drop(colsToDrop, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Lead Quality"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking Lead Quality\ndf['Lead Quality'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since 'Lead Quality' is based on an employees intuition, let us inpute any NAN values with 'Not Sure'\ndf['Lead Quality'] = df['Lead Quality'].fillna('Not Sure')\ndf['Lead Quality'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BarPlot(df, 'Lead Quality', (10,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Observation\nFrom the abover barplot we see that the number of values for 'Not Sure' are considerable high at 63.14%. We will drop this column"},{"metadata":{"trusted":true},"cell_type":"code","source":"df =df.drop('Lead Quality', axis=1)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### City"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['City'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.City.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BarPlot(df, 'City', (10,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Observation\nWe will impute the missing values with 'Mumbai' since it has the highest count."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.City= df.City.fillna('Mumbai')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['City'].value_counts(normalize= True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Specialization"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Exploring 'Specialization' column which hs 36.58% null values\ndf.Specialization.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BarPlot(df, 'Specialization', (10,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Note:\nSince there are 36% null values that haven't yet been accounted for, we will replace those with 'Others'. This is being done because the NaN values have the highest percentage of values that haven't been shown above. It simply means that the user did not have any option relevant to them in this field."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Specialization= df['Specialization'].fillna('Others')\ndf.Specialization.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tags"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Tags.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BarPlot(df,'Tags', (10,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Observation\nFrom the above bar plot it looks like we have a lot of small categories within the tags section. Moreover these tags are added by the sales team. We can safely drop this column as this doesn't provide a lot of insight."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['Tags'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking again the Null values\nprint(round(100*(df.isnull().sum(axis=0)/len(df.index)).sort_values(ascending=False),2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Observation\nWe still have a few columns that have a high number of null values."},{"metadata":{},"cell_type":"markdown","source":"### What matters most to you in choosing a course"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['What matters most to you in choosing a course'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BarPlot(df, 'What matters most to you in choosing a course',(8,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Observation\nWe can clearly see that this column is heavily skewed towards better career prospects. Since it doesn't really provide any more information, we can drop this column and keep note that all candidates that take this course are looking to have a better career."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping 'What matters most to you in choosing a course'\ndf= df.drop(['What matters most to you in choosing a course'], axis =1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### What is your current occupation"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['What is your current occupation'].value_counts(normalize= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BarPlot(df, 'What is your current occupation',(10,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Observation\nThe data in this column looks skewed, however, it also defines potential target market for company X. We will impute the missing values with 'Unemployed' and drop the column if analysis further down seems it necessary"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['What is your current occupation']=df['What is your current occupation'].fillna('Unemployed')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BarPlot(df, 'What is your current occupation',(10,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Country"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Country'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Country'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Note\nSince a mode value for this column is India, we can replace the missing values with India. Since this will potentially skew the data heavily in the model we will drop this column."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Country']= df['Country'].fillna('India')\ndf['Country'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df.drop(['Country'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking Null Values Again\n\nprint(round(100*(df.isnull().sum()/len(df)).sort_values(ascending= False), 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Observation\nWe have reduced a lot of columns and imputed missing values in a few of them. In the remaining columns we can safely impute the missing value with the mode value since it has less than 5% missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"### Imputing missing values in Lead Source\ndf['Lead Source'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Imputing missing values with 'Google'\n\ndf['Lead Source'] = df['Lead Source'].fillna('Google')\ndf['Lead Source'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Lead Source'] =  df['Lead Source'].apply(lambda x: x.capitalize())\ndf['Lead Source'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Page Views Per Visit"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Page Views Per Visit'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Page Views Per Visit'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Page Views Per Visit'].median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Imputing the missing values with '2.0' which is the median value\ndf['Page Views Per Visit']= df['Page Views Per Visit'].fillna(2.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Page Views Per Visit']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### TotalVisits"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.TotalVisits.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### We will impute this value with the meadian value since the \n### mean and the median values are relatively close to each other\ndf.TotalVisits = df.TotalVisits.fillna(3.0)\ndf.TotalVisits.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Last Activity"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Last Activity'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Imputing the missing values with 'Email Opened'\ndf['Last Activity'] = df['Last Activity'].fillna('Email Opened')\ndf['Last Activity'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Finally Checking Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(round(100*(df.isnull().sum()/len(df)).sort_values(ascending= False), 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation\nNow that our dataset is clear of all the null values we can begin performing analysis on the remaining columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting shape of dataframe after cleanup\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 3: EDA\nSteps Perfomed in this section\n- Outlier Treatment\n- Univariate Analysis\n- Bivariate Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['TotalVisits','Total Time Spent on Website','Page Views Per Visit']\n# Plotting Box plot for continuous columns\nplt.figure(figsize = (20,12))\nfor i in enumerate(features):\n    plt.subplot(2,2,i[0]+1)\n    sns.boxplot(df[i[1]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Observation\nThere are lot of outliers in **TotalVisits** and **Page Views per Visit**"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Caping data at the 1% & 95% mark so as to not lose any values or drop rows\nq1 = df['Page Views Per Visit'].quantile(0.01)\ndf['Page Views Per Visit'][df['Page Views Per Visit']<= q1] = q1\n\nq3 = df['Page Views Per Visit'].quantile(0.95)\ndf['Page Views Per Visit'][df['Page Views Per Visit']>= q3] = q3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Caping data at the 1% & 95% mark so as to not lose any values or drop rows\nq1 = df['TotalVisits'].quantile(0.01)\ndf['TotalVisits'][df['TotalVisits']<= q1] = q1\n\nq3 = df['TotalVisits'].quantile(0.95)\ndf['TotalVisits'][df['TotalVisits']>= q3] = q3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting Box plot for continuous columns to check after caping outliers\nplt.figure(figsize = (20,12))\nfor i in enumerate(features):\n    plt.subplot(2,2,i[0]+1)\n    sns.boxplot(df[i[1]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking Categorical Variables"},{"metadata":{},"cell_type":"markdown","source":"#### Lead Origin"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Lead Origin'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BarPlot(df,'Lead Origin', (15,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CountPlot(df,'Lead Origin','Conversion based on Lead Origin',(15,10),hue='Converted')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation\nFrom the barplot above we can infer that:\n\n- Lead Add Form has the highest conversion rate at 92%\n- Quick Add Form has 100% conversion rate but it has only 1 entry, so it might not be that reliable as a lead to go on\n- API has the least amount of conversions"},{"metadata":{},"cell_type":"markdown","source":"#### Lead Source"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df['Lead Source'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"BarPlot(df,'Lead Source', (15,4))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,30))\nsns.countplot(data = df, x= 'Lead Source', order=df['Lead Source'].value_counts().index,hue = 'Converted')                      \nplt.xticks(rotation=45)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Printing % of converted Lead with respect to Lead Source\nconvertcount=df.pivot_table(values='Lead Number',index='Lead Source',columns='Converted', aggfunc='count').fillna(0)\nconvertcount[\"Conversion(%)\"] =round(convertcount[1]/(convertcount[0]+convertcount[1]),2)*100\nprint(convertcount.sort_values(ascending=False,by=\"Conversion(%)\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observations\nBased on the plots above we observe that:\n\n- Most students found X education via 'Google' search\n- However, most of the google search leads weren't converted to actual students of the platform\n- References had the highest number of conversions at 92%\n- Welingak website also had a significantly high number of conversions at 99%\n- Welearn & Nc_edm had 100% conversion but due to their low numbers overall it might not be a correct picture of the situation\n- No conversions were made through the youtube channel, blog, press releases, pay per click ads or Welearnblog_home\n\n**Note:** Let us merge the columns with low numbers into a common category: 'Others'"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols=['Click2call', 'Live chat', 'Nc_edm', 'Pay per click ads', 'Press_release',\n  'Social media', 'Welearn', 'Bing', 'Blog', 'Testone', 'Welearnblog_home', 'Youtubechannel']\ndf['Lead Source'] = df['Lead Source'].replace(cols, 'Others')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BarPlot(df,'Lead Source', (15,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,30))\nsns.countplot(data = df, x= 'Lead Source', order=df['Lead Source'].value_counts().index,hue = 'Converted')                      \nplt.xticks(rotation=45)\nplt.show()\n\nconvertcount=df.pivot_table(values='Lead Number',index='Lead Source',columns='Converted', aggfunc='count').fillna(0)\nconvertcount[\"Conversion(%)\"] =round(convertcount[1]/(convertcount[0]+convertcount[1]),2)*100\nprint(convertcount.sort_values(ascending=False,by=\"Conversion(%)\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\nTo imporve the conversion rate X education should focus on providing incentives to referrals as well as improve the lead conversion through olark chat, organic search, direct traffic, and google leads and generate more leads from reference and welingak website."},{"metadata":{},"cell_type":"markdown","source":"#### Do Not Email"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Do Not Email'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BarPlot(df,'Do Not Email', (15,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CountPlot(df,'Do Not Email','Do Not Email',(15,10),hue='Converted')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\nBased on the above we can see that the data is highly skewed. Therefore we will be dropping this column eventually"},{"metadata":{},"cell_type":"markdown","source":"#### Do Not Call"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Do Not Call'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BarPlot(df,'Do Not Call', (15,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CountPlot(df,'Do Not Call','Conversion based on Do Not Call',(15,10),hue='Converted')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations\nBased on the above we can see that the data is highly skewed. Therefore we will be dropping this column eventually\n\nNote\nWe will be dropping the columns Do Not Email & Do Not Call as the data is highly skewed towards the No section. These two columns can be take as safe assumptions by company X that 99% of their prospective customers do not like to be called or receive emails."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['Do Not Call', 'Do Not Email'], axis=1)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Last Activity"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Last Activity'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"BarPlot(df,'Last Activity', (15,5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CountPlot(df,'Last Activity','Conversion based on Last Activity',(15,10),hue='Converted')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\nBased on the plots above we can infer that:\n\n- People interacting with the portal usually send sms the most\n- Only 24% of people who visit the website convert to actual students"},{"metadata":{},"cell_type":"markdown","source":"#### Note:\n**Search, Newspaper Article, Education Forums, Newspaper , Digital Advertisement and Through Recommendations**, are already represented in the 'Lead Source' column.\n\nWe will carry out basic univariate analysis on them and make a decision on if we need to drop them or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['Search', 'Newspaper Article', 'X Education Forums', 'Newspaper' , 'Digital Advertisement','Through Recommendations']\nfor i in enumerate(features):\n    print(df[i[1]].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['Search', 'Newspaper Article', 'X Education Forums', 'Newspaper' , 'Digital Advertisement','Through Recommendations']\nplt.figure(figsize = (20,20))\nfor i in enumerate(features):\n    plt.subplot(3,2,i[0]+1)\n    sns.countplot(x = df[i[1]], data = df) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\nThe data feels highly skewed in these columns and confirms our assumption that it is correctly represented in the Lead Source Column\n\nSince the data for these columns is already correctly represented in the 'Lead Source' column, we will drop these columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"Cols = ['Search', 'Newspaper', 'X Education Forums', 'Newspaper Article' , 'Digital Advertisement','Through Recommendations']\ndf = df.drop(Cols,axis=1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### A free copy of Mastering The Interview"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['A free copy of Mastering The Interview'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BarPlot(df,'A free copy of Mastering The Interview', (15,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CountPlot(df,'A free copy of Mastering The Interview','Conversion based on A free copy of Mastering The Interview',(15,10),hue='Converted')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\nA large number of candidates, 64% didnt opt for any course even though the would like a free copy. 60% of the candidates didn't opt for any course or the free book."},{"metadata":{},"cell_type":"markdown","source":"### Last Notable Activity "},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Last Notable Activity'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BarPlot(df,'Last Notable Activity', (15,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\nFrom our analysis we see that this column holds similar data represented in the **Last Activity column**. We will drop this one and keep the Last Activity column."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Droping Last Notable Activity \ndf= df.drop(['Last Notable Activity'], axis=1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Specialization"},{"metadata":{"trusted":true},"cell_type":"code","source":"BarPlot(df,'Specialization', (15,10))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"CountPlot(df,'Specialization','Conversion based on Specialization',(15,10),hue='Converted')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\n- Healthcare Management specialization is having 50% conversion rate\n- Finance Management specialization is having 45% conversion rate"},{"metadata":{},"cell_type":"markdown","source":"### City"},{"metadata":{"trusted":true},"cell_type":"code","source":"BarPlot(df,'City', (15,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CountPlot(df,'City','Conversion based on City',(15,10),hue='Converted')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\n- Mumbai is having 37% Conversion rate"},{"metadata":{},"cell_type":"markdown","source":"### What is your current occupation"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['What is your current occupation'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BarPlot(df,'What is your current occupation', (15,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CountPlot(df,'What is your current occupation','Conversion based on What is your current occupation',(15,10),hue='Converted')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\n- Housewife are having 100% Conversion rate but their count are very less\n- Unemployed are having 34% Conversion rate\n- Student are having 37% Conversion rate"},{"metadata":{},"cell_type":"markdown","source":"### Checking Continuous Variables"},{"metadata":{},"cell_type":"markdown","source":"#### Total  Visit"},{"metadata":{"trusted":true},"cell_type":"code","source":"BarPlot(df,'TotalVisits', (15,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CountPlot(df,'TotalVisits','Conversion based on Total Visit',(15,10),hue='Converted')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations\nFrom the above analysis we can conclude that:\n\n- People who make more than 10 visits are almost 50% likely to be converted\n- Only 15% of people who visited the website once converted to student. This could imply that people weren't able to gather all the information they needed easily. Hence, decided not to opt for any course."},{"metadata":{},"cell_type":"markdown","source":"#### Total Time Spent on Website"},{"metadata":{},"cell_type":"markdown","source":"#### Page Views Per Visit  "},{"metadata":{"trusted":true},"cell_type":"code","source":"BarPlot(df,'Page Views Per Visit', (25,20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CountPlot(df,'Page Views Per Visit','Page Views Per Visit vs Conversion',(20,20),hue='Converted')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation\nFrom the above plots it is safe to infer that:\n\n- People who dont visit any pages have the highest count of conversion overall\n- Less than half the people who visit 2 pages on average convert to students"},{"metadata":{},"cell_type":"markdown","source":"#### Total Time Spent on Website"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Total Time Spent on Website'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\nThe above looks like time spent was recorded in minutes. We will convert the entire column into hours for ease of analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Total Time Spent on Website'] = df['Total Time Spent on Website'].apply(lambda x: round((x/60), 2))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['Total Time Spent on Website'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us split our dataframe to perform better analysis\ndf1=df[df['Total Time Spent on Website']>=1.0]\ndf1[\"Hours Spent\"]= df1[\"Total Time Spent on Website\"].astype(int)\n\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CountPlot(df1,'Hours Spent','Conversion based on Last Activity',(15,10),hue='Converted')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nplt.xticks(rotation=45)\nplt.yscale('log')\nsns.boxplot(data =df1, x='TotalVisits',y='Total Time Spent on Website', hue ='Converted',orient='v')\nplt.title('Total Time Spent Vs Total Visits based on Conversion')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation\nFrom the above bar plot we can infer that:\n\n- The highest number of conversions happen when people are spending around 18 hours or above on the website\n- People who spent around 3 hours on the website didn't opt for any courses.\n- From the boxplot we can see better that the longer you stay on the website, the higher your chances of conversion as well.\n \n Overall more time the user spends on the website, the better their chances of becoming a student."},{"metadata":{},"cell_type":"markdown","source":"### Checking  Corelation b/w Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 20))\nsns.heatmap(df.corr(), cmap='YlGnBu',annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\nBased on the above heatmap we can see that we don't have any highly correlated features. Therefore there is no multicollinearity in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final Dataframe\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 4 Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"### Creating Dummy Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"### First we will convert the Yes/No values in the 'A free copy of Mastering The Interview' column to 1/0\n\ndf['A free copy of Mastering The Interview'] = df['A free copy of Mastering The Interview'].map(dict(Yes=1, No=0))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy_Cols = ['Lead Origin', 'Lead Source', 'Last Activity', 'Specialization','What is your current occupation','City']\ndummy = pd.get_dummies(df[dummy_Cols],drop_first=True)\ndummy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined = df.copy()\ncombined.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined = pd.concat([combined, dummy], axis=1)\ncombined.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### We will now drop the original columns and the columns that have 'Others' as a sub heading since we had \n### combined various values to create those columns\n\ncols = ['Lead Origin', 'Lead Source', 'Last Activity', 'Specialization','What is your current occupation','City',\n                     'Lead Source_Others','Specialization_Others']\ncombined = combined.drop(cols, axis=1)\ncombined.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Performing Train - Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"### First we will drop the Converted & Lead Number columns \nX = combined.drop(['Converted','Lead Number'], axis=1)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Adding the target variable 'Converted' to y\ny = combined['Converted']\n\ny.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the data into train and test\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Scaling numeric Variables\nscaler = StandardScaler()\n\nX_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.fit_transform(X_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])\n\n\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nsns.heatmap(combined.corr(),cmap='YlGnBu',annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Dropping highly correlated variables\nX_train = X_train.drop(['Lead Origin_Lead Add Form', 'Lead Source_Facebook'], axis=1)\nX_test = X_test.drop(['Lead Origin_Lead Add Form', 'Lead Source_Facebook'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,20))\nsns.heatmap(combined[X_train.columns].corr(),cmap='YlGnBu',annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\nWe have successfully removed the highly corelated variables from the traiing and test datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 5 Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Creating Logistic Regression Model\nlogisticRegressionModel = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogisticRegressionModel.fit().summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection Using RFE"},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression()\n\nrfe = RFE(logreg, 15)\nrfe= rfe.fit(X_train,y_train)\nrfe.support_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = X_train.columns[rfe.support_]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.columns[~rfe.support_]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Assessing the model with StatsModels"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train1= X_train[col]\nX_train1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train1)\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Lead_Score_Prob':y_train_pred})\ny_train_pred_final['Lead'] = y_train.index\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['Final_Predicted_Hot_Lead'] = y_train_pred_final.Lead_Score_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted_Hot_Lead)\nprint(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted_Hot_Lead))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Checking VIF values\nvif = pd.DataFrame()\nvif['Features'] = X_train1.columns\nvif['VIF'] = [variance_inflation_factor(X_train1.values, i) for i in range(X_train1.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\n- The VIF values for all the variables in the model look to be under control\n- The p value for **Last Activity_Resubscribed to emails is very high** at 1 & above the threshold 0.05\n- We will be dropping Last Activity_Resubscribed to emails in the next model\n- Model's accuracy is 79%"},{"metadata":{},"cell_type":"markdown","source":"### Model 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train2 = X_train1.drop('Last Activity_Resubscribed to emails', axis=1)\nX_train2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train2)\nlogm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm).values.reshape(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['Lead_Score_Prob'] = y_train_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['Final_Predicted_Hot_Lead'] = y_train_pred_final.Lead_Score_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted_Hot_Lead)\nprint(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted_Hot_Lead))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vif = pd.DataFrame()\nvif['Features'] = X_train2.columns\nvif['VIF'] = [variance_inflation_factor(X_train2.values, i) for i in range(X_train2.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\n- The VIF values for all the variables in the model look to be under control\n- The p value for What is your current occupation_Housewife is very high at 0.999 & above the threshold 0.05\n- We will be dropping What is your current occupation_Housewife in the next model\n- Accuracy is same as previous model 79.80"},{"metadata":{},"cell_type":"markdown","source":"### Model 4"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train3 = X_train2.drop('What is your current occupation_Housewife', axis=1)\nX_train3.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train3)\nlogm4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"y_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['Lead_Score_Prob'] = y_train_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['Final_Predicted_Hot_Lead'] = y_train_pred_final.Lead_Score_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted_Hot_Lead)\nprint(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted_Hot_Lead))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vif = pd.DataFrame()\nvif['Features'] = X_train3.columns\nvif['VIF'] = [variance_inflation_factor(X_train3.values, i) for i in range(X_train3.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\nThe p value for What is your current occupation_Working Professional is above the threshold at 0.440\n\nAccuracy is same as previous model 79.80\n\nWe will be dropping this variable in the next model"},{"metadata":{},"cell_type":"markdown","source":"### Model 5"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train4 = X_train3.drop('What is your current occupation_Working Professional', axis=1)\nX_train4.columns ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train4)\nlogm4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['Lead_Score_Prob'] = y_train_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['Final_Predicted_Hot_Lead'] = y_train_pred_final.Lead_Score_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted_Hot_Lead)\nprint(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted_Hot_Lead))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vif = pd.DataFrame()\nvif['Features'] = X_train4.columns\nvif['VIF'] = [variance_inflation_factor(X_train4.values, i) for i in range(X_train4.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation\nModel 5 meets all our criteria:\n\n- The VIF values are under 3\n- The p values are under 0.05\n- The 12 selected features look significant\n\nLet us generate a heatmap to confirm that there is no multicollinearity"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nsns.heatmap(X_train_sm.corr(),cmap='YlGnBu',annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\nAs we can confirm with our heatmap, there is no multicollinearity in the model."},{"metadata":{},"cell_type":"markdown","source":"#### Generating predicted values on the training set"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = res.predict(X_train_sm)\ny_train_pred.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['Final_Predicted_Hot_Lead'] = y_train_pred_final.Lead_Score_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted_Hot_Lead)\nprint(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\nprint(round(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted_Hot_Lead),2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Creating new column 'Lead_Score'\nLead_Score would be equal to (Lead_Score_Prob * 100)"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['Lead_Score'] = round((y_train_pred_final['Lead_Score_Prob'] * 100),0)\n\ny_train_pred_final['Lead_Score'] = y_train_pred_final['Lead_Score'].astype('int')\n\n# Let's see the head\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the sensitivity of our logistic regression model\nround((TP / float(TP+FN)),2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us calculate specificity\nround((TN / float(TN+FP)),2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\nBased on the above statistics for Accuracy(80%), Sensitivity(64%) and Specificity(89%) we can say that our trained model is currently highly specific but not very sensitive. Our objective is to create a highly sensitive model with 80% sensitivity. Let us find cut-off values using ROC curves to improve this."},{"metadata":{},"cell_type":"markdown","source":"#### Plotting the ROC Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Converted, y_train_pred_final.Lead_Score_Prob, drop_intermediate = False )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_roc(y_train_pred_final.Converted, y_train_pred_final.Lead_Score_Prob)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\nFrom the ROC curve we can say that the model will be able to provide us with a good result overall."},{"metadata":{},"cell_type":"markdown","source":"### Finding Optimal Cutoff Point\nOptimal cutoff probability is that prob where we get balanced sensitivity and specificity"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Lead_Score_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['Probability','Accuracy','Sensitivity','Specificty'])\n\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])/total1\n    \n    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='Probability', y=['Accuracy','Sensitivity','Specificty'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\nFrom the above curve we can see that the optimal cutoff is at 0.33. This is the point where all the parameters are equally balanced"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['Final_Predicted_Hot_Lead'] = y_train_pred_final.Lead_Score_Prob.map( lambda x: 1 if x > 0.33 else 0)\n\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy\nround(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted_Hot_Lead),2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion2 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted_Hot_Lead )\nconfusion2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the sensitivity of our logistic regression model\nround(TP / float(TP+FN),2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us calculate specificity\nround(TN / float(TN+FP),2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\nAs we can see above, when we are selecting the optimal cutoff = 0.33, the various performance parameters Accuracy, Sensitivity & Specificity are all around 80%\n\nThis meets our objective of getting a highly sensitive model with 80% sensitivity"},{"metadata":{},"cell_type":"markdown","source":"### Precision and Recall Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Calculating Precision\nprecision =round(TP/float(TP+FP),2)\nprecision","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Calculating Recall\nrecall = round(TP/float(TP+FN),2)\nrecall","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\nFrom the above scores we note that our model has a good overall relevancy, defined by Precision, at 70% & a great return of relevant results, defined by Recall, at 80%.\n\nFor the purposes of our model we will focus on the Recall result as we would not like to miss out on any hot leads that are willing to be converted."},{"metadata":{"trusted":true},"cell_type":"code","source":"### Let us generate the Precision vs Recall tradeoff curve \np ,r, thresholds=precision_recall_curve(y_train_pred_final.Converted,y_train_pred_final['Lead_Score_Prob'])\nplt.title('Precision vs Recall tradeoff')\nplt.plot(thresholds, p[:-1], \"g-\")    # Plotting precision\nplt.plot(thresholds, r[:-1], \"r-\")    # Plotting Recall\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\nThe precision vs recall tradeoff value from the above graph is at 0.4"},{"metadata":{"trusted":true},"cell_type":"code","source":"### The F statistic is given by 2 * (precision * recall) / (precision + recall)\n## The F score is used to measure a test's accuracy, and it balances the use of precision and recall to do it.\n### The F score can provide a more realistic measure of a test's performance by using both precision and recall\nF1 =2 * (precision * recall) / (precision + recall)\nround(F1,2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\nBased on the F1 score we can say that our model is fairly accurate. Let us test this accuracy on the test set."},{"metadata":{},"cell_type":"markdown","source":"### Making Predictions on test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.transform(X_test[['TotalVisits',\n                                'Total Time Spent on Website','Page Views Per Visit']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train4.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = X_test[X_train4.columns]\n\nX_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_sm = sm.add_constant(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Making predictions on the test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred = res.predict(X_test_sm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)\ny_pred_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_df['Lead'] = y_test_df.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Renaming the column \n\ny_pred_final= y_pred_final.rename(columns={ 0 : 'Lead_Score_Prob'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rearranging the columns\n\ny_pred_final = y_pred_final.reindex(['Lead','Converted','Lead_Score_Prob'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding Lead_Score column\n\ny_pred_final['Lead_Score'] = round((y_pred_final['Lead_Score_Prob'] * 100),0)\n\ny_pred_final['Lead_Score'] = y_pred_final['Lead_Score'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the head of y_pred_final\ny_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final['Final_Predicted_Hot_Lead'] = y_pred_final.Lead_Score_Prob.map(lambda x: 1 if x > 0.33 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\nround(metrics.accuracy_score(y_pred_final.Converted, y_pred_final.Final_Predicted_Hot_Lead),2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion3 = metrics.confusion_matrix(y_pred_final.Converted, y_pred_final.Final_Predicted_Hot_Lead )\nconfusion3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP = confusion3[1,1] # true positive \nTN = confusion3[0,0] # true negatives\nFP = confusion3[0,1] # false positives\nFN = confusion3[1,0] # false negatives","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the sensitivity of our logistic regression model\nround((TP / float(TP+FN)),2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us calculate specificity\nround(TN / float(TN+FP),2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation And Conclusion (on Test Set)\nAs we can see above when cut-off = 0.33, the various Model Performance parameters on test set are as per below\n- Sensitivity = 80%\n- Specificity = 80%\n- Accuracy = 80%%\n\nAll the 3 performance parameters on test set appear to be almost same with no much variation, so we are good with the modeling now."},{"metadata":{},"cell_type":"markdown","source":"### Final Model Reporting & Equation"},{"metadata":{},"cell_type":"markdown","source":"**log odds = 1.3837 +(1.0659 Total Time Spent on Website) + (1.1280 Lead Source_Olark chat) + (3.5984 Lead Source_Reference) + (5.4963 Lead Source_Welingak website) + (-1.2127 Last Activity_Converted to Lead) + (-1.7984 Last Activity_Email Bounced) + (2.1604 Last Activity_Had a Phone Conversation) + (-1.4009 Last Activity_Olark Chat Conversation) + (1.1884 Last Activity_SMS Sent)+(-2.8435 What is your current occupation_Other)+(-2.3752 What is your current occupation_Student)+(-2.7984 * What is your current occupation_Unemployed)**"},{"metadata":{},"cell_type":"markdown","source":"### Insights\n- Hot Leads are identified as 'Customers having lead score of 33 or above'\n- Sales Team of the company should first focus on the 'Hot Leads'\n- Higher the Lead Score, higher the chances of conversion of 'Hot Leads' into 'Paying Customers'\n- The 'Cold Leads'(Customer having lead score < 33) should be focused after the Sales Team is done with the 'Hot Leads'"},{"metadata":{},"cell_type":"markdown","source":"### Generating Leads Table\nAssigning Lead score to the respective lead numbers present in our original dataset."},{"metadata":{},"cell_type":"markdown","source":"### For training set"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final = y_train_pred_final.reindex(['Lead','Converted','Lead_Score_Prob','Lead_Score','Final_Predicted_Hot_Lead'], axis=1)\ny_train_pred_final","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Generating table\nresultingTable1 = pd.merge(y_train_pred_final,df,how='inner',left_on='Lead',right_index=True)\nresultingTable1[['Lead Number','Lead_Score']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### For testing set"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Generating table\nresultingTable2 = pd.merge(y_pred_final,df,how='inner',left_on='Lead',right_index=True)\nresultingTable2[['Lead Number','Lead_Score']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Merging both resulting tabel"},{"metadata":{"trusted":true},"cell_type":"code","source":"result_df= pd.concat([resultingTable1, resultingTable2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### renaming Converted_x to Converted and droping Converted_y as both are same\nresult_df=result_df.rename(columns={'Converted_x' : 'Converted'})\nresult_df= result_df.drop(['Converted_y'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation\nFrom the resultant shape we can confirm that the number of rows in the final dataset are the same as it's in original. Therefore we can use the values from the 'result_df' dataset to pursue the leads.based on the key insights identified above."},{"metadata":{"trusted":true},"cell_type":"code","source":"# coefficients of our final model \n\npd.options.display.float_format = '{:.2f}'.format\nnew_params = res.params[1:]\nnew_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting a relative coeffient value for all the features wrt the feature with the highest coefficient\n\nfeature_importance = new_params\nfeature_importance = 100.0 * (feature_importance / feature_importance.max())\nfeature_importance","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}