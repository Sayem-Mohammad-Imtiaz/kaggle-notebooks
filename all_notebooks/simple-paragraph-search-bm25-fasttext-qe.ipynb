{"cells":[{"metadata":{},"cell_type":"markdown","source":"## What is this notebook about?"},{"metadata":{},"cell_type":"markdown","source":"I see the CORD-19 challenge (https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks) as a search problem: we need to find answers to the given questions in the given corpus of papers. In this notebook, I build a simple but hopefully useful search tool: paragraph-level BM25 search enhanced with fastText query expansion. It aims for simplicity: minimal dependecies and minimal code but relevant results."},{"metadata":{},"cell_type":"markdown","source":"## What can you learn here?"},{"metadata":{},"cell_type":"markdown","source":"* How to build a simple search engine with Gensim\n* How to build fastText word embeddings with Gensim\n* How to enhance your search engine with fastText-based query expansion"},{"metadata":{},"cell_type":"markdown","source":"## Why this approach?"},{"metadata":{},"cell_type":"markdown","source":"This approach is based on two observations.\n\n1. Abstracts usually contain high-level information or overview of a paper, hence, they can be insufficient to answer a specific question. On the other hand, the whole body of an article can be long and cumbersome for searching an answer inside.\n2. A well-known drawback of standard BM25-based search engines is their inability to comprehend meaning (semantics) of key words, i.e. synonyms, different syntactic forms, mispellings, abbreviations, etc.\n\nTo address Point 1, I implement paragraph-level search that can give more detailed answers than abstract-level search and more succinct and readable answers than full-text search.\n\nTo address Point 2, I leverage word embeddings to expand a query with closely related terms per key word. \n\nI choose fastText as my word embedding model here (why not Word2Vec?) because fastText can handle out-of-vocabulary words naturally. This is important since questions/queries can contain terms which are not present in the corpus. Moreover, as we will see in the following, the given corpus contains many mispellings which are naturally handled by fastText."},{"metadata":{},"cell_type":"markdown","source":"## How to improve?"},{"metadata":{},"cell_type":"markdown","source":"* The implemented search engine is not scalable: it works fine for the given corpus (without exceeding the RAM limit). But, if the corpus is significantly extended, e.g. x10 in size, the engine is likely to go out of memory because it keeps the index in memory. More sophisticated and industry-standard search engines like ElasticSearch can be used to deal with this issue.\n\n* Instead of training word embeddings from a given corpus, one could try using a pre-trained biomedical embeddings. They might have been trained on a bigger corpus (e.g. PubMed), but they could miss or poorly encode important terms for the task at hand.\n\n* Nowadays, we have more advanced embedding models than fastText, such as ELMo, Universal Sentence Encoder, BERT, etc. In comparison to fastText, they are context-aware and take order of words into account when generating embeddings. It would be interesting in future to leverage those models for better query expnasion or even building a search engine with them alone (thus removing BM25). Their downside, however, is high computation cost and low interpretability and, hence, a slower and harder-to-debug search engine.\n\n* Here, I define \"paragraphs\" as pieces of text separated by \"\\n\". While being the simplest, it is not necessarily the best definition. It could be done based on other rules, e.g. max N sentences, or even paragraph chunking ML models.\n\n* Text preprocessing is as simple as possible here. It would be worthwhile to experiment with different tokenization and punctuation/number/stopword removal methods. This could be done separately for BM25 and fastText preprocessing."},{"metadata":{},"cell_type":"markdown","source":"## Output"},{"metadata":{},"cell_type":"markdown","source":"* cord_19_tasks.json: all CORD-19 tasks with their questions in JSON\n* cord_19_answers.json: top-10 answers for each question in all CORD-19 tasks in JSON\n* cord_19_fasttext.model[.\\*]: a fastText model trained on CORD-19 corpus"},{"metadata":{},"cell_type":"markdown","source":"## Aknowledgements"},{"metadata":{},"cell_type":"markdown","source":"1. CORD-19 organizers and supporters: this is a great initiative to beat the disease together.\n2. @xhlulu for notebook https://www.kaggle.com/xhlulu/cord-19-eda-parse-json-and-generate-clean-csv that provided clean input data."},{"metadata":{},"cell_type":"markdown","source":"## Outline"},{"metadata":{},"cell_type":"markdown","source":"1. Prepare Paragraphs\n2. Search Paragraphs with BM25\n3. Query Expansion with fastText\n4. Retrieve Answers for All Tasks"},{"metadata":{},"cell_type":"markdown","source":"## Prepare Paragraphs"},{"metadata":{},"cell_type":"markdown","source":"Let's load and then concatenate all 4 dataframes from '/kaggle/input/cord-19-eda-parse-json-and-generate-clean-csv'. I will only use columns 'paper_id', 'title', 'authors', 'text' but more columns can be used if needed."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\ndef load_corpus():\n    cols = ['paper_id', 'title', 'authors', 'text']\n    \n    biorxiv_clean = pd.read_csv('/kaggle/input/cord-19-eda-parse-json-and-generate-clean-csv/biorxiv_clean.csv', na_filter=False, usecols=cols)\n    clean_comm_use = pd.read_csv('/kaggle/input/cord-19-eda-parse-json-and-generate-clean-csv/clean_comm_use.csv', na_filter=False, usecols=cols)\n    clean_noncomm_use =  pd.read_csv('/kaggle/input/cord-19-eda-parse-json-and-generate-clean-csv/clean_noncomm_use.csv', na_filter=False, usecols=cols)\n    clean_pmc =  pd.read_csv('/kaggle/input/cord-19-eda-parse-json-and-generate-clean-csv/clean_pmc.csv', na_filter=False, usecols=cols)\n\n    corpus = pd.concat([biorxiv_clean, \n                        clean_comm_use, \n                        clean_noncomm_use, \n                        clean_pmc])\n    \n    # 'paper_id' is used as an index column\n    corpus.set_index('paper_id', inplace=True)\n    \n    return corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = load_corpus()\ncorpus.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The full text of a paper is contained in column 'text'. Let's split it in paragraphs based on a simple rule: paragraphs are separated by \"\\n\", a paragraph must be at least 100 characters in length. \n\nWe also return respective paper IDs because we want to go from a paragraph to its respective paper later."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_paragraphs(corpus, sep=\"\\n\", min_length=100, verbose=10000):\n    paragraphs, paper_ids = [], []\n    \n    for i, (paper_id, row) in enumerate(corpus.iterrows()):\n        for s in row['text'].split(sep):\n            \n            if len(s) >= min_length:\n                paragraphs.append(s)\n                paper_ids.append(paper_id)\n        \n        # print progress if needed\n        if verbose > 0 and (i + 1) % verbose == 0:\n            print(f\"Progress: {i + 1}\")\n            \n    return paragraphs, paper_ids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"paragraphs, paper_ids = list(get_paragraphs(corpus, verbose=-1))\nlen(paragraphs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This gives us 383,159 paragraphs to be searched."},{"metadata":{"trusted":true},"cell_type":"code","source":"paragraphs[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's preprocess and then tokenize paragraphs for the search engine. For this purpose, we use 'gensim.parsing.preprocessing.preprocess_string'. It removes all punctuation, numbers, whitespaces, and stop words from a given string, and then tokenizes the result.\n\n*Note:* function \"get_tokens\" is a **generator**: this way we can tokenize the entire corpus on the fly, i.e. without loading the result in memory."},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import preprocess_string\n\ndef get_tokens(docs, preprocess=preprocess_string, verbose=10000):\n    \n    for i, doc in enumerate(docs):\n        yield preprocess(doc)\n        \n        # print progress if needed\n        if verbose > 0 and (i + 1) % verbose == 0:\n            print(f\"Progress: {i + 1}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"paragraph_tokens = list(get_tokens(paragraphs))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Search Paragraphs with BM25"},{"metadata":{},"cell_type":"markdown","source":"I use class BM25 from Gensim to build the paragraph search index based on their tokens."},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.summarization.bm25 import BM25\n\nbm25 = BM25(paragraph_tokens)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Having built the paragraph index, we now need to implement function \"get_top_n\" retrieving top N best matching results for a given query (which is list of key words). It does the following:\n1. Calculate BM25 scores of all indexed documents with respect to a given query.\n2. Get indices of top N scores using Numpy's efficient \"argpartition()\" function.\n3. Sort retieved top N scores descendingly and return their indices."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\ndef get_top_n(bm25, query, n=10):\n    \n    # score docs\n    scores = np.array(bm25.get_scores(query))\n    \n    # get indices of top N scores\n    idx = np.argpartition(scores, -n)[-n:]\n    \n    # sort top N scores and return their indices\n    return idx[np.argsort(-scores[idx])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's test our search engine."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_query = [\"covid\",\"coronavirus\"]\ntop_idx = get_top_n(bm25, test_query)[0]\nparagraphs[top_idx]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since function \"get_top_n\" returns indices instead of documents, it can be easily used to get a respective paper ID. "},{"metadata":{"trusted":true},"cell_type":"code","source":"paper_ids[top_idx]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to check which key words occur in a retrieved paragraph, I implement function \"highlight\" that highlights in red given key words (as tokens) in a given paragraph (as tokens). "},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.core.display import display, HTML\n\ndef mark(s, color='black'):\n    return \"<text style=color:{}>{}</text>\".format(color, s)\n\ndef highlight(keywords, tokens, color='red'):\n    kw_set = set(keywords)\n    tokens_hl = []\n    \n    for t in tokens:\n        if t in kw_set:\n            tokens_hl.append(mark(t, color=color))\n        else:\n            tokens_hl.append(t)\n    \n    return \" \".join(tokens_hl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"HTML(highlight(test_query, paragraph_tokens[top_idx]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both 'covid' and 'coronavirus' are highlighted in the paragraph."},{"metadata":{},"cell_type":"markdown","source":"## Query Expansion with fastText"},{"metadata":{},"cell_type":"markdown","source":"In order to expand a query with words close by meaning, I use a word embedding approach. I will train a fastText model using Gensim.\n\nSince word embedding models are trained on sentences, we need to split paragraphs into sentences first. I will use NLTK for this purpose.\n\n*Note:* function \"get_sentences\" is a **generator** but we will get the list this time."},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\n\ndef get_sentences(docs, verbose=10000):\n\n    for i, doc in enumerate(docs):\n        for s in nltk.sent_tokenize(doc):\n            yield s\n            \n        # print progress if needed\n        if verbose > 0 and (i + 1) % verbose == 0:\n            print(f\"Progress: {i + 1}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = list(get_sentences(paragraphs))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(sentences)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5,507,300 sentences"},{"metadata":{},"cell_type":"markdown","source":"The reason why I made the list (thus storing the results in memory) is that I am going to use it twice: for building vocabulary and then fastText training. This just saves a lot of runtime in this case."},{"metadata":{},"cell_type":"markdown","source":"Let's initialize a fastText model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models.fasttext import FastText\n\nft_model = FastText(\n    sg=1, # use skip-gram: usually gives better results\n    size=100, # embedding dimension (default)\n    window=10, # window size: 10 tokens before and 10 tokens after to get wider context\n    min_count=10, # only consider tokens with at least 10 occurrences in the corpus\n    negative=15, # negative subsampling: bigger than default to sample negative examples more\n    min_n=2, # min character n-gram\n    max_n=5 # max character n-gram\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Build a vocabulary using the in-memory list of sentences and generator of tokens."},{"metadata":{"trusted":true},"cell_type":"code","source":"ft_model.build_vocab(get_tokens(sentences, verbose=100000))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train a model for 3 epochs using the in-memory list of sentences and generator of tokens. Since we use a generator to get tokens on the fly, we need to call \"train\" 3 times specifying \"epochs=1\" each time. Otherwise, the generator will go out of items after epoch 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 3\n\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch}\")\n    \n    ft_model.train(\n        get_tokens(sentences, verbose=100000),\n        epochs=1,\n        total_examples=ft_model.corpus_count, \n        total_words=ft_model.corpus_total_words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check top N most similar terms for \"coronavirus\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"ft_model.wv.most_similar(\"coronavirus\", topn=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Save the trained fastText model."},{"metadata":{"trusted":true},"cell_type":"code","source":"ft_model.save('cord_19_fasttext.model')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to expand a query, for each key word, we get top N most similar terms occurring in the corpus using the fastText model and add them to the query."},{"metadata":{"trusted":true},"cell_type":"code","source":"def expand_query(query, wv, topn=10):\n    expanded_query = [t for t in query]\n    \n    for t in query:\n        expanded_query.extend(s for s, f in wv.most_similar(t, topn=topn))\n        \n    return expanded_query","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's test the expanded test query now."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_query","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_query_exp = expand_query(test_query, ft_model.wv)\ntop_idx = get_top_n(bm25, test_query_exp)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"HTML(highlight(test_query_exp, paragraph_tokens[top_idx]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, terms like \"sars\" and \"mers\" are highlighted even though they are not mentioned in the test query. They are included into the expanded query by the fastText model."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Retrieve Answers for All Tasks"},{"metadata":{},"cell_type":"markdown","source":"Let's now retreive top 10 answers for all questions in all CORD-19 tasks. First, let's make a dictionary of the CORD-19 tasks and store it as JSON in the output folder."},{"metadata":{"trusted":true},"cell_type":"code","source":"t1 = {\n    \"txt\": \"What is known about transmission, incubation, and environmental stability?\",\n    \"qs\": [\n             \"Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery.\",\n             \"Prevalence of asymptomatic shedding and transmission (e.g., particularly children).\",\n             \"Seasonality of transmission.\",\n             \"Physical science of the coronavirus (e.g., charge distribution, adhesion to hydrophilic/phobic surfaces, environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding).\",\n             \"Persistence and stability on a multitude of substrates and sources (e.g., nasal discharge, sputum, urine, fecal matter, blood).\",\n             \"Persistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic).\",\n             \"Natural history of the virus and shedding of it from an infected person\",\n             \"Implementation of diagnostics and products to improve clinical processes\",\n             \"Disease models, including animal models for infection, disease and transmission\",\n             \"Tools and studies to monitor phenotypic change and potential adaptation of the virus\",\n             \"Immune response and immunity\",\n             \"Effectiveness of movement control strategies to prevent secondary transmission in health care and community settings\",\n             \"Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings\",\n             \"Role of the environment in transmission\"\n    ]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t2 = {\n    \"txt\": \"What do we know about COVID-19 risk factors?\",\n    \"qs\": [\n        \"Smoking, pre-existing pulmonary disease\",\n        \"Co-infections (determine whether co-existing respiratory/viral infections make the virus more transmissible or virulent) and other co-morbidities\",\n        \"Neonates and pregnant women\",\n        \"Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.\",\n        \"Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors\",\n        \"Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups\",\n        \"Susceptibility of populations\",\n        \"Public health mitigation measures that could be effective for control\"\n    ]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t3 = {\n    \"txt\": \"What do we know about virus genetics, origin, and evolution?\",\n    \"qs\": [\n        \"Real-time tracking of whole genomes and a mechanism for coordinating the rapid dissemination of that information to inform the development of diagnostics and therapeutics and to track variations of the virus over time.\",\n        \"Access to geographic and temporal diverse sample sets to understand geographic distribution and genomic differences, and determine whether there is more than one strain in circulation. Multi-lateral agreements such as the Nagoya Protocol could be leveraged.\",\n        \"Evidence that livestock could be infected (e.g., field surveillance, genetic sequencing, receptor binding) and serve as a reservoir after the epidemic appears to be over.\",\n        \"Evidence of whether farmers are infected, and whether farmers could have played a role in the origin.\",\n        \"Surveillance of mixed wildlife- livestock farms for SARS-CoV-2 and other coronaviruses in Southeast Asia.\",\n        \"Experimental infections to test host range for this pathogen.\",\n        \"Animal host(s) and any evidence of continued spill-over to humans\",\n        \"Socioeconomic and behavioral risk factors for this spill-over\",\n        \"Sustainable risk reduction strategies\"\n    ]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t4 = {\n    \"txt\": \"What do we know about vaccines and therapeutics?\",\n    \"qs\": [\n        \"Effectiveness of drugs being developed and tried to treat COVID-19 patients.\",\n        \"Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication.\",\n        \"Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients.\",\n        \"Exploration of use of best animal models and their predictive value for a human vaccine.\",\n        \"Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.\",\n        \"Alternative models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up. This could include identifying approaches for expanding production capacity to ensure equitable and timely distribution to populations in need.\",\n        \"Efforts targeted at a universal coronavirus vaccine.\",\n        \"Efforts to develop animal models and standardize challenge studies\",\n        \"Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers\",\n        \"Approaches to evaluate risk for enhanced disease after vaccination\",\n        \"Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models [in conjunction with therapeutics]\"\n    ]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t5 = {\n    \"txt\": \"What has been published about medical care?\",\n    \"qs\": [\n        \"Resources to support skilled nursing facilities and long term care facilities.\",\n        \"Mobilization of surge medical staff to address shortages in overwhelmed communities\",\n        \"Age-adjusted mortality data for Acute Respiratory Distress Syndrome (ARDS) with/without other organ failure – particularly for viral etiologies\",\n        \"Extracorporeal membrane oxygenation (ECMO) outcomes data of COVID-19 patients\",\n        \"Outcomes data for COVID-19 after mechanical ventilation adjusted for age.\",\n        \"Knowledge of the frequency, manifestations, and course of extrapulmonary manifestations of COVID-19, including, but not limited to, possible cardiomyopathy and cardiac arrest.\",\n        \"Application of regulatory standards (e.g., EUA, CLIA) and ability to adapt care to crisis standards of care level.\",\n        \"Approaches for encouraging and facilitating the production of elastomeric respirators, which can save thousands of N95 masks.\",\n        \"Best telemedicine practices, barriers and faciitators, and specific actions to remove/expand them within and across state boundaries.\",\n        \"Guidance on the simple things people can do at home to take care of sick people and manage disease.\",\n        \"Oral medications that might potentially work.\",\n        \"Use of AI in real-time health care delivery to evaluate interventions, risk factors, and outcomes in a way that could not be done manually.\",\n        \"Best practices and critical challenges and innovative solutions and technologies in hospital flow and organization, workforce protection, workforce allocation, community-based support resources, payment, and supply chain management to enhance capacity, efficiency, and outcomes.\",\n        \"Efforts to define the natural history of disease to inform clinical care, public health interventions, infection prevention control, transmission, and clinical trials\",\n        \"Efforts to develop a core clinical outcome set to maximize usability of data across a range of trials\",\n        \"Efforts to determine adjunctive and supportive interventions that can improve the clinical outcomes of infected patients (e.g. steroids, high flow oxygen)\"\n    ]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t6 = {\n    \"txt\": \"What do we know about non-pharmaceutical interventions?\",\n    \"qs\": [\n        \"Guidance on ways to scale up NPIs in a more coordinated way (e.g., establish funding, infrastructure and authorities to support real time, authoritative (qualified participants) collaboration with all states to gain consensus on consistent guidance and to mobilize resources to geographic areas where critical shortfalls are identified) to give us time to enhance our health care delivery system capacity to respond to an increase in cases.\",\n        \"Rapid design and execution of experiments to examine and compare NPIs currently being implemented. DHS Centers for Excellence could potentially be leveraged to conduct these experiments.\",\n        \"Rapid assessment of the likely efficacy of school closures, travel bans, bans on mass gatherings of various sizes, and other social distancing approaches.\",\n        \"Methods to control the spread in communities, barriers to compliance and how these vary among different populations..\",\n        \"Models of potential interventions to predict costs and benefits that take account of such factors as race, income, disability, age, geographic location, immigration status, housing status, employment status, and health insurance status.\",\n        \"Policy changes necessary to enable the compliance of individuals with limited resources and the underserved with NPIs.\",\n        \"Research on why people fail to comply with public health advice, even if they want to do so (e.g., social or financial costs may be too high).\",\n        \"Research on the economic impact of this or any pandemic. This would include identifying policy and programmatic alternatives that lessen/mitigate risks to critical government services, food distribution and supplies, access to critical household supplies, and access to health diagnoses, treatment, and needed care, regardless of ability to pay.\"\n    ]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t7 = {\n    \"txt\": \"What do we know about diagnostics and surveillance?\",\n    \"qs\": [\n        \"How widespread current exposure is to be able to make immediate policy recommendations on mitigation measures. Denominators for testing and a mechanism for rapidly sharing that information, including demographics, to the extent possible. Sampling methods to determine asymptomatic disease (e.g., use of serosurveys (such as convalescent samples) and early detection of disease (e.g., use of screening of neutralizing antibodies such as ELISAs).\",\n        \"Efforts to increase capacity on existing diagnostic platforms and tap into existing surveillance platforms.\",\n        \"Recruitment, support, and coordination of local expertise and capacity (public, private—commercial, and non-profit, including academic), including legal, ethical, communications, and operational issues.\",\n        \"National guidance and guidelines about best practices to states (e.g., how states might leverage universities and private laboratories for testing purposes, communications to public health officials and the public).\",\n        \"Development of a point-of-care test (like a rapid influenza test) and rapid bed-side tests, recognizing the tradeoffs between speed, accessibility, and accuracy.\",\n        \"Rapid design and execution of targeted surveillance experiments calling for all potential testers using PCR in a defined area to start testing and report to a specific entity. These experiments could aid in collecting longitudinal samples, which are critical to understanding the impact of ad hoc local interventions (which also need to be recorded).\",\n        \"Separation of assay development issues from instruments, and the role of the private sector to help quickly migrate assays onto those devices.\",\n        \"Efforts to track the evolution of the virus (i.e., genetic drift or mutations) and avoid locking into specific reagents and surveillance/detection schemes.\",\n        \"Latency issues and when there is sufficient viral load to detect the pathogen, and understanding of what is needed in terms of biological and environmental sampling.\",\n        \"Use of diagnostics such as host response markers (e.g., cytokines) to detect early disease or predict severe disease progression, which would be important to understanding best clinical practice and efficacy of therapeutic interventions.\",\n        \"Policies and protocols for screening and testing.\",\n        \"Policies to mitigate the effects on supplies associated with mass testing, including swabs and reagents.\",\n        \"Technology roadmap for diagnostics.\",\n        \"Barriers to developing and scaling up new diagnostic tests (e.g., market forces), how future coalition and accelerator models (e.g., Coalition for Epidemic Preparedness Innovations) could provide critical funding for diagnostics, and opportunities for a streamlined regulatory environment.\",\n        \"New platforms and technology (e.g., CRISPR) to improve response times and employ more holistic approaches to COVID-19 and future diseases.\",\n        \"Coupling genomics and diagnostic testing on a large scale.\",\n        \"Enhance capabilities for rapid sequencing and bioinformatics to target regions of the genome that will allow specificity for a particular variant.\",\n        \"Enhance capacity (people, technology, data) for sequencing with advanced analytics for unknown pathogens, and explore capabilities for distinguishing naturally-occurring pathogens from intentional.\",\n        \"One Health surveillance of humans and potential sources of future spillover or ongoing exposure for this organism and future pathogens, including both evolutionary hosts (e.g., bats) and transmission hosts (e.g., heavily trafficked and farmed wildlife and domestic food and companion species), inclusive of environmental, demographic, and occupational risk factors.\"\n    ]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t8 = {\n    \"txt\": \"What has been published about ethical and social science considerations?\",\n    \"qs\": [\n        \"Efforts to articulate and translate existing ethical principles and standards to salient issues in COVID-2019\",\n        \"Efforts to embed ethics across all thematic areas, engage with novel ethical issues that arise and coordinate to minimize duplication of oversight\",\n        \"Efforts to support sustained education, access, and capacity building in the area of ethics\",\n        \"Efforts to establish a team at WHO that will be integrated within multidisciplinary research and operational platforms and that will connect with existing and expanded global networks of social sciences.\",\n        \"Efforts to develop qualitative assessment frameworks to systematically collect information related to local barriers and enablers for the uptake and adherence to public health measures for prevention and control. This includes the rapid identification of the secondary impacts of these measures. (e.g. use of surgical masks, modification of health seeking behaviors for SRH, school closures)\",\n        \"Efforts to identify how the burden of responding to the outbreak and implementing public health measures affects the physical and psychological health of those providing care for Covid-19 patients and identify the immediate needs that must be addressed.\",\n        \"Efforts to identify the underlying drivers of fear, anxiety and stigma that fuel misinformation and rumor, particularly through social media.\"\n    ]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t9 = {\n    \"txt\": \"What has been published about information sharing and inter-sectoral collaboration?\",\n    \"qs\": [\n        \"Methods for coordinating data-gathering with standardized nomenclature.\",\n        \"Sharing response information among planners, providers, and others.\",\n        \"Understanding and mitigating barriers to information-sharing.\",\n        \"How to recruit, support, and coordinate local (non-Federal) expertise and capacity relevant to public health emergency response (public, private, commercial and non-profit, including academic).\",\n        \"Integration of federal/state/local public health surveillance systems.\",\n        \"Value of investments in baseline public health response infrastructure preparedness\",\n        \"Modes of communicating with target high-risk populations (elderly, health care workers).\",\n        \"Risk communication and guidelines that are easy to understand and follow (include targeting at risk populations’ families too).\",\n        \"Communication that indicates potential risk of disease to all population groups.\",\n        \"Misunderstanding around containment and mitigation.\",\n        \"Action plan to mitigate gaps and problems of inequity in the Nation’s public health capability, capacity, and funding to ensure all citizens in need are supported and can access information, surveillance, and treatment.\",\n        \"Measures to reach marginalized and disadvantaged populations.\",\n        \"Data systems and research priorities and agendas incorporate attention to the needs and circumstances of disadvantaged populations and underrepresented minorities.\",\n        \"Mitigating threats to incarcerated people from COVID-19, assuring access to information, prevention, diagnosis, and treatment.\",\n        \"Understanding coverage policies (barriers and opportunities) related to testing, treatment, and care\"\n    ]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tasks = {\"t1\": t1, \n         \"t2\": t2, \n         \"t3\": t3, \n         \"t4\": t4,\n         \"t5\": t5,\n         \"t6\": t6,\n         \"t7\": t7,\n         \"t8\": t8,\n         \"t9\": t9}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\n\nwith open('/kaggle/working/cord_19_tasks.json', 'w') as f:\n    json.dump(tasks, f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Function \"answer_questions\" uses the same preprocessing method as we used for building the BM25 paragraph index and training the fastText model (and it should do so). It returns the dictionary of results containing top N paragraph hits per question along with their respective paper IDs. Assuming that the main question of a CORD-19 task is made more specific by its sub-questions, I merge them together and also add \"coronavirus\" to a query to force scoring paragraphs that mention \"coronavirus\" slightly higher."},{"metadata":{"trusted":true},"cell_type":"code","source":"def answer_questions(n_hits=10, print_hits=0, preprocess=preprocess_string):\n    \n    # make results dictionary\n    results = {}\n    \n    # answer questions\n    for tid, task in tasks.items():\n        results[tid] = {\"txt\": task[\"txt\"]}\n        title_query = preprocess(task[\"txt\"])\n        \n        for qid, question in enumerate(task[\"qs\"]):\n            results[tid][f\"q{qid + 1}\"] = {\"txt\": question}\n            question_query = preprocess(question)\n            \n            # make a query\n            query = [\"coronavirus\"]\n            query.extend(title_query)\n            query.extend(question_query)\n            expanded_query = expand_query(query, ft_model.wv)\n            \n            # get top hits\n            top_idx = get_top_n(bm25, expanded_query, n=n_hits)\n            \n            # fill in results\n            for hid, idx in enumerate(top_idx):\n                results[tid][f\"q{qid + 1}\"][f\"h{hid + 1}\"] = {\n                    \"txt\": paragraphs[idx], \"pid\": paper_ids[idx]}\n            \n            # print if needed\n            for hid in range(min(print_hits, n_hits)):\n                display(HTML(\"<br/>\".join([\n                    mark(f'{tid.upper()}: {task[\"txt\"]}', color=\"blue\"),\n                    mark(f'Q{qid + 1}: {question}', color=\"green\"),\n                    mark(f'H{hid + 1}:', color=\"red\"),\n                    highlight(expanded_query, paragraph_tokens[top_idx[hid]], color=\"red\")\n                ])))\n            \n    return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = answer_questions(n_hits=10, print_hits=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('/kaggle/working/cord_19_answers.json', 'w') as f:\n    json.dump(results, f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dictionary of results can easily be used to retrieve each answer paragraph and its respective paper ID. Let's do this for Task 1, Question 1, Hit 1 (best matching paragraph)."},{"metadata":{"trusted":true},"cell_type":"code","source":"t1q1h1_txt = results[\"t1\"][\"q1\"][\"h1\"][\"txt\"]\nprint(t1q1h1_txt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t1q1h1_pid = results[\"t1\"][\"q1\"][\"h1\"][\"pid\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Paper ID is sufficient to get the full paper record from the corpus dataframe. "},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus.loc[[t1q1h1_pid]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's it! Thanks for reading so far and please let me know if you have any questions or feedback! Please also take it to production if you can! **Let's beat the disease together!**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}