{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nRowRead = 3000 # specify 'None' if want to read whole file\noriginal_dataset = pd.read_csv(\"/kaggle/input/prediction-of-asteroid-diameter/Asteroid_Updated.csv\", delimiter = ',', nrows = nRowRead)\ndataset = original_dataset.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndataset.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Describe Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Display Data Information"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.hist(bins = 50, figsize = (20,15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Convert diameter To float\nconvertDict = {'diameter' : float}\ndataset = dataset.astype(convertDict) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = dataset.corr()\ncorr_matrix.columns\ncorr_matrix['diameter'].sort_values(ascending = False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning\n## Removing Numerical Columns\nAs We can see that Columns:\n'extent' has 10 non null values out of 3000,hence it will be illogical to fill Nan, because it does show any relation with diameter\n'GM' has 11 non null values out of 3000,hence it will be illogical to fill Nan, because it does show any relation with diameter\n'G' has 113 non null values out of 3000,hence it will be illogical to fill Nan, because it does show any relation with diameter\n'IR' has 0 non null values\n(extent : 10/3000,GM : 11/3000, 113/3000, 'G' : 113/3000,'IR' : 0 /3000)\nHence They should be removed from Dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"#(extent : 10/3000,GM : 11/3000, 113/3000, 'G' : 113/3000, IR : 0 /3000) Thse rows have maximun null value\ndropColumn = ['extent','GM','G','IR']\ndataset = dataset.drop(dropColumn, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['diameter'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['diameter'].median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As per Analysis of  columns diameter, we should feel this column with its mean value\n#dataset['diameter'].filna(dataset['diameter'].mean())\ndataset['diameter'].fillna(dataset['diameter'].mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['diameter'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As per Analysis of  columns albedo, we should feel this column with its median value\ndataset['albedo'].fillna(dataset['albedo'].median(), inplace=True)\ndataset['albedo'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As per Analysis of  columns rot_per, we should feel this column with its mean value\ndataset['rot_per'].fillna(dataset['rot_per'].mean(), inplace=True)\ndataset['rot_per'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As per Analysis of  columns BV,UB, we should feel this column with its mean value\ndataset['BV'].fillna(dataset['BV'].mean(), inplace=True)\ndataset['UB'].fillna(dataset['UB'].mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## We have filled Numerical data\nNow lets analyse thse data with diameter columns values"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analysys of Numerical data with Diameter"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Looking for Coorelation\ncorr_matrix = dataset.corr()\ncorr_matrix['diameter'].sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dataset.plot(kind = 'scatter', x = 'rot_per',y = 'diameter', alpha = 0.6)\nimport seaborn as sns\n#dataset.info()\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnum_data = dataset.select_dtypes(include=numerics)\n#num_data.info()\nplt.subplots(figsize=(15,12))\nsns.heatmap(num_data.corr(),annot=True,annot_kws={'size':10})\n#num_data.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# After analysing HeatMap we can element some columns which have no multicolinearity\n#e,i,w, condition_cofde, n_obs_use,albedo,not_per,ma\ndropNumColumn = ['e','i','w','condition_code','n_obs_used','rot_per','ma']\ndataset = dataset.drop(dropNumColumn, axis = 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(15,12))\nnum_data = dataset.select_dtypes(include=numerics)\nsns.heatmap(num_data.corr(),annot=True,annot_kws={'size':10})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lets Play With Categorical Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#corr_matrix.columns\ndataset.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categoricalData = dataset.select_dtypes(include=['object']).copy()\ncategoricalData.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categoricalData.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fill Missing data in categorical"},{"metadata":{"trusted":true},"cell_type":"code","source":"#categoricalData['spec_B'].value_counts()\ncategoricalData = categoricalData.fillna(categoricalData['spec_B'].value_counts().index[0])\ncategoricalData = categoricalData.fillna(categoricalData['spec_T'].value_counts().index[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Columns wise Distribution\nprint(categoricalData.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Represent Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# As we can see that\nclass_count = categoricalData['class'].value_counts()\nsns.set(style=\"darkgrid\")\nsns.barplot(class_count.index, class_count.values, alpha=0.9)\nplt.title('Frequency Distribution of Class')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Carrier', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dataset['neo'].value_counts()\ncategoricalData['pha'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder,OneHotEncoder\nlabelEnc = LabelEncoder()\ncategoricalData['neo'] = labelEnc.fit_transform(categoricalData['neo'])\ncategoricalData['pha'] = labelEnc.fit_transform(categoricalData['pha'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categoricalData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now do one hot encoder\ncategoricalData = pd.get_dummies(categoricalData, columns=['neo','pha'])\ncategoricalData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelBinarizer\n#categoricalDataClass = categoricalDataCopy.copy()\nlb = LabelBinarizer()\nlb_results = lb.fit_transform(categoricalData['class'])\nlb_results_df = pd.DataFrame(lb_results, columns=lb.classes_)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categoricalData = pd.concat([categoricalData, lb_results_df], axis=1)\ncategoricalData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categoricalData['class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncategoricalData['spec_B'] = labelEnc.fit_transform(categoricalData['spec_B'])\ncategoricalData['spec_T'] = labelEnc.fit_transform(categoricalData['spec_T'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categoricalData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now Drob Class column beacse it has been converted into LabelBinarizor\n# Drop name column it jus a name\ncategoricalData.drop(['name','class'], inplace = True, axis = 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categoricalData.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_data.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now Add Numerical and Categorical data which we hace cleaned and transformed"},{"metadata":{"trusted":true},"cell_type":"code","source":"cleanDataset = pd.concat([categoricalData,num_data],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleanDataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Split Data into features and target"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split Data into features and target\ny = cleanDataset['diameter']\nX = cleanDataset.drop(['diameter'],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.iloc[:,:].values\nX.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Scaling\nPrimarily, there are two types of feature scaling method:\n1. min-max scaling(Normalization)\n(values -min) / (max - min)  # Lies 0-1\n2. Standardization:\n(values - mean / std)\nfor this sklearn provide class standardScaler\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nmy_pipeline = Pipeline([\n     ('std_scaler', StandardScaler()),\n    # Add as many as you can\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_std = my_pipeline.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_std.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split Data set into Training and test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Selecting a Desired Model for Our Project"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n#model = LinearRegression()\n#model = DecisionTreeRegressor()\nmodel = RandomForestRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluating Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\ndiameterPrediction  = model.predict(X_test)\nlin_mse = mean_squared_error(y_test, diameterPrediction)\nlin_mse = np.sqrt(lin_mse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lin_mse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# R-Square\nR-squared is a statistical measure that represents the goodness of fit of a regression model. The ideal value for r-square is 1. The closer the value of r-square to 1, the better is the model fitted."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\nr2 = r2_score(y_test,diameterPrediction)\nprint(\"R2 : \",r2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using Better Eveluation Techniques:\nHow it works:\n1 2 3 4 5 : it create  5 group(cv : fold) (example it may more)\nit trains 2 3 4 5 and test 1\nagian it trains 1 3 4 5 and test 2\n and so on , finalyy returns score"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(model, X_train, y_train, scoring = \"neg_mean_squared_error\", cv = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rm_error = np.sqrt(-scores)# - because sqrt does not calculate negative value\nrm_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_score(score):\n    print(\"Score: \", score)\n    print(\"Mean: \", score.mean())\n    print(\"Std: \", score.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_score(rm_error)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}