{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\nimport subprocess\nfrom sklearn import tree\nfrom os import system\nfrom IPython.display import SVG\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.utils import shuffle\nfrom sklearn.preprocessing import LabelEncoder \nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Reading the data set**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def read_dataset():\n    df = pd.read_csv(\"../input/mines-vs-rocks/sonar.all-data.csv\")# Reading input sheet with read_csv method\n    X=df.iloc[:,0:60]\n    y=df.iloc[:,-1]\n    #Encode the independent variable \n    encoder=LabelEncoder()\n    encoder.fit(y)\n    Y=encoder.transform(y)\n    return(X,Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X,Y=read_dataset()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Shuffle the data to mix up rows**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X,Y=shuffle(X,Y,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Split dataset into training set and test set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=1) # 70% training and 30% test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Applying RandomForestClassifier**\n\n   *  To find the best hyper parameters for sonar dataset\n        * max_depth : max_depth represents the depth of each tree in the forest. The deeper the tree, the more splits it has and it captures more information about the data\n        * min_samples_leaf is The minimum number of samples required to be at a leaf node\n        * n_estimators represents the number of trees in the forest. Usually the higher the number of trees the better to learn the data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.ensemble import RandomForestClassifier\n\nmax_depth_array = [2,3,4,5]\nmin_samples_leaf_array = [2,3,4,5,6,7,8]\nn_estimators_array = [10,20,30,40,50,60,70,80,90,100]\n\n# Finding ROC_AUC_SCORE & Confusion Matrix for train and test data with multiple max depths,min_samples_leaf,n_estimators\nfor x in max_depth_array:\n    for y in min_samples_leaf_array:\n        for z in n_estimators_array:\n            randomforest_classifier = RandomForestClassifier(criterion='gini',\n                max_depth=x, max_features='auto',min_samples_leaf=y,\n                n_estimators=z,random_state=0)\n            \n            rf=randomforest_classifier.fit(X_train, y_train)\n            y_pred = rf.predict(X_train)\n            print(\"max_depth,min_samples_leaf,n_estimator -: \",x,'-',y,'-',z)\n            MSE = np.square(np.subtract(y_train,y_pred)).mean() \n            print(MSE)\n            print(confusion_matrix(y_train, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Min mean Square error and better prediction & confusion matrix**\n\nmax_depth,min_samples_leaf,n_estimator -: 3 - 2 - 100\n\nMSE - 0.041666666666666664\n\nConfusion Matrix\n\n[[80 0]\n\n[ 6 58]]"},{"metadata":{"trusted":true},"cell_type":"code","source":"randomforest_classifier = RandomForestClassifier(criterion='gini',\n                max_depth=3, max_features='auto',min_samples_leaf=2,\n                n_estimators=100,random_state=0)\n            \nrf=randomforest_classifier.fit(X_train, y_train)\ny_pred = rf.predict(X_train)\nMSE = np.square(np.subtract(y_train,y_pred)).mean() \nprint('Min Mean squared error - ',MSE)\nrf_cf=confusion_matrix(y_train, y_pred)\n\ncolors = [\"#0101DF\", \"#DF0101\"]\nsns.heatmap(rf_cf,annot=True, cmap=plt.cm.copper)\nplt.title('Random Forest Classifier \\n Confusion Matrix', fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**GradientBoostingClassifier**\n\n**Automated to find out best hyper parameters with prediction and min mse**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\n# Finding min mean squared error & Confusion Matrix for train and test data with multiple max depths,min_samples_leaf,n_estimators\nmax_depth_array = [2,3,4,5]\nmax_features_array = [2,3,4,5,6,7,8]\nn_estimators_array = [10,20,30,40,50,60,70,80,90,100]\nsample_weight_array= [1,2,3,4,5,6,7,8]\n\nfor x in max_depth_array:\n    for y in max_features_array:\n        for z in n_estimators_array:\n            for w in sample_weight_array:\n                gradientBoosting_classifier = GradientBoostingClassifier(\n                    learning_rate=0.05,\n                    n_estimators=z,\n                    max_depth=x,\n                    random_state=1,\n                    max_features=y)\n            \n                gb=gradientBoosting_classifier.fit(X_train, y_train)\n                y_pred = gb.predict(X_train)\n\n                print(\"max_depth,min_samples_leaf,n_estimator -: \",x,'-',y,'-',z,'-',w)\n                MSE = np.square(np.subtract(y_train,y_pred)).mean() \n                print(MSE)\n                print(confusion_matrix(y_train, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Min mean Square error and better prediction & confusion matrix**\n\nmax_depth,min_samples_leaf,n_estimator -: 2 - 2 - 90 - 1\n\nMSE - 0.0\n\nConfusion Matrix\n\n[[80 0]\n\n[ 0 64]"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\ngradientBoosting_classifier = GradientBoostingClassifier(\n                    learning_rate=0.05,\n                    n_estimators=90,\n                    max_depth=2,\n                    random_state=1,\n                    max_features=2)\n            \ngb=gradientBoosting_classifier.fit(X_train, y_train)\ny_pred = gb.predict(X_train)\n\nprint(\"max_depth,min_samples_leaf,n_estimator -: \",x,'-',y,'-',z,'-',w)\nMSE = np.square(np.subtract(y_train,y_pred)).mean() \nprint('Min Mean squared error - ',MSE)\nrf_cf=confusion_matrix(y_train, y_pred)\nprint(\"Accuracy score: \", accuracy_score(y_train,y_pred))\nprint(y_train.shape)\nsns.heatmap(rf_cf,annot=True, cmap=plt.cm.copper)\nplt.title('Random Forest Classifier \\n Confusion Matrix', fontsize=14)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}