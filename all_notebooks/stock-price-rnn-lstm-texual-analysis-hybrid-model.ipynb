{"cells":[{"metadata":{},"cell_type":"markdown","source":"## **Stock Market Prediction using Numerical and Textual Analysis**"},{"metadata":{},"cell_type":"markdown","source":"\n* Objective: Create a hybrid model for stock price/performance prediction using numerical analysis of historical stock prices, and sentimental analysis of news headlines.\n* Stock to analyze and predict - SENSEX (S&P BSE SENSEX)\n\n* Download historical stock prices from finance.yahoo.com\n* Download textual (news) data from https://bit.ly/36fFPI6\n\n\n\n\n# Author: Muhammet Varlı"},{"metadata":{},"cell_type":"markdown","source":"# **Analysis of Stock Market Prices and Stock Price Prediction by RNN and LSTM**"},{"metadata":{},"cell_type":"markdown","source":"### **Some information about financial data**"},{"metadata":{},"cell_type":"markdown","source":"* The adjusted closing price amends a stock's closing price to reflect that stock's value after accounting for any corporate actions.\n* It is often used when examining historical returns or doing a detailed analysis of past performance.\n* Stock values are stated in terms of the closing price and the adjusted closing price.\n* The closing price is the raw price, which is just the cash value of the last transacted price before the market closes.\n* The adjusted closing price factors in anything that might affect the stock price after the market closes."},{"metadata":{},"cell_type":"markdown","source":"* The high is the highest price at which a stock traded during a period. \n* The low is the lowest price of the period. "},{"metadata":{},"cell_type":"markdown","source":"* Volume is the total number of shares traded in a security over a period. Every time buyers and sellers exchange shares, the amount gets added to the period’s total volume. Studying volume patterns are an essential aspect of technical analysis because it can show the significance of a stock’s price movement.\n\n* A price change that occurs in high volume can carry more weight because it indicates that many traders were behind the move. Conversely, a lower volume price move can be perceived as less important."},{"metadata":{},"cell_type":"markdown","source":"## The changes in price of the stock overtime."},{"metadata":{},"cell_type":"markdown","source":"* Here we will analyze the changes in the stocks of various technology companies with simple visualization methods."},{"metadata":{"trusted":true},"cell_type":"code","source":"from warnings import filterwarnings\nfilterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Some Libraries Imported\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# For reading stock data from yahoo\nfrom pandas_datareader.data import DataReader\n# For time stamps\nfrom datetime import datetime","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The tech stocks we'll use for this analysis\ntech_list = ['AAPL', 'GOOG', 'TSLA', 'FB']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Take the datas for 2 years"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up End and Start times for data grab (We will analyze for 2 years)\nend = datetime.now()\nstart = datetime(end.year - 2, end.month, end.day)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#For loop for grabing yahoo finance data and setting as a dataframe\nfor stock in tech_list:   \n    # Set DataFrame as the Stock Ticker\n    globals()[stock] = DataReader(stock, 'yahoo', start, end)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"company_list = [AAPL, GOOG, TSLA, FB]\ncompany_name = [\"APPLE\", \"GOOGLE\", \"TESLA\", \"FACEBOOK\"]\n\nfor company, com_name in zip(company_list, company_name):\n    company[\"company_name\"] = com_name\n    \ndf = pd.concat(company_list, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Adj Close**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see a historical view of the closing price of companies\n\nplt.figure(figsize=(12, 8))\nplt.subplots_adjust(top=1.25, bottom=1.2)\ncolorlist=['Red','Blue','Green','Purple']\nfor i, company in enumerate(company_list, 1):\n    plt.subplot(2, 2, i)\n    company['Adj Close'].plot(color=colorlist[i-1])\n    plt.ylabel('Adj Close')\n    plt.xlabel(None)\n    plt.title(f\"{tech_list[i - 1]}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Volume**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's plot the total volume of stock being traded each day\nplt.figure(figsize=(15, 10))\nplt.subplots_adjust(top=1.25, bottom=1.2)\ncolorlist=['Black','Blue','Green','Purple']\nfor i, company in enumerate(company_list, 1):\n    plt.subplot(2, 2, i)\n    company['Volume'].plot(color=colorlist[i-1])\n    plt.ylabel('Volume')\n    plt.xlabel(None)\n    plt.title(f\"{tech_list[i - 1]}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The moving average of the various stocks"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the Moving Average Day\nma_day = [10, 20, 50]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for ma in ma_day:\n    for company in company_list:\n        column_name = f\"MA for {ma} days\"\n        company[column_name] = company['Adj Close'].rolling(ma).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(AAPL.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's go ahead and plot all the additional Moving Averages"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby(\"company_name\").hist(figsize=(12, 12),color='seagreen');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualization of companies changes over various MA days and 'Adj Close'"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=2, ncols=2)\nfig.set_figheight(8)\nfig.set_figwidth(15)\n\nAAPL[['Adj Close', 'MA for 10 days', 'MA for 20 days', 'MA for 50 days']].plot(ax=axes[0,0])\naxes[0,0].set_title('APPLE')\n\nGOOG[['Adj Close', 'MA for 10 days', 'MA for 20 days', 'MA for 50 days']].plot(ax=axes[0,1])\naxes[0,1].set_title('GOOGLE')\n\nTSLA[['Adj Close', 'MA for 10 days', 'MA for 20 days', 'MA for 50 days']].plot(ax=axes[1,0])\naxes[1,0].set_title('TESLA')\n\nFB[['Adj Close', 'MA for 10 days', 'MA for 20 days', 'MA for 50 days']].plot(ax=axes[1,1])\naxes[1,1].set_title('FACEBOOK')\n\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The daily return of the stock on average."},{"metadata":{"trusted":true},"cell_type":"code","source":"# We'll use pct_change to find the percent change for each day\nfor company in company_list:\n    company['Daily Return'] = company['Adj Close'].pct_change()\n\n# Then we'll plot the daily return percentage\nfig, axes = plt.subplots(nrows=2, ncols=2)\nfig.set_figheight(8)\nfig.set_figwidth(15)\n\nAAPL['Daily Return'].plot(ax=axes[0,0], legend=True, linestyle='--', marker='o',markerfacecolor='black')\naxes[0,0].set_title('APPLE')\n\nGOOG['Daily Return'].plot(ax=axes[0,1], legend=True, linestyle='--', marker='o',markerfacecolor='black')\naxes[0,1].set_title('GOOGLE')\n\nTSLA['Daily Return'].plot(ax=axes[1,0], legend=True, linestyle='--', marker='o',markerfacecolor='black')\naxes[1,0].set_title('TESLA')\n\nFB['Daily Return'].plot(ax=axes[1,1], legend=True, linestyle='--', marker='o',markerfacecolor='black')\naxes[1,1].set_title('FACEBOOK')\n\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Note the use of dropna() here, otherwise the NaN values can't be read by seaborn\nplt.figure(figsize=(12, 12))\n\nfor i, company in enumerate(company_list, 1):\n    plt.subplot(2, 2, i)\n    sns.distplot(company['Daily Return'].dropna(), bins=100, color='orange')\n    plt.ylabel('Daily Return')\n    plt.title(f'{company_name[i - 1]}')\n    # Skewness and Kurtosis\n    print(f'{company_name[i - 1]}'+\" Skewness: %f\" % company['Daily Return'].skew())\n    print(f'{company_name[i - 1]}'+\" Kurtosis: %f\" % company['Daily Return'].kurt())\n# Could have also done:\n#AAPL['Daily Return'].hist()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The correlation between different stocks Adj Close prices."},{"metadata":{},"cell_type":"markdown","source":"To build a DataFrame with all the ['Close'] columns for each of the stocks dataframes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grab all the closing prices for the tech stock list into one DataFrame\nclosing_df = DataReader(tech_list, 'yahoo', start, end)['Adj Close']\n\n# Let's take a quick look\nclosing_df.head() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have all the closing prices, let's go ahead and get the daily return for all the stocks, like we did for the Apple stock."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a new tech returns DataFrame\ntech_rets = closing_df.pct_change()\ntech_rets.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can compare the daily percentage return of two stocks to check how correlated. First let's see a sotck compared to itself."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Comparing Google to itself should show a perfectly linear relationship\nsns.jointplot('TSLA', 'TSLA', tech_rets, kind='scatter', color='seagreen')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We'll use joinplot to compare the daily returns of Google and Microsoft\nsns.jointplot('GOOG', 'FB', tech_rets, kind='scatter',color='darkblue')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If two stocks are perfectly (and positivley) correlated with each other a linear relationship bewteen its daily return values should occur.  We will use sns.pairplot() to automatically create this plot."},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can simply call pairplot on our DataFrame for an automatic visual analysis \n# of all the comparisons\n\nsns.pairplot(tech_rets, kind='kde')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can simply call pairplot on our DataFrame for an automatic visual analysis \n# of all the comparisons\n\nsns.pairplot(tech_rets, kind='reg')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above we can see all the relationships between all stocks regarding daily returns. For example, there does not seem to be a very high correlation relationship between Google and Tesla."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up our figure by naming it returns_fig, call PairPLot on the DataFrame\nreturn_fig = sns.PairGrid(tech_rets.dropna())\n\n# Using map_upper we can specify what the upper triangle will look like.\nreturn_fig.map_upper(plt.scatter, color='seagreen')\n\n# We can also define the lower triangle in the figure, inclufing the plot type (kde) \n# or the color map (BluePurple)\nreturn_fig.map_lower(sns.kdeplot, cmap='Greens_r')\n\n# Finally we'll define the diagonal as a series of histogram plots of the daily return\nreturn_fig.map_diag(plt.hist, bins=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up our figure by naming it returns_fig, call PairPLot on the DataFrame\nreturns_fig = sns.PairGrid(closing_df)\n\n# Using map_upper we can specify what the upper triangle will look like.\nreturns_fig.map_upper(plt.scatter,color='darkblue')\n\n# We can also define the lower triangle in the figure, inclufing the plot type (kde) or the color map (BluePurple)\nreturns_fig.map_lower(sns.kdeplot,cmap='Blues_r')\n\n# Finally we'll define the diagonal as a series of histogram plots of the daily return\nreturns_fig.map_diag(plt.hist,bins=30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's go ahead and use sebron for a quick correlation plot for the daily returns\nsns.heatmap(tech_rets.corr(), annot=True, cmap='summer')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(closing_df.corr(), annot=True, cmap='summer')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just as we suspected with our PairPlot, we see here that numerically and visually, Google and Tesla's daily stock return do not have very strong correlations compared to others. The strongest correlation is seen between Apple and Tesla. It's also interesting to see all tech companies positively associated."},{"metadata":{},"cell_type":"markdown","source":"# Risk Analysis"},{"metadata":{},"cell_type":"markdown","source":"One of the most basic ways to measure risk is to compare the expected return with the standard deviation of daily returns."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's start by defining a new DataFrame as a clenaed version of the oriignal tech_rets DataFrame\nrets = tech_rets.dropna()\n\narea = np.pi*20\n\nplt.figure(figsize=(12, 10))\nplt.scatter(rets.mean(), rets.std(), s=area)\nplt.xlabel('Expected return')\nplt.ylabel('Risk')\n\nfor label, x, y in zip(rets.columns, rets.mean(), rets.std()):\n    plt.annotate(label, xy=(x, y), xytext=(50, 50), textcoords='offset points', ha='right', va='bottom', \n                 arrowprops=dict(arrowstyle='-', color='blue', connectionstyle='arc3,rad=-0.3'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Selecting a company and concentrating on analysis on it**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary Stats\nAAPL.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# General info\nAAPL.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Column_List = [\"High\", \"Low\",\"Open\",\"Close\", \"Volume\",\"Adj Close\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate whisker plots to detect the presence of any outliers\nfig, ax = plt.subplots (len(Column_List), figsize = (10, 20))\n\nfor i, col_list in enumerate(Column_List):\n    sns.boxplot(AAPL[col_list], ax = ax[i], palette = \"winter\", orient = 'h')\n    ax[i].set_title(\"Whisker Plot for Outlier Detection on Apple Datas on\" + \" \" + col_list, fontsize = 10)\n    ax[i].set_ylabel(col_list, fontsize = 8)\n    fig.tight_layout(pad = 1.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize the spread and skweness through the distribution plot\n\n# Use the Column_List : list initialized above in the following steps\nfig, ax = plt.subplots(len(Column_List), figsize = (15, 10))\n\nfor i, col_list in enumerate(Column_List):\n    sns.distplot(AAPL[col_list], hist = True, ax = ax[i])\n    ax[i].set_title (\"Apple Datas Frequency Distribution of\" + \" \" + col_list, fontsize = 10)\n    ax[i].set_xlabel (col_list, fontsize = 8)\n    ax[i].set_ylabel ('Distribution Value', fontsize = 8)\n    fig.tight_layout (pad = 1.1) # To provide space between plots\n    ax[i].grid('on') # Enabled to view and make markings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AAPL[[\"Open\",\"High\",\"Low\",\"Close\"]].plot.area(figsize=(15,10),alpha=0.5);\nplt.title('Apple Finance Stock Trend')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A glimpse of how the market shares varied over the given time\n\n# Create a list for numerical columns that are to be visualized\nColumn_List = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n\n# Plot to view the same\nAAPL.plot(y = Column_List, subplots = True, layout = (3, 3), figsize = (15, 15), sharex = False, title = \"Apple Stock Value Trend from 2019-11 - 2020-11\", rot = 45);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Predicting the closing price stock price of APPLE :"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get the stock quote\ndf = DataReader('AAPL', data_source='yahoo', start='2019-06-30', end='2020-06-30')\n#Show teh data\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,8))\nplt.title('Close Price History')\nplt.plot(df['Close'])\nplt.xlabel('Date', fontsize=18)\nplt.ylabel('Close Price USD ($)', fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a new dataframe with only the 'Close column\ndata = df.filter(['Close'])\n#Convert the dataframe to a numpy array\ndataset = data.values\n#Get the number of rows to train the model on\ntraining_data_len = int(np.ceil( len(dataset) * .8 ))\n\ntraining_data_len","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Scale the data\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(0,1))\nscaled_data = scaler.fit_transform(dataset)\n\nscaled_data[0:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* LSTMs expect our data to be in a specific format, usually a 3D array. We start by creating data in 60 timesteps and converting it into an array using NumPy. Next, we convert the data into a 3D dimension array with X_train samples, 60 timestamps, and one feature at each step."},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"#Create the training data set\n#Create the scaled training data set\ntrain_data = scaled_data[0:int(training_data_len), :]\n#Split the data into x_train and y_train data sets\nx_train = []\ny_train = []\n\nfor i in range(60, len(train_data)):\n    x_train.append(train_data[i-60:i, 0])\n    y_train.append(train_data[i, 0])\n    if i<= 61:\n        print(x_train)\n        print(y_train)\n        print()\n        \n# Convert the x_train and y_train to numpy arrays \nx_train, y_train = np.array(x_train), np.array(y_train)\n\n#Reshape the data\nx_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n# x_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create the testing data set\n#Create a new array containing scaled values from index 1543 to 2002 \ntest_data = scaled_data[training_data_len - 60: , :]\n#Create the data sets x_test and y_test\nx_test = []\ny_test = dataset[training_data_len:, :]\nfor i in range(60, len(test_data)):\n    x_test.append(test_data[i-60:i, 0])\n    \n# Convert the data to a numpy array\nx_test = np.array(x_test)\n\n# Reshape the data\nx_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1 ))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Creating LSTM Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, LSTM\n\n#Build the LSTM model\nmodel_lstm = Sequential()\nmodel_lstm.add(LSTM(64, return_sequences=True, input_shape= (x_train.shape[1], 1)))\nmodel_lstm.add(LSTM(64, return_sequences= False))\nmodel_lstm.add(Dense(32))\nmodel_lstm.add(Dense(1))\n\n# Compile the model\nmodel_lstm.compile(optimizer='adam', loss='mean_squared_error')\n\n#Train the model\nmodel_lstm.fit(x_train, y_train, batch_size=20, epochs=20)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.019687,"end_time":"2020-09-06T15:29:06.75719","exception":false,"start_time":"2020-09-06T15:29:06.737503","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## **Creating RNN model**"},{"metadata":{"jupyter":{"outputs_hidden":true},"papermill":{"duration":124.480374,"end_time":"2020-09-06T15:31:11.257475","exception":false,"start_time":"2020-09-06T15:29:06.777101","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# importing libraries\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import SimpleRNN\nfrom keras.layers import Dropout\n\n# initializing the RNN\nmodel_rnn = Sequential()\n\n# adding first RNN layer and dropout regulatization\nmodel_rnn.add(SimpleRNN(units = 50,activation = \"tanh\", return_sequences = True,input_shape = (x_train.shape[1],1)))\nmodel_rnn.add(Dropout(0.2))\n# adding second RNN layer and dropout regulatization\nmodel_rnn.add(SimpleRNN(units = 50, activation = \"tanh\", return_sequences = True))\nmodel_rnn.add(Dropout(0.2))\n# adding third RNN layer and dropout regulatization\nmodel_rnn.add(SimpleRNN(units = 50,activation = \"tanh\", return_sequences = False))\nmodel_rnn.add(Dropout(0.2))\n# adding the output layer\nmodel_rnn.add(Dense(units = 1))\n# compiling RNN\nmodel_rnn.compile(optimizer = \"adam\", loss = \"mean_squared_error\")\n# fitting the RNN\nmodel_rnn.fit(x_train, y_train, epochs = 100, batch_size = 32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the LSTM model predicted price values \npredictions_lstm = model_lstm.predict(x_test)\npredictions_lstm = scaler.inverse_transform(predictions_lstm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the RNN models predicted price values \npredictions_rnn = model_rnn.predict(x_test)\npredictions_rnn = scaler.inverse_transform(predictions_rnn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Models Scores**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\n# Get the root mean squared error (RMSE)\nmse_lstm = metrics.mean_squared_error(y_test, predictions_lstm)\nrmse_lstm = np.sqrt(mse_lstm)\n\nprint(\"LSTM Model RMSE: \",rmse_lstm)\n# Get r2 score\nr2_lstm = metrics.r2_score(y_test, predictions_lstm)\nprint(\"LSTM Model r2: \",r2_lstm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the root mean squared error (RMSE)\nmse_rnn = metrics.mean_squared_error(y_test, predictions_rnn)\nrmse_rnn = np.sqrt(mse_rnn)\n\nprint(\"RNN Model RMSE: \",rmse_rnn)\n# Get r2 score\nr2_rnn = metrics.r2_score(y_test, predictions_rnn)\nprint(\"RNN Model r2: \",r2_rnn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Plotting Train Data, Validation Data and Predictions RNN Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the data\ntrain = data[:training_data_len]\nvalid = data[training_data_len:]\nvalid['Predictions_RNN'] = predictions_rnn\n# Visualize the data\nplt.figure(figsize=(16,8))\nplt.title('Model')\nplt.xlabel('Date', fontsize=18)\nplt.ylabel('Close Price USD ($)', fontsize=18)\nplt.plot(train['Close'])\nplt.plot(valid[['Close', 'Predictions_RNN']])\nplt.legend(['Train', 'Val', 'Predictions_RNN'], loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Show the valid and predicted prices\nvalid[0:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Plotting Train Data, Validation Data and Predictions LSTM Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the data\ntrain = data[:training_data_len]\nvalid = data[training_data_len:]\nvalid['Predictions_LSTM'] = predictions_lstm\n# Visualize the data\nplt.figure(figsize=(16,8))\nplt.title('Model')\nplt.xlabel('Date', fontsize=18)\nplt.ylabel('Close Price USD ($)', fontsize=18)\nplt.plot(train['Close'])\nplt.plot(valid[['Close', 'Predictions_LSTM']])\nplt.legend(['Train', 'Val', 'Predictions_LSTM'], loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Show the valid and predicted prices\nvalid[0:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Sentiment Analysis of Indian News Headlines**"},{"metadata":{},"cell_type":"markdown","source":"### Loading Textual Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"ndf = pd.read_csv('../input/tsf-datasets/india-news-headlines.csv', parse_dates=[0], infer_datetime_format=True,error_bad_lines=False,usecols =[\"publish_date\",\"headline_text\"])\nndf = ndf.rename(columns={\"publish_date\": \"Date\"})\nndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ndf.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_date = pd.to_datetime('2019-06-30')\nend_date = pd.to_datetime('2020-06-30')\nndf=ndf.loc[(ndf['Date'] > start_date) & (ndf['Date'] < end_date)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ndf=ndf.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ndf=ndf.drop(\"index\",axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ndf.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping duplicates by grouping the same dates.\nndf['headline_text'] = ndf.groupby(['Date']).transform(lambda x : ' '.join(x)) \nndf = ndf.drop_duplicates() \nndf.reset_index(inplace = True, drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Pre-Processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# uppercase-lowercase conversion\nndf['headline_text'] = ndf['headline_text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# punctuation\nndf['headline_text'] = ndf['headline_text'].str.replace('[^\\w\\s]','')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# numbers\nndf['headline_text'] = ndf['headline_text'].str.replace('\\d','')\n","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"#!pip install nltk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#stopwords\nimport nltk\nnltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nsw = stopwords.words('english')\nndf['headline_text'] = ndf['headline_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Deletion of sparse.\ndelete = pd.Series(' '.join(ndf['headline_text']).split()).value_counts()[-1000:]\nndf['headline_text'] = ndf['headline_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in delete))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.download('wordnet')","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"#!pip install textblob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lemmatisation\nfrom textblob import Word\n\nndf['headline_text'] = ndf['headline_text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()])) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ndf.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ndf['headline_text'][0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ndf[ndf['headline_text'].duplicated(keep=False)].sort_values('headline_text').head(8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sentiment Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"from textblob import TextBlob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Functions to get the subjectivity and polarity\ndef getSubjectivity(text):\n    return TextBlob(text).sentiment.subjectivity\n\ndef getPolarity(text):\n    return  TextBlob(text).sentiment.polarity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ndf['Subjectivity'] = ndf['headline_text'].apply(getSubjectivity)\nndf['Polarity'] = ndf['headline_text'].apply(getPolarity)\nndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,6))\nndf['Polarity'].hist(color = 'purple')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,6))\nndf['Subjectivity'].hist(color = 'blue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.download('vader_lexicon')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Adding sentiment score to ndf by using SentimentIntensityAnalyzer\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nsia = SentimentIntensityAnalyzer()\n\nndf['Compound'] = [sia.polarity_scores(v)['compound'] for v in ndf['headline_text']]\nndf['Negative'] = [sia.polarity_scores(v)['neg'] for v in ndf['headline_text']]\nndf['Neutral'] = [sia.polarity_scores(v)['neu'] for v in ndf['headline_text']]\nndf['Positive'] = [sia.polarity_scores(v)['pos'] for v in ndf['headline_text']]\nndf[0:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_merge = pd.merge(df, ndf, how='inner', on='Date')\ndf_merge.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_merge.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Hybrid model Analysis**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final = df_merge[['Close','Subjectivity', 'Polarity', 'Compound', 'Negative', 'Neutral' ,'Positive']]\ndf_final","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Scaling using MinMaxScaler"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler()\ndf_scaled = pd.DataFrame(sc.fit_transform(df_final))\ndf_scaled.columns = df_final.columns\ndf_scaled.index = df_final.index\ndf_scaled.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create Hybrid Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df_scaled.drop(\"Close\",axis=1)\ny=df_scaled[\"Close\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train-Test "},{"metadata":{"trusted":true},"cell_type":"code","source":"# shuffle and split training and test sets\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\nfrom sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Using Random Forest Regression\nrf = RandomForestRegressor()\nrf.fit(X_train, y_train)\ny_pred_rf=rf.predict(X_test)\n\nmse_rf = metrics.mean_squared_error(y_test, y_pred_rf)\nrmse_rf = np.sqrt(mse_rf)\n\nprint(\"Random Forest Model RMSE: \",rmse_rf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Xgboost"},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"# !pip install xgboost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\nxgb = XGBRegressor(colsample_bytree = 0.6, \n                         learning_rate = 0.01, \n                         max_depth = 2, \n                         n_estimators = 1000) \nxgb.fit(X_train, y_train)\ny_pred_xgb=xgb.predict(X_test)\n\nmse_xgb = metrics.mean_squared_error(y_test, y_pred_xgb)\nrmse_xgb = np.sqrt(mse_xgb)\n\nprint(\"XGB Model RMSE: \",rmse_xgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Light GBM"},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"# !pip install lightgbm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMRegressor\nlgbm = LGBMRegressor(learning_rate = 0.1, \n                           max_depth = 8, \n                           n_estimators = 50,\n                           colsample_bytree = 0.4,\n                           num_leaves = 10)\nlgbm.fit(X_train, y_train)\ny_pred_lgbm=lgbm.predict(X_test)\n\nmse_lgbm = metrics.mean_squared_error(y_test, y_pred_lgbm)\nrmse_lgbm = np.sqrt(mse_lgbm)\n\nprint(\"Light GBM Model RMSE: \",rmse_lgbm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Catboost"},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"# !pip install catboost","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"from catboost import CatBoostRegressor \ncatb = CatBoostRegressor(iterations = 500, \n                               learning_rate = 0.1, \n                               depth = 5)\ncatb.fit(X_train, y_train)\ny_pred_cat=catb.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mse_cat = metrics.mean_squared_error(y_test, y_pred_cat)\nrmse_cat = np.sqrt(mse_cat)\n\nprint(\"Catboost Model RMSE: \",rmse_cat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### AdaBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Using AdaBoostRegressor\nadb = AdaBoostRegressor()\nadb.fit(X_train, y_train)\ny_pred_adb=adb.predict(X_test)\n\nmse_adb = metrics.mean_squared_error(y_test, y_pred_adb)\nrmse_adb = np.sqrt(mse_adb)\n\nprint(\"AdaBoost Model RMSE: \",rmse_adb)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}