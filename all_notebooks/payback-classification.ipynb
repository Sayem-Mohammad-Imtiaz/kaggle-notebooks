{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n!pip install pycaret\n!pip install pycaret\nfrom pycaret import classification\nfrom pycaret.classification import * \nimport numpy as np \nimport pandas as pd\npd.set_option(\"display.max_columns\", 80)\npd.set_option(\"display.max_rows\", 20)\nimport matplotlib.pyplot as plt\npd.plotting.register_matplotlib_converters()\n%matplotlib inline\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(10,7)})\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold, StratifiedKFold, GridSearchCV\nfrom sklearn.feature_selection import VarianceThreshold, SelectKBest,f_classif,mutual_info_classif\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom itertools import chain, combinations\nfrom xgboost import XGBClassifier\nfrom xgboost import XGBRFClassifier\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nprint('ok')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loading Data \ndane = pd.read_csv('../input/predicting-profitable-customer-segments/customerTargeting.csv', delimiter=',')\n#dane.dataframeName = 'customerTargeting.csv'\n#Dropping variables which was recorded after the campaign was run\ndane = dane.drop(['g1_21', 'g2_21', 'c_28'], axis = 1)\nY = dane['target']\nX = dane.drop(['target'],axis=1)\nX_train, X_valid,Y_train,Y_valid = train_test_split(X, Y, test_size = 0.25)\ndane.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dane.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dane.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualization functions\n# Distribution graphs (histogram/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()\n    \n# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n    #filename = df.dataframeName\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()\n    \n    \ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <h1 style=\"background-color:blue;color:black;text-align:center;\">Distribution graphs (histogram/bar graph) of sampled columns</h1>\n","metadata":{}},{"cell_type":"code","source":"plotPerColumnDistribution(dane, 30, 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <h1 style=\"background-color:blue;color:black;text-align:center;\">Mutual Information</h1>","metadata":{}},{"cell_type":"code","source":"mutual_info = mutual_info_classif(X,Y)\nmutual_info = pd.Series(mutual_info)\nmutual_info.index = X.columns\nmutual_info.sort_values(ascending=False)\n\nplt.figure(dpi=100, figsize=(17, 17))\nplot_mi_scores(mutual_info)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mutual_info_best = mutual_info[mutual_info > mutual_info.median()]\nplot_mi_scores(mutual_info_best)\ncols_MI = list(mutual_info_best.index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <h1 style=\"background-color:blue;color:black;text-align:center;\">Correlation matrix</h1>\n","metadata":{}},{"cell_type":"code","source":"plotCorrelationMatrix(dane[cols_MI], 17)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking for constant columns ","metadata":{}},{"cell_type":"code","source":"var_thres=VarianceThreshold(threshold=0)\nvar_thres.fit(X)\nsum(var_thres.get_support()) #Counting columns with variance threshold by grt_support method\nconstant_columns = [column for column in X.columns #Checking for contsant columns \n                    if column not in X.columns[var_thres.get_support()]]\nprint('constant_columns: \\n' + str(constant_columns))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Selecting K-Best Features based on Target","metadata":{}},{"cell_type":"code","source":"#Automatyczna selekcja kolumn z scikit learn\nsel_cols = SelectKBest(f_classif, k=10).fit(X[cols_MI],Y)\nBest_cols = list(X[cols_MI].columns[sel_cols.get_support()])\ncols_ignore = list(set(dane.columns) - set(Best_cols) - set(['target']))\n#Podział kolumn na typy\ng1_cols = [col for col in X.columns if 'g1_' in col]\ng2_cols = [col for col in X.columns if 'g2_' in col]\nc_cols = [col for col in X.columns if 'c_' in col]\n#Reczny wybór kolumn\nJ_best_cols = ['g1_1', 'g2_1', 'g1_12', 'g2_12', 'g1_13', 'g2_13'] + c_cols\nJ_cols_ignore = list(set(dane.columns) - set(J_best_cols) - set(['target']))\n#Podsumowanie\nprint('Best_cols '+str(len(Best_cols))+' \\n' + str(Best_cols) )\n      #+'\\n\\nJ_best_cols '+str(len(J_best_cols))+' \\n' + str(J_best_cols))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <h1 style=\"background-color:blue;color:black;text-align:center;\">Analysing Target</h1>","metadata":{}},{"cell_type":"code","source":"plt.title(\"Distribution of target variable in numbers\")\ngraph = sns.countplot(x='target', data = dane)\nfor p in graph.patches:\n    graph.annotate('{:.2f}'.format(p.get_height()), (p.get_x()+0.3, p.get_height()),\n            ha='center', va='bottom',\n            color= 'black')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.title(\"Distribution of target variable in percent\")\ngraph = sns.barplot(x=\"target\", y=\"target\", data=dane, estimator=lambda x: len(x) / len(dane) )\ngraph.set(ylabel=\"Percent\")\nfor p in graph.patches:\n    graph.annotate(\"{:.1%}\".format(p.get_height()), (p.get_x()+0.3, p.get_height()),\n            ha='center', va='bottom',\n            color= 'black')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Pycaret\n# Setting up of variables depending on its nature (continuous or Categorical) and also selecting those feature which are not important \n# for further analysis.\nclassification_setup=setup(data = dane ,target='target', numeric_features=Best_cols,\n                           ignore_features = cols_ignore ,silent = True)\n\n#compare_models()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"compare_models()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Random Forest Classifier\nmy_model = RandomForestClassifier(n_estimators = 200, max_depth = 3, max_features  = 10,\n                                 oob_score = True, criterion = 'entropy')\nmy_model.fit(X[Best_cols], Y)\nprint('Accuracy: %.2f%%' % (my_model.oob_score_*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Gradient Boosting Classifier\nmy_model = XGBRFClassifier(n_estimators = 100,  max_depth = 2, \n                           eval_metric = 'mlogloss', learning_rate = 0.01, \n                           booster = 'dart', use_label_encoder=False)\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\nresults = cross_val_score(my_model, X[Best_cols], Y, cv=kfold, scoring = 'accuracy')\nprint('Accuracy: %.2f%%' % (results.mean()*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ada Boost Classifier\nmodel = AdaBoostClassifier(DecisionTreeClassifier(max_depth = 1),\n                          n_estimators = 20, learning_rate = 0.5)\n# evaluate the model\ncv = StratifiedKFold(n_splits=5, random_state=1)\nn_scores = cross_val_score(model, X, Y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n# report performance\nprint('Accuracy: %.2f%%' % (mean(n_scores)*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#Deep learning - prosta sieć neuronowa\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.utils import np_utils\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\nseed = 10\nnp.random.seed(seed)\n# Normalize features within range 0 (minimum) and 1 (maximum)\nscaler = MinMaxScaler(feature_range=(0, 1))\n#X = scaler.fit_transform(X)\n#X = pd.DataFrame(X)\n# Convert target Y to one hot encoded Y for Neural Network\nY_nn = pd.get_dummies(Y)\n# For Keras, convert dataframe to array values (Inbuilt requirement of Keras)\nX_nn = X[Best_cols].values\nY_nn = Y_nn.values\n\nX_train, X_valid, Y_train, Y_valid = train_test_split(X_nn, Y_nn, test_size = 0.25)\nmodel = keras.Sequential([\n    layers.Dropout(0.05, input_shape=[10]),\n    layers.Dense(8, activation='relu'),\n    layers.Dense(4, activation='relu'),    \n    layers.Dense(3, activation='sigmoid'),\n])\n\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy'],\n)\n\nearly_stopping = keras.callbacks.EarlyStopping(\n    patience=10,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\n\nhistory = model.fit(\n    X_train, Y_train,\n    validation_data=(X_valid, Y_valid),\n    batch_size=200,\n    epochs=200,\n    #callbacks=[early_stopping],\n    verbose=0, # hide the output because we have so many epochs\n)\n\nhistory_df = pd.DataFrame(history.history)\n# Start the plot at epoch 5\nhistory_df.loc[5:, ['loss', 'val_loss']].plot()\nhistory_df.loc[5:, ['accuracy', 'val_accuracy']].plot()\n\nprint((\"Best Validation Loss: {:0.4f}\" +\\\n      \"\\nBest Validation Accuracy: {:0.4f}\")\\\n      .format(history_df['val_loss'].min(), \n              history_df['val_accuracy'].max()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**This is the solution that I was able to get in the available time. The best models have an accuracy very close to 60%. This is certainly a big improvement over randomizing. The model can certainly still be significantly improved. Not all of my work is shown here, I also checked models with a complete set of variables, but they gave worse results. The key factor here seems to be a detailed analysis of the variables and their appropriate selection. Jerzy Szocik. Greetings.**\n","metadata":{}}]}