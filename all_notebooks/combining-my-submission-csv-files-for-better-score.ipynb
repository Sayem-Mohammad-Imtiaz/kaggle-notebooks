{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Combining my submission.csv files for a better score\nOver time I have have experimented with various machine learning techniques applied to the [House Prices: Advanced Regression Techniques competition](https://www.kaggle.com/c/house-prices-advanced-regression-techniques), each time submitting a `submission.csv` file. Here is a table of my results:\n\n| Technique | Score | Explained variance |\n| :--- | --- | --- |\n| [neural network](https://www.kaggle.com/carlmcbrideellis/very-simple-neural-network-regression)| 0.23181 | 0.69091 |\n| [Gaussian process](https://www.kaggle.com/carlmcbrideellis/gaussian-process-regression-sample-script) | 0.21004 | 0.76409 |\n| [Random forest](https://www.kaggle.com/carlmcbrideellis/random-forest-regression-minimalist-script) | 0.17734 | 0.86514 |\n| [XGBoost](https://www.kaggle.com/carlmcbrideellis/very-simple-xgboost-regression) | 0.15617 | 0.90148 |\n| [CatBoost](https://www.kaggle.com/carlmcbrideellis/catboost-regression-minimalist-script) | 0.15270 | 0.90096 |\n\nI thought it would be fun to find a [linear combination](https://en.wikipedia.org/wiki/Linear_combination) of my `submission.csv` files that gives a better leaderboard score than any of the individual submissions, as well as having a better [explained variance score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.explained_variance_score.html). To do this I make use of the excellent notebook [\"Finding Ensemble Weights\"](https://www.kaggle.com/hsperr/finding-ensamble-weights) written by [Henning Sperr](https://www.kaggle.com/hsperr).","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy  as np\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.metrics import explained_variance_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"read in my `submission.csv` files","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"s1 = pd.read_csv(\"../input/very-simple-neural-network-regression/submission.csv\")\ns2 = pd.read_csv(\"../input/gaussian-process-regression-sample-script/submission.csv\")\ns3 = pd.read_csv(\"../input/random-forest-regression-minimalist-script/submission.csv\")\ns4 = pd.read_csv(\"../input/very-simple-xgboost-regression/submission.csv\")\ns5 = pd.read_csv(\"../input/catboost-regression-minimalist-script/submission.csv\")\n\n\nn_submission_files = 5\n# also create a placeholder dataFrame\ns_final = pd.read_csv(\"../input/very-simple-xgboost-regression/submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we shall also read in the [ground truth (correct) target values](https://www.kaggle.com/carlmcbrideellis/house-prices-advanced-regression-solution-file):","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"solution   = pd.read_csv('../input/house-prices-advanced-regression-solution-file/submission.csv')\ny_true     = solution[\"SalePrice\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now use [scipy.optimize.minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) to find the lowest score using the evaluation metric of the House Prices competition, which in this case is the root of the [mean squared logarithmic error regression loss](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_log_error.html)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.optimize import minimize\n\ntmp_scores = []\ntmp_weights = []\npredictions = []\npredictions.append( s1[\"SalePrice\"] )\npredictions.append( s2[\"SalePrice\"] )\npredictions.append( s3[\"SalePrice\"] )\npredictions.append( s4[\"SalePrice\"] )\npredictions.append( s5[\"SalePrice\"] )\n\ndef scoring_function(weights):\n    final_prediction = 0\n    for weight, prediction in zip(weights, predictions):\n            final_prediction += weight*prediction\n    return np.sqrt(mean_squared_log_error(y_true, final_prediction))\n\nfor i in range(150):\n    starting_values = np.random.uniform(size=n_submission_files)\n    bounds = [(0,1)]*len(predictions)\n    result = minimize(scoring_function, \n                      starting_values, \n                      method='L-BFGS-B', \n                      bounds=bounds, \n                      options={'disp': False, 'maxiter': 10000})\n    tmp_scores.append(result['fun'])\n    tmp_weights.append(result['x'])\n\nbestWeight = tmp_weights[np.argmin(tmp_scores)]\nprint('Best weights', bestWeight)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can see that the best combination is a mix consisting of 2.73% Gaussian process, 7.3% random forest, 17.7% XGBoost and finally 72% CatBoost. Let us now take a look at the results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"s_final[\"SalePrice\"] = s1[\"SalePrice\"]*bestWeight[0] + s2[\"SalePrice\"]*bestWeight[1] +  s3[\"SalePrice\"]*bestWeight[2] +  s4[\"SalePrice\"]*bestWeight[3] +  s5[\"SalePrice\"]*bestWeight[4]\n\nprint(\"The new score is %.5f\" % np.sqrt( mean_squared_log_error(y_true, s_final[\"SalePrice\"]) ) )\nprint(\"The new explained variance is %.5f\" % explained_variance_score(y_true, s_final[\"SalePrice\"]) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Success!\nIt looks like we were able to find a judicious combination of weights that does indeed result in a better `submission.csv` than any of the component `submission.csv` files. Let us now submit this new solution to the competition:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"s_final.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}