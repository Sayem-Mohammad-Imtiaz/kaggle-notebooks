{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import libraris\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport warnings \nwarnings.filterwarnings('ignore')\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.value_counts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Unnamed: 32']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Unnamed: 32'].value_counts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Unnamed: 32'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df.drop(['Unnamed: 32'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['diagnosis'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Histogram \n\n* A histogram is a graphical display of data using bars of different heights, How many times each value appears in dataset. The discription is called the distribution of variable.\n\n* Most common way to represent distribution of variable is histogram that is graph which shows frequency of each values.\n \n* frequency= Number of time each value appears.\n \n* Example:  [1,1,1,2,2,2,2] Frequency of 1 is three and frequency of 2 is four."},{"metadata":{"trusted":true},"cell_type":"code","source":"m = plt.hist(df[df['diagnosis']=='M'].radius_mean,bins=30,fc = (1,0,0,0.5),label='Malignant')\nprint(m[0])\nprint(m[1])\nprint(m[2])\nb = plt.hist(df[df['diagnosis']=='B'].radius_mean,bins=30,fc = (0,1,0,0.5),label='Bening')\nprint(b[0])\nprint(b[1])\nprint(b[2])\nplt.legend()\nplt.xlabel('Radius Mean Values')\nplt.ylabel('Frequency')\nplt.title('Histogram of radius mean for Bening and Malignant Tumors')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this graph you can see that radius mean of malignant tumors are bigger than radius mean of bening tumors mostly.\n\nThe bening distribution is approximately bell shaped that is shape of normal distribution(gaussian distribution)\n"},{"metadata":{},"cell_type":"markdown","source":"## Outliers \n\nwhile looking histogram as you can see there are rare values in bening distribution \n\nthere values can be errors or rare values , these can be called outliers.\n\nCalculating Outliers:\n\n* first we need to first Quartile Q1/25%\n\n* find IQR = q3-q1\n\n* finally compute Q1 - 1.5IQR and Q3 + 1.5IQR\n\n* Anything outside this range is an outlier"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_bening = df[df['diagnosis']=='B']\ndata_malignant=df[df['diagnosis']=='M']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_bening.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_malignant.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"desc=data_bening.radius_mean.describe()\ndesc.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(desc[0])\nprint(desc[1])\nprint(desc[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Q1=desc[4]\nQ1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Q2=desc[5]\nQ2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Q3=desc[6]\nQ3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IQR=Q3-Q1\nlower_bound = Q1 - 1.5*IQR\nupper_bound = Q3 + 1.5*IQR\nprint(\"Anything outside this range is an outlier: (\", lower_bound ,\",\", upper_bound,\")\")\ndata_bening[data_bening.radius_mean < lower_bound].radius_mean\nprint(\"Outliers: \",data_bening[(data_bening.radius_mean < lower_bound) | (data_bening.radius_mean > upper_bound)].radius_mean.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Box Plot"},{"metadata":{},"cell_type":"markdown","source":"* You can see outliers also from box plots\n* We found 3 outlier in bening radius mean and in box plot there are 3 outlier."},{"metadata":{"trusted":true},"cell_type":"code","source":"melted_data = pd.melt(df,id_vars = \"diagnosis\",value_vars = ['radius_mean', 'texture_mean'])\nprint(melted_data)\nplt.figure(figsize = (15,10))\nsns.boxplot(x = melted_data[\"variable\"], y = melted_data[\"value\"], hue=melted_data[\"diagnosis\"],data=melted_data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Summary Statistics"},{"metadata":{},"cell_type":"markdown","source":"* Mean\n* Variance: spread of distribution\n* Standart deviation square root of variance\n* Lets look at summary statistics of bening tumor radiance mean"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"mean: \",data_bening.radius_mean.mean())\nprint(\"variance: \",data_bening.radius_mean.var())\nprint(\"standart deviation (std): \",data_bening.radius_mean.std())\nprint(\"describe method: \",data_bening.radius_mean.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Cumulative Distribution Function**\n\nCDF is the probability that the variable takes a less than or equal to x.\n\nP(X<x)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(data_bening.radius_mean,bins=50,fc=(0,1,0,0.5),label='Bening',density=True,cumulative = True)\n\nsorted_data = np.sort(data_bening.radius_mean)\n\ny = np.arange(len(sorted_data))/float(len(sorted_data)-1)\n\nplt.plot(sorted_data,y,color='red')\n\nplt.title('CDF of bening tumor radius mean')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In above graph p(x<12) is 0.5 , we can observe how the data distributed while plotting the CDF"},{"metadata":{},"cell_type":"markdown","source":"## Relation between Variables\n\n* we can say that two variables are related with each other,if one of them gives information about others \n\n* scatter plot is simplest way to check relationship between two variables.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,10))\nsns.jointplot(df.radius_mean,df.area_mean,kind=\"regg\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### we can look relationship between more than 2 distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style = \"white\")\ndf1 = df.loc[:,[\"radius_mean\",\"area_mean\",\"fractal_dimension_se\"]]\ng = sns.PairGrid(df1,diag_sharey = False,)\ng.map_lower(sns.kdeplot,cmap=\"Blues_d\")\ng.map_upper(plt.scatter)\ng.map_diag(sns.kdeplot,lw =3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation \n\n* strength of the relationship between teo variables.\n\n* lets look at correlation between all features."},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(figsize = (18,18))\nsns.heatmap(df.corr(),annot= True,linewidths=0.5,fmt = \".1f\",ax=ax)\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.title('Correlation Map')\nplt.savefig('graph.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Huge matrix that includes a lot of numbers\n* The range of this numbers are -1 to 1.\n* Meaning of 1 is two variable are positively correlated with each other like radius mean and area mean\n* Meaning of zero is there is no correlation between variables like radius mean and fractal dimension se\n* Meaning of -1 is two variables are negatively correlated with each other like radius mean and fractal dimension mean.\n* Actually correlation between of them is not -1, it is -0.3 but the idea is that if sign of correlation is negative that means that there is negative correlation."},{"metadata":{},"cell_type":"markdown","source":"## Covariance \n\n* Covariance is measure of the tendency of two variables to vary together, so Covariance is maximized if two vectors are identical.\n\n* Covariance is zero if they are orthogonal.\n \n* Covariance is negative if they point in opposite direction\n \n* lets look at covariance between radius mean and area mean, Then look at radius mean and fractal dimension."},{"metadata":{"trusted":true},"cell_type":"code","source":"np.cov(df1.radius_mean,df1.area_mean)\nprint(\"Covariance between radius mean and area mean: \",df1.radius_mean.cov(df1.area_mean))\nprint(\"Covariance between radius mean and fractal dimension se: \",df1.radius_mean.cov(df1.fractal_dimension_se))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pearson Correlation \n\n* Division of covariance by standard deviation of variables.\n\n* lets look at pearson correlation radius mean and area mean\n\n* First lets use .corr() method that we used actually at correlation part. In correlation part we actually used pearson correlation \n\n* p1 and p2 is the same. In p1 we use corr() method, in p2 we apply definition of pearson correlation (cov(A,B)/(std(A)*std(B)))\n\n* As we expect pearson correlation between area_mean and area_mean is 1 that means that they are same distribution\n\n* Also pearson correlation between area_mean and radius_mean is 0.98 that means that they are positively correlated with each other and relationship between of the is very high.\n\n* To be more clear what we did at correlation part and pearson correlation part is same.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"p1 = df1.loc[:,[\"area_mean\",\"radius_mean\"]].corr(method= \"pearson\")\np2 = df1.radius_mean.cov(df1.area_mean)/(df1.radius_mean.std()*df1.area_mean.std())\nprint('Pearson correlation: ')\nprint(p1)\nprint('Pearson correlation: ',p2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Spearman's Rank Correlation \n\n* Pearson correlation works well if the relationship between variables are linear and variables are roughly normal. But it is not robust, if there are outliers\n* To compute spearman's correlation we need to compute rank of each value"},{"metadata":{"trusted":true},"cell_type":"code","source":"ranked_data = df1.rank()\nspearman_corr = ranked_data.loc[:,[\"area_mean\",\"radius_mean\"]].corr(method= \"pearson\")\nprint(\"Spearman's correlation: \")\nprint(spearman_corr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Spearman's correlation is little higher than pearson correlation\n* If relationship between distributions are non linear, spearman's correlation tends to better estimate the strength of relationship\n* Pearson correlation can be affected by outliers. Spearman's correlation is more robust."},{"metadata":{},"cell_type":"markdown","source":"## Mean VS Median\n\n* Sometimes instead of mean we need to use median. I am going to explain why we need to use median with an example\n* Lets think that there are 10 people who work in a company. Boss of the company will make raise in their salary if their mean of salary is smaller than 5"},{"metadata":{"trusted":true},"cell_type":"code","source":"salary = [1,4,3,2,5,4,2,3,1,500]\nprint(\"Mean of salary: \",np.mean(salary))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Median of salary: \",np.median(salary))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now median of the salary is 3 and it is less than 5 and employees will take raise in their sallaries and they are happy and this situation is fair "},{"metadata":{},"cell_type":"markdown","source":"## Hypothesis Testing\n\nClassical Hypothesis Testing\n\n* we want to answer this question: given a sample and a apparent effect what is the probability of seeing such an effect by chance.\n\n* the first step is to quantify the size of the apparent effect by choosing a test statistics. Natural choice for the test statistics is the difference in means between groups.\n\n* The second step is to define null hypothesis that is model of the system based on the assumption that the apparent effect is not real. A null hypothesis is a type of hypothesis used in statistics that proposes that no statistical significance exists in a set of given observations. The null hypothesis is a hypothesis which people tries to disprove it. Alternative hypothesis is a hypothesis which people want to tries to prove it.\n\n* Third step is compute p-value that is probablity of seeing the apparent effect if the null hypothesis is true. Suppose we have null hypothesis test. Then we calculate p value. If p value is less than or equal to a threshold, we reject null hypothesis.\n\n* If the p-value is low, the effect is said to be statistacally significant that means that it is unlikely to have occured by chance. Therefore we can say that the effect is more likely to appear in the larger population.\n\n* Lets have an example. Null hypothesis: world is flatten. Alternative hypothesis: world is round. Several scientists set out to disprove the null hypothesis. This eventually led to the refection of the null hypothesis and acceptance of the alternative hypothesis.\n\n* Other example. \"this effect is real\" this is null hypothesis. Based on that assumption we compute the probability of the apparent effect. That is the p-value. If p-value is low, we conclude that null hypothesis is unlikely to be true.\n\n\n* Now lets make our example:\n    1. I want to learn that are radius mean and area mean related with each other? My null hypothesis is that \"relationship between radius mean and area mean is zero in tumor population'.\n    2. Now we need to refute this null hypothesis in order to demonstrate that radius mean and area mean are related. (actually we know it from our previous experiences)\n\nlets find p-value (probability value)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"statistic, p_value = stats.ttest_rel(df1.radius_mean,df1.area_mean)\nprint('p-value: ',p_value)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"P value is almost zero, so we can reject Null Hypothesis "},{"metadata":{},"cell_type":"markdown","source":"## **Normal(Gaussian) Distribution and z-score**"},{"metadata":{},"cell_type":"markdown","source":"https://en.wikipedia.org/wiki/Normal_distribution\n\nlet's, create 100000 sample and visualize it with histogram.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# parameters of normal distribution\nmu, sigma = 110, 20  # mean and standard deviation\ns = np.random.normal(mu, sigma, 100000)\nprint(\"mean: \", np.mean(s))\nprint(\"standart deviation: \", np.std(s))\n# visualize with histogram\nplt.figure(figsize = (10,7))\nplt.hist(s, 100, density=False)\nplt.ylabel(\"frequency\")\nplt.xlabel(\"IQ\")\nplt.title(\"Histogram of IQ\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As it can be seen from histogram most of the people are cumulated near to 110 that is mean of our normal distribution"},{"metadata":{},"cell_type":"markdown","source":"Based on above graph we can get lot information , like we can cal Confidence intervals which help us to understand the data how the data distributed.\n\nwe can calculate CDF , PDF , Z-score etc \n\nSo we can use QQ Plot, KS test and Log transformation etc use these techniques and convert into Gaussian distribution then we can easily analysis of the data and find insights of the data."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}