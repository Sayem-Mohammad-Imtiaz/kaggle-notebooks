{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nimport cv2\n\nfrom tensorflow.keras.applications import vgg16\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import layers, models, optimizers, callbacks\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.experimental import enable_hist_gradient_boosting  # noqa\nfrom sklearn.ensemble import HistGradientBoostingClassifier, VotingClassifier, RandomForestClassifier\nfrom sklearn.svm import SVC\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline, make_pipeline\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/identify-the-dance-form/","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_labels = pd.read_csv(\"../input/identify-the-dance-form/train.csv\")\ntrain_labels.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = cv2.imread('/kaggle/input/identify-the-dance-form/train/404.jpg')\nplt.imshow(img)\nplt.title(train_labels[train_labels.Image==\"404.jpg\"].target.values[0])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_data(df, path):\n    images = []\n    labels = []\n    for i in zip(df.values):\n        file = i[0][0]\n        label = i[0][1]\n        image = cv2.resize(cv2.imread(path+file), \n                           (224,224))\n        image = vgg16.preprocess_input(image)\n        images.append(image)\n        labels.append(label)\n    return np.array(images), np.array(labels)\n\nimage_path = \"/kaggle/input/identify-the-dance-form/train/\"\n\nX, y = load_data(train_labels, image_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25)\n\nencoder = OneHotEncoder(sparse=False)\ny_train = encoder.fit_transform(y_train.reshape(-1,1))\ny_val = encoder.transform(y_val.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Transfer Learning for Feature Extraction\n\nHere we will train/fine tune our NN model using transfer learning and cut the model to the second last layer and put a more complex model for the final prediction. And a ImageDataGenerator is used for data augmentation to avoid overfitting of the train set. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_gen = ImageDataGenerator(rotation_range=30,\n                              width_shift_range=0.25,\n                              height_shift_range=0.25,\n                              shear_range=0.2,\n                              zoom_range=0.3,\n                              horizontal_flip=True,\n                              fill_mode='nearest')\n\ndata_gen.fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_classes = len(np.unique(y))\n\nmodel = vgg16.VGG16(include_top=False, weights='imagenet', \n                    pooling='avg', input_shape = (224, 224, 3))\n\n\n\nfor layer in model.layers[:17]:\n    layer.trainable = False\n\n\n\nx = layers.Dense(1024, activation='relu')(model.output)\nx = layers.Dense(512, activation='relu')(x)\noutput = layers.Dense(num_classes, activation='softmax')(x)\n\nmodel = models.Model(model.input, output)\n\nmodel.compile(loss=\"categorical_crossentropy\",\n              optimizer=optimizers.RMSprop(lr = 0.001),\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_reduce = callbacks.ReduceLROnPlateau(monitor='val_loss', \n                                        patience=2, \n                                        verbose=1, \n                                        factor=0.5, \n                                        min_lr=0.00001)\n\nhist = model.fit(data_gen.flow(X_train, y_train, batch_size=64),\n                 steps_per_epoch=len(X_train)/64, epochs=50,\n                 validation_data=(X_val, y_val),\n                 callbacks = [lr_reduce])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,7))\nplt.subplot(121)\nplt.plot(hist.history[\"loss\"])\nplt.plot(hist.history[\"val_loss\"])\nplt.legend([\"train\",\"val\"])\n\nplt.subplot(122)\nplt.plot(hist.history[\"accuracy\"])\nplt.plot(hist.history[\"val_accuracy\"])\nplt.legend([\"train\",\"val\"])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Extraction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"extractor = models.Model(model.input, model.layers[-3].output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_new_train = extractor.predict(X_train)\nX_new_val = extractor.predict(X_val)\n\nscaler = StandardScaler()\nX_new_train = scaler.fit_transform(X_new_train)\nX_new_val = scaler.transform(X_new_val)\n\nprint(\"Train Shape:\", X_new_train.shape, \", Val Shape:\", X_new_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_new_train = encoder.inverse_transform(y_train).reshape(-1)\ny_new_val = encoder.inverse_transform(y_val).reshape(-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# New model with hyper parameter optimization\n\nIn the last part we use a model which will be optimized using a 3-CV. And SMOTE will be used for data augmentation!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate = np.linspace(1e-3, 1, num=500)\n\nmax_iter = list(range(50,301))\n\nl2_regularization = np.linspace(0, 5, num=500)\n\nmax_leaf_nodes = list(range(10, 51))\n\nmax_depth = list(range(3, 21))\n\nmin_samples_leaf = list(range(10,51))\n\n# Create the random grid\nrandom_grid = {'learning_rate': learning_rate,\n               'max_iter': max_iter,\n               'max_leaf_nodes': max_leaf_nodes,\n               'l2_regularization': l2_regularization,\n               'max_depth': max_depth,\n               'min_samples_leaf':min_samples_leaf}\n\nrandom_grid = {'histgradientboostingclassifier__' + key: random_grid[key] for key in random_grid}\n\nsmote_par = {\"mohiniyattam\":60, \"odissi\":60, \"bharatanatyam\":60,\n             \"kathakali\":60, \"sattriya\":60, \"kathak\":60,\n             \"kuchipudi\":60, \"manipuri\":60}\nclf = make_pipeline(SMOTE(smote_par),\n                    HistGradientBoostingClassifier(loss=\"categorical_crossentropy\"))\n\ngb_random = RandomizedSearchCV(clf, random_grid, n_iter=50, cv=3,\n                              random_state=1, n_jobs=-1, refit=True)\n    \ngb_random.fit(X_new_train, y_new_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(gb_random.best_score_)\ngb_random.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = gb_random.best_estimator_.predict(X_new_val)\n\nprint(classification_report(y_new_val,y_pred,\n      labels=np.unique(y_new_val)))\n\nsns.heatmap(confusion_matrix(y_new_val,y_pred), \n            xticklabels=np.unique(y_new_val),\n            yticklabels=np.unique(y_new_val),\n            annot=True, cbar=False, cmap=\"Blues\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_grid = {'C': [0.1, 1, 5, 10, 100], \n              'gamma': [1,0.1,0.01,0.001, 0.0005],\n              'degree': [2, 3, 4],\n              'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\n\nrandom_grid = {'svc__' + key: random_grid[key] for key in random_grid}\n\nsmote_par = {\"mohiniyattam\":60, \"odissi\":60, \"bharatanatyam\":60,\n             \"kathakali\":60, \"sattriya\":60, \"kathak\":60,\n             \"kuchipudi\":60, \"manipuri\":60}\nclf = make_pipeline(SMOTE(smote_par),\n                    SVC())\n\nsvm_random = RandomizedSearchCV(clf, random_grid, n_iter=50, cv=3,\n                              random_state=1, n_jobs=-1, refit=True)\n    \nsvm_random.fit(X_new_train, y_new_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(svm_random.best_score_)\nsvm_random.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = svm_random.best_estimator_.predict(X_new_val)\n\nprint(classification_report(y_new_val,y_pred,\n      labels=np.unique(y_new_val)))\n\nsns.heatmap(confusion_matrix(y_new_val,y_pred), \n            xticklabels=np.unique(y_new_val),\n            yticklabels=np.unique(y_new_val),\n            annot=True, cbar=False, cmap=\"Blues\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_estimators = [int(x) for x in np.linspace(start = 100, stop = 300, num = 11)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n# Maximum depth of the trees\nmax_depth = [int(x) for x in np.linspace(10, 50, num = 10)]\n\n# Minimum number of samples required for the split\nmin_samples_split = [2, 5, 10]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4, 6, 8, 10]\n\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\nrandom_grid = {'randomforestclassifier__' + key: random_grid[key] for key in random_grid}\n\nsmote_par = {\"mohiniyattam\":60, \"odissi\":60, \"bharatanatyam\":60,\n             \"kathakali\":60, \"sattriya\":60, \"kathak\":60,\n             \"kuchipudi\":60, \"manipuri\":60}\n\nclf = make_pipeline(SMOTE(smote_par),\n                    RandomForestClassifier())\n\nrf_random = RandomizedSearchCV(clf, random_grid, n_iter=50, cv=3,\n                              random_state=1, n_jobs=-1, refit=True)\n    \nrf_random.fit(X_new_train, y_new_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(rf_random.best_score_)\nrf_random.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = rf_random.best_estimator_.predict(X_new_val)\n\nprint(classification_report(y_new_val,y_pred,\n      labels=np.unique(y_new_val)))\n\nsns.heatmap(confusion_matrix(y_new_val,y_pred), \n            xticklabels=np.unique(y_new_val),\n            yticklabels=np.unique(y_new_val),\n            annot=True, cbar=False, cmap=\"Blues\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble of the models\nLet's create an hard voting ensemble from the three previous models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble = VotingClassifier([(\"hgb\",gb_random.best_estimator_[\"histgradientboostingclassifier\"]),\n                             (\"svm\",svm_random.best_estimator_[\"svc\"]),\n                             (\"rf\",rf_random.best_estimator_[\"randomforestclassifier\"])])\n\nclf = make_pipeline(SMOTE(smote_par),\n                    ensemble)\nclf.fit(X_new_train, y_new_train)\n\ny_pred = clf.predict(X_new_val)\n\nprint(classification_report(y_new_val,y_pred,\n      labels=np.unique(y_new_val)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(confusion_matrix(y_new_val,y_pred), \n            xticklabels=np.unique(y_new_val),\n            yticklabels=np.unique(y_new_val),\n            annot=True, cbar=False, cmap=\"Blues\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}