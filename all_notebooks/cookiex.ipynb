{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional,Activation\nfrom keras.optimizers import SGD\nimport math\nimport keras\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# for dirname, _, filenames in os.walk('/kaggle/output/kaggle/working'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\nimport time\n\npd.set_option('display.max_rows', 10000)\n\nd = {'hour': 3600, 'half_hour': 1800, '15_min': 900}\nchoose_resolution = 'hour'\nresolution = d[choose_resolution]\n\nstart = '1/01/2020'\nstart_ts = time.mktime(datetime.datetime.strptime(start, \"%d/%m/%Y\").timetuple())/resolution # 1800 to have half hours\nend =  '1/01/2021'\nend_ts = time.mktime(datetime.datetime.strptime(end, \"%d/%m/%Y\").timetuple())/resolution\n\nhours = list(np.arange(start_ts, end_ts))\nprint(len(hours))\n\ndf = pd.DataFrame({'time':hours, 'hour in year':list(np.arange(len(hours))), 'occupancy': [0]*len(hours), 'revenue (CHF)':[0]*len(hours), })\n\n\ncsv_path = r'/kaggle/input/cookiex/parkrail-sale-app.csv'\n\nparking = pd.read_csv(csv_path, usecols=['start', 'end'], sep= ';')\nprint(parking.head())\n\ndef find_hour(s, e):\n    s_str  = datetime.datetime.fromisoformat(s)\n    time_st = (time.mktime(s_str.timetuple()) / resolution) - start_ts\n\n    e_str= datetime.datetime.fromisoformat(e)\n    time_e = (time.mktime(e_str.timetuple()) / resolution) - start_ts\n\n    return(time_st, time_e)\n\n\nfor index, r in parking.iterrows():\n    s, e = find_hour(r['start'], r['end'])\n    try:\n        if int(e)- int(s) > 0:\n            for i in list(np.arange(int(s), int(e))):\n                # ind = df[df['hour in year'] == int(i)].index.values.astype(int)[0]\n\n                df.at[int(i), 'occupancy'] += 1\n\n        else:\n            # ind = df[df['hour in year'] == int(s)].index.values.astype(int)[0]\n            df.at[int(s), 'occupancy'] += 1\n    except  Exception:\n        pass\n\n\n\ndf['revenue (CHF)'] = df['occupancy']/4\ndf.to_csv('timeseries_'+choose_resolution+'.csv', sep=';', index = False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"thirty_minutes = pd.read_csv('/kaggle/input/timeseries/timeseries_30minutes.csv',sep= ';' )\nhour = pd.read_csv('/kaggle/input/timeseries/timeseries_hourly.csv',sep= ';')\nfifteen_minutes = pd.read_csv('/kaggle/input/timeseries/timeseries_15minutes.csv',sep= ';')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"thirty_minutes.head(100)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"thirty_minutes.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nhour['time_int'] = hour['time'].astype(int)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\n\ndatetime.datetime.fromtimestamp(438289).strftime('%Y-%m-%d %H:%M:%S')\n# hour['date_time'] = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(hour['time_int']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hour.info\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hour.head(100)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fifteen_minutes.head(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,5))\nplt.plot(hour.occupancy)\nplt.title('occupancy')\nplt.xlabel ('Time(Hour)')\nplt.ylabel ('occupancy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,5))\nplt.plot(fifteen_minutes.occupancy)\nplt.title('occupancy')\nplt.xlabel ('Time(15mins)')\nplt.ylabel ('occupancy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,5))\nplt.plot(thirty_minutes.occupancy)\nplt.title('occupancy')\nplt.xlabel ('Time(30mins)')\nplt.ylabel ('occupancy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nscaler = MinMaxScaler() \nclose_price =  hour['occupancy'].values.reshape(-1, 1) \nscaled_close = scaler.fit_transform(close_price)\nscaled_close = scaled_close[~np.isnan(scaled_close)] \nscaled_close = scaled_close.reshape(-1, 1) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEQ_LEN = 100 \ndef to_sequences(data, seq_len):\n    d = []\n    for index in range(len(data) - seq_len):\n        d.append(data[index: index + seq_len])\n        return np.array(d)\ndef preprocess(data_raw, seq_len, train_split):\n    data = to_sequences(data_raw, seq_len)\n    num_train = int(train_split * data.shape[0])\n    X_train = data[:num_train, :-1, :]\n    y_train = data[:num_train, -1, :]\n    X_test = data[num_train:, :-1, :]\n    y_test = data[num_train:, -1, :]\n    return X_train, y_train, X_test, y_test\n\"\"\"Walk forward validation: \nInitial SEQ_LEN is defined above, so, walk forward will be shifting one position to the right and create another sequence.\nThe process is repeated until all possible positions are used.\"\"\"\nX_train, y_train, X_test, y_test = preprocess(scaled_close, SEQ_LEN, train_split = 0.95) \n# 5% of the data saved for testing.print(X_train.shape, X_test.shape)\"\"\"Our model will use 1805 sequences representing 99 hours of Bitcoin price changes each for training. We shall be predicting the price for 96 hours in the future\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DROPOUT = 0.2 \n# 20% Dropout is used to control over-fitting during training\nWINDOW_SIZE = SEQ_LEN - 1\nmodel = keras.Sequential()# Input layer\nmodel.add(Bidirectional(LSTM(WINDOW_SIZE, return_sequences=True), input_shape=(WINDOW_SIZE, X_train.shape[-1])))\n\"\"\"Bidirectional RNNs allows to train on the sequence data in forward and backward direction.\"\"\"\nmodel.add(Dropout(rate=DROPOUT))# 1st Hidden layer\nmodel.add(Bidirectional(LSTM((WINDOW_SIZE * 2), return_sequences = True)))\nmodel.add(Dropout(rate=DROPOUT))# 2nd Hidden layer\nmodel.add(Bidirectional(LSTM(WINDOW_SIZE, return_sequences=False)))# output layer\nmodel.add(Dense(units=1))\nmodel.add(Activation('linear'))\n\"\"\"Output layer has a single neuron (predicted Bitcoin price). We use Linear activation function which activation is proportional to the input.\"\"\"\nBATCH_SIZE = 64\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nhistory = model.fit(X_train, y_train, epochs=50, batch_size=BATCH_SIZE, shuffle=False, validation_split=0.1)  #try 50\n# shuffle not advisable during training of Time Series","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history.history.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # history for loss\n# plt.figure(figsize = (10,5))\n# plt.plot(history.history['loss'])\n# plt.plot(history.history['val_loss'])\n# plt.title('model loss')\n# # plt.ylabel('loss')\n# plt.xlabel('epoch')\n# plt.legend(['train', 'test'], loc='upper left')\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # prediction on test data\n# y_pred = model.predict(X_test) \n# # invert the test to original values\n# y_test_inverse = pd.DataFrame(scaler.inverse_transform(y_test)) \n# # assigning datetime\n# y_test_inverse.index = hour.index[-len(y_test):] \n# print('Test data:',)\n# print(y_test_inverse.tail(3)); print();# invert the prediction to understandable values\n# y_pred_inverse = pd.DataFrame(scaler.inverse_transform(y_pred)) \n# # assigning datetime\n# y_pred_inverse.index = y_test_inverse.index \n# print('Prediction data:',)\n# print(y_pred_inverse.tail(3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(f'MAE {mean_absolute_error(y_test, y_pred)}')\n# print(f'MSE {mean_squared_error(y_test, y_pred)}')\n# print(f'RMSE {np.sqrt(mean_squared_error(y_test, y_pred))}')\n# # print(f'R2 {r2_score(y_test, y_pred)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.figure(figsize = (15,5))\n# plt.plot(y_test_inverse)\n# plt.plot(y_pred_inverse)\n# plt.title('Actual vs Prediction plot (Price prediction model)')\n# plt.ylabel('price')\n# plt.xlabel('date')\n# plt.legend(['actual', 'prediction'], loc='upper left')\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"close_data = hour['occupancy'].values\nclose_data = close_data.reshape((-1,1))\n\nsplit_percent = 0.70\nsplit = int(split_percent*len(close_data))\n\nclose_train = close_data[:split]\nclose_test = close_data[split:]\n\ndate_train = hour['time'][:split]\ndate_test = hour['time'][split:]\n\nprint(len(close_train))\nprint(len(close_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fifteen_minutes.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"close_data_30 = fifteen_minutes['occupancy'].values\nclose_data_30 = close_data_30.reshape((-1,1))\n\nsplit_percent = 0.70\nsplit = int(split_percent*len(close_data))\n\nclose_train_30 = close_data_30[:split]\nclose_test_30 = close_data_30[split:]\n\ndate_train_30 = fifteen_minutes['time'][:split]\ndate_test_30 = fifteen_minutes['time'][split:]\n\nprint(len(close_train_30))\nprint(len(close_test_30))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nimport tensorflow as tf\nfrom keras.preprocessing.sequence import TimeseriesGenerator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"look_back = 15\n\ntrain_generator = TimeseriesGenerator(close_train, close_train, length=look_back, batch_size=20)     \ntest_generator = TimeseriesGenerator(close_test, close_test, length=look_back, batch_size=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"look_back = 15\n\ntrain_generator_30 = TimeseriesGenerator(close_train_30, close_train_30, length=look_back, batch_size=20)     \ntest_generator_30 = TimeseriesGenerator(close_test_30, close_test_30, length=look_back, batch_size=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import LSTM, Dense\n\nmodel = Sequential()\nmodel.add(\n    LSTM(10,\n        activation='relu',\n        input_shape=(look_back,1))\n)\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\n\nnum_epochs = 5\nmodel.fit_generator(train_generator_30, epochs=num_epochs, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import LSTM, Dense\n\nmodel = Sequential()\nmodel.add(\n    LSTM(10,\n        activation='relu',\n        input_shape=(look_back,1))\n)\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\n\nnum_epochs = 5\nmodel.fit_generator(train_generator, epochs=num_epochs, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from plotly.offline import init_notebook_mode, iplot, plot\nimport plotly as py\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = model.predict_generator(test_generator_30)\n\nclose_train = close_train_30.reshape((-1))\nclose_test = close_test_30.reshape((-1))\nprediction = prediction.reshape((-1))\n\ntrace1 = go.Scatter(\n    x = date_train_30,\n    y = close_train,\n    mode = 'lines',\n    name = 'Data'\n)\ntrace2 = go.Scatter(\n    x = date_test_30,\n    y = prediction,\n    mode = 'lines',\n    name = 'Prediction'\n)\ntrace3 = go.Scatter(\n    x = date_test_30,\n    y = close_test,\n    mode='lines',\n    name = 'Ground Truth'\n)\nlayout = go.Layout(\n    title = \"Predict Occupancy for 15 mins \",\n    xaxis = {'title' : \"Time\"},\n    yaxis = {'title' : \"Occupancy\"}\n)\nfig = go.Figure(data=[trace1, trace2,trace3], layout=layout)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = model.predict_generator(test_generator)\n\nclose_train = close_train.reshape((-1))\nclose_test = close_test.reshape((-1))\nprediction = prediction.reshape((-1))\n\ntrace1 = go.Scatter(\n    x = date_train,\n    y = close_train,\n    mode = 'lines',\n    name = 'Data'\n)\ntrace2 = go.Scatter(\n    x = date_test,\n    y = prediction,\n    mode = 'lines',\n    name = 'Prediction'\n)\ntrace3 = go.Scatter(\n    x = date_test,\n    y = close_test,\n    mode='lines',\n    name = 'Ground Truth'\n)\nlayout = go.Layout(\n    title = \"Predict Occupancy for hour \",\n    xaxis = {'title' : \"Time\"},\n    yaxis = {'title' : \"Occupancy\"}\n)\nfig = go.Figure(data=[trace1, trace2,trace3], layout=layout)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"close_data = close_data.reshape((-1))\n\ndef predict(num_prediction, model):\n    prediction_list = close_data[-look_back:]\n    \n    for _ in range(num_prediction):\n        x = prediction_list[-look_back:]\n        x = x.reshape((1, look_back, 1))\n        out = model.predict(x)[0][0]\n        prediction_list = np.append(prediction_list, out)\n    prediction_list = prediction_list[look_back-1:]\n        \n    return prediction_list\n    \ndef predict_dates(num_prediction):\n    last_date = hour['time'].values[-1]\n    prediction_dates = pd.date_range(last_date, periods=num_prediction+1).tolist()\n    return prediction_dates\n\nnum_prediction = 30\nforecast = predict(num_prediction, model)\nforecast_dates = predict_dates(num_prediction)\n\ntrace1 = go.Scatter(\n    x = hour['time'].tolist(),\n    y = close_data,\n    mode = 'lines',\n    name = 'Data'\n)\ntrace2 = go.Scatter(\n    x = forecast_dates,\n    y = forecast,\n    mode = 'lines',\n    name = 'Prediction'\n)\nlayout = go.Layout(\n    title = \"Predict Occupancy\",\n    xaxis = {'title' : \"Date\"},\n    yaxis = {'title' : \"Occupancy\"}\n)\n\nfig = go.Figure(data=[trace1, trace2], layout=layout)\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Recurrent Neural Networks\nIn a recurrent neural network we store the output activations from one or more of the layers of the network. Often these are hidden later activations. Then, the next time we feed an input example to the network, we include the previously-stored outputs as additional inputs. You can think of the additional inputs as being concatenated to the end of the “normal” inputs to the previous layer. For example, if a hidden layer had 10 regular input nodes and 128 hidden nodes in the layer, then it would actually have 138 total inputs (assuming you are feeding the layer’s outputs into itself à la Elman) rather than into another layer). Of course, the very first time you try to compute the output of the network you’ll need to fill in those extra 128 inputs with 0s or something.\n\nSource: [Quora](https://www.quora.com/What-is-a-simple-explanation-of-a-recurrent-neural-network)\n<img src=\"https://cdn-images-1.medium.com/max/1600/1*NKhwsOYNUT5xU7Pyf6Znhg.png\">\n\nSource: [Medium](https://medium.com/ai-journal/lstm-gru-recurrent-neural-networks-81fe2bcdf1f9)\n\nLet me give you the best explanation of Recurrent Neural Networks that I found on internet: https://www.youtube.com/watch?v=UNmqTiOnRfg&t=3s"},{"metadata":{},"cell_type":"markdown","source":"Now, even though RNNs are quite powerful, they suffer from  **Vanishing gradient problem ** which hinders them from using long term information, like they are good for storing memory 3-4 instances of past iterations but larger number of instances don't provide good results so we don't just use regular RNNs. Instead, we use a better variation of RNNs: **Long Short Term Networks(LSTM).**\n\n### What is Vanishing Gradient problem?\nVanishing gradient problem is a difficulty found in training artificial neural networks with gradient-based learning methods and backpropagation. In such methods, each of the neural network's weights receives an update proportional to the partial derivative of the error function with respect to the current weight in each iteration of training. The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value. In the worst case, this may completely stop the neural network from further training. As one example of the problem cause, traditional activation functions such as the hyperbolic tangent function have gradients in the range (0, 1), and backpropagation computes gradients by the chain rule. This has the effect of multiplying n of these small numbers to compute gradients of the \"front\" layers in an n-layer network, meaning that the gradient (error signal) decreases exponentially with n while the front layers train very slowly.\n\n\n\n<img src=\"https://cdn-images-1.medium.com/max/1460/1*FWy4STsp8k0M5Yd8LifG_Q.png\">\n\nSource: [Medium](https://medium.com/@anishsingh20/the-vanishing-gradient-problem-48ae7f501257)"},{"metadata":{},"cell_type":"markdown","source":"## Long Short Term Memory(LSTM)\nLong short-term memory (LSTM) units (or blocks) are a building unit for layers of a recurrent neural network (RNN). A RNN composed of LSTM units is often called an LSTM network. A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell is responsible for \"remembering\" values over arbitrary time intervals; hence the word \"memory\" in LSTM. Each of the three gates can be thought of as a \"conventional\" artificial neuron, as in a multi-layer (or feedforward) neural network: that is, they compute an activation (using an activation function) of a weighted sum. Intuitively, they can be thought as regulators of the flow of values that goes through the connections of the LSTM; hence the denotation \"gate\". There are connections between these gates and the cell.\n\nThe expression long short-term refers to the fact that LSTM is a model for the short-term memory which can last for a long period of time. An LSTM is well-suited to classify, process and predict time series given time lags of unknown size and duration between important events. LSTMs were developed to deal with the exploding and vanishing gradient problem when training traditional RNNs.\n\n\n\n<img src=\"https://cdn-images-1.medium.com/max/1600/0*LyfY3Mow9eCYlj7o.\">\n\nSource: [Medium](https://codeburst.io/generating-text-using-an-lstm-network-no-libraries-2dff88a3968)\n\nGood explanation: https://medium.com/deep-math-machine-learning-ai/chapter-10-1-deepnlp-lstm-long-short-term-memory-networks-with-math-21477f8e4235\n"},{"metadata":{},"cell_type":"markdown","source":"## Components of LSTMs\nSo the LSTM cell contains the following components\n* Forget Gate “f” ( a neural network with sigmoid)\n* Candidate layer “C\"(a NN with Tanh)\n* Input Gate “I” ( a NN with sigmoid )\n* Output Gate “O”( a NN with sigmoid)\n* Hidden state “H” ( a vector )\n* Memory state “C” ( a vector)\n\n* Inputs to the LSTM cell at any step are X<sub>t</sub> (current input) , H<sub>t-1</sub> (previous hidden state ) and C<sub>t-1</sub> (previous memory state).  \n* Outputs from the LSTM cell are H<sub>t</sub> (current hidden state ) and C<sub>t</sub> (current memory state)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}