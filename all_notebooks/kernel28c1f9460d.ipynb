{"cells":[{"metadata":{},"cell_type":"markdown","source":"*1. INTRODUCTION*\n\nWe will be working on the vehicle dataset from cardekho dataset. This dataset contains information about used cars listed on www.cardekho.com.  \n\nWe will predict the selling price of a used car based on various factors such as  \n\n        Kms_Driven     -> kilometers driven \n\n        Year                   -> year of purchase  \n\n        Present_Price  -> present price of a new car \n\n        Fuel_Type         -> type of fuel being used (petrol, diesel, CNG) \n\n        Transmission    -> automatic or manual gear transmission  \n\n        Owner               -> number of previous owners \n\n        Seller_Type       -> dealer or individual  "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\n\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n \n%matplotlib inline \n%config InlineBackend.figure_format = 'retina' \nimport warnings  \nwarnings.filterwarnings('ignore') \nimport os \nprint(os.listdir(\"../input\")) ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"cars=pd.read_csv(\"../input/car data.csv\")\ncars.sample()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us have a look at the number of data entries we have pertining to each vehicle."},{"metadata":{"trusted":true},"cell_type":"code","source":"cars['Car_Name'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*2. Data handling *\n\nCheck for null values. Since our data has none, we can skip this step. "},{"metadata":{"trusted":true},"cell_type":"code","source":"cars.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Convert categorical variables (Fuel_Type,Transmission,Seller_Type) into or labels into numeric form so as to convert it into the machine-readable form. \n\nMachine learning algorithms can then decide in a better way on how those labels must be operated. \n\nWe use label encoder to transform values of Fuel type, Transmission and Seller type to 0,1 and 2. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlb = LabelEncoder()\ncars[\"trans_code\"] = lb.fit_transform(cars[\"Transmission\"])\ncars[\"Transmission\"].replace(cars[\"trans_code\"], inplace=True)\ncars[\"seller_code\"] = lb.fit_transform(cars[\"Seller_Type\"])\ncars[\"Seller_Type\"].replace(cars[\"seller_code\"], inplace=True)\ncars[\"fuel_code\"] = lb.fit_transform(cars[\"Fuel_Type\"])\ncars[\"Fuel_Type\"].replace(cars[\"fuel_code\"], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We combine the two variables Year and Kms-Driven to a new feature Rating which is a measure of number of kilometers driven each day.\n\nRating is the ratio of number of kilometers driven in total to the approximate number of days since the vehicle has been purchased."},{"metadata":{"trusted":true},"cell_type":"code","source":"year=cars[\"Year\"]\nmile=cars[\"Kms_Driven\"]\nrate=mile/((2019-year)*365)\ncars[\"Rating\"]=rate","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*3. Understanding the data and Data Visualization *\n\nPlot relationship with numerical variables. \n\nFollowing variables can play an important role in this problem: \n\n    1. Kms_Driven \n    2. Year\n    3. Present_Price"},{"metadata":{"trusted":true},"cell_type":"code","source":"var = 'Kms_Driven'\ndata = pd.concat([cars[\"Selling_Price\"], cars[var]], axis=1)\ndata.plot.scatter(x=var, y='Selling_Price');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    Kms_Driven \n\nAs expected, this graph shows a negative relation between kilometers driven and the selling price. \nThe more the kilometers driven, the lesser the selling price because this means more wear and tear of various parts of the car which reduces its value. "},{"metadata":{"trusted":true},"cell_type":"code","source":"var = 'Year'\ndata = pd.concat([cars[\"Selling_Price\"], cars[var]], axis=1)\ndata.plot.scatter(x=var, y='Selling_Price',);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    Year \n\nAs mentioned earlier, older cars tend to lower longevity and hence sell at a lower price compared to newer cars maintained in good condition. "},{"metadata":{"trusted":true},"cell_type":"code","source":"var = 'Present_Price'\ndata = pd.concat([cars[\"Selling_Price\"], cars[var]], axis=1)\ndata.plot.scatter(x=var, y='Selling_Price',);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    Present_Price \n\nWe can observe a linear behavior. There are no outliers above the line because the selling price of a used vehicle is always lower than the present value of a new one. "},{"metadata":{},"cell_type":"markdown","source":"*4. Training and testing a model *\n\nWe split the data into training and testing set. \nWe first fit each model on the training set. \nThen we predict the dependent variable in the test set. \nNow we compare the obtained results with the actual data at hand and get a score for the performance of each algorithm. "},{"metadata":{},"cell_type":"markdown","source":"    Linear regressor  \n\nBuild a linear regression model by ordinary least squares and test it for given data set. \nWe check the significance of different features by comparing their p value to see if it is below the confidence level of 0.05. \n\nFit the model for different combinations of independent variables to see which yields the best results.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport statsmodels.api as sm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=cars[[\"Present_Price\",\"fuel_code\",\"seller_code\",\"trans_code\",\"Year\",\"Kms_Driven\",\"Owner\"]]\ny=cars[\"Selling_Price\"]\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\nX = sm.add_constant(X)\nmodel = sm.OLS(y_train.astype(float), X_train.astype(float)).fit()\npredictions = model.predict(X_test)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=cars[[\"Present_Price\",\"fuel_code\",\"seller_code\",\"trans_code\",\"Rating\",\"Owner\"]]\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\nX = sm.add_constant(X)\nmodel = sm.OLS(y_train.astype(float), X_train.astype(float)).fit()\npredictions = model.predict(X_test)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that multicollinearity existed between two of our variables Year and Kms driven"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=cars[[\"Present_Price\",\"fuel_code\",\"seller_code\",\"trans_code\",\"Year\",\"Kms_Driven\",\"Owner\"]]\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\nX = sm.add_constant(X)\nmodel1 = sm.OLS(y_train.astype(float), X_train.astype(float)).fit()\nr1=model1.rsquared\nprint('R squared value=', +r1)\n\npredictions = model1.predict(X_test)\npredictions=pd.DataFrame(predictions)\npredictions=predictions.reset_index()\ntest_index=y_test.reset_index()[\"Selling_Price\"]\nax=test_index.plot(label=\"originals\",figsize=(12,6),linewidth=2,color=\"r\")\nax=predictions[0].plot(label = \"predictions\",figsize=(12,6),linewidth=2,color=\"g\")\nplt.legend(loc='upper right')\nplt.title(\"Linear Regressor\")\nplt.xlabel(\"index\")\nplt.ylabel(\"values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we begin to remove each variable one after the other to see which values give the best result. \nIf we replace Year and Kms_Driven variables with Rating feature that we created the following results are obtained"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=cars[[\"Present_Price\",\"fuel_code\",\"trans_code\",\"seller_code\",\"Rating\",\"Owner\"]]\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\nX = sm.add_constant(X)\nmodel2 = sm.OLS(y_train.astype(float), X_train.astype(float)).fit()\nr2=model2.rsquared\nprint('R squared value=', +r2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing Present price we see the performance of the model goes down drastically. :"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=cars[[\"fuel_code\",\"seller_code\",\"trans_code\",\"Year\",\"Kms_Driven\",\"Owner\"]]\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\nX = sm.add_constant(X)\nmodel3 = sm.OLS(y_train.astype(float), X_train.astype(float)).fit()\nr3=model3.rsquared\nprint('R squared value=', +r3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similarly, by removing categorical values we observe that we get a slightly higher value of Rsquared when we neglect seller type feature. "},{"metadata":{"trusted":true},"cell_type":"code","source":"X=cars[[\"Present_Price\",\"fuel_code\",\"trans_code\",\"Year\",\"Kms_Driven\",\"Owner\"]]\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\nX = sm.add_constant(X)\nmodel4 = sm.OLS(y_train.astype(float), X_train.astype(float)).fit()\nr4=model4.rsquared\nprint('R squared value=', +r4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing Fuel type:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=cars[[\"Present_Price\",\"seller_code\",\"trans_code\",\"Year\",\"Kms_Driven\",\"Owner\"]]\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\nX = sm.add_constant(X)\nmodel5 = sm.OLS(y_train.astype(float), X_train.astype(float)).fit()\nr5=model5.rsquared\nprint('R squared value=', +r5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing Transmission:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=cars[[\"Present_Price\",\"fuel_code\",\"seller_code\",\"Year\",\"Kms_Driven\",\"Owner\"]]\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\nX = sm.add_constant(X)\nmodel6 = sm.OLS(y_train.astype(float), X_train.astype(float)).fit()\nr6=model6.rsquared\nprint('R squared value=', +r6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Plot results of linear regression by considering different features\n\ndata=[r1,r2,r3,r4,r5,r6]\n\nplt.subplots(figsize = (15,8))\nax = plt.plot(data)\nplt.axis([0, 5, 0.6, 1])\nplt.ylabel('Rsquared value',size=25)\nplt.xlabel('Trial',size=25)\nlabels = (['default', 'w rating', 'w/o present \\n price','w/o seller','w/o fuel','w/o \\n transmission'])\nval = [0,1,2,3,4,5]  \nplt.xticks(val, labels);\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    AdaBoost regressor  \n\nAdaptive boosting is a boosting technique which helps you combine multiple “weak classifiers” into a single “strong classifier”. \n\nEach new classifier/predictor is given a training set where the difficult examples are increasingly represented, this is achieved either through weighting or resampling. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.datasets import make_regression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regr = AdaBoostRegressor()\nX=cars[[\"Present_Price\",\"fuel_code\",\"trans_code\",\"Year\",\"Kms_Driven\",\"Owner\"]]\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\nmodel=regr.fit(X_train, y_train) \nx3=regr.score(X,y)\nprint('R squared value=', +x3)\npredictions = model.predict(X_test)\npredictions=pd.DataFrame(predictions)\n\npredictions=predictions.reset_index()\ntest_index=y_test.reset_index()[\"Selling_Price\"]\nax=test_index.plot(label=\"originals\",figsize=(12,6),linewidth=2,color=\"r\")\nax=predictions[0].plot(label = \"predictions\",figsize=(12,6),linewidth=2,color=\"g\")\nplt.legend(loc='upper right')\nplt.title(\"ADABOOST Regressor\")\nplt.xlabel(\"index\")\nplt.ylabel(\"values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    Decision tree regressor \n\nDecision tree regressor is a model of decisions and all of their possible results, including outcomes, input costs and utility. \n\nDecision tree algorithm falls under the category of supervised learning algorithms. It works for both continuous as well as categorical output variables. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\ndtr = DecisionTreeRegressor()\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=cars[[\"Present_Price\",\"fuel_code\",\"trans_code\",\"Year\",\"Kms_Driven\",\"Owner\"]]\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.1)\ndtr.fit(X_train,y_train)\npredicts=dtr.predict(X_test)\nprediction=pd.DataFrame(predicts)\nR_2=r2_score(y_test,prediction)\n\n\n    \n    # Printing results  \nprint(dtr,\"\\n\") \nprint(\"R squared value=\",R_2,\"\\n\")\n\n    \n    # Plot for prediction vs originals\ntest_index=y_test.reset_index()[\"Selling_Price\"]\nax=test_index.plot(label=\"originals\",figsize=(12,6),linewidth=2,color=\"r\")\nax=prediction[0].plot(label = \"predictions\",figsize=(12,6),linewidth=2,color=\"g\")\nplt.legend(loc='upper right')\nplt.title(\"Decision Tree Regressor\")\nplt.xlabel(\"index\")\nplt.ylabel(\"values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    Decision tree regression with Adaboost "},{"metadata":{"trusted":true},"cell_type":"code","source":"regr_2 = AdaBoostRegressor(DecisionTreeRegressor())\n\nmodel=regr_2.fit(X_train, y_train)\n\ny_2 = regr_2.predict(X_test)\n\nx4=regr_2.score(X_test,y_test)\nprint('R squared value=', +x4)\n\npredictions=pd.DataFrame(y_2)\npredictions=predictions.reset_index()\ntest_index=y_test.reset_index()[\"Selling_Price\"]\nax=test_index.plot(label=\"originals\",figsize=(12,6),linewidth=2,color=\"r\")\nax=predictions[0].plot(label = \"predictions\",figsize=(12,6),linewidth=2,color=\"g\")\nplt.legend(loc='upper right')\nplt.title(\"Decision tree with ADABOOST Regressor\")\nplt.xlabel(\"index\")\nplt.ylabel(\"values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5. Conclusions \n\nEven though we obtained a fairly high Rsqured value for the linear regression algorithm, our results were improved by adabost technique. \n\nWe tested the model for different combinations of independent variables to figure out which yields the best results. \n\nThe decision tree model turned out to be the best suited algorithm to predict the selling price of used cars. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from tabulate import tabulate\nprint(tabulate([['Linear regression', r1], ['Adaboost linear regression', x3],['Decision tree regressor',R_2],['Decision tree with adaboost',x4]], headers=['Model used', 'Rsquared value']))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}