{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"da313597-6dd7-4fab-8f33-c207762a11f8","_cell_guid":"37eec355-16e3-4510-96a0-91d31d337bfe","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-06T18:40:08.386893Z","iopub.execute_input":"2021-06-06T18:40:08.387355Z","iopub.status.idle":"2021-06-06T18:40:08.406246Z","shell.execute_reply.started":"2021-06-06T18:40:08.3873Z","shell.execute_reply":"2021-06-06T18:40:08.404944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = np.load('/kaggle/input/ptbxl-atrial-fibrillation-detection/ecgeq-500hzsrfava.npy')\nY = pd.read_csv('/kaggle/input/ptbxl-atrial-fibrillation-detection/coorteeqsrafva.csv',sep=';')","metadata":{"execution":{"iopub.status.busy":"2021-06-06T18:40:13.138405Z","iopub.execute_input":"2021-06-06T18:40:13.138791Z","iopub.status.idle":"2021-06-06T18:40:33.296253Z","shell.execute_reply.started":"2021-06-06T18:40:13.138761Z","shell.execute_reply":"2021-06-06T18:40:33.295277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sr = X[Y[Y['ritmi']=='SR'].index]\nfa = X[Y[Y['ritmi']=='AF'].index]","metadata":{"execution":{"iopub.status.busy":"2021-06-06T18:40:33.297999Z","iopub.execute_input":"2021-06-06T18:40:33.298386Z","iopub.status.idle":"2021-06-06T18:40:33.971027Z","shell.execute_reply.started":"2021-06-06T18:40:33.298343Z","shell.execute_reply":"2021-06-06T18:40:33.96978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fa.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-06T18:40:33.973566Z","iopub.execute_input":"2021-06-06T18:40:33.973899Z","iopub.status.idle":"2021-06-06T18:40:33.982739Z","shell.execute_reply.started":"2021-06-06T18:40:33.97387Z","shell.execute_reply":"2021-06-06T18:40:33.981437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xeq = np.vstack([sr,fa])","metadata":{"execution":{"iopub.status.busy":"2021-06-06T18:40:33.984792Z","iopub.execute_input":"2021-06-06T18:40:33.985475Z","iopub.status.idle":"2021-06-06T18:40:34.685198Z","shell.execute_reply.started":"2021-06-06T18:40:33.985417Z","shell.execute_reply":"2021-06-06T18:40:34.68411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xeq.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-06T18:40:34.686829Z","iopub.execute_input":"2021-06-06T18:40:34.687322Z","iopub.status.idle":"2021-06-06T18:40:34.693622Z","shell.execute_reply.started":"2021-06-06T18:40:34.687253Z","shell.execute_reply":"2021-06-06T18:40:34.692395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = np.hstack((np.zeros(sr.shape[0]),np.ones(fa.shape[0])))","metadata":{"execution":{"iopub.status.busy":"2021-06-06T18:40:34.695222Z","iopub.execute_input":"2021-06-06T18:40:34.69598Z","iopub.status.idle":"2021-06-06T18:40:34.705115Z","shell.execute_reply.started":"2021-06-06T18:40:34.695933Z","shell.execute_reply":"2021-06-06T18:40:34.703768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(labels)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T18:40:34.706951Z","iopub.execute_input":"2021-06-06T18:40:34.707663Z","iopub.status.idle":"2021-06-06T18:40:34.717665Z","shell.execute_reply.started":"2021-06-06T18:40:34.707618Z","shell.execute_reply":"2021-06-06T18:40:34.716319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import layers, models\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import utils\nfrom collections import Counter\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom tensorflow.keras.layers import AveragePooling1D, Input","metadata":{"execution":{"iopub.status.busy":"2021-06-06T18:40:34.721038Z","iopub.execute_input":"2021-06-06T18:40:34.721587Z","iopub.status.idle":"2021-06-06T18:40:41.738685Z","shell.execute_reply.started":"2021-06-06T18:40:34.721543Z","shell.execute_reply":"2021-06-06T18:40:41.737566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.signal import butter, filtfilt\nimport numpy as np\n\ndef butter_bandpass(lcutoff,hcutoff, fs, order=5):\n    nyq = 0.5 * fs\n    lcoff = lcutoff / nyq\n    hcoff = hcutoff / nyq\n    b, a = butter(order, [lcoff,hcoff], btype='bandpass', analog=False)\n    return b, a\n\ndef butter_bandpass_filter(data, lcutoff,hcutoff, fs, order=5):\n    b, a = butter_bandpass(lcutoff,hcutoff, fs, order=order)\n    y = filtfilt(b, a, data)\n    return y","metadata":{"execution":{"iopub.status.busy":"2021-06-06T18:40:41.742584Z","iopub.execute_input":"2021-06-06T18:40:41.74299Z","iopub.status.idle":"2021-06-06T18:40:41.796121Z","shell.execute_reply.started":"2021-06-06T18:40:41.742959Z","shell.execute_reply":"2021-06-06T18:40:41.795055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args = (10,200,5000,3)\nfXeq = np.apply_along_axis(butter_bandpass_filter,1,Xeq,*args)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T18:40:41.799242Z","iopub.execute_input":"2021-06-06T18:40:41.799673Z","iopub.status.idle":"2021-06-06T18:41:19.606881Z","shell.execute_reply.started":"2021-06-06T18:40:41.799642Z","shell.execute_reply":"2021-06-06T18:41:19.60555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_val, y_train, y_val = train_test_split(fXeq[:,:,[1,6]],labels,test_size=.2,shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T18:41:19.608515Z","iopub.execute_input":"2021-06-06T18:41:19.609185Z","iopub.status.idle":"2021-06-06T18:41:19.886399Z","shell.execute_reply.started":"2021-06-06T18:41:19.609131Z","shell.execute_reply":"2021-06-06T18:41:19.884362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train.astype('float32')))\ntrain_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n\nval_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val.astype('float32')))\nval_dataset = val_dataset.batch(batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T18:41:19.888378Z","iopub.execute_input":"2021-06-06T18:41:19.889102Z","iopub.status.idle":"2021-06-06T18:41:22.577637Z","shell.execute_reply.started":"2021-06-06T18:41:19.889041Z","shell.execute_reply":"2021-06-06T18:41:22.576574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xeq = []\nsr = []\nfa = []\nX = []","metadata":{"execution":{"iopub.status.busy":"2021-06-06T18:41:22.580742Z","iopub.execute_input":"2021-06-06T18:41:22.581182Z","iopub.status.idle":"2021-06-06T18:41:22.606313Z","shell.execute_reply.started":"2021-06-06T18:41:22.581137Z","shell.execute_reply":"2021-06-06T18:41:22.604985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@tf.function\ndef resnet1d(data,nfilters,res):\n    x = tf.expand_dims(data,-1)\n    \n    x = layers.Conv1D(256,100,strides=5,activation='relu',padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    \n    x = layers.Conv1D(256,50,strides=5,activation='relu',padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    \n    x = layers.Conv1D(nfilters,res,strides=1,activation='relu',padding='same')(x)\n    x = layers.Conv1D(nfilters,res,strides=1,activation='relu',padding='same')(x)\n    b1o = layers.BatchNormalization()(x)\n    \n    x = layers.Conv1D(nfilters,res,strides=1,activation='relu',padding='same')(b1o)\n    x = layers.Conv1D(nfilters,res,strides=1,activation='relu',padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    b2o = layers.add([x,b1o])\n\n    x = layers.Conv1D(nfilters,res,strides=1,activation='relu',padding='same')(b2o)\n    x = layers.Conv1D(nfilters,res,strides=1,activation='relu',padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    b3o = layers.add([x,b2o])\n    \n    x = layers.Conv1D(nfilters,res,strides=1,activation='relu',padding='same')(b3o)\n    x = layers.Conv1D(nfilters,res,strides=1,activation='relu',padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    b4o = layers.add([x,b3o])\n    \n    x = layers.Conv1D(nfilters,res,strides=1,activation='relu',padding='same')(b4o)\n    x = layers.Conv1D(nfilters,res,strides=1,activation='relu',padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    b5o = layers.add([x,b4o])\n    \n    x = layers.Conv1D(nfilters,res,strides=1,activation='relu',padding='same')(b5o)\n    x = layers.Conv1D(nfilters,res,strides=1,activation='relu',padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    b6o = layers.add([x,b5o])\n    \n    x = layers.Conv1D(nfilters,res,strides=1,activation='relu',padding='same')(b6o)\n    x = layers.Conv1D(nfilters,res,strides=1,activation='relu',padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    b7o = layers.add([x,b6o])\n    \n    x = layers.Conv1D(nfilters,res,strides=1,activation='relu',padding='same')(b7o)\n    x = layers.Conv1D(nfilters,res,strides=1,activation='relu',padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    b8o = layers.add([x,b7o])\n    \n    x = layers.Conv1D(nfilters,res,strides=1,activation='relu',padding='same')(b8o)\n    x = layers.Conv1D(nfilters,res,strides=1,activation='relu',padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    b9o = layers.add([x,b8o])\n    \n    x = layers.Conv1D(nfilters,res,strides=1,activation='relu',padding='same')(b9o)\n    x = layers.Conv1D(nfilters,res,strides=1,activation='relu',padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    b10o = layers.add([x,b9o])\n    \n    x = layers.Conv1D(nfilters,res,strides=1,activation='relu',padding='same')(b10o)\n    x = layers.Conv1D(nfilters,res,strides=1,activation='relu',padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    b11o = layers.add([x,b10o])\n    \n    x = layers.Conv1D(nfilters,res,strides=1,activation='relu',padding='same')(b11o)\n    x = layers.Conv1D(nfilters,res,strides=1,activation='relu',padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    b12o = layers.add([x,b11o])\n    \n    x = layers.Conv1D(nfilters,res,strides=1,activation='relu',padding='same')(b12o)\n    x = layers.Conv1D(nfilters,res,strides=1,activation='relu',padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    b13o = layers.add([x,b12o])\n    \n    x = layers.Conv1D(nfilters,res,strides=1,activation='relu',padding='same')(b13o)\n    x = layers.Conv1D(nfilters,res,strides=1,activation='relu',padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    b14o = layers.add([x,b13o])\n    \n    x = layers.Conv1D(nfilters,res,strides=1,activation='relu',padding='same')(b14o)\n    x = layers.Conv1D(nfilters,res,strides=1,activation='relu',padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    b15o = layers.add([x,b14o])\n    \n    x = layers.Conv1D(nfilters,res,strides=1,activation='relu',padding='same')(b15o)\n    x = layers.Conv1D(nfilters,res,strides=1,activation='relu',padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    b16o = layers.add([x,b15o])\n    \n\n    x = layers.Conv1D(nfilters*2,res,strides=1,activation='relu',padding='same')(b16o)\n\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dense(64)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(.5)(x)\n    x = layers.Dense(32)(x)\n    x = layers.BatchNormalization()(x)\n\n    return(x)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T19:03:41.376699Z","iopub.execute_input":"2021-06-06T19:03:41.37706Z","iopub.status.idle":"2021-06-06T19:03:41.413717Z","shell.execute_reply.started":"2021-06-06T19:03:41.37703Z","shell.execute_reply":"2021-06-06T19:03:41.412215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@tf.function\ndef logL1(ytrue,ypred):\n    hazard_ratio = tf.exp(ypred)\n    log_risk = tf.math.log(tf.reduce_sum(hazard_ratio))\n    uncensored_likelihood = ypred - log_risk\n    censored_likelihood = uncensored_likelihood * ytrue\n    num_observed_events = tf.reduce_sum(ytrue)\n    neg_likelihood = - tf.reduce_sum(censored_likelihood) / num_observed_events\n    return neg_likelihood\n\ndef logL2(fail_indicator,logits):\n    logL = 0\n    #logits = 1e-5 * tf.reduce_sum(tf.square(logits))\n    hazard_ratio = tf.math.exp(logits)\n    cumsum_hazard_ratio = tf.cumsum(hazard_ratio,0)\n    log_risk = tf.math.log(cumsum_hazard_ratio)\n    likelihood = logits - log_risk\n    uncensored_likelihood = likelihood * fail_indicator\n    logL = - tf.reduce_sum(uncensored_likelihood,0)\n    observation = tf.reduce_sum(fail_indicator,0) + 1\n    return 1.0 * (logL / observation)# + 1e-3 * tf.reduce_sum(tf.square(logits))\n","metadata":{"execution":{"iopub.status.busy":"2021-06-06T19:03:41.808051Z","iopub.execute_input":"2021-06-06T19:03:41.808483Z","iopub.status.idle":"2021-06-06T19:03:41.8201Z","shell.execute_reply.started":"2021-06-06T19:03:41.808432Z","shell.execute_reply":"2021-06-06T19:03:41.818723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ActivityRegularizationLayer(layers.Layer):\n    def call(self, inputs):\n        self.add_loss(1e-5 * tf.reduce_sum(tf.square(inputs)))\n        return inputs\n","metadata":{"execution":{"iopub.status.busy":"2021-06-06T18:59:59.655551Z","iopub.execute_input":"2021-06-06T18:59:59.655941Z","iopub.status.idle":"2021-06-06T18:59:59.66204Z","shell.execute_reply.started":"2021-06-06T18:59:59.655898Z","shell.execute_reply":"2021-06-06T18:59:59.660418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate an optimizer.\noptimizer = keras.optimizers.SGD(learning_rate=1e-5,momentum=.99,nesterov=True)\n#optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n# Instantiate a loss function.\n#loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\nloss_fn = logL2\n\ntrain_acc_metric = keras.metrics.BinaryAccuracy()\nval_acc_metric = keras.metrics.BinaryAccuracy()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T19:04:14.911417Z","iopub.execute_input":"2021-06-06T19:04:14.911954Z","iopub.status.idle":"2021-06-06T19:04:14.951874Z","shell.execute_reply.started":"2021-06-06T19:04:14.911907Z","shell.execute_reply":"2021-06-06T19:04:14.95054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomModel(keras.Model):\n    def train_step(self, data):\n        # Unpack the data. Its structure depends on your model and\n        # on what you pass to `fit()`.\n        x, y = data\n\n        with tf.GradientTape() as tape:\n            y_pred = self(x, training=True)  # Forward pass\n            # Compute the loss value\n            # (the loss function is configured in `compile()`)\n            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n\n        # Compute gradients\n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n        # Update weights\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        # Update metrics (includes the metric that tracks the loss)\n        self.compiled_metrics.update_state(y, y_pred)\n        # Return a dict mapping metric names to current value\n        return {m.name: m.result() for m in self.metrics}","metadata":{"execution":{"iopub.status.busy":"2021-06-06T19:00:42.886187Z","iopub.execute_input":"2021-06-06T19:00:42.886601Z","iopub.status.idle":"2021-06-06T19:00:42.895165Z","shell.execute_reply.started":"2021-06-06T19:00:42.886568Z","shell.execute_reply":"2021-06-06T19:00:42.893646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"in1 = layers.Input(shape=(5000))\nin2 = layers.Input(shape=(5000))\nx1 = resnet1d(in1,16,10)\nx2 = resnet1d(in2,16,10)\nx = layers.add([x1,x2])\n#x = ActivityRegularizationLayer()(x)\noutputs = layers.Dense(1,'relu', kernel_regularizer=tf.keras.regularizers.L1(0.01), activity_regularizer=tf.keras.regularizers.L2(0.01))(x)\n\nmodel = models.Model([in1,in2],outputs)\n#model = CustomModel([in1,in2],outputs)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T19:04:24.910616Z","iopub.execute_input":"2021-06-06T19:04:24.911039Z","iopub.status.idle":"2021-06-06T19:04:26.747437Z","shell.execute_reply.started":"2021-06-06T19:04:24.911005Z","shell.execute_reply":"2021-06-06T19:04:26.746381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss=loss_fn,optimizer=optimizer,metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-06-06T19:04:28.006995Z","iopub.execute_input":"2021-06-06T19:04:28.007439Z","iopub.status.idle":"2021-06-06T19:04:28.027711Z","shell.execute_reply.started":"2021-06-06T19:04:28.007406Z","shell.execute_reply":"2021-06-06T19:04:28.026078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit([x_train[:,:,0],x_train[:,:,1]],y_train,batch_size=12,epochs=20)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T19:04:30.863186Z","iopub.execute_input":"2021-06-06T19:04:30.863576Z","iopub.status.idle":"2021-06-06T19:12:26.437482Z","shell.execute_reply.started":"2021-06-06T19:04:30.863543Z","shell.execute_reply":"2021-06-06T19:12:26.43631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef train_step(x, y):\n    with tf.GradientTape() as tape:\n        logits = model(x, training=True)\n        loss_value = loss_fn(y, logits)\n        # Add any extra losses created during the forward pass.\n        loss_value += sum(model.losses)\n    grads = tape.gradient(loss_value, model.trainable_weights)\n    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n    train_acc_metric.update_state(y, logits)\n    return loss_value\n\n@tf.function\ndef test_step(x, y):\n    val_logits = model(x, training=False)\n    val_acc_metric.update_state(y, val_logits)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T14:27:42.266234Z","iopub.execute_input":"2021-06-06T14:27:42.266577Z","iopub.status.idle":"2021-06-06T14:27:42.464581Z","shell.execute_reply.started":"2021-06-06T14:27:42.266542Z","shell.execute_reply":"2021-06-06T14:27:42.463831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\n\nepochs = 10\nfor epoch in range(1,epochs+1):\n    print(\"\\nStart of epoch %d\" % (epoch,))\n    start_time = time.time()\n\n    # Iterate over the batches of the dataset.\n    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n        loss_value = train_step(x_batch_train, y_batch_train)\n        #if np.isnan(loss_value.numpy()):\n        #    break\n\n        # Log every 200 batches.\n        if step % 10 == 0:\n            print(\"\\rTraining loss at step %d: %.4f     \" % (step, float(loss_value)),end='')\n           # print(\"Seen so far: %d samples\" % ((step + 1) * 12))\n\n    # Display metrics at the end of each epoch.\n    train_acc = train_acc_metric.result()\n    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n\n    # Reset training metrics at the end of each epoch\n    train_acc_metric.reset_states()\n\n    # Run a validation loop at the end of each epoch.\n    for x_batch_val, y_batch_val in val_dataset:\n        test_step(x_batch_val, y_batch_val)\n\n    val_acc = val_acc_metric.result()\n    val_acc_metric.reset_states()\n    print(\"Validation acc: %.4f\" % (float(val_acc),))\n    print(\"Time taken: %.2fs\" % (time.time() - start_time))","metadata":{"execution":{"iopub.status.busy":"2021-06-06T14:27:42.914862Z","iopub.execute_input":"2021-06-06T14:27:42.915181Z","iopub.status.idle":"2021-06-06T14:28:32.963528Z","shell.execute_reply.started":"2021-06-06T14:27:42.915153Z","shell.execute_reply":"2021-06-06T14:28:32.96189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test Model\n#test_list = []\n#for i in range(len(leads)):\n#    test_list.append(Xtest[:,:,i])\nyprob = model.predict([x_val[:,:,0],x_val[:,:,1]])","metadata":{"execution":{"iopub.status.busy":"2021-06-06T19:12:48.75817Z","iopub.execute_input":"2021-06-06T19:12:48.758633Z","iopub.status.idle":"2021-06-06T19:12:51.355993Z","shell.execute_reply.started":"2021-06-06T19:12:48.758599Z","shell.execute_reply":"2021-06-06T19:12:51.354781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yprob[:10]","metadata":{"execution":{"iopub.status.busy":"2021-06-06T19:12:51.358158Z","iopub.execute_input":"2021-06-06T19:12:51.358658Z","iopub.status.idle":"2021-06-06T19:12:51.368221Z","shell.execute_reply.started":"2021-06-06T19:12:51.3586Z","shell.execute_reply":"2021-06-06T19:12:51.366827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logL2(y_val.astype('float32'),yprob).shape","metadata":{"execution":{"iopub.status.busy":"2021-06-06T19:12:51.370904Z","iopub.execute_input":"2021-06-06T19:12:51.371432Z","iopub.status.idle":"2021-06-06T19:12:51.383937Z","shell.execute_reply.started":"2021-06-06T19:12:51.371375Z","shell.execute_reply":"2021-06-06T19:12:51.382397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cutoff =.5\nyp = np.array([1 if x >= cutoff else 0 for x in yprob])\nya = y_val\ncm = confusion_matrix(yp, ya)\nprint(cm)\nprint(classification_report(ya, yp, target_names=['nFA','FA']))","metadata":{"execution":{"iopub.status.busy":"2021-06-06T19:12:51.38622Z","iopub.execute_input":"2021-06-06T19:12:51.38689Z","iopub.status.idle":"2021-06-06T19:12:51.411585Z","shell.execute_reply.started":"2021-06-06T19:12:51.386843Z","shell.execute_reply":"2021-06-06T19:12:51.410196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}