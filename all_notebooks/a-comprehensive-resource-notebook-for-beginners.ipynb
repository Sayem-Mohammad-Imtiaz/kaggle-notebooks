{"cells":[{"metadata":{},"cell_type":"markdown","source":"# About this Notebook\n\n\"Wanted to contribute to ongoing fight against deadly Sars-CoV2 ,got to know about the new CORD dataset and Kaggle's initiative.Explored the page,tasks and got overwhelmed by the data and tasks\". Well if that's your story and somehow you have found this kernel , <b> You are in luck , because I am going to take you on a learning ride</b>.This is a Kernel which is contains all the resources for learning and gettinf started with CORD19 dataset,NLP,Unsupervised learning,etc, so that anyone with some or very little knowledge of data science and python can understand<br>\n<br>\n<br>\nThis is a long kernel which I've put together with a lot of effort .I promise if you stay with me till the end , you would have learned the following by the end :-\n* What is Sars-Cov2 exactly? How and why it came into Existence and how it is different from other Viruses?\n* All the resources and information which we have currently on Sars-Cov2 \n* Complete Understanding of CORD dataset .Its structure and way to parse and understand the overwhelming information it has.\n* Analysis, Insights and Visualization of new cleaned Dataset\n<br>\n<br>\n<b>In addition to these I will give a complete walkthough on tackling and solving one of the tasks </b>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport seaborn as sns\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input director","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. What is Sars-CoV2?\n\nSo how can we start analyzing something without having any domain knowledge,it would be like shooting arrows in the dark.<br>\n<br>\nWhat is Coronavirus?\n<br><br>\nThe coronavirus is a family of viruses that can cause a range of illnesses in humans including common cold and more severe forms like SARS and MERS which are life-threatening. The virus is named after its shape which takes the form of a crown with protrusions around it and hence is known as coronavirus.\n<br> https://youtu.be/aerq4byr7ps - Video explaining what exactly is Sars-Cov2\n<br><br>\nHow Did the recent Outbreak occur?\n<br><br>\nThe recent outbreak of coronavirus is believed to have occurred in a market for illegal wildlife in the central Chinese city of Wuhan. Chinese health authorities and the WHO are investigating the outbreak of the recent coronavirus which has claimed 17 lives and reportedly infected hundreds.\n<br>\nhttps://www.pennmedicine.org/news/news-blog/2020/march/the-biology-of-coronaviruses  - Here is great article so as to why the outbreak occur"},{"metadata":{},"cell_type":"markdown","source":"Here is a very infromative video as to why these deadly viruses keep appearing in china \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import IFrame, YouTubeVideo\nYouTubeVideo('TPpoJGYlW54',width=600, height=400)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is a video explaining how Viruses work.Getting this knowledge will help generating relevant features in the future\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import IFrame, YouTubeVideo\nYouTubeVideo('mWeJ0lakG_A',width=600, height=400)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Current Situation and Available Resources On Sars-CoV2"},{"metadata":{},"cell_type":"markdown","source":"Here is an article on how the world is fighting against the Outbreak of Sars-CoV2 \n<br>https://www.usatoday.com/story/news/world/2020/03/17/coronavirus-how-countries-across-globe-responding-covid-19/5065867002/\n<br><br>\nHere is a list of Resources available on and outside kaggle to understand the current situation and also to complete the tasks associated with CORD-19 Dataset:\n* https://coronavirus.jhu.edu/map.html  --> Interactive Day by Day Analysis of Outbreak By John Hopkins University\n* https://www.healthmap.org/covid-19/\n* https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset --> Day level Information on spread\n* https://www.kaggle.com/imdevskp/covid-19-analysis-viz-prediction-comparisons --> Comprehensive Kernel On visualization\n* https://www.kaggle.com/paultimothymooney/coronavirus-genome-sequence --> Coronavirus Genome Sequence Dataset\n* https://www.kaggle.com/paultimothymooney/coronavirus-genome-sequence --> Repository of CoronaVirus Genomes\n* https://www.kaggle.com/jamzing/sars-coronavirus-accession --> Exploration of Sars Mutation"},{"metadata":{},"cell_type":"markdown","source":"# 3. Understanding and Cleaning the CORD-19 Dataset\n\nNow that we have background information on Sars-CoV2 and we know about the current resources and situation , we can begin our dive into the CORD-19 Dataset and tasks associated with it\n<br><br>\nLet's first start with the data which is available in csv format because being a beginner its the easiest to wrap your head around.Right?"},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_data = pd.read_csv('/kaggle/input/CORD-19-research-challenge/2020-03-13/all_sources_metadata_2020-03-13.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now Before going any Further , Let's understand what each columns are and what the value in the columns really mean\n* Source - It is one of the most important columns of dataset its understanding is critical for our analysis\n           CHZ - It stands for CHen Zuckerberg Initiative\n           PMC - PUBMed Central is a free full text archieve of biomedical journal literature At US NIH. More can be learned here - https://en.wikipedia.org/wiki/PubMed_Central\n           BiorVix And MedrVix - are preprint server for biology which are online platform for publication of paperby Cold Spring Harbour University\n* Sha - 17K of the paper records have PDFs and the hash of the PDFs are in 'sha'\n* title - Contains the title of papers\n* DOI - A DOI, or Digital Object Identifier, is a string of numbers, letters and symbols used to permanently identify an article or document and link to it on the web. A DOI will help your reader easily locate a document from your citation.More can be learned here - https://en.wikipedia.org/wiki/Digital_object_identifier\n* pmcid and pubmed_id - https://nexus.od.nih.gov/all/2015/08/31/pmid-vs-pmcid-whats-the-difference/\n* Licence - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4845398/  This articles explains licensing and copyright for scholary articles. It may seem overwhelming at first but you can understand it by thorough reading\nRest of the columns can be understood by reading the readme file in the datset"},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_data.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So after intial analysis of our metadata file we can see the following things:-\n* There are only 24654 unique titles that means we have duplicate files\n* doi.org is missing from some of the values in doi columns \n* We have articles from 1732 unique journals"},{"metadata":{},"cell_type":"markdown","source":"Now Let's go ahead and drop duplicate and preprocess doi column"},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_data.drop_duplicates(subset=['title'], inplace=True)\nmeta_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def doi_url(d):\n    if d.startswith('http://'):\n        return d\n    elif d.startswith('doi.org'):\n        return f'http://{d}'\n    else:\n        return f'http://doi.org/{d}'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_data.doi = meta_data.doi.fillna('').apply(doi_url)\nmeta_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now Let's Look at the missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We have 2290 missing abstract values .I wonder if we can get the abstract values from the dataset obtained from all JSON Files , We will perform this exercise and see later in this kernel\n* We also have a value with missing title . I will simply drop that value"},{"metadata":{},"cell_type":"markdown","source":"So you might be wondering , we have a so many useful things in the metadata csv file. Why do we even use the json files?\n<br><br> Well you can see that we have titles,abstracts and authors available but not the real text of the paper which contains all the information. Once we form the dataset from Json files we can merge both datasets somehow and can get text corresponding to each paper in meta_data csv file and then a whole lot of things can be done . Also we can account for the missing values in the metadata dataframe."},{"metadata":{},"cell_type":"markdown","source":"Now I will not analyze metadata dataframe because we have beautiful kernels written for the same . I will provide links to them :-\n* https://www.kaggle.com/docxian/cord-19-metadata-evaluation written by Chris X\n<br><br>\nI will directly go to extracting a csv file from all the Json's Files and then I will merge the two dataframes to get a final working dataFrame"},{"metadata":{},"cell_type":"markdown","source":"# Extracting Information From JSON Files\n\nSo using pd.read_json() won't work here because the underlying structure of the json files are very complex.\n<br> The underlying structure can be seen from file name json.schema present in the dataset directory .Thus to extract information from the json files we will have to write our own customized pipeline . Thankfully for us this work has already been done by two great people :-\n* https://www.kaggle.com/xhlulu/cord-19-eda-parse-json-and-generate-clean-csv --> You can go through this kernel to learn how to build a pipeline to parse this huge amount of data\n* https://www.kaggle.com/fmitchell259/create-corona-csv-file --> From here you can learn how to get a dataframe out of this specific data . The former kernel is more comprehensive and great for learning. This kernel however just lets you easily get a csv file of all the json files. I will use the output of this file to build my final dataframe"},{"metadata":{},"cell_type":"markdown","source":"I have written code below to get the structure for any JSON file present . You can go through the helper functions and code from xhulu's kernel while checking it with this structure to learn how to parse files. That's how I learned . Hope that helps"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import json  ##CODE TO PRINT A JSON FILE TO CHECK ITS STRUCTURE\nfile_path = '/kaggle/input/CORD-19-research-challenge/2020-03-13/noncomm_use_subset/noncomm_use_subset/252878458973ebf8c4a149447b2887f0e553e7b5.json'\nwith open(file_path) as json_file:\n     json_file = json.load(json_file)\njson_file['metadata']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analysis of Final Data\nYou can add output of https://www.kaggle.com/fmitchell259/create-corona-csv-file this kernel as  (\"File\" -> \"Add or upload data\" -> \"Kernel Output File\" tab -> search the name of this notebook)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# metadata\nmeta_data = pd.read_csv('/kaggle/input/CORD-19-research-challenge/2020-03-13/all_sources_metadata_2020-03-13.csv')\n\nmeta_data.drop_duplicates(subset=['sha'], inplace=True)\n\ndef doi_url(d):\n    if d.startswith('http://'):\n        return d\n    elif d.startswith('doi.org'):\n        return f'http://{d}'\n    else:\n        return f'http://doi.org/{d}'\n\nmeta_data.doi = meta_data.doi.fillna('').apply(doi_url)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Json = pd.read_csv(\"../input/create-corona-csv-file/kaggle_covid-19_open_csv_format.csv\")\nJson = Json.iloc[1:, 1:].reset_index(drop=True)\n\n# merge frames\ncols_to_use = meta_data.columns.difference(Json.columns)\nall_data = pd.merge(Json, meta_data[cols_to_use], left_on='doc_id', right_on='sha', how='left')\n\nall_data.title = all_data.title.astype(str) # change to string, there are also some numeric values\n\nall_data.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below are some of the best written kernels so far , for this datset:\n* https://www.kaggle.com/maksimeren/covid-19-literature-clustering ---> Explains all the things that can be done using this data\n* https://www.kaggle.com/ratan123/cord-19-understanding-papers-with-textanalytics --> Some very great visualization for the data\n* https://www.kaggle.com/danielwolffram/topic-modeling-finding-related-articles --> Example of how topic modeling is done\n* https://www.kaggle.com/shahules/eda-find-similar-papers-easily --> Kernel exploring EDA and clustering\n\nHere are some important discussions which will help you gain more insights into data:\n* https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/discussion/137256\n* https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/discussion/137041\n* https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/discussion/136935\n* https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/discussion/136907\n\nUpdate :\n* https://www.kaggle.com/tarunpaparaju/covid-19-dataset-gaining-actionable-insights --> This is kernel by someone who is doing constant good work , he comes up with new and creative visualizations and kernels and is worth mentioning as a resource \n* https://www.kaggle.com/arturkiulian/covid-19-global-team-collaboration-join-slack --> If you want to collaborate go ahead and see this kernel"},{"metadata":{},"cell_type":"markdown","source":"I have tried my best to put together all the resources in a single notebook, so that any beginner,intermediate level data enthusiast can start the exploration of CORD data . I will keep on updating this kernel with more resources as soon as I come across them and hopefully I will publish more kernels exploring and applying different NLP and unsupervised techniques myself."},{"metadata":{},"cell_type":"markdown","source":"<b>If You liked my efforts , please upvote my kernel so that it reaches every new person who is overwhelmed by this dataset and quits before starting . Any kind of feedback to improve my kernel will be appreciated"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}