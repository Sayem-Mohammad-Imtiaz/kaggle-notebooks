{"cells":[{"metadata":{"_cell_guid":"5a7c334d-3afa-4508-874f-3366da0a9c2c","_kg_hide-output":false,"_uuid":"1139f2793448fc6a647e9a0c8e8679963d9f206d","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"import os\nimport pandas as pd\nfrom gensim import corpora, models, similarities"},{"metadata":{"_cell_guid":"7cdec987-42d7-495d-8aed-d904f430707a","_uuid":"2ab794b9bdd2400c1eb349493361f325fef51eee","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"datafile = '../input/bias-media-cat/all3.csv'"},{"metadata":{"_cell_guid":"52abb44e-881a-4fa5-aaad-cca178c148b4","_uuid":"3247c5f08df034b7afe4be36e52a99baa625a5ba"},"cell_type":"code","execution_count":null,"outputs":[],"source":"import pandas as pd\ntweets = pd.read_csv(datafile, encoding='utf8')\n#// tweets = tweets.assign(Time=pd.to_datetime(tweets.time)).drop('tweet_id', axis='columns')\n#x = \"\"+tweets.year+\"/\"+tweets.month + \"/\" +tweets.date +\" \" +tweets.time\n#tweets = tweets.assign(Time=pd.to_datetime(x)).drop('tweet_id', axis='columns')\n\ntweets.head(10)"},{"metadata":{"_cell_guid":"fdadead9-8dea-4c34-8bf2-1c4652418f36","_kg_hide-output":false,"scrolled":true,"_uuid":"a9bbf206e9d8b2b17c15da0df8c67f1365f03dab"},"cell_type":"code","execution_count":null,"outputs":[],"source":"range(len(tweets['tweet']))\n\nrange( len(tweets[tweets['tweet'].str.contains(\"atal|Catalonia|Catalunya|Cataluña\")==True]['tweet'] ))"},{"metadata":{"_cell_guid":"b06b4ca0-f1ab-45a1-9550-1492a30c3c74","_uuid":"ffb58f64bee55b852c07d0bc87be430d1258d976","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"corpus=[]\na=[]\n# remove punctiation\ntweets[\"tweet\"] = tweets['tweet'].str.replace('[^\\w\\s]','')\ntweets[\"tweet\"] = tweets[\"tweet\"] + \" \" + tweets[\"screen_name\"] \nfor a in tweets[tweets['tweet'].str.contains(\"Catalonia|Catalunya|Cataluña\")==True]['tweet']:\n        corpus.append(a)"},{"metadata":{"_cell_guid":"e943c950-1007-46ff-a868-412100b6830f","_uuid":"eeb90a8ffa5442ee9e32a456b227df1f61be706f"},"cell_type":"code","execution_count":null,"outputs":[],"source":"corpus[0:5]"},{"metadata":{"_cell_guid":"d117ab5e-3395-4430-8dad-0750a520c313","_uuid":"e6f1dd9faefafd8045c94fc71a033c201c5443ff"},"cell_type":"code","execution_count":null,"outputs":[],"source":"import gensim\nimport logging\nimport tempfile\n\nTEMP_FOLDER = tempfile.gettempdir()\nprint('Folder \"{}\" will be used to save temporary dictionary and corpus.'.format(TEMP_FOLDER))\n\nfrom gensim import corpora\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"},{"metadata":{"_cell_guid":"068cfe59-dbc2-4481-a20f-6cd321cbc4bb","_uuid":"df0a72a7a17efcf0da96a51753855ec2039cffb8","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"from nltk.corpus import stopwords\nfrom string import punctuation\n\n# remove common words and tokenize\nlist1 = ['RT','rt','es','el','per','amb','pel','parter','directo','és','els','tras','minuto','hora','social']\nstoplist =  stopwords.words('spanish') + stopwords.words('english') + list(punctuation) + list1\n\ntexts = [[word for word in str(document).lower().split() if word not in stoplist] for document in corpus]"},{"metadata":{"_cell_guid":"41eea25d-97f6-478c-97bb-8073cecb3101","_kg_hide-output":false,"_uuid":"c9824ceb1efd0895923b23ff8d058a86a43b9c54"},"cell_type":"code","execution_count":null,"outputs":[],"source":"dictionary = corpora.Dictionary(texts)\ndictionary.save(os.path.join(TEMP_FOLDER, 'elon.dict'))  # store the dictionary, for future reference\n#print(dictionary)"},{"metadata":{"_cell_guid":"38f1a502-a6fe-4c3d-b8bf-27c9272c3753","scrolled":false,"_uuid":"3944206c06ca0af2822f81c3ef0dadf994f5e023","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#print(dictionary.token2id)"},{"metadata":{"_cell_guid":"b80d4a62-6126-48e5-aba8-8de557c5d00d","_uuid":"040914ec58061f9f66a39df81c8c1ae4cb9dbeb5"},"cell_type":"code","execution_count":null,"outputs":[],"source":"corpus = [dictionary.doc2bow(text) for text in texts]\ncorpora.MmCorpus.serialize(os.path.join(TEMP_FOLDER, 'elon.mm'), corpus)  # store to disk, for later use"},{"metadata":{"_cell_guid":"30731758-2dd0-482c-8ad8-995ebf37b686","_uuid":"547b528b1af3eac74eed647c6013c67304a44f42"},"cell_type":"markdown","source":"In the previous cells, we created a corpus of documents represented as a stream of vectors. To continue, let’s fire up gensim and use that corpus:"},{"metadata":{"_cell_guid":"c654c601-bf99-4ca3-8fcd-c035dbe8af8f","_kg_hide-output":true,"_uuid":"b446ef66b9fb10675b0b8c0216527a35cddb5fa4","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"from gensim import corpora, models, similarities"},{"metadata":{"_cell_guid":"8c2fae3e-3c57-4a1b-b302-7f9bfe2fb5a0","_uuid":"516974a573c993c655cec2ea8ccda4a3e4e3d899"},"cell_type":"markdown","source":"### Creating a transformation"},{"metadata":{"_cell_guid":"c8cc6eba-dfde-4078-93aa-49ec66b79483","_uuid":"95d9a030b86689a097ff84cf988ce6bfee42b6d6"},"cell_type":"markdown","source":"\nThe transformations are standard Python objects, typically initialized by means of a training corpus:\n\nDifferent transformations may require different initialization parameters; in case of TfIdf, the “training” consists simply of\ngoing through the supplied corpus once and computing document frequencies of all its features.\nTraining other models, such as Latent Semantic Analysis or Latent Dirichlet Allocation, is much more involved and,\nconsequently, takes much more time."},{"metadata":{"_cell_guid":"a7966606-dc6a-44a8-8fe5-d1a79929e203","_uuid":"91cd194bf43c84b564c693c6e0ea109d28724abc"},"cell_type":"code","execution_count":null,"outputs":[],"source":"tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model"},{"metadata":{"_cell_guid":"47a7ad12-9917-4a74-b190-b6f039e87baf","_uuid":"902cd861d68967c9d6d78f7d074fe6e4d31e4547"},"cell_type":"markdown","source":"### Note\nTransformations always convert between two specific vector spaces. The same vector space (= the same set of feature ids) must be used for training as well as for subsequent vector transformations. Failure to use the same input feature space, such as applying a different string preprocessing, using different feature ids, or using bag-of-words input vectors where TfIdf vectors are expected, will result in feature mismatch during transformation calls and consequently in either garbage output and/or runtime exceptions."},{"metadata":{"_cell_guid":"31035482-743c-4143-a016-af6e7aafa8b2","_uuid":"12084cb081ffae38fd076b271f748faa6e3ff5af"},"cell_type":"markdown","source":"From now on, tfidf is treated as a read-only object that can be used to apply a transformation to a whole corpus:"},{"metadata":{"_cell_guid":"8d3211bd-a12d-4518-b822-eb92df9a564d","_uuid":"6ccc9986f35d02916c62139410d0cbc6455e3149","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"corpus_tfidf = tfidf[corpus]  # step 2 -- use the model to transform vectors"},{"metadata":{"_cell_guid":"6096b0ef-bb2b-40d1-aac4-f538dae0f1e3","_uuid":"39f769bd5e8fea11c952a7c33bf32b329618f837"},"cell_type":"markdown","source":"### LDA:\nhttps://en.wikipedia.org/wiki/Latent_Dirichlet_allocation"},{"metadata":{"_cell_guid":"7c793905-48bb-4526-9e78-1d5d1a9ed70b","_uuid":"8aca0810b66308f947046dd9689409fc56067b1a"},"cell_type":"markdown","source":"Latent Dirichlet Allocation, LDA is yet another transformation from bag-of-words counts into a topic space of lower dimensionality. LDA is a probabilistic extension of LSA (also called multinomial PCA), so LDA’s topics can be interpreted as probability distributions over words. These distributions are, just like with LSA, inferred automatically from a training corpus. Documents are in turn interpreted as a (soft) mixture of these topics (again, just like with LSA)."},{"metadata":{"_cell_guid":"cc49ccb3-bd68-4359-8fec-7f6cbff2a182","_uuid":"5771a4cc1b5f20878d52409e11f3fecaa59a9ac6","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"total_topics = 5"},{"metadata":{"_cell_guid":"8fb9df90-fdc2-491b-a2aa-02c0eca4c29a","scrolled":true,"_uuid":"db447043098a7546237bbaf4c7d1145103209160"},"cell_type":"code","execution_count":null,"outputs":[],"source":"lda = models.LdaModel(corpus, id2word=dictionary, num_topics=total_topics)\ncorpus_lda = lda[corpus_tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi"},{"metadata":{"_cell_guid":"dfbb2723-ec7e-490e-8abd-fb646335bc55","_uuid":"53e59fb13b5f55abb2e8158cdd0b244bcb055b3f"},"cell_type":"code","execution_count":null,"outputs":[],"source":"#Show first n important word in the topics:\nlda.show_topics(total_topics,5)"},{"metadata":{"_cell_guid":"5e31a267-03c5-4a4e-8a73-c72f7abd4644","scrolled":true,"_uuid":"25d66c5df87a0c365dcd0725e779dc8724ab63af","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"from collections import OrderedDict\n\ndata_lda = {i: OrderedDict(lda.show_topic(i,25)) for i in range(total_topics)}\n#data_lda"},{"metadata":{"_cell_guid":"409fed91-48dd-4355-9587-a6299b63f181","_uuid":"edeb3eedc9a10cc135c7bbef50fda042f330482d"},"cell_type":"code","execution_count":null,"outputs":[],"source":"import pandas as pd\n\ndf_lda = pd.DataFrame(data_lda)\nprint(df_lda.shape)\ndf_lda = df_lda.fillna(0).T\nprint(df_lda.shape)"},{"metadata":{"_cell_guid":"6410b413-a0d4-49be-a2c4-6844cfc8cb0c","_uuid":"f81dd0ed3ef908b520c2416cf626473a4dd8acd6"},"cell_type":"code","execution_count":null,"outputs":[],"source":"df_lda"},{"metadata":{"_cell_guid":"19a51407-5488-44d3-bdd2-9a0ec19ecf35","_uuid":"5206e6ec92bc2678c20b427efd788c0be67adcbd"},"cell_type":"code","execution_count":null,"outputs":[],"source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ng=sns.clustermap(df_lda.corr(), center=0, cmap=\"RdBu\", metric='cosine', linewidths=.75, figsize=(12, 12))\nplt.setp(g.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\nplt.show()\n#plt.setp(ax_heatmap.get_yticklabels(), rotation=0)  # For y axis"},{"metadata":{"_cell_guid":"bc653426-6a63-4b41-887d-92d270dc1168","_uuid":"415d0d14427e94f9900776f3126d5bf8c2391beb"},"cell_type":"code","execution_count":null,"outputs":[],"source":"import pyLDAvis.gensim\n\npyLDAvis.enable_notebook()\npanel = pyLDAvis.gensim.prepare(lda, corpus_lda, dictionary, mds='tsne')\npanel"}],"metadata":{"language_info":{"nbconvert_exporter":"python","pygments_lexer":"ipython3","mimetype":"text/x-python","name":"python","version":"3.6.3","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py"},"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat":4,"nbformat_minor":1}