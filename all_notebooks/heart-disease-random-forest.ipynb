{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fd90d316746cb4f42459d692660494e276060ca"},"cell_type":"code","source":"#Objectives:\n#        i)   Using pandas and sklearn for modeling\n#        ii)  Feature engineering\n#                  a) Using statistical measures\n#                  b) Using Random Projections\n#                  c) Using clustering\n#                  d) USing interaction variables\n#       iii)  Feature selection\n#                  a) Using derived feature importance from modeling\n#                  b) Using sklearn FeatureSelection Classes\n#        iv)  One hot encoding of categorical variables\n#         v)  Classifciation using Decision Tree and RandomForest\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4044ac8152a3f96ba7afe5b442769c3e19539dce"},"cell_type":"code","source":"# 1.0 Clear memory\n%reset -f","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"556c0b854fea10b36c5b91655864537f80ab14fe"},"cell_type":"code","source":"# 1.1 Call data manipulation libraries\nimport pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1cccabdc1e1c4d15e90be0eda4f0773c002c4800"},"cell_type":"code","source":"# 1.2 Feature creation libraries\nfrom sklearn.random_projection import SparseRandomProjection as sr  # Projection features\nfrom sklearn.cluster import KMeans                    # Cluster features\nfrom sklearn.preprocessing import PolynomialFeatures  # Interaction features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cde52d3d87e31d36ffdcf8980be9d1c28ba19ade"},"cell_type":"code","source":"# 1.3 For feature selection\n# Ref: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_classif  # Selection criteria","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c5582e7e4af829841b02e11444d138793a97773"},"cell_type":"code","source":"# 1.4 Data processing\n# 1.4.1 Scaling data in various manner\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, scale\n# 1.4.2 Transform categorical (integer) to dummy\nfrom sklearn.preprocessing import OneHotEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"448e0914e74608cc3913ed0bb0caa3590da9e6b5"},"cell_type":"code","source":"# 1.5 Splitting data\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcf61587a669cf7916878e735bbead9ea64b931d"},"cell_type":"code","source":"# 1.6 Decision tree modeling\n# http://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree\n# http://scikit-learn.org/stable/modules/tree.html#tree\nfrom sklearn.tree import  DecisionTreeClassifier as dt\nfrom sklearn.metrics import roc_curve, auc #for model evaluation\nfrom sklearn.metrics import classification_report #for model evaluation\nfrom sklearn.metrics import confusion_matrix #for model evaluation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8de2f6464abc782f4f5d0efe1d394fcf303d96d3"},"cell_type":"code","source":"# 1.7 RandomForest modeling\nfrom sklearn.ensemble import RandomForestClassifier as rf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8640cbb81b7f37d9423f1bbb28f34db1c72991a8"},"cell_type":"code","source":"# 1.8 Plotting libraries to plot feature importance\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.tree import export_graphviz #plot tree","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fee96719419bf69a036dced26a59edcb478c1cd4"},"cell_type":"code","source":"# 1.9 Misc\nimport os, time, gc\nimport eli5 #for purmutation importance\nfrom eli5.sklearn import PermutationImportance\nimport shap #for SHAP values\nfrom pdpbox import pdp, info_plots #for partial plots\nnp.random.seed(123) #ensure reproducibility","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf05f1ffbe31affec87be375e1fccb6b65980771"},"cell_type":"code","source":"################## AA. Reading data from files and exploring ####################\n\n# 2.0 Set working directory and read file\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb58ea8e7c1f7a327cb6c1581b4e47c0ef4fdab4"},"cell_type":"code","source":"# 2.1 Read heart.csv files\ndata = pd.read_csv(\"../input/heart.csv\") #Loading of Data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83c436b91ee711af970eb5b32068f8acf76de5cc"},"cell_type":"code","source":"# 2.2 Look at data\ndata.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b5e543dae028453db142692d0a29dfb30fcf63a"},"cell_type":"code","source":"data.shape    #(303,14)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da153c25253c83b7c8be9e235ab0957e2bee4ffd"},"cell_type":"code","source":"# 2.3 Data types\ndata.dtypes.value_counts()   # All afeatures are integers except target i.e.float64","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64b47181452cfb67ee3fb8e21f3e28b906efe90b"},"cell_type":"code","source":"# 3 Check if there are Missing values? None\n# 3 Check if there are Missing values? None\ndata.isnull().sum().sum()  # 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ec5f90a3a47a09503011a4558891189ceb7bbe7"},"cell_type":"code","source":"#3.1 Splitting the data in Train and Test\nX_train, X_test, y_train, y_test = train_test_split(data.drop('target', 1), data['target'], test_size = 0.3, random_state=10) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f26e9fcc1ba47a7205e08b8cb96404123978a8e0"},"cell_type":"code","source":"#3.2 Check the splits\nX_train.shape       # 212 X 13\nX_test.shape        #  91 X 13\ny_train.shape       # (212,)\ny_test.shape        # ( 91,)\n\n\n# Check if there are Missing values? None\nX_train.isnull().sum().sum()  # 0\nX_test.isnull().sum().sum()   # 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8a15e32ca42f19e65f3cbeeb199bdb8907e1f2f"},"cell_type":"code","source":"############################ Using Statistical Numbers #####################\n\n\n#  4. Feature 1: Row sums of features 1:93. More successful\n#                when data is binary.\n\nX_train['sum'] = X_train.sum(numeric_only = True, axis=1)  # numeric_only= None is default\nX_test['sum'] = X_test.sum(numeric_only = True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39b4c9f589c171eece3baf9b9699c099c24ab52e"},"cell_type":"code","source":"# 4.1 Assume that value of '0' in a cell implies missing feature\n#     Transform train and test dataframes\n#     replacing '0' with NaN\n#     Use pd.replace()\ntmp_train = X_train.replace(0, np.nan)\ntmp_test = X_test.replace(0,np.nan)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6934bf218664fa0a4a7e3f212009e7e503de24ae"},"cell_type":"code","source":"# 4.2 Check if tmp_train is same as train or is a view\n#     of train? That is check if tmp_train is a deep-copy\n\ntmp_train is X_train                # False\ntmp_train._is_view                # False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f7a2f19ed12b1607a4b9905a93d4d9e949a64b7"},"cell_type":"code","source":"# 4.3 Check if 0 has been replaced by NaN\ntmp_train.head(2)\ntmp_test.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f065bcae3a77026f79c4283a8acb7454387dca8f"},"cell_type":"code","source":"# 5. Feature 2 : For every row, how many features exist\n#                that is are non-zero/not NaN.\n#                Use pd.notna()\ntmp_train.notna().head(1)\nX_train[\"count_not0\"] = tmp_train.notna().sum(axis = 1)\nX_test['count_not0'] = tmp_test.notna().sum(axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"132744de2ee0582463a7d479cf1ee2820af4f837"},"cell_type":"code","source":"# 6. Similary create other statistical features\n#    Feature 3\n#    Pandas has a number of statistical functions\n#    Ref: https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#computations-descriptive-stats\n\nfeat = [ \"var\", \"median\", \"mean\", \"std\", \"max\", \"min\"]\nfor i in feat:\n    X_train[i] = tmp_train.aggregate(i,  axis =1)\n    X_test[i]  = tmp_test.aggregate(i,axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14f1304e363e9873531ddb12ae5e6ac5e3514138"},"cell_type":"code","source":"# 7 Delete not needed variables and release memory\ndel(tmp_train)\ndel(tmp_test)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb3ad7f485cb7b60f1b0e09f68d5fb8b27f1ce80"},"cell_type":"code","source":"# 7.1 So what do we have finally\nX_train.shape                # 212 X (1+ 13 + 7) ; 13th Index is target\nX_train.head(1)\nX_test.shape                 # 91 X (13 + 8)\nX_test.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"647d483bba5da3a93df517be15acd81e1b01e535"},"cell_type":"code","source":"# 8  Store column names of our data somewhere\n#     We will need these later (at the end of this code)\ncolNames = X_train.columns.values\ncolNames","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3bc0235014de45213d1bd5aa47e874d2ac253553"},"cell_type":"code","source":"################ Feature creation Using Random Projections ##################\n# 9. Generate features using random projections\n#     First stack train and test data, one upon another\ntmp = pd.concat([X_train,X_test],\n                axis = 0,            # Stack one upon another (rbind)\n                ignore_index = True\n                )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26a14ae58f379f64015d9cc83468372eb39cb773"},"cell_type":"code","source":"# 9.1\ntmp.shape     # 303 X 21","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"512baf1e3bf43fc3bc51cf773dc5a682c99a6e6a"},"cell_type":"code","source":"# 9.2 Transform tmp to numpy array\ntmp = tmp.values\ntmp.shape    #(303, 21)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9492a446e8edbd230cac2988957bba405e6066e8"},"cell_type":"code","source":"# 10. Let us create 4 random projections/columns\nNUM_OF_COM = 4\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db5bcf5ea00bd04c8cdfb66634f83a05a1fa9f8b"},"cell_type":"code","source":"# 10.1 Create an instance of class\nrp_instance = sr(n_components = NUM_OF_COM)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b0c46f8c5eaf00ecf11191d347b54f48ba3d9c4"},"cell_type":"code","source":"# 10.2 fit and transform the (original) dataset\n#      Random Projections with desired number\n#      of components are returned\nrp = rp_instance.fit_transform(tmp[:, :13])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35772affeeca44a7b50ca35dba0c49c1d807925f"},"cell_type":"code","source":"# 10.3 Look at some features\nrp[: 5, :  3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68601a294ab1dc0c93712f35908d58ae8f0d635b"},"cell_type":"code","source":"# 10.4 Create some column names for these columns\n#      We will use them at the end of this code\nrp_col_names = [\"r\" + str(i) for i in range(5)]\nrp_col_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea4cb4b8dfee08256c54550aa472da7eecc0a1c8"},"cell_type":"code","source":"############################ Feature creation using kmeans ####################\n# 11.1 Create a StandardScaler instance\nse = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd2a33743651db3fda61bc052c28db2512b7e6de"},"cell_type":"code","source":"# 11.2 fit() and transform() in one step\ntmp = se.fit_transform(tmp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9e731cf37b2a2ea03211172ce123e68970f0e69"},"cell_type":"code","source":"# 11.3\ntmp.shape               # 303 X 21 (an ndarray)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30d6bdbdc1b5c97413345ba0512ff64962e831b9"},"cell_type":"code","source":"# 12. Perform kmeans using 13 features.\n#     No of centroids is no of classes in the 'target'\ncenters = y_train.nunique()  \ncenters       # 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4e5aeeb96a15139ba1af68b2ccfaa46fb54faf5"},"cell_type":"code","source":"# 13 Begin clustering\n#13.1 First create object to perform clustering\nkmeans = KMeans(n_clusters=centers, # How many\n                n_jobs = 5)         # Parallel jobs for n_init","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d500a40fd618cea4690f4f1a927f2f094d35ac2c"},"cell_type":"code","source":"# 13.2 Next train the model on the original data only\nkmeans.fit(tmp[:, : 13])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"833287b61597b8cbdf1cc887b97b810e3533402d"},"cell_type":"code","source":"# 14 Get clusterlabel for each row (data-point)\nkmeans.labels_\nkmeans.labels_.size ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7dff82258fbb68e4ecae4734d499d00f08d897dd"},"cell_type":"code","source":"# 15. Cluster labels are categorical. So convert them to dummy\n\n# 15.1 Create an instance of OneHotEncoder class\nohe = OneHotEncoder(sparse = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"207fe1f099fdb4d4fd8ea1c50a615ec78a9c0ff6"},"cell_type":"code","source":"# 15.2 Use ohe to learn data\n#      ohe.fit(kmeans.labels_)\nohe.fit(kmeans.labels_.reshape(-1,1))     # reshape(-1,1) recommended by fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f54b77de52f53e306e1905ac6fb8959669068ff"},"cell_type":"code","source":"# 15.3 Transform data now\ndummy_clusterlabels = ohe.transform(kmeans.labels_.reshape(-1,1))\ndummy_clusterlabels\ndummy_clusterlabels.shape    (303,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17f545b7d44a4fd2b85fd9a830cf416d36550712"},"cell_type":"code","source":"# 15.4 We will use the following as names of new 2 columns\n#      We need them at the end of this code\n\nk_means_names = [\"k\" + str(i) for i in range(2)]\nk_means_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d852c7edda3ddda53ae400dda0564aa20373102d"},"cell_type":"code","source":"# 16. Will require lots of memory if we take large number of features\n#     Best strategy is to consider only impt features\n\ndegree = 2\npoly = PolynomialFeatures(degree,                 # Degree 2\n                          interaction_only=True,  # Avoid e.g. square(a)\n                          include_bias = False    # No constant term\n                          )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b59bb027d041959881d1152be05bf44915c265db"},"cell_type":"code","source":"# 16.1 Consider only first 5 features\n#      fit and transform\ndf =  poly.fit_transform(tmp[:, : 5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a61409ffa3564fe6d01292e725433b1fa630d376"},"cell_type":"code","source":"df.shape     # 303 X 15","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c3116ef2241bba9c4fec7ae27862ae586ac74ec"},"cell_type":"code","source":"# 16.2 Generate some names for these 15 columns\npoly_names = [ \"poly\" + str(i)  for i in range(15)]\npoly_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90b8084f5f6bdaf45197ddc426c1270f628d01d6"},"cell_type":"code","source":"################# concatenate all features now ##############################\n\n# 17 Append now all generated features together\n# 17 Append random projections, kmeans and polynomial features to tmp array\n\ntmp.shape          # 303 X 21\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20b23ef9ba167b811c8565fc34170d7eaf55bbd4"},"cell_type":"code","source":"#  17.1 If variable, 'dummy_clusterlabels', exists, stack kmeans generated\n#       columns also else not. 'vars()'' is an inbuilt function in python.\n#       All python variables are contained in vars().\n\nif ('dummy_clusterlabels' in vars()):               #\n    tmp = np.hstack([tmp,rp,dummy_clusterlabels, df])\nelse:\n    tmp = np.hstack([tmp,rp, df])       # No kmeans      <==","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff5e840ad2e55705bc86edf7c1280086b3e97782"},"cell_type":"code","source":"tmp.shape     #(303,63)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"441921a29e4c2b60c8acba2c4cfe37fbcc6fb49c"},"cell_type":"code","source":"# 18.1 Separate train and test\nX = tmp[: X_train.shape[0], : ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a16f5bdb7c112079ba8189abdeb1a5d82dd60dc7"},"cell_type":"code","source":"X.shape   #(212,63)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76b00a6e1dd7f96e79ddef8366e57b038c0ee047"},"cell_type":"code","source":"# 18.2\ntest = tmp[X_train.shape[0] :, : ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb8712758767a722f28cb580349f681c255d4fdc"},"cell_type":"code","source":"test.shape   # (91,63)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08676d049054782c92cc86705406cff61156b3e5"},"cell_type":"code","source":"# 18.3 Delete tmp\ndel tmp\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5229be2c3c84346b5585bb726b6bd95ab610f94"},"cell_type":"code","source":"################## Model building #####################\n\n\n# 19. Split train into training and validation dataset\nX_train, X_test, y_train, y_test = train_test_split(\n                                                    X,\n                                                    y_train,\n                                                    test_size = 0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f88eccc63cfddc48dcdfd5f3ef6480cafd2e307"},"cell_type":"code","source":"X_train.shape ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16ff55be1fc7565a073f5b9ed4dc994cbbd2e6f9"},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2bb7aa491d0633e5df17c8fce5154b6506018bc5"},"cell_type":"code","source":"# 24 Decision tree classification\n# 24.1 Create an instance of class\nclf = dt(min_samples_split = 5,\n         min_samples_leaf= 5\n        )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ab5826ff04886b93d832ca2256db4e75fabc557"},"cell_type":"code","source":"# 24.2 Fit/train the object on training data\n#      Build model\nclf = clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c85f62ea9416ff41dd79d06275fd7e1caa958fcc"},"cell_type":"code","source":"# 24.3 Use model to make predictions\nclasses = clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd2263c7482a86b97b82d47ced898989a7e79ea7"},"cell_type":"code","source":"# 24.4 Check accuracy\n(classes == y_test).sum()/y_test.size ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45887207fc102fbfdfb3a9849191206b1f5f188a"},"cell_type":"code","source":"# 25. Instantiate RandomForest classifier\nclf = rf(n_estimators=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ee218a447af211bca4c53a2c1b33f2dfd90b6db"},"cell_type":"code","source":"# 25.1 Fit/train the object on training data\n#      Build model\n\nclf = clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9594938eaf04e1fa71243d9dd6198de54c0d7ae2"},"cell_type":"code","source":"# 25.2 Use model to make predictions\nclasses = clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"598989b42c5787f9d0b9bae3be24172eb1c94432"},"cell_type":"code","source":"# 25.3 Check accuracy\n(classes == y_test).sum()/y_test.size      # 82%","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3dea465ab796f840bbf53afb572beff3be847cd0"},"cell_type":"code","source":"################## Feature selection #####################\n\n##****************************************\n## Using feature importance given by model\n##****************************************\n\n# 26. Get feature importance\nclf.feature_importances_        # Column-wise feature importance\nclf.feature_importances_.size   # 63\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31b3861427413385402e357bfc1368bcb3ef8c21"},"cell_type":"code","source":"# 26.1 To our list of column names, append all other col names\n#      generated by random projection, kmeans (onehotencoding)\n#      and polynomial features\n#      But first check if kmeans was used to generate features\n\nif ('dummy_clusterlabels' in vars()):       # If dummy_clusterlabels labels are defined\n    colNames = list(colNames) + rp_col_names+ k_means_names + poly_names\nelse:\n    colNames = colNames = list(colNames) + rp_col_names +  poly_names      # No kmeans      <==","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"130c7e8ae5c2bf79ad77a4f782bc260ef25df98a"},"cell_type":"code","source":"# 26.1.1 So how many columns?\nlen(colNames)           # 65","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f7cbc834148aa6c369682c329e88a8aea13d3d7"},"cell_type":"code","source":"#27 The Model\n#The next part fits a random forest model to the data,\n#split the data\nX_train, X_test, y_train, y_test = train_test_split(data.drop('target', 1), data['target'], test_size = .2, random_state=10) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"510a0ab7625d618d5beed3265ff952aa8baaf2c5"},"cell_type":"code","source":"model = rf(max_depth=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b71c8eef5512b45e558f68354aa529588134dc1d"},"cell_type":"code","source":"model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c3f1a0d5228aee710c10bec7c8e8d2b45f4825a"},"cell_type":"code","source":"rf(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=5, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"885d50c0aea6227bc004d6a66b074ecba49202f1"},"cell_type":"code","source":"#We plot the consequent decision tree, to see what it's doing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bbcb1c8413102bb8f842085a3d6b0c7b9c51911e"},"cell_type":"code","source":"estimator = model.estimators_[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa7ced50459bb9c5752d3ca3617418bfae060bcc"},"cell_type":"code","source":"feature_names = [i for i in X_train.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0823b90e011fbe7edabf52cb5dcb436bd5821a58"},"cell_type":"code","source":"y_train_str = y_train.astype('str')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5fb96a5726ee6ec30f95408104f15bc746855418"},"cell_type":"code","source":"y_train_str[y_train_str == '0'] = 'no disease'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3cd5d4d2994cc0b2417ca365adcdcbd1740eb712"},"cell_type":"code","source":"y_train_str[y_train_str == '1'] = 'disease'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"13f5f8be8cc13cc54ef3efdfc605d30879eb253d"},"cell_type":"code","source":"y_train_str = y_train_str.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f7f8877673a448f7ca6be4d290a19e93c34ed56"},"cell_type":"code","source":"#code from https://towardsdatascience.com/how-to-visualize-a-decision-tree-from-a-random-forest-in-python-using-scikit-learn-38ad2d75f21c\n\nexport_graphviz(estimator, out_file='tree.dot', \n                feature_names = feature_names,\n                class_names = y_train_str,\n                rounded = True, proportion = True, \n                label='root',\n                precision = 2, filled = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3922cec9aca1e37692edcfd7492d8aa4b3dd275a"},"cell_type":"code","source":"from subprocess import call","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14ccf9d8bb4948642f79ab756dd8b17ea399c469"},"cell_type":"code","source":"call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c1ee7bb4d1a5bbdb18bc0eef73058449904097d"},"cell_type":"code","source":"from IPython.display import Image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a2ce8f706453e660edcb474e502078da1cdb95a"},"cell_type":"code","source":"#This gives us on explainability tool. However, we can't glance at this and get a quick sense of the most important features.\n\nImage(filename = 'tree.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d46605ae58220315eca4c1f4a0784d86d00e13e"},"cell_type":"code","source":"y_predict = model.predict(X_test)\ny_pred_quant = model.predict_proba(X_test)[:, 1]\ny_pred_bin = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9b427562ed3d8346da5f4f696c5ba12a0ba5046"},"cell_type":"code","source":"#Assess the fit with a confusion matrix,\nconfusion_matrix = confusion_matrix(y_test, y_pred_bin)\nconfusion_matrix  # array([[26,  9],[ 3, 23]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bffc20c8ae941dd1a9446db424a0fd112928569e"},"cell_type":"code","source":"#Diagnostic tests are often sold, marketed, cited and used with sensitivity and specificity \n#as the headline metrics. \n#Sensitivity and specificity are defined as,\n#Sensitivity=TruePositivesTruePositives+FalseNegatives\n#Specificity=TrueNegativesTrueNegatives+FalsePositives\n#Let's see what this model is giving,\ntotal=sum(sum(confusion_matrix))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af04d7133b0454be4ed059c829102ee00a4bfc3d"},"cell_type":"code","source":"sensitivity = confusion_matrix[0,0]/(confusion_matrix[0,0]+confusion_matrix[1,0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37dfb6c6e6cf91e4babbcc8585335723c1129e56"},"cell_type":"code","source":"print('Sensitivity : ', sensitivity ) # 0.896551724137931","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e4b55346c6c8befb88783a7b99fa44a04a934a6"},"cell_type":"code","source":"specificity = confusion_matrix[1,1]/(confusion_matrix[1,1]+confusion_matrix[0,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ad90718f1a034f8c24f2f4c21740a34be3ebf6a"},"cell_type":"code","source":"print('Specificity : ', specificity) # 0.71875","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52ffc97331af28637169825b634e83973b076623"},"cell_type":"code","source":"#Now also check with a Receiver Operator Curve (ROC),\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_quant)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a7d803a667051fb651cd9265a960fb07acf0423"},"cell_type":"code","source":"fig, ax = plt.subplots()\nax.plot(fpr, tpr)\nax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for diabetes classifier')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e82420ba6f57b7db89366499ced923e29bc7770f"},"cell_type":"code","source":"#Another common metric is the Area Under the Curve, or AUC. \n#This is a convenient way to capture the performance of a model in a single number, although it's not without certain issues. \n#As a rule of thumb, an AUC can be classed as follows,\n#0.90 - 1.00 = excellent\n#0.80 - 0.90 = good\n#0.70 - 0.80 = fair\n#0.60 - 0.70 = poor\n#0.50 - 0.60 = fail\n#Checking what the above ROC gives us,\n\nauc(fpr, tpr) #0.8901098901098901\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c088051315023279acdb7f4d347b324215a8d1bb"},"cell_type":"code","source":"#Now let's see what the model gives us from the ML explainability tools.\n#Permutation importance is the first tool for understanding a machine-learning model, \n#and involves shuffling individual variables \n#in the validation data (after a model has been fit), \n#and seeing the effect on accuracy. Learn more here.\n\nperm = PermutationImportance(model, random_state=1).fit(X_test, y_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0843cd7cee576c418a5ed67d4ce0998cb4acc14b"},"cell_type":"code","source":"eli5.show_weights(perm, feature_names = X_test.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72ebb89679ea5760bfced2637dc4b1fe82aa3520"},"cell_type":"code","source":"base_features = data.columns.values.tolist()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8d55e88199eb571e3a160ef7096d15741717319"},"cell_type":"code","source":"base_features.remove('target')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93acafae1752d7d4d13454feb4db9226e2041a33"},"cell_type":"code","source":"feat_name = 'ca'\npdp_dist = pdp.pdp_isolate(model=model, dataset=X_test, model_features=base_features, feature=feat_name)\n\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e44a3fe7873bd2e31a5068ed1ab68d9ebfeb716"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}