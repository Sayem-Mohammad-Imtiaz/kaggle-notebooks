{"cells":[{"metadata":{},"cell_type":"markdown","source":"# GAN-based Drinks Model\nSince the standard sequence-based models did not seem to work well at being creative with drinks, here is an attempt to try and grade the models using a GAN-approach instead of a simple loss function. The condition GAN consists of a generator and a discriminator. The generator takes the sequence of letters in the drink name and returns a vector of ingredients and the discriminator decides for a sequence of letter + ingredient vector pair if it is 'real' or not. The model is currently trained with the entire dataset for every interation and an additional step is added to train the generator a bit more on the actual task, but this is potentially very bad form"},{"metadata":{"_cell_guid":"b9d47a5d-1ad7-4cb0-b1e1-472973addf2a","_uuid":"48276b78006168cb8054f6a12ef28c2c5f3d06bb","trusted":true},"cell_type":"code","source":"from json import loads\nimport pandas as pd\nfrom itertools import chain\nfrom dask import bag\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8110599f-e7e8-44d4-a762-1cf3db5cc10d","_uuid":"8cc80387c85a26431fb9981e3309e9b048b6c26c","trusted":true},"cell_type":"code","source":"#drink_df = pd.read_csv('../input/all_drinks.csv')\n#drink_df.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"409148fa-6f23-47eb-9778-c648cc195b42","collapsed":true,"_uuid":"20cac86a84bf1507109592c42dc04871320718dd"},"cell_type":"markdown","source":"# Parse the drink names \nThey are the input for our model (this is a simple approach by just counting the letters that show up"},{"metadata":{"_cell_guid":"6a7f2a98-a9a2-4d09-bbcb-f81ca05d7b20","_uuid":"8940610977d156d0656e1942b82417065936734f"},"cell_type":"markdown","source":"## Tokenize the names\nHere we translate the names into a tokenized vector so we can feed it to a sequence to sequence model"},{"metadata":{"_cell_guid":"e0b8ad99-6a59-4d60-a6cb-b3124cae3489","_uuid":"4598c4e7b9f7080effb411aa46bd4dc5a4d2a49a","trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"edd8e4fc-1080-40e9-bc6d-11663bebccfa","_uuid":"9967d2f6b07d1bc9cc41540b1c78c1e22d0ce1da","trusted":true},"cell_type":"code","source":"#str_vec = drink_df['strDrink'].str.lower()\n#MAX_NB_WORDS, MAX_SEQUENCE_LENGTH = 100, 40\n#tokenizer = Tokenizer(num_words=MAX_NB_WORDS, char_level=True)\n#tokenizer.fit_on_texts(str_vec)\n#train_sequences = tokenizer.texts_to_sequences(str_vec)\n#train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n#char_index = tokenizer.word_index\n#print('Found %s unique tokens.' % len(char_index))\n#train_x = np.stack([to_categorical(x, num_classes=len(char_index)+1) for x in train_data],0)\n#print(train_x.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(str_vec[0])\n#print(train_sequences[0])\n#print(train_data[0])\n#print(train_x[0][-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/devresearch-xydata/X_train.txt', sep=' ', header=None).values","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5943b049-70cd-4afa-97b2-8ceff7472e2f","_uuid":"3810f3cd230968aad8699ff5a24bd9197efee75c"},"cell_type":"markdown","source":"# Process Ingredients\nThis is what we want to predict so we need to transform it to a reasonable vector"},{"metadata":{"_cell_guid":"07440455-ff7a-4dcb-8a3a-915057efd38f","_uuid":"99cb954ee5aa3a430b37f2dca12b86415e1e3a10","trusted":true},"cell_type":"code","source":"def isempty(x):\n    try:\n        if x is None: \n            return True\n        elif len(x)<1:\n            return True\n        else:\n            return False\n    except:\n        # floating point nans\n        return True\n#all_ingred = drink_df[[x for x in drink_df.columns \n#                       if 'Ingredient' in x]].apply(lambda c_row: [v.lower() for k,v in c_row.items() if not isempty(v)],1)\n#all_ingred[0:3]\n\nall_ingred = pd.read_csv('../input/devresearch-xydata/y_train.txt', sep=' ', header=None)[0].apply(lambda x: [x])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9a03efd7-ed82-4132-91ec-016d39a22485","_uuid":"a2998ea109da1cb32186347549f87930778020c5","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\ningred_label = LabelEncoder()\ningred_label.fit(list(chain(*all_ingred.values)))\nprint('Found', len(ingred_label.classes_), 'unique ingredients, ', ingred_label.classes_)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9fe8cd23-f8f0-465d-b877-2ed9de00fc28","_uuid":"8edb1f1b641085f5e97166b37eddea5366d56df7"},"cell_type":"markdown","source":"Convert each ingredient to a one hot vector and sum them all together"},{"metadata":{"_cell_guid":"e256797e-1a15-454f-9507-9bbc50c0d73b","_uuid":"603e425c087917a865ab808c3124b80fd7499f30","trusted":true},"cell_type":"code","source":"y_vec = np.stack(all_ingred.map(lambda x: np.sum(to_categorical(ingred_label.transform(x), \n                                        num_classes=len(ingred_label.classes_)),0)),0).clip(0,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(all_ingred[1])\n#print(ingred_label.transform(all_ingred[1]))\n#print(ingred_label.inverse_transform([147]))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2c0ba4a6-7418-45e7-a87a-e0f81125b827","_uuid":"1e60ab2f423e7a0f175b45155b193ff9ea919f61"},"cell_type":"markdown","source":"# Prepare Training"},{"metadata":{"_cell_guid":"7a85ec42-82af-49ae-a948-9b28c4ab4b61","_uuid":"299496883153b71f9aec483ce1f6c8741f6bc575","trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\ntrain_idx, test_idx = train_test_split(range(y_vec.shape[0]), \n                                                    random_state = 12345,\n                                                   train_size = 0.7)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"89a46d03-1a5b-4d85-956c-841c3e287439","scrolled":true,"_uuid":"188b305c1ec518575ae6be84d4b3298724078c81","trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Embedding, Dense, Dropout, Masking, Conv1D, GlobalMaxPooling1D\nfrom keras.optimizers import Adam\n#simple_embed_lay = Embedding(len(char_index)+1, len(char_index)+1, \n#                                    mask_zero = False, \n#                                    weights = [np.eye(len(char_index)+1)], # start with a 1-1 weighting\n#                            name = '1_1_Mapping'       \n#                            )\n\n\n# XXX: I just changed len(char_index)+1 to 3, although I don't know if that makes any sense...\nsimple_embed_lay = Embedding(3, 3, \n                                    mask_zero = False, \n                                    weights = [np.eye(3)], # start with a 1-1 weighting\n                            name = '1_1_Mapping'       \n                            )\n\nsimple_embed_lay.trainable = False\nsimple_sequence_model = Sequential(name = 'Generator')\nsimple_sequence_model.add(Masking(0, input_shape = (None,)))\nsimple_sequence_model.add(simple_embed_lay)\nsimple_sequence_model.add(Conv1D(64, kernel_size = (3,), strides = 1, padding = 'same'))\nsimple_sequence_model.add(Conv1D(128, kernel_size = (3,), strides = 2, padding = 'same'))\nsimple_sequence_model.add(Conv1D(256, kernel_size = (3,), strides = 2, padding = 'same'))\nsimple_sequence_model.add(Conv1D(512, kernel_size = (3,), strides = 1, padding = 'same'))\nsimple_sequence_model.add(GlobalMaxPooling1D())\nsimple_sequence_model.add(Dropout(0.25))\nsimple_sequence_model.add(Dense(y_vec.shape[1], \n                                activation = 'sigmoid'))\nsimple_sequence_model.compile(loss = 'binary_crossentropy', # categorical and mae don't work well here\n                              optimizer = Adam(lr = 5e-4, decay = 1e-6), \n                             metrics = ['mae'])\nsimple_sequence_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input, concatenate\nseq_in = Input(shape = (None,), name = 'Name_Input')\ningred_in = Input(shape = (y_vec.shape[1],), name = 'Ingredients_Input')\nseq_proc = simple_embed_lay(Masking(0, input_shape = (None,))(seq_in))\nseq_feat = Conv1D(64, kernel_size = (3,), strides = 1, padding = 'same')(seq_proc)\nseq_feat = Conv1D(128, kernel_size = (3,), strides = 2, padding = 'same')(seq_feat)\nseq_feat = Conv1D(256, kernel_size = (3,), strides = 2, padding = 'same')(seq_feat)\nseq_gap_feat = Dense(y_vec.shape[1], activation='relu')(GlobalMaxPooling1D()(seq_feat))\nall_feat = Dropout(0.5)(concatenate([seq_gap_feat, ingred_in]))\n\nout_layer = Dense(2, activation = 'softmax')(all_feat)\n\ndisc_model = Model(inputs = [seq_in, ingred_in],\n                  outputs = [out_layer], name = 'Discriminator')\ndisc_model.compile(loss = 'categorical_crossentropy', \n                  optimizer = 'adam',\n                  metrics = ['acc'])\ndisc_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seq_in = Input(shape = (None,), name = 'Name_Input')\ngen_output = simple_sequence_model(seq_in)\ningred_in = Input(shape = (y_vec.shape[1],), name = 'Ingredients_Input')\ndisc_output = disc_model([seq_in, gen_output])\n\ncomb_model = Model(inputs = [seq_in, ingred_in],\n                  outputs = [disc_output])\ncomb_model.layers[-1].trainable = False\nprint('a')\nprint(comb_model.layers[-1].summary())\ncomb_model.layers[-1].compile(loss = 'categorical_crossentropy', \n                  optimizer = 'adam',\n                  metrics = ['acc'])\nprint('b')\nprint(comb_model.layers[-1].summary())\ncomb_model.compile(loss = 'categorical_crossentropy', \n                  optimizer = 'adam',\n                  metrics = ['acc'])\ncomb_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gen_loss = []\ndisc_loss = []\ndef show_loss(loss_history, prefix):\n    epich = np.cumsum(np.concatenate(\n        [np.linspace(0.5, 1, len(mh.epoch)) for mh in loss_history]))\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n    _ = ax1.semilogy(epich,\n                 np.concatenate([mh.history['loss'] for mh in loss_history]),\n                 'b-',\n                 epich, np.concatenate(\n            [mh.history['val_loss'] for mh in loss_history]), 'r-')\n    ax1.legend(['Training', 'Validation'])\n    ax1.set_title('%s Loss' % prefix)\n\n    _ = ax2.plot(epich, np.concatenate(\n        [mh.history['acc'] for mh in loss_history]), 'b-',\n                     epich, np.concatenate(\n            [mh.history['val_acc'] for mh in loss_history]),\n                     'r-')\n    ax2.legend(['Training', 'Validation'])\n    ax2.set_title('%s Accuracy' % prefix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"verbose = 0\nfrom tqdm import tqdm\nfor i in tqdm(range(80)):\n    # train the generator directly (optional step)\n    simple_sequence_model.fit(train_data[train_idx], \n                              y_vec[train_idx], \n                              verbose = verbose, epochs = 2)\n    # improve the discriminator\n    gen_ing_train = simple_sequence_model.predict(train_data[train_idx])\n    gen_ing_test = simple_sequence_model.predict(train_data[test_idx])\n    disc_train_input = [\n        np.concatenate([train_data[train_idx],train_data[train_idx]],0),\n        np.concatenate([y_vec[train_idx],gen_ing_train],0)\n    ]\n\n    disc_test_input = [\n        np.concatenate([train_data[test_idx],train_data[test_idx]],0),\n        np.concatenate([y_vec[test_idx],gen_ing_test],0)\n    ]\n\n    disc_train_output = to_categorical(np.concatenate([np.ones((len(train_idx))), np.zeros((len(train_idx)))],0))\n    disc_test_output = to_categorical(np.concatenate([np.ones((len(test_idx))), np.zeros((len(test_idx)))],0))\n\n    disc_loss += [disc_model.fit(disc_train_input, disc_train_output,\n                   validation_data = (disc_test_input, disc_test_output),\n                   verbose = verbose, shuffle = True)]\n    # improve the generator\n    gen_loss += [comb_model.fit([train_data[train_idx], y_vec[train_idx]], \n                   to_categorical(np.ones(len(train_idx))),\n                   validation_data = ([train_data[test_idx], y_vec[test_idx]], to_categorical(np.ones(len(test_idx)))),\n                   verbose = verbose, epochs = 2,\n                                shuffle = True)]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"show_loss(gen_loss, 'Generator')\nshow_loss(disc_loss, 'Discriminator')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Original Task\nHere we see how well it does at the original task compared to the labeled data"},{"metadata":{"_cell_guid":"6126970f-0a43-4ed3-890d-0664d6c9efb8","_uuid":"189a32ac0251170f5698ea1c8b943d5d34de7596","trusted":true},"cell_type":"code","source":"pred_vec = simple_sequence_model.predict(train_data[test_idx])\nprint('Mean Error %2.2f%%' % (100*mean_absolute_error(y_vec[test_idx][1:2], pred_vec[1:2])))\nprint('test', y_vec[test_idx][1:2])\nprint(train_data[0])\nprint(train_data[1])\nprint('out',simple_sequence_model.predict(train_data[0][0:1]))\nprint('out2',simple_sequence_model.predict(train_data[1][0:1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[328]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7da2ea4a-bdd4-4d78-9f6f-86ad310e59db","_uuid":"26116a9f2fb2a34cdf393fb13d8b3515fa262d9f","trusted":true},"cell_type":"code","source":"print('Input Name:', drink_df['strDrink'].values[test_idx[0]])\nprint('Real Ingredients', all_ingred.values[test_idx[0]])\n\nproc_pred = lambda out_pred: sorted([(ingred_label.inverse_transform([idx]), out_pred[idx])\n                              for idx in np.where(out_pred>0)[0]], key = lambda x: -x[1])\n\nprint('Predicted Ingredients')\nfor _, (i,j) in zip(range(5), proc_pred(pred_vec[0])):\n    print('%25s\\t\\t%2.2f%%' % (i,100*j))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pred_vec[0])\nprint(pred_vec[1])\nprint(pred_vec[100])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e3b77723-16d9-41b0-aa0b-4b4d20e425bb","_uuid":"0f65e7e1ddf6787664a4801e58665a7b4287e44a"},"cell_type":"markdown","source":"# Test Case"},{"metadata":{"_cell_guid":"85de7516-f0d9-4b80-b419-8c8ad94bb924","_uuid":"e32e2a39e42c9d4a0f734d73183c0f22390b8b6c","trusted":true},"cell_type":"code","source":"for rand_idx in np.random.choice(range(len(test_idx)), size = 3):\n    print('Input Name:', drink_df['strDrink'].values[test_idx[rand_idx]])\n    print('Real Ingredients', all_ingred.values[test_idx[rand_idx]])\n\n    proc_pred = lambda out_pred: sorted([(ingred_label.inverse_transform([idx]), out_pred[idx])\n                                  for idx in np.where(out_pred>0)[0]], key = lambda x: -x[1])\n\n    print('Predicted Ingredients')\n    for _, (i,j) in zip(range(5), proc_pred(pred_vec[rand_idx])):\n        print('%25s\\t\\t%2.2f%%' % (i,100*j))\n    print('')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c9cb83fa-3fee-4d05-a419-0dc75fa52602","_uuid":"75422bece0ef1db5bce40e65b95273355b3c0247","trusted":true},"cell_type":"code","source":"def predict_from_name(in_drink_name):\n    seq_arr = np.array(tokenizer.texts_to_sequences([in_drink_name.lower()]))\n    c_pred = simple_sequence_model.predict(seq_arr)\n    for _, (i,j) in zip(range(5), proc_pred(c_pred[0])):\n        print('%25s\\t\\t%2.2f%%' % (i,100*j))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dd8403a7-d3a0-47c8-ac60-d6beb3628001","_uuid":"bbc56c964578883d8254a0ef306fb32832f52c90","trusted":true},"cell_type":"code","source":"predict_from_name('super fancy drink')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"66c8fd57-e6a2-45d5-a454-732774150eb3","_uuid":"a041af1c7e58c70146ddd5899291b3af41056320","trusted":true},"cell_type":"code","source":"predict_from_name('hopping hippo')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fd324854-44b0-4010-8ce0-a6f8995c33c9","_uuid":"0f9004e316e17956c5b0e76d9844bd82ca7fdc7d","trusted":true},"cell_type":"code","source":"predict_from_name('kevs special')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"789ba3a48c43331b7f6ebb2be2514465e8501765","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","version":"3.5.2","mimetype":"text/x-python","pygments_lexer":"ipython3","nbconvert_exporter":"python"},"kernelspec":{"name":"python3","language":"python","display_name":"Python [default]"}},"nbformat":4,"nbformat_minor":1}