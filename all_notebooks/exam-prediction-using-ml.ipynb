{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Hey People. This notebook is aimed at beginners where you can learn about the most important ML algorithms.\nThe algorithms i have used to predict the exam marks are:**\n<ol>\n  <li>Simple Linear Regression</li>\n  <li>Random forest</li>\n  <li>SVM and its types</li>\n</ol>\n\nNote: The data is already cleaned. So, no data analysis is done."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#importing the libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = pd.read_csv('../input/exam-marks-prediction/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d.isnull()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**No null values are present. Now we can can go ahead and play with the data**"},{"metadata":{},"cell_type":"markdown","source":"One Hot Encoding \n\nIt refers to splitting the column which contains numerical categorical data to many columns depending on the number of categories present in that column. Each column contains “0” or “1” corresponding to which column it has been placed.\n\n\nSometimes in datasets, we encounter columns that contain numbers of no specific order of preference. The data in the column usually denotes a category or value of the category and also when the data in the column is label encoded. This confuses the machine learning model, to avoid this the data in the column should be One Hot encoded."},{"metadata":{"trusted":true},"cell_type":"code","source":"file_handler = open(\"../input/exam-marks-prediction/train.csv\", \"r\") \n \ndata = pd.read_csv(file_handler, sep = \",\")\ndata = data.rename(columns = {\"parental level of education\":\"pEdu\", \"test preparation course\":\"testPrep\"}) \n  \nfile_handler.close() \n  \nGender = {'male': 1,'female': 2}\nEthnicity = {'group A': 1, 'group B': 2, 'group C': 3, 'group D': 4, 'group E': 5}\nPEdu = {'high school': 1, 'some high school': 2, 'associate\\'s degree':3, 'bachelor\\'s degree': 4, 'master\\'s degree': 5, 'some college': 6 }\nLunch = {'standard': 1, 'free/reduced': 2}\nTestPrep = {\"none\": 0, 'completed':1}\n  \ndata.gender = [Gender[item] for item in data.gender] \ndata.ethnicity = [Ethnicity[item] for item in data.ethnicity]\ndata.pEdu = [PEdu[item] for item in data.pEdu]\ndata.lunch = [Lunch[item] for item in data.lunch]\ndata.testPrep = [TestPrep[item] for item in data.testPrep]\ndata.head ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_handler = open(\"../input/exam-marks-prediction/test.csv\", \"r\") \n \ndatat = pd.read_csv(file_handler, sep = \",\")\ndatat = datat.rename(columns = {\"parental level of education\":\"pEdu\", \"test preparation course\":\"testPrep\"}) \n  \nfile_handler.close() \n  \nGender = {'male': 1,'female': 2}\nEthnicity = {'group A': 1, 'group B': 2, 'group C': 3, 'group D': 4, 'group E': 5}\nPEdu = {'high school': 1, 'some high school': 2, 'associate\\'s degree':3, 'bachelor\\'s degree': 4, 'master\\'s degree': 5, 'some college': 6 }\nLunch = {'standard': 1, 'free/reduced': 2}\nTestPrep = {\"none\": 0, 'completed':1}\n  \ndatat.gender = [Gender[item] for item in datat.gender] \ndatat.ethnicity = [Ethnicity[item] for item in datat.ethnicity]\ndatat.pEdu = [PEdu[item] for item in datat.pEdu]\ndatat.lunch = [Lunch[item] for item in datat.lunch]\ndatat.testPrep = [TestPrep[item] for item in datat.testPrep]\ndatat.head ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datat = datat.drop([\"Unnamed: 0\"], axis = 1)#Droping unnecessary columns\ndata = data.drop([\"Unnamed: 0\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop([\"math score\"], axis = 1)\ny = data[\"math score\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Splitting the data to train and test data.**\n\n**The idea behind StandardScaler is that it will transform your data such that its distribution will have a mean value 0 and standard deviationof 1.**\n\n**In case of multivariate data, this is done feature-wise (in other words independently for each column of the data).\nGiven the distribution of the data, each value in the dataset will have the mean value subtracted, and then divided by the standard deviation of the whole dataset (or feature in the multivariate case).**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nX_train, X_test, y_train, y_test = train_test_split(X, y)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>Linear Regression"},{"metadata":{},"cell_type":"markdown","source":"Simple Linear Regression is a great first machine learning algorithm to implement as it requires you to estimate properties from your training dataset, but is simple enough for beginners to understand. Linear regression is a prediction method that is more than 200 years old."},{"metadata":{},"cell_type":"markdown","source":"Linear regression assumes a linear or straight line relationship between the input variables (X) and the single output variable (y). More specifically, that output (y) can be calculated from a linear combination of the input variables (X). When there is a single input variable, the method is referred to as a simple linear regression.\n\nIn simple linear regression we can use statistics on the training data to estimate the coefficients required by the model to make predictions on new data.\n\nThe line for a simple linear regression model can be written as:\n\n                                          y=b0+b1∗x\n \nwhere  b0  and  b1  are the coefficients we must estimate from the training data. Once the coefficients are known, we can use this equation to estimate output values for  y  given new input examples of  x . It requires that you calculate statistical properties from the data such as mean, variance and covariance."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression #importing Linear Regression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nlr = LinearRegression()\nmodel = lr.fit(X_train,y_train)\n\ndef get_cv_scores(model):\n    scores = cross_val_score(model,\n                             X_train,\n                             y_train,\n                             cv=5,\n                             scoring='r2')\n    \n    print('CV Mean: ', np.mean(scores))\n    print('STD: ', np.std(scores))\n    print('\\n')\n\n# get cross val scores\nget_cv_scores(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Testing the accuracy of the model\nscore1 = model.score(X_test, y_test)\nprint(\"Accuracy:\",score1*100,'\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>Random Forest"},{"metadata":{},"cell_type":"markdown","source":"Random forest is a supervised learning algorithm. It has two variations – one is used for classification problems and other is used for regression problems. It is one of the most flexible and easy to use algorithm. It creates decision trees on the given data samples, gets prediction from each tree and selects the best solution by means of voting. It is also a pretty good indicator of feature importance.\n\nRandom forest algorithm combines multiple decision-trees, resulting in a forest of trees, hence the name Random Forest. In the random forest classifier, the higher the number of trees in the forest results in higher accuracy."},{"metadata":{},"cell_type":"markdown","source":"Random Forest algorithm intuition \n\nRandom forest algorithm intuition can be divided into two stages.\n\nIn the first stage, we randomly select “k” features out of total m features and build the random forest. In the first stage, we proceed as follows:-\n<ol>\n    <li>Randomly select k features from a total of m features where k<m. </li>\n<li>Among the k features, calculate the node d using the best split point.</li>\n<li>Split the node into daughter nodes using the best split.</li>\n<li>Repeat 1 to 3 steps until l number of nodes has been reached.</li>\n<li>Build forest by repeating steps 1 to 4 for n number of times to create n number of trees.</li>\n</ol>\nIn the second stage, we make predictions using the trained random forest algorithm.\n<ol>\n<li>We take the test features and use the rules of each randomly created decision tree to predict the outcome and stores the predicted outcome.</li>\n<li>Then, we calculate the votes for each predicted target.</li>\n<li>Finally, we consider the high voted predicted target as the final prediction from the random forest algorithm.</li>"},{"metadata":{},"cell_type":"markdown","source":"In the first step we have to chech the standard deviation of the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.std()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The deviation of 'reading score' and 'writing score' is high. To reduce them np.log() is used"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"reading score\"] = np.log(data[\"reading score\"])\ndata[\"writing score\"] = np.log(data[\"writing score\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Initialing Random forest with all the parameters set to default\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier\\\n(n_estimators=200, criterion='gini', max_depth=9,\nmin_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0,\nmax_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0,\nmin_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None,\nrandom_state=4, verbose=1, warm_start=False, class_weight=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = rf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DT_score = clf.score(X_test, y_test)\nprint(\"Decision Tree Accuracy:\" , DT_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, the score is really low. This is because of either the data is over/under fit or the parameters of the forest are wrong.\n\nTo find the best parameters for the random forest:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\ngrid={'max_depth':[3,6,9,12],'n_estimators':[10,50,100,200]}\na=GridSearchCV(rf, param_grid=grid, scoring='accuracy',\n                            n_jobs=-1, refit=True,\n                            cv=4, verbose=2, error_score=np.nan, return_train_score=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = a.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(a.best_params_)# this gives us the best params for the forest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_new = rf = RandomForestClassifier\\\n(n_estimators=100, max_depth=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new = rf_new.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_new = rf_new.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DT_score = a.score(X_test, y_test)\nprint(\"Decision Tree Accuracy:\" , DT_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>SVM Algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SUPPORT VECTOR MACHINES AND METHODS :"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Linear SVM\nfor this_C in [1,3,5,10,40,60,80,100]:\n    clf = SVC(kernel='linear',C=this_C).fit(X_train,y_train)\n    scoretrain = clf.score(X_train,y_train)\n    scoretest  = clf.score(X_test,y_test)\n    print(\"Linear SVM value of C:{}, training score :{:2f} , Test Score: {:2f} \\n\".format(this_C,scoretrain,scoretest))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score,StratifiedKFold,LeaveOneOut\nclf1 = SVC(kernel='linear',C=20).fit(X_train,y_train)\nscores = cross_val_score(clf1,X_train,y_train,cv=5)\nstrat_scores = cross_val_score(clf1,X_train,y_train,cv=StratifiedKFold(5,random_state=10,shuffle=True))\n#Loo = LeaveOneOut()\n#Loo_scores = cross_val_score(clf1,X_train,Y_train,cv=Loo)\nprint(\"The Cross Validation Score :\"+str(scores))\nprint(\"The Average Cross Validation Score :\"+str(scores.mean()))\nprint(\"The Stratified Cross Validation Score :\"+str(strat_scores))\nprint(\"The Average Stratified Cross Validation Score :\"+str(strat_scores.mean()))\n#print(\"The LeaveOneOut Cross Validation Score :\"+str(Loo_scores))\n#print(\"The Average LeaveOneOut Cross Validation Score :\"+str(Loo_scores.mean()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.dummy import DummyClassifier\n\nfor strat in ['stratified', 'most_frequent', 'prior', 'uniform']:\n    dummy_maj = DummyClassifier(strategy=strat).fit(X_train,y_train)\n    print(\"Train Stratergy :{} \\n Score :{:.2f}\".format(strat,dummy_maj.score(X_train,y_train)))\n    print(\"Test Stratergy :{} \\n Score :{:.2f}\".format(strat,dummy_maj.score(X_test,y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The score of the above :\"+str(clf1.score(X,y)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Linear Support vector machine with only C Parameter \nfrom sklearn.svm import LinearSVC\n\nfor this_C in [1,3,5,10,40,60,80,100]:\n    clf2 = LinearSVC(C=this_C).fit(X_train,y_train)\n    scoretrain = clf2.score(X_train,y_train)\n    scoretest  = clf2.score(X_test,y_test)\n    print(\"Linear SVM value of C:{}, training score :{:2f} , Test Score: {:2f} \\n\".format(this_C,scoretrain,scoretest))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apparently we got better scores with SVC where we defined the kernal as linear than with just LinearSVC\n\nThe LinearSVC class is based on the liblinear library, which implements an optimized algorithm for linear SVMs.\n<ol>\n<li>It does not support the kernel trick, but it scales almost linearly with the number of training instances and the number of features: its training time complexity is roughly O(m × n).</li>\n\nThe SVC class is based on the libsvm library, which implements an algorithm that supports the kernel trick.\n<ol>\n    <li>The training time complexity is usually between O(m2 × n) and O(m3 × n).</li>\n<li>LinearSVC is much faster than SVC(kernel=\"linear\")</li?"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVR\n\nsvr = SVR(kernel='linear',C=1,epsilon=.01).fit(X_train,y_train)\nprint(\"{:.2f} is the accuracy of the SV Regressor\".format(svr.score(X_train,y_train)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<ol>\n<li>SVM supports linear and nonlinear regression.\n<li>SVM Regression tries to fit as many instances as possible on the decision boundary while limiting margin violations.\n<li>The width of the decision boundary is controlled by a hyperparameter ϵ.\n"},{"metadata":{},"cell_type":"markdown","source":"NON LINEAR SVM\n\nA method to Handle Non linear relationships in our data set is to use polynomial Kernal or using a similarity function with our SVM.\n\nWe will use the Gaussian Radial Basis Function(RBF) function for the same. to handle this in Sklearn there is a Gamma hyperparameter. Check the Gausian RBF Function - for more info.\n\nTechnically, the gamma parameter is the inverse of the standard deviation of the RBF kernel (Gaussian function), which is used as similarity measure between two points. Intuitively, a small gamma value define a Gaussian function with a large variance. In this case, two points can be considered similar even if are far from each other. In the other hand, a large gamma value means define a Gaussian function with a small variance and in this case, two points are considered similar just if they are close to each other.\n\nInitution : we create different landmarks and then check how far the training examples are from the landmark. In practise, if we have n training examples then we will have n landmarks and we will thus create a feature set of n values with n landmarks. When the training example is closest to a landmark the value the variance will be small and when far the value will be large and hence we will associate the close to the landmark example with a 1 and those that are far with a 0. This ability makes the SVM very powerful."},{"metadata":{"trusted":true},"cell_type":"code","source":"# SMV with RBF KERNAL AND ONLY C PARAMETER \n\nfor this_C in [1,5,10,25,50,100]:\n    clf3 = SVC(kernel='rbf',C=this_C).fit(X_train,y_train)\n    clf3train = clf3.score(X_train,y_train)\n    clf3test  = clf3.score(X_test,y_test)\n    print(\"SVM for Non Linear \\n C:{} Training Score : {:2f} Test Score : {:2f}\\n\".format(this_C,clf3train,clf3test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SVM WITH RBF KERNAL, C AND GAMMA HYPERPARAMTER \nfor this_gamma in [.1,.5,.10,.25,.50,1]:\n    for this_C in [1,5,7,10,15,25,50]:\n        clf3 = SVC(kernel='rbf',C=this_C,gamma=this_gamma).fit(X_train,y_train)\n        clf3train = clf3.score(X_train,y_train)\n        clf3test  = clf3.score(X_test,y_test)\n        print(\"SVM for Non Linear \\n Gamma: {} C:{} Training Score : {:2f} Test Score : {:2f}\\n\".format(this_gamma,this_C,clf3train,clf3test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# grid search method \nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {'C': [1,5,7,10,15,25,50],\n              'gamma': [.1,.5,.10,.25,.50,1]}\nGS = GridSearchCV(SVC(kernel='rbf'),param_grid,cv=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GS.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"the parameters {} are the best.\".format(GS.best_params_))\nprint(\"the best score is {:.2f}.\".format(GS.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Kernalized SVM machine \nsvr2 = SVR(degree=2,C=100,epsilon=.01).fit(X_train,y_train)\nprint(\"{:.2f} is the accuracy of the SV Regressor\".format(svr2.score(X_train,y_train)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>KNN Algorithm\n    "},{"metadata":{},"cell_type":"markdown","source":"KNN is one of the simplest forms of machine learning algorithms mostly used for classification. It classifies the data point on how its neighbor is classified.\n\nKNN classifies the new data points based on the similarity measure of the earlier stored data points. For example, if we have a dataset of cars and bikes. KNN will store similar measures like shape and build. When a new object comes it will check its similarity with the build and shape."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding the best K value\nfrom sklearn.neighbors import KNeighborsClassifier\ntest_scores = []\ntrain_scores = []\n\nfor i in range(1,15):\n\n    knn = KNeighborsClassifier(i)\n    knn.fit(X_train,y_train)\n    \n    train_scores.append(knn.score(X_train,y_train))\n    test_scores.append(knn.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## score that comes from testing on the same datapoints that were used for training\nmax_train_score = max(train_scores)\ntrain_scores_ind = [i for i, v in enumerate(train_scores) if v == max_train_score]\nprint('Max train score {} % and k = {}'.format(max_train_score*100,list(map(lambda x: x+1, train_scores_ind))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## score that comes from testing on the datapoints that were split in the beginning to be used for testing solely\nmax_test_score = max(test_scores)\ntest_scores_ind = [i for i, v in enumerate(test_scores) if v == max_test_score]\nprint('Max test score {} % and k = {}'.format(max_test_score*100,list(map(lambda x: x+1, test_scores_ind))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Setup a knn classifier with k neighbors\nknn = KNeighborsClassifier(14)\n\nknn.fit(X_train,y_train)\nknn.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n#In case of classifier like knn the parameter to be tuned is n_neighbors\nparam_grid = {'n_neighbors':np.arange(1,50)}\nknn = KNeighborsClassifier()\nknn_cv= GridSearchCV(knn,param_grid,cv=5)\nknn_cv.fit(X,y)\n\nprint(\"Best Score:\" + str(knn_cv.best_score_))\nprint(\"Best Parameters: \" + str(knn_cv.best_params_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Other algorithms using Gradient boosting**"},{"metadata":{},"cell_type":"markdown","source":"<h1>XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit model no training data\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test)\npredictions = [round(value) for value in y_pred]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}