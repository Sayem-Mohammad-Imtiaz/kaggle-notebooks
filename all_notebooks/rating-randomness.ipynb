{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 注意：该版分析使用的数据是第一版的数据，丢失率极高。而新版数据不存在这种情况","metadata":{}},{"cell_type":"code","source":"### packages for data manipilation ###\nimport numpy as np \nimport pandas as pd \n\n### packages for plotting ###\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import iplot, init_notebook_mode\n\n### web info extract###\nfrom bs4 import BeautifulSoup\nimport requests\n\n### other helpers ###\nimport os, json, gc, datetime\nfrom datetime import timedelta\n\n# just in case that Chinese character cannot be recognised\nfrom matplotlib.font_manager import FontProperties\nfont_set = FontProperties(fname=\"/kaggle/input/chinesecharacter/NotoSansHans-Regular.otf\",size=15)\n\n# import datasets\nrecord = pd.read_csv('/kaggle/input/bangumi/record_2020_03_10.tsv',delimiter='\\t')\nuser   = pd.read_csv('/kaggle/input/bangumi/user_2020_03_10.tsv'  ,delimiter='\\t')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 用户数据\n\n因为爬记录的时候会顺带爬主页，就可以顺便看看：","metadata":{}},{"cell_type":"code","source":"user.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"我设置爬取id从1到529000的用户数据，成功爬取了508825名用户的主页。成功率还蛮高的。","metadata":{}},{"cell_type":"markdown","source":"# 记录数据\n\n分析的重头戏，不知道效果咋样：","metadata":{}},{"cell_type":"code","source":"record.loc[record.comment.notnull()].head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"不知道数据缺失情况怎么样，这里拿Clannad AS当个例子：","metadata":{}},{"cell_type":"code","source":"record.loc[record.iid == 876].state.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"而真实情况是 1820人想看 / 13401人看过 / 399人在看 / 427人搁置 / 145人抛弃。  \n我的天，“看过”和“想看”少了不少，蛋疼。 \n\n---\n虽然不清楚scrapy的机理，但爬取数据的成败应该是只与网络有关。所以对于分析来说，随机缺失（missing at random，MAR）的假设是有效的。\n\n出于谨慎，看看不同分数（1-10）的丢失率有没有不同：","metadata":{}},{"cell_type":"code","source":"mirror_bgm = 'http://mirror.bgm.rin.cat/'\ndef rating_status(iid):\n    \"\"\"get rating status from the subject \n    return: a dictionary -- {rate: no. of rating}\"\"\"\n    temp = requests.get(mirror_bgm + f'subject/{iid}')\n    temp.encoding = \"UTF-8\"\n    temp = temp.text\n    bans = BeautifulSoup(temp, 'html.parser') \n    \n    ouput = [eval(i.get_text()[1:-1]) for i in bans.find_all('span', class_= 'count')]\n    ouput.reverse()\n    return ouput\n\ndef loss_rate(iid):\n    \"\"\"calculate % missing for each rate\n    return: dictionary with keys 1-10\"\"\"\n    \n    temp = record.loc[record.iid == eval(iid)].rate.value_counts().to_dict()\n    temp_dict = {i+1:0 for i in range(10)}\n    for key,value in temp.items():\n        temp_dict[key]+=value\n        \n    for idx, val in enumerate(rating_status(iid)):\n        temp_dict[idx+1] = 100 * (1 - temp_dict[idx+1]/val) if val != 0 else 0\n    \n    \n    return temp_dict\n\ndef get_anime_list(pages):\n    \"\"\"get samples of anime subjects\n    return: a list of string\"\"\"\n    ouput = []\n    for count in range(1,pages+1):\n        temp = requests.get(mirror_bgm + f'anime/browser?sort=rank&page={count}')\n        temp.encoding = \"UTF-8\"\n        temp = temp.text\n        bans = BeautifulSoup(temp, 'html.parser') \n        \n        ouput += [i['href'][9:] for i in bans.find_all('a', class_ = \"subjectCover cover ll\")]\n    return ouput","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"anime_list = get_anime_list(25)\ndata_dict = {i+1:[] for i in range(10)}\ndata_dict['iid'] = []\nfor iid in anime_list:\n    temp = loss_rate(iid)\n    for key,value in temp.items():\n        data_dict[key].append(value)\n    data_dict['iid'].append(iid)   \n    \nloss_df = pd.DataFrame(data_dict).set_index('iid')\nloss_df.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"不同分数的丢失率（%）平均值：","metadata":{}},{"cell_type":"code","source":"loss_df.mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"发现低分数段和10分丢失率较高。\n\n不同分数的丢失率标准差：","metadata":{}},{"cell_type":"code","source":"loss_df.std()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"推测因为低分数段评分相较高分数段偏少，所以标准差大。","metadata":{}},{"cell_type":"markdown","source":"### 一些动画的评分丢失率分布\n样本大小为600部","metadata":{}},{"cell_type":"code","source":"sns.set(rc={'figure.figsize':(10,6)})\nloss_df.apply(np.mean, axis = 1).hist(bins = 25)#.mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"平均丢失率为{round(loss_df.apply(np.mean, axis = 1).mean(),2)}%。\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 总结\n\n本次数据爬取结果一般，作为整体分析的用途而言只能说是勉强可行，没有到推荐的程度。而针对隔别条目的分析就更是完全不可用了。  \n另一方面，低分段和10分丢失率相较之下偏高，是个比较有趣的现象，目前没有想到合理的推测。","metadata":{}}]}