{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"import pandas as pd\nimport csv\nimport os\nimport numpy\nimport copy\n%matplotlib inline\nimport numpy as np\nimport seaborn as sns\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom gensim import corpora, models, similarities, matutils\nfrom gensim.corpora.dictionary import Dictionary\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.decomposition import NMF, PCA\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom collections import Counter, defaultdict, OrderedDict\nimport seaborn as sns\nimport matplotlib.pyplot as plt"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"folder_path = 'your/directory/path'\n\n#plotting params\nmpl.rcParams['figure.figsize'] = (8,5)\nmpl.rcParams['lines.linewidth'] = 3\nplt.style.use('ggplot')"},{"cell_type":"markdown","metadata":{},"source":"# Load ISIS data set and clean tweets"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"df = pd.read_csv(\"../input/tweets.csv\", parse_dates= [6])\ndf.username = df.username.str.lower()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"import re\n \nemoticons_str = r\"\"\"\n    (?:\n        [:=;] # Eyes\n        [oO\\-]? # Nose (optional)\n        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n    )\"\"\"\n \nregex_str = [\n    emoticons_str,\n    r'<[^>]+>', # HTML tags\n    r'(?:@[\\w_]+)', # @-mentions\n    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n \n    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n    r'(?:[\\w_]+)', # other words\n    r'(?:\\S)' # anything else\n]\n    \ntokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\nemoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n \ndef tokenize(s):\n    return tokens_re.findall(s)\n \ndef preprocess(s, lowercase=False):\n    tokens = tokenize(s)\n    if lowercase:\n        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n    return tokens"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"def clean_tweet(tweet):\n    ext = \"http\"\n    text = tweet[:tweet.find(ext)].lower()\n    text = re.sub(\"[^\\S]\", \" \", text)\n    text = re.sub(\"english translation \", \"\", text)\n    textOnly = re.sub(\"[^a-zA-Z0-9@# ]\", \"\", text)\n    return(textOnly)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"def remove_users(tweet):\n    text = tweet.lower()\n    textOnly = re.sub(r\"@\\w+\", \"\", text)\n    return(textOnly)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"df.tweets = df.tweets.apply(clean_tweet)"},{"cell_type":"markdown","metadata":{},"source":"Currently we have occurences of usernames (twitter handles) in all tweets (original tweets and mentions). We only want original tweets so we can determine who is having an influence on who. This does not mean that we should remove all retweets, but rather extract those retweets and treat them as new tweets from new users. \n\n1. We would like to extract every retweet from a given user and reassign that tweet to the first handle in that text string.\n\n    - For example: \"RT @GIIMedia_CH004: Rules Of Imarah Part2 - Conditions of Imamah\" would create@GIIMedia_CH004 as a new user and assign everything after his/her handle in the text string as a tweet for that user.\n \n2. We need to create a dictionary for every user with,\n     - every user they have mentioned (affiliates),\n     - every hashtag they have used and\n     - every tweet they've sent\n\n3. consolidate every tweet for every user into one document and add it to their dictionary"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"infoDict = OrderedDict()\nfor r in df[['username','tweets']].iterrows():\n    match = re.search('^rt', r[1][1])\n    if match:\n        m = list(re.findall(r\"@\\w+\", r[1][1]))\n        if m:\n            username=m[0][1:]\n            tweet=r[1][1][len('rt ' + m[0]):] +' @' + r[1][0]\n    else:\n        username=r[1][0]\n        tweet=r[1][1]\n    if username not in infoDict:\n        user = {}\n        user['affil'] = []\n        user['hashtags'] = []\n        user['tweets'] = []\n        user['doc'] = ''\n        infoDict[username] = user\n    if tweet not in infoDict[username]['tweets']:\n        infoDict[username]['tweets'].append(tweet)\n        infoDict[username]['doc']+=' ' + tweet\n    infoDict[username]['hashtags'].extend(re.findall('(?<=#)\\w+', tweet))\n    infoDict[username]['affil'].extend(re.findall('(?<=@)\\w+', tweet))"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"print('We went from 112 unique users to', len(infoDict), 'users') "},{"cell_type":"markdown","metadata":{},"source":"### Treat all the tweets from one user as a single document and vectorize\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"docs = [remove_users(v['doc']) for k, v in infoDict.items()]"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"Tfidf_vectorizer = TfidfVectorizer(analyzer='word', tokenizer=tokenize,\n                                  ngram_range=(1,1), stop_words='english',\n                                  token_pattern='\\\\b[a-z][a-z]+\\\\b')\ntfidf_docs = Tfidf_vectorizer.fit_transform(docs)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"count_vectorizer = CountVectorizer(analyzer='word',\n                                  ngram_range=(1,3), stop_words='english',\n                                  token_pattern='\\\\b[a-z][a-z]+\\\\b')\ncv_tweets = count_vectorizer.fit_transform(docs)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#treat each tweets seperately\nTfidf_vectorizer = TfidfVectorizer(analyzer='word', tokenizer=tokenize,\n                                  ngram_range=(1,2), stop_words='english',\n                                  token_pattern='\\\\b[a-z][a-z]+\\\\b')\ntfidf_tweets = Tfidf_vectorizer.fit_transform(df.tweets)"},{"cell_type":"markdown","metadata":{},"source":"----"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"from sklearn.utils.extmath import randomized_svd\nU, Sigma, VT = randomized_svd(tfidf_docs, n_components=15,\n                                      n_iter=5,\n                                      random_state=None)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"sigma = []\nfor k,v in enumerate(Sigma):\n    sigma.append((k,v))"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"f = plt.scatter(*zip(*sigma))"},{"cell_type":"markdown","metadata":{},"source":"Based on the sigma values from the resulting matrix we can choose about 6 components to use in our NMF model"},{"cell_type":"markdown","metadata":{},"source":"### NMF decomposition\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"tfidf_docs.shape"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"num_topics = 6\nmodel = NMF(n_components=num_topics, init='random', random_state=0)\nnmf = model.fit_transform(tfidf_docs)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"doc_cluster = [list(r).index(max(r)) for r in nmf]"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"print (doc_cluster[0:20])\nprint (doc_cluster[41:60])\nprint (doc_cluster[61:80])\nprint( doc_cluster[-40:-20])"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"cluster_size = [0,0,0,0,0,0]\nfor v in doc_cluster:\n    cluster_size[v]+=1\ncluster_size"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"data = pd.DataFrame(infoDict).T\ndata.reset_index(inplace=True)\ndata['cluster'] = doc_cluster"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# create dictionary that maps a user to their specific cluster\nuser_docs = {}\nfor k, cluster in enumerate(doc_cluster):\n    user_docs[data['index'][k]] = cluster"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"words = sorted([(i,v) for v,i in Tfidf_vectorizer.vocabulary_.items()])\ntopic_words = []\nfor r in model.components_:\n    a = sorted([(v,i) for i,v in enumerate(r)],reverse=True)[0:7]\n    topic_words.append([words[e[1]] for e in a])"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Create the a list of topic words but only inlude them if they are the highest among all clusters\n# they also need to be weighted appropriately within their cluster\n\n\nword_cluster = [(list(r).index(max(r)),max(r)) for r in model.components_.transpose()]\nfor i,r in enumerate(model.components_.transpose()):\n    s = sorted(r)\n    if (s[0]-s[1])/s[1]<0.25:\n        word_cluster[i] = (-1,-1)\ntopic_words = []\nfor c in range(6):\n    a = sorted([(v[1],i) for i,v in enumerate(word_cluster) if v[0]==c], reverse=True)[0:7]\n    topic_words.append([words[e[1]] for e in a])"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"topic_words"},{"cell_type":"markdown","metadata":{},"source":"---"},{"cell_type":"markdown","metadata":{},"source":"## Creating Graph"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"def is_retweet(tweet):\n    match = re.search('^rt', tweet)\n    if match:\n        return True\n    return False\n\ndef check_string(string):\n    return string in df.username.unique()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# number of retweets\n\ncount=0\nrt = []\nfor tweet in df.tweets:\n    m = re.findall(r\"^rt @\", tweet)\n    if m:\n        count+=1\n        rt.append(tweet)\ncount\n#len(set(rt))"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# create the nodes for the graph. Dictionary keys as usernames \n# with each item in the value list being a connection with number of times mentioned\n# NOTE!! this is for every mentioned user\n\nnodes = defaultdict(str)\nfor K, V in infoDict.items():\n    nodes[K] = [(k, v) for k,v in Counter(list(V['affil'])).items() if k != K]\nfor k, v in list(nodes.items()):\n    if not v:\n        del nodes[k]"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"matches = 0\nfor k, v in list(nodes.items()):\n    if not v:\n        del nodes[k]\n        \n    for i in range(len(v)):\n        if v[i][0] == k:\n            matches+=1\n            v.remove(v[i])"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#create dictionary that maps usernames to a uniqe ID and the cluster they belong to\n# user_docs = a list of documents per user\n\nID = defaultdict()\nfor k, v in enumerate(list(nodes.items())):\n    ID[v[0]] = k, user_docs[v[0]]\n    for i in range(0, len(v[1])):\n        try:\n            ID[v[1][i][0]] = k, user_docs[v[1][i][0]]\n        except:\n            pass"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"df2 = pd.DataFrame(ID).T\n# save the nodes as a csv\n# pd.DataFrame(ID).T.to_csv(folder_path+'/nodes3.csv')"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# create dict with a source, target tied to a weight\nedge_dict = {}\nfor k, v in nodes.items():\n    for i in range(len(v)):\n        try:\n            edge_dict[ID[[k][0]][0], ID[v[i][0]][0]] = v[i][1]\n        except:\n            pass"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# save edges as a csv\n# pd.DataFrame([[k[0],k[1],v] for k, v in edge_dict.items()]).to_csv(folder_path+'/edges3.csv')"},{"cell_type":"markdown","metadata":{},"source":"Output from nodes and edges extraction were saved as csv files.\n\nOutput was then used in a graphing tool called Gephi. \n\nThe results can be viewed at http://app.patricknieto.com"},{"cell_type":"markdown","metadata":{},"source":"---"},{"cell_type":"markdown","metadata":{},"source":"## Time series analysis of all tweets grouped by day"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"from scipy import interpolate\nimport seaborn as sns\n\ndf.time = pd.to_datetime(df.time)\nperhr = df.set_index(df['time']).resample('D', how='count')\npd.rolling_mean(perhr, window=7).tweets['2016-01-01':].plot()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"fig, ax = plt.subplots(figsize = (20,8))\n\nperhr['2016-01-01':].numberstatuses.interpolate(method='linear').plot(ax = ax, color=\"black\", fontsize=12, alpha=0.1)\npd.rolling_mean(perhr, window=7).tweets['2016-01-01':].plot(color ='r')\n\n#sns.timeseries(perhr, ax=ax)\n\nyemen = '2016-01-29'\nbrussels = '2016-03-22'\n\n\nax.annotate('Bombing in Brussels',xy=(brussels, 200),xytext=('2016-03-03', 310),\n            arrowprops=dict(facecolor='white', shrink=0.05), size=15)\n\nax.annotate('Car bombing in Yemen',xy=(yemen, 200),xytext=('2016-01-10', 310),\n            arrowprops=dict(facecolor='white', shrink=0.05),size=15)\n\nax.margins(None,0.1)\nax.legend(['Tweets Per Day','7-Day Rolling Average'], loc = 'upper right',\n           numpoints = 1, labelspacing = 2.0, fontsize = 14)\nax.set_xlabel('Date')\nax.set_ylabel('Number of Tweets')\nax.set_title('Frequency of ISIS Tweets in 2016')\n\n# ax.spines['bottom'].set_color('w')\n# ax.spines['top'].set_color('w') \n# ax.spines['right'].set_color('w')\n# ax.spines['left'].set_color('w')\n\n# ax.tick_params(axis='x', colors='w')\n# ax.tick_params(axis='y', colors='w')\n\n# ax.yaxis.label.set_color('w')\n# ax.xaxis.label.set_color('w')\n\n# ax.set_axis_bgcolor('w')\nfig.savefig('temp.png')\nplt.show()"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}