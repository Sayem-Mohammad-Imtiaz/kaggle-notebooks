{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-08-27T10:48:17.042855Z","iopub.execute_input":"2021-08-27T10:48:17.043202Z","iopub.status.idle":"2021-08-27T10:48:18.1543Z","shell.execute_reply.started":"2021-08-27T10:48:17.043171Z","shell.execute_reply":"2021-08-27T10:48:18.153339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This short kernel is just talks about two things \n- basic way of identifying and handling outlier\n- a very simple attempt at exploring the data. \n\nMy aim is to see how the distribution of output is behaving w.r.t the target variable. There are great kernels out there that gives detailed walkthroughs of modelling, but I just wanted to see if we can get the data talking.","metadata":{}},{"cell_type":"markdown","source":"## **Setup**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/30-days-of-ml/train.csv\")\ndf_test = pd.read_csv(\"../input/30-days-of-ml/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-27T11:55:36.09445Z","iopub.execute_input":"2021-08-27T11:55:36.094793Z","iopub.status.idle":"2021-08-27T11:55:38.047482Z","shell.execute_reply.started":"2021-08-27T11:55:36.094761Z","shell.execute_reply":"2021-08-27T11:55:38.0465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking the target","metadata":{}},{"cell_type":"markdown","source":"Decided to go from the reverse. First step, understanding the target.*Outlier removal*  in the target variable.\n\nThe easiest way to identify outliers is using the Inter Quantile Range. Any data points that lie 1.5 times of IQR above Q3 *(75th percentile)* and below Q1 *(25th percentile)* can be considered as outliers.","metadata":{}},{"cell_type":"code","source":"sns.boxplot(x='target', data=df)\n_ = plt.title('Box plot of target column', fontsize=14)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T12:56:17.354752Z","iopub.execute_input":"2021-08-27T12:56:17.355207Z","iopub.status.idle":"2021-08-27T12:56:17.511118Z","shell.execute_reply.started":"2021-08-27T12:56:17.35516Z","shell.execute_reply":"2021-08-27T12:56:17.510017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The *whiskers* of the plot looks too highlighted. Which means that there are outliers that we need to take care of.","metadata":{}},{"cell_type":"code","source":"percent_25 = df['target'].describe()[\"25%\"]\npercent_75 = df['target'].describe()[\"75%\"]\niqr = percent_75 - percent_25\nlbound = percent_25 - (1.5 * iqr)\nubound = percent_75 + (1.5 * iqr)\n\ndf[\"outliers\"] = df[\"target\"].apply(lambda x: \"yes\" if (x < lbound or x > ubound) else \"no\")\noutliers = df[df[\"outliers\"]==\"yes\"][\"target\"].values","metadata":{"execution":{"iopub.status.busy":"2021-08-27T11:55:38.210301Z","iopub.execute_input":"2021-08-27T11:55:38.210697Z","iopub.status.idle":"2021-08-27T11:55:38.557062Z","shell.execute_reply.started":"2021-08-27T11:55:38.210648Z","shell.execute_reply":"2021-08-27T11:55:38.556137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(x=range(len(outliers)), y=outliers,color=\"olive\")","metadata":{"execution":{"iopub.status.busy":"2021-08-27T11:55:38.558638Z","iopub.execute_input":"2021-08-27T11:55:38.559037Z","iopub.status.idle":"2021-08-27T11:55:38.762022Z","shell.execute_reply.started":"2021-08-27T11:55:38.558996Z","shell.execute_reply":"2021-08-27T11:55:38.760976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like any target value less than 6 and greater/around 10 are outliers. And out of 3L data, these constitute for around 3000 points. There are two easy ways to handle these outliers. \n1. One is to drop the columns pertaining to these rows.\n2. Other is to upper or lower bound them to a certain range. We can round them up to the inter-quartile ranges calculated earlier.  \n\nFor this notebook, I will go with Option-1. **However, it is always better to go with option-2**. Option-2 will ebsure that we will not end up losing out on important input features. This will particularly have a bearing in cases like churn prediction or fraud detection.","metadata":{}},{"cell_type":"markdown","source":"### Out with the outliers.","metadata":{}},{"cell_type":"code","source":"new_df = df[df[\"outliers\"] == \"no\"].reset_index(drop=True)\nnew_df.drop(\"outliers\", axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T11:55:38.763419Z","iopub.execute_input":"2021-08-27T11:55:38.763789Z","iopub.status.idle":"2021-08-27T11:55:38.940963Z","shell.execute_reply.started":"2021-08-27T11:55:38.763749Z","shell.execute_reply":"2021-08-27T11:55:38.940007Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x='target', data=new_df)\n_ = plt.title('Box plot of target column after removing outliers', fontsize=14)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T12:48:11.129141Z","iopub.execute_input":"2021-08-27T12:48:11.129487Z","iopub.status.idle":"2021-08-27T12:48:11.287081Z","shell.execute_reply.started":"2021-08-27T12:48:11.129457Z","shell.execute_reply":"2021-08-27T12:48:11.286046Z"},"jupyter":{"source_hidden":true},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks clean!","metadata":{}},{"cell_type":"markdown","source":"## **Exploring the data**\n\nConsidering I am doing it the reverse way *(target -> data)*,we will try to get some relation between how the features behave with respect to the target. To do this a little differently, I created \"psuedo-target\" by grouping the target into 3-bins.  \n\nThese bins are decided on the data range present in the target variable.","metadata":{}},{"cell_type":"markdown","source":"### Creating the bins  ","metadata":{}},{"cell_type":"code","source":"new_df[\"psuedo_target\"] = pd.cut(new_df['target'], bins=3)\nprint (\"Created bins: \")\nprint(new_df[\"psuedo_target\"].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-08-27T11:55:39.106679Z","iopub.execute_input":"2021-08-27T11:55:39.107068Z","iopub.status.idle":"2021-08-27T11:55:39.127642Z","shell.execute_reply.started":"2021-08-27T11:55:39.10703Z","shell.execute_reply":"2021-08-27T11:55:39.126711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On visualization, we observe that these bins give us good enough spread.","metadata":{}},{"cell_type":"code","source":"sns.histplot(new_df['target'], kde=True, bins=3)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T11:55:40.456405Z","iopub.execute_input":"2021-08-27T11:55:40.456675Z","iopub.status.idle":"2021-08-27T11:55:42.395537Z","shell.execute_reply.started":"2021-08-27T11:55:40.456648Z","shell.execute_reply":"2021-08-27T11:55:42.394513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking the spread ","metadata":{}},{"cell_type":"code","source":"cat_features = [feat for feat in new_df.columns if\n                new_df[feat].nunique() <= 15 and #arbitrarily chosen\n                new_df[feat].dtype == \"object\"]\n\nnum_features = [feat for feat in new_df.columns if\n                new_df[feat].dtype in [\"int64\", \"float64\"] and \n                feat not in ['target','psuedo_target']]\n\nuseful_features = cat_features + num_features","metadata":{"execution":{"iopub.status.busy":"2021-08-27T12:06:07.759634Z","iopub.execute_input":"2021-08-27T12:06:07.759977Z","iopub.status.idle":"2021-08-27T12:06:08.438351Z","shell.execute_reply.started":"2021-08-27T12:06:07.759946Z","shell.execute_reply":"2021-08-27T12:06:08.437448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Checking the spread of categorical variable ","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(5,2, figsize=(18,10))\n\nfor indx,feature in enumerate(cat_features):\n    row = indx // 2\n    col = indx % 2\n    sns.countplot(ax=ax[row, col],x=feature, hue='psuedo_target', data=new_df)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T11:55:43.091164Z","iopub.execute_input":"2021-08-27T11:55:43.091693Z","iopub.status.idle":"2021-08-27T11:55:49.039771Z","shell.execute_reply.started":"2021-08-27T11:55:43.09165Z","shell.execute_reply":"2021-08-27T11:55:49.03868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Checking the spread of numerical variable ","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(5,3, figsize=(18,10))\n\nfor indx,feature in enumerate(num_features):\n    row = indx // 3\n    col = indx % 3\n    sns.kdeplot(ax=ax[row, col],x=feature, hue='psuedo_target', data=new_df)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T11:55:49.041518Z","iopub.execute_input":"2021-08-27T11:55:49.041919Z","iopub.status.idle":"2021-08-27T11:56:17.103927Z","shell.execute_reply.started":"2021-08-27T11:55:49.041864Z","shell.execute_reply":"2021-08-27T11:56:17.103033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unfortunately, no patterns are emerging from the data. The distributions looks really similar between the different target scales. We can, I guess, try one experiment based on this observation - work with 30% of the sample data and check if there are variations in the overall result. ","metadata":{}},{"cell_type":"markdown","source":"## **Getting the feature importance from the model**","metadata":{}},{"cell_type":"markdown","source":"Training a quick model to check if it can yield any insights into important features.","metadata":{}},{"cell_type":"code","source":"from sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor","metadata":{"execution":{"iopub.status.busy":"2021-08-27T12:36:59.485421Z","iopub.execute_input":"2021-08-27T12:36:59.485736Z","iopub.status.idle":"2021-08-27T12:36:59.49194Z","shell.execute_reply.started":"2021-08-27T12:36:59.485707Z","shell.execute_reply":"2021-08-27T12:36:59.491009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/30days-folds/train_folds.csv')\ndf_test = pd.read_csv('../input/30-days-of-ml/test.csv')\nsample_submission = pd.read_csv('../input/30-days-of-ml/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-27T12:36:59.911758Z","iopub.execute_input":"2021-08-27T12:36:59.912127Z","iopub.status.idle":"2021-08-27T12:37:01.749376Z","shell.execute_reply.started":"2021-08-27T12:36:59.912096Z","shell.execute_reply":"2021-08-27T12:37:01.7484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"useful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if col.startswith(\"cat\")]\ndf_test = df_test[useful_features]","metadata":{"execution":{"iopub.status.busy":"2021-08-27T12:37:02.62684Z","iopub.execute_input":"2021-08-27T12:37:02.627182Z","iopub.status.idle":"2021-08-27T12:37:02.654717Z","shell.execute_reply.started":"2021-08-27T12:37:02.627153Z","shell.execute_reply":"2021-08-27T12:37:02.653809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_params = {\n    'random_state': 1, \n    'n_jobs': 4,\n    'booster': 'gbtree',\n    'n_estimators': 1000,\n    'learning_rate': 0.034682894846408095,\n    'reg_lambda': 1.224383455634919,\n    'reg_alpha': 36.043214512614476,\n    'subsample': 0.9219010649982458,\n    'colsample_bytree': 0.11247495917687526,\n    'max_depth': 3,\n    'min_child_weight': 6,\n    'tree_method':'gpu_hist',\n}","metadata":{"execution":{"iopub.status.busy":"2021-08-27T12:41:00.138283Z","iopub.execute_input":"2021-08-27T12:41:00.138608Z","iopub.status.idle":"2021-08-27T12:41:00.144143Z","shell.execute_reply.started":"2021-08-27T12:41:00.138579Z","shell.execute_reply":"2021-08-27T12:41:00.14292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_predictions = []\nscores = []\nfor fold in range(5):\n    xtrain = df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    model= XGBRegressor(**xgb_params)\n    model.fit(\n        xtrain, ytrain,\n        early_stopping_rounds=300,\n        eval_set=[(xvalid, yvalid)], \n        verbose=1000\n    )\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    scores.append(rmse)\n    print(fold, rmse)\n\nprint(np.mean(scores), np.std(scores))","metadata":{"execution":{"iopub.status.busy":"2021-08-27T12:41:22.522119Z","iopub.execute_input":"2021-08-27T12:41:22.522582Z","iopub.status.idle":"2021-08-27T12:42:05.292386Z","shell.execute_reply.started":"2021-08-27T12:41:22.52254Z","shell.execute_reply":"2021-08-27T12:42:05.290946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Getting the feature importance","metadata":{}},{"cell_type":"code","source":"importance = model.feature_importances_\nimportance","metadata":{"execution":{"iopub.status.busy":"2021-08-27T12:42:15.082674Z","iopub.execute_input":"2021-08-27T12:42:15.083009Z","iopub.status.idle":"2021-08-27T12:42:15.201361Z","shell.execute_reply.started":"2021-08-27T12:42:15.082979Z","shell.execute_reply":"2021-08-27T12:42:15.200188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get importance\nimportance = model.feature_importances_\n# summarize feature importance\nfor feature,imp in zip(xtrain.columns, importance):\n    print(f'feature:{feature}, Importance score: {imp:.5f}')","metadata":{"execution":{"iopub.status.busy":"2021-08-27T12:45:10.319902Z","iopub.execute_input":"2021-08-27T12:45:10.320249Z","iopub.status.idle":"2021-08-27T12:45:10.441469Z","shell.execute_reply.started":"2021-08-27T12:45:10.320218Z","shell.execute_reply":"2021-08-27T12:45:10.440548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot feature importance\nsns.barplot(xtrain.columns, importance)\n_ = plt.xticks(rotation=90)\n_ = plt.title(\"Feature Importance Chart\")","metadata":{"execution":{"iopub.status.busy":"2021-08-27T12:47:53.892749Z","iopub.execute_input":"2021-08-27T12:47:53.893132Z","iopub.status.idle":"2021-08-27T12:47:54.275805Z","shell.execute_reply.started":"2021-08-27T12:47:53.893101Z","shell.execute_reply":"2021-08-27T12:47:54.274767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this small experiment, looks like there is no clear winner. Perhaps we can re-iterate the model by removing the features that contibute less than 0.02% to the overall result. Might give a bump to the score.","metadata":{}},{"cell_type":"markdown","source":"## **Conclusion**","metadata":{}},{"cell_type":"markdown","source":"This is quite an intriguing competition since there is no direct, visible patter that helps in predicting the targe variables. But then again, it is a wonderful opportunity to play around and underestand different modeling techniques like ensembling, stacking and blending. Excellently and patiently covered by one and only Abhishek Thakur in this playlist [here](https://www.youtube.com/watch?v=_55G24aghPY&list=PL98nY_tJQXZnP-k3qCDd1hljVSciDV9_N).","metadata":{}}]}