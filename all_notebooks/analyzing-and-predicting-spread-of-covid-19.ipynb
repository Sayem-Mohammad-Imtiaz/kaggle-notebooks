{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Capstone Project\n### Applied Data Science by IBM\n### Michael Lynch\n### April 12, 2020"},{"metadata":{},"cell_type":"markdown","source":"## Table of contents\n* [Introduction: Business Problem](#Introduction:-Business-Problem)\n* [Data](#Data)\n* [Methodology](#Methodology)\n* [Analysis](#Analysis)\n* [Results and Discussion](#Results-and-Discussion)\n* [Conclusion](#Conclusion)"},{"metadata":{},"cell_type":"markdown","source":"# Introduction: Business Problem"},{"metadata":{},"cell_type":"markdown","source":"In this project we will analyze the spread of COVID-19 over time and space. We will also try to predict to increase in number of cases into the near future. This is important because it will help others see trends in the spread and will help myself learn python and complete the CourseEra Capstone course for IBM data science."},{"metadata":{},"cell_type":"markdown","source":"# Data"},{"metadata":{},"cell_type":"markdown","source":"There is plentiful data for COVID-19 stats. Datasets can be found on GitHub and Kaggle.com. In this project we will utilze three datasets. The first dataset will be from covidtracking.com and it contains the data we will need to plot the rise in covid cases over time as well as predict the rise in cases into the near future. The second dataset is from John Hopkins' GitHub repository. This dataset contains covid-19 stats for each county in the USA which. The third dataset is from the NYTimes GitHub repository and is similar to the second dataset, except it doesn't come with the geospatial coordinates for each county. We will utilize a Python library called geocoder to get thsi information. Finally, we use the third dataset to make a heatmap with of covid cases that changes with each day so that we can visualize the spread of covid cases over time and space."},{"metadata":{},"cell_type":"markdown","source":"# Methodology"},{"metadata":{},"cell_type":"markdown","source":"We will use the SVM machine learning algorithm to predict the increase in number of cases for each state in the USA. To find the best hyperparameters for the algorithm we will use a speacialized function in ski-kit-learn. We will see that the function did a good job in predicting the best hyperparameters by plotting the error with respect to one of the hyperparameters and see the error is lowest with the chosen hyperparameter. We will also use k-means clustering to find regions that are most concentrated with cases of COVID-19. We would expect regions high in population to be highest in cases of COVID-19."},{"metadata":{},"cell_type":"markdown","source":"# Analysis"},{"metadata":{},"cell_type":"markdown","source":"### First, we import all nessesary libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport folium\nfrom sklearn.cluster import KMeans\n!pip install geocoder\nimport geocoder\nimport json\nimport time\nimport datetime\n!pip install wget\nimport wget\n\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Our first dataset is downloaded from https://covidtracking.com/. Note that the dataset is updated each day. In this first analysis we will plot and predict the number of cases in a specific state, then will will use for loops to do the same for all states."},{"metadata":{},"cell_type":"markdown","source":"### Try to open the dataset in your current folder if you already have it saved. If it's not there, download and use the one from the website."},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    with open('daily.csv') as us_states_covid19_daily:\n        df=pd.read_csv(us_states_covid19_daily)\nexcept IOError: \n    url='https://covidtracking.com/api/v1/states/daily.csv'\n    df=pd.read_csv(url)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sort by state primarily and date secondarly"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df.sort_values(['state','date'],inplace=True,ascending=(True,False))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Make a list of all states. Note that this list actually contains territories to. We will make a revised list fo states later."},{"metadata":{"trusted":true},"cell_type":"code","source":"states=df['state'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Delete Columns that don't have useful data that we care about"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df.drop(['dateChecked','hash','pending','negative','deathIncrease','hospitalizedIncrease','negativeIncrease','positiveIncrease','totalTestResultsIncrease','fips','total','totalTestResults','posNeg'],axis=1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The below cell splits the dataset into len(states) number of datasets, one for each state"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_dict={}\nfor i in range(len(states)):\n    df_dict[states[i]]=[]\n    for j in range(len(df)):\n        if df['state'].iloc[j]==states[i]:\n            df_dict[states[i]].append(df.iloc[j])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(df_dict)):\n    df_dict[states[i]]=pd.DataFrame(df_dict[states[i]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confirm that worked for a specific state"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_dict['MN']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Get a list of all the dates included in the dataset so that we can use it as our x-axis in some plots. Convert the dates to number of days since the first date. Also cast the data into a numpy array and reshape so that it is in the correct format for feeding into the SVM algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"dates_mn=df_dict['MN']['date']\ndates_mn=pd.to_datetime(dates_mn, format='%Y%m%d')\nday_num_mn=[i for i in range(len(dates_mn))]\nday_num_mn=np.array(day_num_mn).reshape(-1,1)\n#mn_dates=pd.Index(df_dict['MN']['date'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cases_mn=np.array(df_dict['MN']['positive'].tolist())\ncases_mn=np.flip(cases_mn).reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"plt.plot(day_num_mn,cases_mn)\nplt.title('Number of People Tested Positive for COVID-19 in MN')\nplt.xlabel('Date')\nplt.xticks(rotation=90)\nplt.ylabel('Number of People')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define the number of days into the future we want to SVM algorithm to predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"days_in_future = 10\nfuture_forcast_mn = np.array([i for i in range(len(dates_mn)+days_in_future)]).reshape(-1, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split data into training and testing sets"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"X_train_mn, X_test_mn, y_train_mn, y_test_mn = train_test_split(day_num_mn, cases_mn, test_size=0.1, shuffle=False) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The following lines of code I altered from: https://www.kaggle.com/therealcyberlord/coronavirus-covid-19-visualization-prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# use this to find the optimal parameters for SVR\n#c = [0.01, 0.1, 1]\n#gamma = [0.01,0.015,0.02] #I had to alter this line because my system would crash with larger values\n#epsilon = [0.01, 0.1, 1]\n#shrinking = [True, False]\n#degree = [3, 4, 5, 6, 7]\n#\n#svm_grid = {'C': c, 'gamma' : gamma, 'epsilon': epsilon, 'shrinking' : shrinking, 'degree': degree}\n#\n#svm = SVR(kernel='poly')\n#svm_search = RandomizedSearchCV(svm, svm_grid, scoring='neg_mean_squared_error', cv=3, return_train_score=True, n_jobs=-1, n_iter=30, verbose=1)\n#svm_search.fit(X_train_mn, np.ravel(y_train_mn))\n#\n#svm_search.best_params_\n\n#svm_confirmed_mn = svm_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### After finding the best parameters I kept them so I didn't always have to re-run teh above cell"},{"metadata":{"trusted":true},"cell_type":"code","source":"#svm_confirmed_mn = svm_search.best_estimator_\nsvm_confirmed_mn = SVR(shrinking=False, kernel='poly',gamma=0.01, epsilon=0.1,degree=3, C=1)\nsvm_confirmed_mn.fit(X_train_mn, np.ravel(y_train_mn))\nsvm_pred_mn = svm_confirmed_mn.predict(future_forcast_mn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Get the Mean Absolute Error for degrees 1 thru 9 so that we can make a plot of that"},{"metadata":{"trusted":true},"cell_type":"code","source":"MAE=[]\nfor i in range(1,10):\n    svm_confirmed_degree_test = SVR(shrinking=False, kernel='poly',gamma=0.01, epsilon=0.1,degree=i, C=1)\n    svm_confirmed_degree_test.fit(X_train_mn, np.ravel(y_train_mn))\n    svm_pred_degree_test = svm_confirmed_degree_test.predict(future_forcast_mn)\n    svm_test_pred_degree_test = svm_confirmed_degree_test.predict(X_test_mn)\n    MAE.append(mean_absolute_error(svm_test_pred_degree_test, y_test_mn))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Make a plot of degree vs. error to confirm that are hyperparameters are well chosen."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"degree_num=list(range(1,10))\nplt.plot(degree_num,MAE)\nplt.title('Tuning Degree Parameter')\nplt.ylabel('SVM Error')\nplt.xlabel('Degree Number')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot the predicted number of cases on top of the actual number of cases"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"plt.plot(day_num_mn, cases_mn)\nplt.plot(future_forcast_mn, svm_pred_mn, linestyle='dashed', color='purple')\nplt.title('Predicted Increase in Number of Cases')\nplt.legend(['Confirmed Cases','SVM Prediction'])\nplt.xlabel('Day')\nplt.ylabel('Number of Confirmed Cases')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Get rid of the elements in the data that are not states. This will make our subplot below look better because we can shape it as 5x10, which we can do with 50 elements, but not 56."},{"metadata":{"trusted":true},"cell_type":"code","source":"not_states=['AS','GU','DC','PR','MP','VI']\nfor i in range(len(not_states)):\n    del df_dict[not_states[i]]\nkey_list=list(df_dict.keys())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Below we make a list of all states and repeat the process we did above on MN for all states using a lot of for loops."},{"metadata":{"trusted":true},"cell_type":"code","source":"states_list=[]\nfor i in range(len(states)):\n    if states[i] not in not_states:\n        states_list.append(states[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dates=[]\nfor i in range(len(states_list)):\n    dates.append(pd.Index(df_dict[states_list[i]]['date'].unique()))\nday_num=[]\nfor i in range(len(dates)):\n    day_num.append([j for j in range(len(dates[i]))])\n    day_num[i]=np.array(day_num[i]).reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"day_num_length_list=[]\nfor i in range(len(day_num)):\n    day_num_length_list.append(len(day_num[i]))\n    \nmin_day_num=min(day_num_length_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cases=[]\nfor i in range(len(df_dict)):\n    cases.append(np.array(df_dict[states_list[i]]['positive'].tolist()))\n    cases[i]=np.flip(cases[i]).reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#cases_us=[]\n#for day in list(df['date'].unique()):\n#    cases_us.append(df[df['date']==day]['positive'].sum())\n#cases_us=np.array(cases_us)\n#cases_us=np.flip(cases_us).reshape(-1,1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"days_in_future=10\nfuture_forcast=[]\nfor i in range(len(dates)):\n    future_forcast.append(np.array([j for j in range(len(dates[i])+days_in_future)]).reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_confirmed = svm_confirmed_mn\n\nX_train=[0 for i in range(len(cases))]\nX_test=[0 for i in range(len(cases))]\ny_train=[0 for i in range(len(cases))]\ny_test=[0 for i in range(len(cases))]\nfor i in range(len(cases)):\n    X_train[i], X_test[i], y_train[i], y_test[i] = train_test_split(day_num[i], cases[i], test_size=0.1, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_pred=[0 for i in range(len(cases))]\nfor i in range(len(cases)):\n        svm_confirmed.fit(X_train[i], np.ravel(y_train[i]))\n        svm_pred[i] = svm_confirmed.predict(future_forcast[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Make a plot of the number of cases vs time for each of the fifty states of the US."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8),dpi=100)\nfor i in range(50):\n    plt.subplot(10,5,i+1)    \n    plt.plot(day_num[i],cases[i])\n    plt.title(key_list[i],loc='center',pad=-10,fontsize=10)\n    plt.tick_params(\n    axis='x',          # changes apply to the x-axis\n    which='both',      # both major and minor ticks are affected\n    bottom=False,      # ticks along the bottom edge are off\n    top=False,         # ticks along the top edge are off\n    labelbottom=False)\n    plt.tick_params(\n    axis='y',          # changes apply to the x-axis\n    which='both',      # both major and minor ticks are affected  \n    direction='in',\n    labelsize=5,\n    pad=-20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Make a plot of the number of predicted cases for each state on top of the actual data for each state"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8),dpi=100)\nfor i in range(50):\n    plt.subplot(10,5,i+1)    \n    plt.plot(day_num[i],cases[i])\n    plt.plot(future_forcast[i], svm_pred[i], linestyle='dashed', color='purple')\n    plt.title(states_list[i],loc='center',pad=-10,fontsize=10)\n    plt.legend(['Actual','Predicted'],fontsize=3)\n    plt.tick_params(\n    axis='x',          # changes apply to the x-axis\n    which='both',      # both major and minor ticks are affected\n    bottom=False,      # ticks along the bottom edge are off\n    top=False,         # ticks along the top edge are off\n    labelbottom=False)\n    plt.tick_params(\n    axis='y',         \n    which='both',      \n    direction='in',\n    labelsize=5,\n    pad=-20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now let's make dataframe with only the cumulative data from the most recent date of each state. That way we can see how the virus is spread over space."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cum=df.sort_index()\ndf_cum.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Retrieve only the first 56 rows because that is today's date and drop all unnessary columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cum=df_cum.iloc[0:56]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get rid of rows that are us territories, not states: AS, GU, DC, PR, MP, VI\ndrop_ind=[3,8,12,27,42,50]\ndf_cum.drop(index=drop_ind,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cum.reset_index(drop=True,inplace=True)\n#df_cum['positive'].astype('int',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### I had to create the following list with the states organized alphabetically by their two letter designation"},{"metadata":{"trusted":true},"cell_type":"code","source":"state_names=['Alaska','Alabama','Arkansas','Arizona','California','Colorado', 'Connecticut', 'Delaware', 'Florida', 'Georgia', 'Hawaii', 'Iowa', 'Indiana','Idaho', 'Illinois', 'Kansas', 'Kentucky', 'Louisiana', 'Massachusetts', 'Maryland','Maine','Michigan', 'Minnesota', 'Missouri','Mississippi', 'Montana', 'North Carolina', 'North Dakota','Nebraska', 'New Hampshire','New Jersey','New Mexico','Nevada','New York', 'Ohio','Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina','South Dakota', 'Tennessee', 'Texas', 'Utah','Virginia','Vermont', 'Washington', 'Wisconsin','West Virginia', 'Wyoming']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cum['State_Names']=pd.DataFrame(state_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cum=df_cum[['date','state','State_Names','positive','hospitalizedCurrently','inIcuCurrently','inIcuCumulative','onVentilatorCurrently','onVentilatorCumulative','recovered','death','hospitalized']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's view the whole dataset to confirm the two letter designation matches with the full state name"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_cum","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Import a json file containing the state boundaries so we can use it for a choropleth map. If it's not in your current working directory download it from GitHub using the wget library. State boundaries json downloaded from https://github.com/PublicaMundi/MappingAPI/blob/master/data/geojson/us-states.json?short_path=1c1ebe5"},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    with open('us-states.json') as jsonfile:\n        json_path=jsonfile.name\nexcept IOError:\n    #import wget\n    us_states_json_url='https://raw.githubusercontent.com/PublicaMundi/MappingAPI/master/data/geojson/us-states.json'\n    us_states_json=wget.download(us_states_json_url)\n    json_path=us_states_json","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"us_map = folium.Map(location=[37,-96], zoom_start=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We bin the data so that the colors on the chororpleth map are nicely spread"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"bins = list(df_cum['positive'].quantile([0, 0.25, 0.5, 0.75, 1]))\nbins","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Make a choropleth map of the number of cases in each state"},{"metadata":{"trusted":true},"cell_type":"code","source":"folium.Choropleth(\n    geo_data=json_path,\n    data=df_cum,\n    columns=['State_Names', 'positive'],\n    key_on='feature.properties.name',\n    fill_color='YlGn', \n    fill_opacity=0.7, \n    line_opacity=0.2,\n    legend_name='COVID-19',\n    bins=bins,\n    reset=True\n).add_to(us_map)\n\n\n# display map\nus_map","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset 2: Next we will import a database of all COVID-19 cases in each county of the US so that we can use it to to run k-means clustering to map the areas of the country that are dense in COVID-19 cases. Dataset from John Hopkins GitHub Repository: https://github.com/CSSEGISandData/COVID-19"},{"metadata":{},"cell_type":"markdown","source":"### Similar code to what we did above with importing the first dataset"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"try:\n    with open('time_series_covid19_confirmed_US.csv') as timeseries:\n        df_counties=pd.read_csv(timeseries)\nexcept IOError:\n    counties_url='https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv'\n    df_counties=pd.read_csv(counties_url)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_counties.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Delete the columns we don't care about"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_counties=df_counties.drop(['UID','iso2','iso3','code3','FIPS'],axis=1)\ndf_counties.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Make a dataframe with just latitude, longitude, and number of cases"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"most_recent_day='4/11/20'\ndf_counties_small=df_counties[['Lat','Long_',most_recent_day]]\ndf_counties_small.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Setup our kmean parameters and run kmeans to get the centers where cases are the most dense"},{"metadata":{"trusted":true},"cell_type":"code","source":"kclusters=100\nkmeans=KMeans(n_clusters=kclusters,random_state=0).fit(df_counties_small)\ncenters=kmeans.cluster_centers_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lats=pd.Series(centers[:,0])\nlongs=pd.Series(centers[:,1])\nweights=centers[:,2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now we make the map of the cluster centers"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"cluster_map=folium.Map(location=[37,-96], zoom_start=4)\nfor lat,long in zip(lats,longs):\n    folium.CircleMarker(\n            [lat,long],\n            radius=5).add_to(cluster_map)\n\ncluster_map","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The clusters seemed to be centered around areas that have large populations, which makes sense."},{"metadata":{},"cell_type":"markdown","source":"##  Dataset 3: Let's use another dataset which doesn't explictly have the latitudes and longitudes for each county. We can use geocoding to get the latitude and longitude for each county if we aren't explictly given it. The dataset we will use is found in the NY Times GitHub Repository here: https://github.com/nytimes/covid-19-data"},{"metadata":{},"cell_type":"markdown","source":"### Import the data using similar code we did for the first and second datasets"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"try:\n    with open('us-counties.csv') as uscounties:\n        df_counties2=pd.read_csv(uscounties)\nexcept IOError:\n    counties2_url='https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv'\n    df_counties2=pd.read_csv(counties2_url)\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"df_counties2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We make a column of the combined name of county and state. If the county name is unknown this causes problems with the geocoding API. So that we don't get that problem we don't include the county name there."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"for i in range(len(df_counties2)):\n    if df_counties2['county'][i]!='Unknown':\n        df_counties2.at[i,'county, state']=df_counties2['county'][i]+', '+df_counties2['state'][i]\n    elif df_counties2['county'][i]=='Unknown':\n        df_counties2.at[i,'county, state']=df_counties2['state'][i]\ndf_counties2.tail()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"county_state_list=list(df_counties2['county, state'].unique())\ncounty_state_list\nlen(county_state_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **The next few cells take a long time because they send many calls to the API. Be aware of the API usage restrictions, quotas, and limits. To bypass this I split the API calls up into 3 chunks and ran each chunk separatly. I saved the data after running each chunk. If you have problems running this part you can just download the file from my github, but I can't promise it will be up to date: https://github.com/mjlynch91/Coursera_Capstone/blob/master/county_lat_lng.txt"},{"metadata":{},"cell_type":"markdown","source":"### You really have to be careful which cells you run the in next few cells depending on whether you are downloading the data, or just loading the data from you local disk."},{"metadata":{"trusted":true},"cell_type":"code","source":"API_KEY='Put Your Api key here'\n#g_total=[]\n#for name in county_state_list[0:1000]:\n#    g_current = geocoder.google(name,key=API_KEY)\n#    g_total.append(g_current)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for name in county_state_list[1000:2000]:\n#    g_current = geocoder.google(name,key=API_KEY)\n#    g_total.append(g_current)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#for name in county_state_list[2573:len(county_state_list)]:\n#    g_current = geocoder.google(name,key=API_KEY)\n#    g_total.append(g_current)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### So that we don't have to run the above cell everytime, I've saved the data to a file"},{"metadata":{"trusted":true},"cell_type":"code","source":"#g_dict={}\n#g_dict['counties']=[]\n#for i in range(len(g_total)):\n #   if type(g_total[i])==type(g_current):\n  #      g_dict['counties'].append(g_total[i].json)\n   # else:\n    #    g_dict['counties'].append(g_total[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#with open('county_lat_lng.txt', 'w') as outfile:\n #   json.dump(g_dict, outfile)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#for i in range(len(g_total)):\n #   if type(g_total[i])!=dict:\n  #      print(i,type(g_total[i]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Read the file from your local disk if you already have it."},{"metadata":{"trusted":true},"cell_type":"code","source":"jsonpath='../input/county-lat-lng-json/county_lat_lng.json'\nwith open(jsonpath,\"r\") as json_file:\n    g_dict = json.load(json_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g_total=g_dict['counties']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now that we have the coordinates, we put them into the dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"# g_current = geocoder.google('Minneasota',key=API_KEY)\ng_current=0\nlat_long=[]\nfor i in range(len(g_total)):\n    if type(g_total[i])==type(g_current):\n        lat_long.append([g_total[i].lat, g_total[i].lng])\n    else:\n        lat_long.append([g_total[i]['lat'], g_total[i]['lng']])\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#changing lat_long to a numpy array since it's easier to get columns and rows out\nlat_long=np.array([[i[0] for i in lat_long],[i[1] for i in lat_long]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"county_dict={}\nfor i in range(len(county_state_list)):\n    county_dict.update({county_state_list[i]:tuple(lat_long[:,i])})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lat_series=[]\nlong_series=[]\nfor i in range(len(df_counties2)):\n    lat_series.append(county_dict[df_counties2['county, state'][i]][0])\n    long_series.append(county_dict[df_counties2['county, state'][i]][1])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"df_counties2['latitude']=lat_series\ndf_counties2['longitude']=long_series\ndf_counties2.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now we map a heatmap of all the cases over the entire US. This is a different method to show which areas are most dense in virus cases"},{"metadata":{"trusted":true},"cell_type":"code","source":"most_recent_day2='2020-04-11'\ndf_counties_curr=df_counties2[df_counties2['date']==most_recent_day2]\ndf_counties_curr.head()\nheatmap_list=df_counties_curr[['latitude','longitude','cases']].values.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heatmap_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from folium.plugins import HeatMap\nbase_map=folium.Map(location=[37,-96], zoom_start=4)\nHeatMap(data=heatmap_list,radius=10).add_to(base_map)\nbase_map    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Finally we visualize the spread of COVID-19 over both TIME AND SPACE. Let's extend our heatmap list in the time dimension so we can watch it's evolution"},{"metadata":{"trusted":true},"cell_type":"code","source":"heatmap_list_time=[]\nfor date in df_counties2['date'].unique():\n    df_counties_curr=df_counties2[df_counties2['date']==date]\n    heatmap_list_time.append(df_counties_curr[['latitude','longitude','cases']].values.tolist())","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"from folium.plugins import HeatMapWithTime\nbase_map=folium.Map(location=[37,-96], zoom_start=4)\nHeatMapWithTime(data=heatmap_list_time, radius=10,auto_play=True).add_to(base_map)\nbase_map","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results and Discussion"},{"metadata":{},"cell_type":"markdown","source":"It is clear from the above analysis that the number of coronavirus cases will continue to increase in every state of the US. It is also clear the areas with larger populations have a larger number of cases, which is what would be expected, unless drastic quarantine or stay-at-home orders were issued in those areas."},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"Using Python and various libraries for Python I have analyzed and shown visualization of the spread of coronavirus over the US in both space and time. It is clear from the above analysis that the situation is serious, and the spread isn’t stopping anytime soon. "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"}},"nbformat":4,"nbformat_minor":4}