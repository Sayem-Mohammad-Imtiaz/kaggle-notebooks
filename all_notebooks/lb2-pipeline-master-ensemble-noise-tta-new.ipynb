{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Notes","metadata":{"papermill":{"duration":0.04953,"end_time":"2021-02-16T15:59:39.560117","exception":false,"start_time":"2021-02-16T15:59:39.510587","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"ikMF-nQSdTbF","outputId":"041c6767-86ab-46e4-9b22-a4840121ecca","papermill":{"duration":1.301859,"end_time":"2021-02-16T15:59:41.363841","exception":false,"start_time":"2021-02-16T15:59:40.061982","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-27T10:40:54.246178Z","iopub.execute_input":"2021-05-27T10:40:54.246608Z","iopub.status.idle":"2021-05-27T10:40:54.998001Z","shell.execute_reply.started":"2021-05-27T10:40:54.24651Z","shell.execute_reply":"2021-05-27T10:40:54.996983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nimport numpy as np\nimport librosa as lb\nimport soundfile as sf\nimport pandas as pd\nimport cv2\nfrom pathlib import Path\nimport re\n\nimport torch\nfrom torch import nn\nfrom  torch.utils.data import Dataset, DataLoader\n\n\nimport sys\nsys.path.append(\"../input/timm-pytorch-image-models/pytorch-image-models-master\")\nsys.path.append(\"../input/resnest50-fast-package/resnest-0.0.6b20200701/resnest\")\nsys.path.append(\"../input/audiomentations/audiomentations-0.15.0-py3-none-any.whl\")\nsys.path.append(\"../input/evaluations\")\nsys.path.append(\"../input/torchlibrosa/torchlibrosa-0.0.5-py3-none-any.whl\")\n\n\n\n\n\nfrom tqdm.notebook import tqdm\n\nimport time\nfrom resnest.torch import resnest50\nimport timm\n\n\nimport torchaudio\nimport torchaudio.functional as F\nimport torchaudio.transforms as T\nfrom albumentations.core.transforms_interface import ImageOnlyTransform\nfrom audiomentations import Compose,ClippingDistortion\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"kSCcqxf0c_O7","papermill":{"duration":4.153186,"end_time":"2021-02-16T15:59:57.894303","exception":false,"start_time":"2021-02-16T15:59:53.741117","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-27T10:40:55.000415Z","iopub.execute_input":"2021-05-27T10:40:55.000758Z","iopub.status.idle":"2021-05-27T10:41:02.168553Z","shell.execute_reply.started":"2021-05-27T10:40:55.000723Z","shell.execute_reply":"2021-05-27T10:41:02.167516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from evaluations.kaggle_2020 import *","metadata":{"papermill":{"duration":0.046502,"end_time":"2021-02-16T15:59:59.284533","exception":false,"start_time":"2021-02-16T15:59:59.238031","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-27T10:41:02.169859Z","iopub.execute_input":"2021-05-27T10:41:02.17018Z","iopub.status.idle":"2021-05-27T10:41:02.24117Z","shell.execute_reply.started":"2021-05-27T10:41:02.170151Z","shell.execute_reply":"2021-05-27T10:41:02.240074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configs","metadata":{"papermill":{"duration":0.046996,"end_time":"2021-02-16T15:59:59.377834","exception":false,"start_time":"2021-02-16T15:59:59.330838","status":"completed"},"tags":[]}},{"cell_type":"code","source":"NUM_CLASSES = 397\nSR = 32_000\nDURATION = 5\n# THRESH = 0.65\n\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"DEVICE:\", DEVICE)\n\nTEST_AUDIO_ROOT = Path(\"../input/birdclef-2021/test_soundscapes\")\nSAMPLE_SUB_PATH = \"../input/birdclef-2021/sample_submission.csv\"\nTARGET_PATH = None\n    \nif not len(list(TEST_AUDIO_ROOT.glob(\"*.ogg\"))):\n    TEST_AUDIO_ROOT = Path(\"../input/birdclef-2021/train_soundscapes\")\n    SAMPLE_SUB_PATH = None\n    # SAMPLE_SUB_PATH = \"../input/birdclef-2021/sample_submission.csv\"\n    TARGET_PATH = Path(\"../input/birdclef-2021/train_soundscape_labels.csv\")","metadata":{"id":"rJhYZVIDc_O9","papermill":{"duration":0.440614,"end_time":"2021-02-16T15:59:59.86408","exception":false,"start_time":"2021-02-16T15:59:59.423466","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-27T10:41:02.242758Z","iopub.execute_input":"2021-05-27T10:41:02.243194Z","iopub.status.idle":"2021-05-27T10:41:02.267855Z","shell.execute_reply.started":"2021-05-27T10:41:02.243149Z","shell.execute_reply":"2021-05-27T10:41:02.266913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import timm\n\nclass CustomModel(nn.Module):\n    def __init__(self, model_name='efficientnet_b0', pretrained=False):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained)\n        \n                ## rexnet family\n        if hasattr(self.model, \"head\"):\n            nb_ft = self.model.head.fc.in_features  # 1280\n            self.model.head.fc = nn.Identity()\n            # self.model.head.fc = nn.Linear(nb_ft, NUM_CLASSES)\n            self.model.head.fc = nn.Sequential(nn.Linear(nb_ft, 512),\n                                               nn.SiLU(),\n                                               nn.Dropout(0.3),\n                                               nn.Linear(512, NUM_CLASSES))\n        \n        \n        if hasattr(self.model, \"fc\"):\n            nb_ft = self.model.fc.in_features\n#             print()\n            self.model.fc = nn.Identity()\n            self.model.fc = nn.Sequential( \n                nn.Linear(nb_ft, 1024), nn.ReLU(), nn.Dropout(p=0.2),\n                nn.Linear(1024, 1024), nn.ReLU(), nn.Dropout(p=0.2),\n                nn.Linear(1024, NUM_CLASSES)\n            )\n            \n#             self.model.fc = nn.Linear(nb_ft, NUM_CLASSES)\n        elif hasattr(self.model, \"_fc\"):\n            nb_ft = self.model._fc.in_features\n            self.model._fc = nn.Linear(nb_ft, NUM_CLASSES)\n        elif hasattr(self.model, \"classifier\"):\n            nb_ft = self.model.classifier.in_features\n            self.model.classifier = nn.Linear(nb_ft, NUM_CLASSES)\n        elif hasattr(self.model, \"last_linear\"):\n            nb_ft = self.model.last_linear.in_features\n            self.model.last_linear = nn.Linear(nb_ft, NUM_CLASSES)\n        \n    def forward(self, x):\n        x = self.model(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-05-27T10:41:02.270878Z","iopub.execute_input":"2021-05-27T10:41:02.271224Z","iopub.status.idle":"2021-05-27T10:41:02.283299Z","shell.execute_reply.started":"2021-05-27T10:41:02.271192Z","shell.execute_reply":"2021-05-27T10:41:02.282443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{"papermill":{"duration":0.046026,"end_time":"2021-02-16T16:00:00.054107","exception":false,"start_time":"2021-02-16T16:00:00.008081","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class MelSpecComputer:\n    def __init__(self, sr, n_mels, fmin, fmax, **kwargs):\n        self.sr = sr\n        self.n_mels = n_mels\n        self.fmin = fmin\n        self.fmax = fmax\n        kwargs[\"n_fft\"] = kwargs.get(\"n_fft\", self.sr//10)\n        kwargs[\"hop_length\"] = kwargs.get(\"hop_length\", self.sr//(10*4))\n        self.kwargs = kwargs\n\n    def __call__(self, y):\n\n        melspec = lb.feature.melspectrogram(\n            y, sr=self.sr, n_mels=self.n_mels, fmin=self.fmin, fmax=self.fmax, **self.kwargs,\n        )\n\n        melspec = lb.power_to_db(melspec).astype(np.float32)\n        \n        return melspec","metadata":{"id":"4fwNhdbJc_O-","papermill":{"duration":0.056975,"end_time":"2021-02-16T16:00:00.157168","exception":false,"start_time":"2021-02-16T16:00:00.100193","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-27T10:41:02.286089Z","iopub.execute_input":"2021-05-27T10:41:02.286377Z","iopub.status.idle":"2021-05-27T10:41:02.305639Z","shell.execute_reply.started":"2021-05-27T10:41:02.286349Z","shell.execute_reply":"2021-05-27T10:41:02.304334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def mono_to_color(X, eps=1e-6, mean=None, std=None):\n#     mean = mean or X.mean()\n#     std = std or X.std()\n#     X = (X - mean) / (std + eps)\n    \n#     _min, _max = X.min(), X.max()\n\n#     if (_max - _min) > eps:\n#         V = np.clip(X, _min, _max)\n#         V = 255 * (V - _min) / (_max - _min)\n#         V = V.astype(np.uint8)\n#     else:\n#         V = np.zeros_like(X, dtype=np.uint8)\n\n#     return V\n\ndef crop_or_pad(y, length):\n    if len(y) < length:\n        y = np.concatenate([y, length - np.zeros(len(y))])\n    elif len(y) > length:\n        y = y[:length]\n    return y","metadata":{"id":"Nk0haSTIc_O_","papermill":{"duration":0.065406,"end_time":"2021-02-16T16:00:00.271104","exception":false,"start_time":"2021-02-16T16:00:00.205698","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-27T10:41:02.307315Z","iopub.execute_input":"2021-05-27T10:41:02.307726Z","iopub.status.idle":"2021-05-27T10:41:02.327078Z","shell.execute_reply.started":"2021-05-27T10:41:02.30768Z","shell.execute_reply":"2021-05-27T10:41:02.32602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import transforms\n\n\ndef mono_to_color(X: np.ndarray, height, width, mean=0.5, std=0.5, eps=1e-6):\n    trans = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize([height, width]), transforms.ToTensor(),\n        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]\n    )\n    \n    X = np.stack([X, X, X], axis=-1)\n    V = (255 * X).astype(np.uint8)\n    V = (trans(V)+1)/2\n    return V","metadata":{"execution":{"iopub.status.busy":"2021-05-27T10:41:02.328351Z","iopub.execute_input":"2021-05-27T10:41:02.328752Z","iopub.status.idle":"2021-05-27T10:41:02.342423Z","shell.execute_reply.started":"2021-05-27T10:41:02.32871Z","shell.execute_reply":"2021-05-27T10:41:02.341355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def random_power(images, power = 1.5, c= 0.7):\n    images = images - images.min()\n    images = images/(images.max()+0.0000001)\n    images = images**(random.random()*power + c)\n    return images\n\n\nclass BirdCLEFDataset(Dataset):\n    def __init__(self, data, wh=(128,201), sr=SR, n_mels=128, fmin=0, fmax=None, duration=DURATION, step=None, res_type=\"kaiser_fast\", resample=True, tta=True):\n        \n        self.data = data\n        \n        self.sr = sr\n        self.n_mels = n_mels\n        self.fmin = fmin\n        self.fmax = fmax or self.sr//2\n\n        self.duration = duration\n        self.audio_length = self.duration*self.sr\n        self.step = step or self.audio_length\n        \n        self.res_type = res_type\n        self.resample = resample\n        self.wh = wh\n        self.tta = tta\n\n        self.mel_spec_computer = MelSpecComputer(sr=self.sr, n_mels=self.n_mels, fmin=self.fmin,\n                                                 fmax=self.fmax)\n    def __len__(self):\n        return len(self.data)\n    \n    @staticmethod\n    def normalize(image):\n        image = image.astype(\"float32\", copy=False) / 255.0\n        image = np.stack([image, image, image])\n        return image\n    \n    def audio_to_image(self, audio):\n        image = self.mel_spec_computer(audio) \n        image = (image+80)/80\n        \n        image = random_power(image, power=2, c= 0.7) # new line maybe wrong\n        \n#         image = mono_to_color(image, height=128, width=201)\n        image = mono_to_color(image, height=self.wh[0], width=self.wh[1])\n#         image = self.normalize(image)\n        \n        return image\n\n    def read_file(self, filepath):\n        audio, orig_sr = sf.read(filepath, dtype=\"float32\")\n\n        if self.resample and orig_sr != self.sr:\n            audio = lb.resample(audio, orig_sr, self.sr, res_type=self.res_type)\n          \n        audios = []\n        for i in range(self.audio_length, len(audio) + self.step, self.step):\n            start = max(0, i - self.audio_length)\n            end = start + self.audio_length\n            \n            \n            if self.tta:\n                dummy = ClippingDistortion(\n                    min_percentile_threshold = 25, max_percentile_threshold = 25, p = 1.0\n                )(samples = audio[start:end], sample_rate = self.sr)\n                \n                audios.append(dummy)\n            else:\n                audios.append(audio[start:end])\n            \n        if len(audios[-1]) < self.audio_length:\n            audios = audios[:-1]\n            \n        images = [self.audio_to_image(audio) for audio in audios]\n        images = np.stack(images)\n        \n        return images\n    \n        \n    def __getitem__(self, idx):\n        return self.read_file(self.data.loc[idx, \"filepath\"])","metadata":{"execution":{"iopub.status.busy":"2021-05-27T10:41:02.343804Z","iopub.execute_input":"2021-05-27T10:41:02.344166Z","iopub.status.idle":"2021-05-27T10:41:02.365344Z","shell.execute_reply.started":"2021-05-27T10:41:02.344133Z","shell.execute_reply":"2021-05-27T10:41:02.363541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.DataFrame(\n     [(path.stem, *path.stem.split(\"_\"), path) for path in Path(TEST_AUDIO_ROOT).glob(\"*.ogg\")],\n    columns = [\"filename\", \"id\", \"site\", \"date\", \"filepath\"]\n)\nprint(data.shape)\ndata.head()","metadata":{"id":"rVwlOrbxc_PD","outputId":"9b53ec99-634a-4b30-f9b5-75036184482b","papermill":{"duration":0.162256,"end_time":"2021-02-16T16:00:01.102152","exception":false,"start_time":"2021-02-16T16:00:00.939896","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-27T10:41:02.366724Z","iopub.execute_input":"2021-05-27T10:41:02.36703Z","iopub.status.idle":"2021-05-27T10:41:02.421605Z","shell.execute_reply.started":"2021-05-27T10:41:02.367001Z","shell.execute_reply":"2021-05-27T10:41:02.420873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(\"../input/birdclef-2021/train_metadata.csv\")\n\nLABEL_IDS = {label: label_id for label_id,label in enumerate(sorted(df_train[\"primary_label\"].unique()))}\nINV_LABEL_IDS = {val: key for key,val in LABEL_IDS.items()}","metadata":{"execution":{"iopub.status.busy":"2021-05-27T10:41:02.422789Z","iopub.execute_input":"2021-05-27T10:41:02.423131Z","iopub.status.idle":"2021-05-27T10:41:02.994095Z","shell.execute_reply.started":"2021-05-27T10:41:02.423098Z","shell.execute_reply":"2021-05-27T10:41:02.993017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LABEL_IDS = {label : label_id for label_id, label in enumerate(primary_labels)}\n# LABEL_IDS_INV = {label_id : label for label, label_id in LABEL_IDS.items()}","metadata":{"execution":{"iopub.status.busy":"2021-05-27T10:41:02.995605Z","iopub.execute_input":"2021-05-27T10:41:02.995941Z","iopub.status.idle":"2021-05-27T10:41:03.000462Z","shell.execute_reply.started":"2021-05-27T10:41:02.995909Z","shell.execute_reply":"2021-05-27T10:41:02.999233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{"id":"BNsivZZZc_PG","papermill":{"duration":0.048675,"end_time":"2021-02-16T16:00:08.424781","exception":false,"start_time":"2021-02-16T16:00:08.376106","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class RegnetModel(nn.Module):\n    def __init__(self, model_name='efficientnet_b0', pretrained=False):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained)\n        \n#         if hasattr(self.model, \"fc\"):\n        nb_ft = self.model.head.fc.in_features\n        self.model.head.fc = nn.Identity()\n        self.model.head.fc = nn.Sequential( \n            nn.Linear(nb_ft, 1024), nn.ReLU(), nn.Dropout(p=0.2),\n            nn.Linear(1024, 1024), nn.ReLU(), nn.Dropout(p=0.2),\n            nn.Linear(1024, NUM_CLASSES)\n        )\n            \n        # print(self.model)\n        \n    def forward(self, x):\n        x = self.model(x)\n        return x\n\n    \n\ndef load_net(checkpoint_path, model_name,  num_classes=NUM_CLASSES, parallel=False):\n    \n    if model_name == \"regnetx_008\":\n        net = RegnetModel(model_name=model_name, pretrained=False) \n    else:\n        net = CustomModel(model_name=model_name, pretrained=False)    \n    dummy_device = torch.device(\"cpu\")\n    if parallel:\n        net = nn.DataParallel(net)\n    d = torch.load(checkpoint_path, map_location=dummy_device)\n    net.load_state_dict(d)\n    net = net.to(DEVICE)\n    net = net.eval()\n    return net","metadata":{"papermill":{"duration":0.05945,"end_time":"2021-02-16T16:00:14.565804","exception":false,"start_time":"2021-02-16T16:00:14.506354","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-27T10:41:03.002252Z","iopub.execute_input":"2021-05-27T10:41:03.002596Z","iopub.status.idle":"2021-05-27T10:41:03.01867Z","shell.execute_reply.started":"2021-05-27T10:41:03.002563Z","shell.execute_reply":"2021-05-27T10:41:03.017524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef get_thresh_preds(out, thresh=None, use_pp=True):\n    thresh = thresh or THRESH\n    o = (-out).argsort(1)\n    npreds = (out > thresh).sum(1)\n    preds = []\n    for oo, npred in zip(o, npreds):\n        if use_pp:\n            preds.append(oo[:npred].tolist())\n        else:\n            preds.append(oo[:npred].cpu().numpy().tolist())\n    return preds\n\n\ndef get_bird_names(preds):\n    bird_names = []\n    for pred in preds:\n        if not pred:\n            bird_names.append(\"nocall\")\n        else:\n            bird_names.append(\" \".join([INV_LABEL_IDS[bird_id] for bird_id in pred]))\n    return bird_names\n\n\ndef post_process(preds, threshold=0.5, maxpreds=3):\n    preds = preds * (preds >= threshold)  # remove preds < threshold\n    next_preds = np.concatenate([preds[1:], np.zeros((1, preds.shape[-1]))])  # pred corresponding to next window\n    prev_preds = np.concatenate([np.zeros((1, preds.shape[-1])), preds[:-1]])  # pred corresponding to previous window\n    score = preds + 0.5 * next_preds + 0.5 * prev_preds  # Aggregating with neighbouring predictions\n    \n    # test this\n#     n_birds = (score >= threshold).sum(-1)  # Counting birds\n#     n_birds = np.clip(n_birds, 0, maxpreds)  # keep at most maxpreds birds\n    \n    return score","metadata":{"execution":{"iopub.status.busy":"2021-05-27T10:41:03.020468Z","iopub.execute_input":"2021-05-27T10:41:03.020812Z","iopub.status.idle":"2021-05-27T10:41:03.048429Z","shell.execute_reply.started":"2021-05-27T10:41:03.020779Z","shell.execute_reply":"2021-05-27T10:41:03.047018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_preds1(test_data, names=False): # 128, 201\n    \n    ckps = [\n        Path(\"../input/clef-effb0/birdclef_efficientnet_b0_fold0_epoch_34_f1_val_04335_20210517183043.pth\"),\n        Path(\"../input/clef-effb0/birdclef_efficientnet_b0_fold0_epoch_39_f1_val_04217_20210517185903.pth\"),\n        Path(\"../input/clef-effb0/birdclef_efficientnet_b0_fold0_epoch_43_f1_val_04890_20210517024249.pth\"),\n        Path(\"../input/clef-effb0/birdclef_efficientnet_b0_fold0_epoch_45_f1_val_06436_20210517135849.pth\"),\n        Path(\"../input/clef-effb0/birdclef_efficientnet_b0_fold0_epoch_47_f1_val_06460_20210517140640.pth\"),\n        Path(\"../input/clef-effb0/birdclef_efficientnet_b0_fold0_epoch_49_f1_val_04890_20210517030949.pth\"),\n        \n        \n        ## d5 - new\n        Path('../input/bird-models-ioa/densenet121_d5_focal_new/densenet121_fold0_epoch_76_f1_val_06552.pth'),\n        Path('../input/bird-models-ioa/densenet121_d5_focal_new/densenet121_fold1_epoch_78_f1_val_06542.pth'),\n        Path('../input/bird-models-ioa/densenet121_d5_focal_new/densenet121_fold2_epoch_76_f1_val_06565.pth'),\n        Path('../input/bird-models-ioa/densenet121_d5_focal_new/densenet121_fold3_epoch_63_f1_val_06540.pth'),\n        Path('../input/bird-models-ioa/densenet121_d5_focal_new/densenet121_fold4_epoch_72_f1_val_06515.pth'),\n\n        ## d7\n        Path('../input/bird-models-ioa/densenet121_d7_focal/densenet121_fold0_epoch_73_f1_val_06540.pth'), \n        \n        \n        # resnext100\n        Path('../input/bird-models-ioa/rexnet_100_d7_bce/rexnet_100_fold0_epoch_68_f1_val_06533.pth'),\n        Path('../input/bird-models-ioa/rexnet_100_d7_bce/rexnet_100_fold0_epoch_69_f1_val_06516.pth'),\n        \n        \n#         Path('../input/bird-models-ioa/densenet121_d5_focal_new/densenet121_fold0_epoch_72_f1_val_06520.pth'),\n#         Path('../input/bird-models-ioa/densenet121_d5_focal_new/densenet121_fold1_epoch_77_f1_val_06515.pth'),\n#         Path('../input/bird-models-ioa/densenet121_d5_focal_new/densenet121_fold2_epoch_73_f1_val_06557.pth'),\n#         Path('../input/bird-models-ioa/densenet121_d5_focal_new/densenet121_fold3_epoch_79_f1_val_06526.pth'),\n#         Path('../input/bird-models-ioa/densenet121_d5_focal_new/densenet121_fold4_epoch_76_f1_val_06486.pth'),\n        \n        \n        ## d7\n#         Path('../input/bird-models-ioa/densenet121_d7_focal/densenet121_fold0_epoch_77_f1_val_06538.pth')\n\n    ]\n    \n    \n    nets = [\n        # effnets\n        load_net(ckps[0].as_posix(), model_name=\"efficientnet_b0\"),\n        load_net(ckps[1].as_posix(), model_name=\"efficientnet_b0\"),\n        load_net(ckps[2].as_posix(), model_name=\"efficientnet_b0\"),\n        load_net(ckps[3].as_posix(), model_name=\"efficientnet_b0\"),\n        load_net(ckps[4].as_posix(), model_name=\"efficientnet_b0\"),\n        load_net(ckps[5].as_posix(), model_name=\"efficientnet_b0\"),\n        \n        \n        # densenets\n        load_net(ckps[6].as_posix(), model_name=\"densenet121\"),\n        load_net(ckps[7].as_posix(), model_name=\"densenet121\"),\n        load_net(ckps[8].as_posix(), model_name=\"densenet121\"),\n        load_net(ckps[9].as_posix(), model_name=\"densenet121\"),\n        load_net(ckps[10].as_posix(), model_name=\"densenet121\"),\n        load_net(ckps[11].as_posix(), model_name=\"densenet121\"),\n        \n        #rexnet\n        load_net(ckps[12].as_posix(), model_name=\"rexnet_100\"),\n        load_net(ckps[13].as_posix(), model_name=\"rexnet_100\"),\n        \n    ]\n    \n    THRESH = 0.3\n    \n    preds = []\n    with torch.no_grad():\n        for idx in  tqdm(list(range(len(test_data)))):\n            xb = torch.from_numpy(test_data[idx]).to(DEVICE)\n            pred = 0.\n            for net in nets:\n                o = net(xb)\n                o = torch.sigmoid(o)\n                pred += o\n\n            pred /= len(nets)\n\n#             # add pp here\n#             pred = post_process(pred.cpu().numpy(), threshold=THRESH)\n\n            if names:\n                pred = get_bird_names(get_thresh_preds(pred))\n\n            preds.append(pred)\n            \n    return preds\n    ","metadata":{"execution":{"iopub.status.busy":"2021-05-27T10:41:03.05022Z","iopub.execute_input":"2021-05-27T10:41:03.05072Z","iopub.status.idle":"2021-05-27T10:41:03.069622Z","shell.execute_reply.started":"2021-05-27T10:41:03.05068Z","shell.execute_reply":"2021-05-27T10:41:03.068505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_preds2(test_data, names=False):\n    \n    ckps = [\n        Path(\"../input/clef-effb0/birdclef_efficientnet_b0_fold0_epoch_41_f1_val_06781_20210519042210.pth\"),\n        Path(\"../input/clef-effb0/birdclef_efficientnet_b0_fold0_epoch_43_f1_val_06777_20210519044405.pth\"),\n        \n#         Path(\"../input/clef-effb0/birdclef_efficientnet_b0_fold0_epoch_43_f1_val_06777_20210519044405.pth\")\n        Path(\"../input/birdcall-224-448/birdclef_regnetx_008_fold0_epoch_46_f1_val_06390_20210519205412.pth\"),\n        Path(\"../input/birdcall-224-448/birdclef_regnetx_008_fold0_epoch_47_f1_val_06343_20210519210404.pth\"),\n        \n        Path(\"../input/birdcall-224-448/birdclef_tf_efficientnet_b0_ap_fold0_epoch_43_f1_val_06231_20210521112036.pth\"),\n        Path(\"../input/birdcall-224-448/birdclef_tf_efficientnet_b0_ap_fold0_epoch_45_f1_val_06205_20210521115612.pth\"),\n        \n        Path(\"../input/birdcall-224-448/birdclef_tf_efficientnet_b0_ap_fold0_epoch_48_f1_val_06769_20210520200135.pth\"),\n        Path(\"../input/birdcall-224-448/birdclef_tf_efficientnet_b0_ap_fold0_epoch_45_f1_val_06764_20210520192838.pth\"),\n        \n#         Path(\"../input/birdcall-224-448/birdclef_tf_efficientnet_b0_ap_fold1_epoch_47_f1_val_06234_20210522105603.pth\"),\n#         Path(\"../input/birdcall-224-448/birdclef_tf_efficientnet_b0_ap_fold1_epoch_45_f1_val_06215_20210522102142.pth\")\n        \n#         Path('../input/bird-models-ioa/tf_efficientnet_b3_ns_d5_bce_224/tf_efficientnet_b3_ns_fold0_epoch_71_f1_val_06342.pth'),        \n    ]\n    \n    \n    nets = [\n        load_net(ckps[0].as_posix(), model_name=\"efficientnet_b0\", parallel=True),\n        load_net(ckps[1].as_posix(), model_name=\"efficientnet_b0\", parallel=True),\n        \n        load_net(ckps[2].as_posix(), model_name=\"regnetx_008\", parallel=True),\n        load_net(ckps[3].as_posix(), model_name=\"regnetx_008\", parallel=True),\n        \n        load_net(ckps[4].as_posix(), model_name=\"tf_efficientnet_b0_ap\", parallel=True),\n        load_net(ckps[5].as_posix(), model_name=\"tf_efficientnet_b0_ap\", parallel=True),\n        \n        load_net(ckps[6].as_posix(), model_name=\"tf_efficientnet_b0_ap\", parallel=True),\n        load_net(ckps[7].as_posix(), model_name=\"tf_efficientnet_b0_ap\", parallel=True),\n        \n        \n#         load_net(ckps[8].as_posix(), model_name=\"tf_efficientnet_b0_ap\", parallel=True),\n#         load_net(ckps[9].as_posix(), model_name=\"tf_efficientnet_b0_ap\", parallel=True),\n        \n#         load_net(ckps[2].as_posix(), model_name=\"tf_efficientnet_b3_ns\"),\n        \n    ]\n    \n    THRESH = 0.4\n    \n    preds = []\n    with torch.no_grad():\n        for idx in  tqdm(list(range(len(test_data)))):\n            xb = torch.from_numpy(test_data[idx]).to(DEVICE)\n            pred = 0.\n            for net in nets:\n                o = net(xb)\n                o = torch.sigmoid(o)\n                pred += o\n\n            pred /= len(nets)\n\n#             # add pp here\n#             pred = post_process(pred.cpu().numpy(), threshold=THRESH)\n\n            if names:\n                pred = get_bird_names(get_thresh_preds(pred))\n\n            preds.append(pred)\n            \n    return preds","metadata":{"execution":{"iopub.status.busy":"2021-05-27T10:41:03.071788Z","iopub.execute_input":"2021-05-27T10:41:03.072331Z","iopub.status.idle":"2021-05-27T10:41:03.094063Z","shell.execute_reply.started":"2021-05-27T10:41:03.072276Z","shell.execute_reply":"2021-05-27T10:41:03.092065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_preds3():\n\n    class MelSpecComputer:\n        def __init__(self, sr, n_mels, fmin, fmax, **kwargs):\n            self.sr = sr\n            self.n_mels = n_mels\n            self.fmin = fmin\n            self.fmax = fmax\n            kwargs[\"n_fft\"] = kwargs.get(\"n_fft\", self.sr//10)\n            kwargs[\"hop_length\"] = kwargs.get(\"hop_length\", self.sr//(10*4))\n            self.kwargs = kwargs\n\n        def __call__(self, y):\n\n            melspec = lb.feature.melspectrogram(\n                y, sr=self.sr, n_mels=self.n_mels, fmin=self.fmin, fmax=self.fmax, **self.kwargs,\n            )\n\n            melspec = lb.power_to_db(melspec).astype(np.float32)\n            return melspec\n        \n        \n    def mono_to_color(X, eps=1e-6, mean=None, std=None):\n        mean = mean or X.mean()\n        std = std or X.std()\n        X = (X - mean) / (std + eps)\n\n        _min, _max = X.min(), X.max()\n\n        if (_max - _min) > eps:\n            V = np.clip(X, _min, _max)\n            V = 255 * (V - _min) / (_max - _min)\n            V = V.astype(np.uint8)\n        else:\n            V = np.zeros_like(X, dtype=np.uint8)\n\n        return V\n\n    def crop_or_pad(y, length):\n        if len(y) < length:\n            y = np.concatenate([y, length - np.zeros(len(y))])\n        elif len(y) > length:\n            y = y[:length]\n        return y\n    \n    \n    class BirdCLEFDataset(Dataset):\n        def __init__(self, data, sr=SR, n_mels=128, fmin=0, fmax=None, duration=DURATION, step=None, res_type=\"kaiser_fast\", resample=True, tta=True):\n\n            self.data = data\n\n            self.sr = sr\n            self.n_mels = n_mels\n            self.fmin = fmin\n            self.fmax = fmax or self.sr//2\n\n            self.duration = duration\n            self.audio_length = self.duration*self.sr\n            self.step = step or self.audio_length\n\n            self.tta = tta\n            \n            self.res_type = res_type\n            self.resample = resample\n\n            self.mel_spec_computer = MelSpecComputer(sr=self.sr, n_mels=self.n_mels, fmin=self.fmin,\n                                                     fmax=self.fmax)\n        def __len__(self):\n            return len(self.data)\n\n        @staticmethod\n        def normalize(image):\n            image = image.astype(\"float32\", copy=False) / 255.0\n            image = np.stack([image, image, image])\n            return image\n\n        def audio_to_image(self, audio):\n            melspec = self.mel_spec_computer(audio) \n            image = mono_to_color(melspec)\n            image = self.normalize(image)\n            return image\n\n        def read_file(self, filepath):\n            audio, orig_sr = sf.read(filepath, dtype=\"float32\")\n\n            if self.resample and orig_sr != self.sr:\n                audio = lb.resample(audio, orig_sr, self.sr, res_type=self.res_type)\n\n            audios = []\n            for i in range(self.audio_length, len(audio) + self.step, self.step):\n                start = max(0, i - self.audio_length)\n                end = start + self.audio_length\n                \n                if self.tta:\n                    dummy = ClippingDistortion(\n                        min_percentile_threshold = 25, max_percentile_threshold = 25, p = 1.0\n                    )(samples = audio[start:end], sample_rate = self.sr)\n                    \n                    audios.append(dummy)\n                    \n                else:\n                    audios.append(audio[start:end])\n                    \n\n            if len(audios[-1]) < self.audio_length:\n                audios = audios[:-1]\n\n            images = [self.audio_to_image(audio) for audio in audios]\n            images = np.stack(images)\n\n            return images\n\n\n        def __getitem__(self, idx):\n            return self.read_file(self.data.loc[idx, \"filepath\"])\n        \n        \n    \n    def load_net(checkpoint_path,model_name = \"\", num_classes=NUM_CLASSES):\n        \n        if model_name == \"efficientnet_b0\":\n            net = CustomModel(model_name=model_name, pretrained=False)\n        else:\n            net = resnest50(pretrained=False)\n            net.fc = nn.Linear(net.fc.in_features, num_classes)\n        dummy_device = torch.device(\"cpu\")\n        d = torch.load(checkpoint_path, map_location=dummy_device)\n        \n        \n        if model_name != \"efficientnet_b0\":\n            for key in list(d.keys()):\n                d[key.replace(\"model.\", \"\")] = d.pop(key)\n        net.load_state_dict(d)\n        net = net.to(DEVICE)\n        net = net.eval()\n        return net\n    \n        \n    test_data = BirdCLEFDataset(data=data)\n    print(len(test_data), test_data[0].shape)\n    \n    \n    ckps = [\n        Path(\"../input/kkiller-birdclef-models-public/birdclef_resnest50_fold0_epoch_10_f1_val_06471_20210417161101.pth\"),\n                        \n#         Path(f\"../input/bird-models/resnest50_d7_v1/resnest50_fold0_epoch_27_f1_07675.pth\"),\n#         Path(f\"../input/bird-models/resnest50_d7_v1/resnest50_fold0_epoch_29_f1_07666.pth\"),\n#         Path(f\"../input/bird-models/resnest50_d7_v1/resnest50_fold1_epoch_29_f1_07628.pth\"),\n#         Path(f\"../input/bird-models/resnest50_d7_v1/resnest50_fold2_epoch_28_f1_07727.pth\"),\n#         Path(f\"../input/bird-models/resnest50_d7_v1/resnest50_fold3_epoch_29_f1_07682.pth\"),\n#         Path(f\"../input/bird-models/resnest50_d7_v1/resnest50_fold4_epoch_20_f1_07703.pth\"),  \n    ]\n\n\n    nets = [\n        load_net(checkpoint_path.as_posix()) for checkpoint_path in ckps        \n    ]\n    \n    \n    THRESH = 0.15\n    \n    preds = []\n    with torch.no_grad():\n        for idx in  tqdm(list(range(len(test_data)))):\n            xb = torch.from_numpy(test_data[idx]).to(DEVICE)\n            pred = 0.\n            for net in nets:\n                o = net(xb)\n                o = torch.sigmoid(o)\n                pred += o\n\n            pred /= len(nets)\n\n#             # add pp here\n#             pred = post_process(pred.cpu().numpy(), threshold=THRESH)\n\n            preds.append(pred)\n                \n    return preds\n    ","metadata":{"execution":{"iopub.status.busy":"2021-05-27T10:41:03.096407Z","iopub.execute_input":"2021-05-27T10:41:03.096979Z","iopub.status.idle":"2021-05-27T10:41:03.137212Z","shell.execute_reply.started":"2021-05-27T10:41:03.096915Z","shell.execute_reply":"2021-05-27T10:41:03.136037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# os.listdir(\"../input/birdclef-kkiler-train/efficientnet_b0_sr32000_d7_v1_v1\")","metadata":{"execution":{"iopub.status.busy":"2021-05-27T10:41:03.138683Z","iopub.execute_input":"2021-05-27T10:41:03.139029Z","iopub.status.idle":"2021-05-27T10:41:03.158713Z","shell.execute_reply.started":"2021-05-27T10:41:03.138982Z","shell.execute_reply":"2021-05-27T10:41:03.157456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = BirdCLEFDataset(data=data, tta=False)\ntest_data1 = BirdCLEFDataset(data=data, wh=(224,448), tta=False)\n\nprint(len(test_data), test_data[0].shape)\nprint(len(test_data1), test_data1[0].shape)","metadata":{"execution":{"iopub.status.busy":"2021-05-27T10:41:03.160064Z","iopub.execute_input":"2021-05-27T10:41:03.160448Z","iopub.status.idle":"2021-05-27T10:41:13.028121Z","shell.execute_reply.started":"2021-05-27T10:41:03.160409Z","shell.execute_reply":"2021-05-27T10:41:13.027022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probs2 = get_preds2(test_data1)","metadata":{"execution":{"iopub.status.busy":"2021-05-27T10:41:13.02972Z","iopub.execute_input":"2021-05-27T10:41:13.030442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probs1 = get_preds1(test_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probs3 = get_preds3()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# len(list(((np.array(probs1) + np.array(probs2)) / 2)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pred_probas = list(0.7*np.array(probs1) + 0.3*np.array(probs2))\n\n# pred_probas = list((0.7*np.array(probs1) + 0.3*np.array(probs2) + 0.2*(np.array(probs3))) / 1.2 )\n\n# pred_probas = list((0.7*np.array(probs1) + 0.3*np.array(probs2) + 0.2*(np.array(probs3))) / 1.2 )\n\npred_probas = list((0.7*np.array(probs1) + 0.3*np.array(probs2) + 0.7*(np.array(probs3))) / 1.7 )\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"THRESH = 0.35\n\npreds = [get_bird_names(get_thresh_preds(pred, thresh=THRESH)) for pred in pred_probas]\n# preds[:2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_probas[0].shape\n# torch.Size([120, 397])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preds_as_df(data, preds):\n    sub = {\n        \"row_id\": [],\n        \"birds\": [],\n    }\n    \n    for row, pred in zip(data.itertuples(False), preds):\n        row_id = [f\"{row.id}_{row.site}_{5*i}\" for i in range(1, len(pred)+1)]\n        sub[\"birds\"] += pred\n        sub[\"row_id\"] += row_id\n        \n    sub = pd.DataFrame(sub)\n    \n    if SAMPLE_SUB_PATH:\n        sample_sub = pd.read_csv(SAMPLE_SUB_PATH, usecols=[\"row_id\"])\n        sub = sample_sub.merge(sub, on=\"row_id\", how=\"left\")\n        sub[\"birds\"] = sub[\"birds\"].fillna(\"nocall\")\n    return sub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub1 = preds_as_df(data, preds)\nprint(sub1.shape)\nsub1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub1.to_csv(\"submission1.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Small validation","metadata":{}},{"cell_type":"code","source":"def get_metrics(s_true, s_pred):\n    s_true = set(s_true.split())\n    s_pred = set(s_pred.split())\n    n, n_true, n_pred = len(s_true.intersection(s_pred)), len(s_true), len(s_pred)\n    \n    prec = n/n_pred\n    rec = n/n_true\n    f1 = 2*prec*rec/(prec + rec) if prec + rec else 0\n    \n    return {\"f1\": f1, \"prec\": prec, \"rec\": rec, \"n_true\": n_true, \"n_pred\": n_pred, \"n\": n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TARGET_PATH:\n    sub_target = pd.read_csv(TARGET_PATH)\n    sub_target = sub_target.merge(sub1, how=\"left\", on=\"row_id\")\n    \n    print(sub_target[\"birds_x\"].notnull().sum(), sub_target[\"birds_x\"].notnull().sum())\n    assert sub_target[\"birds_x\"].notnull().all()\n    assert sub_target[\"birds_y\"].notnull().all()\n    \n    df_metrics = pd.DataFrame([get_metrics(s_true, s_pred) for s_true, s_pred in zip(sub_target.birds_x, sub_target.birds_y)])\n    \n    print(df_metrics.mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w_nocall = 0.56\nsc1 = row_wise_micro_averaged_f1_score(sub_target[sub_target.birds_x=='nocall']['birds_x'], sub_target[sub_target.birds_x=='nocall']['birds_y'])\nsc2 = row_wise_micro_averaged_f1_score(sub_target[sub_target.birds_x!='nocall']['birds_x'], sub_target[sub_target.birds_x!='nocall']['birds_y'])\n\nfinal_score = w_nocall*sc1 + (1-w_nocall)*sc2\nfinal_score, sc1, sc2\n\n\nprint(f\"Your LB will be around {final_score} | Other Metrics {sc1}, {sc2}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 0.3529\n\n# version 12: Your LB will be around 0.6958094119631681 | Other Metrics 0.9627207325048923, 0.35610409491006456\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Add ioannis full sub kernel here","metadata":{}},{"cell_type":"code","source":"!pip install ../input/torchlibrosa/torchlibrosa-0.0.9-py2.py3-none-any.whl -q\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os, sys, gc\nimport random\nimport time\nimport warnings\nimport logging\nfrom tqdm import tqdm\n\nimport numpy as np\nimport pandas as pd\nfrom contextlib import contextmanager\nfrom pathlib import Path\nfrom typing import Optional\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom pathlib import Path\nfrom collections import Counter\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\n\nimport torchvision\nimport torchvision.models as M\nimport timm\n\nimport cv2\nimport audioread\nimport soundfile as sf\nimport librosa\nimport librosa as lb\nfrom torchlibrosa.stft import LogmelFilterBank, Spectrogram\nfrom torchlibrosa.augmentation import SpecAugmentation\nfrom albumentations.core.transforms_interface import ImageOnlyTransform\n\nfrom evaluations.kaggle_2020 import *\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import precision_score, recall_score, fbeta_score\n\nfrom resnest.torch import resnest50\nfrom resnest.torch.resnest import ResNet, Bottleneck\n\nprint('Torch version: ', torch.__version__)\nprint('TIMM version: ', timm.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed: int=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)  # type: ignore\n    torch.backends.cudnn.deterministic = True  # type: ignore\n    torch.backends.cudnn.benchmark = True  # type: ignore\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set_seed(1213)\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n\n    # Model #\n    num_classes = 397    # NUM_CLASSES = 397\n    in_channels = 1      # todo: try with 3\n\n    n_mels = 128\n    n_fft = 2048\n    hop_length = 512\n    sample_rate = 32000\n    fmin = 50\n    fmax = 16000\n\n#     melspectrogram_parameters = {\n#         \"n_mels\": n_mels,\n#         \"fmin\": fmin,\n#         \"fmax\": fmax\n#     }\n    \n\n    seed = 2020        # 1213\n    img_size = None    # n_mels\n\n    sr = sample_rate\n\n    # ######################\n    # # Data #\n    # ######################\n    train_csv = \"../input/birdclef-2021/train_metadata.csv\"\n    train_soundscape = \"../input/birdclef-2021/train_soundscape_labels.csv\"\n\n    \n    # todo:  add transf\n    ## ['RandomVolume', \"PinkNoise\", \"GaussianNoise\",  \"PitchShift\", \"CosineVolume\", \"TimeStretch\"]\n    transforms = {\n        \"train\": [{\"name\": \"Normalize\"},\n                  {\"name\": \"PinkNoise\"},\n                  {\"name\": \"GaussianNoise\"},\n                  # {\"name\": \"RandomVolume\"},\n                  # {\"name\": \"PitchShift\"},\n                  ],\n        \"valid\": [{\"name\": \"Normalize\"}],\n        \"test\": [{\"name\": \"Normalize\"}]\n    }\n\n    \n    \ncfg = CFG.__dict__","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(CFG.train_csv)  # df_train\nBIRD_CODE = {p:i for i,p in enumerate(sorted(df.primary_label.unique()))}             # LABEL_IDS\nINV_BIRD_CODE = {i:p for i,p in enumerate(sorted(df.primary_label.unique()))}         # INV_LABEL_IDS\nlen(BIRD_CODE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TEST = (len(list(Path(\"../input/birdclef-2021/test_soundscapes/\").glob(\"*.ogg\"))) != 0)\nif TEST:\n    DATADIR = Path(\"../input/birdclef-2021/test_soundscapes/\")  # TEST_AUDIO_ROOT\nelse:\n    DATADIR = Path(\"../input/birdclef-2021/train_soundscapes/\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TEST_AUDIO_ROOT = Path(\"../input/birdclef-2021/test_soundscapes\")  ## DATADIR\nSAMPLE_SUB_PATH = \"../input/birdclef-2021/sample_submission.csv\"\nTARGET_PATH = None\n    \nif not len(list(TEST_AUDIO_ROOT.glob(\"*.ogg\"))):\n    TEST_AUDIO_ROOT = Path(\"../input/birdclef-2021/train_soundscapes\")\n    SAMPLE_SUB_PATH = None\n    # SAMPLE_SUB_PATH = \"../input/birdclef-2021/sample_submission.csv\"\n    TARGET_PATH = Path(\"../input/birdclef-2021/train_soundscape_labels.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TARGET_SR = 32000  # SR \n\nall_audios = list(DATADIR.glob(\"*.ogg\"))\nall_audio_ids = [\"_\".join(audio_id.name.split(\"_\")[:2]) for audio_id in all_audios]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### -----------------------------------------------------\n# **************    Dataset    **************\n#### -----------------------------------------------------\ndef mono_to_color(X, eps=1e-6, mean=None, std=None):\n    X = np.stack([X, X, X], axis=-1)\n\n    # Standardize\n    mean = mean or X.mean()\n    std = std or X.std()\n    X = (X - mean) / (std + eps)\n\n    # Normalize to [0, 255]\n    _min, _max = X.min(), X.max()\n\n    if (_max - _min) > eps:\n        V = np.clip(X, _min, _max)\n        V = 255 * (V - _min) / (_max - _min)\n        V = V.astype(np.uint8)\n    else:\n        V = np.zeros_like(X, dtype=np.uint8)\n\n    return V\n\n\ndef resize(image, size=None):\n    if size is not None:\n        h, w, _ = image.shape\n        new_w, new_h = int(w * size / h), size\n        image = cv2.resize(image, (new_w, new_h))\n\n    return image\n\n\ndef normalize(image, mean=None, std=None):\n    image = image / 255.0\n    if mean is not None and std is not None:\n        image = (image - mean) / std\n    return np.moveaxis(image, 2, 0).astype(np.float32)\n\n\ndef load_audio(path, sr):\n    clip, _ = librosa.load(path, sr=sr, mono=True, res_type=\"kaiser_fast\")\n    # clip, _ = sf.read(path)\n    return clip\n\n\n\ndef mono_to_color2(X, eps=1e-6, mean=None, std=None):\n    mean = mean or X.mean()\n    std = std or X.std()\n    X = (X - mean) / (std + eps)\n    \n    _min, _max = X.min(), X.max()\n\n    if (_max - _min) > eps:\n        V = np.clip(X, _min, _max)\n        V = 255 * (V - _min) / (_max - _min)\n        V = V.astype(np.uint8)\n    else:\n        V = np.zeros_like(X, dtype=np.uint8)\n\n    return V\n\ndef crop_or_pad(y, length):\n    if len(y) < length:\n        y = np.concatenate([y, length - np.zeros(len(y))])\n    elif len(y) > length:\n        y = y[:length]\n    return y\n\n\n\nclass MelSpecComputer:\n    def __init__(self, sr, n_mels, fmin, fmax, **kwargs):\n        self.sr = sr\n        self.n_mels = n_mels\n        self.fmin = fmin\n        self.fmax = fmax\n        kwargs[\"n_fft\"] = kwargs.get(\"n_fft\", self.sr//10)\n        kwargs[\"hop_length\"] = kwargs.get(\"hop_length\", self.sr//(10*4))\n        self.kwargs = kwargs\n\n    def __call__(self, y):\n        melspec = lb.feature.melspectrogram(\n            y, sr=self.sr, n_mels=self.n_mels, fmin=self.fmin, fmax=self.fmax, **self.kwargs)\n        melspec = lb.power_to_db(melspec).astype(np.float32)\n        return melspec\n\n\n    \n\nclass TestDataset(Dataset):\n    def __init__(self,\n                 df: pd.DataFrame,\n                 clip: np.ndarray,\n                 waveform_transforms=None,\n                 in_channels=1\n                 ):\n        self.df = df\n        self.clip = clip\n        self.waveform_transforms = waveform_transforms\n        self.in_channels = in_channels\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx: int):\n        SR = 32000\n        sample = self.df.loc[idx, :]\n        row_id = sample.row_id\n\n        end_seconds = int(sample.seconds)\n        start_seconds = int(end_seconds - 5)\n\n        start_index = SR * start_seconds\n        end_index = SR * end_seconds\n\n        y = self.clip[start_index:end_index].astype(np.float32)\n        y = np.nan_to_num(y)\n\n        if self.waveform_transforms:\n            y = self.waveform_transforms(y)\n        y = np.nan_to_num(y)\n\n\n        return y, row_id\n\n\n    \n######## ===========================================    \n### resnest dataset\n####### ============================================\n\n# DURATION = 5\n# THRESH = 0.25\n\nclass TestDataset2(Dataset):\n    def __init__(self, \n                 data, \n                 sr=TARGET_SR, \n                 n_mels=128, \n                 fmin=CFG.fmin, \n                 fmax=CFG.fmax, \n                 duration=5, \n                 step=None, \n                 res_type=\"kaiser_fast\", \n                 resample=True\n                ):\n        \n        self.data = data\n        self.sr = sr\n        self.n_mels = n_mels\n        self.fmin = fmin\n        self.fmax = fmax or self.sr//2\n        self.duration = duration\n        self.audio_length = self.duration*self.sr\n        self.step = step or self.audio_length\n        self.res_type = res_type\n        self.resample = resample\n\n        self.mel_spec_computer = MelSpecComputer(sr=self.sr, n_mels=self.n_mels, fmin=self.fmin,fmax=self.fmax)\n    \n    def __len__(self):\n        return len(self.data)\n    \n    @staticmethod\n    def normalize(image):\n        image = image.astype(\"float32\", copy=False) / 255.0\n        image = np.stack([image, image, image])\n        return image\n    \n    def audio_to_image(self, audio):\n        melspec = self.mel_spec_computer(audio) \n        image = mono_to_color2(melspec)\n        image = self.normalize(image)\n        return image\n\n    def read_file(self, filepath):\n        audio, orig_sr = sf.read(filepath, dtype=\"float32\")\n\n        if self.resample and orig_sr != self.sr:\n            audio = lb.resample(audio, orig_sr, self.sr, res_type=self.res_type)\n          \n        audios = []\n        for i in range(self.audio_length, len(audio) + self.step, self.step):\n            start = max(0, i - self.audio_length)\n            end = start + self.audio_length\n            audios.append(audio[start:end])\n            \n        if len(audios[-1]) < self.audio_length:\n            audios = audios[:-1]\n            \n        images = [self.audio_to_image(audio) for audio in audios]\n        images = np.stack(images)\n        \n        return images\n    \n        \n    def __getitem__(self, idx):\n        return self.read_file(self.data.loc[idx, \"filepath\"])\n    \n    \n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_transforms(phase: str):\n    transforms = CFG.transforms\n    if transforms is None:\n        return None\n    else:\n        if transforms[phase] is None:\n            return None\n        trns_list = []\n        for trns_conf in transforms[phase]:\n            trns_name = trns_conf[\"name\"]\n            trns_params = {} if trns_conf.get(\"params\") is None else trns_conf[\"params\"]\n            if globals().get(trns_name) is not None:\n                trns_cls = globals()[trns_name]\n                trns_list.append(trns_cls(**trns_params))\n\n        if len(trns_list) > 0:\n            return Compose(trns_list)\n        else:\n            return None\n        \n        \nclass Normalize:\n    def __call__(self, y: np.ndarray):\n        max_vol = np.abs(y).max()\n        y_vol = y * 1 / max_vol\n        return np.asfortranarray(y_vol)\n\n\n\nclass NewNormalize:\n    def __call__(self, y: np.ndarray):\n        y_mm = y - y.mean()\n        return y_mm / y_mm.abs().max()\n\n\nclass Compose:\n    def __init__(self, transforms: list):\n        self.transforms = transforms\n\n    def __call__(self, y: np.ndarray):\n        for trns in self.transforms:\n            y = trns(y)\n        return y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## for Resnest50\n## https://www.kaggle.com/kneroma/clean-fast-simple-bird-identifier-inference\n\ndef predict2(nets, test_data):\n    preds = []\n    with torch.no_grad():\n        for idx in  list(range(len(test_data))):\n            xb = torch.from_numpy(test_data[idx]).to(device)\n            pred = 0.\n            for net in nets:\n                o = net(xb)\n                o = torch.sigmoid(o)\n                pred += o\n            pred /= len(nets)\n            \n#             preds.append(pred)\n    return pred # preds\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Jaideep helpers\n\n# import torch.nn.functional as F\n# import timm\n\ndef get_model_jaid(name, num_classes=397):\n    print(name)\n    if \"resnest\" in name:\n        model = getattr(resnest_torch, name)(pretrained=True)\n    elif \"wsl\" in name:\n        model = torch.hub.load(\"facebookresearch/WSL-Images\", name)\n    elif name.startswith(\"resnext\") or  name.startswith(\"resnet\"):\n        model = torch.hub.load(\"pytorch/vision:v0.6.0\", name, pretrained=True)\n    elif name.startswith(\"tf_efficientnet_b\"):\n        model = getattr(timm.models.efficientnet, name)(pretrained=False)\n        print('name')\n    elif \"efficientnet-b\" in name:\n        model = EfficientNet.from_pretrained(name)\n        \n    elif name.startswith(\"tf_mobile\") :\n        model = getattr(timm.models.mobilenetv3, name)(pretrained=False)\n        print('name')     \n    \n    else:\n        model = pretrainedmodels.__dict__[name](pretrained='imagenet')\n\n    if hasattr(model, \"fc\"):\n        nb_ft = model.fc.in_features\n        model.fc = nn.Linear(nb_ft, num_classes)\n    elif hasattr(model, \"_fc\"):\n        nb_ft = model._fc.in_features\n        model._fc = nn.Linear(nb_ft, num_classes)\n    elif hasattr(model, \"classifier\"):\n        nb_ft = model.classifier.in_features\n        model.classifier = nn.Linear(nb_ft, num_classes)\n    elif hasattr(model, \"last_linear\"):\n        nb_ft = model.last_linear.in_features\n        model.last_linear = nn.Linear(nb_ft, num_classes)\n    return model\n\n\n\nclass MishFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        return x * torch.tanh(F.softplus(x))   # x * tanh(ln(1 + exp(x)))\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x = ctx.saved_variables[0]\n        sigmoid = torch.sigmoid(x)\n        tanh_sp = torch.tanh(F.softplus(x)) \n        return grad_output * (tanh_sp + x * sigmoid * (1 - tanh_sp * tanh_sp))\n\nclass Mish(nn.Module):\n    def forward(self, x):\n        return MishFunction.apply(x)\n\ndef to_Mish(model):\n    for child_name, child in model.named_children():\n        if isinstance(child, nn.ReLU):\n            setattr(model, child_name, Mish())\n        else:\n            to_Mish(child)\n            \n\nclass Flatten(nn.Module):\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n\n\nclass AdaptiveConcatPool2d(nn.Module):\n    \"Layer that concats `AdaptiveAvgPool2d` and `AdaptiveMaxPool2d`\"\n    def __init__(self, size=None):\n        super().__init__()\n        self.size = size or 1\n        self.ap = nn.AdaptiveAvgPool2d(self.size)\n        self.mp = nn.AdaptiveMaxPool2d(self.size)\n    def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1)\n    \n    \n\n    \n    \nclass custom_model_mbnet(nn.Module):\n    def __init__(self,n=NUM_CLASSES,name=None):\n        super().__init__()\n        self.model=get_model_jaid(name)\n        self.model.classifier=nn.Linear(1280,n)\n        self.group_length=grp_length\n    \n     \n    def forward(self,x):\n        if self.group_length !=1:\n            n=len(x)\n            shape=x[0].shape\n            x = torch.stack(x,1).view(-1,shape[1],shape[2],shape[3])\n            x=self.model(x)\n            shape = x.shape\n            x = x.view(-1,n,shape[1],shape[2],shape[3]).permute(0,2,1,3,4).contiguous()\\\n            .view(-1,shape[1],shape[2]*n,shape[3])\n        else:\n            x=self.model(x)\n        \n        return x\n         \n\n  \n\n    \n    \n    \n## EffB1 - Jai    \nclass custom_model_b1(nn.Module):\n    def __init__(self, n=CFG.num_classes):\n        super().__init__()\n        self.model=nn.Sequential((*list(get_model_jaid('tf_efficientnet_b1').children())[:-2]))\n        nc=list(get_model_jaid('tf_efficientnet_b1').children())[-5].out_channels\n        self.head= nn.Sequential( AdaptiveConcatPool2d(),Flatten(),nn.BatchNorm1d(2*nc ) ,\n                                 nn.Linear(2*nc ,512), Mish() ,nn.Dropout(0.3),nn.Linear(512,n))\n    def forward(self,x):\n        x=self.model(x)\n        x=self.head(x)\n        return x\n\n    \ngrp_length=1 #2    \nclass custom_model_b2(nn.Module):\n    def __init__(self, n=CFG.num_classes, name=None):\n        super().__init__()\n        self.model=nn.Sequential((*list(get_model_jaid('tf_efficientnet_b2_ns').children())[:-2]))\n        nc=list(get_model_jaid('tf_efficientnet_b2_ns').children())[-5].out_channels\n        self.head= nn.Sequential( AdaptiveConcatPool2d(),Flatten(),nn.BatchNorm1d(2*nc ) ,\n                                 nn.Linear(2*nc ,512), torch.nn.SiLU(inplace=True) ,nn.Dropout(0.3),nn.Linear(512,n))\n        self.group_length=grp_length\n    \n     \n    def forward(self,x):\n        if self.group_length !=1:\n            \n            n=len(x)\n            shape=x[0].shape\n            x = torch.stack(x,1).view(-1,shape[1],shape[2],shape[3])\n            x=self.model(x)\n            shape = x.shape\n            x = x.view(-1,n,shape[1],shape[2],shape[3]).permute(0,2,1,3,4).contiguous().view(-1,shape[1],shape[2]*n,shape[3])\n        else:\n            x=self.model(x)\n            x=self.head(x)\n        return x    \n    \n\n    \nclass CustomModel_2(nn.Module):\n    def __init__(self, model_name='efficientnet_b0', pretrained=True, coord=1):\n        super().__init__()\n        self.coord = coord\n        self.trunk = timm.create_model(model_name, pretrained=pretrained, num_classes=CFG.num_classes, in_chans=3+coord)\n        self.do = nn.Dropout2d(0.2)\n\n    def forward(self, x):\n        bs, _, freq_bins, time_bins = x.size()\n        x_coord = torch.linspace(-1, 1, freq_bins, dtype=x.dtype, device=x.device).view(1,1,-1,1).expand(bs,1,-1,time_bins)\n        if self.coord==1: x = torch.cat([x, x_coord], dim=1)\n        x = self.do(x)\n        x = self.trunk(x)\n        return x    \n    \n\n\n## load Resnest_v1 - Ioa\ndef load_model_weights_resnest(checkpoint_path, num_classes=CFG.num_classes):\n    net = resnest50(pretrained=False)\n    net.fc = nn.Linear(net.fc.in_features, num_classes)\n    dummy_device = torch.device(\"cpu\")\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    d = torch.load(checkpoint_path, map_location=dummy_device)\n    for key in list(d.keys()):\n        d[key.replace(\"model.\", \"\")] = d.pop(key)\n    net.load_state_dict(d)\n    net = net.to(device)\n    net = net.eval()\n    return net\n\n\n\n## load Resnest_v3 - Ioa\ndef load_model_weights_resnest_v3(checkpoint_path, num_classes=CFG.num_classes):\n    net = getattr(timm.models, 'resnest50d_1s4x24d')(pretrained=False)\n    net.fc = nn.Linear(net.fc.in_features, num_classes)\n    dummydevice = torch.device(\"cpu\")\n    d = torch.load(checkpoint_path, map_location=dummydevice)\n    for key in list(d.keys()):\n        d[key.replace(\"model.\", \"\")] = d.pop(key)\n    net.load_state_dict(d)\n    net = net.to(device)\n    net = net.eval()\n    return net\n\n\n\nMODEL_CONFIGS = {\n    \"resnest50_fast_1s1x64d\":\n    {\n        \"num_classes\": 264,\n        \"block\": Bottleneck,\n        \"layers\": [3, 4, 6, 3],\n        \"radix\": 1,\n        \"groups\": 1,\n        \"bottleneck_width\": 64,\n        \"deep_stem\": True,\n        \"stem_width\": 32,\n        \"avg_down\": True,\n        \"avd\": True,\n        \"avd_first\": True\n    }\n}\n\n\n## load Resnest_v6 - Ioa\ndef load_model_weights_resnest_v6(checkpoint_path, num_classes=CFG.num_classes):\n    net = ResNet(**MODEL_CONFIGS[\"resnest50_fast_1s1x64d\"])\n    n_features = net.fc.in_features\n    net.fc = nn.Linear(n_features, num_classes)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    d = torch.load(checkpoint_path, map_location=device)\n    for key in list(d.keys()):\n        d[key.replace(\"model.\", \"\")] = d.pop(key)\n    net.load_state_dict(d)\n    net = net.to(device)\n    net = net.eval()\n    return net\n\n\n\n## load EffB1 - Jai\ndef load_net_b1(checkpoint_path, num_classes=CFG.num_classes):\n    net = custom_model_b1()\n    net.load_state_dict(torch.load(checkpoint_path, map_location=device))\n    net = net.to(device)\n    net = net.eval()\n    return net\n\n\n\n## load EffB2 - Jai\ndef load_net_b2(checkpoint_path, num_classes=CFG.num_classes):\n    net = custom_model_b2()\n    net.load_state_dict(torch.load(checkpoint_path, map_location=device))\n    net = net.to(device)\n    net = net.eval()\n    return net\n\n\ndef load_net_mbnet(checkpoint_path, num_classes=CFG.num_classes):\n    net = custom_model_mbnet(name=\"tf_mobilenetv3_large_100\")\n    net.load_state_dict(torch.load(checkpoint_path, map_location=device))\n    net = net.to(device)\n    net = net.eval()\n    return net\n\n\n\n## load Densenet121 - Ioa\ndef load_net2(checkpoint_path, num_classes=CFG.num_classes):\n    net = CustomModel(model_name='densenet121', pretrained=False)   \n    net.load_state_dict(torch.load(checkpoint_path, map_location=device))\n    net = net.to(device)\n    net = net.eval()\n    return net\n\n## load Rexnet - Ioa\ndef load_net3(checkpoint_path, num_classes=CFG.num_classes):\n    net = CustomModel(model_name=\"rexnet_100\", pretrained=False)   # \n    net.load_state_dict(torch.load(checkpoint_path, map_location=device))\n    net = net.to(device)\n    net = net.eval()\n    return net\n\n\n## load Effnet - Ioa\ndef load_net4(checkpoint_path, num_classes=CFG.num_classes):\n    net = CustomModel_2(model_name=\"tf_efficientnet_b3_ns\", pretrained=False)   # \n    net.load_state_dict(torch.load(checkpoint_path, map_location=device))\n    net = net.to(device)\n    net = net.eval()\n    return net","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Resnest50_v1 - ioa (kkiller data)\n\nckp_paths = [\n    Path(\"../input/kkiller-birdclef-models-public/birdclef_resnest50_fold0_epoch_10_f1_val_06471_20210417161101.pth\"),\n    ####\n    Path(f\"../input/bird-models/resnest50_d7_v1/resnest50_fold0_epoch_27_f1_07675.pth\"),\n    Path(f\"../input/bird-models/resnest50_d7_v1/resnest50_fold0_epoch_29_f1_07666.pth\"),\n    Path(f\"../input/bird-models/resnest50_d7_v1/resnest50_fold1_epoch_29_f1_07628.pth\"),\n    Path(f\"../input/bird-models/resnest50_d7_v1/resnest50_fold2_epoch_28_f1_07727.pth\"),\n    Path(f\"../input/bird-models/resnest50_d7_v1/resnest50_fold3_epoch_29_f1_07682.pth\"),\n    Path(f\"../input/bird-models/resnest50_d7_v1/resnest50_fold4_epoch_20_f1_07703.pth\"),\n]\n\n\n## Resnest50_v3 - ioa (kkiller data)\nckp_paths_v3 = [\n    Path(\"../input/bird-models/resnest50d_d7_v3/resnest50d_1s4x24d_fold0_epoch_28_f1_07400.pth\"),\n    Path(\"../input/bird-models/resnest50d_d7_v3/resnest50d_1s4x24d_fold0_epoch_29_f1_07409.pth\"),\n]\n\n## Resnest50_v4 - ioa (kkiller data)\nckp_paths_v4 = [\n    Path(\"../input/bird-models/resnest50_d7_v4/resnest50_fold0_epoch_26_f1_07411.pth\"),\n    Path(\"../input/bird-models/resnest50_d7_v4/resnest50_fold0_epoch_28_f1_07430.pth\"),\n]\n\n\n## Resnest50_v6 - ioa (kkiller data)\nckp_paths_v6 = [\n    Path(\"../input/bird-models/resnest2020_d7_v6/resnest2020_fold0_epoch_17_f1_07340.pth\"),\n    Path(\"../input/bird-models/resnest2020_d7_v6/resnest2020_fold0_epoch_18_f1_07316.pth\"),\n    Path(\"../input/bird-models/resnest2020_d7_v6/resnest2020_fold0_epoch_21_f1_07344.pth\"),\n]\n\n\n## Densenet121 - ioa (rohit data)\nckp_paths_dense = [\n    ## d5 - new\n    Path('../input/bird-models-ioa/densenet121_d5_focal_new/densenet121_fold0_epoch_72_f1_val_06520.pth'),\n    Path('../input/bird-models-ioa/densenet121_d5_focal_new/densenet121_fold0_epoch_76_f1_val_06552.pth'),\n    Path('../input/bird-models-ioa/densenet121_d5_focal_new/densenet121_fold1_epoch_77_f1_val_06515.pth'),\n    Path('../input/bird-models-ioa/densenet121_d5_focal_new/densenet121_fold1_epoch_78_f1_val_06542.pth'),\n    Path('../input/bird-models-ioa/densenet121_d5_focal_new/densenet121_fold2_epoch_73_f1_val_06557.pth'),\n    Path('../input/bird-models-ioa/densenet121_d5_focal_new/densenet121_fold2_epoch_76_f1_val_06565.pth'),\n    Path('../input/bird-models-ioa/densenet121_d5_focal_new/densenet121_fold3_epoch_63_f1_val_06540.pth'),\n    Path('../input/bird-models-ioa/densenet121_d5_focal_new/densenet121_fold3_epoch_79_f1_val_06526.pth'),\n    Path('../input/bird-models-ioa/densenet121_d5_focal_new/densenet121_fold4_epoch_72_f1_val_06515.pth'),\n    Path('../input/bird-models-ioa/densenet121_d5_focal_new/densenet121_fold4_epoch_76_f1_val_06486.pth'),\n    ## d7\n    Path('../input/bird-models-ioa/densenet121_d7_focal/densenet121_fold0_epoch_73_f1_val_06540.pth'),\n    Path('../input/bird-models-ioa/densenet121_d7_focal/densenet121_fold0_epoch_77_f1_val_06538.pth')\n]\n\n\n\n## Rexnet100 - ioa (rohit data)\nckp_paths_rex = [\n    Path('../input/bird-models-ioa/rexnet_100_d7_bce/rexnet_100_fold0_epoch_68_f1_val_06533.pth'),\n    Path('../input/bird-models-ioa/rexnet_100_d7_bce/rexnet_100_fold0_epoch_69_f1_val_06516.pth'),\n]\n\n\n## EffB3 - ioa (rohit data)\nckp_paths_eff = [\n    Path('../input/bird-models-ioa/tf_efficientnet_b3_ns_d5_bce_224/tf_efficientnet_b3_ns_fold0_epoch_71_f1_val_06342.pth'),\n    Path('../input/bird-models-ioa/tf_efficientnet_b3_ns_d5_bce_224/tf_efficientnet_b3_ns_fold0_epoch_77_f1_val_06338.pth'),\n    Path('../input/bird-models-ioa/tf_efficientnet_b3_ns_d5_bce_224/tf_efficientnet_b3_ns_fold0_epoch_79_f1_val_06340.pth'),\n]\n\n\n\n## EffB1 - Jai\nckp_paths_jaid_b1 = [\n    Path('../input/bird-clf-models/b1_concat_samp_0.8438136045980652_0_12'),\n]\n\nckp_paths_jaid_b2 = [\n    Path('../input/clean-fast-simple-bird-identifier-training-colab/birdclef_b2_effnet_fold0_epoch_16_f1_val_05515_20210519164405.pth'),\n    Path('../input/clean-fast-simple-bird-identifier-training-colab/birdclef_b2_effnet_fold1_epoch_39_f1_val_06363_20210520165108.pth'),\n]\n\n\nckp_paths_jaid_mbnet = [\n    Path('../input/jaid-mbnet/mobilenet_v3_sr32000_d5_v1_v1/birdclef_mobilenet_v3_fold2_epoch_37_f1_val_04512_20210524112151.pth'),\n    Path('../input/jaid-mbnet1/mobilenet_v3_d5_v1/birdclef_mobilenet_v3_fold0_epoch_59_f1_val_04431_20210524201737.pth'),\n#     Path('../input/mbnet-fold3/mobilenet_v3_d5_v1/birdclef_mobilenet_v3_fold1_epoch_51_f1_val_04556_20210525192949.pth')\n    Path('..../input/rank2bird-identifier-training/mobilenet_v3_sr32000_d5_v1_v1/birdclef_mobilenet_v3_fold3_epoch_49_f1_val_04422_20210525233716.pth')\n]\n\n\n#### ===================================================================================================\n\n## Resnest Ioann\n# models2 = [ load_model_weights_resnest(c.as_posix()) for c in ckp_paths ]\n\nmodels2 = []\n\n\n## Eff Jaideep\nmodels2 += [load_net_b1(checkpoint_path.as_posix()) for checkpoint_path in ckp_paths_jaid_b1 ]\nmodels2 += [load_net_b2(checkpoint_path.as_posix()) for checkpoint_path in ckp_paths_jaid_b2 ]\nmodels2 += [load_net_mbnet(checkpoint_path.as_posix()) for checkpoint_path in ckp_paths_jaid_mbnet]\n\n# ## Densenet Ioa\n# models2 += [load_net2(checkpoint_path.as_posix()) for checkpoint_path in ckp_paths_dense ]\n\n## Rexnet Ioa\n# models2 += [load_net3(checkpoint_path.as_posix()) for checkpoint_path in ckp_paths_rex ]\n\n\n## EffB3 Ioa\n# models2 += [load_net4(checkpoint_path.as_posix()) for checkpoint_path in ckp_paths_eff ]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(models2)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference(test_audio, models, threshold=0.5, device='cpu'): \n    \n    pred_dfs = []\n    raw_preds = []\n    for i, audio_id in enumerate(test_audio):\n\n        seconds = []\n        row_ids = []\n        for second in range(5, 605, 5):\n            row_id = \"_\".join(audio_id.name.split(\"_\")[:2]) + f\"_{second}\"\n            seconds.append(second)\n            row_ids.append(row_id)\n            \n            \n        test_df = pd.DataFrame([(audio_id.stem, *audio_id.stem.split(\"_\"), audio_id)], \n                                columns = [\"filename\", \"id\", \"site\", \"date\", \"filepath\"])\n        dataset = TestDataset2(data=test_df)\n        \n        print(f'\\n [{i+1}/{len(test_audio)}] Making predictions for audio: {audio_id}  ')  # in {site}\n        \n        ####### --------------------------\n        preds = []\n        \n        if models is not None:\n            ## Resnest50 prediction \n            pred2 = predict2(models, dataset)  ## pred_probas Resnest50\n            preds.append(pred2.detach().cpu().numpy())    \n    \n\n        ## Mean of all preds \n        preds = np.mean(preds, 0)\n        \n#         preds_pp = post_process(preds, threshold=threshold)        \n#         raw_preds.append(preds_pp)\n        raw_preds.append(preds)\n            \n    return raw_preds\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T15:31:44.743144Z","iopub.execute_input":"2021-05-26T15:31:44.743526Z","iopub.status.idle":"2021-05-26T15:31:44.753218Z","shell.execute_reply.started":"2021-05-26T15:31:44.743478Z","shell.execute_reply":"2021-05-26T15:31:44.752017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nTHRES = 0.5       # 0.704208\n\n# sub2, raw_preds = inference(all_audios, models=None, configs=configs, params=CFG, models2=models2, threshold=THRES, device=device)\n\nraw_preds = inference(all_audios, models=models2, threshold=THRES, device=device)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T15:31:45.317526Z","iopub.execute_input":"2021-05-26T15:31:45.317911Z","iopub.status.idle":"2021-05-26T15:33:22.665944Z","shell.execute_reply.started":"2021-05-26T15:31:45.317879Z","shell.execute_reply":"2021-05-26T15:33:22.661872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## add Rahul's model here SED","metadata":{"execution":{"iopub.status.busy":"2021-05-26T15:33:22.66836Z","iopub.execute_input":"2021-05-26T15:33:22.668791Z","iopub.status.idle":"2021-05-26T15:33:22.674329Z","shell.execute_reply.started":"2021-05-26T15:33:22.668745Z","shell.execute_reply":"2021-05-26T15:33:22.672821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport audioread\nimport logging\nimport os\nimport random\nimport time\nimport warnings\n\nimport librosa\nimport numpy as np\nimport pandas as pd\nimport soundfile as sf\nimport timm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as torchdata\n\nfrom contextlib import contextmanager\nfrom pathlib import Path\nfrom typing import Optional\n\nfrom albumentations.core.transforms_interface import ImageOnlyTransform\nfrom torchlibrosa.stft import LogmelFilterBank, Spectrogram\nfrom torchlibrosa.augmentation import SpecAugmentation\nfrom tqdm import tqdm\n\nimport timm\nfrom timm.models.efficientnet import tf_efficientnet_b2_ns , tf_efficientnet_b0","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:11:19.94007Z","iopub.execute_input":"2021-05-26T16:11:19.94046Z","iopub.status.idle":"2021-05-26T16:11:19.953539Z","shell.execute_reply.started":"2021-05-26T16:11:19.940426Z","shell.execute_reply":"2021-05-26T16:11:19.952064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_preds_sed():\n    import cv2\n    import audioread\n    import logging\n    import os\n    import random\n    import time\n    import warnings\n\n    import librosa\n    import numpy as np\n    import pandas as pd\n    import soundfile as sf\n    import timm\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.utils.data as torchdata\n\n    from contextlib import contextmanager\n    from pathlib import Path\n    from typing import Optional\n\n    from albumentations.core.transforms_interface import ImageOnlyTransform\n    from torchlibrosa.stft import LogmelFilterBank, Spectrogram\n    from torchlibrosa.augmentation import SpecAugmentation\n    from tqdm import tqdm\n\n    import timm\n    from timm.models.efficientnet import tf_efficientnet_b2_ns , tf_efficientnet_b0\n    \n    \n    def set_seed(seed: int = 42):\n        random.seed(seed)\n        np.random.seed(seed)\n        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)  # type: ignore\n        torch.backends.cudnn.deterministic = True  # type: ignore\n        torch.backends.cudnn.benchmark = True  # type: ignore\n    \n    \n    def get_logger(out_file=None):\n        logger = logging.getLogger()\n        formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n        logger.handlers = []\n        logger.setLevel(logging.INFO)\n\n        handler = logging.StreamHandler()\n        handler.setFormatter(formatter)\n        handler.setLevel(logging.INFO)\n        logger.addHandler(handler)\n\n        if out_file is not None:\n            fh = logging.FileHandler(out_file)\n            fh.setFormatter(formatter)\n            fh.setLevel(logging.INFO)\n            logger.addHandler(fh)\n        logger.info(\"logger set up\")\n        return logger\n\n\n    @contextmanager\n    def timer(name: str, logger: Optional[logging.Logger] = None):\n        t0 = time.time()\n        msg = f\"[{name}] start\"\n        if logger is None:\n            print(msg)\n        else:\n            logger.info(msg)\n        yield\n\n        msg = f\"[{name}] done in {time.time() - t0:.2f} s\"\n        if logger is None:\n            print(msg)\n        else:\n            logger.info(msg)\n    \n    \n    logger = get_logger(\"main.log\")\n    set_seed(1213)\n    \n    \n    class CFG:\n        ######################\n        # Globals #\n        ######################\n        seed = 1213\n        epochs = 35\n        train = True\n        folds = [0]\n        img_size = 224\n        main_metric = \"epoch_f1_at_05\"\n        minimize_metric = False\n\n        ######################\n        # Data #\n        ######################\n        train_datadir = Path(\"../input/birdclef-2021/train_short_audio\")\n        train_csv = \"../input/birdclef-2021/train_metadata.csv\"\n        train_soundscape = \"../input/birdclef-2021/train_soundscape_labels.csv\"\n\n        ######################\n        # Dataset #\n        ######################\n        transforms = {\n            \"train\": [{\"name\": \"Normalize\"}],\n            \"valid\": [{\"name\": \"Normalize\"}],\n            \"test\": [{\"name\": \"Normalize\"}]\n        }\n        period = 10\n        n_mels = 256\n        fmin = 40\n        fmax = 16000\n        n_fft = 1024\n        hop_length = 320\n        sample_rate = 32000\n        melspectrogram_parameters = {\n            \"n_mels\": 224,\n            \"fmin\": 20,\n            \"fmax\": 16000\n        }\n\n        target_columns = [\n            'acafly', 'acowoo', 'aldfly', 'ameavo', 'amecro',\n            'amegfi', 'amekes', 'amepip', 'amered', 'amerob',\n            'amewig', 'amtspa', 'andsol1', 'annhum', 'astfly',\n            'azaspi1', 'babwar', 'baleag', 'balori', 'banana',\n            'banswa', 'banwre1', 'barant1', 'barswa', 'batpig1',\n            'bawswa1', 'bawwar', 'baywre1', 'bbwduc', 'bcnher',\n            'belkin1', 'belvir', 'bewwre', 'bkbmag1', 'bkbplo',\n            'bkbwar', 'bkcchi', 'bkhgro', 'bkmtou1', 'bknsti', 'blbgra1',\n            'blbthr1', 'blcjay1', 'blctan1', 'blhpar1', 'blkpho',\n            'blsspa1', 'blugrb1', 'blujay', 'bncfly', 'bnhcow', 'bobfly1',\n            'bongul', 'botgra', 'brbmot1', 'brbsol1', 'brcvir1', 'brebla',\n            'brncre', 'brnjay', 'brnthr', 'brratt1', 'brwhaw', 'brwpar1',\n            'btbwar', 'btnwar', 'btywar', 'bucmot2', 'buggna', 'bugtan',\n            'buhvir', 'bulori', 'burwar1', 'bushti', 'butsal1', 'buwtea',\n            'cacgoo1', 'cacwre', 'calqua', 'caltow', 'cangoo', 'canwar',\n            'carchi', 'carwre', 'casfin', 'caskin', 'caster1', 'casvir',\n            'categr', 'ccbfin', 'cedwax', 'chbant1', 'chbchi', 'chbwre1',\n            'chcant2', 'chispa', 'chswar', 'cinfly2', 'clanut', 'clcrob',\n            'cliswa', 'cobtan1', 'cocwoo1', 'cogdov', 'colcha1', 'coltro1',\n            'comgol', 'comgra', 'comloo', 'commer', 'compau', 'compot1',\n            'comrav', 'comyel', 'coohaw', 'cotfly1', 'cowscj1', 'cregua1',\n            'creoro1', 'crfpar', 'cubthr', 'daejun', 'dowwoo', 'ducfly', 'dusfly',\n            'easblu', 'easkin', 'easmea', 'easpho', 'eastow', 'eawpew', 'eletro',\n            'eucdov', 'eursta', 'fepowl', 'fiespa', 'flrtan1', 'foxspa', 'gadwal',\n            'gamqua', 'gartro1', 'gbbgul', 'gbwwre1', 'gcrwar', 'gilwoo',\n            'gnttow', 'gnwtea', 'gocfly1', 'gockin', 'gocspa', 'goftyr1',\n            'gohque1', 'goowoo1', 'grasal1', 'grbani', 'grbher3', 'grcfly',\n            'greegr', 'grekis', 'grepew', 'grethr1', 'gretin1', 'greyel',\n            'grhcha1', 'grhowl', 'grnher', 'grnjay', 'grtgra', 'grycat',\n            'gryhaw2', 'gwfgoo', 'haiwoo', 'heptan', 'hergul', 'herthr',\n            'herwar', 'higmot1', 'hofwoo1', 'houfin', 'houspa', 'houwre',\n            'hutvir', 'incdov', 'indbun', 'kebtou1', 'killde', 'labwoo', 'larspa',\n            'laufal1', 'laugul', 'lazbun', 'leafly', 'leasan', 'lesgol', 'lesgre1',\n            'lesvio1', 'linspa', 'linwoo1', 'littin1', 'lobdow', 'lobgna5', 'logshr',\n            'lotduc', 'lotman1', 'lucwar', 'macwar', 'magwar', 'mallar3', 'marwre',\n            'mastro1', 'meapar', 'melbla1', 'monoro1', 'mouchi', 'moudov', 'mouela1',\n            'mouqua', 'mouwar', 'mutswa', 'naswar', 'norcar', 'norfli', 'normoc', 'norpar',\n            'norsho', 'norwat', 'nrwswa', 'nutwoo', 'oaktit', 'obnthr1', 'ocbfly1',\n            'oliwoo1', 'olsfly', 'orbeup1', 'orbspa1', 'orcpar', 'orcwar', 'orfpar',\n            'osprey', 'ovenbi1', 'pabspi1', 'paltan1', 'palwar', 'pasfly', 'pavpig2',\n            'phivir', 'pibgre', 'pilwoo', 'pinsis', 'pirfly1', 'plawre1', 'plaxen1',\n            'plsvir', 'plupig2', 'prowar', 'purfin', 'purgal2', 'putfru1', 'pygnut',\n            'rawwre1', 'rcatan1', 'rebnut', 'rebsap', 'rebwoo', 'redcro', 'reevir1',\n            'rehbar1', 'relpar', 'reshaw', 'rethaw', 'rewbla', 'ribgul', 'rinkin1',\n            'roahaw', 'robgro', 'rocpig', 'rotbec', 'royter1', 'rthhum', 'rtlhum',\n            'ruboro1', 'rubpep1', 'rubrob', 'rubwre1', 'ruckin', 'rucspa1', 'rucwar',\n            'rucwar1', 'rudpig', 'rudtur', 'rufhum', 'rugdov', 'rumfly1', 'runwre1',\n            'rutjac1', 'saffin', 'sancra', 'sander', 'savspa', 'saypho', 'scamac1',\n            'scatan', 'scbwre1', 'scptyr1', 'scrtan1', 'semplo', 'shicow', 'sibtan2',\n            'sinwre1', 'sltred', 'smbani', 'snogoo', 'sobtyr1', 'socfly1', 'solsan',\n            'sonspa', 'soulap1', 'sposan', 'spotow', 'spvear1', 'squcuc1', 'stbori',\n            'stejay', 'sthant1', 'sthwoo1', 'strcuc1', 'strfly1', 'strsal1', 'stvhum2',\n            'subfly', 'sumtan', 'swaspa', 'swathr', 'tenwar', 'thbeup1', 'thbkin',\n            'thswar1', 'towsol', 'treswa', 'trogna1', 'trokin', 'tromoc', 'tropar',\n            'tropew1', 'tuftit', 'tunswa', 'veery', 'verdin', 'vigswa', 'warvir',\n            'wbwwre1', 'webwoo1', 'wegspa1', 'wesant1', 'wesblu', 'weskin', 'wesmea',\n            'westan', 'wewpew', 'whbman1', 'whbnut', 'whcpar', 'whcsee1', 'whcspa',\n            'whevir', 'whfpar1', 'whimbr', 'whiwre1', 'whtdov', 'whtspa', 'whwbec1',\n            'whwdov', 'wilfly', 'willet1', 'wilsni1', 'wiltur', 'wlswar', 'wooduc',\n            'woothr', 'wrenti', 'y00475', 'yebcha', 'yebela1', 'yebfly', 'yebori1',\n            'yebsap', 'yebsee1', 'yefgra1', 'yegvir', 'yehbla', 'yehcar1', 'yelgro',\n            'yelwar', 'yeofly1', 'yerwar', 'yeteup1', 'yetvir']\n\n        ######################\n        # Loaders #\n        ######################\n        loader_params = {\n            \"train\": {\n                \"batch_size\": 64,\n                \"num_workers\": 20,\n                \"shuffle\": True\n            },\n            \"valid\": {\n                \"batch_size\": 64,\n                \"num_workers\": 20,\n                \"shuffle\": False\n            },\n            \"test\": {\n                \"batch_size\": 64,\n                \"num_workers\": 20,\n                \"shuffle\": False\n            }\n        }\n\n        ######################\n        # Split #\n        ######################\n        split = \"StratifiedKFold\"\n        split_params = {\n            \"n_splits\": 5,\n            \"shuffle\": True,\n            \"random_state\": 1213\n        }\n\n        ######################\n        # Model #\n        ######################\n        base_model_name = \"tf_efficientnet_b0_ns\"\n        pooling = \"max\"\n        pretrained = True\n        num_classes = 397\n        in_channels = 1\n\n        ######################\n        # Criterion #\n        ######################\n        loss_name = \"BCEFocal2WayLoss\"\n        loss_params: dict = {}\n\n        ######################\n        # Optimizer #\n        ######################\n        optimizer_name = \"Adam\"\n        base_optimizer = \"Adam\"\n        optimizer_params = {\n            \"lr\": 0.001\n        }\n        # For SAM optimizer\n        base_optimizer = \"Adam\"\n\n        ######################\n        # Scheduler #\n        ######################\n        scheduler_name = \"CosineAnnealingLR\"\n        scheduler_params = {\n            \"T_max\": 10\n        }\n      \n    class CFG_2:\n            period = 30\n            n_mels = 128\n            fmin = 40\n            fmax = 16000\n            n_fft = 2048\n            hop_length = 256\n            sample_rate = 32000\n        \n    TARGET_SR = 32000\n    TEST = (len(list(Path(\"../input/birdclef-2021/test_soundscapes/\").glob(\"*.ogg\"))) != 0)\n    if TEST:\n        DATADIR = Path(\"../input/birdclef-2021/test_soundscapes/\")\n    else:\n        DATADIR = Path(\"../input/birdclef-2021/train_soundscapes/\")   \n        \n        \n    all_audios = list(DATADIR.glob(\"*.ogg\"))\n    all_audio_ids = [\"_\".join(audio_id.name.split(\"_\")[:2]) for audio_id in all_audios]\n    submission_df = pd.DataFrame({\n        \"row_id\": all_audio_ids\n    })\n    submission_df\n    \n    from functools import partial\n    encoder_params = {\n        \"tf_efficientnet_b1_ns\": {\n            \"features\": 1280,\n            \"init_op\": partial(tf_efficientnet_b0, pretrained=False, drop_path_rate=0.2)\n        }\n    }\n    \n    \n    def init_layer(layer):\n        nn.init.xavier_uniform_(layer.weight)\n\n        if hasattr(layer, \"bias\"):\n            if layer.bias is not None:\n                layer.bias.data.fill_(0.)\n\n\n    def init_bn(bn):\n        bn.bias.data.fill_(0.)\n        bn.weight.data.fill_(1.0)\n\n\n    def init_weights(model):\n        classname = model.__class__.__name__\n        if classname.find(\"Conv2d\") != -1:\n            nn.init.xavier_uniform_(model.weight, gain=np.sqrt(2))\n            model.bias.data.fill_(0)\n        elif classname.find(\"BatchNorm\") != -1:\n            model.weight.data.normal_(1.0, 0.02)\n            model.bias.data.fill_(0)\n        elif classname.find(\"GRU\") != -1:\n            for weight in model.parameters():\n                if len(weight.size()) > 1:\n                    nn.init.orghogonal_(weight.data)\n        elif classname.find(\"Linear\") != -1:\n            model.weight.data.normal_(0, 0.01)\n            model.bias.data.zero_()\n\n\n    def do_mixup(x: torch.Tensor, mixup_lambda: torch.Tensor):\n        \"\"\"Mixup x of even indexes (0, 2, 4, ...) with x of odd indexes\n        (1, 3, 5, ...).\n        Args:\n          x: (batch_size * 2, ...)\n          mixup_lambda: (batch_size * 2,)\n        Returns:\n          out: (batch_size, ...)\n        \"\"\"\n        out = (x[0::2].transpose(0, -1) * mixup_lambda[0::2] +\n               x[1::2].transpose(0, -1) * mixup_lambda[1::2]).transpose(0, -1)\n        return out\n\n\n        class Mixup(object):\n            def __init__(self, mixup_alpha, random_seed=1234):\n                \"\"\"Mixup coefficient generator.\n                \"\"\"\n                self.mixup_alpha = mixup_alpha\n                self.random_state = np.random.RandomState(random_seed)\n\n            def get_lambda(self, batch_size):\n                \"\"\"Get mixup random coefficients.\n                Args:\n                  batch_size: int\n                Returns:\n                  mixup_lambdas: (batch_size,)\n                \"\"\"\n                mixup_lambdas = []\n                for n in range(0, batch_size, 2):\n                    lam = self.random_state.beta(\n                        self.mixup_alpha, self.mixup_alpha, 1)[0]\n                    mixup_lambdas.append(lam)\n                    mixup_lambdas.append(1. - lam)\n\n                return torch.from_numpy(np.array(mixup_lambdas, dtype=np.float32))\n\n\n\n    def interpolate(x: torch.Tensor, ratio: int):\n        \"\"\"Interpolate data in time domain. This is used to compensate the\n        resolution reduction in downsampling of a CNN.\n        Args:\n          x: (batch_size, time_steps, classes_num)\n          ratio: int, ratio to interpolate\n        Returns:\n          upsampled: (batch_size, time_steps * ratio, classes_num)\n        \"\"\"\n        (batch_size, time_steps, classes_num) = x.shape\n        upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n        upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n        return upsampled\n\n\n    def pad_framewise_output(framewise_output: torch.Tensor, frames_num: int):\n        \"\"\"Pad framewise_output to the same length as input frames. The pad value\n        is the same as the value of the last frame.\n        Args:\n          framewise_output: (batch_size, frames_num, classes_num)\n          frames_num: int, number of frames to pad\n        Outputs:\n          output: (batch_size, frames_num, classes_num)\n        \"\"\"\n        pad = framewise_output[:, -1:, :].repeat(\n            1, frames_num - framewise_output.shape[1], 1)\n        \"\"\"tensor for padding\"\"\"\n\n        output = torch.cat((framewise_output, pad), dim=1)\n        \"\"\"(batch_size, frames_num, classes_num)\"\"\"\n\n        return output\n\n\n    def gem(x: torch.Tensor, p=3, eps=1e-6):\n        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1. / p)\n\n\n    class GeM(nn.Module):\n        def __init__(self, p=3, eps=1e-6):\n            super().__init__()\n            self.p = nn.Parameter(torch.ones(1) * p)\n            self.eps = eps\n\n        def forward(self, x):\n            return gem(x, p=self.p, eps=self.eps)\n\n        def __repr__(self):\n            return self.__class__.__name__ + f\"(p={self.p.data.tolist()[0]:.4f}, eps={self.eps})\"\n\n\n\n\n\n    class AttBlock(nn.Module):\n        def __init__(self,\n                     in_features: int,\n                     out_features: int,\n                     activation=\"linear\",\n                     temperature=1.0):\n            super().__init__()\n\n            self.activation = activation\n            self.temperature = temperature\n            self.att = nn.Conv1d(\n                in_channels=in_features,\n                out_channels=out_features,\n                kernel_size=1,\n                stride=1,\n                padding=0,\n                bias=True)\n            self.cla = nn.Conv1d(\n                in_channels=in_features,\n                out_channels=out_features,\n                kernel_size=1,\n                stride=1,\n                padding=0,\n                bias=True)\n\n            self.bn_att = nn.BatchNorm1d(out_features)\n            self.init_weights()\n\n        def init_weights(self):\n            init_layer(self.att)\n            init_layer(self.cla)\n            init_bn(self.bn_att)\n\n        def forward(self, x):\n            # x: (n_samples, n_in, n_time)\n            norm_att = torch.softmax(torch.clamp(self.att(x), -10, 10), dim=-1)\n            cla = self.nonlinear_transform(self.cla(x))\n            x = torch.sum(norm_att * cla, dim=2)\n            return x, norm_att, cla\n\n        def nonlinear_transform(self, x):\n            if self.activation == 'linear':\n                return x\n            elif self.activation == 'sigmoid':\n                return torch.sigmoid(x)    \n\n    class TimmSED(nn.Module):\n        def __init__(self, base_model_name: str, pretrained=False, num_classes=24, in_channels=1):\n            super().__init__()\n            # Spectrogram extractor\n            self.spectrogram_extractor = Spectrogram(n_fft=CFG_2.n_fft, hop_length=CFG_2.hop_length,\n                                                     win_length=CFG_2.n_fft, window=\"hann\", center=True, pad_mode=\"reflect\",\n                                                     freeze_parameters=True)\n\n            # Logmel feature extractor\n            self.logmel_extractor = LogmelFilterBank(sr=CFG_2.sample_rate, n_fft=CFG_2.n_fft,\n                                                     n_mels=CFG_2.n_mels, fmin=CFG_2.fmin, fmax=CFG_2.fmax, ref=1.0, amin=1e-10, top_db=None,\n                                                     freeze_parameters=True)\n\n            # Spec augmenter\n            self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2,\n                                                   freq_drop_width=8, freq_stripes_num=2)\n\n            self.bn0 = nn.BatchNorm2d(CFG_2.n_mels)\n\n            self.encoder = encoder_params[\"tf_efficientnet_b1_ns\"][\"init_op\"]()\n            self.fc1 = nn.Linear(encoder_params[\"tf_efficientnet_b1_ns\"][\"features\"], 2048, bias=True)\n\n            self.att_block = AttBlock(\n                2048, num_classes, activation=\"sigmoid\")\n\n            self.init_weight()\n\n        def init_weight(self):\n            init_layer(self.fc1)\n            init_bn(self.bn0)\n\n\n        def preprocess(self, input, mixup_lambda=None):\n            # t1 = time.time()\n            x = self.spectrogram_extractor(input)  # (batch_size, 1, time_steps, freq_bins)\n            x = self.logmel_extractor(x)  # (batch_size, 1, time_steps, mel_bins)\n\n            frames_num = x.shape[2]\n\n            x = x.transpose(1, 3)\n            x = self.bn0(x)\n            x = x.transpose(1, 3)\n\n            if self.training:\n                x = self.spec_augmenter(x)\n\n            # Mixup on spectrogram\n            return x\n\n\n\n        def forward(self, input):\n            # (batch_size, 1, time_steps, freq_bins)\n            x = self.spectrogram_extractor(input)\n            x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n\n            frames_num = x.shape[2]\n\n            x = x.transpose(1, 3)\n            x = self.bn0(x)\n            x = x.transpose(1, 3)\n\n            if self.training:\n                x = self.spec_augmenter(x)\n\n    #         x = x.transpose(2, 3)\n\n            x = x.expand(x.shape[0], 3, x.shape[2], x.shape[3])\n            #print(x.shape)\n            x = self.encoder.forward_features(x)\n\n            # (batch_size, channels, frames)\n            x = torch.mean(x, dim=3)\n\n            # channel smoothing\n            x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n            x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n            x = x1 + x2\n\n            x = F.dropout(x, p=0.5, training=self.training)\n            x = x.transpose(1, 2)\n            x = F.relu_(self.fc1(x))\n            x = x.transpose(1, 2)\n            x = F.dropout(x, p=0.5, training=self.training)\n            (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n            logit = torch.sum(norm_att * self.att_block.cla(x), dim=2)\n            segmentwise_logit = self.att_block.cla(x).transpose(1, 2)\n            segmentwise_output = segmentwise_output.transpose(1, 2)\n\n            interpolate_ratio = frames_num // segmentwise_output.size(1)\n\n            # Get framewise output\n            framewise_output = interpolate(segmentwise_output,\n                                           interpolate_ratio)\n            framewise_output = pad_framewise_output(framewise_output, frames_num)\n\n            framewise_logit = interpolate(segmentwise_logit, interpolate_ratio)\n            framewise_logit = pad_framewise_output(framewise_logit, frames_num)\n\n            output_dict = {\n                \"framewise_output\": framewise_output,\n                \"segmentwise_output\": segmentwise_output,\n                \"logit\": logit,\n                \"framewise_logit\": framewise_logit,\n                \"clipwise_output\": clipwise_output\n            }\n\n            return output_dict\n\n    class AttBlock_cnn(nn.Module):\n        def __init__(self,\n                     in_features: int,\n                     out_features: int,\n                     activation=\"linear\",\n                     temperature=1.0):\n            super().__init__()\n\n            self.activation = activation\n            self.temperature = temperature\n            self.att = nn.Conv1d(\n                in_channels=in_features,\n                out_channels=out_features,\n                kernel_size=1,\n                stride=1,\n                padding=0,\n                bias=True)\n            self.cla = nn.Conv1d(\n                in_channels=in_features,\n                out_channels=out_features,\n                kernel_size=1,\n                stride=1,\n                padding=0,\n                bias=True)\n\n            self.bn_att = nn.BatchNorm1d(out_features)\n            self.init_weights()\n\n        def init_weights(self):\n            init_layer(self.att)\n            init_layer(self.cla)\n            init_bn(self.bn_att)\n\n        def forward(self, x):\n            # x: (n_samples, n_in, n_time)\n            norm_att = torch.softmax(torch.clamp(self.att(x), -10, 10), dim=-1)\n            cla = self.nonlinear_transform(self.cla(x))\n            x = torch.sum(norm_att * cla, dim=2)\n            return x, norm_att, cla\n\n        def nonlinear_transform(self, x):\n            if self.activation == 'linear':\n                return x\n            elif self.activation == 'sigmoid':\n                return torch.sigmoid(x) \n\n    class CNNSED(nn.Module):\n        def __init__(self, base_model_name: str, pretrained=False, num_classes=24, in_channels=1):\n            super().__init__()\n            # Spectrogram extractor\n            self.spectrogram_extractor = Spectrogram(n_fft=CFG.n_fft, hop_length=CFG.hop_length,\n                                                     win_length=CFG.n_fft, window=\"hann\", center=True, pad_mode=\"reflect\",\n                                                     freeze_parameters=True)\n\n            # Logmel feature extractor\n            self.logmel_extractor = LogmelFilterBank(sr=CFG.sample_rate, n_fft=CFG.n_fft,\n                                                     n_mels=CFG.n_mels, fmin=CFG.fmin, fmax=CFG.fmax, ref=1.0, amin=1e-10, top_db=None,\n                                                     freeze_parameters=True)\n\n            # Spec augmenter\n            self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2,\n                                                   freq_drop_width=8, freq_stripes_num=2)\n\n            self.bn0 = nn.BatchNorm2d(CFG.n_mels)\n\n            self.encoder = encoder_params_2[\"tf_efficientnet_b1_ns\"][\"init_op\"]()\n            self.fc1 = nn.Linear(encoder_params_2[\"tf_efficientnet_b1_ns\"][\"features\"], 2048, bias=True)\n\n            self.att_block = AttBlock(\n                2048, num_classes, activation=\"sigmoid\")\n\n            self.init_weight()\n\n        def init_weight(self):\n            init_layer(self.fc1)\n            init_bn(self.bn0)\n\n\n        def preprocess(self, input, mixup_lambda=None):\n            # t1 = time.time()\n            x = self.spectrogram_extractor(input)  # (batch_size, 1, time_steps, freq_bins)\n            x = self.logmel_extractor(x)  # (batch_size, 1, time_steps, mel_bins)\n\n            frames_num = x.shape[2]\n\n            x = x.transpose(1, 3)\n            x = self.bn0(x)\n            x = x.transpose(1, 3)\n\n            if self.training:\n                x = self.spec_augmenter(x)\n\n            # Mixup on spectrogram\n            return x\n\n\n\n        def forward(self, input):\n            # (batch_size, 1, time_steps, freq_bins)\n            x = self.spectrogram_extractor(input)\n            x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n\n            frames_num = x.shape[2]\n\n            x = x.transpose(1, 3)\n            x = self.bn0(x)\n            x = x.transpose(1, 3)\n\n            if self.training:\n                x = self.spec_augmenter(x)\n\n    #         x = x.transpose(2, 3)\n\n            x = x.expand(x.shape[0], 3, x.shape[2], x.shape[3])\n            #print(x.shape)\n            x = self.encoder.forward_features(x)\n\n            # (batch_size, channels, frames)\n            x = torch.mean(x, dim=3)\n\n            # channel smoothing\n            x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n            x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n            x = x1 + x2\n\n            x = F.dropout(x, p=0.5, training=self.training)\n            x = x.transpose(1, 2)\n            x = F.relu_(self.fc1(x))\n            x = x.transpose(1, 2)\n            x = F.dropout(x, p=0.5, training=self.training)\n            (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n            logit = torch.sum(norm_att * self.att_block.cla(x), dim=2)\n            segmentwise_logit = self.att_block.cla(x).transpose(1, 2)\n            segmentwise_output = segmentwise_output.transpose(1, 2)\n\n            interpolate_ratio = frames_num // segmentwise_output.size(1)\n\n            # Get framewise output\n            framewise_output = interpolate(segmentwise_output,\n                                           interpolate_ratio)\n            framewise_output = pad_framewise_output(framewise_output, frames_num)\n\n            framewise_logit = interpolate(segmentwise_logit, interpolate_ratio)\n            framewise_logit = pad_framewise_output(framewise_logit, frames_num)\n\n            output_dict = {\n                \"framewise_output\": framewise_output,\n                \"segmentwise_output\": segmentwise_output,\n                \"logit\": logit,\n                \"framewise_logit\": framewise_logit,\n                \"clipwise_output\": clipwise_output\n            }\n\n            return output_dict\n\n\n\n\n    encoder_params_2 = {\n        \"tf_efficientnet_b1_ns\": {\n            \"features\": 1408,\n            \"init_op\": partial(tf_efficientnet_b2_ns, pretrained=False, drop_path_rate=0.2)\n        }\n    }    \n    \n    \n    class TestDataset(torchdata.Dataset):\n        def __init__(self, df: pd.DataFrame, clip: np.ndarray,\n                     waveform_transforms=None):\n            self.df = df\n            self.clip = clip\n            self.waveform_transforms=waveform_transforms\n\n        def __len__(self):\n            return len(self.df)\n\n        def __getitem__(self, idx: int):\n            SR = 32000\n            sample = self.df.loc[idx, :]\n            row_id = sample.row_id\n\n            end_seconds = int(sample.seconds)\n            start_seconds = max(0,int(end_seconds - 5))\n\n            start_index = SR * start_seconds\n            end_index = SR * end_seconds\n\n            y = self.clip[start_index:end_index].astype(np.float32)\n\n            y = np.nan_to_num(y)\n\n            if self.waveform_transforms:\n                y = self.waveform_transforms(y)\n\n            y = np.nan_to_num(y)\n\n            return y, row_id\n        \n        \n    def get_transforms(phase: str):\n        transforms = CFG.transforms\n        if transforms is None:\n            return None\n        else:\n            if transforms[phase] is None:\n                return None\n            trns_list = []\n            for trns_conf in transforms[phase]:\n                trns_name = trns_conf[\"name\"]\n                trns_params = {} if trns_conf.get(\"params\") is None else \\\n                    trns_conf[\"params\"]\n                if globals().get(trns_name) is not None:\n                    trns_cls = globals()[trns_name]\n                    trns_list.append(trns_cls(**trns_params))\n\n            if len(trns_list) > 0:\n                return Compose(trns_list)\n            else:\n                return None\n\n\n    def get_waveform_transforms(config: dict, phase: str):\n        return get_transforms(config, phase)\n\n\n    def get_spectrogram_transforms(config: dict, phase: str):\n        transforms = config.get('spectrogram_transforms')\n        if transforms is None:\n            return None\n        else:\n            if transforms[phase] is None:\n                return None\n            trns_list = []\n            for trns_conf in transforms[phase]:\n                trns_name = trns_conf[\"name\"]\n                trns_params = {} if trns_conf.get(\"params\") is None else \\\n                    trns_conf[\"params\"]\n                if hasattr(A, trns_name):\n                    trns_cls = A.__getattribute__(trns_name)\n                    trns_list.append(trns_cls(**trns_params))\n                else:\n                    trns_cls = globals().get(trns_name)\n                    if trns_cls is not None:\n                        trns_list.append(trns_cls(**trns_params))\n\n            if len(trns_list) > 0:\n                return A.Compose(trns_list, p=1.0)\n            else:\n                return None\n            \n            \n    def prepare_model_for_inference(model, path: Path):\n        if not torch.cuda.is_available():\n            ckpt = torch.load(path, map_location=\"cpu\")\n        else:\n            ckpt = torch.load(path)\n        model.load_state_dict(ckpt[\"model_state_dict\"])\n        model.eval()\n        return model\n    \n    \n    def prediction_for_clip(test_df: pd.DataFrame, \n                        clip: np.ndarray, \n                        model,model_2,model_3,model_4,model_5,\n                        threshold=0.5):\n\n        dataset = TestDataset(df=test_df, \n                              clip=clip,\n                              waveform_transforms=get_transforms(phase=\"test\"))\n        loader = torchdata.DataLoader(dataset, batch_size=1, shuffle=False)\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        model_2.eval()\n        model_3.eval()\n        model_4.eval()\n        model.eval()\n        model_5.eval()\n        prediction_dict = {}\n        probas = []\n        for image, row_id in tqdm(loader):\n            row_id = row_id[0]\n            image = image.to(device)\n\n            with torch.no_grad():\n                prediction = model(image)\n                prediction_2 =model_2(image)\n                prediction_3 =model_3(image)\n                prediction_4 =model_4(image)\n                prediction_5 =model_5(image)\n                preds= ( 0.2* prediction[\"clipwise_output\"] + 0.2*prediction_2[\"clipwise_output\"]+\n                       0.2* prediction_3[\"clipwise_output\"] + 0.2*prediction_4[\"clipwise_output\"] +\n                        0.2* prediction_5[\"clipwise_output\"]\n                       )\n                \n                proba = preds.detach().cpu().numpy().reshape(-1)\n                \n                \n            events = proba >= threshold\n            labels = np.argwhere(events).reshape(-1).tolist()\n\n            probas.append(proba)\n#             print(events.shape)\n            \n            if len(labels) == 0:\n                prediction_dict[row_id] = \"nocall\"\n            else:\n                labels_str_list = list(map(lambda x: CFG.target_columns[x], labels))\n                label_string = \" \".join(labels_str_list)\n                prediction_dict[row_id] = label_string\n        return prediction_dict, probas\n    \n    \n    def prediction(test_audios,\n               weights_path: Path,\n               threshold=0.5):\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        model_5 = TimmSED(base_model_name=CFG.base_model_name,\n                        pretrained=False,\n                        num_classes=CFG.num_classes,\n                        in_channels=CFG.in_channels).cuda()\n        \n        model = CNNSED(base_model_name=CFG.base_model_name,\n                        pretrained=False,\n                        num_classes=CFG.num_classes,\n                        in_channels=CFG.in_channels).cuda()\n        model = prepare_model_for_inference(model, weights_path).to(device)\n        model_2 = CNNSED(base_model_name=CFG.base_model_name,\n                        pretrained=False,\n                        num_classes=CFG.num_classes,\n                        in_channels=CFG.in_channels).cuda()\n        model_3 = CNNSED(base_model_name=CFG.base_model_name,\n                        pretrained=False,\n                        num_classes=CFG.num_classes,\n                        in_channels=CFG.in_channels).cuda()\n\n        model_4 = CNNSED(base_model_name=CFG.base_model_name,\n                        pretrained=False,\n                        num_classes=CFG.num_classes,\n                        in_channels=CFG.in_channels).cuda()\n\n        model_2.load_state_dict(torch.load(\"../input/birdfold2/best.pth\")[\"model_state_dict\"])\n        model_3.load_state_dict(torch.load(\"../input/b2-fold3/checkpoints/best.pth\")[\"model_state_dict\"])\n        model_4.load_state_dict(torch.load(\"../input/b2-fold4/checkpoints/best.pth\")[\"model_state_dict\"])\n        model_5.load_state_dict(torch.load(\"../input/bird-30s-effb0/checkpoints/best.pth\")[\"model_state_dict\"])\n        warnings.filterwarnings(\"ignore\")\n        prediction_dfs = []\n        probs = []\n        for audio_path in test_audios:\n            with timer(f\"Loading {str(audio_path)}\", logger):\n                clip, _ = sf.read(audio_path)\n\n            seconds = []\n            row_ids = []\n            for second in range(5, 605, 5):\n                row_id = \"_\".join(audio_path.name.split(\"_\")[:2]) + f\"_{second}\"\n                seconds.append(second)\n                row_ids.append(row_id)\n\n            test_df = pd.DataFrame({\n                \"row_id\": row_ids,\n                \"seconds\": seconds\n            })\n            with timer(f\"Prediction on {audio_path}\", logger):\n                prediction_dict, events = prediction_for_clip(test_df,\n                                                      clip=clip,\n                                                      model=model,model_2=model_2,model_3=model_3,model_4=model_4,model_5=model_5,\n                                                      threshold=threshold)\n                \n                \n            \n            row_id = list(prediction_dict.keys())\n            birds = list(prediction_dict.values())\n            prediction_df = pd.DataFrame({\n                \"row_id\": row_id,\n                \"birds\": birds\n            })\n            prediction_dfs.append(prediction_df)\n            \n            probs.append(events)\n            \n\n        prediction_df = pd.concat(prediction_dfs, axis=0, sort=False).reset_index(drop=True)\n        return prediction_df, np.concatenate(np.array(probs), axis = 0)\n    \n    \n    weights_path = Path(\"../input/b2-fold1/checkpoints/best.pth\")\n    submission, preds_raw = prediction(test_audios=all_audios,\n                            weights_path=weights_path,\n                            threshold=0.6)\n\n    \n    return preds_raw","metadata":{"execution":{"iopub.status.busy":"2021-05-26T15:15:08.576668Z","iopub.execute_input":"2021-05-26T15:15:08.577054Z","iopub.status.idle":"2021-05-26T15:15:08.733921Z","shell.execute_reply.started":"2021-05-26T15:15:08.577019Z","shell.execute_reply":"2021-05-26T15:15:08.732897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probs_sed = get_preds_sed()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T15:15:09.512355Z","iopub.execute_input":"2021-05-26T15:15:09.512759Z","iopub.status.idle":"2021-05-26T15:20:37.855448Z","shell.execute_reply.started":"2021-05-26T15:15:09.512725Z","shell.execute_reply":"2021-05-26T15:20:37.853885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## ioannis exp 21: CV 0.8\n\n\n### global vars timer, logger, all_audios\n\n## Effnet-B0-ap 1x128x854 BCE Folds 0 - Ioannis\ndef get_preds_exp21():\n\n    ##\n    ## conda env: 192.168.2.200 dev_ime_fastai (Work server)\n\n    import os, random, gc\n    import re, time, json\n    from ast import literal_eval\n    import numpy as np\n    import pandas as pd\n#     import torchsummary\n    from matplotlib import pyplot as plt\n    from  sklearn.model_selection  import StratifiedKFold\n    from sklearn.metrics import label_ranking_average_precision_score\n    from tqdm import tqdm\n    import joblib\n    import librosa as lb\n    import librosa.display as lbd\n    import soundfile as sf\n    from soundfile import SoundFile\n    from pathlib import Path\n    import torch\n    from torch import nn, optim\n    import torch.nn.functional as F\n    from torch.utils.data import Dataset, DataLoader\n    import timm\n    import warnings\n\n    #%% Inference helpers from SED\n\n\n    def set_seed(seed: int = 42):\n        random.seed(seed)\n        np.random.seed(seed)\n        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)  # type: ignore\n        torch.backends.cudnn.deterministic = True  # type: ignore\n        torch.backends.cudnn.benchmark = True  # type: ignore\n    \n    \n    def get_logger(out_file=None):\n        logger = logging.getLogger()\n        formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n        logger.handlers = []\n        logger.setLevel(logging.INFO)\n\n        handler = logging.StreamHandler()\n        handler.setFormatter(formatter)\n        handler.setLevel(logging.INFO)\n        logger.addHandler(handler)\n\n        if out_file is not None:\n            fh = logging.FileHandler(out_file)\n            fh.setFormatter(formatter)\n            fh.setLevel(logging.INFO)\n            logger.addHandler(fh)\n        logger.info(\"logger set up\")\n        return logger\n\n\n    @contextmanager\n    def timer(name: str, logger: Optional[logging.Logger] = None):\n        t0 = time.time()\n        msg = f\"[{name}] start\"\n        if logger is None:\n            print(msg)\n        else:\n            logger.info(msg)\n        yield\n\n        msg = f\"[{name}] done in {time.time() - t0:.2f} s\"\n        if logger is None:\n            print(msg)\n        else:\n            logger.info(msg)\n    \n    \n    logger = get_logger(\"main.log\")\n    set_seed(1213)\n    \n    \n    class CFG:\n        ######################\n        # Globals #\n        ######################\n        seed = 1213\n        epochs = 35\n        train = True\n        folds = [0]\n        img_size = 224\n        main_metric = \"epoch_f1_at_05\"\n        minimize_metric = False\n\n        ######################\n        # Data #\n        ######################\n        train_datadir = Path(\"../input/birdclef-2021/train_short_audio\")\n        train_csv = \"../input/birdclef-2021/train_metadata.csv\"\n        train_soundscape = \"../input/birdclef-2021/train_soundscape_labels.csv\"\n\n        ######################\n        # Dataset #\n        ######################\n        transforms = {\n            \"train\": [{\"name\": \"Normalize\"}],\n            \"valid\": [{\"name\": \"Normalize\"}],\n            \"test\": [{\"name\": \"Normalize\"}]\n        }\n        period = 10\n        n_mels = 256\n        fmin = 40\n        fmax = 16000\n        n_fft = 1024\n        hop_length = 320\n        sample_rate = 32000\n        melspectrogram_parameters = {\n            \"n_mels\": 224,\n            \"fmin\": 20,\n            \"fmax\": 16000\n        }\n\n        target_columns = [\n            'acafly', 'acowoo', 'aldfly', 'ameavo', 'amecro',\n            'amegfi', 'amekes', 'amepip', 'amered', 'amerob',\n            'amewig', 'amtspa', 'andsol1', 'annhum', 'astfly',\n            'azaspi1', 'babwar', 'baleag', 'balori', 'banana',\n            'banswa', 'banwre1', 'barant1', 'barswa', 'batpig1',\n            'bawswa1', 'bawwar', 'baywre1', 'bbwduc', 'bcnher',\n            'belkin1', 'belvir', 'bewwre', 'bkbmag1', 'bkbplo',\n            'bkbwar', 'bkcchi', 'bkhgro', 'bkmtou1', 'bknsti', 'blbgra1',\n            'blbthr1', 'blcjay1', 'blctan1', 'blhpar1', 'blkpho',\n            'blsspa1', 'blugrb1', 'blujay', 'bncfly', 'bnhcow', 'bobfly1',\n            'bongul', 'botgra', 'brbmot1', 'brbsol1', 'brcvir1', 'brebla',\n            'brncre', 'brnjay', 'brnthr', 'brratt1', 'brwhaw', 'brwpar1',\n            'btbwar', 'btnwar', 'btywar', 'bucmot2', 'buggna', 'bugtan',\n            'buhvir', 'bulori', 'burwar1', 'bushti', 'butsal1', 'buwtea',\n            'cacgoo1', 'cacwre', 'calqua', 'caltow', 'cangoo', 'canwar',\n            'carchi', 'carwre', 'casfin', 'caskin', 'caster1', 'casvir',\n            'categr', 'ccbfin', 'cedwax', 'chbant1', 'chbchi', 'chbwre1',\n            'chcant2', 'chispa', 'chswar', 'cinfly2', 'clanut', 'clcrob',\n            'cliswa', 'cobtan1', 'cocwoo1', 'cogdov', 'colcha1', 'coltro1',\n            'comgol', 'comgra', 'comloo', 'commer', 'compau', 'compot1',\n            'comrav', 'comyel', 'coohaw', 'cotfly1', 'cowscj1', 'cregua1',\n            'creoro1', 'crfpar', 'cubthr', 'daejun', 'dowwoo', 'ducfly', 'dusfly',\n            'easblu', 'easkin', 'easmea', 'easpho', 'eastow', 'eawpew', 'eletro',\n            'eucdov', 'eursta', 'fepowl', 'fiespa', 'flrtan1', 'foxspa', 'gadwal',\n            'gamqua', 'gartro1', 'gbbgul', 'gbwwre1', 'gcrwar', 'gilwoo',\n            'gnttow', 'gnwtea', 'gocfly1', 'gockin', 'gocspa', 'goftyr1',\n            'gohque1', 'goowoo1', 'grasal1', 'grbani', 'grbher3', 'grcfly',\n            'greegr', 'grekis', 'grepew', 'grethr1', 'gretin1', 'greyel',\n            'grhcha1', 'grhowl', 'grnher', 'grnjay', 'grtgra', 'grycat',\n            'gryhaw2', 'gwfgoo', 'haiwoo', 'heptan', 'hergul', 'herthr',\n            'herwar', 'higmot1', 'hofwoo1', 'houfin', 'houspa', 'houwre',\n            'hutvir', 'incdov', 'indbun', 'kebtou1', 'killde', 'labwoo', 'larspa',\n            'laufal1', 'laugul', 'lazbun', 'leafly', 'leasan', 'lesgol', 'lesgre1',\n            'lesvio1', 'linspa', 'linwoo1', 'littin1', 'lobdow', 'lobgna5', 'logshr',\n            'lotduc', 'lotman1', 'lucwar', 'macwar', 'magwar', 'mallar3', 'marwre',\n            'mastro1', 'meapar', 'melbla1', 'monoro1', 'mouchi', 'moudov', 'mouela1',\n            'mouqua', 'mouwar', 'mutswa', 'naswar', 'norcar', 'norfli', 'normoc', 'norpar',\n            'norsho', 'norwat', 'nrwswa', 'nutwoo', 'oaktit', 'obnthr1', 'ocbfly1',\n            'oliwoo1', 'olsfly', 'orbeup1', 'orbspa1', 'orcpar', 'orcwar', 'orfpar',\n            'osprey', 'ovenbi1', 'pabspi1', 'paltan1', 'palwar', 'pasfly', 'pavpig2',\n            'phivir', 'pibgre', 'pilwoo', 'pinsis', 'pirfly1', 'plawre1', 'plaxen1',\n            'plsvir', 'plupig2', 'prowar', 'purfin', 'purgal2', 'putfru1', 'pygnut',\n            'rawwre1', 'rcatan1', 'rebnut', 'rebsap', 'rebwoo', 'redcro', 'reevir1',\n            'rehbar1', 'relpar', 'reshaw', 'rethaw', 'rewbla', 'ribgul', 'rinkin1',\n            'roahaw', 'robgro', 'rocpig', 'rotbec', 'royter1', 'rthhum', 'rtlhum',\n            'ruboro1', 'rubpep1', 'rubrob', 'rubwre1', 'ruckin', 'rucspa1', 'rucwar',\n            'rucwar1', 'rudpig', 'rudtur', 'rufhum', 'rugdov', 'rumfly1', 'runwre1',\n            'rutjac1', 'saffin', 'sancra', 'sander', 'savspa', 'saypho', 'scamac1',\n            'scatan', 'scbwre1', 'scptyr1', 'scrtan1', 'semplo', 'shicow', 'sibtan2',\n            'sinwre1', 'sltred', 'smbani', 'snogoo', 'sobtyr1', 'socfly1', 'solsan',\n            'sonspa', 'soulap1', 'sposan', 'spotow', 'spvear1', 'squcuc1', 'stbori',\n            'stejay', 'sthant1', 'sthwoo1', 'strcuc1', 'strfly1', 'strsal1', 'stvhum2',\n            'subfly', 'sumtan', 'swaspa', 'swathr', 'tenwar', 'thbeup1', 'thbkin',\n            'thswar1', 'towsol', 'treswa', 'trogna1', 'trokin', 'tromoc', 'tropar',\n            'tropew1', 'tuftit', 'tunswa', 'veery', 'verdin', 'vigswa', 'warvir',\n            'wbwwre1', 'webwoo1', 'wegspa1', 'wesant1', 'wesblu', 'weskin', 'wesmea',\n            'westan', 'wewpew', 'whbman1', 'whbnut', 'whcpar', 'whcsee1', 'whcspa',\n            'whevir', 'whfpar1', 'whimbr', 'whiwre1', 'whtdov', 'whtspa', 'whwbec1',\n            'whwdov', 'wilfly', 'willet1', 'wilsni1', 'wiltur', 'wlswar', 'wooduc',\n            'woothr', 'wrenti', 'y00475', 'yebcha', 'yebela1', 'yebfly', 'yebori1',\n            'yebsap', 'yebsee1', 'yefgra1', 'yegvir', 'yehbla', 'yehcar1', 'yelgro',\n            'yelwar', 'yeofly1', 'yerwar', 'yeteup1', 'yetvir']\n\n        ######################\n        # Loaders #\n        ######################\n        loader_params = {\n            \"train\": {\n                \"batch_size\": 64,\n                \"num_workers\": 20,\n                \"shuffle\": True\n            },\n            \"valid\": {\n                \"batch_size\": 64,\n                \"num_workers\": 20,\n                \"shuffle\": False\n            },\n            \"test\": {\n                \"batch_size\": 64,\n                \"num_workers\": 20,\n                \"shuffle\": False\n            }\n        }\n\n        ######################\n        # Split #\n        ######################\n        split = \"StratifiedKFold\"\n        split_params = {\n            \"n_splits\": 5,\n            \"shuffle\": True,\n            \"random_state\": 1213\n        }\n\n        ######################\n        # Model #\n        ######################\n        base_model_name = \"tf_efficientnet_b0_ns\"\n        pooling = \"max\"\n        pretrained = True\n        num_classes = 397\n        in_channels = 1\n\n        ######################\n        # Criterion #\n        ######################\n        loss_name = \"BCEFocal2WayLoss\"\n        loss_params: dict = {}\n\n        ######################\n        # Optimizer #\n        ######################\n        optimizer_name = \"Adam\"\n        base_optimizer = \"Adam\"\n        optimizer_params = {\n            \"lr\": 0.001\n        }\n        # For SAM optimizer\n        base_optimizer = \"Adam\"\n\n        ######################\n        # Scheduler #\n        ######################\n        scheduler_name = \"CosineAnnealingLR\"\n        scheduler_params = {\n            \"T_max\": 10\n        }\n      \n    class CFG_2:\n            period = 30\n            n_mels = 128\n            fmin = 40\n            fmax = 16000\n            n_fft = 2048\n            hop_length = 256\n            sample_rate = 32000\n    \n    \n    class TestDataset(Dataset):\n        def __init__(self, df: pd.DataFrame, clip: np.ndarray, waveform_transforms=None):\n            self.df = df\n            self.clip = clip\n            self.waveform_transforms = waveform_transforms\n    \n        def __len__(self):\n            return len(self.df)\n    \n        def __getitem__(self, idx: int):\n            SR = 32000\n            sample = self.df.loc[idx, :]\n            row_id = sample.row_id\n            end_seconds = int(sample.seconds)\n            start_seconds = max(0, int(end_seconds - 5))\n            start_index = SR * start_seconds\n            end_index = SR * end_seconds\n            y = self.clip[start_index:end_index].astype(np.float32)\n            y = np.nan_to_num(y)\n            if self.waveform_transforms:\n                y = self.waveform_transforms(y)\n            y = np.nan_to_num(y)\n            return y, row_id\n\n\n\n    def prediction_for_clip(test_df: pd.DataFrame, clip: np.ndarray, model, threshold=0.5):\n\n        dataset = TestDataset(df=test_df, clip=clip, waveform_transforms=get_transforms(phase=\"test\"))\n        loader = DataLoader(dataset, batch_size=1, shuffle=False)\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        model.eval()\n        prediction_dict = {}\n        probas = []\n        for image, row_id in tqdm(loader):\n            row_id = row_id[0]\n            image = image.to(device)\n            with torch.no_grad():\n                preds = model(image)\n                proba = preds.detach().cpu().numpy().reshape(-1)\n            events = proba >= threshold\n            labels = np.argwhere(events).reshape(-1).tolist()\n            probas.append(proba)\n            if len(labels) == 0:\n                prediction_dict[row_id] = \"nocall\"\n            else:\n                labels_str_list = list(map(lambda x: CFG.target_columns[x], labels))\n                label_string = \" \".join(labels_str_list)\n                prediction_dict[row_id] = label_string\n        return prediction_dict, probas\n\n\n    #%%\n\n    NUM_CLASSES = 397\n    SR = 32_000\n    N_MELS = 128\n    FMIN, FMAX = 100, 16000\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    \n    TARGET_SR = 32000\n    TEST = (len(list(Path(\"../input/birdclef-2021/test_soundscapes/\").glob(\"*.ogg\"))) != 0)\n    if TEST:\n        DATADIR = Path(\"../input/birdclef-2021/test_soundscapes/\")\n    else:\n        DATADIR = Path(\"../input/birdclef-2021/train_soundscapes/\")   \n    \n    \n        \n    all_audios = list(DATADIR.glob(\"*.ogg\"))\n    all_audio_ids = [\"_\".join(audio_id.name.split(\"_\")[:2]) for audio_id in all_audios]\n    submission_df = pd.DataFrame({\n        \"row_id\": all_audio_ids\n    })\n    submission_df\n    \n    \n    THRESH = 0.4\n\n    ckps = [\n#         Path(\"../input/bird-models-ioa/tf_efficientnet_b0_ap_d10_audio/tf_efficientnet_b0_ap_fold0_epoch_44_f1_val_07993.pth\"),\n        Path(\"../input/exp21-effnet-ioa-temp/tf_efficientnet_b0_ap_fold0_epoch_58_f1_val_08005.pth\")\n    ]\n\n    ### ------------------------------------------\n    ### Model Loader\n    ### ------------------------------------------\n\n    from torchlibrosa.stft import LogmelFilterBank, Spectrogram\n    from torchlibrosa.augmentation import SpecAugmentation\n\n    def init_bn(bn):\n        bn.bias.data.fill_(0.)\n        bn.weight.data.fill_(1.0)\n\n\n    class CustomAudioModel(nn.Module):\n        def __init__(self, base_model_name: str, pretrained=False, num_classes=397, in_channels=3):\n            super().__init__()\n\n            # Spectrogram extractor - 1  #  bs x 1 x 854 x 513\n            self.spectrogram_extractor = Spectrogram(n_fft=1024,\n                                                     hop_length=375,\n                                                     win_length=892,\n                                                     window=\"hann\", center=True, pad_mode=\"reflect\",\n                                                     freeze_parameters=True)\n\n            # Logmel feature extractor  #  bs x 1 x 854 x 128\n            self.logmel_extractor = LogmelFilterBank(sr=SR, n_fft=1024,\n                                                     n_mels=N_MELS, fmin=FMIN, fmax=FMAX,\n                                                     ref=1.0, amin=1e-10, top_db=None,\n                                                     freeze_parameters=True)\n\n            # Spec augmenter\n            self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2,\n                                                   freq_drop_width=8, freq_stripes_num=2)\n            self.bn0 = nn.BatchNorm2d(N_MELS)\n            self.encoder = timm.create_model(base_model_name, pretrained=pretrained, in_chans=in_channels)\n\n\n            ## rexnet family\n            if hasattr(self.encoder, \"head\"):\n                nb_ft = self.encoder.head.fc.in_features  # 1280\n                self.encoder.head.fc = nn.Identity()\n                # self.encoder.head.fc = nn.Linear(nb_ft, NUM_CLASSES)\n                self.encoder.head.fc = nn.Sequential(nn.Linear(nb_ft, 1024),\n                                                     nn.SiLU(),  # nn.ELU()\n                                                     nn.Dropout(0.2),\n                                                     nn.Linear(1024, num_classes))\n\n            ## effnets/..\n            if hasattr(self.encoder, \"fc\"):\n                nb_ft = self.encoder.fc.in_features\n                self.encoder.fc = nn.Identity()\n                self.encoder.fc = nn.Sequential(\n                    nn.Linear(nb_ft, 1024), nn.ELU(), nn.Dropout(0.2), # todo: change to 1280 for effB0\n                    nn.Linear(1024, 1024), nn.ELU(), nn.Dropout(0.2),\n                    nn.Linear(1024, num_classes))\n\n            ## densenet\n            if hasattr(self.encoder, \"classifier\"):\n                nb_ft = self.encoder.classifier.in_features  # 1024\n                self.encoder.classifier = nn.Identity()\n                self.encoder.classifier = nn.Sequential(\n                    nn.Linear(nb_ft, 1024), nn.ELU(), nn.Dropout(0.2),\n                    # nn.Linear(1024, 1024), nn.ELU(), nn.Dropout(0.2),\n                    nn.Linear(1024, num_classes))\n\n            self.init_weight()\n\n        def init_weight(self):\n            init_bn(self.bn0)\n\n        def forward(self, input):                    # (320000,)\n            s1 = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins) // torch.Size([1, 1, 854, 513])\n            # print('s1', s1.shape)\n            x = self.logmel_extractor(s1)            # (batch_size, 1, time_steps, mel_bins) // torch.Size([1, 1, 854, 513])\n            # print('mel', x.shape)\n\n            x = x.transpose(1, 3)\n            x = self.bn0(x)\n            x = x.transpose(1, 3)\n            if self.training:\n                x = self.spec_augmenter(x)\n\n            x = x.transpose(2, 3)\n            # (batch_size, channels, freq, frames)\n            out = self.encoder(x)\n            # out = self.head(x)\n            return out\n        \n\n\n    def load_net(checkpoint_path, model_name=\"\", num_classes=NUM_CLASSES):\n        net = CustomAudioModel(model_name, pretrained=False, in_channels=1)\n        net = nn.DataParallel(net)\n        dummy_device = torch.device(\"cpu\")\n        d = torch.load(checkpoint_path, map_location=dummy_device)\n        net.load_state_dict(d)\n        net = net.to(DEVICE)\n        net = net.eval()\n        \n        return net\n\n    ### ------------------------------------------\n    ### Select nets\n    ### ------------------------------------------\n\n    nets = [\n        load_net(c.as_posix(), model_name='tf_efficientnet_b0_ap') for c in ckps\n    ]\n\n\n    ### use SED inference pipeline\n    def prediction(test_audios, weights_path: Path=None, threshold=0.5):\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        model = nets[0]\n        # model_1 = nets[1]\n        # model_2 = nets[2]\n        # # model = prepare_model_for_inference(model, weights_path).to(device)\n\n        warnings.filterwarnings(\"ignore\")\n        prediction_dfs = []\n        probs = []\n        for audio_path in test_audios:\n            with timer(f\"Loading {str(audio_path)}\", logger):\n                clip, _ = sf.read(audio_path)\n\n            seconds = []\n            row_ids = []\n            for second in range(5, 605, 5):\n                row_id = \"_\".join(audio_path.name.split(\"_\")[:2]) + f\"_{second}\"\n                seconds.append(second)\n                row_ids.append(row_id)\n\n            test_df = pd.DataFrame({\n                \"row_id\": row_ids,\n                \"seconds\": seconds\n            })\n            with timer(f\"Prediction on {audio_path}\", logger):\n                prediction_dict, events = prediction_for_clip(test_df,\n                                                              clip=clip,\n                                                              model=model,\n                                                              threshold=threshold)\n\n            row_id = list(prediction_dict.keys())\n            birds = list(prediction_dict.values())\n            prediction_df = pd.DataFrame({\n                \"row_id\": row_id,\n                \"birds\": birds\n            })\n            prediction_dfs.append(prediction_df)\n            probs.append(events)\n\n        prediction_df = pd.concat(prediction_dfs, axis=0, sort=False).reset_index(drop=True)\n        return prediction_df, np.concatenate(np.array(probs), axis=0)\n\n\n\n    ### Run inference\n    subm, preds_raw = prediction(test_audios=all_audios,\n                            weights_path=None,\n                            threshold=0.5)    # 0.6\n\n    return preds_raw\n\n#%%\n\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:16:16.101155Z","iopub.execute_input":"2021-05-26T16:16:16.101568Z","iopub.status.idle":"2021-05-26T16:16:16.235898Z","shell.execute_reply.started":"2021-05-26T16:16:16.101508Z","shell.execute_reply":"2021-05-26T16:16:16.234394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probs_exp21 = get_preds_exp21()\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:16:16.692885Z","iopub.execute_input":"2021-05-26T16:16:16.693292Z","iopub.status.idle":"2021-05-26T16:17:33.531889Z","shell.execute_reply.started":"2021-05-26T16:16:16.69326Z","shell.execute_reply":"2021-05-26T16:17:33.530398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# first ensemble of 3 models \npred_probas = list((0.85*np.array(probs1) + 0.15*np.array(probs2) + 0.8*(np.array(probs3))) / 1.8 )\n# 20*397","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:20:22.455183Z","iopub.execute_input":"2021-05-26T16:20:22.455545Z","iopub.status.idle":"2021-05-26T16:20:22.467299Z","shell.execute_reply.started":"2021-05-26T16:20:22.455512Z","shell.execute_reply":"2021-05-26T16:20:22.465864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# raw_preds[0]\n# raw_preds[0]\n\n\nps1 = []\nfor val in pred_probas:\n    ps1.append(val.cpu().numpy())\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:20:22.928816Z","iopub.execute_input":"2021-05-26T16:20:22.929208Z","iopub.status.idle":"2021-05-26T16:20:22.937361Z","shell.execute_reply.started":"2021-05-26T16:20:22.929176Z","shell.execute_reply":"2021-05-26T16:20:22.93615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probs_sed = np.array(probs_sed).reshape(np.array(ps1).shape)\n\nprobs_exp21 = np.array(probs_sed).reshape(np.array(ps1).shape)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:20:23.113421Z","iopub.execute_input":"2021-05-26T16:20:23.113758Z","iopub.status.idle":"2021-05-26T16:20:23.123461Z","shell.execute_reply.started":"2021-05-26T16:20:23.113696Z","shell.execute_reply":"2021-05-26T16:20:23.122172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # concatenate all predictions to have same shape\n\n# pred1 = np.concatenate(np.array(ps1), axis = 0)\n# pred2 = np.concatenate(np.array(raw_preds), axis = 0)\n# # pred3 = np.concatenate(np.array(probs_sed), axis = 0)\n\n# pred3 = np.array(probs_sed)\n\n# print(pred1.shape), print(pred2.shape), print(pred3.shape)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:20:23.595989Z","iopub.execute_input":"2021-05-26T16:20:23.596406Z","iopub.status.idle":"2021-05-26T16:20:23.603879Z","shell.execute_reply.started":"2021-05-26T16:20:23.59636Z","shell.execute_reply":"2021-05-26T16:20:23.60259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# final_probs =  list( (0.85* np.array(ps1)  + 0.15* np.array(raw_preds) + 0.6*  np.array(probs_sed))/ 1.6)\n\n# final_probs =  list( (0.85* np.array(ps1)  + 0.15* np.array(raw_preds) + 0.6* np.array(probs_sed))/ 1.6)\n\nfinal_probs =  list( (0.85* np.array(ps1)  + 0.15* np.array(raw_preds) + 0.6* np.array(probs_sed) +  0.09*np.array(probs_exp21))/ 1.69)\n\n\n# final_probs = list(0.9*np.array(final_probs) + 0.1*np.array(probs_exp21))\n\n\n# final_probs = np.array(probs_exp21)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:28:52.827073Z","iopub.execute_input":"2021-05-26T16:28:52.827544Z","iopub.status.idle":"2021-05-26T16:28:52.850142Z","shell.execute_reply.started":"2021-05-26T16:28:52.827486Z","shell.execute_reply":"2021-05-26T16:28:52.849107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pred1.shape","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:28:53.5467Z","iopub.execute_input":"2021-05-26T16:28:53.547126Z","iopub.status.idle":"2021-05-26T16:28:53.552572Z","shell.execute_reply.started":"2021-05-26T16:28:53.547083Z","shell.execute_reply":"2021-05-26T16:28:53.550599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# final_probs.shape","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:28:53.905947Z","iopub.execute_input":"2021-05-26T16:28:53.906351Z","iopub.status.idle":"2021-05-26T16:28:53.911803Z","shell.execute_reply.started":"2021-05-26T16:28:53.906318Z","shell.execute_reply":"2021-05-26T16:28:53.910355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nps = []\nfor val in final_probs:\n    preds_pp = post_process(val, threshold=0.27)  # 0.22\n    ps.append(preds_pp)\n\n# final_probs =  list(np.array(ps1))\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:28:54.150634Z","iopub.execute_input":"2021-05-26T16:28:54.150966Z","iopub.status.idle":"2021-05-26T16:28:54.168389Z","shell.execute_reply.started":"2021-05-26T16:28:54.15092Z","shell.execute_reply":"2021-05-26T16:28:54.166938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ps = []\n# for val in final_probs:\n#     preds_pp = post_process(val, threshold=0.25)  # 0.23\n#     ps.append(preds_pp)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:28:54.365321Z","iopub.execute_input":"2021-05-26T16:28:54.365664Z","iopub.status.idle":"2021-05-26T16:28:54.370114Z","shell.execute_reply.started":"2021-05-26T16:28:54.365631Z","shell.execute_reply":"2021-05-26T16:28:54.368855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add post processing here\n\ndef get_threshold(threshold_dict, pred_probas):\n    threshold = np.ones_like(pred_probas) * threshold_dict['median']\n    threshold_matrix = np.ones_like(pred_probas) * threshold_dict['median']\n    for i in range(0, pred_probas.shape[0], 120):\n        is_confident = np.sum(pred_probas[i : i + 120] > threshold_dict['high'], axis = 0).astype(bool)\n        threshold_slice = threshold_matrix[i : i + 120]\n        threshold_slice[:, is_confident] = threshold_dict['low']\n        #if threshold_slice[:, is_confident]\n        code_confident = np.where(is_confident)[0]\n        \n        for rank in range(1):\n            col_max = np.argsort(pred_probas[i : i + 120], axis = 1)[:, -rank-1]\n            row_max = np.isin(col_max, code_confident)\n            threshold_slice[row_max, col_max[row_max]] = threshold_dict['bottom'] * (1 - (10 - rank) / 10)\n            \n            \n        threshold_matrix[i : i + 120] = threshold_slice\n    return threshold_matrix\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:38:22.845554Z","iopub.execute_input":"2021-05-26T16:38:22.845921Z","iopub.status.idle":"2021-05-26T16:38:22.856946Z","shell.execute_reply.started":"2021-05-26T16:38:22.845887Z","shell.execute_reply":"2021-05-26T16:38:22.85571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_exp = np.concatenate(np.array(final_probs), axis = 0)\n\n# pred_exp = np.concatenate(np.array(ps), axis = 0)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:38:23.961434Z","iopub.execute_input":"2021-05-26T16:38:23.96182Z","iopub.status.idle":"2021-05-26T16:38:23.969195Z","shell.execute_reply.started":"2021-05-26T16:38:23.961786Z","shell.execute_reply":"2021-05-26T16:38:23.96785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pred_exp = final_probs","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:38:24.401526Z","iopub.execute_input":"2021-05-26T16:38:24.402069Z","iopub.status.idle":"2021-05-26T16:38:24.407448Z","shell.execute_reply.started":"2021-05-26T16:38:24.40202Z","shell.execute_reply":"2021-05-26T16:38:24.406024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# x.shape  #(2400, 397)\n\n# row_ids = []\n# for name in filenames:\n#     for seconds in range(0, 600, 5):\n#         row_ids.append(name[0] + f\"_{seconds + 5}\")\n# print(len(row_ids))","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:38:24.791854Z","iopub.execute_input":"2021-05-26T16:38:24.792496Z","iopub.status.idle":"2021-05-26T16:38:24.79851Z","shell.execute_reply.started":"2021-05-26T16:38:24.79245Z","shell.execute_reply":"2021-05-26T16:38:24.797148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_preds(probas, threshold):\n    rows, values = np.where(probas > threshold)\n    y_class = dict(enumerate([[-1]] * len(probas)))\n    for row in rows:\n        y_class[row] = values[np.where(rows == row)]\n        \n    INV_LABEL_IDS[-1] = \"nocall\"\n    y_class = list(y_class.values())\n    y_class = [[INV_LABEL_IDS[c] for c in c_list] for c_list in y_class]\n    submit_preds = [' '.join(c_list) for c_list in y_class]\n    return submit_preds","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:38:25.183414Z","iopub.execute_input":"2021-05-26T16:38:25.183823Z","iopub.status.idle":"2021-05-26T16:38:25.192627Z","shell.execute_reply.started":"2021-05-26T16:38:25.183789Z","shell.execute_reply":"2021-05-26T16:38:25.191038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 0.51 | 0.45\n# 0.51\n# 2400 2400\n# f1        0.803819\n# prec      0.815125\n# rec       0.805778\n# n_true    1.130000\n# n_pred    1.117917\n# n         0.882917\n# dtype: float64\n# Your LB will be around 0.7846548562214767 | Other Metrics 0.8940483976455086, 0.6454267125908906","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:38:25.849118Z","iopub.execute_input":"2021-05-26T16:38:25.849488Z","iopub.status.idle":"2021-05-26T16:38:25.854129Z","shell.execute_reply.started":"2021-05-26T16:38:25.849444Z","shell.execute_reply":"2021-05-26T16:38:25.852617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_exp.shape","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:38:26.947935Z","iopub.execute_input":"2021-05-26T16:38:26.948512Z","iopub.status.idle":"2021-05-26T16:38:26.957116Z","shell.execute_reply.started":"2021-05-26T16:38:26.94845Z","shell.execute_reply":"2021-05-26T16:38:26.955648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for high in [0.1, .2, .3, .4, 0.49,  .5, .51, .52, .6, .7, .8, .9]:\n\n    threshold_dict = {\n        'high': high, # 0.7\n        'median': 0.45,\n        'low' : 0.1,\n        'bottom': 0.05\n    }\n    \n    print(high)\n    \n    kk_threshold = get_threshold(threshold_dict, pred_exp)\n\n\n    kk_preds = get_preds(pred_exp, kk_threshold)\n\n\n    row_ids = []\n\n    for row, pred in zip(data.itertuples(False), final_probs):\n        row_id = [f\"{row.id}_{row.site}_{5*i}\" for i in range(1, len(pred)+1)]\n        row_ids.extend(row_id)\n\n\n    sub2 = pd.DataFrame({\"row_id\" : row_ids, \"birds\" : kk_preds})\n    sub2[\"birds\"].fillna(\"nocall\", inplace = True)\n    sub2.to_csv('submission.csv', index=False)\n\n\n\n    def get_metrics(s_true, s_pred):\n        s_true = set(s_true.split())\n        s_pred = set(s_pred.split())\n        n, n_true, n_pred = len(s_true.intersection(s_pred)), len(s_true), len(s_pred)\n\n        prec = n/n_pred\n        rec = n/n_true\n        f1 = 2*prec*rec/(prec + rec) if prec + rec else 0\n\n        return {\"f1\": f1, \"prec\": prec, \"rec\": rec, \"n_true\": n_true, \"n_pred\": n_pred, \"n\": n}\n\n\n    if TARGET_PATH:\n        sub_target = pd.read_csv(TARGET_PATH)\n        sub_target = sub_target.merge(sub2, how=\"left\", on=\"row_id\")\n\n        print(sub_target[\"birds_x\"].notnull().sum(), sub_target[\"birds_x\"].notnull().sum())\n        assert sub_target[\"birds_x\"].notnull().all()\n        assert sub_target[\"birds_y\"].notnull().all()\n\n        df_metrics = pd.DataFrame([get_metrics(s_true, s_pred) for s_true, s_pred in zip(sub_target.birds_x, sub_target.birds_y)])\n\n        print(df_metrics.mean())\n\n\n    w_nocall = 0.56\n    sc1 = row_wise_micro_averaged_f1_score(sub_target[sub_target.birds_x=='nocall']['birds_x'], sub_target[sub_target.birds_x=='nocall']['birds_y'])\n    sc2 = row_wise_micro_averaged_f1_score(sub_target[sub_target.birds_x!='nocall']['birds_x'], sub_target[sub_target.birds_x!='nocall']['birds_y'])\n\n    final_score = w_nocall*sc1 + (1-w_nocall)*sc2\n    final_score, sc1, sc2\n\n\n    print(f\"Your LB will be around {final_score} | Other Metrics {sc1}, {sc2}\")","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:38:27.383561Z","iopub.execute_input":"2021-05-26T16:38:27.383928Z","iopub.status.idle":"2021-05-26T16:38:28.948997Z","shell.execute_reply.started":"2021-05-26T16:38:27.383893Z","shell.execute_reply":"2021-05-26T16:38:28.947736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"threshold_dict = {\n    'high': 0.51,\n    'median': 0.45,\n    'low' : 0.1,\n    'bottom': 0.05\n}\n\nkk_threshold = get_threshold(threshold_dict, pred_exp)\n\n# kk_preds = get_preds(final_probs, kk_threshold)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:36:16.59595Z","iopub.execute_input":"2021-05-26T16:36:16.596637Z","iopub.status.idle":"2021-05-26T16:36:16.942528Z","shell.execute_reply.started":"2021-05-26T16:36:16.596587Z","shell.execute_reply":"2021-05-26T16:36:16.941449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# kk_threshold[1000]","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:36:16.944303Z","iopub.execute_input":"2021-05-26T16:36:16.9449Z","iopub.status.idle":"2021-05-26T16:36:16.949809Z","shell.execute_reply.started":"2021-05-26T16:36:16.944853Z","shell.execute_reply":"2021-05-26T16:36:16.948588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kk_preds = get_preds(pred_exp, kk_threshold)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:36:16.951383Z","iopub.execute_input":"2021-05-26T16:36:16.952088Z","iopub.status.idle":"2021-05-26T16:36:17.001062Z","shell.execute_reply.started":"2021-05-26T16:36:16.95204Z","shell.execute_reply":"2021-05-26T16:36:16.999869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# kk_preds ","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:36:17.004084Z","iopub.execute_input":"2021-05-26T16:36:17.004733Z","iopub.status.idle":"2021-05-26T16:36:17.009621Z","shell.execute_reply.started":"2021-05-26T16:36:17.004686Z","shell.execute_reply":"2021-05-26T16:36:17.007961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"row_ids = []\n\nfor row, pred in zip(data.itertuples(False), final_probs):\n    row_id = [f\"{row.id}_{row.site}_{5*i}\" for i in range(1, len(pred)+1)]\n    row_ids.extend(row_id)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:36:17.011567Z","iopub.execute_input":"2021-05-26T16:36:17.012279Z","iopub.status.idle":"2021-05-26T16:36:17.025221Z","shell.execute_reply.started":"2021-05-26T16:36:17.012229Z","shell.execute_reply":"2021-05-26T16:36:17.023645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(row_ids), len(kk_preds)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:36:17.026855Z","iopub.execute_input":"2021-05-26T16:36:17.027583Z","iopub.status.idle":"2021-05-26T16:36:17.039339Z","shell.execute_reply.started":"2021-05-26T16:36:17.027536Z","shell.execute_reply":"2021-05-26T16:36:17.038013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# len(row_ids)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:36:17.041053Z","iopub.execute_input":"2021-05-26T16:36:17.041831Z","iopub.status.idle":"2021-05-26T16:36:17.048441Z","shell.execute_reply.started":"2021-05-26T16:36:17.04172Z","shell.execute_reply":"2021-05-26T16:36:17.047177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub2 = pd.DataFrame({\"row_id\" : row_ids, \"birds\" : kk_preds})\nsub2[\"birds\"].fillna(\"nocall\", inplace = True)\n\nsub2","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:36:17.050202Z","iopub.execute_input":"2021-05-26T16:36:17.050946Z","iopub.status.idle":"2021-05-26T16:36:17.076605Z","shell.execute_reply.started":"2021-05-26T16:36:17.050896Z","shell.execute_reply":"2021-05-26T16:36:17.075441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ps = []\n# for val in final_probs:\n#     preds_pp = post_process(val, threshold=0.23)  # 0.23\n#     ps.append(preds_pp)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:31:40.620021Z","iopub.execute_input":"2021-05-26T16:31:40.620368Z","iopub.status.idle":"2021-05-26T16:31:40.624947Z","shell.execute_reply.started":"2021-05-26T16:31:40.620336Z","shell.execute_reply":"2021-05-26T16:31:40.6236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# THRESH = 0.28 # 0.28\n\n# # preds4 = [get_bird_names(get_thresh_preds(pred, thresh=THRESH, use_pp=True)) for pred in final_probs]\n# preds4 = [get_bird_names(get_thresh_preds(pred, thresh=THRESH, use_pp=True)) for pred in ps]\n\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:31:42.739223Z","iopub.execute_input":"2021-05-26T16:31:42.739594Z","iopub.status.idle":"2021-05-26T16:31:42.746714Z","shell.execute_reply.started":"2021-05-26T16:31:42.73956Z","shell.execute_reply":"2021-05-26T16:31:42.745324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # preds4\n\n# sub2 = preds_as_df(data, preds4)\n# print(sub2.shape)\n# sub2","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:31:43.139202Z","iopub.execute_input":"2021-05-26T16:31:43.139552Z","iopub.status.idle":"2021-05-26T16:31:43.144135Z","shell.execute_reply.started":"2021-05-26T16:31:43.139519Z","shell.execute_reply":"2021-05-26T16:31:43.142913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\nsub2.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:31:43.281751Z","iopub.execute_input":"2021-05-26T16:31:43.282084Z","iopub.status.idle":"2021-05-26T16:31:43.303226Z","shell.execute_reply.started":"2021-05-26T16:31:43.282045Z","shell.execute_reply":"2021-05-26T16:31:43.301947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sub2.birds.value_counts()[:10]","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:31:43.517516Z","iopub.execute_input":"2021-05-26T16:31:43.517866Z","iopub.status.idle":"2021-05-26T16:31:43.526269Z","shell.execute_reply.started":"2021-05-26T16:31:43.517834Z","shell.execute_reply":"2021-05-26T16:31:43.523854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndef get_metrics(s_true, s_pred):\n    s_true = set(s_true.split())\n    s_pred = set(s_pred.split())\n    n, n_true, n_pred = len(s_true.intersection(s_pred)), len(s_true), len(s_pred)\n    \n    prec = n/n_pred\n    rec = n/n_true\n    f1 = 2*prec*rec/(prec + rec) if prec + rec else 0\n    \n    return {\"f1\": f1, \"prec\": prec, \"rec\": rec, \"n_true\": n_true, \"n_pred\": n_pred, \"n\": n}\n\n\nif TARGET_PATH:\n    sub_target = pd.read_csv(TARGET_PATH)\n    sub_target = sub_target.merge(sub2, how=\"left\", on=\"row_id\")\n    \n    print(sub_target[\"birds_x\"].notnull().sum(), sub_target[\"birds_x\"].notnull().sum())\n    assert sub_target[\"birds_x\"].notnull().all()\n    assert sub_target[\"birds_y\"].notnull().all()\n    \n    df_metrics = pd.DataFrame([get_metrics(s_true, s_pred) for s_true, s_pred in zip(sub_target.birds_x, sub_target.birds_y)])\n    \n    print(df_metrics.mean())\n    \n    \nw_nocall = 0.56\nsc1 = row_wise_micro_averaged_f1_score(sub_target[sub_target.birds_x=='nocall']['birds_x'], sub_target[sub_target.birds_x=='nocall']['birds_y'])\nsc2 = row_wise_micro_averaged_f1_score(sub_target[sub_target.birds_x!='nocall']['birds_x'], sub_target[sub_target.birds_x!='nocall']['birds_y'])\n\nfinal_score = w_nocall*sc1 + (1-w_nocall)*sc2\nfinal_score, sc1, sc2\n\n\nprint(f\"Your LB will be around {final_score} | Other Metrics {sc1}, {sc2}\")","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:31:43.649032Z","iopub.execute_input":"2021-05-26T16:31:43.649344Z","iopub.status.idle":"2021-05-26T16:31:43.719631Z","shell.execute_reply.started":"2021-05-26T16:31:43.649314Z","shell.execute_reply":"2021-05-26T16:31:43.715114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TARGET_PATH, CFG.train_soundscape\n\n# Your LB will be around 0.6951567092844804 | Other Metrics 0.9437540876389673, 0.37876004592422424","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:31:49.34039Z","iopub.execute_input":"2021-05-26T16:31:49.340812Z","iopub.status.idle":"2021-05-26T16:31:49.345872Z","shell.execute_reply.started":"2021-05-26T16:31:49.34078Z","shell.execute_reply":"2021-05-26T16:31:49.344588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sub1.birds.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:31:49.542743Z","iopub.execute_input":"2021-05-26T16:31:49.543122Z","iopub.status.idle":"2021-05-26T16:31:49.54849Z","shell.execute_reply.started":"2021-05-26T16:31:49.543074Z","shell.execute_reply":"2021-05-26T16:31:49.54691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_scores(preds_th, ):\n    \n    # load ground truth\n    train_sc = pd.read_csv(CFG.train_soundscape)\n    # merge with preds\n    train_preds = preds_th.merge(train_sc[['row_id', 'birds']], on='row_id')\n    train_preds.columns = ['row_id', 'birds', 'label']\n    # train_preds.head(3)\n    \n    print(f'Competition Metric - Validation score on {len(all_audios)} Train soundscape clips\\n')\n    print()\n    print(f'Row-wise F1 [th={THRES}]:', np.round(row_wise_micro_averaged_f1_score(train_preds.label, train_preds.birds), 6) )\n    print(f'Fbeta-sklearn [th={THRES}]:', np.round(fbeta_score(train_preds.label, train_preds.birds, average='micro', beta=1), 6) )\n#     print(f'F1-sklearn [th={THRES}]:', np.round(metrics.f1_score(train_preds.label, train_preds.birds, average='samples'), 6) )\n    print('-'*30)\n    print('macro-precision:', np.round(precision_score(train_preds.label, train_preds.birds, average='macro'), 6) )\n    print('micro-precision:', np.round(precision_score(train_preds.label, train_preds.birds, average='micro'), 6) )\n    print('weighted-precision:', np.round(precision_score(train_preds.label, train_preds.birds, average='weighted'), 6) )\n    print()\n    print('macro-recall:', np.round(recall_score(train_preds.label, train_preds.birds, average='macro'), 6) )\n    print('micro-recall:', np.round(recall_score(train_preds.label, train_preds.birds, average='micro'), 6) )\n    print('weighted-recall:', np.round(recall_score(train_preds.label, train_preds.birds, average='weighted'), 6) )\n    print()\n    print('ACC:', np.round(metrics.accuracy_score(train_preds.label, train_preds.birds), 6) )","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:31:49.744875Z","iopub.execute_input":"2021-05-26T16:31:49.745281Z","iopub.status.idle":"2021-05-26T16:31:49.757445Z","shell.execute_reply.started":"2021-05-26T16:31:49.745248Z","shell.execute_reply":"2021-05-26T16:31:49.75595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nprint(f'CV Scores for Threshold: {THRES} \\n')\nget_scores(sub2)\n\n# print('Model 2')\n# get_scores(train_preds2_th)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T16:31:50.805566Z","iopub.execute_input":"2021-05-26T16:31:50.806822Z","iopub.status.idle":"2021-05-26T16:31:51.112268Z","shell.execute_reply.started":"2021-05-26T16:31:50.806771Z","shell.execute_reply":"2021-05-26T16:31:51.111064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}