{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Heart Failure Prediction\n## K-Nearest Neighbors vs Random Forest vs Naive Bayes\n\nHello, in this notebook we will compare the accuracy between 3 classifier algorithms in predicting heart failure based on clinical data.\nFirst, we import the relevant libraries."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we load the CSV file and inspect the dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv')\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we assign the feature columns as variable X, and label column as Y. This establishes the ground truth for evaluating the predicted values later on."},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_df=df[['age','anaemia','creatinine_phosphokinase','diabetes','ejection_fraction','high_blood_pressure','platelets','serum_creatinine','serum_sodium','sex','smoking','time']]\nX=np.asarray(feature_df)\nY=y=np.asarray(df['DEATH_EVENT'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We split the dataframe into training dataset and testing dataset. We will allocate 30% of the dataframe as test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train Test Split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### K Nearest Neighbors\n\nWe will attempt to obtain the most optimum number for nearest neighbors, and obtain the highest accuracy score."},{"metadata":{"trusted":true},"cell_type":"code","source":"e_knn=np.zeros(100);\nfor i in range(0,len(e_knn)):\n    knn_model = KNeighborsClassifier(n_neighbors=i+1)\n    knn_model.fit(X_train, y_train)\n    yh_knn=knn_model.predict(X_test)\n    e_knn[i]=accuracy_score(y_test, yh_knn)\n    \nprint(\"KNN Prediction Accuracy Score: \",np.round(e_knn.max(),3),' with N = ',e_knn.argmax()+1)\nplt.plot(np.arange(1,101),e_knn)\nplt.plot(e_knn.argmax()+1,e_knn.max(),'or')\nplt.title('KNN Accuracy Score')\nplt.xlabel('N')\nplt.ylabel('Accuracy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest\n\nOne of the most widely used algorithms for classifying labels. We will loop through 10-110 to obtain the most optimum number of estimators parameter."},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model = RandomForestClassifier(criterion='gini')\nrf_model.fit(X_train, y_train)\nyh_rf=rf_model.predict(X_test)\ne_rf=accuracy_score(y_test, yh_rf)\n    \nprint(\"Random Forest Prediction Accuracy Score: \",np.round(e_rf.max(),3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Naive Bayes\n\nAnd lastly we model our prediction with Naive Bayes algorithm. The parameter to adjust will be the smoothing variable. This usually is done within a logarithmic space."},{"metadata":{"trusted":true},"cell_type":"code","source":"e_gnb=np.zeros(100)\nparams_NB = np.logspace(0,-9, num=100)\nfor i in range(0,len(params_NB)):\n    gnb_model = GaussianNB(var_smoothing=params_NB[i])\n    gnb_model.fit(X_train, y_train)\n    yh_gnb=gnb_model.predict(X_test)\n    e_gnb[i]=accuracy_score(y_test, yh_gnb)\n    \nprint(\"Naive Bayes Prediction Accuracy Score: \",np.round(e_gnb.max(),3),' with Smoothing = ',params_NB[e_gnb.argmax()])\nplt.plot(params_NB,e_gnb,'.-')\nplt.plot(params_NB[e_gnb.argmax()],e_gnb.max(),'or')\nplt.title('Naive Bayes Accuracy Score')\nplt.xscale('log')\nplt.xlabel('Var Smoothing')\nplt.ylabel('Accuracy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.bar(['K-Nearest Neighbors','Random Forest','Naive Bayes'],[e_knn.max(),e_rf.max(),e_gnb.max()])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can clearly see above that Random Forest algorithm provides the highest accuracy in predicting heart failure cases (>90% accuracy)."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}