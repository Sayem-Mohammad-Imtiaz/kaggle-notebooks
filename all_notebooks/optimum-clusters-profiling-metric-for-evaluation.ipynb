{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Business Problem"},{"metadata":{},"cell_type":"markdown","source":"This case requires to develop a customer segmentation to define marketing strategy. The sample dataset summarizes the usage behavior of about 9000 active credit card holders during the last 6 months. The file is at a customer level with 18 behavioral variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cust_data = pd.read_csv('/kaggle/input/ccdata/CC GENERAL.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cust_data.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cust_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Inspection"},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert column names to lower case for easy interpretation\ncust_data.columns = cust_data.columns.str.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cust_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cust_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Create important derived variables\n\ndef month_avg_purchase(prch, tenure):\n    return prch/tenure\n\ndef month_cash_advance(cash, tenure):\n    return cash/tenure\n\ndef monthly_usage(blnc, limit):\n    return blnc/limit\n\ndef prch_type(x, y):\n    if ((x <= 0) & (y <= 0)):\n        return 'none'\n    elif((x > 0) & (y <= 0)):\n        return 'one_off'\n    elif((x <= 0) & (y > 0)):\n        return 'installments'\n    elif((x > 0) & (y > 0)):\n        return 'both'\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cust_data['monthly_avg_purchase'] = cust_data.apply(lambda x : month_avg_purchase(x['purchases'], x['tenure']), axis = 1)\ncust_data['monthly_cash_advance'] = cust_data.apply(lambda x : month_cash_advance(x['cash_advance'], x['tenure']), axis = 1)\ncust_data['monthly_usage'] = cust_data.apply(lambda x : monthly_usage(x['balance'], x['credit_limit']), axis = 1)\ncust_data['purchase_type'] = cust_data.apply(lambda x : prch_type(x['oneoff_purchases'], x['installments_purchases']), axis = 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cust_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Renaming the columns for better undersatnding\n#purchases_frequency - freq of months wth atleast 1 purcase\n#balance_frequency - balance in last 12months/ balance\n\ncust_data.rename(columns = {'balance' : 'avg_monthly_balance'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cust_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Profiling"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas_profiling\ncust = cust_data.profile_report()\n#cust.to_file(output_file = 'cust_segmentation_profile.html')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cust","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From here we can see that Purchases is highly correlated with one-off purchases variable"},{"metadata":{},"cell_type":"markdown","source":"### Identify Categorical and continous variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"cust_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_var_names=[key for key in dict(cust_data.dtypes) if dict(cust_data.dtypes)[key] in ['float64', 'int64', 'float32', 'int32']]\ncat_var_names=[key for key in dict(cust_data.dtypes) if dict(cust_data.dtypes)[key] in ['object']]\nprint(numeric_var_names)\nprint(cat_var_names)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"cust_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data audit report"},{"metadata":{"trusted":true},"cell_type":"code","source":"def continous_var_summary(x):\n    return pd.Series([x.count(), x.isnull().sum(), x.sum(), x.mean(), x.median(),x.std(),\n                      x.var(), x.min(), x.quantile(0.01), x.quantile(0.05), x.quantile(0.10), x.quantile(0.25),\n                      x.quantile(0.50), x.quantile(0.75), x.quantile(0.90), x.quantile(0.95), \n                      x.quantile(0.99), x.max()],\n                    index = ['N', 'NMiss', 'Sum', 'Mean', 'Median', 'SD', 'Var', 'Min', 'P1', 'P5', 'P10','P25',\n                            'P50', 'P75', 'P90', 'P95', 'P99', 'Max'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def categorical_var_summary(x):\n    Mode = x.value_counts().sort_values(ascending = False)[0:1].reset_index()\n    return pd.Series([x.count(), x.isnull().sum(), Mode.iloc[0, 0], Mode.iloc[0,1],\n                     round((Mode.iloc[0,1]*100)/x.count(), 2)],\n                     index = ['N', 'NMiss', 'Mode', 'Freq', 'Percent'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cust_data[numeric_var_names].apply(lambda x : continous_var_summary(x)).T.round(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cust_data[cat_var_names].apply(lambda x : categorical_var_summary(x)).T.round(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning"},{"metadata":{},"cell_type":"markdown","source":"#### 1. Outlier treatment"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Handling Outliers - at 99%tile or 95%tile if required \ndef outlier_capping(x):\n    x = x.clip(upper=x.quantile(0.95))\n    x = x.clip(lower=x.quantile(0.05))\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cust_data[numeric_var_names]=cust_data[numeric_var_names].apply(lambda x: outlier_capping(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2. Missing value imputation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Handling missings \ndef Missing_imputation(x):\n    x = x.fillna(x.mean())\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cust_data[numeric_var_names]=cust_data[numeric_var_names].apply(lambda x: Missing_imputation(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cust_data[numeric_var_names].apply(lambda x : continous_var_summary(x)).round(3).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Correlation Matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cust_corr = cust_data.corr()\n#cust_corr.to_excel('cust_corr.xlsx')\nsns.heatmap(cust_corr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. purchases with one off & monthly_avg_purchase\n2. one-off with monthly_avg_purchase\n3. cahs_adv wth monthly_cash advance"},{"metadata":{},"cell_type":"markdown","source":"We can drop purchases column and cash_adv based on our finding from correlation matrix"},{"metadata":{},"cell_type":"markdown","source":"#### 3. Dummy variable creation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#function for creating dummy variable\ndef create_dummies(df, colname):\n    col_dummies = pd.get_dummies(df[colname], prefix = colname, drop_first= True)\n    df = pd.concat([df, col_dummies], axis = 1)\n    df.drop(colname, axis = 1, inplace= True)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_var_names = cust_data[cat_var_names].columns.difference(['cust_id'])\ncat_var = cust_data[cat_var_names]\ncat_var.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for c_feature in cat_var_names:\n    cat_var[c_feature] = cat_var[c_feature].astype('category')\n    cat_var = create_dummies(cat_var, c_feature)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_var.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4. Dropping unecessary variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"#As cust_id is unique and have no variance so it may add unecessary noise to our data. Hence we need to drop it\n\ncust_data.drop(columns=['cust_id'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Combining numeric and categorical data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_final = pd.concat([cust_data[numeric_var_names], cat_var], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Standardizing the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_final.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prescreening of variables to remove less useful vraiable for segmentation\ndata_feature = data_final.drop(columns=['purchases', 'cash_advance'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_final_scaled = pd.DataFrame(sc.fit_transform(data_feature))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_final_scaled.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Applying PCA to reduce the variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=21)\npca.fit(data_final_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.explained_variance_ ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The amount of variance that each PC explains\nvar = pca.explained_variance_ratio_\nvar","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cummilative var explained\nvar1 = np.cumsum(np.round(pca.explained_variance_ratio_ , decimals=4)*100)\nvar1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({'Eigen Values' : pca.explained_variance_, 'Cumulative Variance' : var1}, index=range(1,22))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"either 7 or 8 seems a perfect candidate for no of componenets in our clustering model"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_final = PCA(n_components=7).fit(data_final_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_final.explained_variance_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduced_cr = pca_final.fit_transform(data_final_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dimensions = pd.DataFrame(reduced_cr)\ndimensions.columns = [\"C1\", \"C2\", \"C3\", \"C4\", \"C5\",\"C6\", \"C7\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dimensions.shape)\nprint(dimensions.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clustering model(k-means)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"km_3 = KMeans(n_clusters=3, random_state=123)\nkm_3.fit(dimensions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"km_4 = KMeans(n_clusters=4, random_state=123).fit(dimensions)\nkm_5 = KMeans(n_clusters=5, random_state=123).fit(dimensions)\nkm_6 = KMeans(n_clusters=6, random_state=123).fit(dimensions)\nkm_7 = KMeans(n_clusters=7, random_state=123).fit(dimensions)\nkm_8 = KMeans(n_clusters=8, random_state=123).fit(dimensions)\nkm_9 = KMeans(n_clusters=9, random_state=123).fit(dimensions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the cluster labels and sort by cluster\ndata_final['cluster_3'] = km_3.labels_\ndata_final['cluster_4'] = km_4.labels_\ndata_final['cluster_5'] = km_5.labels_\ndata_final['cluster_6'] = km_6.labels_\ndata_final['cluster_7'] = km_7.labels_\ndata_final['cluster_8'] = km_8.labels_\ndata_final['cluster_9'] = km_9.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_final.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Segment Size check"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_final['cluster_3'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_final['cluster_3'].value_counts()/sum(data_final['cluster_3'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_final['cluster_4'].value_counts()/sum(data_final['cluster_4'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_final['cluster_5'].value_counts()/sum(data_final['cluster_5'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_final['cluster_6'].value_counts()/sum(data_final['cluster_6'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can choose 5-6 cluster solution as optimum solution"},{"metadata":{},"cell_type":"markdown","source":"# Quantitative Evaluation of model"},{"metadata":{},"cell_type":"markdown","source":"### 1. Silhouette Coefficient(Higher the better)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import  metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.silhouette_score(dimensions, labels=km_3.labels_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k_range = range(2, 16)\nscores = []\nfor k in k_range:\n    km = KMeans(n_clusters=k, random_state=123)\n    km.fit(dimensions)\n    scores.append(metrics.silhouette_score(dimensions, labels=km.labels_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot( k_range, scores)\nplt.xlabel('Number of clusters')\nplt.ylabel('Silhouette Score')\nplt.grid('True')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here 5 cluter seems optimal solution as the sc score is highest around it."},{"metadata":{},"cell_type":"markdown","source":"### 2. Elbow Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_range= range(2, 20)\nerrors = []\n\nfor num_clusters in cluster_range:\n    clusters = KMeans( num_clusters )\n    clusters.fit(dimensions)\n    errors.append(clusters.inertia_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"errors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clusters_df = pd.DataFrame({'Cluster_no' : range(2, 20), 'Unexpalined_variance' : errors})\nclusters_df[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nplt.plot(clusters_df.Cluster_no, clusters_df.Unexpalined_variance, marker = 'o')\nplt.xlabel('No of clusters')\nplt.ylabel('Unexplained Variance(error)')\nplt.grid('True')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here after cluster 7 the incremental decrease in error is almost constant"},{"metadata":{},"cell_type":"markdown","source":"# Qualitative Analysis(Profiling)"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"size = pd.concat([pd.Series(data_final.cluster_3.size), pd.Series(data_final.cluster_3.value_counts()).sort_index(), pd.Series(data_final.cluster_4.value_counts()).sort_index(),\n          pd.Series(data_final.cluster_5.value_counts()).sort_index(), pd.Series(data_final.cluster_6.value_counts()).sort_index(), pd.Series(data_final.cluster_7.value_counts()).sort_index(),\n          pd.Series(data_final.cluster_8.value_counts()).sort_index(), pd.Series(data_final.cluster_9.value_counts()).sort_index()])\nsize","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Seg_size = pd.DataFrame(size, columns=['seg_size'])\nSeg_pct = pd.DataFrame(size/data_final.cluster_3.size, columns= ['Seg_pct'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.concat([Seg_size.T, Seg_pct.T], axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mean value gives a good indication of the distribution of data. So we are finding mean value for each variable for each cluster\nProfiling_output = pd.concat([data_final.apply(lambda x: x.mean()).T, data_final.groupby('cluster_3').apply(lambda x : x.mean()).T,\n                             data_final.groupby('cluster_4').apply(lambda x : x.mean()).T, data_final.groupby('cluster_5').apply(lambda x : x.mean()).T, \n                             data_final.groupby('cluster_6').apply(lambda x : x.mean()).T, data_final.groupby('cluster_7').apply(lambda x : x.mean()).T,\n                             data_final.groupby('cluster_8').apply(lambda x : x.mean()).T, data_final.groupby('cluster_9').apply(lambda x : x.mean()).T], axis =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Profiling_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Profiling_output_final=pd.concat([Seg_size.T, Seg_pct.T, Profiling_output], axis=0)\nProfiling_output_final.columns = ['Overall', 'KM3_1', 'KM3_2', 'KM3_3',\n                                'KM4_1', 'KM4_2', 'KM4_3', 'KM4_4',\n                                'KM5_1', 'KM5_2', 'KM5_3', 'KM5_4', 'KM5_5',\n                                'KM6_1', 'KM6_2', 'KM6_3', 'KM6_4', 'KM6_5','KM6_6',\n                                'KM7_1', 'KM7_2', 'KM7_3', 'KM7_4', 'KM7_5','KM7_6','KM7_7',\n                                'KM8_1', 'KM8_2', 'KM8_3', 'KM8_4', 'KM8_5','KM8_6','KM8_7','KM8_8',\n                                'KM9_1', 'KM9_2', 'KM9_3', 'KM9_4', 'KM9_5','KM9_6','KM9_7','KM9_8', 'KM9_9']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Profiling_output_final","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Profiling_output_final.to_csv('Profiling_output1.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From profiling we conclude that *__8 cluster solution__* seems the best one. Below is the detailed characteristic description of each cluster for future marketing strategy-\n\n**Cluster 0** - These include customers with average credit limit who are mostly involved in one off type of purchases with their credit cards. They dont prefer cash transactions on their cards. They maybe targeted for offers on different partner outlets.\n\n**Cluster 1** - These are the customers with high credit limit who spend alot on purchases of both installment and one-off type. Amount and number of transactions are quite high for these card holders. As a result the balance is quite low for them.\n\n**Cluster 2** -  This cluster targets a group of customers who have a high balance and cash advances with low purchase frequency. We can assume that this customer segment uses their credit cards as a loan facility.\n\n**Cluster 3** - This cluster includes uninvolved customers which rarely use their cards and that also on small amount of purchases. Hence they have low minimun payments inspite of decent credit limit. We may target them to diffrenet market strategies like emi/installments purchases.\n\n**Cluster 4** - These customers are similar to cluster no. 2 but with lower balance and lower credit limit.\n\n**Cluster 5** - These customers purchase frequently with highest amount of installment purchases contrast of a lower cash advance percentage. They have lower credit limit maybe that is the reason for not spending on other type of services. Also they pay their bill on time compared to other customers.\n\n**Cluster 6** - These are the customers who frequently use all the services with high amount whether it be any kind of purchase or cash transactions. They have the highest credit limit and minimun payment. In short these are the involved customers.\n\n**Cluster 7** -  These customers are almost similar to cluster no. 5 but with higher minimum payment and they don't pay their bill on time."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}