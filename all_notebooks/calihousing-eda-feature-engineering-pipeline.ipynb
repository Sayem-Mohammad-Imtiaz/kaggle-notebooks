{"cells":[{"metadata":{},"cell_type":"markdown","source":"I finished working in this notebook. If you'd like to take a look at my further work on this dataset (mostly regarding optimizing predictive models), please go [here](https://www.kaggle.com/mateuszbagiski/calihousing-fine-tuning-ml-models)."},{"metadata":{},"cell_type":"markdown","source":"# 0. Import everything and load the data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Basic stuff\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nfrom tqdm import tqdm\n\n# Preprocessing\nfrom sklearn.model_selection import train_test_split as tts\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\n\n\n# Models, metrics etc\nfrom sklearn.linear_model import LinearRegression as LR\nfrom sklearn.tree import DecisionTreeRegressor as DTR\nfrom sklearn.ensemble import RandomForestRegressor as RFR\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.model_selection import cross_val_score as cvs\nimport tensorflow as tf\nfrom tensorflow.keras import models, layers, optimizers, utils, callbacks","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_dir = '../input/california-housing-prices/housing.csv'\ndata = pd.read_csv(housing_dir)\ndata.shape, data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"Let's take a rough look at the data, we're dealing with here"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.shape)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we a table with 20640 rows. Each rows contains information about one district in California, which includes its geographical location (longitude and latitude, ocean_proximity) as well as demographics (population, median_income) and residential buildings (housing_median_age, total_rooms, total_bedrooms, households, median_house_value).\n\nAll of these features are numeric with one exception, ocean_proximity, which is categorical. Let's see how many unique labels this feature contains.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['ocean_proximity'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see whether we have any missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 207 missing values in the total_bedrooms column. Obviously, we cannot feed NaNs (Not-a-Numbers) into a Machine Learning Model, so they need to be either removed or replaced with, for example, mean or median values. Later we will employ the latter strategy, but for now let's continue to explore the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Maximal values for columns median_house_value and median_income (which, by the way, seems to display income in tens of thousands of dollars rather than single dollars) are suspiciously \"rounded\" at values almost equal to 500000 and 15, respectively. This could mean that in the original data there were rows with values higher than that, but someone considered them to be outliers and \"rounded down\" to a pre-set maximum value. This is potentially disadvantegous for ML model's performance and/or could result in abnormally huge amount of rows containing these maximal values in their respective columns.\n\nWe can see that that's the case, if we plot the distributions of each numerical feature with a histogram. Look at these \"skyscrappers\" at the right:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.hist(bins=50, figsize=(20,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There seem not to have been to many outliers in terms of median_income, but quite the opposite for median_house_value and apparently also housing_median_age, which was \"capped\" at the value of 52.\n\nIn order to clean our data, we will later remove all the rows which satisfy at least one of the below criteria:\n\n1) median_income equal to or greater than 15\n\n2) median_house_value equal to or greater than 500000\n\n3) housing_median_age equal to or greater than 52\n\nLet's look at how these features correlate with one another"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_mat = data.corr()\ncorr_mat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"More particularly, we are interested here with correlation between median_house_value and other features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_mhv = corr_mat['median_house_value'].sort_values()\ncorr_mhv","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aside from from median_income, these correlations are rather weak, although taken together may turn out to be good predictors of median_house_value. However, we can also perform Feature Engineering to obtain new useful information, absent from the original data, which may turn out to be highly correlated with median_house_value.\n\nNote that correlation matrices do not take into account categorical features such as ocean_proximity. We can use Seaborn's violinplot function to see whether there is some kind of relationship between this label and median_house_value.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,10))\n\nsns.violinplot(\n    x='ocean_proximity', y='median_house_value',\n    inner='box',\n    data=data, ax=ax\n)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lo and behold, there it is. Although in each of these categories we can find a district with median_house_value of above 500000, the means (marked by small white dots inside the inner boxplots) are much smaller for INLAND category than for other for categories. The overall distributions of values also differ significantly, which is clearly seen on both the outer violinplots and the inner boxplots.\n\nWe can also print this information for each individual label as a DataFrame: "},{"metadata":{"trusted":true},"cell_type":"code","source":"ocean_proximity_df = {\n    label: data.query(' `ocean_proximity` == @label ')['median_house_value'].describe()\n    for label in set(data['ocean_proximity'].values)\n}\n\nocean_proximity_df = pd.DataFrame(ocean_proximity_df).round(1)\n\nocean_proximity_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we know what kind of data, we are dealing with, we can start cleaning it."},{"metadata":{},"cell_type":"markdown","source":"# 2. Data Cleaning"},{"metadata":{},"cell_type":"markdown","source":"## 2.1. Dealing with NaNs"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 207 NaNs (not-a-numbers, a.k.a. missing values) in the total_bedrooms column. We will replace them with the median number of bedrooms:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Before: %i NaNs\" % data['total_bedrooms'].isnull().sum())\n\ndata['total_bedrooms'].fillna(data['total_bedrooms'].median(), inplace=True)\n\nprint(\"After: %i NaNs\" % data['total_bedrooms'].isnull().sum())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.2. Removing \"lines\" and \"cappings\""},{"metadata":{"trusted":true},"cell_type":"code","source":"data.hist(bins=50, figsize=(20,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These \"skyscrappers\" at the right end of median_house_value and housing_median_age histograms clearly indicate, that this data was capped â€“ the most expensive households were \"rounded down\" to the value of $500.000. Similarly for households older than 50 years. Since we cannot retrieve the original values, we need to get rid of this instances:"},{"metadata":{},"cell_type":"markdown","source":"## 2.2.1. The ordinary way\n\nWe could do it just by dropping these rows with a .drop() method..."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Before: %i datapoints.\" % data.shape[0])\n\ndata_cleaned_1 = data.drop(\n    index = data.query(' `median_house_value` >= 500000 | `housing_median_age` >= 52 | `median_income` >= 15 ').index.values,\n    inplace = False\n)\n\nprint(\"After: %i datapoints.\" % data_cleaned_1.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"(we've lost over 10% of our original data)"},{"metadata":{},"cell_type":"markdown","source":"... or we could use a custom transformer, which would allow us to include this step in a pipeline:"},{"metadata":{"trusted":true},"cell_type":"code","source":"class OutlierRemover(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        X = X.copy() # To make sure that we don't change the original DataFrame\n        X_cleaned = X.drop(index = X.query(' `median_house_value` >= 500000 | `housing_median_age` >= 52 | `median_income` >= 15 ').index.values)\n        return X_cleaned\n    \noutlier_remover = OutlierRemover()\ndata_cleaned_2 = outlier_remover.transform(data)\ndata_cleaned_2.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can check that the two ways are perfectly equivalent:"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.all(data_cleaned_1 == data_cleaned_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = outlier_remover.transform(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is one more data-cleaning problem left:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(\n    x='median_income', y='median_house_value',\n    alpha=.4,\n    data=data\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In a very suspicious way, datapoints tend to aggreagate along some precise, discrete values of median_house_value variable: 45.000, 35.000, 28.000... It looks like their original values were much more dispersed, but similar (though originally different) values were lumped together into discrete categories before being included in our data."},{"metadata":{"trusted":true},"cell_type":"code","source":"mhv_counts = data['median_house_value'].value_counts().sort_index()\nmhv_counts.loc[448000:452000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = mhv_counts.index\ny = mhv_counts.values\n\nplt.plot(x, y)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above plot clearly shows that there are many more such \"lumpings\" in the data. Removing this values, however would mean losing a lot of data, so I'm going to leave it there for now. \n\nPossible options:\n\n* Remove the \"lumpings\"\n* Add some random noise to this \"lumpings\""},{"metadata":{},"cell_type":"markdown","source":"# 3. Data encoding"},{"metadata":{},"cell_type":"markdown","source":"There is one categorical variable in the data: ocean_proximity. Since Machine Learning algorithms can deal only with data represented numerically, we need to perform one-hot encoding.\n\nFor that we will use Scikit-learn's OneHotEncoder:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data['ocean_proximity'].value_counts())\n\nencoder = OneHotEncoder()\n#data_op = data[['ocean_proximity']]\nop_ohe = encoder.fit_transform(data[['ocean_proximity']]).toarray()\n\nop_ohe, op_ohe.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for category_i, category in enumerate(encoder.categories_[0]):\n    print(category_i, category, data['ocean_proximity'].value_counts()[category], op_ohe[:,category_i].sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for category_i, category in enumerate(encoder.categories_[0]):\n    data[category] = op_ohe[:, category_i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[:, 'ocean_proximity':]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus encoded categorical data could be easily fed to a machine learning model. I think, however, that there is a way to extract a more precise information regarding ocean proximity, which will make these one-hot-encoded data redundant. We will do that in section 4.3."},{"metadata":{},"cell_type":"markdown","source":"# 4. Feature Engineering\n\nHere we will create new features for our dataset by processing/combining the original ones. Hopefully they will turn out to be more predictive of the value we want to measure. i.e. median_house_value."},{"metadata":{},"cell_type":"markdown","source":"## 4.1. Simple combined features\n\nTotal number of rooms or bedroom in a given district don't tell us much by themselves. They much more informative when expressed in relation to this district's population or total number of households. Another possibly interesting featuer may be the fraction of all rooms being bedrooms.\n\n* rooms_per_household, bedrooms_per_household\n\n* rooms_per_person, bedrooms_per_person\n\n* bedrooms_fraction"},{"metadata":{},"cell_type":"markdown","source":"## 4.1.1. Combining features the vanilla way\n\nWe can add this features in a very simple way, one by one, like this:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_expanded_1 = data.copy()\n\ndata_expanded_1['rooms_per_household'] = data_expanded_1['total_rooms'] / data_expanded_1['households']\ndata_expanded_1['bedrooms_per_household'] = data_expanded_1['total_bedrooms'] / data_expanded_1['households']\n\ndata_expanded_1['rooms_per_person'] = data_expanded_1['total_rooms'] / data_expanded_1['population']\ndata_expanded_1['bedrooms_per_person'] = data_expanded_1['total_bedrooms'] / data_expanded_1['population']\n\ndata_expanded_1['bedrooms_fraction'] = data_expanded_1['total_bedrooms'] / data_expanded_1['total_rooms']\n\ndata_expanded_1['people_per_household'] = data_expanded_1['population'] / data_expanded_1['households']\n\ndata_expanded_1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.1.2. Combining features the \"fancy\" way\n\n... or we could use a custom transfomer, like the one below, based on the one from the second chapter of Hands-On Machine Learning notebook:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Names of the new features/columns\nnew_features = [\n    'rooms_per_household',\n    'bedrooms_per_household',\n    'rooms_per_person',\n    'bedrooms_per_person',\n    'bedrooms_fraction',\n    'people_per_household'\n]\nclass FeatExpander(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        X = X.copy()\n        X['rooms_per_household'] = X['total_rooms'] / X['households']\n        X['bedrooms_per_household'] = X['total_bedrooms'] / X['households']\n        X['rooms_per_person'] = X['total_rooms'] / X['population']\n        X['bedrooms_per_person'] = X['total_bedrooms'] / X['population']\n        X['bedrooms_fraction'] = X['total_bedrooms'] / X['total_rooms']\n        X['people_per_households'] = X['population'] / X['households']\n        return X\n    \nfeat_expander = FeatExpander()\ndata_expanded_2 = feat_expander.transform(data)\n\ndata_expanded_2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can check that the two ways of adding new features are perfectly equivalent:"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.all(data_expanded_1.index == data_expanded_2.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = feat_expander.fit_transform(data)\ndata.corr()['median_house_value']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"bedrooms_fraction is quite highly negatively correlated with median_house_value.\n\nrooms_per_person and rooms_per_household also may turn out to be useful predictors."},{"metadata":{},"cell_type":"markdown","source":"Transformations we performed so far can be combined into the following pipeline. We use ColumnTransformer to separate transformations performed on the categorical attribute (ocean_proximity) from those performed on the numerical attributes (all the rest).\n\n(the only difference is that using this pipeline we are going to drop the ocean_proximity attribute, but that's okay)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Re-load the original data\ndata_original = pd.read_csv(housing_dir)\n\n# Separate the numerical part of the data\ndata_num = data_original.drop('ocean_proximity', axis=1)\n\n# Names of the numerical attributes in the original data\nattribs_num = data_num.columns.tolist()\n\n# Name of the only categorical attribute in the original data\nattribs_cat = ['ocean_proximity']\n\n# I initialize an encoder here only to extract the list of labels in the same order in which they will be given later in the pipeline\nencoder = OneHotEncoder()\nencoder.fit(data[attribs_cat])\noh_labels = encoder.categories_[0].tolist()\n\n# Names of the attributes added by FeatExpander\nnew_features = [\n    'rooms_per_household',\n    'bedrooms_per_household',\n    'rooms_per_person',\n    'bedrooms_per_person',\n    'bedrooms_fraction',\n    'people_per_household'\n]\n\n# Names of columns needed for reconversion of the numpy array returned by column_transformer back into a DataFrame\ncolumns_tr = [*attribs_num, *new_features, *oh_labels]\n\n\nclass OutlierRemover(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        X = X.copy() # To make sure that we don't change the original DataFrame\n        X_cleaned = X.drop(index = X.query(' `median_house_value` >= 500000 | `housing_median_age` >= 52 | `median_income` >= 15 ').index.values)\n        return X_cleaned\n\n# I made my own imputer, because Scikit-learn's SimpleImputer returns a numpy array, whereas I prefer working on DataFrames\nclass MyImputer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        X = X.copy()\n        X['total_bedrooms'].fillna(value=X['total_bedrooms'].median(), inplace=True)\n        return X\n        \n\nclass FeatExpander(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        X = X.copy()\n        X['rooms_per_household'] = X['total_rooms'] / X['households']\n        X['bedrooms_per_household'] = X['total_bedrooms'] / X['households']\n        X['rooms_per_person'] = X['total_rooms'] / X['population']\n        X['bedrooms_per_person'] = X['total_bedrooms'] / X['population']\n        X['bedrooms_fraction'] = X['total_bedrooms'] / X['total_rooms']\n        X['people_per_households'] = X['population'] / X['households']\n        return X\n    \n# Another custom transformer - just to reconvert NumPy arrays returned by column_transformer back into a DataFrame\nclass DFConverter(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        X = X.copy()\n        X_df = pd.DataFrame(X)\n        X_df.columns = columns_tr #\n        return X_df\n\n# A pipeline for numerical attributes\npipeline_num = Pipeline([\n    ('imputer', MyImputer()),\n    ('feat_expander', FeatExpander()),\n])\n\n\ncolumn_transformer = ColumnTransformer([\n    ('num', pipeline_num, attribs_num), # For the numerical attributes\n    ('cat', OneHotEncoder(), attribs_cat), # For the categorical attribute\n])\n\npipeline_full = Pipeline([\n    ('outlier_remover', OutlierRemover()), # Remove the outliers\n    ('column_transformer', column_transformer), # Process the numerical attributes and the categorical attribute separately and concatenate them after\n    ('df_converter', DFConverter()) # Reconvert the concatenated NumPy array back into a DataFrame\n])\n\ndata_tr = pipeline_full.fit_transform(data_original)\ndata_tr.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2. hotspot_distance - A distance from an area of high price\n\nA quick look at the heatmap displaying median_house_value on a longitude/latitude plot indicates that there are two major \"centers\" distinguished by exceptionally high prices:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('white')\n\ndata.plot(\n    x='longitude', y='latitude',\n    kind='scatter', figsize=(10,7),\n    alpha=.4,\n    c='median_house_value', cmap=plt.get_cmap('jet'), colorbar=True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_mhv = data.corr()['median_house_value'].sort_values(ascending=False)\ncorr_mhv","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no significant simple, linear correlation between longitude and median_house_value. Latitude is weakly negatively correlated. However, if we plot longitude and latitude against median_house_value we can see that there is possibly a more complex, less-obvious, non-linear trend.\n\nThis can be especially evident, when we overlay a lineplot (in red) of mean median_house_value for all the districts placed along a particular longitude/latitude (in blue)."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('darkgrid')\n\nfig, ax = plt.subplots(1,2, figsize=(30,12))\n\nsns.scatterplot(\n    x = 'longitude', y = 'median_house_value',\n    alpha=.33,\n    data=data, ax=ax[0]\n)\nsns.lineplot(\n    x='longitude', y='median_house_value',\n    ci=None, color='red', linewidth=1, alpha=.8,\n    data=data, ax=ax[0]\n)\nax[0].set_title('Linear correlation: %.5f' % (data.corr().loc['median_house_value', 'longitude']))\n\nsns.scatterplot(\n    x = 'latitude', y = 'median_house_value',\n    alpha=.33,\n    data=data, ax=ax[1]\n)\nsns.lineplot(\n    x='latitude', y='median_house_value',\n    ci=None, color='red', linewidth=1, alpha=.8,\n    data=data, ax=ax[1]\n)\nax[1].set_title('Linear correlation: %.5f' % (data.corr().loc['median_house_value', 'latitude']))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, while there is no significant linear correlation between median_house_value and longitude, there seem to be two high-price regions: one around -122 and the another around -118 longitude. The same goes for 33 and 37 latitude. Moreover, a quick look at the previous map indicates that the two are not independent. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('white')\n\ndata.plot(\n    x='longitude', y='latitude',\n    kind='scatter', figsize=(10,7),\n    alpha=.4,\n    c='median_house_value', cmap=plt.get_cmap('jet'), colorbar=True\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So there two high-price regions, which I will further refer to as \"hotspots\":\n\n1) North-western (NW): around longitude -122 and latitude 37\n\n2) South-eastern (SE): around longitude -118 and latitude 35\n\nDistance from these hotspots may be highly correlated with median_house_value.\n\nWe could settle at these roughly estimated values. This, however does not satisfy us and we're going to search for them methodically.\n\nFirst, we will look for two longitude values, distances from which are most highly correlated with median_house_value. Then we will do the same for latitude. After that, we will combine the results into two pairs of coordinates, one for each high-price hotspot and perform a little fine-tuning."},{"metadata":{},"cell_type":"markdown","source":"### 4.2.1. Longitude"},{"metadata":{"trusted":true},"cell_type":"code","source":"hotspot_NW_long = [0, 0] # correlation, longitude\nhotspot_SE_long = [0, 0] # ^\ninter_hotspot_long = -120 # longitude\n\nwhile True:\n    data_NW = data.copy().query('longitude < @inter_hotspot_long')\n    data_SE = data.copy().query('longitude > @inter_hotspot_long')\n    \n    # hotspot_NW\n    for long_val in np.arange(-124, inter_hotspot_long, .01):\n        data_NW['hotspot_NW_long'] = data_NW['longitude'].apply(lambda x: abs(long_val-x))\n        correlation = data_NW.corr().loc['hotspot_NW_long', 'median_house_value']\n        if abs(correlation)>abs(hotspot_NW_long[0]):\n            hotspot_NW_long = [correlation, long_val]\n    \n    # hotspot_SE\n    for long_val in np.arange(inter_hotspot_long, -116, .01):\n        data_SE['hotspot_SE_long'] = data_SE['longitude'].apply(lambda x: abs(long_val-x))\n        correlation = data_SE.corr().loc['hotspot_SE_long', 'median_house_value']\n        if abs(correlation)>abs(hotspot_SE_long[0]):\n            hotspot_SE_long = [correlation, long_val]\n            \n    # inter_hotspot\n    new_inter_hotspot_long = (hotspot_NW_long[1]+hotspot_SE_long[1])/2\n    if new_inter_hotspot_long!=inter_hotspot_long:\n        inter_hotspot_long = new_inter_hotspot_long\n    else:\n        break\n\nprint(\"NW:\\t\", hotspot_NW_long)\nprint(\"SE:\\t\", hotspot_SE_long)\nprint(\"inter-hotspot:\\t\", inter_hotspot_long)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2.2. Latitude"},{"metadata":{"trusted":true},"cell_type":"code","source":"hotspot_NW_lat = [0, 0] # correlation, latitude\nhotspot_SE_lat = [0, 0] # ^\ninter_hotspot_lat = 36 # latitude\n\nwhile True:\n    data_NW = data.copy().query('latitude > @inter_hotspot_lat')\n    data_SE = data.copy().query('latitude < @inter_hotspot_lat')\n    \n    # hotspot_NW\n    for lat_val in np.arange(inter_hotspot_lat, inter_hotspot_lat+2, .01):\n        data_NW['hotspot_NW_lat'] = data_NW['latitude'].apply(lambda x: abs(lat_val-x))\n        correlation = data_NW.corr().loc['hotspot_NW_lat', 'median_house_value']\n        if abs(correlation)>abs(hotspot_NW_lat[0]):\n            hotspot_NW_lat = [correlation, lat_val]\n    \n    # hotspot_SE\n    for lat_val in np.arange(inter_hotspot_lat-2, inter_hotspot_lat, .01):\n        data_SE['hotspot_SE_lat'] = data_SE['latitude'].apply(lambda x: abs(lat_val-x))\n        correlation = data_SE.corr().loc['hotspot_SE_lat', 'median_house_value']\n        if abs(correlation)>abs(hotspot_SE_lat[0]):\n            hotspot_SE_lat = [correlation, lat_val]\n            \n    # inter_hotspot\n    new_inter_hotspot_lat = (hotspot_NW_lat[1]+hotspot_SE_lat[1])/2\n    if new_inter_hotspot_lat!=inter_hotspot_lat:\n        inter_hotspot_lat = new_inter_hotspot_lat\n    else:\n        break\n\nprint(\"NW:\\t\", hotspot_NW_lat)\nprint(\"SE:\\t\", hotspot_SE_lat)\nprint(\"inter-hotspot:\\t\", inter_hotspot_lat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2.3. Longitude + latitude"},{"metadata":{"trusted":true},"cell_type":"code","source":"hotspot_NW = [hotspot_NW_long[1], hotspot_NW_lat[1]] # longitude, latitude\nhotspot_SE = [hotspot_SE_long[1], hotspot_SE_lat[1]] # ^\n\ninter_hotspot = [inter_hotspot_long, inter_hotspot_lat] # ^\n\nnew_hotspot_NW = [0, 0, 0]   # correlation, longitude, latitue\nnew_hotspot_SE = [0, 0, 0]   # ^\n\n# hotspot_NW\ndata_NW = data.copy().query('longitude < @inter_hotspot[0] & latitude > @inter_hotspot[1]')\nfor long_val in tqdm(np.arange(hotspot_NW[0]-.5, hotspot_NW[0]+.5, .05)):\n    for lat_val in np.arange(hotspot_NW[1]-.5, hotspot_NW[1]+.5, .05):\n        data_NW['hotspot_NW_distance'] = data_NW.apply(lambda x: np.sqrt((x['longitude']-long_val)**2 + (x['latitude']-lat_val)**2), axis=1)\n        correlation = data_NW.corr().loc['hotspot_NW_distance', 'median_house_value']\n        if abs(correlation)>abs(new_hotspot_NW[0]):\n            new_hotspot_NW = [correlation, long_val, lat_val]\n            \n# hotspot_SE\ndata_SE = data.copy().query('longitude > @inter_hotspot[0] & latitude < @inter_hotspot[1]')\nfor long_val in tqdm(np.arange(hotspot_SE[0]-.5, hotspot_SE[0]+.5, .05)):\n    for lat_val in np.arange(hotspot_SE[1]-.5, hotspot_SE[1]+.5, .05):\n        data_SE['hotspot_SE_distance'] = data_SE.apply(lambda x: np.sqrt((x['longitude']-long_val)**2 + (x['latitude']-lat_val)**2), axis=1)\n        correlation = data_SE.corr().loc['hotspot_SE_distance', 'median_house_value']\n        if abs(correlation)>abs(new_hotspot_SE[0]):\n            new_hotspot_SE = [correlation, long_val, lat_val]\n\n# inter_hotspot\ninter_hotspot = [(new_hotspot_NW[1]+new_hotspot_SE[1])/2, (new_hotspot_NW[2]+new_hotspot_SE[2])/2]\n\nprint(\"inter_hotspot:\\t\", inter_hotspot)\nprint(\"NW:\\t\", new_hotspot_NW)\nprint(\"SE:\\t\", new_hotspot_SE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hotspot_NW = new_hotspot_NW[1:] # longitude, latitude\nhotspot_SE = new_hotspot_SE[1:] # ^\n\n\n# hotspot_NW\ndata_NW = data.copy().query('longitude < @inter_hotspot[0] & latitude > @inter_hotspot[1]')\nfor long_val in tqdm(np.arange(hotspot_NW[0]-.05, hotspot_NW[0]+.05, .01)):\n    for lat_val in np.arange(hotspot_NW[1]-.05, hotspot_NW[1]+.05, .01):\n        data_NW['hotspot_NW_distance'] = data_NW.apply(lambda x: np.sqrt((x['longitude']-long_val)**2 + (x['latitude']-lat_val)**2), axis=1)\n        correlation = data_NW.corr().loc['hotspot_NW_distance', 'median_house_value']\n        if abs(correlation)>abs(new_hotspot_NW[0]):\n            new_hotspot_NW = [correlation, long_val, lat_val]\n            \n# hotspot_SE\ndata_SE = data.copy().query('longitude > @inter_hotspot[0] & latitude < @inter_hotspot[1]')\nfor long_val in tqdm(np.arange(hotspot_SE[0]-.05, hotspot_SE[0]+.05, .01)):\n    for lat_val in np.arange(hotspot_SE[1]-.05, hotspot_SE[1]+.05, .01):\n        data_SE['hotspot_SE_distance'] = data_SE.apply(lambda x: np.sqrt((x['longitude']-long_val)**2 + (x['latitude']-lat_val)**2), axis=1)\n        correlation = data_SE.corr().loc['hotspot_SE_distance', 'median_house_value']\n        if abs(correlation)>abs(new_hotspot_SE[0]):\n            new_hotspot_SE = [correlation, long_val, lat_val]\n\n# inter_hotspot\ninter_hotspot = [(new_hotspot_NW[1]+new_hotspot_SE[1])/2, (new_hotspot_NW[2]+new_hotspot_SE[2])/2]\nprint(\"inter_hotspot:\\t\", inter_hotspot)\n\nprint(\"NW:\\t\", new_hotspot_NW)\nprint(\"SE:\\t\", new_hotspot_SE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hotspot_NW = new_hotspot_NW[1:] # longitude, latitude\nhotspot_SE = new_hotspot_SE[1:] # ^","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2.4. Extracting hotspot_distance"},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_hotspot_distance(long_val, lat_val):\n    NW_distance = np.sqrt((hotspot_NW[0]-long_val)**2 + (hotspot_NW[1]-lat_val)**2)\n    SE_distance = np.sqrt((hotspot_SE[0]-long_val)**2 + (hotspot_SE[1]-lat_val)**2)\n    return np.min([NW_distance, SE_distance])\n    \n\ndata['hotspot_distance'] = data.apply(lambda x: calculate_hotspot_distance(x['longitude'], x['latitude']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.corr().loc['median_house_value', 'hotspot_distance']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also create a heatmap, displaying not only the relative distance of each district from its nearest hotspot (marked by their hue), but also our calculated \"perfect\" locations of these idealized hotspots:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,1, figsize=(15,10))\n\ndata.plot(\n    x='longitude', y='latitude',\n    kind='scatter', figsize=(10,7),\n    alpha=.4,\n    c='hotspot_distance', cmap=plt.get_cmap('Spectral'), colorbar=True, ax=ax\n)\nax.scatter(x=hotspot_NW[0], y=hotspot_NW[1], marker='X', color='k')\nax.scatter(x=hotspot_SE[0], y=hotspot_SE[1], marker='X', color='k')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As it turns out, they are located right on the surface of the ocean."},{"metadata":{},"cell_type":"markdown","source":"## 4.3 ocean_distance - Calculating the distance from the ocean\n\nInstead of labeling each district as being located \"inland\", \"near bay\" etc..., we can simply calculate its distance to the closest district being located close to the body of water. This means that \"NEAR BAY\", \"NEAR OCEAN\" and \"ISLAND\" categories will automatically receive 0 value while in the cases of \"<1H OCEAN\" and \"INLAND\" categories we will search through the distances to all the districts lying close to a body of water and take the smallest distance.\n\nThe expectation is that this smallest distant to the ocean (which we will place in the new column ocean_distance) will be negatively correlated with the median_house_value.\n\nHowever, districts located along the shore are quite densely packed, so we don't lose much information by using only a small fraction, like 1/20, of these datapoints. I ran some tests and it turned out that the resulting correlation is even slightly higher (in absolute terms) than if we used all the datapoints."},{"metadata":{"trusted":true},"cell_type":"code","source":"tqdm.pandas()\n\nland_labels = ['<1H OCEAN', 'INLAND']\ndata_ocean = data.copy().query(' ocean_proximity == \"NEAR OCEAN\" | ocean_proximity == \"NEAR BAY\"').sample(frac=1/20, random_state=42) # skipping island districts, because they will obviously be always very far from the inland ones\ndef calculate_ocean_distance(long_val, lat_val):\n    data_ocean['district_distance'] = data_ocean.apply(lambda x: np.sqrt((long_val-x['longitude'])**2 + (lat_val-x['latitude'])**2), axis=1)\n    return data_ocean['district_distance'].min()\n\ndata['ocean_distance'] = data.progress_apply(lambda x: calculate_ocean_distance(x['longitude'], x['latitude']) if x['ocean_proximity'] in land_labels else 0, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.corr().loc['median_house_value', 'ocean_distance']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once again, we have a very high negative correlation. This feature is also much more correlated with median_house_value than most ocean_proximity-derived labels and we can safely assume that this makes them redundant."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.corr().loc['median_house_value', ['ocean_distance']+encoder.categories_[0].tolist()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, we can create a heatmap to see that the districts located closer to the ocean really have higher ocean_distance values:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,1, figsize=(15,10))\n\n\ndata.plot(\n    x='longitude', y='latitude',\n    kind='scatter', figsize=(10,7),\n    alpha=.4,\n    c='ocean_distance', cmap=plt.get_cmap('Spectral'), colorbar=True, ax=ax\n)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Normalization, the ultimate pipeline, and feature selection"},{"metadata":{},"cell_type":"markdown","source":"For our models to perform well, we should normalize the data, that is, transform the values so that they have mean equal to 0 and standard deviation equal to 1. This can ve easily done with Scikit-learn's StandardScaler.\n\nNote that we transform only the values of numerical features, not the categorical labels (one-hot-encoded ocean_proximity labels). We also don't want to normalize the target feature (median_house_value), since this is a regression task and we want to predict the value of this feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nnum_cols = [col for col in data.columns if col!='median_house_value' and col!='ocean_proximity' and col not in oh_labels]\ndata[num_cols] = scaler.fit_transform(data[num_cols])\ndata.describe().round(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the features transformed have now their mean equal to 0 and standard deviation (std) equal to 1."},{"metadata":{},"cell_type":"markdown","source":"Everything done so far can be combined into the following pipeline (except data selection and splitting):\n\nI changed the ordering slightly: I moved calculation of the ocean_distance to the beginning (right after the outliers removal), in order to make use of the ocean_proximity attribute (still present at that stage). This was much more convenient that the alternative. hotspot_distance is being computed in the numerical attributes pipeline, right after the expanded features, and followed by standard scaling applied to all numerical attributes, except the target attribute median_house_value. After that, all that's left is to combine all the processed numerical attributes with ocean_proximity-derived labels into a DataFrame with DFConverter."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Re-load the original data\ndata_original = pd.read_csv(housing_dir)\n\n# Separate the numerical part of the data\ndata_num = data_original.drop('ocean_proximity', axis=1)\n\n# Names of the numerical attributes in the original data\nattribs_num = [*data_num.columns.tolist(), 'ocean_distance'] # #####\n\n# Name of the only categorical attribute in the original data\nattribs_cat = ['ocean_proximity']\n\n# I initialize an encoder here only to extract the list of labels in the same order in which they will be given later in the pipeline\nencoder = OneHotEncoder()\nencoder.fit(data_original[attribs_cat])\noh_labels = encoder.categories_[0].tolist()\n\n# Names of the attributes added by FeatExpander\nnew_features = [\n    'rooms_per_household',\n    'bedrooms_per_household',\n    'rooms_per_person',\n    'bedrooms_per_person',\n    'bedrooms_fraction',\n    'people_per_household'\n]\n\n# Names of columns needed for reconversion of the numpy array returned by column_transformer back into a DataFrame\ncolumns_tr = [*attribs_num, *new_features, 'hotspot_distance', *oh_labels]\n\n\nclass OutlierRemover(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        X = X.copy() # To make sure that we don't change the original DataFrame\n        X_cleaned = X.drop(index = X.query(' `median_house_value` >= 500000 | `housing_median_age` >= 52 | `median_income` >= 15 ').index.values)\n        return X_cleaned\n\nclass MyImputer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        X = X.copy()\n        X['total_bedrooms'].fillna(value=X['total_bedrooms'].median(), inplace=True)\n        return X\n\nclass FeatExpander(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        X = X.copy()\n        X['rooms_per_household'] = X['total_rooms'] / X['households']\n        X['bedrooms_per_household'] = X['total_bedrooms'] / X['households']\n        X['rooms_per_person'] = X['total_rooms'] / X['population']\n        X['bedrooms_per_person'] = X['total_bedrooms'] / X['population']\n        X['bedrooms_fraction'] = X['total_bedrooms'] / X['total_rooms']\n        X['people_per_households'] = X['population'] / X['households']\n        return X\n\n        \nclass DFConverter(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        X = X.copy()\n        X_df = pd.DataFrame(X)\n        X_df.columns = columns_tr\n        return X_df\n    \nhotspot_NW = [-122.94, 37.04]\nhotspot_SE = [-118.915, 33.165]\ndef calculate_hotspot_distance(long_val, lat_val):\n    NW_distance = np.sqrt((hotspot_NW[0]-long_val)**2 + (hotspot_NW[1]-lat_val)**2)\n    SE_distance = np.sqrt((hotspot_SE[0]-long_val)**2 + (hotspot_SE[1]-lat_val)**2)\n    return np.min([NW_distance, SE_distance])\nclass HotspotDistanceCalculator(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        X = X.copy()\n        X['hotspot_distance'] = X.apply(lambda x: calculate_hotspot_distance(x['longitude'], x['latitude']), axis=1)\n        return X\n        \ntqdm.pandas()\nland_labels = ['<1H OCEAN', 'INLAND']\ndata_ocean = data_original.copy().query(' ocean_proximity == \"NEAR OCEAN\" | ocean_proximity == \"NEAR BAY\"').sample(frac=1/20, random_state=42) # skipping island districts, because they will obviously be always very far from the inland ones\ndef calculate_ocean_distance(long_val, lat_val):\n    data_ocean['district_distance'] = data_ocean.apply(lambda x: np.sqrt((long_val-x['longitude'])**2 + (lat_val-x['latitude'])**2), axis=1)\n    return data_ocean['district_distance'].min()\nclass OceanDistanceCalculator(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        X = X.copy()\n        X['ocean_distance'] = X.progress_apply(lambda x: calculate_ocean_distance(x['longitude'], x['latitude']) if x['ocean_proximity'] in land_labels else 0, axis=1)\n        return X\n        \nclass MyScaler(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        X = X.copy()\n        columns_to_norm = [col for col in X.columns if col!='median_house_value']\n        X[columns_to_norm] = StandardScaler().fit_transform(X[columns_to_norm])\n        return X\n        \n        \npipeline_num = Pipeline([\n    ('imputer', MyImputer()),\n    ('feat_expander', FeatExpander()),\n    ('hotspot_distance_calculator', HotspotDistanceCalculator()),\n    ('scaler', MyScaler())\n])        \n\ncolumn_transformer = ColumnTransformer([\n    ('num', pipeline_num, attribs_num),\n    ('cat', OneHotEncoder(), attribs_cat)\n])\n\npipeline_full = Pipeline([\n    ('outlier_remover', OutlierRemover()),\n    ('ocean_distance_calculator', OceanDistanceCalculator()),\n    ('column_transformer', column_transformer),\n    ('df_converter', DFConverter()),\n])\n\ndata_original = pd.read_csv(housing_dir)\n\ndata_tr = pipeline_full.fit_transform(data_original)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_tr.describe().round(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now have to decide, which features we are going to use in training our models.\n\nInitially, I wanted to retain only the features, which have absolute correlation with the target feature (median_house_value) greater than 0.1.\n\nDiscarding features with low correlation with the target value can speed up the training process, since it lower its computational load. While this usually is a desirable strategy when dealing with datasets with huge numbers of features (most of which probably are not very highly correlated with whatever it is we are trying to predict), in case of datasets with relatively few features (like this one), it can mean losing a significant amoun of information and thus lowering the predictive power of our models.\n\nTo inspect the relative significance of various features, I decided to make 5 sets of features (numbered from 0 to 1) and test the performance of several models, when trained on each of them. As it turns out, retaining all the features, including ones with very low absolute correlation with the target (below 0.1) or suspected to be redundant (one-hot-encoded ocean_proximity) can significantly lower the root mean squared error (RMSE, our metric).\n\nSo, I created the following splits:\n\n**Set 0**: The baseline split, only the original numerical attributes (no feature-engineering) and one-hot-encoded ocean_proximity labels.\n\n**Set 1**: Only the numerical features (including the engineered ones), without the ocean_proximity labels.\n\n**Set 2**: Replication of the work from the Hands-On Machine Learning handbook: original numerical features, 3 combined attributes (3 out of 6 from section 4.1 of this notebook) and one-hot-encoded ocean_proximity labels.\n\n**Set 3**: Set 1, but with the ocean_proximity labels retained.\n\n**Set 4**: All attributes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Information about correlation of each feature with median_house_value\ncorr_mhv = data_tr.corr()['median_house_value']\n\n# Set 0: only the original num + OH\nset_0 = [a for a in data_original.columns if a!='median_house_value' and a!='ocean_proximity']+oh_labels\n# Set 1: absolute correlation above 0.1 (excluding OH) - my original idea\nset_1 = [a for a in corr_mhv.index[:-5] if abs(corr_mhv[a])>.1 and a!='median_house_value']\n# Set 2: replication of handbook's - original numerical + OH + handbook's combined attributes\nset_2 = [a for a in data_original.columns if a!='median_house_value' and a!='ocean_proximity']+['rooms_per_household', 'people_per_household', 'bedrooms_fraction']+oh_labels\n# Set 3: absolute correlation above 0.1 + OH - my original idea, but without excluding OH\nset_3 = [a for a in corr_mhv.index[:-5] if abs(corr_mhv[a])>.1 and a!='median_house_value']+oh_labels\n# Set 4: all attributes\nset_4 = [a for a in corr_mhv.index.tolist() if a!='median_house_value']\n\nsets_all = [\n    set_0,\n    set_1,\n    set_2,\n    set_3,\n    set_4\n]\n\ndata_X_all = [ data_tr[set_] for set_ in sets_all]\n\ndata_y = data_tr['median_house_value']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Models\n\nIn an attempt to replicate the results reported in the Hands-On Machine Learning handbook, we will test the same 3 models provided by Scikit-learn (Linear Regression, Decision Tree Regressor and Random Forest Regressor) and use the same cross-validation with 10 splits. The performance measure is going to be the root mean squared error (RMSE)."},{"metadata":{},"cell_type":"markdown","source":"## 6.1. Linear Regression\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\tLinear Regression:\")\nlr_cvs_rmse = [] # a list the scores will be written into\nfor i, data_X in enumerate(data_X_all):\n    lr = LR()\n    rmse = np.sqrt(-cvs(lr, data_X, data_y, scoring='neg_mean_squared_error', cv=10, n_jobs=-1))\n    lr_cvs_rmse.append(rmse)\n    print(f\"Set {i}:\\tMean: {rmse.mean().round(2)}\\tStd: {rmse.std().round(2)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First observations:\n\n1. Model trained on Set 0 (baseline) performs the worst of all. This could be predicted.\n\n2. My original idea (Set 1), that is discarding features with absolute correlation below 0.1 and the one-hot-encoded ocean_proximity labels achieves better score (lower RMSE) than what was reported in the handbook (Set 2), so calculating hotspot_distance and/or ocean_distance seems to have been a good feature-engineering idea.\n\n3. However, without discarding features with low absolute correlation (Set 3) we get even better results. Also, we get the best results, when we don't discard any features and train our Linear Regression model on all of them (Set 4)."},{"metadata":{},"cell_type":"markdown","source":"## 6.2. Decision Tree Regressor"},{"metadata":{},"cell_type":"markdown","source":"Decision Tree Regressor is much less deterministic than  Linear Regression, which can be easily seen if we run the same code several times. Whereas Linear Regression converges on the same solution, when trained on each feature set, Decision Trees achieve much more stochastic results (with their default values)."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\tLinear Regression:\")\nfor i, data_X in enumerate(data_X_all):\n    print(f\"Set {i}:\")\n    for run_i in range(5):\n        lr = LR()\n        rmse = np.sqrt(-cvs(lr, data_X, data_y, scoring='neg_mean_squared_error', cv=10, n_jobs=-1))\n        print(f\"\\tRun {run_i}:\\tMean: {rmse.mean().round(2)}\\tStd: {rmse.std().round(2)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\tDecision Tree Regressor:\")\ndtr_cvs_rmse = []\nfor i, data_X in enumerate(data_X_all):\n    rmses = 0\n    print(f\"Set {i}:\")\n    for run_i in range(5):\n        dtr = DTR()\n        rmse = np.sqrt(-cvs(dtr, data_X, data_y, scoring='neg_mean_squared_error', cv=10, n_jobs=-1))\n        rmses += rmse\n        print(f\"\\tRun {run_i}:\\tMean: {rmse.mean().round(2)}\\tStd: {rmse.std().round(2)}\")\n    rmses /= 5\n    dtr_cvs_rmse.append(rmses)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"However, after averaging over all the runs, we will see that algorithms train on data, which includes the newly engineered features achieve better performance, compared to the baseline (Set 0) and the handbook approach (Set 2), although there does not seem to be a significant difference between Sets 1, 3, and 4."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\tDecision Tree Regressor:\")\nfor i, rmse in enumerate(dtr_cvs_rmse):\n    print(f\"Set {i}:\\tMean: {rmse.mean().round(2)}\\tStd: {rmse.std().round(2)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.3. Random Forest Regressor"},{"metadata":{},"cell_type":"markdown","source":"The same is true for random forests, which are just ensembles of decision trees."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\tRandom Forest Regressor:\")\nrfr_cvs_rmse = []\nfor i, data_X in enumerate(data_X_all):\n    rfr = RFR()\n    rmse = np.sqrt(-cvs(rfr, data_X, data_y, scoring='neg_mean_squared_error', cv=10, n_jobs=-1))\n    rfr_cvs_rmse.append(rmse)\n    print(f\"Set {i}:\\tMean: {rmse.mean().round(2)}\\tStd: {rmse.std().round(2)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.4. Keras Sequential Model\n\nLet's now test a simple neural network, consisting solely of l2-regularized Dense layers, with batch normalization and a little dropout between each layer. It takes quite a while to train, but achieves much better results than the previous models.\n\n10-fold cross-validation would take very long, so I decide just to train this model once on every feature set.\n\nOf course, we need to split the data into train and test set first."},{"metadata":{"trusted":true},"cell_type":"code","source":"def lr_scheduler(epoch, lr):\n    if epoch==110 or epoch==130:\n        return lr/3\n    else:\n        return lr\n\ncallbacks_list = [\n    callbacks.LearningRateScheduler(lr_scheduler),\n    callbacks.ReduceLROnPlateau(factor=.1, monitor='val_loss', patience=3),\n    #callbacks.ModelCheckpoint(filepath='model_best.h5', monitor='val_loss', save_best_only=True, save_freq='epoch'),\n]\n\ndef build_model(n_features):\n    model = models.Sequential(layers=[\n        layers.Dense(32, activation='relu', kernel_regularizer='l2', input_shape=(n_features,)),\n        layers.BatchNormalization(),\n        layers.Dropout(.1),\n        layers.Dense(64, activation='relu', kernel_regularizer='l2'),\n        layers.BatchNormalization(),\n        layers.Dropout(.1),\n        layers.Dense(64, activation='relu', kernel_regularizer='l2'),\n        layers.BatchNormalization(),\n        layers.Dropout(.1),\n        layers.Dense(64, activation='relu', kernel_regularizer='l2'),\n        layers.BatchNormalization(),\n        layers.Dropout(.1),\n        layers.Dense(64, activation='relu', kernel_regularizer='l2'),\n        layers.BatchNormalization(),\n        layers.Dropout(.1),\n        layers.Dense(1)\n    ])\n    return model\n    \n\ntrain_data, test_data = tts(data_tr, test_size=.1, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"histories = []\nmodels_ = [] # with an underscore (_), because 'models' name is taken by a Keras module\n\nfor i, set_ in tqdm(enumerate(sets_all)):\n    train_X, train_y = train_data[set_], train_data['median_house_value']\n    \n    model = build_model(n_features=train_X.shape[1])\n    \n    model.compile(\n        optimizer='rmsprop',\n        loss='mse',\n        metrics=['mae']\n    )\n\n    history = model.fit(\n        train_X, train_y,\n        validation_split=.1,\n        callbacks = callbacks_list,\n        epochs=150, batch_size=32,\n        shuffle=True,\n        verbose=0\n    )\n    \n    histories.append(history)\n    models_.append(model)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can plot the training process..."},{"metadata":{"trusted":true},"cell_type":"code","source":"history = histories[1]\nepochs = np.arange(1, len(history.history['loss'])+1)\nprint(\"epochs:\", len(epochs))\n\ntrain_loss = history.history['loss']\nval_loss = history.history['val_loss']\nplt.plot(epochs, train_loss, 'r-', label='train_loss')\nplt.plot(epochs, val_loss, 'g--', label='val_loss')\nplt.legend()\nprint(\"Training and validation loss:\")\nplt.show()\n\ntrain_mae = history.history['mae']\nval_mae = history.history['val_mae']\nplt.plot(epochs, train_mae, 'r-', label='train_mae')\nplt.plot(epochs, val_mae, 'g--', label='val_mae')\nplt.legend()\nprint(\"Training and validation MAE:\")\nplt.show()\n\nlr = history.history['lr']\nplt.plot(epochs, lr, 'b--', label='lr')\nplt.legend()\nprint(\"Learning rate:\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"... and evaluate each model's performance:"},{"metadata":{"trusted":true},"cell_type":"code","source":"nn_rmse = []\n\nprint(\"\\tNeural Network:\")\nfor i, set_ in enumerate(sets_all):\n    train_X, train_y = train_data[set_], train_data['median_house_value']\n    test_X, test_y = test_data[set_], test_data['median_house_value']\n    \n    model = models_[i]\n    \n    train_rmse = np.sqrt(model.evaluate(train_X, train_y, verbose=0)[0])\n    test_rmse = np.sqrt(model.evaluate(test_X, test_y, verbose=0)[0])\n    \n    nn_rmse.append(test_rmse)\n    \n    print(f\"Set {i}:\\tTrain: {train_rmse.round(2)}\\tTest: {test_rmse.round(2)}\")\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For some reason unbeknownst to me, including one-hot encodings completely disturbs this network (I tried normalizing them and it didn't help at all). Also, it seems that in this case discarding low-correlation features improves the model's performance, although the difference is very small, so it may not be significant after all."},{"metadata":{},"cell_type":"markdown","source":"To have it all in one place, we'll make a dataframe containing RMSE scores of all the models tested so far. For comparison, we can also include the RMSE reported in the handbook for models, which for some reason differ from we've obtained for Set 2 (in attempt to replicate these results)."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_cvs_rmse = [\n    lr_cvs_rmse,\n    dtr_cvs_rmse,\n    rfr_cvs_rmse\n]\n\nscores_df = pd.DataFrame({\n    f'Set {i}': [rmse[i].mean().round(2) for rmse in all_cvs_rmse]+[nn_rmse[i].round(2)] for i in range(len(sets_all))\n})\nscores_df['Handbook RMSE'] = [69052.46, 71407.69, 50182.30, None]\nscores_df.index = ['Linear Regression', 'Decision Tree Regressor', 'Random Forest Regressor', 'Neural Network']\n\nscores_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's it for now. I will continue to work on this dataset, mostly trying to fine-tune these models and/or find and fine-tune better ones in [this notebook](https://www.kaggle.com/mateuszbagiski/calihousing-fine-tuning-ml-models)."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}