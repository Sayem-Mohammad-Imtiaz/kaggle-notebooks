{"cells":[{"metadata":{"_uuid":"043e78ba82baf8748dcc07876d7b2f4a8a0678ee"},"cell_type":"markdown","source":"# Caso de Estudio: Riesgo Crediticio\n\n*DESCARGO DE RESPONSABILIDAD: esta es un planteamiento para problemas de clasificación. El texto que se documenta proviene de diferentes fuentes (descripción del conjunto de datos de kaggle, documentación de sklearn, documentación de matplotlib, wikipedia, etc.), para sustento del proyecto FEDU de la Universidad Nacional San Antonio Abad del Cusco.*\n\n## Table of content\n\n* [Descripción general del conjunto de datos](#ds)\n* [Análisis exploratorio](#explo)\n    * [Estadísticas descriptivas de préstamos PAGADOS](#descp)\n    * [Estadísticas descriptivas para préstamos INCUMPLIDOS](#descd)\n    * [INCUMPLIMIENTO en función del motivo de adquisición de los préstamos](#reason)\n    * [INCUMPLIMIENTO en función de la ocupación](#occupation)\n    * [Resumen gráfico](#graph)\n    * [Trama de violín](#violin)\n    * [Matriz de correlación](#corr)\n* [Prueba de clasificadores predeterminados](#classification)\n* [Evaluación del modelo](#eval)\n    * [Precisión y recuperación](#per)\n    * [F1](#f1)\n    * [Característica Operativa del Receptor](#roc)\n    * [Matriz de confusión](#confusion)\n    * [robabilidad de clasificación](#prob)\n* [Regresión logística](#logit)\n* [Clasificador SGD](#sgd)\n* [Clasificador de vectores de apoyo](#svc)\n* [Clasificador de aumento de gradiente](#gbrt)\n* [Bosque de árbol al azar](#frt)\n    * [Clasificador de bosque aleatorio](#rfc)\n    * [Árbol extremadamente aleatorio](#ert)\n* [Comparación y conclusión del modelo](#conclusion)\n\n## Visión General del Dataset\n<a id='ds'></a>\n\nEl conjunto de datos utilizado contiene información de referencia y de rendimiento de préstamos para 5,960 préstamos. El objetivo (BAD) es una variable binaria que indica si un solicitante finalmente incurrió en incumplimiento o en grave mora en alguna entidad bancaria. Y por muestreo rápido. Este resultado adverso se produjo en 1.189 casos (20%) del total de la muestra.\n\nPara cada aspirante a una tarjeta de crédito, se registraron 11 variables de entrada:\n\n> * BAD: 1 = candidato con préstamo incumplido o con mora; 0 = candidato que paga su deuda y no tiene registro negativo\n* LOAN: Monto de solicitud de préstamo\n* MORTDUE: Monto adeudado de la hipoteca existente\n* VALUE: Valor actual del bien o propiedad\n* REASON: DebtCon = consolidación de la deuda; HomeImp = mejoras para el hogar\n* JOB: Categorias ocupacionales o profesionales\n* YOJ: Años es esu trabajo actual\n* DEROG: Número de informes derogados o cancelados importantes\n* DELINQ: Número de lineas de crédito morosas\n* CLAGE: Antiguedad de la linea de crédito más antigua en meses\n* NINQ:Número de consultas crediticas recientes\n* CLNO: Número de líneas de crédito"},{"metadata":{"trusted":true,"_uuid":"a2ef2622d9d982f44f73097c44a0618969909c4c"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom pprint import pprint\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport holoviews as hv\nhv.extension('bokeh', 'matplotlib', logo=False)\n\n# Evitar que aparezcan advertencias\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99f7547021739168db64988549a9535a5bc06a72"},"cell_type":"code","source":"df=pd.read_csv('../input/hmeq.csv', low_memory=False) # Sin columnas duplicadas, sin columnas altamente correlacionadas\ndf.drop('DEBTINC', axis=1, inplace=True) # El significado de esta variable no está claro. Por lo que se excluye del planteamiento\ndf.dropna(axis=0, how='any', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"689f230ab38b172609ebbf62c0810b6403b2cb14"},"cell_type":"markdown","source":"## Análisis Exploratorio\n<a id='explo'></a>\n\nSe resume las principales características del conjunto de datos con métodos visuales y estadísticas resumidas. Se utiliza la variable objetivo (BAD) para dividir el conjunto de datos en submuestras y se busca específicamente variables, características y correlación que contengan poder de clasificación.\n\n### Estadísticas descriptivas de préstamos PAGADOS\n\n<a id='descp'></a>"},{"metadata":{"trusted":true,"_uuid":"7714ab305dd9e853ab266438144d91dc857d38c1"},"cell_type":"code","source":"df[df['BAD']==0].drop('BAD', axis=1).describe().style.format(\"{:.2f}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf50d833946ba558c3a5a85b9a02a7b08b60f068"},"cell_type":"markdown","source":"### Estadísticas descriptivas para préstamos INCUMPLIDOS\n<a id='descd'></a>"},{"metadata":{"trusted":true,"_uuid":"7ae114f0450eb4ca523d9dfb4b548d37e07bd769"},"cell_type":"code","source":"df[df['BAD']==1].drop('BAD', axis=1).describe().style.format(\"{:.2f}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15b6b46578e4bf2999ef1a750d086f8824809d28"},"cell_type":"markdown","source":"1. De las estadísticas descriptivas anteriores, podemos sacar la siguiente consideración:\n\n* El monto del préstamo solicitado, el monto de la hipoteca adeudada y el valor de la garantía subyacente son estadísticamente consistentes tanto para los préstamos que fueron PAGADOS como para los que resultaron en INCUMPLIMIENTO. Esto sugiere que esas variables pueden no proporcionar un poder de discriminación significativo para separar las dos clases.\n\n* El número de años en el trabajo actual (YOJ) parece discriminar las dos clases, ya que los INCUMPLIMIENTOS parecen más frecuentes en los contratistas que tienen una antigüedad más corta. Esta tendencia está respaldada por los cuantiles de valor promedio correspondientes, que indican una distribución sesgada hacia una antigüedad más corta.\n\n* Se aplican consideraciones similares a las variables relacionadas con el historial crediticio del contratista, tales como: el número de informes despectivos importantes (DEROG), el número de líneas de crédito morosas (DELINQ), la antigüedad de la línea de crédito más antigua en meses (CLAGE) y el número de consultas crediticias recientes (NINQ). En el caso de INCUMPLIMIENTO, la distribución de estas variables está sesgada hacia valores que sugieren una historia crediticia peor que la distribución correspondiente para los contratistas de préstamos PAGADOS.\n\n* Finalmente, el número de líneas de crédito abiertas (CLNO) parece estadísticamente consistente en ambos casos, lo que sugiere que esta variable no tiene un poder de discriminación significativo."},{"metadata":{"trusted":true,"_uuid":"f8539626c720ae398e548fa3ae509317aa8b5bf1"},"cell_type":"code","source":"df.loc[df.BAD == 1, 'STATUS'] = 'DEFAULT'\ndf.loc[df.BAD == 0, 'STATUS'] = 'PAID'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd6a0d12befc4ce3733c3f0c340e393bd512da9f"},"cell_type":"markdown","source":"### INCUMPLIMIENTO en función del motivo de adquisición de los préstamos\n<a id='reason'></a>\nLa fracción de préstamos PAGADOS e INCUMPLIDOS no parece depender en gran medida del motivo de la adquisición del préstamo. En promedio, se ha pagado el 80% de los préstamos, mientras que el 20% está en INCUMPLIMIENTO. La discrepancia del 2% observada no es estadísticamente significativa dada la cantidad de préstamos en el conjunto de datos.\n"},{"metadata":{"trusted":true,"_uuid":"b6ea91d888b48ee9274cd26f85456665fb293ce1"},"cell_type":"code","source":"g = df.groupby('REASON')\ng['STATUS'].value_counts(normalize=True).to_frame().style.format(\"{:.1%}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ae118a39513b77d1748e7c0aaeefd5f693e801b"},"cell_type":"markdown","source":"* ###  INCUMPLIMIENTO en función de la ocupación\n\n<a id='occupation'></a>\nLa fracción de préstamos PAGADOS e INCUMPLIDOS muestra cierta dependencia de la ocupación del contratista. Los trabajadores de oficina y los ejecutivos profesionales tienen la mayor probabilidad de pagar sus préstamos, mientras que los vendedores y los autónomos tienen la mayor probabilidad de impago. La ocupación muestra un buen poder de discriminación y probablemente será una característica importante de nuestro modelo de clasificación.\n"},{"metadata":{"trusted":true,"_uuid":"14c58774c8872b1e0a932d70dd121eebbbbac460"},"cell_type":"code","source":"g = df.groupby('JOB')\ng['STATUS'].value_counts(normalize=True).to_frame().style.format(\"{:.1%}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62bc0016776d966c4b7069677998f6cd634da494"},"cell_type":"code","source":"%%opts Bars[width=700 height=400 tools=['hover'] xrotation=45]{+axiswise +framewise}\n\n# Categorical\n\ncols = ['REASON', 'JOB']\n\ndd={}\n\nfor col in cols:\n\n    counts=df.groupby(col)['STATUS'].value_counts(normalize=True).to_frame('val').reset_index()\n    dd[col] = hv.Bars(counts, [col, 'STATUS'], 'val') \n    \nvar = [*dd]\nkdims=hv.Dimension(('var', 'Variable'), values=var)    \nhv.HoloMap(dd, kdims=kdims)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"145c45f430d806214a84bf90a0855bd0fb97d937"},"cell_type":"markdown","source":"### Resumen gráfico\n\n<a id='graph'></a>\nA continuación se muestra una descripción gráfica coherente del conjunto de datos. Para cada variable, muestro un histograma para todo el conjunto de datos, para los préstamos PAGADOS y DEFUALT, respectivamente. Las correlaciones entre variables también se resumen en diagramas de dispersión bidimensionales.\n"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"e6b5c5ed3ca124dfa07a73cd3137e845b6539f5d"},"cell_type":"code","source":"%%opts Histogram[width=700 height=400 tools=['hover'] xrotation=0]{+axiswise +framewise}\n\ng = df.groupby('STATUS')\n\ncols = ['LOAN',\n        'MORTDUE', \n        'VALUE',\n        'YOJ',\n        'DEROG',\n        'DELINQ',\n        'CLAGE',\n        'NINQ',\n        'CLNO']\ndd={}\n\n# Histograms\nfor col in cols:\n    \n    freq, edges = np.histogram(df[col].values)\n    dd[col] = hv.Histogram((edges, freq), label='TODOS los préstamos').redim.label(x=' ')\n    \n    freq, edges = np.histogram(g.get_group('PAID')[col].values, bins=edges)\n    dd[col] *= hv.Histogram((edges, freq), label='Préstamos PAGADOS').redim.label(x=' ')\n    \n    freq, edges = np.histogram(g.get_group('DEFAULT')[col].values, bins=edges)\n    dd[col] *= hv.Histogram((edges, freq), label='Préstamos INCUMPLIDOS' ).redim.label(x=' ')   \n    \nvar = [*dd]\nkdims=hv.Dimension(('var', 'Variable'), values=var)    \nhv.HoloMap(dd, kdims=kdims)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"bba53af90d733c7a8593cbe8e76538dce6a013e0"},"cell_type":"code","source":"%%opts Scatter[width=500 height=500 tools=['hover'] xrotation=0]{+axiswise +framewise}\n\ng = df.groupby('STATUS')\n\ncols = ['LOAN',\n        'MORTDUE',\n        'VALUE',\n        'YOJ',\n        'DEROG',\n        'DELINQ',\n        'CLAGE',\n        'NINQ',\n        'CLNO']\n\nimport itertools\nprod = list(itertools.combinations(cols,2))\n\ndd = {}\n\nfor p in prod:\n    dd['_'.join(p)] = hv.Scatter(g.get_group('PAID')[list(p)], label='Préstamos PAGADOS').options(size=5)\n    dd['_'.join(p)] *= hv.Scatter(g.get_group('DEFAULT')[list(p)], label='Préstamos INCUMPLIDOS').options(size=5, marker='x')\n    \nvar = [*dd]\nkdims=hv.Dimension(('var', 'Variable'), values=var)    \nhv.HoloMap(dd, kdims=kdims).collate()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48f49cd93a8b662849f10842b907eaff97c61213"},"cell_type":"code","source":"g=sns.PairGrid(df.drop('BAD',axis=1), hue='STATUS', diag_sharey=False, palette={'PAID': 'C0', 'DEFAULT':'C1'})\ng.map_lower(sns.kdeplot)\ng.map_upper(sns.scatterplot)\ng.map_diag(sns.kdeplot, lw=3)\ng.add_legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7351e8abefcfcf52ffb2da6d4dc11de6c9147f5f"},"cell_type":"markdown","source":"### Trama de violín\n<a id='violin'></a>\nLa gráfica de violín muestra las diferentes formas de la función de densidad de probabilidad para algunas de las variables discutidas anteriormente que parecen las más prometedoras para la tarea de clasificación. El gráfico muestra, en diferentes colores, los préstamos PAGADOS e INCUMPLIDOS. Las líneas discontinuas horizontales indican la posición de la media y los cuantiles de las diferentes distribuciones. Dado que existe una dependencia de la probabilidad PREDETERMINADA de las categorías de ocupación, se muestran los \"violines\" para cada una de ellas."},{"metadata":{"trusted":true,"_uuid":"c3f2d2b3a9d2d10120b9655b3b7b6fc6158bd20b"},"cell_type":"code","source":"cols=['YOJ', 'CLAGE', 'NINQ']\n\nfor col in cols:\n    \n    plt.figure(figsize=(15,5))\n\n    sns.violinplot(x='JOB', y=col, hue='STATUS',\n                   split=True, inner=\"quart\",  palette={'PAID': 'C0', 'DEFAULT':'C1'},\n                   data=df)\n    \n    sns.despine(left=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8204474b9c4bf0681597d41b6f7a84733854ec27"},"cell_type":"markdown","source":"### Matriz de correlación\n\n<a id='corr'></a>\nFinalmente se muestra la matriz de correlación entre las variables discutidas hasta ahora. Las correlaciones son útiles porque pueden indicar una relación predictiva que se puede aprovechar en la tarea de clasificación.\n\nEl gráfico está codificado por colores: los colores más fríos corresponden a una baja correlación, mientras que los colores más cálidos corresponden a una alta correlación. Las variables también se agrupan según su correlación, es decir, las variables con mayor correlación están próximas entre sí.\n\nLas variables relacionadas con el historial crediticio (DELINQ, DEROG, NINQ) son las más correlacionadas con el estado del préstamo (BAD), lo que sugiere que estas serán las variables más discriminatorias. Estas variables también están levemente correlacionadas entre ellas, lo que sugiere que parte de la información podría ser redundante.\n\nComo ya se mencionó, el monto del préstamo o la garantía subyacente no parecen estar relacionados con el estado del préstamo. De todos modos, forman otro grupo de correlación con otras variables como la antigüedad de la línea de crédito más antigua (CLAGE) y el número de líneas de crédito (CLNO). Esto es de esperar ya que esas variables están claramente relacionadas."},{"metadata":{"trusted":true,"_uuid":"62dcf0c50169195a204178098de9043005fc9b4d"},"cell_type":"code","source":"def compute_corr(df,size=10):\n    '''La función traza una matriz de correlación gráfica para cada par de columnas en el marco de datos.\n\n    Entradas:\n        df: pandas DataFrame\n        size: vertical y horizontal tamaño del plot'''\n    import scipy\n    import scipy.cluster.hierarchy as sch\n    \n    corr = df.corr()\n    \n    # Clustering\n    d = sch.distance.pdist(corr)   # vector of ('55' choose 2) pairwise distances\n    L = sch.linkage(d, method='complete')\n    ind = sch.fcluster(L, 0.5*d.max(), 'distance')\n    columns = [df.select_dtypes(include=[np.number]).columns.tolist()[i] for i in list((np.argsort(ind)))]\n    \n    # Reordered df upon custering results\n    df = df.reindex(columns, axis=1)\n    \n    # Recompute correlation matrix w/ clustering\n    corr = df.corr()\n    #corr.dropna(axis=0, how='all', inplace=True)\n    #corr.dropna(axis=1, how='all', inplace=True)\n    #corr.fillna(0, inplace=True)\n    \n    #fig, ax = plt.subplots(figsize=(size, size))\n    #img = ax.matshow(corr)\n    #plt.xticks(range(len(corr.columns)), corr.columns, rotation=45);\n    #plt.yticks(range(len(corr.columns)), corr.columns);\n    #fig.colorbar(img)\n    \n    return corr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5e9979ed5d368f2c877b2ac70c13d709e6ae9e2"},"cell_type":"code","source":"%%opts HeatMap [tools=['hover'] colorbar=True width=500  height=500 toolbar='above', xrotation=45, yrotation=45]\n\ncorr=compute_corr(df)\ncorr=corr.stack(level=0).to_frame('value').reset_index()\nhv.HeatMap(corr).options(cmap='Viridis')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5e8db9cd86ea60f3b63cfe9a0c758ad611a138f"},"cell_type":"markdown","source":"<a id='classification'></a>\n# Prueba de clasificadores predeterminados\nEl análisis exploratorio descrito anteriormente proporciona una buena perspectiva sobre el conjunto de datos y destaca las variables más prometedoras con un buen poder de discriminación para identificar los préstamos que resultan en INCUMPLIMIENTO. En esta sección, desarrollo e investigo clasificadores de aprendizaje automático superpuestos para predecir el resultado de los préstamos. Dada la gran cantidad de algoritmos disponibles en la literatura, empiezo con los métodos simples, como la regresión logística, y gradualmente aumento la complejidad del modelo hasta las técnicas de árboles aleatorios. Finalmente comparo el desempeño de cada modelo y analizo el más apropiado para esta tarea de clasificación de préstamos. En esta sección se desarrollan los siguientes modelos:\n\n* [Regresión logística](#logit)\n* [Clasificador SGD](#sgd)\n* [Clasificador de vectores de apoyo](#svc)\n* [Clasificador de aumento de gradiente](#gbrt)\n* [Bosque de árbol al azar](#frt)\n    * [Clasificador de bosque aleatorio](#rfc)\n    * [Árbol extremadamente aleatorio](#ert)\n* [Comparación y conclusión del modelo](#conclusion)\n\n<a id='eval'></a>\n## Evaluación del modelo\nLa evaluación del desempeño de los clasificadores es relativamente compleja y depende de muchos factores, algunos de los cuales dependen del modelo. Con el fin de identificar el mejor modelo para nuestra tarea de clasificación, adopto diferentes métricas de evaluación que se resumen brevemente a continuación.\n\nPara evitar el sobreentrenamiento, el rendimiento de nuestro modelo de clasificación se evalúa mediante validación cruzada. El conjunto de entrenamiento se divide aleatoriamente en $ N $ subconjuntos distintos llamados pliegues, luego el modelo se entrena y evalúa $ N $ veces mediante el uso de un pliegue diferente para la evaluación de un modelo que se entrena en los otros $ N-1 $ pliegues. Los resultados del procedimiento consisten en puntajes de evaluación de $ N $ para cada métrica que luego se promedian. Estos promedios se utilizan finalmente para comparar las diferentes técnicas consideradas en este estudio.\n\n\n<a id='per'></a>\n### Precisión y recuperación\nPrecision-Recall es una métrica de rendimiento útil para evaluar un modelo en aquellos casos en que las clases están muy desequilibradas. En la recuperación de información, la precisión es una medida de la relevancia de los resultados, mientras que la recuperación es una medida de cuántos resultados verdaderamente relevantes se devuelven. Intuitivamente, la precisión es la capacidad del clasificador de no etiquetar como positiva una muestra negativa, y la recuperación es la capacidad del clasificador de encontrar todas las muestras positivas.\n\nUn sistema con alta recuperación pero baja precisión devuelve muchas etiquetas que tienden a predecirse incorrectamente en comparación con las etiquetas de entrenamiento. Un sistema con alta precisión pero poca recuperación es todo lo contrario, arrojando muy pocos resultados, pero la mayoría de las etiquetas predichas son correctas en comparación con las etiquetas de entrenamiento. Un sistema ideal con alta precisión y alta recuperación devolverá muchos resultados, con muchos resultados etiquetados correctamente.\n\nLa precisión ($ P $) se define como el número de verdaderos positivos ($ T_ {p} $) sobre el número de verdaderos positivos más el número de falsos positivos ($ T_ {p} + F_ {p} $):\n\n$P = \\frac{T_{p}}{T_{p}+F_{p}}$  \n\nLa recuperación ($ R $) se define como el número de verdaderos positivos ($ T_ {p} $) sobre el número de verdaderos positivos más el número de falsos negativos ($ T_ {p} + F_ {n} $):\n\n$R = \\frac{T_{p}}{T_{p}+F_{n}}$\n\n<a id='f1'></a>\n### Medida F1\nA menudo es conveniente combinar la precisión y la recuperación en una única métrica llamada puntuación $ F_ {1} $, definida como una media armónica ponderada de la precisión y la recuperación:\n\n$F_{1} = 2\\times \\frac{P \\times R}{P+R}$\n\nMientras que la media regular trata todos los valores por igual, la media armónica da mucho más peso a los valores bajos. Como resultado, el clasificador solo obtendrá una puntuación F1 alta si tanto la memoria como la precisión son altas.\nLa puntuación $ F_ {1} $ favorece a los clasificadores que tienen una precisión y una recuperación similares. Esto no siempre es lo que desea: en algunos contextos, lo que más le importa es la precisión, y en otros contextos, realmente le importa el recuerdo.\n\n<a id='roc'></a>\n###  Característica Operativa del Receptor\nUna característica operativa del receptor (ROC), o simplemente una curva ROC, es un gráfico que ilustra el rendimiento de un sistema clasificador binario a medida que varía su umbral de discriminación. Se crea trazando la fracción de verdaderos positivos de los positivos (TPR = tasa de verdaderos positivos) frente a la fracción de falsos positivos de los negativos (FPR = tasa de falsos positivos), en varios valores de umbral. TPR también se conoce como sensibilidad, y FPR es uno menos la especificidad o tasa negativa verdadera.\nHay una compensación: cuanto mayor es la recuperación (TPR), más falsos positivos (FPR) produce el clasificador. La línea de puntos representa la curva ROC de un clasificador puramente aleatorio; un buen clasificador permanece lo más lejos posible de esa línea (hacia la esquina superior izquierda).\n\nEl área bajo la curva ROC, que también se denota por AUC, resume la información de la curva en un número. El AUC debe interpretarse como la probabilidad de que un clasificador clasifique una istancia positiva elegida al azar más alta que una negativa elegida al azar. Un clasificador perfecto tendrá un ROC AUC igual a 1, mientras que un clasificador puramente aleatorio tendrá un ROC AUC igual a 0,5.\n\n<a id='confusion'></a>\n### Matriz de confusión\nLa matriz de confusión evalúa la precisión de la clasificación calculando la matriz de confusión con cada fila correspondiente a la clase verdadera. Por definición, la entrada $ i, j $ en una matriz de confusión es el número de observaciones en realidad en el grupo $ i $, pero se predice que estarán en el grupo $ j $. La matriz de confusión no se utiliza para la evaluación del modelo, pero proporciona una buena comprensión del rendimiento general del modelo.\n\n<a id='prob'></a>\n### Probabilidad de clasificación\nLa probabilidad de clasificación proporciona una estimación de la probabilidad de que una instancia determinada de los datos pertenezca a una clase determinada. En un problema de clasificación binaria como el que se está considerando, el histograma de la probabilidad de clasificación para las dos clases proporciona una buena comprensión visual del rendimiento del modelo. Cuanto más alejados estén los picos de la probabilidad de clasificación, mayor será el poder de separación del modelo."},{"metadata":{"trusted":true,"_uuid":"5ce7c5a234213bbcaa3ece610a34ec4ebf437b35"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom pprint import pprint\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5efc75e35bcb100cfb18af07653ebb4e2640b04"},"cell_type":"code","source":"df=pd.read_csv('../input/hmeq.csv', low_memory=False) # Sin columnas duplicadas, sin columnas altamente correlacionadas\ndf=pd.get_dummies(df, columns=['REASON','JOB'])\ndf.drop('DEBTINC', axis=1, inplace=True)\ndf.dropna(axis=0, how='any', inplace=True)\ny = df['BAD']\nX = df.drop(['BAD'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac954ae9704fb17b42b548fb0ade5e91ca917c3c"},"cell_type":"code","source":"def cross_validate_model(model, X, y, \n                         scoring=['f1', 'precision', 'recall', 'roc_auc'], \n                         cv=12, n_jobs=-1, verbose=True):\n    \n    scores = cross_validate(pipe, \n                        X, y, \n                        scoring=scoring,\n                        cv=cv, n_jobs=n_jobs, \n                        verbose=verbose,\n                        return_train_score=False)\n\n    #sorted(scores.keys())\n    dd={}\n    \n    for key, val in scores.items():\n        if key in ['fit_time', 'score_time']:\n            continue\n        #print('{:>30}: {:>6.5f} +/- {:.5f}'.format(key, np.mean(val), np.std(val)) )\n        name = \" \".join(key.split('_')[1:]).capitalize()\n        \n        dd[name] = {'value' : np.mean(val), 'error' : np.std(val)}\n        \n    return  pd.DataFrame(dd)    \n    #print()\n    #pprint(scores)\n    #print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8bc2942ce77db1e02bb570bd7613afaba51a75bb"},"cell_type":"code","source":"def plot_roc(model, X_test ,y_test, n_classes=0):\n    \n    from sklearn.metrics import roc_curve, auc\n    \n    \"\"\"\n    Puntuaciones objetivo, pueden ser estimaciones de probabilidad\n    de la clase positiva, valores de confianza o\n    medida de decisiones sin umbral (como se devuelve\n    por \"decision_function\" en algunos clasificadores).\n    \"\"\"\n    try:\n        y_score = model.decision_function(X_test)\n    except Exception as e:\n        y_score = model.predict_proba(X_test)[:,1]\n    \n    \n    fpr, tpr, _ = roc_curve(y_test.ravel(), y_score.ravel())\n    roc_auc = auc(fpr, tpr)\n\n    # Compute micro-average ROC curve and ROC area\n    #fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n    #roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n    \n    #plt.figure()\n    lw = 2\n    plt.plot(fpr, tpr, color='darkorange',\n             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n\n    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('Tasa de falsos positivos')\n    plt.ylabel('Tasa de verdaderos positivos')\n    plt.title('Ejemplo de característica de funcionamiento del receptor')\n    plt.legend(loc=\"lower right\")\n    #plt.show()\n    \n# mezclar y dividir conjuntos de entrenamiento y prueba\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,\n#                                                    random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c321fe50f6eb4498dd30597623f996803dc741aa"},"cell_type":"code","source":"def plot_confusion_matrix(model, X_test ,y_test,\n                          classes=[0,1],\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \n    import itertools\n    from sklearn.metrics import confusion_matrix\n    \n    y_pred = model.predict(X_test)\n    \n    # Calcular matriz de confusión\n    cm = confusion_matrix(y_test, y_pred)\n    np.set_printoptions(precision=2)\n    \n    \"\"\"\n    Esta función imprime y traza la matriz de confusión.\n    La normalización se puede aplicar configurando `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    #    print(\"Normalized confusion matrix\")\n    #else:\n    #    print('Confusion matrix, without normalization')\n\n    #print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8395e8a2f03ef68d865d6a58dfa8924c143448b6"},"cell_type":"code","source":"def feature_importance(coef, names, verbose=False, plot=True):\n    \n    #importances = model.feature_importances_\n\n    \n    \n    #std = np.std([tree.feature_importances_ for tree in model.estimators_],\n    #             axis=0)\n    indices = np.argsort(coef)[::-1]\n    \n    if verbose:\n    \n        # Imprime la clasificación de funciones\n        print(\"Feature ranking:\")\n    \n        for f in range(len(names)):\n            print(\"{:>2d}. {:>15}: {:.5f}\".format(f + 1, names[indices[f]], coef[indices[f]]))\n        \n    if plot:\n        \n        # Trazar la importancia de las características del bosque\n        #plt.figure(figsize=(5,10))\n        plt.title(\"Feature importances\")\n        plt.barh(range(len(names)), coef[indices][::-1], align=\"center\")\n        #plt.barh(range(X.shape[1]), importances[indices][::-1],\n        #         xerr=std[indices][::-1], align=\"center\")\n        plt.yticks(range(len(names)), names[indices][::-1])\n        #plt.xlim([-0.001, 1.1])\n        #plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b82915abe9c07f603d19a0a4bdd2cd4279cbdd8"},"cell_type":"code","source":"def plot_proba(model, X, y, bins=40, show_class = 1):\n    \n    from sklearn.calibration import CalibratedClassifierCV\n    \n    model = CalibratedClassifierCV(model)#, cv='prefit')\n    \n    model.fit(X, y)\n    \n    proba=model.predict_proba(X)\n    \n    if show_class == 0:\n        sns.kdeplot(proba[y==0,0], shade=True, color=\"r\", label='True class')\n        sns.kdeplot(proba[y==0,1], shade=True, color=\"b\", label='Wrong class')\n        plt.title('Classification probability: Class 0')\n    elif show_class == 1:\n        sns.kdeplot(proba[y==1,1], shade=True, color=\"r\", label='True class')\n        sns.kdeplot(proba[y==1,0], shade=True, color=\"b\", label='Wrong class')\n        plt.title('Classification probability: Class 1')\n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e940ba20f513b744d2fa5132d7c14206b7901680"},"cell_type":"markdown","source":"## Regresión logística\n<a id='logit'></a>\n\nLa regresión logística es el modelo lineal más simple para la clasificación. La regresión logística también se conoce en la literatura como regresión logit, clasificación de máxima entropía (MaxEnt) o clasificador log-lineal. En este modelo, las probabilidades que describen los posibles resultados de un solo ensayo se modelan utilizando una función logística. El problema de optimización se resuelve minimizando una función de costo utilizando un algoritmo de descenso de coordenadas altamente optimizado.\n"},{"metadata":{"trusted":true,"_uuid":"d526e2619034ae819155db168a52baa4db11545e"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nsteps = [('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)),\n         ('model', LogisticRegression(random_state=0))]\n\npipe = Pipeline(steps)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)\npipe.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1238203218684c84f0435627e32531a58af94807"},"cell_type":"code","source":"plt.figure(figsize=(15,10))\n\nplt.subplot(221)\nplot_roc(pipe, X_test ,y_test)\n\nplt.subplot(222)\nplot_confusion_matrix(pipe, X_test ,y_test, normalize=True)\n\nplt.subplot(223)\nplot_proba(pipe, X_test, y_test)\n\nplt.subplot(224)\nfeature_importance(pipe.named_steps['model'].coef_[0], X.columns)\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b7e4bdcf812f7851e5022ad91c9645cbad26ba3"},"cell_type":"code","source":"logit_xval_res = cross_validate_model(pipe, X, y, verbose=False)\nlogit_xval_res.T[['value','error']].style.format(\"{:.2f}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"693856fd38fd7130b689c40f99c3f3fb13b09def"},"cell_type":"markdown","source":"<a id='sgd'></a>\n## Clasificador SGD\nEste algoritmo implementa modelos lineales regularizados con aprendizaje de descenso de gradiente estocástico (SGD): el gradiente de la pérdida se estima en cada muestra a la vez y el modelo se actualiza a lo largo del camino con un programa de fuerza decreciente (también conocido como tasa de aprendizaje). SGD permite el aprendizaje minibatch (en línea / fuera del núcleo).\n"},{"metadata":{"trusted":true,"_uuid":"454d2bb26af8e8707275701fca190befab4a25a2"},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n\nsteps = [('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)),\n         ('model', SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=1000, tol=1e-3, random_state=0))]\n\npipe = Pipeline(steps)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)\npipe.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3bd55c6d67575343ceee4851ecc3cb8df968b45"},"cell_type":"code","source":"plt.figure(figsize=(15,10))\n\nplt.subplot(221)\nplot_roc(pipe, X_test ,y_test)\n\nplt.subplot(222)\nplot_confusion_matrix(pipe, X_test ,y_test, normalize=True)\n\nplt.subplot(223)\nplot_proba(pipe, X_test, y_test)\n\nplt.subplot(224)\nfeature_importance(pipe.named_steps['model'].coef_[0], X.columns)\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46a0c5b6f92da2b5814fee280ee6806add5eee11"},"cell_type":"code","source":"sgd_xval_res = cross_validate_model(pipe, X, y, verbose=False)\nsgd_xval_res.T[['value','error']].style.format(\"{:.2f}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"498ccf064a99a9d4768f55846c3a16837a773b3f"},"cell_type":"markdown","source":"## Clasificador de vectores de apoyo\n\n<a id='svc'></a>\nUna máquina de vectores de soporte construye un hiperplano o un conjunto de hiperplanos en un espacio dimensional alto o infinito, que se puede utilizar para clasificación, regresión u otras tareas. Intuitivamente, se logra una buena separación por el hiperplano que tiene la mayor distancia a los puntos de datos de entrenamiento más cercanos de cualquier clase (el llamado margen funcional), ya que en general cuanto mayor es el margen menor es el error de generalización del clasificador.\n\n\nLas ventajas de las máquinas de vectores de soporte son:\n\n* Efectivo en espacios de alta dimensión.\n* Sigue siendo eficaz en casos en los que el número de dimensiones es mayor que el número de muestras.\n* Utiliza un subconjunto de los puntos de entrenamiento en la función de decisión, por lo que es eficiente en la memoria."},{"metadata":{"trusted":true,"_uuid":"9f559e1a37b21b3ed9c60d6fe224be84bb55cb13"},"cell_type":"code","source":"from sklearn.svm import SVC\n\nsteps = [('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)),\n         ('model', SVC(random_state=0, kernel='linear', probability=True))]\n\npipe = Pipeline(steps)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)\npipe.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd95c844f83f8fb25c28c09a947d7dab261f9264"},"cell_type":"code","source":"plt.figure(figsize=(15,10))\n\nplt.subplot(221)\nplot_roc(pipe, X_test ,y_test)\n\nplt.subplot(222)\nplot_confusion_matrix(pipe, X_test ,y_test, normalize=True)\n\nplt.subplot(223)\nplot_proba(pipe, X_test, y_test)\n\nplt.subplot(224)\nfeature_importance(pipe.named_steps['model'].coef_[0], X.columns)\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9bd23c9851d908847cda25392e12c11050f9aea5"},"cell_type":"code","source":"svc_xval_res = cross_validate_model(pipe, X, y, verbose=False)\nsvc_xval_res.T[['value','error']].style.format(\"{:.2f}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"213ca20e9f3a721b5163c5b44d805eabfc168b56"},"cell_type":"markdown","source":"<a id='gbrt'></a>\n### Clasificador de aumento de gradiente\nGradient Tree Boosting o Gradient Boosted Regression Trees (GBRT) es una generalización del impulso a funciones de pérdida diferenciables arbitrarias. GBRT produce un modelo de predicción en forma de un conjunto de modelos de predicción débiles, típicamente árboles de decisión. Construye el modelo por etapas como lo hacen otros métodos de impulso, y los generaliza al permitir la optimización de una función de pérdida diferenciable arbitraria. GBRT es un procedimiento estándar preciso y eficaz que se puede utilizar tanto para problemas de regresión como de clasificación.\n"},{"metadata":{"trusted":true,"_uuid":"707ffd997c8317c2d893a643ef15619d5ccebdc4"},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\nsteps = [('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)),\n         ('model', GradientBoostingClassifier(n_estimators=250, learning_rate=0.05, random_state=0))]\n\npipe = Pipeline(steps)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)\npipe.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"daf0460f3152c8b71e727e015493717a3c888d66"},"cell_type":"code","source":"plt.figure(figsize=(15,10))\n\nplt.subplot(221)\nplot_roc(pipe, X_test ,y_test)\n\nplt.subplot(222)\nplot_confusion_matrix(pipe, X_test ,y_test, normalize=True)\n\nplt.subplot(223)\nplot_proba(pipe, X_test, y_test)\n\nplt.subplot(224)\nfeature_importance(pipe.named_steps['model'].feature_importances_, X.columns)\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd7779647a57ba8cfaf74e6844253f91e63886e9"},"cell_type":"code","source":"gbc_xval_res = cross_validate_model(pipe, X, y, verbose=False)\ngbc_xval_res.T[['value','error']].style.format(\"{:.2f}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a57986faf29fb12dfe52cf38e48588a85f1329e"},"cell_type":"markdown","source":"<a id='frt'></a>\n### Bosques de árboles aleatorizados\nLos árboles de decisión (DT) son un método de aprendizaje supervisado no paramétrico que se utiliza para clasificación y regresión. El objetivo es crear un modelo que prediga el valor de una variable objetivo mediante el aprendizaje de reglas de decisión simples inferidas de las características de los datos.\n\nLa técnica del bosque de árboles aleatorios incluye dos algoritmos de promediado basados ​​en árboles de decisión aleatorios: el algoritmo RandomForest y el método Extra-Trees. Ambos algoritmos son técnicas de perturbación y combinación diseñadas específicamente para árboles. Esto significa que se crea un conjunto diverso de clasificadores mediante la introducción de aleatoriedad en la construcción del clasificador. La predicción del conjunto se da como la predicción promedio de los clasificadores individuales.\n\n<a id='rfc'></a>\n#### Clasificador de bosque aleatorio\n\nEn bosques aleatorios, cada árbol del conjunto se construye a partir de una muestra extraída con reemplazo (es decir, una muestra de arranque) del conjunto de entrenamiento. Además, al dividir un nodo durante la construcción del árbol, la división que se elige ya no es la mejor división entre todas las características. En cambio, la división que se elige es la mejor división entre un subconjunto aleatorio de características. Como resultado de esta aleatoriedad, el sesgo del bosque generalmente aumenta ligeramente (con respecto al sesgo de un solo árbol no aleatorio) pero, debido al promedio, su varianza también disminuye, generalmente más que compensando el aumento del sesgo. de ahí que produzca un modelo mejor en general.\n"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"6f98e2f06aea23bbcd54a7d110ada03a5b4f1d24"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nsteps = [('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)),\n         ('model', RandomForestClassifier(n_estimators=250, n_jobs=-1, random_state=0))]\n\npipe = Pipeline(steps)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)\npipe.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bdb13c15335b04789b591de91dd606a57ac755a9"},"cell_type":"code","source":"plt.figure(figsize=(15,10))\n\nplt.subplot(221)\nplot_roc(pipe, X_test ,y_test)\n\nplt.subplot(222)\nplot_confusion_matrix(pipe, X_test ,y_test, normalize=True)\n\nplt.subplot(223)\nplot_proba(pipe, X_test, y_test)\n\nplt.subplot(224)\nfeature_importance(pipe.named_steps['model'].feature_importances_, X.columns)\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4bb62c9ba333c45f9d6e58a28c4e6a27c0d0e8ff"},"cell_type":"code","source":"rfc_xval_res = cross_validate_model(pipe, X, y, verbose=False)\nrfc_xval_res.T[['value','error']].style.format(\"{:.2f}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"32fac0f2dc0970d4c7b60cb02229b97df5897bc1"},"cell_type":"markdown","source":"<a id='ert'></a>\n#### Árboles extremadamente aleatorizados\nEn árboles extremadamente aleatorizados, la aleatoriedad va un paso más allá en la forma en que se calculan las divisiones. Al igual que en los bosques aleatorios, se utiliza un subconjunto aleatorio de características candidatas, pero en lugar de buscar los umbrales más discriminativos, los umbrales se dibujan al azar para cada característica candidata y el mejor de estos umbrales generados aleatoriamente se elige como la regla de división. Esto generalmente permite reducir un poco más la varianza del modelo, a expensas de un aumento ligeramente mayor del sesgo.\n"},{"metadata":{"trusted":true,"_uuid":"687c336968c42abd165a735641f5cfbd9de03be9"},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\n\nsteps = [('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)),\n         ('model', ExtraTreesClassifier(n_estimators=250, n_jobs=-1, random_state=0, class_weight='balanced'))]\n\npipe = Pipeline(steps)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)\npipe.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97a8c723c30fdcdf7d040989c5094d7c762ba85b"},"cell_type":"code","source":"plt.figure(figsize=(15,10))\n\nplt.subplot(221)\nplot_roc(pipe, X_test ,y_test)\n\nplt.subplot(222)\nplot_confusion_matrix(pipe, X_test ,y_test, normalize=True)\n\nplt.subplot(223)\nplot_proba(pipe, X_test, y_test)\n\nplt.subplot(224)\nfeature_importance(pipe.named_steps['model'].feature_importances_, X.columns)\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb13d68e18139ed3ca7fe282547d99838f7f2d8b"},"cell_type":"code","source":"ert_xval_res = cross_validate_model(pipe, X, y, verbose=False)\nert_xval_res.T[['value','error']].style.format(\"{:.2f}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59ebb7aa871678f1f7cda4bd66c081e56012fd29"},"cell_type":"markdown","source":"<a id='conclusion'></a>\n## Comparación de modelos y conclusiones\n\nLa siguiente tabla resume el desempeño de los modelos de clasificación que consideré en este estudio. Las actuaciones se ordenan aumentando el valor de $ F_ {1} $. Los mejores rendimientos se obtienen mediante el ** árbol extremadamente aleatorio **, seguido del ** bosque aleatorio ** y la ** regresión logística **.\n\nEl árbol extremadamente aleatorizado permite identificar hasta el 66% de los préstamos que causarían un INCUMPLIMIENTO mientras se retiene el 91% de los préstamos que serían PAGADOS a tiempo. El valor de ROC AUC es tan alto como 96%, lo que indica que la probabilidad de que el clasificador se desempeñe mejor por elección aleatoria es tan baja como 4%."},{"metadata":{"trusted":true,"_uuid":"5dd30c501a228b91bb34fd8f04aeee7ebd9918f4"},"cell_type":"code","source":"from collections import OrderedDict\n\nres_comp = OrderedDict([\n    ('Regresión logística'              , logit_xval_res[1:]),\n    ('Clasificador SGD'                   , sgd_xval_res[1:]  ),\n    ('Clasificador de vectores de apoyo'     , svc_xval_res[1:]  ),\n    ('Clasificador de bosque aleatorio'         , rfc_xval_res[1:]  ),\n    ('Clasificador de árboles extremadamente aleatorio' , ert_xval_res[1:]  ),\n    ('Clasificador de aumento de gradiente'        , gbc_xval_res[1:]  ),\n])\n\nnew_columns = {'level_0' : 'Model'}\n\npd.concat(res_comp).reset_index().drop('level_1', axis=1).rename(columns=new_columns).set_index('Model').sort_values('F1', ascending=False).style.format(\"{:.2f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}