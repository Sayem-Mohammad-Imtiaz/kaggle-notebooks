{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sentiment Analysis with Deep Learning using BERT","metadata":{}},{"cell_type":"code","source":"import torch\nimport pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np # linear algebra\nfrom tqdm.notebook import tqdm","metadata":{"execution":{"iopub.status.busy":"2021-08-17T14:04:37.542011Z","iopub.execute_input":"2021-08-17T14:04:37.542516Z","iopub.status.idle":"2021-08-17T14:04:39.127513Z","shell.execute_reply.started":"2021-08-17T14:04:37.542403Z","shell.execute_reply":"2021-08-17T14:04:39.126439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))# df.set_index('id', inplace=True)\n# \ndf = pd.read_csv('/kaggle/input/emotion-detection-from-text/tweet_emotions.csv')\n\n# Let's have a look at it\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-17T14:04:39.130507Z","iopub.execute_input":"2021-08-17T14:04:39.131161Z","iopub.status.idle":"2021-08-17T14:04:39.338389Z","shell.execute_reply.started":"2021-08-17T14:04:39.131115Z","shell.execute_reply":"2021-08-17T14:04:39.337275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.content.iloc[-10:]","metadata":{"execution":{"iopub.status.busy":"2021-08-17T14:04:39.340521Z","iopub.execute_input":"2021-08-17T14:04:39.340832Z","iopub.status.idle":"2021-08-17T14:04:39.352791Z","shell.execute_reply.started":"2021-08-17T14:04:39.340802Z","shell.execute_reply":"2021-08-17T14:04:39.351147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.sentiment.value_counts()\n#nocode is simply no clear emotions in this tweet","metadata":{"execution":{"iopub.status.busy":"2021-08-17T14:04:39.355551Z","iopub.execute_input":"2021-08-17T14:04:39.356084Z","iopub.status.idle":"2021-08-17T14:04:39.37581Z","shell.execute_reply.started":"2021-08-17T14:04:39.35599Z","shell.execute_reply":"2021-08-17T14:04:39.374326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we want to remove small datasets \n# df = df[~df.sentiment.str.contains('\\|')]   \ndf = df[df.sentiment != 'anger'] #& 'boredom' & 'enthusiasm' & 'empty'\ndf = df[df.sentiment != 'boredom']\ndf = df[df.sentiment != 'enthusiasm']\ndf = df[df.sentiment != 'empty']\ndf = df[df.sentiment != 'sentiment'] #there is sentiment in sentiments!","metadata":{"execution":{"iopub.status.busy":"2021-08-17T14:04:39.377934Z","iopub.execute_input":"2021-08-17T14:04:39.378522Z","iopub.status.idle":"2021-08-17T14:04:39.423268Z","shell.execute_reply.started":"2021-08-17T14:04:39.378456Z","shell.execute_reply":"2021-08-17T14:04:39.422144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.sentiment.value_counts()\n#class imbalance","metadata":{"execution":{"iopub.status.busy":"2021-08-17T14:04:39.424879Z","iopub.execute_input":"2021-08-17T14:04:39.425355Z","iopub.status.idle":"2021-08-17T14:04:39.443133Z","shell.execute_reply.started":"2021-08-17T14:04:39.42531Z","shell.execute_reply":"2021-08-17T14:04:39.441476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#build dictionary, key: emotion, value: \npossible_labels = df.sentiment.unique()","metadata":{"execution":{"iopub.status.busy":"2021-08-17T14:42:16.876506Z","iopub.execute_input":"2021-08-17T14:42:16.876913Z","iopub.status.idle":"2021-08-17T14:42:16.886752Z","shell.execute_reply.started":"2021-08-17T14:42:16.876883Z","shell.execute_reply":"2021-08-17T14:42:16.885275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_dict = {}\n#loop over index\nfor index, possible_label in enumerate(possible_labels):\n    label_dict[possible_label] = index\nprint(label_dict)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T14:04:39.461791Z","iopub.execute_input":"2021-08-17T14:04:39.462506Z","iopub.status.idle":"2021-08-17T14:04:39.470944Z","shell.execute_reply.started":"2021-08-17T14:04:39.462454Z","shell.execute_reply":"2021-08-17T14:04:39.469764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#build new column for these values\ndf['label'] = df.sentiment.replace(label_dict)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-17T14:04:39.629737Z","iopub.execute_input":"2021-08-17T14:04:39.630313Z","iopub.status.idle":"2021-08-17T14:04:39.685042Z","shell.execute_reply.started":"2021-08-17T14:04:39.630281Z","shell.execute_reply":"2021-08-17T14:04:39.683917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Task 3: Training/Validation Split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-08-17T14:04:39.687058Z","iopub.execute_input":"2021-08-17T14:04:39.687528Z","iopub.status.idle":"2021-08-17T14:04:40.789235Z","shell.execute_reply.started":"2021-08-17T14:04:39.687466Z","shell.execute_reply":"2021-08-17T14:04:40.787916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#stratified split\nX_train, X_val, y_train, y_val = train_test_split(df.index.values,\n                                                 df.label.values,\n                                                 test_size = 0.15,\n#                                                  random_state=17,\n                                                 stratify = df.label.values\n                                                 )","metadata":{"execution":{"iopub.status.busy":"2021-08-17T14:04:40.795495Z","iopub.execute_input":"2021-08-17T14:04:40.798272Z","iopub.status.idle":"2021-08-17T14:04:40.854969Z","shell.execute_reply.started":"2021-08-17T14:04:40.798224Z","shell.execute_reply":"2021-08-17T14:04:40.853615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['data_type'] = ['not_set']*df.shape[0]","metadata":{"execution":{"iopub.status.busy":"2021-08-17T14:04:40.861364Z","iopub.execute_input":"2021-08-17T14:04:40.864382Z","iopub.status.idle":"2021-08-17T14:04:40.879977Z","shell.execute_reply.started":"2021-08-17T14:04:40.864335Z","shell.execute_reply":"2021-08-17T14:04:40.87865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-17T14:04:40.886213Z","iopub.execute_input":"2021-08-17T14:04:40.889101Z","iopub.status.idle":"2021-08-17T14:04:40.917082Z","shell.execute_reply.started":"2021-08-17T14:04:40.88905Z","shell.execute_reply":"2021-08-17T14:04:40.913786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.loc[X_train, 'data_type'] = 'train'\ndf.loc[X_val, 'data_type'] = 'val'","metadata":{"execution":{"iopub.status.busy":"2021-08-17T14:04:40.919349Z","iopub.execute_input":"2021-08-17T14:04:40.919812Z","iopub.status.idle":"2021-08-17T14:04:40.94112Z","shell.execute_reply.started":"2021-08-17T14:04:40.919765Z","shell.execute_reply":"2021-08-17T14:04:40.935741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupby(['sentiment', 'label', 'data_type']).count()\n#group by using count","metadata":{"execution":{"iopub.status.busy":"2021-08-17T14:04:41.860381Z","iopub.execute_input":"2021-08-17T14:04:41.860753Z","iopub.status.idle":"2021-08-17T14:04:41.899394Z","shell.execute_reply.started":"2021-08-17T14:04:41.860721Z","shell.execute_reply":"2021-08-17T14:04:41.898022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **PREPROCESSING**","metadata":{"execution":{"iopub.status.busy":"2021-08-17T02:50:34.696561Z","iopub.execute_input":"2021-08-17T02:50:34.696972Z","iopub.status.idle":"2021-08-17T02:50:34.702089Z","shell.execute_reply.started":"2021-08-17T02:50:34.696936Z","shell.execute_reply":"2021-08-17T02:50:34.700733Z"}}},{"cell_type":"markdown","source":"Import the libraries","metadata":{}},{"cell_type":"code","source":"# Install spaCy (run in terminal/prompt)\n# import sys\n# !{sys.executable} -m pip install spacy\n\n# Download spaCy's  'en' Model\n# !{sys.executable} -m spacy download en\n\n# !pip install -U symspellpy\n\n#for spell and slang correction\n# !pip install gingerit\n# from gingerit.gingerit import GingerIt\n\n#for emoticons\n!pip install emot --upgrade\nimport emot \nemot_obj = emot.core.emot() \n\n# from symspellpy.symspellpy import SymSpell, Verbosity\n# import pkg_resources\nimport re, string, json\n# import spacy","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-08-17T14:04:41.901477Z","iopub.execute_input":"2021-08-17T14:04:41.901929Z","iopub.status.idle":"2021-08-17T14:05:01.703367Z","shell.execute_reply.started":"2021-08-17T14:04:41.901884Z","shell.execute_reply":"2021-08-17T14:05:01.702172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ncontraction_list = json.loads(open('/kaggle/input/english-contractions/english_contractions.json.txt', 'r').read())\ncharacter_entity= {'&lt;3':'heart', '&amp;':'and','&quot;':' quote '}\ncontraction_list = {**contraction_list, **character_entity}\n\n\ndef normalization_pipeline(sentences):\n    print(\"##############################\")\n    print(\"Starting Normalization Process\")\n    sentences = _simplify_punctuation_and_whitespace(sentences) # !!!!! \"      \"\n    sentences = _normalize_contractions(sentences) #also corrects spelling now\n    print(\"Normalization Process Finished\")\n    print(\"##############################\")\n    return sentences\n\n    \ndef _simplify_punctuation_and_whitespace(sentence_list):\n    \"\"\"\n    words with more than 4 all-capital words will get <-EMPW \n    \"\"\"\n    norm_sents = []\n    print(\"Replacing -URL- , Replacing @MENTION and #HASHTAG, Reducing character repetitions, \")\n    print(\"Simplifying punctuation, Removing whitespaces\")\n\n    for sentence in tqdm(sentence_list):\n        sent = _replace_urls(sentence)\n        sent = _mention_hash(sent)\n        sent = _simplify_punctuation(sent)\n        sent = _reduce_repetitions(sent)\n        sent = _normalize_whitespace(sent)\n        norm_sents.append(sent)\n    return norm_sents\n\n\ndef _replace_urls(text):\n    url_regex = r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n    text = re.sub(url_regex, \"-URL-\", text)\n    return text\n\n\ndef _mention_hash(in_str):\n    \"\"\"\n     @MENTIONs and #HASHTAGs will take forms of @men and #has \n    note: BEWARE OF USES OF # AND @ AND SPACES BETWEEN THEM\n    \"\"\"\n    in_str = str(in_str)\n    in_str = re.sub('@\\w+', '@MEN', in_str,flags=re.IGNORECASE) # use @\\w+ for word replacement or @ with space after @MEN for keeping mention\n    in_str = re.sub('#', '#HAS ', in_str,flags=re.IGNORECASE)\n#     in_str = re.sub(r'([\\w])\\1+', r'\\1\\1', in_str) #reduce repeated characters to 2\n    return in_str.strip()\n\n\ndef _simplify_punctuation(text):\n    \"\"\"\n    puntuations like '!!!!!' will be transformed into '!! <-EMPP'\n    This function simplifies doubled or more complex punctuation. The exception is '...'. #?! ??? !!!\n    \"\"\"\n    corrected = str(text)\n    corrected = re.sub(r'([!?,;])\\1+', r'\\1\\1 <-EMPP', corrected) #\\1\\1 makes it to 2 consecutive punctuation\n    corrected = re.sub(r'\\.{2,}', r'...', corrected)\n    return corrected\n\ndef _reduce_repetitions(text):\n    \"\"\"\n    Auxiliary function to help with exxagerated (repeated characters in) words.\n    Examples:\n        woooooords -> woords <-EMPW\n        dooorwaaay -> doorwaay <-EMPW\n        SICK -> sick <-EMPU\n    \"\"\"\n    correction = str(text)\n    for index, words in enumerate(str(text).split()):\n        if _is_EMP_word(words)==True :\n            #insert EMPW after word\n            correction = correction.replace(words, words + ' <-EMPW')\n        if (len(words) > 4) & (words.isupper()==True) & (words[0] not in string.punctuation):\n            correction = correction.replace(words, words + ' <-EMPU')\n    #TODO work on complexity reduction.\n    return re.sub(r'([\\w])\\1+', r'\\1\\1', correction) #\\1\\1 will only keep 2 consecutive characters\n\n\ndef _is_EMP_word(word):\n    \"\"\"\n    True/ False: checks if the word has 3 consecutive characters\"\"\"\n    count=1\n    if len(word)>1:\n        for i in range(1,len(word)):\n            if word[i] in string.punctuation: #this function is only for words!\n                return False\n            if word[i-1]==word[i]:\n                count+=1\n                if(count>=3):\n                     return True\n            else :\n                if(count>=3):\n                    return True\n                count=1\n    else :\n        return False\n    return False\n\n\ndef _normalize_whitespace(text):\n    \"\"\"\n    normalizes whitespaces, removing duplicates.\n    \"\"\"\n    corrected = str(text)\n    corrected = re.sub(r\"//t\",r\"\\t\", corrected)\n    corrected = re.sub(r\"( )\\1+\",r\"\\1\", corrected)\n    corrected = re.sub(r\"(\\n)\\1+\",r\"\\1\", corrected)\n    corrected = re.sub(r\"(\\r)\\1+\",r\"\\1\", corrected)\n    corrected = re.sub(r\"(\\t)\\1+\",r\"\\1\", corrected)\n    return corrected.strip(\" \")\n    \n    \n#Substitution of contractions:  -----------------------------------------------------------------------------------------------      \ndef _normalize_contractions(sentence_list):\n    \"\"\"\n    it will correct each word in a sentence for slangs(ginger), emojis -> meaning, entity references and abbreviations(json file) : file can be manually modified above\n    also makes everything lowercase (including EMPW,EMPU, EMPP, URL, etc)\n    \"\"\"\n    #uses contraction_list (a json file) BE SURE TO IMPORT IT ALREADY\n    norm_sents = []\n    print(\"Normalizing contractions, abbreviations, slangs, emojis, character entities\")\n    for sentence in tqdm(sentence_list):\n        norm_sents.append(_normalize_contractions_slang_emoji_entity(sentence))\n    return norm_sents\n\ndef _normalize_contractions_slang_emoji_entity(text):\n    \"\"\"\n    part1:normalizes english contractions.\n    \"\"\"\n    contractions = contraction_list\n    for word in text.split():\n         if word.lower() in contractions:\n            text = text.replace(word, contractions[word.lower()])\n#             print('replacing contraction: '+ word + ' to '+contractions[word.lower()])\n    \"\"\"\n    part 2: using gingerit SMS slang correction:\n    this is too slow and can take many hours for the whole dataset to run\n    \"\"\"\n#     parser = GingerIt()\n#     result=parser.parse(text)\n#     # corrections = result['corrections']\n#     sentence = result['result']\n    sentence = text\n    \"\"\"\n    part3: emoji and character entity reference conversion to meaning\n    \"\"\"\n#     if emot_obj.emoji(sentence)['value'] !=[] : #we do not have emojis in this database text\n#         print(\"found emoji: \"+str(emot_obj.emoji(sentence)['value'])+ sentence)\n    emoticons = emot_obj.emoticons(sentence)\n#     if((emoticons['value']!=[]) ): #for printing\n#         print(\"found: \"+str(emoticons['value']) +'  emoticons in:   '+ sentence) \n    for i in range(0,len(emoticons['value'])):\n#         print('replacing  ' + emoticons['value'][i] + '  with ' +  emoticons['mean'][i])\n        sentence = sentence.replace(emoticons['value'][i], emoticons['mean'][i])\n    \"\"\"\n    part4: make everything lowercase\n    \"\"\"\n    sentence = sentence.lower()\n    return sentence\n\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-08-17T14:10:49.020756Z","iopub.execute_input":"2021-08-17T14:10:49.021272Z","iopub.status.idle":"2021-08-17T14:10:49.053657Z","shell.execute_reply.started":"2021-08-17T14:10:49.021156Z","shell.execute_reply":"2021-08-17T14:10:49.052632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#uncomment for data normalization (else just load from the data)","metadata":{}},{"cell_type":"code","source":"#assessment and examples:\n# # original_examples = ['hi @someone WATCH me #proud :) ;) ...... i h8 it bt w8 !!!!!  <3  wanna go &amp; &lt;3 tHeRe  &quot; bcs my finls clooooose &quot;bananas&quot; &amp; ']\n# original_examples=df.content[0:10]\n# preprocessed_examples = normalization_pipeline(original_examples)\n# for example_index,example in enumerate(preprocessed_examples):\n# #     print(original_examples[example_index])\n#     print(original_examples.values[example_index])\n#     print(example)\n\n\n    \n#run preprocessing\n# df_original=df\n# df.content=normalization_pipeline(df.content.values ) #about 10 minutes to run\n\n#save\n# df.to_csv('df_processed.csv',index=False)\n\n#load\ndf = pd.read_csv('/kaggle/input/english-contractions/df_processed.csv')\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-17T14:19:02.819421Z","iopub.execute_input":"2021-08-17T14:19:02.819773Z","iopub.status.idle":"2021-08-17T14:19:03.039556Z","shell.execute_reply.started":"2021-08-17T14:19:02.819742Z","shell.execute_reply":"2021-08-17T14:19:03.038441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-08-17T14:15:43.310059Z","iopub.execute_input":"2021-08-17T14:15:43.310502Z","iopub.status.idle":"2021-08-17T14:15:43.332242Z","shell.execute_reply.started":"2021-08-17T14:15:43.31047Z","shell.execute_reply":"2021-08-17T14:15:43.329433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"the rest can give error","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom collections import Counter\n\ntokenizee=[]\nfor words in tqdm(range(1,len(df.content)-1)):\n    tokenizee.append(spacy_process(df.content[words]))\n    \n#overview of preprocessed data\nwords = Counter()\nfor s in tokenizee:\n  for w in s:\n    words[w] += 1\n\nsorted_words = list(words.keys())\nsorted_words.sort(key=lambda w: words[w], reverse=True)\nprint(f\"Number of different Tokens in our Dataset: {len(sorted_words)}\")\nprint(sorted_words[:100])\n\n\ncount_occurences = sum(words.values())\naccumulated = 0\ncounter = 0\nwhile accumulated < count_occurences * 0.8:\n  accumulated += words[sorted_words[counter]]\n  counter += 1\n\nprint(f\"The {counter * 100 / len(words)}% most common words \"\n      f\"account for the {accumulated * 100 / count_occurences}% of the occurrences\")\n\nplt.bar(range(100), [words[w] for w in sorted_words[:100]])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-13T06:04:12.309516Z","iopub.execute_input":"2021-08-13T06:04:12.309845Z","iopub.status.idle":"2021-08-13T06:06:55.075939Z","shell.execute_reply.started":"2021-08-13T06:04:12.309816Z","shell.execute_reply":"2021-08-13T06:06:55.072962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer\nfrom torch.utils.data import TensorDataset\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased',\n                                         #all lower case\n                                         do_lower_case = True,\n                                         )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"TRY DIFFERENT MAX_LENGTHS\nhere for tweets I used 50","metadata":{}},{"cell_type":"code","source":"#batch using multiple strings and convert them into tokens\nencoded_data_train = tokenizer.batch_encode_plus(\n        df[df.data_type == 'train'].content.values,\n        add_special_tokens = True,\n        #to know when sentence begins and ends\n        return_attention_mask = True,\n        #set max length to large values for big sentences\n        padding = True,\n        truncation=True, ###\n        max_length = 40,\n        return_tensors = 'pt'\n        #pt: pytorch\n        )\n\nencoded_data_val = tokenizer.batch_encode_plus(\n        df[df.data_type == 'val'].content.values,\n        add_special_tokens = True,\n        #to know when sentence begins and ends\n        return_attention_mask = True,\n        #set max length to large values for big sentences\n        padding = True,\n        truncation=True, ###\n        max_length = 40,\n        return_tensors = 'pt'\n        #pt: pytorch\n        )\n\ninput_ids_train = encoded_data_train['input_ids']\nattention_masks_train = encoded_data_train['attention_mask']\nlabels_train = torch.tensor(df[df.data_type == 'train'].label.values)\n\ninput_ids_val = encoded_data_val['input_ids']\nattention_masks_val = encoded_data_val['attention_mask']\nlabels_val = torch.tensor(df[df.data_type == 'val'].label.values)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T23:13:40.111314Z","iopub.execute_input":"2021-08-12T23:13:40.111674Z","iopub.status.idle":"2021-08-12T23:13:54.609167Z","shell.execute_reply.started":"2021-08-12T23:13:40.111642Z","shell.execute_reply":"2021-08-12T23:13:54.607001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"preprocess the data lemmatization","metadata":{}},{"cell_type":"code","source":"for eachS in range(0,5):\n    print(tokenizer.decode(input_ids_train[eachS]))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-12T18:30:10.863831Z","iopub.execute_input":"2021-08-12T18:30:10.864181Z","iopub.status.idle":"2021-08-12T18:30:10.872872Z","shell.execute_reply.started":"2021-08-12T18:30:10.864146Z","shell.execute_reply":"2021-08-12T18:30:10.871657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_train = TensorDataset(input_ids_train,\n                              attention_masks_train,\n                              labels_train)\n\ndataset_val = TensorDataset(input_ids_val,\n                              attention_masks_val,\n                              labels_val)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:59:12.28189Z","iopub.execute_input":"2021-08-12T17:59:12.282208Z","iopub.status.idle":"2021-08-12T17:59:12.288822Z","shell.execute_reply.started":"2021-08-12T17:59:12.282173Z","shell.execute_reply":"2021-08-12T17:59:12.287904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(dataset_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:59:12.290676Z","iopub.execute_input":"2021-08-12T17:59:12.291279Z","iopub.status.idle":"2021-08-12T17:59:12.301109Z","shell.execute_reply.started":"2021-08-12T17:59:12.29124Z","shell.execute_reply":"2021-08-12T17:59:12.300279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(dataset_val)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:59:12.302347Z","iopub.execute_input":"2021-08-12T17:59:12.302805Z","iopub.status.idle":"2021-08-12T17:59:12.310444Z","shell.execute_reply.started":"2021-08-12T17:59:12.302767Z","shell.execute_reply":"2021-08-12T17:59:12.30945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Task 5: Setting up BERT Pretrained Model","metadata":{}},{"cell_type":"code","source":"from transformers import BertForSequenceClassification","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:48:49.856786Z","iopub.execute_input":"2021-08-12T17:48:49.857255Z","iopub.status.idle":"2021-08-12T17:48:53.89502Z","shell.execute_reply.started":"2021-08-12T17:48:49.857216Z","shell.execute_reply":"2021-08-12T17:48:53.894005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#each sequence will be dealt separate classification\nmodel = BertForSequenceClassification.from_pretrained(\n    'bert-base-uncased',\n    #the other cased one is larger and takes more computation power\n    #we want to fine tune the parts we need\n    num_labels = len(label_dict),\n    output_attentions = False,\n    output_hidden_states = False\n                                     )\n#450 MB needs to be fetched and loaded into memory\n#bert takes into text and encodes into meaningful way according to the huge corpus it was intitially exposed to\n#we are just lying on top of it to get our 6 classes classifier","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:59:20.169644Z","iopub.execute_input":"2021-08-12T17:59:20.170003Z","iopub.status.idle":"2021-08-12T17:59:22.332213Z","shell.execute_reply.started":"2021-08-12T17:59:20.169966Z","shell.execute_reply":"2021-08-12T17:59:22.331382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Task 6: Creating Data Loaders","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader, RandomSampler, SequentialSampler     #will use for training\n \n                    #will use for our validation dataset, gradients are fixed","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:59:33.665526Z","iopub.execute_input":"2021-08-12T17:59:33.665876Z","iopub.status.idle":"2021-08-12T17:59:33.671297Z","shell.execute_reply.started":"2021-08-12T17:59:33.665847Z","shell.execute_reply":"2021-08-12T17:59:33.670224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"BATCH SIZE CAN BE INCREASED:","metadata":{}},{"cell_type":"code","source":"batch_size = 8   #very small due to machine low specs but can increase to 32\n\ndataloader_train = DataLoader(\n            dataset_train,\n            sampler = RandomSampler(dataset_train),\n            #to avoid it learning from any sequences\n            batch_size = batch_size\n            )\n\ndataloader_val = DataLoader(\n            dataset_val,\n            sampler = RandomSampler(dataset_val),\n            #to avoid it learning from any sequences\n            batch_size = 32    #here no many computation, no backpropagation\n            )","metadata":{"execution":{"iopub.status.busy":"2021-08-12T18:16:18.299557Z","iopub.execute_input":"2021-08-12T18:16:18.299884Z","iopub.status.idle":"2021-08-12T18:16:18.306539Z","shell.execute_reply.started":"2021-08-12T18:16:18.299854Z","shell.execute_reply":"2021-08-12T18:16:18.304246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Task 7: Setting Up Optimizer and Scheduler","metadata":{}},{"cell_type":"code","source":"#Optimizer defines our learning rate and how it changed throught each epoch\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n#Adam with weight decay, stochastic optimizer","metadata":{"execution":{"iopub.status.busy":"2021-08-12T18:00:08.645277Z","iopub.execute_input":"2021-08-12T18:00:08.645666Z","iopub.status.idle":"2021-08-12T18:00:08.649185Z","shell.execute_reply.started":"2021-08-12T18:00:08.645635Z","shell.execute_reply":"2021-08-12T18:00:08.648347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"LEARNING RATE:","metadata":{}},{"cell_type":"code","source":"optimizer = AdamW(\n                model.parameters(),\n                lr = 1e-5,         #recommended: 2e-5 > 5e-5\n                eps = 1e-8,\n                )","metadata":{"execution":{"iopub.status.busy":"2021-08-12T18:00:10.629032Z","iopub.execute_input":"2021-08-12T18:00:10.629344Z","iopub.status.idle":"2021-08-12T18:00:10.637425Z","shell.execute_reply.started":"2021-08-12T18:00:10.629314Z","shell.execute_reply":"2021-08-12T18:00:10.63639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"NUMBER OF EPOCHS","metadata":{}},{"cell_type":"code","source":"epochs = 2\n\nschedular = get_linear_schedule_with_warmup(\n        optimizer,     #Adam\n        num_warmup_steps = 0,\n        num_training_steps = len(dataloader_train)*epochs\n        )","metadata":{"execution":{"iopub.status.busy":"2021-08-12T18:02:36.386235Z","iopub.execute_input":"2021-08-12T18:02:36.386593Z","iopub.status.idle":"2021-08-12T18:02:36.391146Z","shell.execute_reply.started":"2021-08-12T18:02:36.38656Z","shell.execute_reply":"2021-08-12T18:02:36.390209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Task 8: Defining our Performance Metrics","metadata":{}},{"cell_type":"markdown","source":"Accuracy metric approach originally used in accuracy function in [this tutorial](https://mccormickml.com/2019/07/22/BERT-fine-tuning/#41-bertforsequenceclassification).","metadata":{}},{"cell_type":"code","source":"import numpy as np","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:49:07.156103Z","iopub.execute_input":"2021-08-12T17:49:07.156833Z","iopub.status.idle":"2021-08-12T17:49:07.163246Z","shell.execute_reply.started":"2021-08-12T17:49:07.156791Z","shell.execute_reply":"2021-08-12T17:49:07.162433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:49:07.16452Z","iopub.execute_input":"2021-08-12T17:49:07.165011Z","iopub.status.idle":"2021-08-12T17:49:07.173339Z","shell.execute_reply.started":"2021-08-12T17:49:07.164974Z","shell.execute_reply":"2021-08-12T17:49:07.172465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"preds = [0.9 0.05 0.05 0 0 0]\n\nwe want to convert it to [1 0 0 0 0 0]","metadata":{}},{"cell_type":"code","source":"#f1-score is good bec. of class imbalance\n#accuracy alone will give me skewed results,\n    #based on f1-score not actually representing what we want\n\ndef f1_score_func(preds, labels):\n    preds_flat = np.argmax(preds, axis = 1).flatten()\n    #flatten to get single list and not array\n    labels_flat = labels.flatten()\n    return f1_score(labels_flat, preds_flat, average = 'weighted')\n#can changed weighted to macro","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:49:07.176266Z","iopub.execute_input":"2021-08-12T17:49:07.176642Z","iopub.status.idle":"2021-08-12T17:49:07.183866Z","shell.execute_reply.started":"2021-08-12T17:49:07.176604Z","shell.execute_reply":"2021-08-12T17:49:07.183006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def accuracy_per_class(preds, labels):\n    label_dict_inverse = {v: k for k, v in label_dict.items()}\n    \n    preds_flat = np.argmax(preds, axis = 1).flatten()\n    labels_flat = labels.flatten()\n    \n    for label in np.unique(labels_flat):\n        y_preds = preds_flat[labels_flat == label]\n#here we are using numpy indexing to index 2 array of the same shape by each other\n        y_true = labels_flat[labels_flat == label]\n        print(f'Class: {label_dict_inverse[label]}')\n        print(f'Accuracy:  {len(y_preds[y_preds == label])}/{len(y_true)}\\n')","metadata":{"execution":{"iopub.status.busy":"2021-08-12T18:14:21.423559Z","iopub.execute_input":"2021-08-12T18:14:21.423875Z","iopub.status.idle":"2021-08-12T18:14:21.429974Z","shell.execute_reply.started":"2021-08-12T18:14:21.423846Z","shell.execute_reply":"2021-08-12T18:14:21.428731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Task 9: Creating our Training Loop","metadata":{}},{"cell_type":"markdown","source":"Approach adapted from an older version of HuggingFace's `run_glue.py` script. Accessible [here](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128).","metadata":{}},{"cell_type":"code","source":"import random\n\nseed_val = 17\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T18:01:30.304503Z","iopub.execute_input":"2021-08-12T18:01:30.304829Z","iopub.status.idle":"2021-08-12T18:01:30.309397Z","shell.execute_reply.started":"2021-08-12T18:01:30.304799Z","shell.execute_reply":"2021-08-12T18:01:30.308257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n#send model to device we are using\nmodel.to(device)\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T18:01:31.624412Z","iopub.execute_input":"2021-08-12T18:01:31.624737Z","iopub.status.idle":"2021-08-12T18:01:31.746921Z","shell.execute_reply.started":"2021-08-12T18:01:31.624707Z","shell.execute_reply":"2021-08-12T18:01:31.746017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2021-08-12T18:01:37.117186Z","iopub.execute_input":"2021-08-12T18:01:37.11755Z","iopub.status.idle":"2021-08-12T18:01:37.124406Z","shell.execute_reply.started":"2021-08-12T18:01:37.117518Z","shell.execute_reply":"2021-08-12T18:01:37.123121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(dataloader_val):\n\n    model.eval()\n    \n    loss_val_total = 0\n    predictions, true_vals = [], []\n    \n    for batch in tqdm(dataloader_val):\n        \n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1],\n                  'labels':         batch[2],\n                 }\n\n        with torch.no_grad():        \n            outputs = model(**inputs)\n            \n        loss = outputs[0]\n        logits = outputs[1]\n        loss_val_total += loss.item()\n\n        logits = logits.detach().cpu().numpy()\n        label_ids = inputs['labels'].cpu().numpy()\n        predictions.append(logits)\n        true_vals.append(label_ids)\n    \n    loss_val_avg = loss_val_total/len(dataloader_val) \n    \n    predictions = np.concatenate(predictions, axis=0)\n    true_vals = np.concatenate(true_vals, axis=0)\n            \n    return loss_val_avg, predictions, true_vals\n","metadata":{"execution":{"iopub.status.busy":"2021-08-12T18:02:49.797917Z","iopub.execute_input":"2021-08-12T18:02:49.798332Z","iopub.status.idle":"2021-08-12T18:02:49.810573Z","shell.execute_reply.started":"2021-08-12T18:02:49.798291Z","shell.execute_reply":"2021-08-12T18:02:49.807746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BEHOLD THE TRAINING!!!","metadata":{}},{"cell_type":"code","source":"print(f'total epochs:{epochs}')\nfor epoch in tqdm(range(1, epochs+1)):\n    print(f'epoch # {epoch}')\n    model.train()\n    \n    loss_train_total = 0\n    #we set it initially as 0\n    \n    progress_bar = tqdm(dataloader_train,\n                        desc = f'Epoch {epoch}',\n                        leave = False,   #overwrite after each epoch\n                        disable = False                        \n                       )\n    #to see where are we, has it crashed\n    \n    for batch in progress_bar:\n#         print(f\"{}\")\n        model.zero_grad()\n        #gradient set to zero\n        \n        batch = tuple(b.to(device) for b in batch)\n        #this is imp for cuda gpu use\n        \n        inputs = {\n            'input_ids':         batch[0],\n            'attention_mask':    batch[1],\n            'labels' :           batch[2]\n        }\n        \n        outputs = model(**inputs)\n        #outputs dictionary directly into inputs\n        \n        loss = outputs[0]\n        loss_train_total += loss.item()\n        loss.backward() #?\n        \n        \n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        #clip our gradient\n        #take gradient and give it normal value that we provide as 1\n        #stop gradients from slipping into becoming exceptionally small or too big\n        #promote generalization\n        \n        optimizer.step()\n        schedular.step()\n#         stroftrainloss = loss.item()/len(batch)\n#         progress_bar.set_postfix(f'training_loss: {stroftrainloss}')\n        #append small dictionary\n        \n#     torch.save(model.state_dict(), f'/kaggle/working/Bert_ft_epoch{epoch}.model')\n        \n    tqdm.write(f'\\nEpoch {epoch}')\n        \n    loss_train_avg = loss_train_total/len(dataloader_train)\n    tqdm.write(f'Training loss: {loss_train_avg}')\n        \n    val_loss, predictions, true_vals = evaluate(dataloader_val)\n        #this is imp if over training\n        #model will have no generalization abilities\n    val_f1 = f1_score_func(predictions, true_vals)\n    tqdm.write(f'Validation loss: {val_loss}')\n    tqdm.write(f'F1 Score (weighted): {val_f1}')\n        \n#Cpu takes 40 minutes\n#gpu takes 30 seconds","metadata":{"execution":{"iopub.status.busy":"2021-08-12T18:03:07.115672Z","iopub.execute_input":"2021-08-12T18:03:07.115988Z","iopub.status.idle":"2021-08-12T18:10:51.683306Z","shell.execute_reply.started":"2021-08-12T18:03:07.115957Z","shell.execute_reply":"2021-08-12T18:10:51.682296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Task 10: Loading and Evaluating our Model","metadata":{}},{"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n                                                      num_labels=len(label_dict),\n                                                      output_attentions=False,\n                                                      output_hidden_states=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T18:11:10.114721Z","iopub.execute_input":"2021-08-12T18:11:10.115077Z","iopub.status.idle":"2021-08-12T18:11:11.896877Z","shell.execute_reply.started":"2021-08-12T18:11:10.115046Z","shell.execute_reply":"2021-08-12T18:11:11.896036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.to(device)\npass   #to not get alot of text output","metadata":{"execution":{"iopub.status.busy":"2021-08-12T18:11:17.690065Z","iopub.execute_input":"2021-08-12T18:11:17.690394Z","iopub.status.idle":"2021-08-12T18:11:17.810727Z","shell.execute_reply.started":"2021-08-12T18:11:17.69035Z","shell.execute_reply":"2021-08-12T18:11:17.809894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(\n    torch.load('Models/finetuned_bert_epoch_1_gpu_trained.model',\n              map_location = torch.device('cpu')))","metadata":{"execution":{"iopub.status.busy":"2021-08-12T18:11:20.475673Z","iopub.execute_input":"2021-08-12T18:11:20.47602Z","iopub.status.idle":"2021-08-12T18:11:20.506532Z","shell.execute_reply.started":"2021-08-12T18:11:20.475989Z","shell.execute_reply":"2021-08-12T18:11:20.505206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_, prediction, true_vals = evaluate(dataloader_val)\n#7 batches\n#will take almost 2 minutes","metadata":{"execution":{"iopub.status.busy":"2021-08-12T18:11:58.25956Z","iopub.execute_input":"2021-08-12T18:11:58.259887Z","iopub.status.idle":"2021-08-12T18:12:06.039589Z","shell.execute_reply.started":"2021-08-12T18:11:58.259857Z","shell.execute_reply":"2021-08-12T18:12:06.038534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_per_class(prediction, true_vals)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T18:14:27.970173Z","iopub.execute_input":"2021-08-12T18:14:27.970503Z","iopub.status.idle":"2021-08-12T18:14:27.981825Z","shell.execute_reply.started":"2021-08-12T18:14:27.970473Z","shell.execute_reply":"2021-08-12T18:14:27.980873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#To summarize:\n# model was trained on Google colab --GPU Instance(k80)\n# batch size  = 32\n# epoch = 10","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}