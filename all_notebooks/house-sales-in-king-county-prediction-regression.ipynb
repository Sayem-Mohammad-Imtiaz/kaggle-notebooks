{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nIn this notebook I load and preprocess a dataset, containing house sale observations for King County, and Seattle  to train and test a multivariate log linear regregession to predict house prices. The king County dataset,was collected between May 2014 and May 2015 and Contains 21613 rows × 21 columns\n"},{"metadata":{},"cell_type":"markdown","source":"## Table of Content \n    \n   \n   1. [Variables Overview](#cell1)\n   2. [Importaing relevant libraries](#cell2)\n   3. [importing dataset](#cell4)\n   4. [preprocessing dataset](#cell5)\n       - Dealing with missing values\n       - Check for duplicate values\n       - Exploring The descriptive statistics of the variables\n       - Exploring PDF(Probility Distribution Functions)\n       - Checking Of Least Squared (OLS) Assumptions\n       - Relaxing OLS assumptions(log Transformation)\n       - Relaxing assumptions\n       - checking for multicolinarity\n   5. [Linear Regression Model](#cell6)\n        - Declaring depedent and indepedent Variable\n        - Scaling data\n        - Train_Test_Split data\n        - fitting model\n   6. [Checking Results of Linear Regression model](#cell7)\n        - Scatter Plot (y_trained vs Predicted X_train)\n        - residual PDf\n        - R^2 score\n        - features and weights\n   7. [Testing](#cell8)\n         - Scatter Plot(y_test vs Predicted X_test)\n         - Actual Value,Predicted Value and Differences chart\n    \n    \n   "},{"metadata":{},"cell_type":"markdown","source":"\n## Variables Overview <a id=\"cell1\"></a>\n\n**id** - Unique ID for each home sold\n\n**date** - Date of the home sale\n\n**price** - Price of each home sold\n\n**bedrooms** - Number of bedrooms\n\n**bathrooms** - Number of bathrooms, where .5 accounts for a room with a toilet but no shower\n\n**sqft_living** - Square footage of the apartments interior living space\n\n**sqft_lot** - Square footage of the land space\n\n**floors** - Number of floors\n\n**waterfront** - A dummy variable for whether the apartment was overlooking the waterfront or not\n\n**view** - An index from 0 to 4 of how good the view of the property was\n\n**condition** - An index from 1 to 5 on the condition of the apartment,\n\n**grade** - An index from 1 to 13, where 1-3 falls short of building construction and design, 7 has an average level of construction and design, and 11-13 have a high quality level of construction and design.\n\n**sqft_above** - The square footage of the interior housing space that is above ground level\n\n**sqft_basement** - The square footage of the interior housing space that is below ground level\n\n**yr_built** - The year the house was initially built\n\n**yr_renovated** - The year of the house’s last renovation\n\n**zipcode** - What zipcode area the house is in\n\n**lat** - Lattitude\n\n**long** - Longitude\n\n**sqft_living15** - The square footage of interior housing living space for the nearest 15 neighbors\n\n**sqft_lot15** - The square footage of the land lots of the nearest 15 neighbors\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Importing Relevant libraries<a id=\"cell2\"></a>\nI import pandas for data manipulation and analysis, matplotlib to visualize data in charts and graphs and seaborn for optimal visualization. I also changed the panda's display format so it will not show scientific notation when visualizing data.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n#set the pandas display format so it will not use scientific notation\npd.set_option('display.float_format', lambda x: '%.3f' % x)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing Dataset<a id=\"cell4\"></a>\nI use the panadas method to read the csv and load it into the raw_data and visualize the first five rows and columns of the dataset; noticing 'date' had extra characters, I strip the extra letters."},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data = pd.read_csv('../input/housesalesprediction/kc_house_data.csv')\nraw_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data['date'] = raw_data['date'].str.replace('T000000', '')\nraw_data['date'] = raw_data['date'].astype(float)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing<a id=\"cell5\"></a>\nWith the dataset successfully loaded, I move on to Preprocessing the dataset. "},{"metadata":{},"cell_type":"markdown","source":"### Dealing with missing values\nisnull().sum() is used on the dataset to find all the null values and return it."},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check for duplicate values \nTo check for duplicated values, i use the .duplicated function ad .sum to return the value of duplicated values"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Since the dataset contained no missing values or duplicated values, i change the dataset to data with no missing values.\ndata_no_mv = raw_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploring The Descriptive statistics of the variables \nUsing the pandas .describe() function to pull the statistacial values of the features in the dataset. A couple of things to take note of when obsererving the chart is huge differnces in Max, mean and the percentales. First thing I notice in respect to the statistical data is the Max of price amounting to 7700000.00 while under 75% price payed is 450000.000 with a mean of 540088.142."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_no_mv.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploring PDF(Probility Distribution Functions) Of features\nI take the data with no missing values and plot the PDF for variables with weird-looking descriptive values for further observation. I am looking for outliers in the features and good distribution. Outliers are observations that lay away from the vast majority of observations and can throw off the model's predictive ability. A great way to remove outliers is to eliminate a percentile or filter the data.\n        \n        "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data_no_mv['price'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#using the .quantile method I the 1% of data from \"price\" variable to handle outliers and achieve a normal distrubtion for optimal regression results\nz = data_no_mv['price'].quantile(0.99)\ndata_1 = data_no_mv[data_no_mv['price']<z]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data_1['price'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the distribution, i noticed outliers that range from up to 35. I isolate the bedrooms feature to visualize the column.\nAfter a search of king county and Seattle houses in Zillow, i find they do not pass 20, so i remove all entries with more than 16 since after 16, it seemed not many places were available "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data_1['bedrooms'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bedrms = pd.DataFrame(raw_data['bedrooms'])\nbedrms = bedrms.dropna(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bedrms.sort_values(by='bedrooms')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_2 = data_1[data_1['bedrooms']<8]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"sns.distplot(data_2['bedrooms'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data_2['sqft_lot'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"using the .quantile method I use 95% of observations from \"sqft_lot\" variable to handle outliers and achieve a normal distribution for optimal regression results"},{"metadata":{"trusted":true},"cell_type":"code","source":"z = data_2['sqft_lot'].quantile(0.95)\ndata_3 = data_2[data_2['sqft_lot']<z]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data_3['sqft_lot'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data_3['sqft_above'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"using the .quantile method I use 99% of observations from \"sqft_above\" variable to handle outliers and achieve a normal distribution for optimal regression results"},{"metadata":{"trusted":true},"cell_type":"code","source":"z = data_3['sqft_above'].quantile(0.99)\ndata_4 = data_3[data_3['sqft_above']<z]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data_4['sqft_above'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Index reset \nI reset the index of the data and drop it into data_cleaned variable "},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cleaned = data_3.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cleaned.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking Of Least Squared (OLS) Assumptions\nI use a scatter plot to plot possible predictors against \"price\" to check for linearity using the of least squared assumptions "},{"metadata":{"trusted":true},"cell_type":"code","source":"f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize =(15,3))\nax1.scatter(data_cleaned['bedrooms'],data_cleaned['price'])\nax1.set_title('price and bedrooms')\n\nax2.scatter(data_cleaned['sqft_living'],data_cleaned['price'])\nax2.set_title('price and sqft_living')\n\nax3.scatter(data_cleaned['yr_built'],data_cleaned['price'])\nax3.set_title('price and yr_built')\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, sharey=True, figsize =(15,3))\nax1.scatter(data_cleaned['grade'],data_cleaned['price'])\nax1.set_title('price and grade')\n\nax2.scatter(data_cleaned['sqft_lot'],data_cleaned['price'])\nax2.set_title('price and sqft_lot')\n\nax3.scatter(data_cleaned['condition'],data_cleaned['price'])\nax3.set_title('price and condition')\n\n\nax4.scatter(data_cleaned['sqft_above'],data_cleaned['price'])\nax4.set_title('price and sqft_above')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Relaxing  assumptions\nUsng .np.log to transfrom 'price' to 'Log_price' to create better linearty against other variables and drop price. Log returns the natural logarithm of a number and relaxs assumptions fro better model fit."},{"metadata":{"trusted":true},"cell_type":"code","source":"log_price = np.log(data_cleaned['price'])\ndata_cleaned['Log_price'] = log_price\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize =(15,3))\nax1.scatter(data_cleaned['bedrooms'],data_cleaned['Log_price'])\nax1.set_title('price and bedrooms')\n\nax2.scatter(data_cleaned['sqft_living'],data_cleaned['Log_price'])\nax2.set_title('price and sqft_living')\n\nax3.scatter(data_cleaned['yr_built'],data_cleaned['Log_price'])\nax3.set_title('price and yr_built')\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, sharey=True, figsize =(15,3))\nax1.scatter(data_cleaned['grade'],data_cleaned['Log_price'])\nax1.set_title('price and grade')\n\nax2.scatter(data_cleaned['sqft_lot'],data_cleaned['Log_price'])\nax2.set_title('price and sqft_lot')\n\nax3.scatter(data_cleaned['condition'],data_cleaned['Log_price'])\nax3.set_title('price and condition')\n\n\nax4.scatter(data_cleaned['sqft_above'],data_cleaned['Log_price'])\nax4.set_title('price and sqft_above')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"data_cleaned = data_cleaned.drop(['price'], axis=1)\ndata_cleaned","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking for multicolinarity\nTo check the Multicolinarity assumption i import Variance_ inflation from stats model, None of the features break this assumption"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvariables = data_cleaned\nvif = pd.DataFrame()\nvif[\"VIF\"] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]\nvif[\"features\"] = variables.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting data_cleaned variable to data_pre_process since the preprocessing was done\ndata_pre_proc = data_cleaned","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear Regression Model<a id=\"cell6\"></a>\n\n### Declaring depedent and indepedent Variable\nDeclaring independent and dependent variables, for independent(x) log_price was dropped since it's the dependent variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = data_pre_proc.drop(['Log_price'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets = data_pre_proc['Log_price']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scaling data\nImporting and using the standard scaler function from sklearn to scale the indepedent variables , so that all the features hold a standard weight towards the depedent variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(inputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_scaled = scaler.transform(inputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_scaled","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### Train_Test_Split data \nSetting a 80/20 split, splitting the training data into 80 and the test data to 20 with a random state of 9"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x_scaled,targets,test_size= 0.20,random_state=9)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fitting model\nFitting the Linear regression module with training data and checking results by creating a scatter plot and plotting the predicted values against the observed values. I also create a Residual PDF using the difference between targets and predictions to visualize the error estimate."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nreg = LinearRegression()\nreg.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.get_params()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking Results<a id=\"cell7\"></a>\n\n### Scatter plot \n\nPlotting predicted values against the observed values to check the results"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat = reg.predict(x_train)\ny_hat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(y_train ,y_hat, alpha=0.2)\nplt.xlabel('Targets (Y_train)', size=15)\nplt.ylabel('Predictions  (Y_hat)', size=15)\nplt.xlim(11,15)\nplt.ylim(11,15)\nplt.title('Actual vs Predicted')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Residual PDf"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Residual shows difference and mean between the targets and predictions \nsns.distplot(y_train - y_hat)\nplt.show","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### R^2 score\nThe R2 score being 76% signifies response variable variation that the linear Regression Model explains"},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.score(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### features and weights\nChecking how much weight each feature has into predicting the price. While positive weight increases, so do price. If it is decreased, so is the price. Values are standardized. Could be used for feature selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"reg_summary = pd.DataFrame(inputs.columns.values, columns=['Features'])\nreg_summary['weights'] = reg.coef_\nreg_summary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Testing<a id=\"cell8\"></a>\nPlotting the predicted and testing data in a scatter plot to show efficency of model predictions, \n\n### Scatter Plot(Trained vs Predicted X_test)"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat_test = reg.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(y_test ,y_hat_test, alpha=0.2)\nplt.xlabel('Targets (Y_train)', size=15)\nplt.ylabel('Predictions  (Y_hat_test)', size=15)\nplt.xlim(11.5,15)\nplt.ylim(11.5,15)\nplt.title('Targets ''Y_train'' vs Predicted')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Actual Value Predicted Value Differences chart\nThe Linear Regression model's final test is to test how good the predictions hold up to the actual data. For this, i use the NumPy method to transform the variables back to their original form. Create a prediction column using the x_test predicated data. I then take y_test and target columns by transforming the data using NumPy.exp and reset the index. I finish it off by visualizing a new dataset with new columns containing predictions, target residuals, differences in percentage to show this model's efficiency."},{"metadata":{"trusted":true},"cell_type":"code","source":"predv =pd.DataFrame(np.exp(y_hat_test), columns=['Predictions'])\npredv['Target'] = np.exp(y_test)\ny_test = y_test.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predv['Residual'] = predv['Target'] - predv['Predictions']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predv['Difference%'] = np.absolute(predv['Residual']/predv['Target']*100)\npredv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}