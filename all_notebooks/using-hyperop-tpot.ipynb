{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport plotly_express as px\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBClassifier\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/heart-disease-uci/heart.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('ggplot')\ng = sns.countplot(x=\"target\", data=data, palette=\"bwr\")\nsns.despine()\ng.figure.set_size_inches(12,7)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting the data into Cross-Validation and Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.iloc[:,:-1]\ny = data['target']\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state=0)\n\nprint(X_train.shape,X_test.shape)\nprint(y_train.shape,y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Running Hyperopt"},{"metadata":{},"cell_type":"markdown","source":"## 1. Importing the Required Library for Hyperopt"},{"metadata":{"trusted":true},"cell_type":"code","source":"from hyperopt import hp, fmin, tpe, STATUS_OK, Trials","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Create the objective function\n\nHere we create an objective function which takes as input a hyperparameter space:\n- defines a classifier, in this case XGBoost. Just try to see how we take the parameters from the space. For example `space['max_depth']` \n- We fit the classifier to the train data\n- We predict on cross validation set\n- We calculate the required metric we want to maximize or minimize\n- Since we only minimize using `fmin` in hyperopt, if we want to minimize `logloss` we just send our metric as is. If we want to maximize accuracy we will try to minimize `-accuracy`"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom hyperopt import hp, fmin, tpe, STATUS_OK, Trials\nimport numpy as np\ndef objective(space):\n    # Instantiate the classifier\n    clf = XGBClassifier(n_estimators =1000,colsample_bytree=space['colsample_bytree'],\n                           learning_rate = .3,\n                            max_depth = int(space['max_depth']),\n                            min_child_weight = space['min_child_weight'],\n                            subsample = space['subsample'],\n                            gamma = space['gamma'],\n                           reg_lambda = space['reg_lambda'])\n    \n    eval_set  = [( X_train, y_train), ( X_test, y_test)]\n    \n    # Fit the classsifier\n    clf.fit(X_train, y_train,\n            eval_set=eval_set, early_stopping_rounds=10,verbose=False)\n    \n    # Predict on Cross Validation data\n    pred = clf.predict(X_test)\n    \n    # Calculate our Metric - accuracy\n    accuracy = accuracy_score(y_test, pred>0.5)\n\n    # return needs to be in this below format. We use negative of accuracy since we want to maximize it.\n    return {'loss': -accuracy, 'status': STATUS_OK }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Create the Space for your classifier\n\nNow, we create the search space for hyperparameters for our classifier.\n\nTo do this we end up using many of hyperopt built in functions which define verious distributions. As you can see we use uniform distribution between 0.7 and 1 for our subsample hyperparameter. It is much better than defining a parameter value using ranges for sure. You can also define a lot of other distributions too. \n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"space ={'max_depth': hp.quniform(\"x_max_depth\", 4, 16, 1),\n        'min_child_weight': hp.quniform ('x_min_child', 1, 10, 1),\n        'subsample': hp.uniform ('x_subsample', 0.7, 1),\n        'gamma' : hp.uniform ('x_gamma', 0.1,0.5),\n        'colsample_bytree' : hp.uniform ('x_colsample_bytree', 0.7,1),\n        'reg_lambda' : hp.uniform ('x_reg_lambda', 0,1)\n    }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Run Hyperopt"},{"metadata":{"trusted":true},"cell_type":"code","source":"trials = Trials()\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=100,\n            trials=trials)\nprint(best)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report,accuracy_score\n\nclf = XGBClassifier(x_colsample_bytree= 0.8743861143035889, x_gamma= 0.15403994099351054, \n                         x_max_depth= 7.0, x_min_child =5.0, x_reg_lambda= 0.015889530822374764, \n                         x_subsample= 0.7716293823039047)\n\nclf.fit(X_train,y_train)\n\ny_pred = clf.predict(X_test)\n\nprint('Confusion Matrix :\\n',pd.crosstab(y_test,y_pred))\n\nprint('Accuracy Socre :',accuracy_score(y_test,y_pred))\n\nprint('\\nClassification Report :\\n',classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Selecting model using TPOT\n<p><a href=\"https://github.com/EpistasisLab/tpot\">TPOT</a> is a Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming.</p>\n<p><img src=\"https://assets.datacamp.com/production/project_646/img/tpot-ml-pipeline.png\" alt=\"TPOT Machine Learning Pipeline\"></p>\n<p>TPOT will automatically explore hundreds of possible pipelines to find the best one for our dataset. Note, the outcome of this search will be a <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\">scikit-learn pipeline</a>, meaning it will include any pre-processing steps as well as the model.</p>\n<p>We are using TPOT to help us zero in on one model that we can then explore and optimize further.</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import TPOTClassifier and roc_auc_score\nfrom tpot import TPOTClassifier\nfrom sklearn.metrics import roc_auc_score\n\n# Instantiate TPOTClassifier\ntpot = TPOTClassifier(\n    generations=5,\n    population_size=20,\n    verbosity=2,\n    scoring='roc_auc',\n    random_state=42,\n    disable_update_check=True,\n    config_dict='TPOT light'\n)\ntpot.fit(X_train, y_train)\n\n# AUC score for tpot model\ntpot_auc_score = roc_auc_score(y_test, tpot.predict_proba(X_test)[:, 1])\nprint(f'\\nAUC score: {tpot_auc_score:.4f}')\n\n# Print best pipeline steps\nprint('\\nBest pipeline steps:', end='\\n')\nfor idx, (name, transform) in enumerate(tpot.fitted_pipeline_.steps, start=1):\n    # Print idx and transform\n    print(f'{idx}.{transform}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing modules\nfrom sklearn.linear_model import LogisticRegression\n\n# Instantiate LogisticRegression\nlogreg = LogisticRegression(C=10.0, class_weight=None,\n                                         dual=False, fit_intercept=True,\n                                         intercept_scaling=1,\n                                         l1_ratio=None, max_iter=100,\n                                         multi_class='warn', n_jobs=None,\n                                         penalty='l2', random_state=None,\n                                         solver='warn', tol=0.0001,\n                                         verbose=0, warm_start=False)\n\n# Train the model\nlogreg.fit(X_train, y_train)\n\n# AUC score for tpot model\nlogreg_auc_score = roc_auc_score(y_test, logreg.predict_proba(X_test)[:, 1])\nprint(f'\\nAUC score: {logreg_auc_score:.4f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = logreg.predict(X_test)\nprint('Accuracy score :',accuracy_score(y_pred,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}