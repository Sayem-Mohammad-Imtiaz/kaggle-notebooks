{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfrom scipy.stats import probplot\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRFRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBRFRegressor, XGBRegressor\nimport catboost as ctb\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder\nfrom sklearn.preprocessing import StandardScaler","metadata":{"execution":{"iopub.status.busy":"2021-07-14T17:33:41.471761Z","iopub.execute_input":"2021-07-14T17:33:41.472265Z","iopub.status.idle":"2021-07-14T17:33:42.874304Z","shell.execute_reply.started":"2021-07-14T17:33:41.472142Z","shell.execute_reply":"2021-07-14T17:33:42.873268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"https://raw.githubusercontent.com/akhil14shukla/Summer-of-Analytics-IITG-Project/master/Train_Data.csv\")\ntest=pd.read_csv(\"https://raw.githubusercontent.com/akhil14shukla/Summer-of-Analytics-IITG-Project/master/Test_Data.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-14T17:33:42.875839Z","iopub.execute_input":"2021-07-14T17:33:42.876119Z","iopub.status.idle":"2021-07-14T17:33:43.428744Z","shell.execute_reply.started":"2021-07-14T17:33:42.87609Z","shell.execute_reply":"2021-07-14T17:33:43.427745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Understanding the Data","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(4, 4, figsize=(25, 20))\nfor i,ax in zip(df.columns,axes.flat):\n    sns.histplot(df[i],ax=ax)\n#     probplot(df[i],dist='norm',plot=ax)\n#     probplot(np.log((df.iloc[:,i])),dist='norm',plot=ax)\n    ax.set_title(i)\nplt.subplots_adjust(wspace=0.2, hspace=0.5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-14T17:37:42.681414Z","iopub.execute_input":"2021-07-14T17:37:42.681872Z","iopub.status.idle":"2021-07-14T17:38:01.894233Z","shell.execute_reply.started":"2021-07-14T17:37:42.681827Z","shell.execute_reply":"2021-07-14T17:38:01.893451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(df['adgroup'],df['revenue'])","metadata":{"execution":{"iopub.status.busy":"2021-07-14T18:01:10.681655Z","iopub.execute_input":"2021-07-14T18:01:10.682055Z","iopub.status.idle":"2021-07-14T18:01:10.905671Z","shell.execute_reply.started":"2021-07-14T18:01:10.682022Z","shell.execute_reply":"2021-07-14T18:01:10.904356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Cleaning and Transformation\n\nPerformed label encoding, dropped the columns (_campaign_ had same value for all the tuples), created new features, which might be useful while building a model.","metadata":{}},{"cell_type":"code","source":"le = LabelEncoder()\n# lb = LabelBinarizer()\ndf['adgroup'] = le.fit_transform(df['adgroup'])\ntest['adgroup']=le.transform(test['adgroup'])\n# df = df.join(pd.DataFrame(lb.fit_transform(df['adgroup'])))\n# test = test.join(pd.DataFrame(lb.transform(test['adgroup'])))\n# df.drop(['adgroup'],axis=1)\n# test.drop(['adgroup'],axis=1)\ndf = df.set_index(df['date'])\ntest = test.set_index(test['date'])\n\ndf[\"impressions\"] = np.log(df[\"impressions\"])\ndf[\"clicks\"] = np.log(df[\"clicks\"])\ndf[\"cost\"] = np.log(df[\"cost\"])\n\ntest[\"impressions\"] = np.log(test[\"impressions\"])\ntest[\"clicks\"] = np.log(test[\"clicks\"])\ntest[\"cost\"] = np.log(test[\"cost\"])\n\ndf.drop(['date', 'campaign'],axis=1,inplace=True)\ntest.drop(['date','campaign'],axis=1,inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-14T17:42:21.722813Z","iopub.execute_input":"2021-07-14T17:42:21.723201Z","iopub.status.idle":"2021-07-14T17:42:21.747377Z","shell.execute_reply.started":"2021-07-14T17:42:21.723168Z","shell.execute_reply":"2021-07-14T17:42:21.746607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Removed the \"ad \" from _ad_ column and converted the rest to numerical type","metadata":{}},{"cell_type":"code","source":"for i in range(len(df)):\n    df['ad'][i] = int(df['ad'][i][3:])\nfor i in range(len(test)):\n    test['ad'][i]=int(test['ad'][i][3:])\ndf['ad']=pd.to_numeric(df['ad'])\ntest['ad']=pd.to_numeric(test['ad'])\ndf.drop(['ad'],axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T17:43:05.422234Z","iopub.execute_input":"2021-07-14T17:43:05.422635Z","iopub.status.idle":"2021-07-14T17:43:08.7989Z","shell.execute_reply.started":"2021-07-14T17:43:05.422603Z","shell.execute_reply":"2021-07-14T17:43:08.797657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sns.pairplot(df,hue=\"revenue\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Created new features, and while doing so, we created infinite or NaN values unknowingly. So, replaced those values, as these are not accepted by models. <br>\nreplaced -infinity with 0 and infinity and NaN with a number greater than the current maximum (randomly chose 65, it could be 100, 10000,...). We just need to preserve the essence of _-infinity_ and _infinity_","metadata":{}},{"cell_type":"code","source":"df['CTR']=df['clicks']/df['impressions']\ndf['CPC']=df['cost']/df['clicks']\ndf['CPA']=df['cost']/df['conversions']\n\ntest['CTR']=test['clicks']/test['impressions']\ntest['CPC']=test['cost']/test['clicks']\ntest['CPA']=test['cost']/test['conversions']\n\ndf['CPC'].fillna(df.CPC.interpolate(),inplace=True)\ndf['CPA'].fillna(df.CPA.interpolate(),inplace=True)\n\ntest['CPC'].fillna(test.CPC.interpolate(),inplace=True)\ntest['CPA'].fillna(test.CPA.interpolate(),inplace=True)\n\ndf.replace([-np.inf], 0,inplace=True)\ntest.replace([-np.inf], 0,inplace=True) \ndf.replace([np.inf,np.nan], 65,inplace=True)\ntest.replace([np.inf,np.nan], 65,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T17:47:26.302356Z","iopub.execute_input":"2021-07-14T17:47:26.302753Z","iopub.status.idle":"2021-07-14T17:47:26.327624Z","shell.execute_reply.started":"2021-07-14T17:47:26.302715Z","shell.execute_reply":"2021-07-14T17:47:26.326668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(df.corr(),cmap=\"YlGnBu\")","metadata":{"execution":{"iopub.status.busy":"2021-07-14T18:01:11.042106Z","iopub.execute_input":"2021-07-14T18:01:11.042588Z","iopub.status.idle":"2021-07-14T18:01:11.377282Z","shell.execute_reply.started":"2021-07-14T18:01:11.042537Z","shell.execute_reply":"2021-07-14T18:01:11.376142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(df)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T18:01:11.711325Z","iopub.execute_input":"2021-07-14T18:01:11.711735Z","iopub.status.idle":"2021-07-14T18:01:52.760374Z","shell.execute_reply.started":"2021-07-14T18:01:11.711699Z","shell.execute_reply":"2021-07-14T18:01:52.759329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Created a custom function for finding Predictive Power Score. Check out the [repository here](https://github.com/akhil14shukla/PyCustom).**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import *\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn import metrics               \nfrom sklearn import preprocessing\nfrom sklearn import utils\nfrom sklearn.metrics import mean_absolute_error\n\n\ndef pps(df,categorical_features=None,numerical_features=None):\n    columns = [\"Feature\", \"Target\",\"PPS\", \"Type of Prediction\", \"Cross-Val Score\", \"Training Score\",\"Naive-Baseline Score\", \"Model\"]\n    pps_df=pd.DataFrame(columns=columns)\n    for i in df.columns:\n        for j in df.columns:\n            if(((categorical_features)!=None and j in categorical_features) or type(df[j])==str):\n                model = DecisionTreeClassifier()\n                lab_enc = preprocessing.LabelEncoder()\n                label_encoded_y = np.array(lab_enc.fit_transform(df[j])).reshape(-1,1)\n                x_train,x_test,y_train,y_test = train_test_split(np.array(df[i]).reshape(-1,1),label_encoded_y)\n                model.fit(x_train,y_train)\n                y_pred=model.predict(x_test)\n                f1 = metrics.f1_score(y_test,y_pred)\n                mode = np.full((len(x_test),1),df[j].mode())\n                f1_naive = metrics.f1_score(y_test,mode)\n                pps_score=max(0,(f1-f1_naive)/(1 - f1_naive))\n                cv_score=model.score(x_test,y_test)\n                train_score=model.score(x_train,y_train)\n                pps_df = pps_df.append({\"Feature\":i,\"Target\":j,\"PPS\":pps_score,\"Type of Prediction\":\"Classification\",\"Cross-Val Score\":cv_score,\"Training Score\":train_score,\"Naive-Baseline Score\":f1_naive,\"Model\":\"DecisionTreeClassifier()\"},ignore_index=True)\n            else:\n                model = DecisionTreeRegressor()\n                # lab_enc = preprocessing.LabelEncoder()\n                # label_encoded_y = np.array(lab_enc.fit_transform(df[j])).reshape(-1,1)\n                x_train,x_test,y_train,y_test = train_test_split(np.array(df[i]).reshape(-1,1),np.array(df[j]).reshape(-1,1))\n                model.fit(x_train,y_train)\n                median = np.full((len(x_test),1),df[j].median())\n                naive_mae = metrics.mean_absolute_error(y_test, median)\n                y_pred=model.predict(x_test)\n                mae=metrics.mean_absolute_error(y_test,y_pred)\n                train_score=mean_absolute_error(y_train,model.predict(x_train))\n                pps_score = max(0,1 - mae/naive_mae)\n                pps_df=pps_df.append({\"Feature\":i,\"Target\":j,\"PPS\":pps_score,\"Type of Prediction\":\"Regression\",\"Cross-Val Score\":mae,\"Training Score\":train_score,\"Naive-Baseline Score\":naive_mae,\"Model\":\"DecisionTreeRegressor()\"},ignore_index=True)\n    \n    return pps_df","metadata":{"execution":{"iopub.status.busy":"2021-07-14T17:49:04.094313Z","iopub.execute_input":"2021-07-14T17:49:04.094994Z","iopub.status.idle":"2021-07-14T17:49:04.11477Z","shell.execute_reply.started":"2021-07-14T17:49:04.094952Z","shell.execute_reply":"2021-07-14T17:49:04.11376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pf = pps(df)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T17:49:06.321406Z","iopub.execute_input":"2021-07-14T17:49:06.322107Z","iopub.status.idle":"2021-07-14T17:49:07.09435Z","shell.execute_reply.started":"2021-07-14T17:49:06.32205Z","shell.execute_reply":"2021-07-14T17:49:07.093598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pf[pf[\"Target\"]==\"revenue\"]","metadata":{"execution":{"iopub.status.busy":"2021-07-14T17:49:07.095908Z","iopub.execute_input":"2021-07-14T17:49:07.096506Z","iopub.status.idle":"2021-07-14T17:49:07.11729Z","shell.execute_reply.started":"2021-07-14T17:49:07.096441Z","shell.execute_reply":"2021-07-14T17:49:07.115948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Used the below library, to ease working with Standard Scaler","metadata":{}},{"cell_type":"code","source":"# df = np.array(df)\nfrom sklearn_pandas import DataFrameMapper","metadata":{"execution":{"iopub.status.busy":"2021-07-14T17:49:09.536069Z","iopub.execute_input":"2021-07-14T17:49:09.536465Z","iopub.status.idle":"2021-07-14T17:49:09.551718Z","shell.execute_reply.started":"2021-07-14T17:49:09.536429Z","shell.execute_reply":"2021-07-14T17:49:09.550561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building the model\nI tried various models CatBoost, XGBoost, RandomForest, XGB turned out to be the best.<br><br>\nThinking behind building the model:<br>\nCreated 4 model for each _adgroup_. As each _adgroup_ might have different distribution or different approach of advertising, which might generate different revenues. To do that, first grouped them based on _adgroup_, then created 4 models, trained them on each _adgroup_(4 in this case) one by one using _for_ loop.<br> \nBut before doing that we need to scale the data, for better performance. Used _StandardScaler()_, so any remaining outliers will be taken care of. But as we are training different models, I decided to use different scaling for each _adgroup_. It seems a lot of work but it was finally worth it.","metadata":{}},{"cell_type":"code","source":"g = df.groupby('adgroup')\n\n# model0 = XGBRFRegressor(n_estimators=10,max_depth=6, min_child_weight=2,subsample=0,learning_rate = 0.3)\n# model0 = lg(kernel='rbf', degree=2, gamma='auto', C = 0.1)\nj=6\n# model0 = ctb.CatBoostRegressor(random_state=0,verbose=False,depth=4,l2_leaf_reg=5,learning_rate=0.4, n_estimators=10000)\n# model1 = ctb.CatBoostRegressor(random_state=0,verbose=False,depth=6,l2_leaf_reg=3,learning_rate=0.01, n_estimators=10000,)\n# model2 = ctb.CatBoostRegressor(random_state=0,verbose=False,depth=5,l2_leaf_reg=8,learning_rate=0.03, n_estimators=10000)\n# model3 = ctb.CatBoostRegressor(random_state=0,verbose=False,depth=5,l2_leaf_reg=8,learning_rate=0.01, n_estimators=10000)\n\nmodel0 = XGBRFRegressor(n_estimators=4000)\nmodel1 = XGBRFRegressor(n_estimators=4000)\nmodel2 = XGBRFRegressor(n_estimators=4000)\nmodel3 = XGBRFRegressor(n_estimators=4000)\n\ndf0 = pd.DataFrame(g.get_group(0))\ny0 = df0['revenue']\ndf0.drop(['revenue'],axis=1,inplace=True)\nmapper0 = DataFrameMapper([(df0.columns, StandardScaler())])\nscaled_features = mapper0.fit_transform(df0.copy())\ndf0 = pd.DataFrame(scaled_features, columns=df0.columns)\n\ndf1 = pd.DataFrame(g.get_group(1))\ny1 = df1['revenue']\ndf1.drop(['revenue'],axis=1,inplace=True)\nmapper1 = DataFrameMapper([(df1.columns, StandardScaler())])\nscaled_features = mapper1.fit_transform(df1.copy())\ndf1 = pd.DataFrame(scaled_features, columns=df1.columns)\n\ndf2 = pd.DataFrame(g.get_group(2))\ny2 = df2['revenue']\ndf2.drop(['revenue'],axis=1,inplace=True)\nmapper2 = DataFrameMapper([(df2.columns, StandardScaler())])\nscaled_features = mapper2.fit_transform(df2.copy())\ndf2 = pd.DataFrame(scaled_features, columns=df2.columns)\n\ndf3 = pd.DataFrame(g.get_group(3))\ny3 = df3['revenue']\ndf3.drop(['revenue'],axis=1,inplace=True)\nmapper3 = DataFrameMapper([(df3.columns, StandardScaler())])\nscaled_features = mapper3.fit_transform(df3.copy())\ndf3 = pd.DataFrame(scaled_features, columns=df3.columns)\n\n# data[f\"df{i}\"]\n\nfor i in range(4):\n    # d_train,d_test,y1_train,y1_test = train_test_split(df,y,random_state=0)\n    grid = {'learning_rate': [0.01, 0.03, 0.1],\n        'depth': [4, 6, 8],\n        'l2_leaf_reg': [0.5, 1, 3],\n        'iteration' : [1500,2000]}\n    if(i==0):\n        d_train,d_test,y1_train,y1_test = train_test_split(df0,y0,random_state=0)\n        model0.fit(d_train,y1_train)\n        # grid_search_result = model0.grid_search(grid, \n        #                                X=d_train, \n        #                                y=y1_train, \n        #                                verbose=False,refit=True, search_by_train_test_split=True)\n        y_pred=model0.predict(d_test)\n        \n    elif (i==1):\n        d_train,d_test,y1_train,y1_test = train_test_split(df1,y1,random_state=0)\n        model1.fit(d_train,y1_train)\n        # grid_search_result = model1.grid_search(grid, \n        #                                X=d_train, \n        #                                y=y1_train, \n        #                                verbose=False,refit=True, search_by_train_test_split=True)\n        y_pred=model1.predict(d_test)\n    elif (i==2):\n        d_train,d_test,y1_train,y1_test = train_test_split(df2,y2,random_state=0)\n        model2.fit(d_train,y1_train)\n        # grid_search_result = model2.grid_search(grid, \n        #                                X=d_train, \n        #                                y=y1_train, \n        #                                verbose=False,refit=True, search_by_train_test_split=True)\n        y_pred=model2.predict(d_test)\n    elif (i==3):\n        d_train,d_test,y1_train,y1_test = train_test_split(df3,y3,random_state=0)\n        model3.fit(d_train,y1_train)\n        # grid_search_result = model3.grid_search(grid, \n        #                                X=d_train, \n        #                                y=y1_train, \n        #                                verbose=False,refit=True, search_by_train_test_split=True) \n        y_pred=model3.predict(d_test)\n    \n    for i in range(len(d_test)):\n        if(y_pred[i]<0):\n            y_pred[i] = 0\n    rms = mean_squared_error(y1_test, y_pred, squared=False)\n    print(rms)\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-07-14T17:57:01.569538Z","iopub.execute_input":"2021-07-14T17:57:01.569951Z","iopub.status.idle":"2021-07-14T17:57:15.096241Z","shell.execute_reply.started":"2021-07-14T17:57:01.569918Z","shell.execute_reply":"2021-07-14T17:57:15.095166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predicting\nPredicting the values for Test Dataset. <br><br>\nIterated through each value of test data set. and depending on its _adgroup_, applied corresponding scaling (using the model created before this cell), and predicted the value using the corresponding model.","metadata":{}},{"cell_type":"code","source":"y_pred=[]\n# x_test.drop('revenue',axis=1,inplace=True)\nx_test = test\nfor i in range(len(x_test)):\n    if(x_test['adgroup'][i]==0):\n        curr = mapper0.transform(pd.DataFrame(x_test.iloc[i]).transpose())\n        y_pred = np.append(y_pred,model0.predict(curr)[0])\n    elif (x_test['adgroup'][i]==1):\n        curr = mapper1.transform(pd.DataFrame(x_test.iloc[i]).transpose())\n        y_pred = np.append(y_pred,model1.predict(curr)[0])\n    elif (x_test['adgroup'][i]==2):\n        curr = mapper2.transform(pd.DataFrame(x_test.iloc[i]).transpose())\n        y_pred = np.append(y_pred,model2.predict(curr)[0])\n    elif (x_test['adgroup'][i]==3):\n        curr = mapper3.transform(pd.DataFrame(x_test.iloc[i]).transpose())\n        y_pred = np.append(y_pred,model3.predict(curr)[0])\n","metadata":{"execution":{"iopub.status.busy":"2021-07-14T17:59:29.942352Z","iopub.execute_input":"2021-07-14T17:59:29.942817Z","iopub.status.idle":"2021-07-14T17:59:34.808362Z","shell.execute_reply.started":"2021-07-14T17:59:29.942774Z","shell.execute_reply":"2021-07-14T17:59:34.807549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model might predict negative revenue in some cases, to take care of that, replaced negative values with 0.","metadata":{}},{"cell_type":"code","source":"for i in range(len(y_pred)):\n    if(y_pred[i]<0):\n        y_pred[i]=0","metadata":{"execution":{"iopub.status.busy":"2021-07-14T18:00:15.981564Z","iopub.execute_input":"2021-07-14T18:00:15.981945Z","iopub.status.idle":"2021-07-14T18:00:15.987424Z","shell.execute_reply.started":"2021-07-14T18:00:15.981913Z","shell.execute_reply":"2021-07-14T18:00:15.986237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred.shape# checking the shape of the final array","metadata":{"execution":{"iopub.status.busy":"2021-07-14T18:00:36.442339Z","iopub.execute_input":"2021-07-14T18:00:36.442766Z","iopub.status.idle":"2021-07-14T18:00:36.449148Z","shell.execute_reply.started":"2021-07-14T18:00:36.442731Z","shell.execute_reply":"2021-07-14T18:00:36.447941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exporting to CSV","metadata":{}},{"cell_type":"code","source":"pd.DataFrame({'revenue':y_pred}).to_csv(\"submission_4.csv\") # Exporting to CSV file","metadata":{"execution":{"iopub.status.busy":"2021-07-14T18:01:10.40228Z","iopub.execute_input":"2021-07-14T18:01:10.40267Z","iopub.status.idle":"2021-07-14T18:01:10.414176Z","shell.execute_reply.started":"2021-07-14T18:01:10.402637Z","shell.execute_reply":"2021-07-14T18:01:10.41299Z"},"trusted":true},"execution_count":null,"outputs":[]}]}