{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 id=\"heading\">\n\n<a class=\"anchor-link\" href=\"https://www.kaggle.com/deb009/predict-customer-churn/notebook#heading\">Â¶</a>\n</h1>\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"from sklearn.model_selection import KFold, cross_val_score, train_test_split\nimport xgboost as xgb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install miceforest\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pandas_profiling import ProfileReport","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = \"../input/hackerearths-reduce-marketing-waste/\"\ntrain = pd.read_csv(f\"{PATH}train.csv\",index_col='Deal_title')\ntest = pd.read_csv(f\"{PATH}test.csv\",index_col='Deal_title')\nsubmission= pd.read_csv(f\"{PATH}sample_submission.csv\",index_col='Deal_title')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_c=pd.read_csv(f\"{PATH}test.csv\",index_col='Deal_title')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Data Exploration","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#adding test and train set together\n\ndf =  pd.concat(objs=[train, test], axis=0).reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"profile = ProfileReport(df)\nprofile","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We will be removing some of the columns based on the count of unique values\ntrain = train.drop([\"Lead_name\",\"Contact_no\",\"POC_name\",\"Lead_POC_email\",\"Date_of_creation\"], axis=1)\ntest = test.drop([\"Lead_name\",\"Contact_no\",\"POC_name\",\"Lead_POC_email\",\"Date_of_creation\"], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As, there are missing values in train and test we will not be dropping the na values.","metadata":{}},{"cell_type":"code","source":"#Two of the columns has dollar sign.We will be removing them now.\ncolstocheck = train.columns\ntrain[colstocheck] = train[colstocheck].replace({r'\\$':''}, regex = True)\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colstocheck = test.columns\ntest[colstocheck] = test[colstocheck].replace({r'\\$':''}, regex = True)\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Coverting deal_value and weighted_amount to type numeric\n\ntest['Weighted_amount'] = pd.to_numeric(test['Weighted_amount'], errors='coerce')\ntrain['Weighted_amount'] = pd.to_numeric(train['Weighted_amount'], errors='coerce')\ntrain['Deal_value'] = pd.to_numeric(train['Deal_value'], errors='coerce')\ntest['Deal_value'] = pd.to_numeric(test['Deal_value'], errors='coerce')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1.1 Missing_values","metadata":{}},{"cell_type":"code","source":"train[\"Industry\"].fillna(train[\"Industry\"].mode()[0], inplace = True)\ntest[\"Industry\"].fillna(test[\"Industry\"].mode()[0], inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"Last_lead_update\"].fillna(train[\"Last_lead_update\"].mode()[0], inplace = True)\ntest[\"Last_lead_update\"].fillna(test[\"Last_lead_update\"].mode()[0], inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"Resource\"].fillna(train[\"Resource\"].mode()[0], inplace = True)\ntest[\"Resource\"].fillna(test[\"Resource\"].mode()[0], inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['Deal_value'].fillna(train['Deal_value'].median(), inplace=True)\ntest['Deal_value'].fillna(test['Deal_value'].median(), inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['Weighted_amount'].fillna(train['Weighted_amount'].median(), inplace=True)\ntest['Weighted_amount'].fillna(test['Weighted_amount'].median(), inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Geography","metadata":{}},{"cell_type":"code","source":"train['Geography'].value_counts(dropna=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[train['Geography'].isnull()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we can understand from the above data is that the area from USA has a , in between them but areas from India doesn't have them.\nSo, we can fillup the Gerography with USA where the area has a , and the one which doesn't have a , as India,","metadata":{}},{"cell_type":"code","source":"train[['Place', 'State']] = train['Location'].str.split(' ', 1, expand=True)\ntrain['State'].fillna('0', inplace = True)\ntrain['Geography'].fillna('USA', inplace = True)\ntrain.loc[train['State'] == '0', 'Geography'] = 'India'\ntrain.drop(['Place', 'State'], axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[['Place', 'State']] = train['Location'].str.split(' ', 1, expand=True)\ntest['State'].fillna('0', inplace = True)\ntest['Geography'].fillna('USA', inplace = True)\ntest.loc[test['State'] == '0', 'Geography'] = 'India'\ntest.drop(['Place', 'State'], axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Location","metadata":{}},{"cell_type":"code","source":"train[train['Location'].isnull()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Success_probability","metadata":{}},{"cell_type":"code","source":"train['Success_probability'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Probability cannot be more than 100.So, we will be dropping rows having values greater than 100\ntrain.drop(train.loc[train['Success_probability']>100].index, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get list of categorical variables\ns = (train.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n#coverting the object type columns to cat type\nfor i in object_cols :\n    train[i] = train[i].astype('category')\n    test[i] = test[i].astype('category')\n#coverting the categorical columns to numeric\n# Apply label encoder to each column with categorical data\nlabel_encoder = LabelEncoder()\nfor col in object_cols:\n    train[col] = label_encoder.fit_transform(train[col])\n    test[col] = label_encoder.fit_transform(test[col])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train.pop('Success_probability')\nX = train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split into train dev and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_xgb = xgb.XGBRegressor(eval_metric='mae')\nmodel_xgb.fit(X_train, y_train, early_stopping_rounds=20, eval_set=[(X_test, y_test)], verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred=model_xgb.predict(X_test)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\nscore=max(0,100-np.sqrt(metrics.mean_squared_error(y_test,y_pred)))\nprint(score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final = model_xgb.predict(test)\ntest['Success_probability']= final\ntest = test.drop(['Industry','Deal_value','Weighted_amount','Pitch','Lead_revenue','Fund_category','Geography','Location','Designation','Hiring_candidate_role','Lead_source','Level_of_meeting','Last_lead_update','Internal_POC','Resource','Internal_rating'], axis=1)\ntest.to_csv('output.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}