{"cells":[{"metadata":{"id":"iVAkzWmrH43N","trusted":true},"cell_type":"code","source":"view = 0\nbatch_sz = 4\nepochs = 1\nsteps_per_epoch = 1000\nvalidation_steps = 100","execution_count":null,"outputs":[]},{"metadata":{"id":"gov_3_yWc_29","outputId":"bf71d002-e3bb-490b-e9a5-bd63a5c68cc5","trusted":true},"cell_type":"code","source":"!git clone https://github.com/GeorgeSeif/Semantic-Segmentation-Suite.git\n  ","execution_count":null,"outputs":[]},{"metadata":{"id":"uiPEYNh19Nqw","trusted":true},"cell_type":"code","source":"import os","execution_count":null,"outputs":[]},{"metadata":{"id":"gIm4Uo4IJRux","trusted":true},"cell_type":"code","source":"from pathlib import Path\ndata_path = Path('Semantic-Segmentation-Suite/CamVid')","execution_count":null,"outputs":[]},{"metadata":{"id":"hj6dRP8c9zFI","outputId":"719a0993-7226-41d0-ad25-bcdd13487660","trusted":true},"cell_type":"code","source":"print('Number of train frames: ' + str(len(os.listdir(data_path/'train'))))\nprint('Number of train labels: ' + str(len(os.listdir(data_path/'train_labels'))))\nprint('Number of val frames: ' + str(len(os.listdir(data_path/'val'))))\nprint('Number of val labels: ' + str(len(os.listdir(data_path/'val_labels'))))\nprint('Number of test frames: ' + str(len(os.listdir(data_path/'test'))))\nprint('Number of test labels: ' + str(len(os.listdir(data_path/'test_labels'))))\nprint('Total frames: ' + str(len(os.listdir(data_path/'train')) + len(os.listdir(data_path/'val')) + len(os.listdir(data_path/'test'))))","execution_count":null,"outputs":[]},{"metadata":{"id":"7C6OliBmA0rP"},"cell_type":"markdown","source":"Now, let's see which classes we have. This can be found in the original CAMVID [text file](http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/data/label_colors.txt). However, under the same repo, the author has dumped it into csv which we will use."},{"metadata":{"id":"FKdmFX1DKDud","trusted":true},"cell_type":"code","source":"import pandas as pd\nclasses = pd.read_csv(data_path / 'class_dict.csv', index_col =0)","execution_count":null,"outputs":[]},{"metadata":{"id":"jiyA9Aa0KMZl","outputId":"3b8c64e1-8588-4e07-f00a-e1e96ee7c2f7","trusted":true},"cell_type":"code","source":"classes","execution_count":null,"outputs":[]},{"metadata":{"id":"yH17IqAxGdN4","outputId":"863aa3a0-20df-43dd-ed94-8e08b61e7869","trusted":true},"cell_type":"code","source":"n_classes = len(classes)\nn_classes","execution_count":null,"outputs":[]},{"metadata":{"id":"WYOiawzIBHWC"},"cell_type":"markdown","source":"This data frame maps the class names to colors.\n\nTo access the colors, we can index the dataframe with its row index name using the .loc operation.\n"},{"metadata":{"id":"rMWbSrv2B3Ns"},"cell_type":"markdown","source":"Now we are ready to create a map from class name to color"},{"metadata":{"id":"u9QZkCt87fAj","trusted":true},"cell_type":"code","source":"cls2rgb = {cl:list(classes.loc[cl, :]) for cl in classes.index}","execution_count":null,"outputs":[]},{"metadata":{"id":"8BAQSaWvGirG"},"cell_type":"markdown","source":"## Now let's visualize and explore some samples:"},{"metadata":{"id":"Q7f1_yHcm3Uh","outputId":"58b5364a-6172-4e9f-b0f5-7fb68d535a8b","trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport cv2\nimport matplotlib.pyplot as plt\n#from google.colab.patches import  cv2_imshow\n\n#img = cv2.imread(data_path/'train/0001TP_006690.png')\nimg = cv2.imread(str(data_path) + '/train/0001TP_006690.png')\nplt.imshow(img)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"kpJ4GpiuGvvB"},"cell_type":"markdown","source":"Let's have a look on the masks (the ground truth)"},{"metadata":{"id":"yQPgxveDG1P7"},"cell_type":"markdown","source":"As you can see the masks are just colors (L,W,3).\nWhat we actually want is a (L,W) matrix, with each value is from 0 to 31 representing the 32 class labels."},{"metadata":{"id":"nMBOk3-3QuLr"},"cell_type":"markdown","source":"Colors are different from the colors in cls2rgb! Because the order is BGR not RGB when using cv2.imread: https://stackoverflow.com/questions/46898979/how-to-check-the-channel-order-of-an-image\n\nIf you want to get the same order as in the color mapping of CAMVID, use the cv converted"},{"metadata":{"id":"7dalUjddkDEb","trusted":true},"cell_type":"code","source":"import numpy as np\nmask = cv2.imread(str(data_path) + '/train_labels/0001TP_006690_L.png')\nmask = cv2.cvtColor((mask).astype(np.uint8), cv2.COLOR_BGR2RGB)# If you want to get the same order as in the color mapping of CAMVID, use the cv converted","execution_count":null,"outputs":[]},{"metadata":{"id":"C533H1FPQ_Uh"},"cell_type":"markdown","source":"Now if you plot the mask again, you will see different colors. For example the red and blue are reversed than before:"},{"metadata":{"id":"mU_LOU2tkF0F","outputId":"6378aabf-ab07-4ef0-f960-e799562f9fd9","trusted":true},"cell_type":"code","source":"plt.imshow(mask)","execution_count":null,"outputs":[]},{"metadata":{"id":"FcY60Jg4RSl7"},"cell_type":"markdown","source":"Another solution is to use load_image from keras which uses RGB (it uses PIL under the hood) unlike cv2.imread"},{"metadata":{"id":"FPnvFddHi0z7","outputId":"6317fd9f-5b1e-4e59-cda3-30a45127198b","trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import load_img\nmask = load_img(str(data_path) + '/train_labels/0001TP_006690_L.png')\nmask","execution_count":null,"outputs":[]},{"metadata":{"id":"7QYCJFNBjB0-","trusted":true},"cell_type":"code","source":"mask = np.array(mask)# Now colors are the same as in the dict, since keras load_img uses RGB order.","execution_count":null,"outputs":[]},{"metadata":{"id":"ZEkNOms5rOFc","outputId":"31917568-53dd-4639-81e7-136bca0f55d3","trusted":true},"cell_type":"code","source":"mask.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"DD7yMe_Oc0qy","outputId":"8959f678-0de7-433b-cbfe-08c8703142e0","trusted":true},"cell_type":"code","source":"mask.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"2h36dbNQ8Jvj","trusted":true},"cell_type":"code","source":"def adjust_mask(mask, flat=False):\n    \n    semantic_map = []\n    for colour in list(cls2rgb.values()):        \n        equality = np.equal(mask, colour)# 256x256x3 with True or False\n        class_map = np.all(equality, axis = -1)# 256x256 If all True, then True, else False\n        semantic_map.append(class_map)# List of 256x256 arrays, map of True for a given found color at the pixel, and False otherwise.\n    semantic_map = np.stack(semantic_map, axis=-1)# 256x256x32 True only at the found color, and all False otherwise.\n    if flat:\n      semantic_map = np.reshape(semantic_map, (-1,256*256))\n\n    return np.float32(semantic_map)# convert to numbers","execution_count":null,"outputs":[]},{"metadata":{"id":"48ZjbwLzZ75X","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"f-msSAjN8Li6","trusted":true},"cell_type":"code","source":"new_mask = adjust_mask(mask)","execution_count":null,"outputs":[]},{"metadata":{"id":"rvAW7ymK_GdT","outputId":"eb2b4e78-159d-4cc0-9764-5326b149ce9d","trusted":true},"cell_type":"code","source":"new_mask.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"DHXtvDcEw7Wc","trusted":true},"cell_type":"code","source":"idx2rgb={idx:np.array(rgb) for idx, (cl, rgb) in enumerate(cls2rgb.items())}\n","execution_count":null,"outputs":[]},{"metadata":{"id":"JcWc882tw7Wg","trusted":true},"cell_type":"code","source":"# Map the idx back to rgb\ndef map_class_to_rgb(p):\n  \n  return idx2rgb[p[0]]\n\nrgb_mask = np.apply_along_axis(map_class_to_rgb, -1, np.expand_dims(np.argmax(new_mask, axis=-1), -1))","execution_count":null,"outputs":[]},{"metadata":{"id":"-XwnT2wjw7Wj","outputId":"f91a869d-7c4c-4941-f362-ae85916cbd31","trusted":true},"cell_type":"code","source":"plt.imshow(rgb_mask)","execution_count":null,"outputs":[]},{"metadata":{"id":"Pg5d04fmsQXu","trusted":true},"cell_type":"code","source":"import numpy as np \nimport os\n#import skimage.io as io\n#import skimage.transform as trans\nimport numpy as np\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.optimizers import *\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler\nfrom keras import backend as keras\n\n\ndef unet(n_classes, pretrained_weights = None,input_size = (256,256,3), flat=False, ohe=True):\n    inputs = Input(input_size)\n    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n    drop4 = Dropout(0.5)(conv4)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n\n    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n    drop5 = Dropout(0.5)(conv5)\n\n    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n    merge6 = concatenate([drop4,up6], axis = 3)\n    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n\n    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n    merge7 = concatenate([conv3,up7], axis = 3)\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n\n    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n    merge8 = concatenate([conv2,up8], axis = 3)\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n\n    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n    merge9 = concatenate([conv1,up9], axis = 3)\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n    #conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n    #conv10 = Conv2D(n_classes, (1,1), activation = 'softmax')(conv9)\n    conv10 = Conv2D(n_classes, (1,1), padding='same')(conv9)\n    if flat:\n      output_layer = Reshape((256*256,n_classes))(conv10)\n    else:\n      output_layer = conv10\n    output_layer = Activation('softmax')(output_layer)\n     \n\n    model = Model( inputs,output_layer)\n\n    if ohe:\n      model.compile(optimizer = Adam(lr = 1e-4), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    else:\n      model.compile(optimizer = Adam(lr = 1e-4), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n    \n    #model.summary()\n\n    if(pretrained_weights):\n        model.load_weights(pretrained_weights)\n\n    return model\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"jg46Yd6K73bl","outputId":"ced621dd-955b-4998-fab5-5fa03970c3d3","trusted":true},"cell_type":"code","source":"\nmodel = unet(n_classes)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"miHfhtkMSGPP","trusted":true},"cell_type":"code","source":"def load_CAMVID(data_type='train', enc='ohe', shape='normal'):\n  img_path = str(data_path) + '/' + data_type + '/'\n  labels_path = str(data_path) + '/' + data_type + '_labels/'\n  # without adding target_size=(256,256) in load_img we get Out of mem: 421x960x720x32x4bytes is around 34GB!\n  x = np.array([np.array(load_img(str(img_path) + file, target_size=(256,256)))*1./255 for file in sorted(os.listdir(img_path))])\n  if(enc=='ohe'):\n    \n    y = np.array([adjust_mask(np.array(load_img(str(labels_path) + file, target_size=(256,256)))) for file in sorted(os.listdir(labels_path))])\n  elif(enc=='sparse_cat'):\n    y = np.array([adjust_mask(np.array(load_img(str(labels_path) + file, target_size=(256,256)))) for file in sorted(os.listdir(labels_path))])\n  if(shape == 'flat'):\n    y = np.reshape(y.shape[0], y.shape[1]*y.shape[2])\n    y = np.expand_dims(y, axis=-1)\n  return x, y\n  ","execution_count":null,"outputs":[]},{"metadata":{"id":"rqyL9si8TkVj","outputId":"8bcc9c39-bc3e-4c5d-e2f1-16ff0efcf114","trusted":true},"cell_type":"code","source":"import time\nstart = time.time()\nx_train, y_train = load_CAMVID(data_type='train')\n#x_test, y_test = load_CAMVID(data_type='test')# Don't load test for RAM consumption\nx_val, y_val = load_CAMVID(data_type='val')\nend = time.time()\nprint('Time elapsed: ', end-start)","execution_count":null,"outputs":[]},{"metadata":{"id":"kXYhotFdT9J9","outputId":"80f72daf-17a2-49c5-c655-338b702bf272","trusted":true},"cell_type":"code","source":"print(x_train.shape)\nprint(y_train.shape)\n#print(x_test.shape)\n#print(y_test.shape)\nprint(x_val.shape)\nprint(y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"WsQL-8xw8yYC"},"cell_type":"markdown","source":"# Train"},{"metadata":{"id":"fMoTMzUxUo9S","outputId":"e46026de-7051-4974-c8d0-cc9a83f95be2","trusted":true},"cell_type":"code","source":"model_checkpoint = ModelCheckpoint('unet_camvid.hdf5', monitor='val_loss',verbose=1, save_best_only=True)\nmodel.fit(x=x_train, \n              y=y_train,\n              validation_data=(x_val, y_val),\n              batch_size=batch_sz,# 32 gives OOM sometimes\n              epochs=epochs,\n              callbacks=[model_checkpoint])","execution_count":null,"outputs":[]},{"metadata":{"id":"HY5WythVpDTf"},"cell_type":"markdown","source":"# Let's try on some samples"},{"metadata":{"id":"5PsYvUuSpS0U","trusted":true},"cell_type":"code","source":"# img (256,256,3)\n# gt_mask: gt_mode=sparse--> (256,256) or ohe --> (256,256,32)\ndef visualize_seg(img, gt_mask, shape='normal', gt_mode='sparse'):\n  plt.figure(1)\n  \n  # Img\n  plt.subplot(311)\n  plt.imshow(img)\n  \n  # Predict\n  pred_mask = model.predict(np.expand_dims(img, 0))\n  pred_mask = np.argmax(pred_mask, axis=-1)\n  pred_mask = pred_mask[0]\n  if shape=='flat':\n    pred_mask = np.reshape(pred_mask, (256,256)) # Reshape only if you use the flat model. O.w. you dont need\n  \n  rgb_mask = np.apply_along_axis(map_class_to_rgb, -1, np.expand_dims(pred_mask, -1))\n  \n  # Prediction\n  plt.subplot(312)\n  plt.imshow(rgb_mask)\n              \n  # GT mask\n  if gt_mode == 'ohe':\n    gt_img_ohe = np.argmax(gt_mask, axis=-1)\n    gt_mask = np.apply_along_axis(map_class_to_rgb, -1, np.expand_dims(gt_img_ohe, -1))              \n  \n  plt.subplot(313)\n  plt.imshow((gt_mask).astype(np.uint8))\n                \n  \n  ","execution_count":null,"outputs":[]},{"metadata":{"id":"kDAr8irfq_r5","outputId":"6ce18d65-8bb0-40ed-d862-0b61d2e1661a","trusted":true},"cell_type":"code","source":"visualize_seg(x_val[100], y_val[100], gt_mode='ohe')","execution_count":null,"outputs":[]},{"metadata":{"id":"kSRXS0mZUGQm","trusted":true},"cell_type":"code","source":"from keras.utils import Sequence\nclass CAMVID_Dataset(Sequence):\n\n\n    def __init__(self, data_path, batch_size=4, dim=(256,256), n_classes=32, data_type='train', shape='normal'):\n\n        self.images_dir = str(data_path) + '/' + data_type + '/'\n        self.masks_dir = str(data_path) + '/' + data_type + '_labels/'\n        assert len(os.listdir(self.images_dir)) == len(os.listdir(self.masks_dir))\n        self.data_type = data_type\n        self.shape = shape\n        self.batch_size = batch_size\n        self.dim = dim\n        self.n = len(os.listdir(self.images_dir))\n        self.n_batches = int(np.floor(self.n  / self.batch_size))\n        self.indexes = np.arange(self.n)\n\n    def __len__(self):\n        return  self.n_batches\n\n    def __getitem__(self, index):\n        X = np.empty((self.batch_size, *self.dim, 3))\n        Y = np.zeros((self.batch_size, *self.dim, n_classes))\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        \n        # Generate data\n        for i, ID in enumerate(indexes):\n          idx = ID\n\n          file = sorted(os.listdir(self.images_dir))[idx]\n\n          # Load image\n          image = np.array(load_img(str(self.images_dir) + file, target_size=(256,256)))*1./255\n\n\n          # Load mask\n          file = sorted(os.listdir(self.masks_dir))[idx]\n          mask = adjust_mask(np.array(load_img(str(self.masks_dir) + file, target_size=(256,256))))\n\n          if(self.shape == 'flat'):\n            mask = np.reshape(mask.shape[0], mask.shape[1]*mask.shape[2])\n            mask = np.expand_dims(mask, axis=-1)        \n          X[i,:] = image\n          Y[i,:] = mask\n          \n        return X, Y\n      ","execution_count":null,"outputs":[]},{"metadata":{"id":"bHmJVO4yVsLy","trusted":true},"cell_type":"code","source":"train_gen = CAMVID_Dataset(str(data_path), batch_size=batch_sz, n_classes=n_classes, data_type='train')\nvalid_gen = CAMVID_Dataset(str(data_path), batch_size=batch_sz, n_classes=n_classes, data_type='val')","execution_count":null,"outputs":[]},{"metadata":{"id":"av0UbltOZYYj","outputId":"1fb71ad8-f6a5-4c8f-d193-67f6be8c96cb","trusted":true},"cell_type":"code","source":"x,y = next(enumerate(train_gen))[1]\nprint(x.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"xCRWbGVoVsnc","outputId":"13e208c8-c0b0-496f-8957-54ce05e96ddd","trusted":true},"cell_type":"code","source":"n_train_samples = len(os.listdir(str(data_path) + '/train/'))\nn_train_samples","execution_count":null,"outputs":[]},{"metadata":{"id":"oLDvJSIVXm60","trusted":true},"cell_type":"code","source":"\nmodel = unet(n_classes)","execution_count":null,"outputs":[]},{"metadata":{"id":"I2yD5z0ZXo8_","outputId":"743be76c-1af1-43fc-a533-12d3390251df","trusted":true},"cell_type":"code","source":"model_checkpoint = ModelCheckpoint('unet_camvid.hdf5', monitor='val_loss',verbose=1, save_best_only=True)\nmodel.fit_generator(train_gen,\n                    validation_data=valid_gen,\n                    steps_per_epoch=n_train_samples,\n                    validation_steps=validation_steps,\n                    epochs=epochs,\n                    callbacks=[model_checkpoint])","execution_count":null,"outputs":[]},{"metadata":{"id":"SBaHaQaVBkPp","outputId":"11df9d8e-2fe5-4bf4-af29-54b48edbfd77","trusted":true},"cell_type":"code","source":"# Data generator\n#https://keras.io/preprocessing/image/\n# Data generator\n#batch_sz = 4\n\nfrom keras.preprocessing.image import ImageDataGenerator\n# we create two instances with the same arguments\n\n# VI Note: use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same\ndata_gen_args = dict(rescale=1./255)\n\n# So our usage here is as data loader instead of loading everything in RAM, not data augmentation\nmask_gen_args = dict()\n\nimage_datagen = ImageDataGenerator(**data_gen_args)\nmask_datagen  = ImageDataGenerator(**mask_gen_args) \n\n# Provide the same seed and keyword arguments to the fit and flow methods\nseed = 1\n#image_datagen.fit(images, augment=True, seed=seed)\n#mask_datagen.fit(masks, augment=True, seed=seed)\n\nimage_generator = image_datagen.flow_from_directory(\n    data_path,\n    class_mode=None,\n    classes=['train'],\n    seed=seed,\n    batch_size=batch_sz,\n    target_size=(256,256))\n\nmask_generator = mask_datagen.flow_from_directory(\n    data_path,\n    classes=['train_labels'],\n    class_mode=None,\n    seed=seed,\n    color_mode='rgb',\n    batch_size=batch_sz,\n    target_size=(256,256))\n\n# combine generators into one which yields image and masks\ntrain_generator = zip(image_generator, mask_generator)\n\n      \n\nval_image_generator = image_datagen.flow_from_directory(\n    data_path,\n    class_mode=None,\n    classes=['val'],\n    seed=seed,\n    batch_size=batch_sz,\n    target_size=(256,256))\n\nval_mask_generator = mask_datagen.flow_from_directory(\n    data_path,\n    classes=['val_labels'],\n    class_mode=None,\n    seed=seed,\n    batch_size=batch_sz,\n    color_mode='rgb',\n    target_size=(256,256))\n\n# combine generators into one which yields image and masks\nval_generator = zip(val_image_generator, val_mask_generator)\n\n      ","execution_count":null,"outputs":[]},{"metadata":{"id":"ggvo-00qCHdu","trusted":true},"cell_type":"code","source":"\ndef train_generator_fn():\n\n    for (img,mask) in train_generator:\n        new_mask = adjust_mask(mask)\n        yield (img,new_mask)        ","execution_count":null,"outputs":[]},{"metadata":{"id":"V5uqwTCvCI2v","trusted":true},"cell_type":"code","source":"\ndef val_generator_fn():\n\n    for (img,mask) in val_generator:\n        new_mask = adjust_mask(mask)\n        yield (img,new_mask)  ","execution_count":null,"outputs":[]},{"metadata":{"id":"_daKGC16FRtr","outputId":"b028f89c-20ae-4422-a66b-64194ec37fa6","trusted":true},"cell_type":"code","source":"n_train_samples = len(os.listdir(str(data_path) + '/train/'))\nn_train_samples","execution_count":null,"outputs":[]},{"metadata":{"id":"wi67xiPWMiaa","trusted":true},"cell_type":"code","source":"\nmodel = unet(n_classes)","execution_count":null,"outputs":[]},{"metadata":{"id":"IVuzPjyU8xJm","outputId":"ff470892-7ed7-402e-aa4b-bc3ab3961bb2","trusted":true},"cell_type":"code","source":"model_checkpoint = ModelCheckpoint('unet_camvid.hdf5', monitor='val_loss',verbose=1, save_best_only=True)\nmodel.fit_generator(train_generator_fn(),\n                    validation_data=val_generator_fn(),\n                    steps_per_epoch=n_train_samples,\n                    validation_steps=validation_steps,\n                    epochs=epochs,\n                    callbacks=[model_checkpoint])","execution_count":null,"outputs":[]},{"metadata":{"id":"fny7K-Q3hd6s"},"cell_type":"markdown","source":"## Let's test the model on sample images"},{"metadata":{"id":"vTP89OeAECaz","trusted":true},"cell_type":"code","source":"visualize_seg(next(val_image_generator)[0], next(val_mask_generator)[0], gt_mode='sparse')","execution_count":null,"outputs":[]},{"metadata":{"id":"T99C1ZbYFwH1","trusted":true},"cell_type":"code","source":"# Data generator\n#batch_sz = 4\n#https://keras.io/preprocessing/image/\nfrom keras.preprocessing.image import ImageDataGenerator\n# we create two instances with the same arguments\n\n# VI Note: use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same\ndata_gen_args = dict(rotation_range=0.2,\n                    width_shift_range=0.05,\n                    height_shift_range=0.05,\n                    shear_range=0.05,\n                    zoom_range=0.05,\n                    horizontal_flip=True,\n                    fill_mode='nearest',\n                    rescale=1./255)\n\nmask_gen_args = dict(rotation_range=0.2,\n                    width_shift_range=0.05,\n                    height_shift_range=0.05,\n                    shear_range=0.05,\n                    zoom_range=0.05,\n                    horizontal_flip=True,\n                    fill_mode='nearest')\n                    \n\nimage_datagen = ImageDataGenerator(**data_gen_args)\nmask_datagen  = ImageDataGenerator(**mask_gen_args) \n\n# Provide the same seed and keyword arguments to the fit and flow methods\nseed = 1\n#image_datagen.fit(images, augment=True, seed=seed)\n#mask_datagen.fit(masks, augment=True, seed=seed)\n\nimage_generator = image_datagen.flow_from_directory(\n    data_path,\n    class_mode=None,\n    classes=['train'],\n    seed=seed,\n    batch_size=batch_sz,\n    target_size=(256,256))\n\nmask_generator = mask_datagen.flow_from_directory(\n    data_path,\n    classes=['train_labels'],\n    class_mode=None,\n    seed=seed,\n    batch_size=batch_sz,\n    color_mode='rgb',\n    target_size=(256,256))\n\n# combine generators into one which yields image and masks\ntrain_generator = zip(image_generator, mask_generator)\n\ndef train_generator_fn():\n    for (img,mask) in train_generator:\n        new_mask = adjust_mask(mask)\n        yield (img,new_mask)  \n        \nval_image_generator = image_datagen.flow_from_directory(\n    data_path,\n    class_mode=None,\n    classes=['val'],\n    seed=seed,\n    batch_size=batch_sz,\n    target_size=(256,256))\n\nval_mask_generator = mask_datagen.flow_from_directory(\n    data_path,\n    classes=['val_labels'],\n    class_mode=None,\n    seed=seed,\n    batch_size=batch_sz,\n    color_mode='rgb',\n    target_size=(256,256))\n\n# combine generators into one which yields image and masks\nval_generator = zip(val_image_generator, val_mask_generator)        \n        \ndef val_generator_fn():\n\n    for (img,mask) in val_generator:\n        new_mask = adjust_mask(mask)\n        yield (img,new_mask)         \n","execution_count":null,"outputs":[]},{"metadata":{"id":"mLZ9GxiCWSx7"},"cell_type":"markdown","source":"\n\nLet's take a look on some augmented images and masks"},{"metadata":{"id":"uhtcWPaOWl2C","trusted":true},"cell_type":"code","source":"img = load_img(str(data_path) + '/train/0001TP_006690.png', target_size=(256,256))\nimg","execution_count":null,"outputs":[]},{"metadata":{"id":"KjWsjYNuRZCN","trusted":true},"cell_type":"code","source":"mask = load_img(str(data_path) + '/train_labels/0001TP_006690_L.png', target_size=(256,256))\nmask","execution_count":null,"outputs":[]},{"metadata":{"id":"IMTa74oJWrFQ","trusted":true},"cell_type":"code","source":"# The .flow() command below generates batches of randomly transformed images.\n# It will loop indefinitely, so we need to `break` the loop at some point!\nfrom keras.preprocessing.image import array_to_img, img_to_array\ni = 0\nimg = img_to_array(img)\nmask = img_to_array(mask)\nfor aug_img, aug_mask in zip(image_datagen.flow(np.expand_dims(img, 0), batch_size=1), mask_datagen.flow(np.expand_dims(mask, 0), batch_size=1)):\n    plt.figure(i)\n    plt.subplot(221)\n    imgplot = plt.imshow(array_to_img(aug_img[0]))\n    plt.subplot(222)\n    imgplot = plt.imshow(array_to_img(aug_mask[0]))\n    i += 1\n    if i > 10:\n        break\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"YykK7jXderjV","trusted":true},"cell_type":"code","source":"model = unet(n_classes)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"eIHI3G6Heyup","trusted":true},"cell_type":"code","source":"model_checkpoint = ModelCheckpoint('unet_camvid.hdf5', monitor='val_loss',verbose=1, save_best_only=True)\nmodel.fit_generator(train_generator_fn(),\n                    validation_data=val_generator_fn(),\n                    steps_per_epoch=steps_per_epoch,\n                    validation_steps=validation_steps,\n                    epochs=epochs,\n                    callbacks=[model_checkpoint])","execution_count":null,"outputs":[]},{"metadata":{"id":"AbrkeENYfbGV"},"cell_type":"markdown","source":"# Let's try some samples"},{"metadata":{"id":"EwMaeapYfajC","trusted":true},"cell_type":"code","source":"img = next(val_image_generator)[0]\ngt_img = next(val_mask_generator)[0]\nvisualize_seg(img, gt_img, gt_mode='sparse')\n","execution_count":null,"outputs":[]},{"metadata":{"id":"WTXEcsjF2j1m","trusted":true},"cell_type":"code","source":"def load_raw_CAMVID(data_type='train', enc='ohe', shape='normal'):\n  img_path = str(data_path) + '/' + data_type + '/'\n  labels_path = str(data_path) + '/' + data_type + '_labels/'\n  # without adding target_size=(256,256) in load_img we get Out of mem: 421x960x720x32x4bytes is around 34GB!\n  x = np.array([np.array(load_img(str(img_path) + file, target_size=(256,256)))*1./255 for file in sorted(os.listdir(img_path))])\n  if(enc=='ohe'):\n    \n    y = np.array([np.array(load_img(str(labels_path) + file, target_size=(256,256))) for file in sorted(os.listdir(labels_path))])\n  elif(enc=='sparse_cat'):\n    y = np.array([np.array(load_img(str(labels_path) + file, target_size=(256,256))) for file in sorted(os.listdir(labels_path))])\n  if(shape == 'flat'):\n    y = np.reshape(y.shape[0], y.shape[1]*y.shape[2])\n    y = np.expand_dims(y, axis=-1)\n  return x, y","execution_count":null,"outputs":[]},{"metadata":{"id":"Ew_gPuyJ262f","trusted":true},"cell_type":"code","source":"import time\nstart = time.time()\nx_train, y_train = load_raw_CAMVID(data_type='train')\n#x_test, y_test = load_raw_CAMVID(data_type='test')# Don't load test for RAM consumption\nx_val, y_val = load_raw_CAMVID(data_type='val')\nend = time.time()\nprint('Time elapsed: ', end-start)\n\nprint(x_train.shape)\nprint(y_train.shape)\n#print(x_test.shape)\n#print(y_test.shape)\nprint(x_val.shape)\nprint(y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"qbE8gHV2O-Ac"},"cell_type":"markdown","source":"__Since we scaled while loading the data, we don't need to scale in the generator__"},{"metadata":{"id":"qsuZTde13aZJ","trusted":true},"cell_type":"code","source":"# Data generator\n#batch_sz = 4\n#https://keras.io/preprocessing/image/\nfrom keras.preprocessing.image import ImageDataGenerator\n# we create two instances with the same arguments\n\n# VI Note: use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same\ndata_gen_args = dict(rotation_range=0.2,\n                    width_shift_range=0.05,\n                    height_shift_range=0.05,\n                    shear_range=0.05,\n                    zoom_range=0.05,\n                    horizontal_flip=True,\n                    fill_mode='nearest')\n                    #rescale=1./255)# Data is already scaled when loaded\n\nmask_gen_args = dict(rotation_range=0.2,\n                    width_shift_range=0.05,\n                    height_shift_range=0.05,\n                    shear_range=0.05,\n                    zoom_range=0.05,\n                    horizontal_flip=True,\n                    fill_mode='nearest')\n                    #preprocessing_function=adjust_mask)# This is not possible since the preprocessing_function can only return the same shape as image\n\nimage_datagen = ImageDataGenerator(**data_gen_args)\nmask_datagen  = ImageDataGenerator(**mask_gen_args) \n\n# Provide the same seed and keyword arguments to the fit and flow methods\nseed = 1\n#image_datagen.fit(images, augment=True, seed=seed)\n#mask_datagen.fit(masks, augment=True, seed=seed)\n\nimage_generator = image_datagen.flow(\n    x_train,\n    seed=seed,\n    batch_size=batch_sz)\n\nmask_generator = mask_datagen.flow( \n    y_train,\n    seed=seed,\n    batch_size=batch_sz)\n\n# combine generators into one which yields image and masks\ntrain_generator = zip(image_generator, mask_generator)\n\ndef train_generator_fn():\n\n    for (img,mask) in train_generator:\n        new_mask = adjust_mask(mask)\n        yield (img,new_mask)  \n        \nval_image_generator = image_datagen.flow(\n    x_val,\n    seed=seed,\n    batch_size=batch_sz)\n\nval_mask_generator = mask_datagen.flow(\n    y_val,\n    seed=seed,\n    batch_size=batch_sz)\n\n# combine generators into one which yields image and masks\nval_generator = zip(val_image_generator, val_mask_generator)        \n        \ndef val_generator_fn():\n\n    for (img,mask) in val_generator:\n        new_mask = adjust_mask(mask)\n        yield (img,new_mask)         \n","execution_count":null,"outputs":[]},{"metadata":{"id":"yVwtajJo3B5E","trusted":true},"cell_type":"code","source":"model = unet(n_classes)","execution_count":null,"outputs":[]},{"metadata":{"id":"L8rnk13o3Cap","trusted":true},"cell_type":"code","source":"model_checkpoint = ModelCheckpoint('unet_camvid.hdf5', monitor='val_loss',verbose=1, save_best_only=True)\nmodel.fit_generator(train_generator_fn(),\n                    validation_data=val_generator_fn(),\n                    steps_per_epoch=steps_per_epoch,\n                    validation_steps=validation_steps,\n                    epochs=epochs,\n                    callbacks=[model_checkpoint])","execution_count":null,"outputs":[]},{"metadata":{"id":"WPT2kZujg6cn"},"cell_type":"markdown","source":"# Try some samples"},{"metadata":{"id":"D6mOqZT2e98w","trusted":true},"cell_type":"code","source":"img = next(val_image_generator)[0]\ngt_img = next(val_mask_generator)[0]\nvisualize_seg(img, gt_img, gt_mode='sparse')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}