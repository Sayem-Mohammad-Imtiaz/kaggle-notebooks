{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* > # EE226 - Coding 2\n## Streaming algorithm & Locality Sensitive Hashing","metadata":{}},{"cell_type":"markdown","source":"### Streaming: DGIM","metadata":{}},{"cell_type":"markdown","source":"DGIM is an efficient algorithm in processing large streams. When it's infeasible to store the flowing binary stream, DGIM can estimate the number of 1-bits in the window. In this coding, you're given the *stream_data.txt* (binary stream), and you need to implement the DGIM algorithm to count the number of 1-bits. Write code and ask the problems below.","metadata":{}},{"cell_type":"markdown","source":"### Your task","metadata":{}},{"cell_type":"markdown","source":"1. Set the window size to 1000, and count the number of 1-bits in the current window.","metadata":{}},{"cell_type":"code","source":"import time\n \nbuckets=[] # the list of bucket\nwindow_size=1000\ntime_now=2000 # Current moment\n\nfileName = '../input/coding2/stream_data.txt'\nf = open(fileName, 'r')\ndata = f.read().split('\\t')  # Load data\nf.close()\n\ndef update(buckets=buckets):\n    for i in range(len(buckets)-1,1,-1):\n        if buckets[i]['bit_sum']==buckets[i-2]['bit_sum']:\n            buckets[i-2]['bit_sum']*=2\n            buckets[i-2]['timestamp']=buckets[i-1]['timestamp']\n            del buckets[i-1]\n\ndef DGIM(data=data, buckets=buckets, window_size=window_size, time_now=time_now):\n    bit_sum=0\n    start_time=time.time()\n    for i in range(time_now):\n        if len(buckets)>0 and i-window_size+1==buckets[0]['timestamp']:\n            del buckets[0] # delete too old bucket\n        tmp=data[i]\n        if int(tmp)==1:\n            bucket={\"timestamp\":i+1,\"bit_sum\":1} # add a new bucket\n            buckets.append(bucket)\n            update(buckets) # check and merge\n    for i in range(len(buckets)):\n        bit_sum+=buckets[i]['bit_sum']\n    bit_sum-=buckets[0]['bit_sum']/2\n    return bit_sum,time.time()-start_time\n\n\nbit_sum,bit_time=DGIM(data,buckets,window_size,time_now)\nprint(\"the number of 1-bits in the current window is {}, and DGIM total running time is {}.\".format(bit_sum,bit_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Write a function that accurately counts the number of 1-bits in the current window, and compare the difference between its running time and space and the DGIM algorithm.","metadata":{}},{"cell_type":"code","source":"def preciseCount(data=data, window_size=window_size, time_now=time_now):\n    bit_sum=0\n    start_time=time.time()\n    offset = max(time_now - window_size, 0)\n    for i in range(min(time_now,window_size)):\n        tmp=data[i+offset]\n        if int(tmp)==1:\n            bit_sum+=1\n \n    return bit_sum,time.time()-start_time\n\nprecise_bit_sum, precise_bit_time=preciseCount(data,window_size,time_now)\nprint(\"the number of 1-bits in the current window is {}, and precise count total running time is {}.\".format(precise_bit_sum,precise_bit_time))\n\nerror=abs(precise_bit_sum-bit_sum)\nerror_rate=100*float(error)/precise_bit_sum\nprint(\"Error rate of DGIM and precise count is {}%.\".format(error_rate))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Locality Sensitive Hashing","metadata":{}},{"cell_type":"markdown","source":"The locality sensitive hashing (LSH) algorithm is efficient in near-duplicate document detection. In this coding, you're given the *docs_for_lsh.csv*, where the documents are processed into set of k-shingles (k = 8, 9, 10). *docs_for_lsh.csv* contains 201 columns, where column 'doc_id' represents the unique id of each document, and from column '0' to column '199', each column represents a unique shingle. If a document contains a shingle ordered with **i**, then the corresponding row will have value 1 in column **'i'**, otherwise it's 0. You need to implement the LSH algorithm and ask the problems below.","metadata":{}},{"cell_type":"markdown","source":"### Your task","metadata":{}},{"cell_type":"markdown","source":"Use minhash algoirthm to create signature of each document, and find 'the most similar' documents under Jaccard similarity. \nParameters you need to determine:\n1) Length of signature (number of distinct minhash functions) *n*. Recommanded value: n > 20.\n\n2) Number of bands that divide the signature matrix *b*. Recommanded value: b > n // 10.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport time\nfrom random import shuffle\nfrom tqdm import trange\nfrom sklearn.metrics import jaccard_score\n\nhash_num=50\nband_num=10\nsignatures = []\n\nfilename = \"../input/coding2/docs_for_lsh.csv\"\ndataset = pd.read_csv(filename,usecols=list(range(1,201)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_sig_mat(data=dataset, hash_num=hash_num):\n    docs_num = dataset.shape[0]\n    shingle_num = dataset.shape[1]\n    \n    permutation = np.arange(shingle_num)\n    shuffle(permutation)\n    \n    signatures = []\n    for i in trange(hash_num):\n        # random permutation Ï€\n        permutation = np.arange(shingle_num)\n        shuffle(permutation)\n        \n        # a copy of dataset\n        df = dataset.copy()\n        df.columns=permutation\n        df = df.sort_index(axis=1).values\n        \n        # get one signature\n        signature = (df!=0).argmax(axis=1)\n        \n        signatures.append(signature)\n        del df\n    return signatures\n                     \nsignatures = generate_sig_mat(dataset, hash_num)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Problem: For document 0 (the one with id '0'), list the **30** most similar document ids (except document 0 itself). You can valid your results with the [sklearn.metrics.jaccard_score()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html) function.\n\nTips: You can adjust your parameters to hash the documents with similarity *s > 0.8* into the same bucket.","metadata":{}},{"cell_type":"code","source":"import hashlib\ndef LSH(signatures=signatures, band_num=band_num, hash_num=hash_num):\n    buckets = {} # a dictionary, buckets as key, docs as value\n    signatures = np.array(signatures)\n    s_len = signatures.shape[1]\n    step_size = hash_num//band_num\n    for i in trange(s_len):\n        for j in range(band_num):\n            hash_md5 = hashlib.md5()\n            band = signatures[int(j*step_size):int((j+1)*step_size),i]\n            band = [str(i) for i in band]\n            band = \"\".join(band)+str(j)\n            hash_md5.update(band.encode('utf-8'))\n            bucket = hash_md5.hexdigest()\n            if bucket not in buckets:\n                buckets[bucket]=[i]\n            elif i not in buckets[bucket]: # add element to an existing bucket\n                buckets[bucket].append(i)\n    return buckets\n\nbuckets = LSH(signatures, band_num, hash_num)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_most_similiar_30(buckets, signatures=signatures, data=dataset.values):\n    signatures = np.array(signatures)\n    scores=np.zeros(signatures.shape[1])\n    for key,value in buckets.items():\n        if 0 in value:\n            for idx in value:\n                scores[idx] += 1\n    scores[0]=0\n    rank=np.argsort(-scores)\n    print(\"The 30 most similar document ids and score is as following:\")\n    print(\"id    : score\")\n    for i in rank[:30]:\n        print(\"{:5d} : {:.5f}\".format(i, jaccard_score(data[0,:],data[i,:])))\n\nfind_most_similiar_30(buckets, signatures, dataset.values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}