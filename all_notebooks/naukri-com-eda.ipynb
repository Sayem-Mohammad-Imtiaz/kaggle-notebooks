{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data=pd.read_csv('/kaggle/input/jobs-on-naukricom/home/sdf/marketing_sample_for_naukri_com-jobs__20190701_20190830__30k_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# There are 649 different Roles let's generalize them a bit\n\n* We would be taking the top chunk of most occouring words in thr 'Role' field and use them to generalize the Roles.\n\n* Same is applied for other String Columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Role'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#There are 649 different Role categories let's generalize it a bit \ndata['Role'] = data['Role'].str.lower()\ndata_job=data['Role'].astype(str) \n#Remove Special Chars from Role Category and splitting each word \ndata_job=data_job.str.replace('[+-/\\,.()@:|;&_~]', ' ')    \ndata_job1=data_job.str.split(expand=True)\n\n#Getting the splitted words in one column for finding the frequencies for each unique word\ny=1\nfor x in range(0, 10):\n    dj1=data_job1[x].dropna()\n    dj1=dj1.to_frame(name=\"A\") \n    dj2=data_job1[y].dropna()\n    dj2=dj2.to_frame(name=\"A\") \n    dj3=dj1.append(dj2,ignore_index=True)\n    if x == 0:\n          dj4=dj3  \n    else:\n          dj4=dj4.append(dj3,ignore_index=True)\n    y=y+1\n\n#Getting the frequency of each unique word and sorting based on the frequencies.\ndj5 = dj4['A'].value_counts().rename_axis('unique_values').reset_index(name='counts')\ndj5[\"Rank\"] = dj5[\"counts\"]. rank(method='first',ascending=False) \n\n\n#Considering only the words which have the frequency 1000  or greater.\ndjX=dj5.loc[dj5['counts'] >= 1000]\n#making the list of considered words only \nsearchfor=djX.unique_values.tolist()\n\n#Creating a generalized job category with the most occouring categories\n#The loop is reversed because if any category has two words the word with highest frequency will be considered\ndata['Job_Cat']='other'\nindices = []    \nfor x in reversed(searchfor):\n    indices = []    \n    for i, elem in enumerate(data['Role'].astype(str)):\n        if x in elem:\n            indices.append(i)\n    data.iloc[indices,data.columns.get_loc('Job_Cat')]=x\n            \n#data['Job_Cat'].value_counts()\n\nless_words=data['Job_Cat'].value_counts()\nless_words=less_words.reset_index()\nless_words=less_words.loc[less_words['Job_Cat'] < 100]\nless_words=less_words['index'].tolist()\n\nfor x in less_words:\n    indices = []    \n    for i, elem in enumerate(data['Job_Cat'].astype(str)):\n        if x == elem:\n            indices.append(i)\n    data.iloc[indices,data.columns.get_loc('Job_Cat')]='other'\n    \ndata.loc[data['Job_Cat']=='nan', 'Job_Cat']= 'other'\n\ndata['Job_Cat'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have generalized the Roles to a point from which it is easy to analyse.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Generalizing the Role Category**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"There are 206 distinct role categories lets genralize them too.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Role Category'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data['Role Category'].value_counts().head(10)\n\ndj5=data['Role Category'].value_counts().head(30)\ndj5=dj5.index.tolist()\n\ndata['New_Role_Category']='Other'\nfor x in dj5:\n    indices = []    \n    for i, elem in enumerate(data['Role Category'].astype(str)):\n        if x in elem:\n            indices.append(i)\n    data.iloc[indices,data.columns.get_loc('New_Role_Category')]=x    \n\ndata['New_Role_Category'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Generalizing the Locations**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removing the Blankspaces from start and end of the location and making it consistent using UpperCase\ndata['New_Location']=data['Location'].str.strip()\ndata['New_Location'] = data['New_Location'].str.upper()\n\n#Getting Indexes of locations where there are multiple locations (i.e. if Comma(,) is present)\nindices=[]\nfor i, elem in enumerate(data['New_Location'].astype(str)):\n        if ',' in elem:\n            indices.append(i)\n#Updaing the new location values in those rows with 'Multi' category\ndata.iloc[indices,data.columns.get_loc('New_Location')]='Multi'\ndata['New_Location'].value_counts()\n\n#taking the top 10 locations along with multi, so top 11 locations\nlocations=data['New_Location'].value_counts().head(11)\nlocations=locations.index.tolist()\n\n# using the top 10 locations to genralize the locations. \n#Here we have used in opreator for cases like 'MUMBAI (WADALA)' will be considered as 'MUMBAI'\ndata['final_location']='OTHER'\nfor x in locations:\n    indices = []    \n    for i, elem in enumerate(data['New_Location'].astype(str)):\n        if x in elem:\n            indices.append(i)\n    data.iloc[indices,data.columns.get_loc('final_location')]=x\n\n\ndata['final_location'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Handling Salaries**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's Clean the Salary field and split them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['New_Job_Salary']=data['Job Salary'].str.strip()\ndata['New_Job_Salary'] = data['New_Job_Salary'].str.upper()\n\ndata.loc[(data['New_Job_Salary']=='OPENINGS: 1') | (data['New_Job_Salary']=='NOT DISCLOSED')\n,'New_Job_Salary']= 'NOT DISCLOSED BY RECRUITER'\n\ndata.loc[(data['New_Job_Salary']=='BEST IN THE INDUSTRY') \n| (data['New_Job_Salary']=='OPENINGS: 2')\n| (data['New_Job_Salary']=='BEST IN INDUSTRY')\n,'New_Job_Salary']= 'NOT DISCLOSED BY RECRUITER'\n\ndata.loc[data['New_Job_Salary']==',', 'New_Job_Salary']= ''\ndata.loc[data['New_Job_Salary']=='PA.', 'New_Job_Salary']= ''\n\ndata['New_Job_Salary']=data['New_Job_Salary'].str.replace(',', '')    \ndata['New_Job_Salary']=data['New_Job_Salary'].str.replace('PA.', '')    \ndata['New_Job_Salary']=data['New_Job_Salary'].str.replace('INR.', '')    \n\ndata['New_Job_Salary'] =data['New_Job_Salary'].str.extract(r'(\\d+ - \\d+)')\n\ndata[['Min_Salary','Max_Salary']]=data['New_Job_Salary'].str.split('-',1,expand=True)\ndata['Min_Salary']=data['Min_Salary'].str.strip()\ndata['Max_Salary']=data['Max_Salary'].str.strip()\n\ndata[['New_Job_Salary','Min_Salary','Max_Salary']].head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Handling Industry**\n\n* Lets Generalize the Industries also.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Industry_new']=data['Industry'].str.replace(',\\s',',')\ndata[['Industry_new','X']]=data['Industry_new'].str.split(',',1,expand=True)\ndata['Industry_new']=data['Industry_new'].str.strip()\n\n\nR=data['Industry_new'].value_counts().head(20)\nR.reset_index()\nR=R.drop(columns=['Industry_new'])    \n    \ndata['Industry_Final']='Other'\nindices = []    \nfor x in reversed(R.index):\n    indices = []    \n    for i, elem in enumerate(data['Industry'].astype(str)):\n        if x in elem:\n            indices.append(i)\n    data.iloc[indices,data.columns.get_loc('Industry_Final')]=x\n    \n    \ndata['Industry_Final'].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Handling Experience**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Experience']=data['Job Experience Required'].str.strip()\ndata['Experience'] =data['Experience'].str.extract(r'(\\d+ - \\d+)')\n\ndata['Experience'].value_counts().sum()\n\ndata[['Min_Ex','Max_Ex']]=data['Experience'].str.split('-',1,expand=True)\ndata['Min_Ex']=data['Min_Ex'].str.strip()\ndata['Max_Ex']=data['Max_Ex'].str.strip()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Key Skills required for top 5 Industries**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\n\ndata['Key Skills']=data['Key Skills'].str.strip()\ndata['Key Skills'] = data['Key Skills'].str.upper()\ndata_skill=data['Key Skills'].str.split('|',expand=True)\n\ndata_skill=data['Key Skills'].str.split('|',expand=True)\ndata_skill['Industry']=data['Industry_Final']\nIndustries= data['Industry_Final'].value_counts().rename_axis('industry').reset_index(name='counts').head(5)\nfor K in Industries['industry']:\n    data_IT=data_skill.iloc[np.where(data_skill['Industry']==K)]\n    data_IT=data_IT.reset_index(drop=True)\n    y=1\n    length=len(data_IT.columns)\n    length=length-1\n    for x in range(length):\n        djj1=data_IT[x].dropna()\n        djj1=djj1.to_frame(name=\"A\") \n        djj2=data_IT[y].dropna()\n        djj2=djj2.to_frame(name=\"A\") \n        djj3=djj1.append(djj2,ignore_index=True)\n        if x == 0:\n            djj4=djj3  \n        else:\n            djj4=djj4.append(djj3,ignore_index=True)\n            y=y+1\n    djj4['A'] = djj4['A'].str.upper()\n    djj4['A'] = djj4['A'].str.strip()\n    djj6= djj4['A'].value_counts().rename_axis('unique_values').reset_index(name='counts').head(20)\n    djj6=djj6.sort_values(by='unique_values')\n    fig3 = px.line_polar(djj6,r='counts', theta='unique_values', line_close=True)\n    fig3.update_traces(fill='toself')  \n    fig3.update_layout(\n    height=400,\n    title_text=K)\n    fig3.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Get the Required New Fields in new dataset ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data1=data[['Uniq Id', 'Job_Cat', 'New_Role_Category','final_location',\n            'Industry_Final', 'Min_Salary', 'Max_Salary', 'Min_Ex','Max_Ex']]\ndata1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Cities, Industries and Job Types With Most Opportunities**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndata_loc=data1['final_location'].value_counts()\ndata_loc = data1['final_location'].value_counts().rename_axis('unique_values').reset_index(name='counts')\nx=data_loc['unique_values']\ny=data_loc['counts']\nplt.bar(x,y)\nplt.xticks(rotation=90)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_loc=data1['Industry_Final'].value_counts()\ndata_loc = data1['Industry_Final'].value_counts().rename_axis('unique_values').reset_index(name='counts')\nx=data_loc['unique_values']\ny=data_loc['counts']\nplt.bar(x,y)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_loc=data1['Job_Cat'].value_counts()\ndata_loc = data1['Job_Cat'].value_counts().rename_axis('unique_values').reset_index(name='counts')\nx=data_loc['unique_values']\ny=data_loc['counts']\nplt.bar(x,y)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CrossTabs For Detailed Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"table = pd.crosstab(data1['Job_Cat'], data1['final_location'], dropna=False,margins=True,margins_name='Total')\n#table = pd.crosstab(data1['Job_Cat'], data1['final_location'], dropna=False)\ntable = table.drop('Total')\ntable=table.sort_values(by=['Total'],ascending=False)\ntable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"table1 = pd.crosstab(data1['Industry_Final'], data1['final_location'], dropna=False,margins=True,margins_name='Total')\n#table = pd.crosstab(data1['Job_Cat'], data1['final_location'], dropna=False)\ntable1 = table1.drop('Total')\ntable1=table1.sort_values(by=['Total'],ascending=False)\ntable1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"table2 = pd.crosstab(data1['Job_Cat'], data1['Industry_Final'], dropna=False,margins=True,margins_name='Total')\ntable2 = table2.drop('Total')\ntable2=table2.sort_values(by=['Total'],ascending=False)\ntable2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Experience And Salaries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataSaL=data1.dropna(subset=['Min_Salary','Max_Salary'])\n\ndataSaL.S_Max_sal=0\ndataSaL.S_Min_sal=0\ndataSaL=dataSaL.reset_index(drop=True)\n\ndataSaL['Min_Salary']= dataSaL[\"Min_Salary\"].astype(int) \ndataSaL['Max_Salary']= dataSaL[\"Max_Salary\"].astype(int)\n\n\ndataSaL.loc[dataSaL[\"Min_Salary\"].between(0, 250000), 'S_Min_sal']= 'A. 0L - 2.5L'\ndataSaL.loc[dataSaL[\"Min_Salary\"].between(250001, 500000), 'S_Min_sal']= 'B. 2.5L - 5L'\ndataSaL.loc[dataSaL[\"Min_Salary\"].between(500001, 750000), 'S_Min_sal']= 'C. 5L - 7.5L'\ndataSaL.loc[dataSaL[\"Min_Salary\"].between(750001, 1000000), 'S_Min_sal']= 'D. 7.5L - 10L'\ndataSaL.loc[dataSaL[\"Min_Salary\"].between(1000001, 1250000), 'S_Min_sal']= 'E. 10L - 12.5L'\ndataSaL.loc[dataSaL[\"Min_Salary\"].between(1250001, 1500000), 'S_Min_sal']= 'F. 12.5L - 15L'\ndataSaL.loc[dataSaL[\"Min_Salary\"].between(1500001, 2000000), 'S_Min_sal']= 'G. 15L - 20L'\ndataSaL.loc[dataSaL[\"Min_Salary\"].between(2000001, 3000000), 'S_Min_sal']= 'H. 20L - 30L'\ndataSaL.loc[dataSaL[\"Min_Salary\"].between(3000001, 5000000), 'S_Min_sal']= 'I. 30L - 50L'\ndataSaL.loc[dataSaL[\"Min_Salary\"].between(5000001, 10000000), 'S_Min_sal']='J. 50L - 1CR'\n\ndataSaL.loc[dataSaL[\"Max_Salary\"].between(0, 250000), 'S_Max_sal']= 'A. 0L - 2.5L'\ndataSaL.loc[dataSaL[\"Max_Salary\"].between(250001, 500000), 'S_Max_sal']= 'B. 2.5L - 5L'\ndataSaL.loc[dataSaL[\"Max_Salary\"].between(500001, 750000), 'S_Max_sal']= 'C. 5L - 7.5L'\ndataSaL.loc[dataSaL[\"Max_Salary\"].between(750001, 1000000), 'S_Max_sal']= 'D. 7.5L - 10L'\ndataSaL.loc[dataSaL[\"Max_Salary\"].between(1000001, 1250000), 'S_Max_sal']= 'E. 10L - 12.5L'\ndataSaL.loc[dataSaL[\"Max_Salary\"].between(1250001, 1500000), 'S_Max_sal']= 'F. 12.5L - 15L'\ndataSaL.loc[dataSaL[\"Max_Salary\"].between(1500001, 2000000), 'S_Max_sal']= 'G. 15L - 20L'\ndataSaL.loc[dataSaL[\"Max_Salary\"].between(2000001, 3000000), 'S_Max_sal']= 'H. 20L - 30L'\ndataSaL.loc[dataSaL[\"Max_Salary\"].between(3000001, 5000000), 'S_Max_sal']= 'I. 30L - 50L'\ndataSaL.loc[dataSaL[\"Max_Salary\"].between(5000001, 10000000), 'S_Max_sal']='J. 50L - 1CR'\n\ndataSaL['S_Max_sal'].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FL=dataSaL.groupby('final_location').median()\nFL.sort_values(by='Min_Salary',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IF=dataSaL.groupby('Industry_Final').median()\nIF.sort_values(by='Min_Salary',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"JC=dataSaL.groupby('Job_Cat').median()\nJC.sort_values(by='Min_Salary',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tablesal = pd.crosstab(dataSaL[\"S_Max_sal\"], dataSaL[\"S_Min_sal\"], dropna=False,margins=True,margins_name='Total')\ntablesal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataSaL2=dataSaL.dropna(subset=['Min_Ex','Max_Ex'])\ndataSaL2=dataSaL2.reset_index(drop=True)\n\n\ndataSaL2['Min_Ex']= dataSaL2[\"Min_Ex\"].astype(int) \n\ndataSaL2.loc[dataSaL2[\"Min_Ex\"]== 0 ,'S_Min_Ex']= 'A. 0 Year'\ndataSaL2.loc[dataSaL2[\"Min_Ex\"]== 1 , 'S_Min_Ex']= 'B. 1 Year'\ndataSaL2.loc[dataSaL2[\"Min_Ex\"]==2, 'S_Min_Ex']= 'C. 2 Years'\ndataSaL2.loc[dataSaL2[\"Min_Ex\"]==3, 'S_Min_Ex']= 'D. 3 Years'\ndataSaL2.loc[dataSaL2[\"Min_Ex\"]==4, 'S_Min_Ex']= 'E. 4 Years'\ndataSaL2.loc[dataSaL2[\"Min_Ex\"]==5, 'S_Min_Ex']= 'F. 5 Years'\ndataSaL2.loc[dataSaL2[\"Min_Ex\"].between(6, 7), 'S_Min_Ex']='G. 6-7 Years'\ndataSaL2.loc[dataSaL2[\"Min_Ex\"].between(8, 10), 'S_Min_Ex']='H. 8-10 Years'\ndataSaL2.loc[dataSaL2[\"Min_Ex\"].between(11, 12), 'S_Min_Ex']='I. 11-12 Years'\ndataSaL2.loc[dataSaL2[\"Min_Ex\"].between(13, 15), 'S_Min_Ex']='J. 13-15 Years'\ndataSaL2.loc[dataSaL2[\"Min_Ex\"] > 15, 'S_Min_Ex']='K. 15+ Years'\n\ndataSaL2['S_Min_Ex'].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tablesalEx = pd.crosstab(dataSaL2[\"Job_Cat\"], dataSaL2[\"S_Min_Ex\"],values=dataSaL2['Min_Salary'],aggfunc=np.median, dropna=False)\ntablesalEx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tablesalEx = pd.crosstab(dataSaL2[\"final_location\"], dataSaL2[\"S_Min_Ex\"],values=dataSaL2['Min_Salary'],aggfunc=np.median, dropna=False)\ntablesalEx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tablesalEx = pd.crosstab(dataSaL2[\"Industry_Final\"], dataSaL2[\"S_Min_Ex\"],values=dataSaL2['Min_Salary'],aggfunc=np.median, dropna=False)\ntablesalEx","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}