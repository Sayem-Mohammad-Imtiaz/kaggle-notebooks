{"cells":[{"metadata":{},"cell_type":"markdown","source":"![http://img.youtube.com/vi/ObpeolfZMPs/sddefault.jpg](http://img.youtube.com/vi/ObpeolfZMPs/sddefault.jpg)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Diabetes is a chronic disease that occurs when the pancreas is no longer able to make insulin, or when the body cannot make good use of the insulin it produces.\n\nInsulin is a hormone made by the pancreas, that acts like a key to let glucose from the food we eat pass from the blood stream into the cells in the body to produce energy. All carbohydrate foods are broken down into glucose in the blood. Insulin helps glucose get into the cells. \n\nNot being able to produce insulin or use it effectively leads to raised glucose levels in the blood (known as hyperglycaemia). Over the long-term high glucose levels are associated with damage to the body and failure of various organs and tissues.\n\n\nTypes of diabetes\nThere are three main types of diabetes â€“ type 1, type 2 and gestational.\n\n1. Type 1 diabetes can develop at any age, but occurs most frequently in children and adolescents. When you have type 1 diabetes, your body produces very little or no insulin, which means that you need daily insulin injections to maintain blood glucose levels under control. Learn more.\n2. Type 2 diabetes is more common in adults and accounts for around 90% of all diabetes cases. When you have type 2 diabetes, your body does not make good use of the insulin that it produces. The cornerstone of type 2 diabetes treatment is healthy lifestyle, including increased physical activity and healthy diet. However, over time most people with type 2 diabetes will require oral drugs and/or insulin to keep their blood glucose levels under control. Learn more.\n3. Gestational diabetes (GDM) is a type of diabetes that consists of high blood glucose during pregnancy and is associated with complications to both mother and child. GDM usually disappears after pregnancy but women affected and their children are at increased risk of developing type 2 diabetes later in life\n\nSource Information :https://www.idf.org/aboutdiabetes/what-is-diabetes.html","execution_count":null},{"metadata":{"papermill":{"duration":0.018693,"end_time":"2020-08-22T05:07:58.081063","exception":false,"start_time":"2020-08-22T05:07:58.06237","status":"completed"},"tags":[]},"cell_type":"markdown","source":"In this study, we tried to classifying diabetes using 6 different algorithms:\n1. Logistic regression classification\n2. SVM (Support Vector Machine) classification\n3. Naive bayes classification\n4. Decision tree classification\n5. Random forest classification\n6. K-Nearest Neighbor classification\n\nPredictor variable use in classifying pima-indians-diabetes-database:\n* Pregnancies\n* Glucose\n* BloodPressure\n* SkinThickness\n* Insulin\n* BMI\n* DiabetesPedigreeFunction\n* Age\n\n\n1. ","execution_count":null},{"metadata":{"papermill":{"duration":0.018788,"end_time":"2020-08-22T05:07:58.118973","exception":false,"start_time":"2020-08-22T05:07:58.100185","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Import Library","execution_count":null},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2020-08-22T05:07:58.165856Z","iopub.status.busy":"2020-08-22T05:07:58.164912Z","iopub.status.idle":"2020-08-22T05:07:59.302769Z","shell.execute_reply":"2020-08-22T05:07:59.301766Z"},"papermill":{"duration":1.16472,"end_time":"2020-08-22T05:07:59.302962","exception":false,"start_time":"2020-08-22T05:07:58.138242","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.018913,"end_time":"2020-08-22T05:07:59.341406","exception":false,"start_time":"2020-08-22T05:07:59.322493","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Data\n\n","execution_count":null},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T05:07:59.393714Z","iopub.status.busy":"2020-08-22T05:07:59.392784Z","iopub.status.idle":"2020-08-22T05:07:59.447392Z","shell.execute_reply":"2020-08-22T05:07:59.44814Z"},"papermill":{"duration":0.087547,"end_time":"2020-08-22T05:07:59.448376","exception":false,"start_time":"2020-08-22T05:07:59.360829","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')\nprint('Dataset :',data.shape)\ndata.info()\ndata[0:10]","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T05:07:59.497024Z","iopub.status.busy":"2020-08-22T05:07:59.496082Z","iopub.status.idle":"2020-08-22T05:07:59.701103Z","shell.execute_reply":"2020-08-22T05:07:59.700186Z"},"papermill":{"duration":0.232348,"end_time":"2020-08-22T05:07:59.701265","exception":false,"start_time":"2020-08-22T05:07:59.468917","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Distribution of Outcome\ndata.Outcome.value_counts()[0:30].plot(kind='bar')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.019653,"end_time":"2020-08-22T05:07:59.741435","exception":false,"start_time":"2020-08-22T05:07:59.721782","status":"completed"},"tags":[]},"cell_type":"markdown","source":"**VISUALIZING THE DATA\n**","execution_count":null},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T05:07:59.790437Z","iopub.status.busy":"2020-08-22T05:07:59.78952Z","iopub.status.idle":"2020-08-22T05:08:07.571961Z","shell.execute_reply":"2020-08-22T05:08:07.57258Z"},"papermill":{"duration":7.810868,"end_time":"2020-08-22T05:08:07.572788","exception":false,"start_time":"2020-08-22T05:07:59.76192","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"sns.set_style(\"whitegrid\")\nsns.pairplot(data,hue=\"Outcome\",size=3);\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot outcome by age\nsns.boxplot(x=\"Outcome\",y=\"Age\",data=data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.021654,"end_time":"2020-08-22T05:08:07.778652","exception":false,"start_time":"2020-08-22T05:08:07.756998","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Data for training and testing\n\nTo select a set of training data that will be input in the Machine Learning algorithm, to ensure that the classification algorithm training can be generalized well to new data. For this study using a sample size of 10%, assumed it ideal ratio between training and testing","execution_count":null},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T05:08:07.831209Z","iopub.status.busy":"2020-08-22T05:08:07.830102Z","iopub.status.idle":"2020-08-22T05:08:07.892232Z","shell.execute_reply":"2020-08-22T05:08:07.892904Z"},"papermill":{"duration":0.092568,"end_time":"2020-08-22T05:08:07.893133","exception":false,"start_time":"2020-08-22T05:08:07.800565","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nY = data['Outcome']\nX = data.drop(columns=['Outcome'])\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=9)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T05:08:07.945569Z","iopub.status.busy":"2020-08-22T05:08:07.944459Z","iopub.status.idle":"2020-08-22T05:08:07.94821Z","shell.execute_reply":"2020-08-22T05:08:07.949094Z"},"papermill":{"duration":0.034268,"end_time":"2020-08-22T05:08:07.949346","exception":false,"start_time":"2020-08-22T05:08:07.915078","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"print('X train shape: ', X_train.shape)\nprint('Y train shape: ', Y_train.shape)\nprint('X test shape: ', X_test.shape)\nprint('Y test shape: ', Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.021737,"end_time":"2020-08-22T05:08:07.992985","exception":false,"start_time":"2020-08-22T05:08:07.971248","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## 1. Logistic regression classification\n\nLogistic regression is a technique that can be applied to binary classification problems. This technique uses the logistic function or sigmoid function, which is an S-shaped curve that can assume any real value number and assign it to a value between 0 and 1, but never exactly in those limits. Thus, logistic regression models the probability of the default class (the probability that an input $(X)$ belongs to the default class $(Y=1)$) $(P(X)=P(Y=1|X))$. In order to make the prediction of the probability, the logistic function is used, which allows us to obtain the log-odds or the probit. Thus, the model is a linear combination of the inputs, but that this linear combination relates to the log-odds of the default class.\n\nStarted from make an instance of the model setting the default values. Specify the inverse of the regularization strength in 10. Trained the logistic regression model with the training data, and then applied such model to the test data.","execution_count":null},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T05:08:08.045269Z","iopub.status.busy":"2020-08-22T05:08:08.04441Z","iopub.status.idle":"2020-08-22T05:08:08.166693Z","shell.execute_reply":"2020-08-22T05:08:08.165787Z"},"papermill":{"duration":0.151295,"end_time":"2020-08-22T05:08:08.166853","exception":false,"start_time":"2020-08-22T05:08:08.015558","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# We defining the model\nlogreg = LogisticRegression(C=10)\n\n# We train the model\nlogreg.fit(X_train, Y_train)\n\n# We predict target values\nY_predict1 = logreg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T05:08:08.229583Z","iopub.status.busy":"2020-08-22T05:08:08.228687Z","iopub.status.idle":"2020-08-22T05:08:08.472988Z","shell.execute_reply":"2020-08-22T05:08:08.472172Z"},"papermill":{"duration":0.284273,"end_time":"2020-08-22T05:08:08.473134","exception":false,"start_time":"2020-08-22T05:08:08.188861","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# The confusion matrix\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\nlogreg_cm = confusion_matrix(Y_test, Y_predict1)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(logreg_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='g', ax=ax, cmap=\"YlGnBu\")\nplt.title('Logistic Regression Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T05:08:08.527826Z","iopub.status.busy":"2020-08-22T05:08:08.526717Z","iopub.status.idle":"2020-08-22T05:08:08.53054Z","shell.execute_reply":"2020-08-22T05:08:08.531111Z"},"papermill":{"duration":0.036461,"end_time":"2020-08-22T05:08:08.531332","exception":false,"start_time":"2020-08-22T05:08:08.494871","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Test score\nscore_logreg = logreg.score(X_test, Y_test)\nprint(score_logreg)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.021694,"end_time":"2020-08-22T05:08:08.575287","exception":false,"start_time":"2020-08-22T05:08:08.553593","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## 2. SVM (Support Vector Machine) classification\n\nSVMs (Support Vector Machine) have shown a rapid proliferation during the last years. The learning problem setting for SVMs corresponds to a some unknown and nonlinear dependency (mapping, function) $y = f(x)$ between some high-dimensional input vector $x$ and scalar output $y$. It is noteworthy that there is no information on the joint probability functions, therefore, a free distribution learning must be carried out. The only information available is a training data set $D = {(x_i, y_i) âˆˆ XÃ—Y }, i = 1$, $l$, where $l$ stands for the number of the training data pairs and is therefore equal to the size of the training data set $D$, additionally, $y_i$ is denoted as $d_i$, where $d$ stands for a desired (target) value. Hence, SVMs belong to the supervised learning techniques.\n\nFrom the classification approach, the goal of SVM is to find a hyperplane in an N-dimensional space that clearly classifies the data points. Thus hyperplanes are decision boundaries that help classify the data points. Data points falling on either side of the hyperplane can be attributed to different classes.\n\n\n\n","execution_count":null},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T05:08:08.628961Z","iopub.status.busy":"2020-08-22T05:08:08.627785Z","iopub.status.idle":"2020-08-22T05:08:10.855745Z","shell.execute_reply":"2020-08-22T05:08:10.854887Z"},"papermill":{"duration":2.258525,"end_time":"2020-08-22T05:08:10.855912","exception":false,"start_time":"2020-08-22T05:08:08.597387","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\n\n# We define the SVM model\nsvmcla = OneVsRestClassifier(BaggingClassifier(SVC(C=10,kernel='rbf',random_state=9, probability=True), \n                                               n_jobs=-1))\n\n# We train model\nsvmcla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict2 = svmcla.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T05:08:10.923764Z","iopub.status.busy":"2020-08-22T05:08:10.922565Z","iopub.status.idle":"2020-08-22T05:08:11.167719Z","shell.execute_reply":"2020-08-22T05:08:11.166938Z"},"papermill":{"duration":0.289334,"end_time":"2020-08-22T05:08:11.167869","exception":false,"start_time":"2020-08-22T05:08:10.878535","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# The confusion matrix\nsvmcla_cm = confusion_matrix(Y_test, Y_predict2)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(svmcla_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='g', ax=ax, cmap=\"YlGnBu\")\nplt.title('SVM Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T05:08:11.222612Z","iopub.status.busy":"2020-08-22T05:08:11.221564Z","iopub.status.idle":"2020-08-22T05:08:11.246883Z","shell.execute_reply":"2020-08-22T05:08:11.246222Z"},"papermill":{"duration":0.056617,"end_time":"2020-08-22T05:08:11.247032","exception":false,"start_time":"2020-08-22T05:08:11.190415","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Test score\nscore_svmcla = svmcla.score(X_test, Y_test)\nprint(score_svmcla)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.022003,"end_time":"2020-08-22T05:08:11.291832","exception":false,"start_time":"2020-08-22T05:08:11.269829","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## 3. Naive bayes classification\n\nThe naive Bayesian classifier is a probabilistic classifier based on Bayes' theorem with strong independence assumptions between the features. Thus, using Bayes theorem $\\left(P(X|Y)=\\frac{P(Y|X)P(X)}{P(Y)}\\right)$, we can find the probability of $X$ happening, given that $Y$ has occurred. Here, $Y$ is the evidence and $X$ is the hypothesis. The assumption made here is that the presence of one particular feature does not affect the other (the predictors/features are independent). Hence it is called naive. In this case we will assume that we assume the values are sampled from a Gaussian distribution and therefore we consider a Gaussian Naive Bayes.","execution_count":null},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T05:08:11.34477Z","iopub.status.busy":"2020-08-22T05:08:11.343865Z","iopub.status.idle":"2020-08-22T05:08:11.356447Z","shell.execute_reply":"2020-08-22T05:08:11.357026Z"},"papermill":{"duration":0.043005,"end_time":"2020-08-22T05:08:11.357237","exception":false,"start_time":"2020-08-22T05:08:11.314232","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\n# We define the model\nnbcla = GaussianNB()\n\n# We train model\nnbcla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict3 = nbcla.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T05:08:11.605683Z","iopub.status.busy":"2020-08-22T05:08:11.60477Z","iopub.status.idle":"2020-08-22T05:08:11.855603Z","shell.execute_reply":"2020-08-22T05:08:11.854867Z"},"papermill":{"duration":0.475845,"end_time":"2020-08-22T05:08:11.855756","exception":false,"start_time":"2020-08-22T05:08:11.379911","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# The confusion matrix\nnbcla_cm = confusion_matrix(Y_test, Y_predict3)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(nbcla_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='g', ax=ax, cmap=\"YlGnBu\")\nplt.title('Naive Bayes Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T05:08:11.911482Z","iopub.status.busy":"2020-08-22T05:08:11.910242Z","iopub.status.idle":"2020-08-22T05:08:11.916038Z","shell.execute_reply":"2020-08-22T05:08:11.916826Z"},"papermill":{"duration":0.038484,"end_time":"2020-08-22T05:08:11.917078","exception":false,"start_time":"2020-08-22T05:08:11.878594","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Test score\nscore_nbcla = nbcla.score(X_test, Y_test)\nprint(score_nbcla)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.022288,"end_time":"2020-08-22T05:08:11.962432","exception":false,"start_time":"2020-08-22T05:08:11.940144","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## 4. Decision tree classification\n\nA decision tree is a flowchart-like tree structure where an internal node represents feature, the branch represents a decision rule, and each leaf node represents the outcome. The decision tree analyzes a set of data to construct a set of rules or questions, which are used to predict a class, i.e., the goal of decision tree is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. In this sense the decision tree selects the best attribute using to divide the records, converting that attribute into a decision node and dividing the data set into smaller subsets, to finally start the construction of the tree repeating this process recursively. ","execution_count":null},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T05:08:12.020451Z","iopub.status.busy":"2020-08-22T05:08:12.019646Z","iopub.status.idle":"2020-08-22T05:08:12.025954Z","shell.execute_reply":"2020-08-22T05:08:12.02527Z"},"papermill":{"duration":0.040966,"end_time":"2020-08-22T05:08:12.026109","exception":false,"start_time":"2020-08-22T05:08:11.985143","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\n# We define the model\ndtcla = DecisionTreeClassifier(random_state=9)\n\n# We train model\ndtcla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict4 = dtcla.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T05:08:12.091995Z","iopub.status.busy":"2020-08-22T05:08:12.091106Z","iopub.status.idle":"2020-08-22T05:08:12.330587Z","shell.execute_reply":"2020-08-22T05:08:12.331148Z"},"papermill":{"duration":0.28235,"end_time":"2020-08-22T05:08:12.331379","exception":false,"start_time":"2020-08-22T05:08:12.049029","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# The confusion matrix\ndtcla_cm = confusion_matrix(Y_test, Y_predict4)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(dtcla_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='g', ax=ax, cmap=\"YlGnBu\")\nplt.title('Decision Tree Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T05:08:12.388639Z","iopub.status.busy":"2020-08-22T05:08:12.387656Z","iopub.status.idle":"2020-08-22T05:08:12.391955Z","shell.execute_reply":"2020-08-22T05:08:12.391187Z"},"papermill":{"duration":0.037485,"end_time":"2020-08-22T05:08:12.392109","exception":false,"start_time":"2020-08-22T05:08:12.354624","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Test score\nscore_dtcla = dtcla.score(X_test, Y_test)\nprint(score_dtcla)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.022488,"end_time":"2020-08-22T05:08:12.437639","exception":false,"start_time":"2020-08-22T05:08:12.415151","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## 5. Random forest classification\n\nBased on the previous classification method, random forest is a supervised learning algorithm that creates a forest randomly. This forest, is a set of decision trees, most of the times trained with the bagging method. The essential idea of bagging is to average many noisy but approximately impartial models, and therefore reduce the variation. Each tree is constructed using the following algorithm:\n\n* Let $N$ be the number of test cases, $M$ is the number of variables in the classifier.\n* Let $m$ be the number of input variables to be used to determine the decision in a given node; $m<M$.\n* Choose a training set for this tree and use the rest of the test cases to estimate the error.\n* For each node of the tree, randomly choose $m$ variables on which to base the decision. Calculate the best partition of the training set from the $m$ variables.\n\nFor prediction a new case is pushed down the tree. Then it is assigned the label of the terminal node where it ends. This process is iterated by all the trees in the assembly, and the label that gets the most incidents is reported as the prediction. We define the number of trees in the forest in 100. ","execution_count":null},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T05:08:12.541274Z","iopub.status.busy":"2020-08-22T05:08:12.510205Z","iopub.status.idle":"2020-08-22T05:08:12.997258Z","shell.execute_reply":"2020-08-22T05:08:12.997893Z"},"papermill":{"duration":0.537257,"end_time":"2020-08-22T05:08:12.99812","exception":false,"start_time":"2020-08-22T05:08:12.460863","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# We define the model\nrfcla = RandomForestClassifier(n_estimators=100,random_state=9,n_jobs=-1)\n\n# We train model\nrfcla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict5 = rfcla.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T05:08:13.078393Z","iopub.status.busy":"2020-08-22T05:08:13.077032Z","iopub.status.idle":"2020-08-22T05:08:13.348447Z","shell.execute_reply":"2020-08-22T05:08:13.347786Z"},"papermill":{"duration":0.325616,"end_time":"2020-08-22T05:08:13.348609","exception":false,"start_time":"2020-08-22T05:08:13.022993","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# The confusion matrix\nrfcla_cm = confusion_matrix(Y_test, Y_predict5)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(rfcla_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='g', ax=ax, cmap=\"YlGnBu\")\nplt.title('Random Forest Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T05:08:13.404946Z","iopub.status.busy":"2020-08-22T05:08:13.403888Z","iopub.status.idle":"2020-08-22T05:08:13.513868Z","shell.execute_reply":"2020-08-22T05:08:13.512646Z"},"papermill":{"duration":0.142351,"end_time":"2020-08-22T05:08:13.514217","exception":false,"start_time":"2020-08-22T05:08:13.371866","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Test score\nscore_rfcla = rfcla.score(X_test, Y_test)\nprint(score_rfcla)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.023244,"end_time":"2020-08-22T05:08:13.561936","exception":false,"start_time":"2020-08-22T05:08:13.538692","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## 6. K-Nearest Neighbor classification\n\nK-Nearest neighbors is a technique that stores all available cases and **classifies new cases based on a similarity measure (e.g., distance functions)**. This technique is non-parametric since there are no assumptions for the distribution of underlying data and it is lazy since it does not need any training data point model generation. All the training data used in the test phase. **This makes the training faster and the test phase slower and more costlier. In this technique, the number of neighbors k is usually an odd number if the number of classes is 2**. For finding closest similar points,  find the distance between points using distance measures such as Euclidean distance, Hamming distance, Manhattan distance and Minkowski distance.\n\n","execution_count":null},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T05:08:13.698252Z","iopub.status.busy":"2020-08-22T05:08:13.696977Z","iopub.status.idle":"2020-08-22T05:08:13.805998Z","shell.execute_reply":"2020-08-22T05:08:13.805205Z"},"papermill":{"duration":0.147031,"end_time":"2020-08-22T05:08:13.80618","exception":false,"start_time":"2020-08-22T05:08:13.659149","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\n# We define the model\nknncla = KNeighborsClassifier(n_neighbors=5,n_jobs=-1)\n\n# We train model\nknncla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict6 = knncla.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T05:08:13.872249Z","iopub.status.busy":"2020-08-22T05:08:13.87139Z","iopub.status.idle":"2020-08-22T05:08:14.116295Z","shell.execute_reply":"2020-08-22T05:08:14.115447Z"},"papermill":{"duration":0.285502,"end_time":"2020-08-22T05:08:14.116477","exception":false,"start_time":"2020-08-22T05:08:13.830975","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# The confusion matrix\nknncla_cm = confusion_matrix(Y_test, Y_predict6)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(knncla_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='g', ax=ax, cmap=\"YlGnBu\")\nplt.title('KNN Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T05:08:14.175087Z","iopub.status.busy":"2020-08-22T05:08:14.173886Z","iopub.status.idle":"2020-08-22T05:08:14.284496Z","shell.execute_reply":"2020-08-22T05:08:14.283356Z"},"papermill":{"duration":0.143753,"end_time":"2020-08-22T05:08:14.284703","exception":false,"start_time":"2020-08-22T05:08:14.14095","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Test score\nscore_knncla= knncla.score(X_test, Y_test)\nprint(score_knncla)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.023515,"end_time":"2020-08-22T05:08:14.332792","exception":false,"start_time":"2020-08-22T05:08:14.309277","status":"completed"},"tags":[]},"cell_type":"markdown","source":"****## Comparison of classification techniques","execution_count":null},{"metadata":{"papermill":{"duration":0.023595,"end_time":"2020-08-22T05:08:14.38053","exception":false,"start_time":"2020-08-22T05:08:14.356935","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Test score","execution_count":null},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T05:08:14.438094Z","iopub.status.busy":"2020-08-22T05:08:14.437115Z","iopub.status.idle":"2020-08-22T05:08:14.441668Z","shell.execute_reply":"2020-08-22T05:08:14.440893Z"},"papermill":{"duration":0.037342,"end_time":"2020-08-22T05:08:14.441814","exception":false,"start_time":"2020-08-22T05:08:14.404472","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"Testscores = pd.Series([score_logreg, score_svmcla, score_nbcla, score_dtcla, score_rfcla, score_knncla], \n                        index=['Logistic Regression Score', 'Support Vector Machine Score', 'Naive Bayes Score', 'Decision Tree Score', 'Random Forest Score', 'K-Nearest Neighbour Score']) \nprint(Testscores)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.02339,"end_time":"2020-08-22T05:08:14.489149","exception":false,"start_time":"2020-08-22T05:08:14.465759","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### The confusion matrix","execution_count":null},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T05:08:14.563551Z","iopub.status.busy":"2020-08-22T05:08:14.56249Z","iopub.status.idle":"2020-08-22T05:08:17.083723Z","shell.execute_reply":"2020-08-22T05:08:17.084322Z"},"papermill":{"duration":2.571905,"end_time":"2020-08-22T05:08:17.08453","exception":false,"start_time":"2020-08-22T05:08:14.512625","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15,15))\nax1 = fig.add_subplot(3, 3, 1) \nax1.set_title('Logistic Regression Classification') \nax2 = fig.add_subplot(3, 3, 2) \nax2.set_title('SVM Classification')\nax3 = fig.add_subplot(3, 3, 3)\nax3.set_title('Naive Bayes Classification')\nax4 = fig.add_subplot(3, 3, 4)\nax4.set_title('Decision Tree Classification')\nax5 = fig.add_subplot(3, 3, 5)\nax5.set_title('Random Forest Classification')\nax6 = fig.add_subplot(3, 3, 6)\nax6.set_title('KNN Classification')\nsns.heatmap(data=logreg_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax1)\nsns.heatmap(data=svmcla_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax2)  \nsns.heatmap(data=nbcla_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax3)\nsns.heatmap(data=dtcla_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax4)\nsns.heatmap(data=rfcla_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax5)\nsns.heatmap(data=knncla_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax6)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.024026,"end_time":"2020-08-22T05:08:17.133034","exception":false,"start_time":"2020-08-22T05:08:17.109008","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### ROC curve","execution_count":null},{"metadata":{"execution":{"iopub.execute_input":"2020-08-22T05:08:17.201441Z","iopub.status.busy":"2020-08-22T05:08:17.196279Z","iopub.status.idle":"2020-08-22T05:08:19.291148Z","shell.execute_reply":"2020-08-22T05:08:19.290481Z"},"papermill":{"duration":2.134106,"end_time":"2020-08-22T05:08:19.291294","exception":false,"start_time":"2020-08-22T05:08:17.157188","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\n\n# Logistic Regression Classification\nY_predict1_proba = logreg.predict_proba(X_test)\nY_predict1_proba = Y_predict1_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_test, Y_predict1_proba)\nplt.subplot(331)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve Logistic Regression')\nplt.grid(True)\n\n# SVM Classification\nY_predict2_proba = svmcla.predict_proba(X_test)\nY_predict2_proba = Y_predict2_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_test, Y_predict2_proba)\nplt.subplot(332)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve SVM')\nplt.grid(True)\n\n# Naive Bayes Classification\nY_predict3_proba = nbcla.predict_proba(X_test)\nY_predict3_proba = Y_predict3_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_test, Y_predict3_proba)\nplt.subplot(333)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve Naive Bayes')\nplt.grid(True)\n\n# Decision Tree Classification\nY_predict4_proba = dtcla.predict_proba(X_test)\nY_predict4_proba = Y_predict4_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_test, Y_predict4_proba)\nplt.subplot(334)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve Decision Tree')\nplt.grid(True)\n\n# Random Forest Classification\nY_predict5_proba = rfcla.predict_proba(X_test)\nY_predict5_proba = Y_predict5_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_test, Y_predict5_proba)\nplt.subplot(335)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve Random Forest')\nplt.grid(True)\n\n# KNN Classification\nY_predict6_proba = knncla.predict_proba(X_test)\nY_predict6_proba = Y_predict6_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_test, Y_predict6_proba)\nplt.subplot(336)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve KNN')\nplt.grid(True)\nplt.subplots_adjust(top=2, bottom=0.08, left=0.10, right=1.4, hspace=0.45, wspace=0.45)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}