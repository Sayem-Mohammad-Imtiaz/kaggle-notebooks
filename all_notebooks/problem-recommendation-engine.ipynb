{"cells":[{"metadata":{},"cell_type":"markdown","source":"Competition Link: https://datahack.analyticsvidhya.com/contest/practice-problem-recommendation-engine\n\nRecommendation Engine\n\nRecommending the questions that a programmer should solve given his/her current expertise is a big challenge for Online Judge Platforms but is an essential task to keep a programmer engaged on their platform.\n\nIn this practice problem, you are given the data of programmers and questions that they have previously solved along with the time that they took to solve that particular question.\n\nAs a data scientist, your task is to build a model that can predict the time taken to solve a problem given the user current status.\n\nThis model will help online judges to decide the next level of questions to recommend to a user.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Necessary imports\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nimport time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_df = pd.read_csv(r'../input/recommendation-engine/user_data.csv')\nproblem_df = pd.read_csv(r'../input/recommendation-engine/problem_data.csv')\ntrain_submussion_df = pd.read_csv(r'../input/recommendation-engine/train_submissions.csv')\ntest_submussion_df = pd.read_csv(r'../input/recommendation-engine/test_submissions_NeDLEvX.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets look at the sample data for each of the data frame. Sample data for user data\nuser_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#description of user data\nuser_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#count of null values\nuser_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#percentage of null values\nuser_df.isnull().mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting submision counts\nsns.distplot(user_df[\"submission_count\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting problem solved counts\nuser_df['submission_count'].quantile([.1, .2, .3, .4, .5, .6, .7, .8, .9])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating bins for submission counts\nsubmission_count_bins = pd.qcut(user_df[\"submission_count\"], 4,labels = False)\n\n#creating the new column for quantiled submission count\nuser_df[\"submission_count_bins\"] = submission_count_bins.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's have a look at the distribution for submission count bins\nsns.distplot(user_df[\"submission_count_bins\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting problem solved counts\nsns.distplot(user_df[\"problem_solved\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting problem solved counts\nuser_df['problem_solved'].quantile([.2, .4,.6, .8])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#quantiling the problem solved counts\nproblem_solved_bins = pd.qcut(user_df[\"problem_solved\"], 5,labels = False)\n\n#creating bins for problem solved counts\nuser_df[\"problem_solved_bins\"] = problem_solved_bins.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's look at the distribution of the problem solved bins\nsns.distplot(user_df[\"problem_solved_bins\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's have a look at the new column for problem solved bins\nuser_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#looks like problem solved count bins and submission count bins are identical. So I will check them once if they are idential\nuser_df['submission_count_bins'].equals(user_df['problem_solved_bins'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"submission_count_bins and problem_solved_bins are not identical so we can proceed with the other columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#define success rate as a column\nuser_df['success_rate'] = user_df['problem_solved']/user_df['submission_count']*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's look at the distribution of the contribution\nsns.distplot(user_df[\"contribution\"], kde=False, rug=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#this is quite skewed with 2530 values as 0\nuser_df[\"contribution\"].value_counts(normalize = True).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will have to remove the fiels contribution as it has 70% values as 0","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now let's look at the number of countries or values\nuser_df[\"country\"].unique().shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Strategy for imputing the null values will be based on the ratio of occurence of the countries in the rest of the data.\nFor example, India occured 25.6% and Bangladesh occured 13.6% and so on. We will use this ratio of all the countries to fill the missing data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Getting all the ratios\ncountry_data = (user_df[\"country\"].value_counts()/user_df[\"country\"].count())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#imputing missing values\nuser_df[\"country\"]= user_df[\"country\"].fillna(pd.Series(np.random.choice(country_data.index,p=country_data.values, size=len(user_df))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Country is a categorical feature and there are 79 levels. I would like to keep the levels to 10. So, I will keep the first 9 countries by count and the rest of the countries will be put under \"other\"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"country_list = user_df['country'].value_counts().index[:9]\nuser_df['country_new'] = np.where(user_df['country'].isin(country_list), user_df['country'], 'Other')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now let's look at the countries distribution\nsns.countplot(user_df[\"country_new\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#value counts of new field\nuser_df[\"country_new\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting follower_count\nsns.distplot(user_df[\"follower_count\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_df.loc[user_df[\"follower_count\"]==0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#quantiling the follower_count\nuser_df['follower_count'].quantile([.2, .4, .6, .8])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating bins for submission counts\nfollower_count_bins = pd.qcut(user_df[\"follower_count\"], 5,labels = False)\n#creating the new column for quantiled submission count\nuser_df[\"follower_count_bins\"] = follower_count_bins.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's have a look at the new distribution\nsns.distplot(user_df[\"follower_count_bins\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's find the age of the user in the platform in months\nuser_df[\"age_in_platform\"] = (user_df[\"last_online_time_seconds\"] - user_df[\"registration_time_seconds\"])/(24*3600*30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(user_df[\"age_in_platform\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting max_rating\nsns.distplot(user_df[\"max_rating\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating bins for max_rating counts\nmax_rating_bins = pd.qcut(user_df[\"max_rating\"], 4,labels = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating the new column for quantiled max_rating count\nuser_df[\"max_rating_bins\"] = max_rating_bins.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting max_rating counts\nsns.distplot(user_df[\"max_rating_bins\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting submision counts\nsns.distplot(user_df[\"rating\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now let's look at the unique ranks\nuser_df[\"rank\"].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now let's look at the rank distribution\nsns.countplot(user_df[\"rank\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Percentage distribution of rank\nsns.barplot(user_df[\"rank\"].value_counts(normalize = True).index, user_df[\"rank\"].value_counts(normalize = True).values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#percentage distribution of rank. It looks good to go\nuser_df[\"rank\"].value_counts(normalize = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#70% values are 0, so we can drop this field\nuser_df.drop(columns = [\"contribution\"],axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop country as we have a new field for country with 'Other'\nuser_df.drop(columns = [\"country\"],axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#registration time in years\nuser_df[\"registration_time\"] = (time.time()-user_df[\"registration_time_seconds\"])/(3600*24*365)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#last online time in years\nuser_df[\"last_online_time\"] = (time.time()-user_df[\"last_online_time_seconds\"])/(3600*24*365)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop last_online_time_seconds and registration_time_seconds as we have new fields for them\nuser_df.drop(columns = [\"last_online_time_seconds\",\"registration_time_seconds\"],axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#change values of country_new using a label encoder\nlabelencoder = LabelEncoder()\nuser_df['country_new'] = labelencoder.fit_transform(user_df['country_new'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#change values of rank to numeric\nrank_dict = {'beginner':0, 'intermediate':1, 'advanced':2, 'expert':3}\nuser_df[\"rank\"] = user_df[\"rank\"].apply(lambda x: rank_dict[x])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the problem data now","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets look at the sample data for problem data.\nproblem_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's look at the null values and the shape of the problem data\nprint(problem_df.shape)\nprint(problem_df.isna().sum())\nprint(problem_df.isna().mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's look at the distribution of the level type\nproblem_df.level_type.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's look at the distribution of the level type\nproblem_df.level_type.value_counts(normalize = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#I will fill up the values based on the ratio of distribution\n#Getting all the ratios\nlevel_type_data = (problem_df[\"level_type\"].value_counts()/problem_df[\"level_type\"].count())\n\n#imputing missing values\nproblem_df[\"level_type_new\"]= problem_df[\"level_type\"].fillna(pd.Series(np.random.choice(level_type_data.index,p=level_type_data.values, size=len(problem_df))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now I will have to label the level_type_new field\nlevel_type_dict = {'A':0, 'B':1, 'C':2, 'D':3,'E':4,'F':5,'G':6,'H':7,'I':8,'J':9,'K':10,'L':11,'M':12,'N':13}\nproblem_df[\"level_type_new\"] = problem_df[\"level_type_new\"].apply(lambda x: level_type_dict[x])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(problem_df[\"points\"].mean())\nprint(problem_df[\"points\"].mode())\nprint(problem_df[\"points\"].median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#imputing missing points values\nproblem_df[\"points\"]= problem_df[\"points\"].fillna(problem_df[\"points\"].mean())\n#I will fill up the values based on the ratio of distribution\n#Getting all the ratios\n#points_data = (problem_df[\"points\"].value_counts()/problem_df[\"points\"].count())\n\n#imputing missing values for points\n#problem_df[\"points\"]= problem_df[\"points\"].fillna(pd.Series(np.random.choice(points_data.index,p=points_data.values, size=len(problem_df))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#I will remove level_type as there is a new field for that. tags should be removed as they have more than 50% null values\nproblem_df.drop(columns = [\"level_type\",\"tags\"],axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"problem_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_submussion_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's look at the distribution of the attempts range\nsns.distplot(train_submussion_df[\"attempts_range\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_submussion_df[\"attempts_range\"].value_counts(normalize = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#merge train submission and user data\ntrain_df = pd.merge(train_submussion_df,user_df,how = 'left',on = \"user_id\")\ntest_df = pd.merge(test_submussion_df,user_df,how = 'left',on = \"user_id\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#merge train data and problem data\ntrain_df = pd.merge(train_df,problem_df,how = 'left',on = \"problem_id\")\ntest_df = pd.merge(test_df,problem_df,how = 'left',on = \"problem_id\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create ID field for train data, ID already there for test data\ntrain_df[\"ID\"] = train_df[\"user_id\"] + train_df[\"problem_id\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#user_id count - number of times user is appearing\ntrain_df['user_id_count'] = train_df.groupby('user_id')['user_id'].transform('count')\ntest_df['user_id_count'] = train_df.groupby('user_id')['user_id'].transform('count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#problem_id count - number of times problem is appearing\ntrain_df['problem_id_count'] = train_df.groupby('problem_id')['problem_id'].transform('count')\ntest_df['problem_id_count'] = train_df.groupby('problem_id')['problem_id'].transform('count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#user id min attempts\ntrain_df['user_id_min_attempts'] = train_df.groupby('user_id')['attempts_range'].transform('min')\ntest_df['user_id_min_attempts'] = train_df.groupby('user_id')['attempts_range'].transform('min')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#user id max attempts\ntrain_df['user_id_max_attempts'] = train_df.groupby('user_id')['attempts_range'].transform('max')\ntest_df['user_id_max_attempts'] = train_df.groupby('user_id')['attempts_range'].transform('max')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#user id mean attempts\ntrain_df['user_id_mean_attempts'] = train_df.groupby('user_id')['attempts_range'].transform('mean')\ntest_df['user_id_mean_attempts'] = train_df.groupby('user_id')['attempts_range'].transform('mean')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#problem id min attempts\ntrain_df['problem_id_min_attempts'] = train_df.groupby('problem_id')['attempts_range'].transform('min')\ntest_df['problem_id_min_attempts'] = train_df.groupby('problem_id')['attempts_range'].transform('min')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#problem id max attempts\ntrain_df['problem_id_max_attempts'] = train_df.groupby('problem_id')['attempts_range'].transform('max')\ntest_df['problem_id_max_attempts'] = train_df.groupby('problem_id')['attempts_range'].transform('max')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#problem id mean attempts\ntrain_df['problem_id_mean_attempts'] = train_df.groupby('problem_id')['attempts_range'].transform('mean')\ntest_df['problem_id_mean_attempts'] = train_df.groupby('problem_id')['attempts_range'].transform('mean')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#user id min level\ntrain_df['user_id_min_level'] = train_df.groupby('user_id')['level_type_new'].transform('min')\ntest_df['user_id_min_level'] = train_df.groupby('user_id')['level_type_new'].transform('min')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#user id max level\ntrain_df['user_id_max_level'] = train_df.groupby('user_id')['level_type_new'].transform('max')\ntest_df['user_id_max_level'] = train_df.groupby('user_id')['level_type_new'].transform('max')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#user id mean level\ntrain_df['user_id_mean_level'] = train_df.groupby('user_id')['level_type_new'].transform('mean')\ntest_df['user_id_mean_level'] = train_df.groupby('user_id')['level_type_new'].transform('mean')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['country_percent'] = train_df.groupby('country_new')['country_new'].transform('count')/len(train_df)\ntest_df['country_percent'] = train_df.groupby('country_new')['country_new'].transform('count')/len(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.columns.shape)\nprint(test_df.columns.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define X\nX = train_df.drop(columns=['user_id','problem_id','ID','attempts_range'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define y\ny = train_df[\"attempts_range\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split training data into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#xgb bseline model\nxgbC = XGBClassifier(n_estimators= 300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgbC.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred = xgbC.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_test, y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score(y_test, y_test_pred, average='weighted')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#use random forest regressor\nfrom sklearn.ensemble import RandomForestClassifier\nRF = RandomForestClassifier(max_depth=5, min_samples_leaf=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fit the model\nRF.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred = xgbC.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\nf1_score(y_test, y_test_pred, average='weighted')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}