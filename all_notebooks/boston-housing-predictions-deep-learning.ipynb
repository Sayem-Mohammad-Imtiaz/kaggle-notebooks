{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Boston House Price Predictions\n\nRather than applying traditional machine learning techniques on this dataset, as is the norm, in this notebook we'll look at forming a Deep Neural Network (DNN) for regression. This is normally difficult with such a small dataset and a DNN, since the complexity of this type of model tends to significantly overfit the data at hand.\n\nWithin this notebook, I'll exemplify some good practices to counteract overfitting during training, with a particular emphasis on DNNs."},{"metadata":{},"cell_type":"markdown","source":"---\n\n## 1. Import dependencies and dataset"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH =  \"/kaggle/input/boston-house-prices/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', \n                'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n\nhousing_df = pd.read_csv(PATH + 'housing.csv', header=None, delimiter=r\"\\s+\", names=column_names)\n\nprint(\"Shape of housing dataset: {0}\".format(housing_df.shape))\n\nhousing_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n## 2. Split our data into training and testing partitions."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = housing_df.iloc[:404, :].copy()\ntest_data = housing_df.iloc[404:, :].copy()\n\nX_train = train_data.iloc[:, :-1].copy()\ny_train = train_data.iloc[:, -1:].copy()\n\nX_test = test_data.iloc[:, :-1].copy()\ny_test = test_data.iloc[:, -1:].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n## 3. Normalization of our features\n\nWith many features that are heterogeneous, we should definitely consider standardising our data through normalisation. For this we should calculate the mean and standard deviation of the features within the training data, and use this to normalise both the training and test sets.\n\nIn the following code, we'll normalize our data with zero mean and unit standard deviation. We'll obtain the mean and standard deviation using the training partition, which we will then use to normalize both the training and test splits."},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_normalisation(train_data, test_data):\n    \"\"\" Normalize our dataframe features with zero mean and unit\n        standard deviation \"\"\"\n    \n    std_data = train_data.copy()\n    \n    mean = train_data.mean(axis=0)\n    std_dev = train_data.std(axis=0)\n    \n    # centre data around zero mean and give unit std dev\n    std_data -= mean\n    std_data /= std_dev\n    \n    # if test data passed to func, convert test data using train mean / std dev\n    test_data -= mean\n    test_data /= std_dev\n        \n    return std_data, test_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test = feature_normalisation(X_train, X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prior to producing a neural network model and making subsequent predictions on an evaluated model, lets visualise the relative importance of features using an off the shelf random forrest regressor."},{"metadata":{"trusted":true},"cell_type":"code","source":"ranf = RandomForestRegressor(random_state=1)\nranf.fit(X_train, y_train.values[:, 0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = list(X_train.columns)\n\nimportances = ranf.feature_importances_\nindices = np.argsort(importances)[::-1]\ncols_ordered = []\n\nfor feat in range(X_train.shape[1]):\n    print(\"{0:<5} {1:<25} {2:.5f}\".format(feat + 1, columns[indices[feat]], importances[indices[feat]]))\n    cols_ordered.append(columns[indices[feat]])\n    \nplt.figure(figsize=(6,4))\nplt.bar(range(X_train.shape[1]), importances[indices], align='center')\nplt.xticks(range(X_train.shape[1]), cols_ordered, rotation=90)\nplt.xlim([-1, X_train.shape[1]])\nplt.tight_layout()\nplt.title(\"Random Forrest Feature Importances\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's clear that the only two really important features in our dataset appears to be LSTAT and RM. In comparison to these, the others are almost negigible in terms of their correlation to changing the output house price. However, since we are using a neural network in the following lines of code, we will retain all columns and train our model accordingly, regardless of their importance. If we were using more traditional methods, such as random forests, support vector machine or simple linear regression, we would likely benefit from reduction of some of these less important features."},{"metadata":{},"cell_type":"markdown","source":"---\n## 4. Formation of Deep Neural Network for regression\n\nForming a deep neural network for regression is relatively simple, especially when armed with high-level libraries like Keras or PyTorch. For regression tasks, we simply need to ensure our final output layer has no activation, unlike in classification tasks where we employ sigmoid or softmax output activations."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import models\nfrom keras import layers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def nn_model(dropout=False):\n    \"\"\" Create a basic Deep NN for regression \"\"\"\n    model = models.Sequential()\n    model.add(layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n    if dropout:\n        model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(64, activation='relu'))\n    if dropout:\n        model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(1))\n    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With such a small dataset, we're best employing K-means cross-validation, which makes more from our data than using just one dedicated partition of samples for the validation set."},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 4\n\nnum_val_samples = len(X_train) // k\n\nepochs = 100\n\nscores = []\n\n# prepare validation and training partitions\nfor i in range(k):\n    print('Cross-validation fold number {0}'.format(i))\n    val_samples_x = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n    val_samples_y = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n    \n    print(\"X Val: {0}, y Val: {1}\".format(val_samples_x.shape, val_samples_y.shape))\n    \n    train_samples_x = np.concatenate([X_train[:i * num_val_samples],\n                                      X_train[(i + 1) * num_val_samples:]], axis=0)\n    \n    train_samples_y = np.concatenate([y_train[:i * num_val_samples], \n                                      y_train[(i + 1) * num_val_samples:]], axis=0)\n    \n    print(\"X Train: {0}, y Train: {1}\".format(train_samples_x.shape, train_samples_y.shape))\n    \n    # instantiate model and fit training samples, then evaluate on val partition\n    model = nn_model()\n    model.fit(train_samples_x, train_samples_y, epochs=epochs, batch_size=1, verbose=0)\n    val_mse, val_mae = model.evaluate(val_samples_x, val_samples_y, verbose=0)\n    scores.append(val_mae)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.mean(scores))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Repeat again but obtain a record of our validation performance at each epoch across all folds."},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 4\n\nnum_val_samples = len(X_train) // k\n\nepochs = 100\n\nmae_histories = []\n\n# prepare validation and training partitions\nfor i in range(k):\n    print('Cross-validation fold number {0}'.format(i))\n    val_samples_x = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n    val_samples_y = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n    \n    print(\"X Val: {0}, y Val: {1}\".format(val_samples_x.shape, val_samples_y.shape))\n    \n    train_samples_x = np.concatenate([X_train[:i * num_val_samples],\n                                      X_train[(i + 1) * num_val_samples:]], axis=0)\n    \n    train_samples_y = np.concatenate([y_train[:i * num_val_samples], \n                                      y_train[(i + 1) * num_val_samples:]], axis=0)\n    \n    print(\"X Train: {0}, y Train: {1}\".format(train_samples_x.shape, train_samples_y.shape))\n    \n    # instantiate model and fit training samples, then evaluate on val partition\n    model = nn_model()\n    history = model.fit(train_samples_x, train_samples_y, \n                        epochs=epochs, batch_size=1, \n                        verbose=0, validation_data=(val_samples_x, val_samples_y))\n    \n    val_mae_hist = history.history['val_mae']\n    \n    mae_histories.append(val_mae_hist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"average_mae_hist = [np.mean([x[i] for x in mae_histories]) for i in range(epochs)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.plot(range(1, len(average_mae_hist) + 1), average_mae_hist)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Validation MAE\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.plot(range(1, len(average_mae_hist) + 1), average_mae_hist)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Validation MAE\")\nplt.xlim(0.0, 100.0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, even after a very low number of iterations we begin to over-fit on our data. This is expected with such a small dataset of only 500 samples. To counteract this, we have made use of cross-folds validation. Something that we can apply further to this is regularisation.\n\nBelow we'll apply dropout regularisation in an effort to reduce overfitting."},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 4\n\nnum_val_samples = len(X_train) // k\n\nepochs = 100\n\nreg_mae_histories = []\n\n# prepare validation and training partitions\nfor i in range(k):\n    print('Cross-validation fold number {0}'.format(i))\n    val_samples_x = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n    val_samples_y = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n    \n    print(\"X Val: {0}, y Val: {1}\".format(val_samples_x.shape, val_samples_y.shape))\n    \n    train_samples_x = np.concatenate([X_train[:i * num_val_samples],\n                                      X_train[(i + 1) * num_val_samples:]], axis=0)\n    \n    train_samples_y = np.concatenate([y_train[:i * num_val_samples], \n                                      y_train[(i + 1) * num_val_samples:]], axis=0)\n    \n    print(\"X Train: {0}, y Train: {1}\".format(train_samples_x.shape, train_samples_y.shape))\n    \n    # instantiate dropout regularised model and fit training samples with val data for eval\n    model = nn_model(dropout=True)\n    history = model.fit(train_samples_x, train_samples_y, \n                        epochs=epochs, batch_size=1, \n                        verbose=0, validation_data=(val_samples_x, val_samples_y))\n    \n    val_mae_hist = history.history['val_mae']\n    \n    reg_mae_histories.append(val_mae_hist)\n\naverage_reg_mae_hist = [np.mean([x[i] for x in reg_mae_histories]) for i in range(epochs)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.plot(range(1, len(average_mae_hist) + 1), average_mae_hist, label='Original Model')\nplt.plot(range(1, len(average_reg_mae_hist) + 1), average_reg_mae_hist, label='Regularised Model')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Validation MAE\")\nplt.xlim(1.0, 100.0)\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our dropout regularised model is much better in terms of reducing overfitting."},{"metadata":{},"cell_type":"markdown","source":"---\n## 5. Formation of our evaluated model into a final model\n\n#### Finally, lets make a final model with the entire training set, followed by predictions for our test set using the trained model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# produce our deep NN model using dropout regularisation, trained on all training data\nfinal_model = nn_model(dropout=True)\nhistory = final_model.fit(X_train, y_train, epochs=200, batch_size=1, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist_dict = history.history\n\ntrg_loss = history.history['loss']\ntrg_acc = history.history['mae']\n\nepochs = range(1, len(trg_acc) + 1)\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\nax[0].plot(epochs, trg_loss, label='Training Loss')\nax[1].plot(epochs, trg_acc, label='Training MAE')\nax[0].set_ylabel('Training Loss')\nax[1].set_ylabel('Training MAE')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = final_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_mse, test_mae = final_model.evaluate(X_test, y_test, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Test set performance: \\n- Test MSE: {0} \\n- Test MAE: {1}\".format(test_mse, test_mae))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our final test set performance is not bad - only 3.21 Mean Absolute Error (MAE)! Basically the average amount that our predictions of house prices deviated from the actual values was $3210. When we consider how varying houses can be, coupled with their relatively high prices, this is not a bad average error to have in our set of predictions, especially with the extremely low amount of data-preprocessing and feature engineering conducted in this notebook to obtain this model."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}