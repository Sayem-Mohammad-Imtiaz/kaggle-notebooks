{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Analysing Car Dataset\nI take an exploratory approach into the cars dataset.\n\nI first do exploratory data analysis, following by predictions on car prices.","metadata":{}},{"cell_type":"markdown","source":"# Import necessary packages","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, PowerTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import Ridge, LinearRegression, Lasso, ElasticNet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read in file","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/cars-dataset-audi-bmw-ford-hyundai-skoda-vw/cars_dataset.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initial view on data\nThere are no nulls present, so I go straight into viewing features.","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initial description\nThere are five numerical features of interest. I will delve into these first.","metadata":{}},{"cell_type":"code","source":"df[['price','mileage','tax','mpg','engineSize']].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining feature space\nI list out the numerical and categorical fields. Whilst year is a numerical attribute, it can also be treated categorically. I use it as such here, as a ordinal category rather than an continuous variable. ","metadata":{}},{"cell_type":"code","source":"numeric_vars = ['price','mileage','tax','mpg','engineSize']\ncat_fields = ['model', 'transmission', 'fuelType', 'Make', 'year']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Density plots\nI view the distribution of the datapoints initially here.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(5,figsize=(10,20))\n\nsns.set_theme(style=\"darkgrid\")\nfor index, cols in enumerate(numeric_vars):\n    sns.kdeplot(data=df, x=cols, ax=axs[index], fill=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Zero engine size?\nI spot here that there are zero engineSize values... so how many are there?","metadata":{}},{"cell_type":"code","source":"str(np.round(100*df[df['engineSize'] == 0].shape[0]/df.shape[0],2)) + \" %\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Drop Zero engineSize\nChecking how many there are, we get 0.31% only. So I simply drop them.","metadata":{}},{"cell_type":"code","source":"df = df[df['engineSize'] != 0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# View categorical variables\nI view the categorical features here. There are make imbalances, which could be important since intuitively different makes are priced differently.\n\nFuel Type demonstrate a strong influence of this variable on other features. Different makes have different fuel distributions!\n\nAlso the number of cars are increasing year on year. Do volumes affect the prices? Or is it merely the vehicle age?","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(3,2,figsize=(15,15))\n\nsns.set_theme(style=\"darkgrid\")\nax = sns.countplot(x=\"Make\", data=df, ax = axs[0,0],\n              order = df['Make'].value_counts().index)\nax = sns.countplot(x=\"Make\", data=df, hue = 'fuelType', ax = axs[0,1],\n              order = df['Make'].value_counts().index)\nax = sns.countplot(x=\"transmission\", data=df, hue = 'fuelType', ax = axs[1,0],\n              order = df['transmission'].value_counts().index)\nax = sns.countplot(x=\"transmission\", data=df, hue = 'Make', ax = axs[1,1])\nax2 = sns.countplot(x=\"year\", data=df[df['year'] >= 2010], hue = 'Make', ax = axs[2,0])\nax3 = sns.countplot(x=\"year\", data=df[df['year'] >= 2010], ax = axs[2,1])\naxlab = ax2.set_xticklabels(ax2.get_xticklabels(), rotation=40, ha=\"right\")\naxlab2 = ax3.set_xticklabels(ax3.get_xticklabels(), rotation=40, ha=\"right\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Viewing Models\nThere are many models per make. Some models are in far fewer frequency, and so I clump together these ones.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(7,figsize=(40,80))\n\nmakes = sorted(list(set(df['Make'])))\n\nfor make_index, make_ in enumerate(makes):\n    df_alt = df[df['Make'] == make_].reset_index()\n    df_alt = df_alt[['Make','model']]\n\n    ax = sns.countplot(x = 'model', data = df_alt, ax = axs[make_index],\n                  order = df_alt['model'].value_counts().index)\n    \n    ax.set_title(make_, fontsize = 30)\n    ax.set_xticklabels(ax.get_xmajorticklabels(), fontsize = 16)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stripping whitespaces\nI remove any leading or trailing whitespace from the values in the categorical variables.","metadata":{}},{"cell_type":"code","source":"cols = df.select_dtypes(['object']).columns\ndf[cols] = df[cols].apply(lambda x: x.str.strip())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Grabbing highest volume model\nFor each make, I isolate the most frequent model and clump all the rest into \"Other\".","metadata":{}},{"cell_type":"code","source":"counts = df[['Make','model']].groupby('model').count().reset_index()\ncounts = counts.rename(columns = {'Make':'counts'})\n\ntemp_df = df.merge(counts, left_on='model', right_on='model')[['Make','model','counts']]\ntemp_df.drop_duplicates(inplace = True)\n\ntemp_df = temp_df.groupby([\"Make\"], sort=False).apply(lambda x: x.sort_values([\"counts\"],\n                                                                    ascending = False)).reset_index(drop = True)\n\ntop_list_per_make = list(temp_df.groupby('Make').head(1)['model'])\n\nindexlist = df[~df['model'].isin(top_list_per_make)].index\n\ndf.loc[indexlist,'model'] = 'Other'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Viewing this\nThe clumping lowers cardinality for later steps. However this also excessively clusters the categories. Therefore this feature will be removed later.\n\nI keep it in just for the following visuals.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(7,figsize=(40,80))\n\nmakes = sorted(list(set(df['Make'])))\n\nfor make_index, make_ in enumerate(makes):\n    df_alt = df[df['Make'] == make_].reset_index()\n    df_alt = df_alt[['Make','model']]\n\n    ax = sns.countplot(x = 'model', data = df_alt, ax = axs[make_index],\n                  order = df_alt['model'].value_counts().index)\n    \n    ax.set_title(make_, fontsize = 30)\n    ax.set_xticklabels(ax.get_xmajorticklabels(), fontsize = 16)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cross-viewing numerical and categorical features\nI view every combination of numerical and categorical feature to see if there are any other interesting patterns. I average the values per categorical feature to get an overall picture.\n\n","metadata":{}},{"cell_type":"code","source":"df_pairs = []\nfor numericval in numeric_vars:\n    for catval in cat_fields:\n        df_pair = df[[catval, numericval]].groupby(catval).mean().sort_values([catval]).reset_index()\n        \n        df_pair.rename(columns={df_pair.columns[1]: \"Average \" + df_pair.columns[1]}, inplace = True)\n        \n        df_pairs.append(df_pair)\n\nfig, axs = plt.subplots(5,5,figsize=(20,40))\n\nj = k = l = m = n = 0\n\nfor i, df_out in enumerate(df_pairs):\n        if df_out.columns[0] == 'transmission':\n            sns.barplot(x = df_out[df_out.columns[0]], y = df_out[df_out.columns[1]], ax=axs[j,0])\n            j += 1\n            \n        elif df_out.columns[0] == 'fuelType':\n            ax = sns.barplot(x = df_out[df_out.columns[0]], y = df_out[df_out.columns[1]], ax=axs[k,1])\n            ax.set_ylabel('')\n            k += 1\n            \n        elif df_out.columns[0] == 'Make':\n            ax = sns.barplot(x = df_out[df_out.columns[0]], y = df_out[df_out.columns[1]], ax=axs[l,2])\n            axlab = ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n            ax.set_ylabel('')\n            l += 1\n            \n        elif df_out.columns[0] == 'year':\n            ax = sns.barplot(x = df_out[df_out.columns[0]], y = df_out[df_out.columns[1]], ax=axs[m,3])\n            axlab = ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n            ax.set_ylabel('')\n            m += 1\n            \n        elif df_out.columns[0] == 'model':\n            ax = sns.barplot(x = df_out[df_out.columns[0]], y = df_out[df_out.columns[1]], ax=axs[n,4])\n            axlab = ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n            ax.set_ylabel('')\n            n += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\nThere are some interesting things to notice here. The mileage drops in a roughly exponential manner after 2006, yet an increase happened before this. Perhaps legacy vehicles (pre 2000) are being kept as staple items?\n\nIn general, electric vehicles are good to have - they have better mileage cover than I expected and also smaller engines than Diesel vehicles on average. The mpg is considerably higher, and as a big bonus there is zero tax!","metadata":{}},{"cell_type":"markdown","source":"# Violin plots of each feature\nBreaking these down, there is some range in the values, with fairly high density at lower values.\n\nThe huge range in values demonstrate potential outliers. However I do not intend to remove these, as the extreme prices for example are potentially important in building a pricing model.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(5,5,figsize=(20,40))\n\nj = k = l = m = n = 0\n\nfor numericval in numeric_vars:\n    for catval in cat_fields:\n        if catval == 'transmission':\n            sns.violinplot(x = catval, y = numericval, data=df, ax=axs[j,0])\n            j += 1\n        \n        elif catval == 'fuelType':\n            ax = sns.violinplot(x = catval, y = numericval, data=df, ax=axs[k,1])\n            ax.set_ylabel('')\n            \n            k += 1\n        \n        elif catval == 'Make':\n            ax = sns.violinplot(x = catval, y = numericval, data=df, ax=axs[l,2])\n            ax.set_ylabel('')\n            \n            l += 1\n        \n        elif catval == 'year':\n            ax = sns.violinplot(x = catval, y = numericval, data=df, ax=axs[m,3])\n            axlab = ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n            ax.set_ylabel('')\n            \n            m += 1\n            \n        elif catval == 'model':\n            ax = sns.violinplot(x = catval, y = numericval, data=df, ax=axs[n,4])\n            axlab = ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n            ax.set_ylabel('')\n            \n            n += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Age of vehicle calculation\nI opt here to switch the year from categorical to a numerical variable. I do this by using it to calculate the age in years of a vehicle, which is important as it drives the price as seen in plots above.\n\nGreater granularity on this quantity could potentially improve models.","metadata":{}},{"cell_type":"code","source":"df['Age_of_vehicle'] = 2021 - df['year']\n\nnumeric_vars.append('Age_of_vehicle')\n\ndf.drop('year', axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Collinearity matrix\nThe collinearity matrix demonstrates little collinearity between variables.\n\nIt also shows that the mileage is strongly related to the age (+0.75 coefficient), which suggests that age increases, so does mileage. This is expected. It however also shows that the rise then drop in mileage we saw earlier is, on average, a drop. The rise is irrelevant.","metadata":{}},{"cell_type":"code","source":"# Compute the correlation matrix\ncorr = df[numeric_vars].corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, annot = True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Correlation map\nMapping the values in pairgrid shows the distribution of variables as visualised in the above correlation matrix. The price exponentially decays with mileage.\n\nWhat can also be seen is the linear relationship between age of vehicle and mileage once again.\n\nPerhaps mileage is not a needed feature, since it is encoded by the age. I keep both in anyway.","metadata":{}},{"cell_type":"code","source":"def CorMap(df):\n    df_corr = df[numeric_vars]\n    \n    corr = df_corr.corr()\n\n    g = sns.PairGrid(df_corr)\n    g.map_diag(plt.hist)\n    g.map_offdiag(plt.scatter);\n    \n    return(corr)\n\nplot = CorMap(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Normalising datasets\nThere are skewed distributions in the data. So power transforms are needed to normalise the distribution. I view these transforms here, just for illustration.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(6,2,figsize=(20,40))\n\nfor index, col in enumerate(numeric_vars):\n    xt, _ = stats.yeojohnson(df[col])\n    sns.histplot(df[col], ax = axs[index,0]).set_title(f\"Original data for {col}\")\n    sns.histplot(xt, ax = axs[index,1]).set_title(f\"Transformed data for {col}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing the model\nAs mentioned earlier, I now remove the model as I believe this variable will unnecessarily clutter the response matrix in one-hot encoding (curse of dimensionality) for little gain. The Make granularity is likely sufficient.","metadata":{}},{"cell_type":"code","source":"X = df.drop(columns = ['price', 'model'])\ny = df['price']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Selecting numerical and categorical feature names\nSince year is no longer present, I can encode the variables directly by type.","metadata":{}},{"cell_type":"code","source":"numerical_ix = X_train.select_dtypes(include=['int64', 'float64']).columns\ncategorical_ix = X_train.select_dtypes(include=['object', 'bool']).columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transforms to apply\nI apply one-hot encoding to the categorical variables and Yeo-Johnson (with centre-scaling) to the numerical variables.","metadata":{}},{"cell_type":"code","source":"t = [('cat', OneHotEncoder(), categorical_ix),\n     ('num', PowerTransformer(method = 'yeo-johnson'), numerical_ix)]\n\ncol_transform = ColumnTransformer(transformers=t)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generic pipeline function\nThis pipeline enables arbitrary regression models to be inserted.","metadata":{}},{"cell_type":"code","source":"# define the data preparation and modeling pipeline\ndef pipeline_model(model):\n    pipeline = Pipeline(steps=[('prep',col_transform), ('model', model)])\n    return(pipeline)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# k-fold cross validation\nThis will enable me to choose which model performs best. I use k = 3, and choose a few different linear and non-linear models. The linear models will perform poorly, intuitively speaking.\n\nI include them anyway, just to highlight how bad model choices lead to poor results.\n\nNote also that I do not use GridSearchCV here to optimise hyperparameters for each model. It would be good practice to do this, but in the interest of time I forgo this step.","metadata":{}},{"cell_type":"code","source":"cv = KFold(n_splits=3, shuffle=True, random_state=1)\n\nmodels = [RandomForestRegressor(),\n          SVR(),\n          Ridge(),\n          LinearRegression(),\n          Lasso(),\n          ElasticNet()\n         ]\n\nscore_list = []\nfor _, model_ in enumerate(models):\n    model = TransformedTargetRegressor(regressor=pipeline_model(model_),\n                                 func=np.log1p, inverse_func=np.expm1)\n    \n    scores = cross_val_score(model, X, y, scoring='r2', cv=cv, n_jobs = -1, verbose = 2)\n    \n    final_score = np.mean(scores)\n    \n    score_list.append(final_score)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results\nThe best performer in the 3-fold tests is RFR, so I use this going ahead.","metadata":{}},{"cell_type":"code","source":"model_name_list = ['Random Forest Regressor',\n          'Support Vector Machine - Regressor',\n          'Ridge Regressor',\n          'Linear Regression',\n          'Lasso Regression',\n          'ElasticNet Regression']\n\nresults = pd.DataFrame(\n    {'Model type': model_name_list,\n     'Mean Score (R^2)': score_list})\n\nresults","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fitting Random forest regressor\nI build the random forest regressor pipeline, with the predictor also transformed (and inverse transformed on completion), as this will improve output. See the above Yeo-Johnson graphs to see why.\n\nIn principle, normal distributions are favoured by models.","metadata":{}},{"cell_type":"code","source":"pipeline = Pipeline(steps=[('prep',col_transform), ('model', RandomForestRegressor())])\n\nmodel = TransformedTargetRegressor(regressor=pipeline, func=np.log1p, inverse_func=np.expm1)\n    \nmodel.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting the output\nI use the model to predict the output.","metadata":{}},{"cell_type":"code","source":"y_hat = model.predict(X_test)\ny_test = y_test.to_numpy()\n\nr2_score(y_hat, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Viewing the prediction\nClearly the model is performing very well in predicting prices.","metadata":{}},{"cell_type":"code","source":"dataset = pd.DataFrame({'prediction': list(y_hat), 'actual': list(y_test)})\ng = sns.jointplot(x=\"prediction\", y=\"actual\", data=dataset, kind='reg',\n                  joint_kws={'line_kws':{'color':'cyan'}})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extracting feature names\nThere is no direct output of feature names, therefore it needs to be coerced out from the fit object.","metadata":{}},{"cell_type":"code","source":"cat_feats = list(model.regressor_.steps[0][1].named_transformers_['cat'].get_feature_names())\nnumerical_feats = list(numerical_ix)\nfull_feature_list = cat_feats + numerical_feats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extracting importances from model\nThis will get an idea of which features were important as per the random forest model.","metadata":{}},{"cell_type":"code","source":"importances = model.regressor_.steps[1][1].feature_importances_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importance visual\nInterestingly, the most important feature is whether a vehicle is manual or not. The three numerical features are all important also.\n\nWhat is more interesting is that the model suggests other features are less relevant in pricing. For instance, the Make doesn't matter so much.\n\nDelving deeper into this, it is likely down to similar types of vehicles (irrespective of Make/model) having similar prices. Essentially the four chosen quantities are a \"catch-all\", and it also tells us that the fuel type (Manual or not) itself carries the most weight in pricing. For those that are NOT Manual, it doesn't really distinguish much between them.\n\nThe correlation matrix only shows relationships between numerical variables, however a cross-categorical analysis is relevant which is done above. This identifies the importance of fuel type.\n\nWith this, we can see it is worth dropping all categorical features, but fuel type, and having a boolean variable (\"Manual/Other\") in place.","metadata":{}},{"cell_type":"code","source":"importance_df = pd.DataFrame({'Features': full_feature_list, 'Importance': list(importances)}).sort_values(by = 'Importance', ascending = False)\nax = sns.barplot(x=\"Features\", y=\"Importance\", data=importance_df)\naxlab = ax.set_xticklabels(ax.get_xticklabels(), rotation=90)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Future work\n1. Optimise hyper-parameters of comparitive models.\n2. Try xgboost also to compare to Random Forest Regressor.\n3. See what removing features and creating Manual/Not-Manual grouping does to the $R^2$.","metadata":{}}]}