{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Preprocessing the Toxic comments\nPreprocess the data in 5 steps:\n1. Lower casing all text, \n2. Data cleaning by removing unnecessary characters such as re-tweet symbol (RT), username, URL, and punctuation\n3. Normalization using 'Alay' dictionary \n4. Stemming using PySastrawi [2]\n5. Stop words removal using list from [3]"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install PySastrawi","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n!ls '../input'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data2 = pd.read_csv('../input/aldon-data-unprocessed/data_train.csv', encoding='latin-1')\ndata_test = pd.read_csv('../input/aldon-data-unprocessed/data_test.csv', encoding='latin-1')\n\nalay_dict = pd.read_csv('../input/aldon-data-unprocessed/new_kamusalay.csv', encoding='latin-1', header=None)\nalay_dict = alay_dict.rename(columns={0: 'original', \n                                      1: 'replacement'})\n\nid_stopword_dict = pd.read_csv('../input/indonesian-stoplist/stopwordbahasa.csv', header=None)\nid_stopword_dict = id_stopword_dict.rename(columns={0: 'stopword'})\n\n#Delete unnamed columns\n#data = data.loc[:, ~data.columns.str.contains('^Unnamed')]\ndata2 = data2.loc[:, ~data2.columns.str.contains('^Unnamed')]\ndata_test = data_test.loc[:, ~data_test.columns.str.contains('^Unnamed')]\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Text Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape: \", data2.shape)\ndata2.head(15)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Test Shape: \", data_test.shape)\ndata_test.head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2.toxic.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Toxic shape: \", data2[(data2['toxic'] == 1)].shape)\nprint(\"Non-toxic shape: \", data2[(data2['toxic'] == 0) ].shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Alay Dict"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape: \", alay_dict.shape)\nalay_dict.head(15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ID Stopword"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape: \", id_stopword_dict.shape)\nid_stopword_dict.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocess"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom Sastrawi.Stemmer.StemmerFactory import StemmerFactory\nfactory = StemmerFactory()\nstemmer = factory.create_stemmer()\n\ndef lowercase(text):\n    return text.lower()\n\ndef remove_unnecessary_char(text):\n    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))',' ',text) # Remove every URL\n    text = re.sub('\\n',' ',text) # Remove every '\\n'\n    text = re.sub('\\r',' ',text) # Remove every '\\r'\n    text = re.sub('(?i)rt',' ',text) # Remove every retweet symbol\n    text = re.sub('@[^\\s]+[ \\t]','',text) # Remove every username\n    text = re.sub('(?i)user','',text) # Remove every username\n    text = re.sub('(?i)url',' ',text) # Remove every url\n    text = re.sub(r'\\\\x..',' ',text) # Remove every emoji\n    text = re.sub('  +', ' ', text) # Remove extra spaces\n    text = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', text) #Remove characters repeating more than twice \n\n    return text\n    \ndef remove_nonaplhanumeric(text):\n    text = re.sub('[^0-9a-zA-Z]+', ' ', text) \n    return text\n\nalay_dict_map = dict(zip(alay_dict['original'], alay_dict['replacement']))\ndef normalize_alay(text):\n    return ' '.join([alay_dict_map[word] if word in alay_dict_map else word for word in text.split(' ')])\n\ndef remove_stopword(text):\n    text = ' '.join(['' if word in id_stopword_dict.stopword.values else word for word in text.split(' ')])\n    text = re.sub('  +', ' ', text) # Remove extra spaces\n    text = text.strip()\n    return text\n\ndef stemming(text):\n    return stemmer.stem(text)\n\ndef preprocess(text):\n    text = remove_unnecessary_char(text) # 1\n    text = lowercase(text) # 2\n    text = remove_nonaplhanumeric(text) # 3\n    text = normalize_alay(text) # 4\n    text = stemming(text) # 5\n    text = remove_stopword(text) # 6\n    return text\n\ndef preprocess2(text):\n    text = remove_unnecessary_char(text) # 1\n    text = lowercase(text) # 2\n    text = remove_nonaplhanumeric(text) # 3\n    text = normalize_alay(text) # 4\n    return text\n\ndef preprocess_test(text):\n    text = remove_unnecessary_char(text) # 1\n    text = lowercase(text) # 2\n    text = remove_nonaplhanumeric(text) # 3\n    text = normalize_alay(text) # 4\n    return text\n\nprint(\"Text awal : RT Halooo,,,,, duniaa!!... Saaatnya menggambar mahatarii yang tenggelaaam... aamiin www.mataharitenggelam.com ðŸ\")\nprint(\"remove_nonaplhanumeric: \", remove_nonaplhanumeric(\"RT Halooo,,,,, duniaa!!... Saaatnya menggambar mahatarii yang tenggelaaam... aamiin www.mataharitenggelam.com ðŸ\"))\nprint(\"lowercase: \", lowercase(\"RT Halooo,,,,, duniaa!!... Saaatnya menggambar mahatarii yang tenggelaaam... aamiin www.mataharitenggelam.com ðŸ\"))\nprint(\"stemming: \", stemming(\"RT Halooo,,,,, duniaa!!... Saaatnya menggambar mahatarii yang tenggelaaam... aamiin www.mataharitenggelam.com ðŸ\"))\nprint(\"remove_unnecessary_char: \", remove_unnecessary_char(\"RT Halooo,,,,, duniaa!!... Saaatnya menggambar mahatarii yang tenggelaaam... aamiin www.mataharitenggelam.com ðŸ\"))\nprint(\"normalize_alay: \", normalize_alay(\"RT Halooo,,,,, duniaa!!... Saaatnya menggambar mahatarii yang tenggelaaam... aamiin www.mataharitenggelam.com ðŸ\"))\nprint(\"remove_stopword: \", remove_stopword(\" RT Halooo,,,,, duniaa!!... Saaatnya menggambar mahatarii yang tenggelaaam... aamiin www.mataharitenggelam.com ðŸ\"))\nprint(\"Hasil akhir : \" )\npreprocess(\"Text awal : RT Halooo,,,,, duniaa!!... Saaatnya menggaaambar mahatarii yang tenggelaaam... aamiin www.mataharitenggelam.com ðŸ\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocess_test(\"@usernamekl Bukti baru kerja paksa etnis Uighur di ladang kapas Xinjiang, China - BB... https://t.co/2cM3WZyOA2 via @YouTube\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2['comment_text'] = data2['comment_text'].apply(preprocess2)\ndata_test['comment_text'] = data_test['comment_text'].apply(preprocess_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape: \", data2.shape)\ndata2.head(15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Save Preprocessed Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape: \", data_test.shape)\ndata_test.head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2.to_csv('preprocessed_indonesian_toxic_tweet_nostemstop.csv', index=False)\ndata_test.to_csv('preprocessed_indonesian_toxic_tweet_nostemstop_test.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}