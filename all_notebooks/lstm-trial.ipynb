{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport tensorflow as tf\nimport numpy as np\nnp.set_printoptions(threshold=np.inf)\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\nimport math\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dense\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-06T13:32:53.807632Z","iopub.execute_input":"2021-08-06T13:32:53.80809Z","iopub.status.idle":"2021-08-06T13:33:00.4682Z","shell.execute_reply.started":"2021-08-06T13:32:53.808005Z","shell.execute_reply":"2021-08-06T13:33:00.467151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/bitcoin-historical-data/bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv\", sep = \",\")\ndf = df.dropna()\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:33:00.469799Z","iopub.execute_input":"2021-08-06T13:33:00.470199Z","iopub.status.idle":"2021-08-06T13:33:10.059197Z","shell.execute_reply.started":"2021-08-06T13:33:00.470164Z","shell.execute_reply":"2021-08-06T13:33:10.058252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], infer_datetime_format=True, unit=\"s\")\ndf = df.set_index(\"Timestamp\")\ndf = df.drop([\"Low\", \"High\", \"Volume_(BTC)\", \"Weighted_Price\"], axis=1)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:33:10.061269Z","iopub.execute_input":"2021-08-06T13:33:10.061676Z","iopub.status.idle":"2021-08-06T13:33:10.25987Z","shell.execute_reply.started":"2021-08-06T13:33:10.061631Z","shell.execute_reply":"2021-08-06T13:33:10.258723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.reindex(columns=[\"Open\", \"Close\", \"Volume_(Currency)\"])\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:33:10.261665Z","iopub.execute_input":"2021-08-06T13:33:10.262112Z","iopub.status.idle":"2021-08-06T13:33:10.310386Z","shell.execute_reply.started":"2021-08-06T13:33:10.262065Z","shell.execute_reply":"2021-08-06T13:33:10.309373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Open\"] = df[\"Open\"].resample(\"1H\").first()\ndf[\"Close\"] = df[\"Close\"].resample(\"1H\").last()\ndf[\"Volume_(Currency)\"] = df[\"Volume_(Currency)\"].resample(\"1H\").sum()\ndf = df.dropna()\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:33:10.312042Z","iopub.execute_input":"2021-08-06T13:33:10.312369Z","iopub.status.idle":"2021-08-06T13:33:10.945675Z","shell.execute_reply.started":"2021-08-06T13:33:10.312338Z","shell.execute_reply":"2021-08-06T13:33:10.944718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.iloc[-int((df.shape[0]/2)):]\nprint(df.head(10))","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:33:10.946871Z","iopub.execute_input":"2021-08-06T13:33:10.947168Z","iopub.status.idle":"2021-08-06T13:33:10.956974Z","shell.execute_reply.started":"2021-08-06T13:33:10.947138Z","shell.execute_reply":"2021-08-06T13:33:10.955882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = df.values\nprint(dataset[:10])\nprint(dataset.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:33:10.958716Z","iopub.execute_input":"2021-08-06T13:33:10.959079Z","iopub.status.idle":"2021-08-06T13:33:10.971064Z","shell.execute_reply.started":"2021-08-06T13:33:10.959048Z","shell.execute_reply":"2021-08-06T13:33:10.970042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean = np.mean(dataset, axis=0)\nstddev = np.std(dataset, axis=0)\ndataset = (dataset - mean) / stddev\nprint(dataset[:10])\nprint(dataset.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:33:10.973314Z","iopub.execute_input":"2021-08-06T13:33:10.973624Z","iopub.status.idle":"2021-08-06T13:33:10.985918Z","shell.execute_reply.started":"2021-08-06T13:33:10.973587Z","shell.execute_reply":"2021-08-06T13:33:10.984764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_sequence(sequence, n_steps):\n#function that splits a dataset sequence into input data and labels\n    X, Y = [], []\n    for i in range(sequence.shape[0]):\n        if (i + n_steps) >= sequence.shape[0]:\n            break\n      # Divide sequence between data (input) and labels (output)\n        seq_X, seq_Y = sequence[i: i + n_steps], sequence[i + n_steps, -2]\n        X.append(seq_X)\n        Y.append(seq_Y)\n    return np.array(X), np.array(Y)\n# Create training and validation datasets\ndataset_size = dataset.shape[0]\nx_train, y_train = split_sequence(dataset[0: math.ceil(0.7 * dataset_size)], 24)\nx_val, y_val = split_sequence(dataset[math.floor(0.7 * dataset_size):], 24)\nx_train.shape[-1]","metadata":{"execution":{"iopub.status.busy":"2021-08-06T13:45:28.215618Z","iopub.execute_input":"2021-08-06T13:45:28.216832Z","iopub.status.idle":"2021-08-06T13:45:28.32588Z","shell.execute_reply.started":"2021-08-06T13:45:28.216688Z","shell.execute_reply":"2021-08-06T13:45:28.32495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 256\nbuffer_size = x_train.shape[0]\n# Provide an infinite dataset\ntrain_iterator = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(buffer_size).batch(batch_size).repeat()\n# Provide an infinite dataset\nval_iterator = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size).repeat()\nn_steps = x_train.shape[-2]\nn_features = x_train.shape[-1]\n# Define the model\nmodel = Sequential()\nmodel.add(Bidirectional(LSTM(64, activation=\"relu\", input_shape=(n_steps, n_features))))\nmodel.add(Dense(1))\n# Compile the model\nmodel.compile(optimizer=\"adam\", loss=\"mse\")\nepochs = 1\nsteps_per_epoch = 800\nvalidation_steps = 80\n# Train with an infinite dataset\nhistory = model.fit(train_iterator, epochs = epochs, steps_per_epoch = steps_per_epoch, validation_data = val_iterator, validation_steps = validation_steps)\nprint('=======================')\nmodel.summary()","metadata":{},"execution_count":null,"outputs":[]}]}