{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Step 1: Importing required libraries","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:00:35.768841Z","iopub.execute_input":"2021-08-12T17:00:35.769569Z","iopub.status.idle":"2021-08-12T17:00:36.739443Z","shell.execute_reply.started":"2021-08-12T17:00:35.769466Z","shell.execute_reply":"2021-08-12T17:00:36.738544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 2: Importing the hotel booking data set","metadata":{}},{"cell_type":"code","source":"df= pd.read_csv('../input/hotel-booking/hotel_booking.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:00:37.738664Z","iopub.execute_input":"2021-08-12T17:00:37.739032Z","iopub.status.idle":"2021-08-12T17:00:38.719976Z","shell.execute_reply.started":"2021-08-12T17:00:37.738986Z","shell.execute_reply":"2021-08-12T17:00:38.719054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### This data set contains booking information for a city hotel and a resort hotel, and includes information such as when the booking was made, length of stay, the number of adults, children, and/or babies, and the number of available parking spaces, among other things.","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:00:38.72111Z","iopub.execute_input":"2021-08-12T17:00:38.721377Z","iopub.status.idle":"2021-08-12T17:00:38.827848Z","shell.execute_reply.started":"2021-08-12T17:00:38.721351Z","shell.execute_reply":"2021-08-12T17:00:38.826923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 3: Data Preparation","metadata":{}},{"cell_type":"markdown","source":"### Coping with missing values","metadata":{}},{"cell_type":"code","source":"#Calculating the percentage of missing data in each columns (feature) and then sort it\ndef missing_percentage(df):\n    nan_percent= 100*(df.isnull().sum()/len(df))\n    nan_percent= nan_percent[nan_percent>0].sort_values()\n    return nan_percent\nnan_percent= missing_percentage(df)\nprint(nan_percent)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:00:40.068474Z","iopub.execute_input":"2021-08-12T17:00:40.068818Z","iopub.status.idle":"2021-08-12T17:00:40.172244Z","shell.execute_reply.started":"2021-08-12T17:00:40.068788Z","shell.execute_reply":"2021-08-12T17:00:40.171111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.barplot(x=nan_percent.index, y=nan_percent, color=(0.2, 0.4, 0.6, 0.6), edgecolor='blue')\nplt.xticks(rotation=90)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:00:40.266995Z","iopub.execute_input":"2021-08-12T17:00:40.267447Z","iopub.status.idle":"2021-08-12T17:00:40.432794Z","shell.execute_reply.started":"2021-08-12T17:00:40.267415Z","shell.execute_reply":"2021-08-12T17:00:40.432132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### As indicated, more than 90% of data in company column are missing; so, we eliminate this feature.","metadata":{}},{"cell_type":"code","source":"df = df.drop(['company'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:00:41.021769Z","iopub.execute_input":"2021-08-12T17:00:41.022288Z","iopub.status.idle":"2021-08-12T17:00:41.055049Z","shell.execute_reply.started":"2021-08-12T17:00:41.022254Z","shell.execute_reply":"2021-08-12T17:00:41.054285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### It is likely that missing values in children column should be equal to zero, hence, we fill them by 0.","metadata":{}},{"cell_type":"code","source":"df[\"children\"]= df[\"children\"].fillna(0)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:00:41.556709Z","iopub.execute_input":"2021-08-12T17:00:41.557061Z","iopub.status.idle":"2021-08-12T17:00:41.563138Z","shell.execute_reply.started":"2021-08-12T17:00:41.557029Z","shell.execute_reply":"2021-08-12T17:00:41.562356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['agent'][df['agent'].isnull()]","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:00:41.893818Z","iopub.execute_input":"2021-08-12T17:00:41.894363Z","iopub.status.idle":"2021-08-12T17:00:41.903624Z","shell.execute_reply.started":"2021-08-12T17:00:41.894317Z","shell.execute_reply":"2021-08-12T17:00:41.90269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### It seems like that the missing values in agent should be equal to zero, therefore, we fill them by 0.","metadata":{}},{"cell_type":"code","source":"df[\"agent\"]= df[\"agent\"].fillna(0)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:00:42.755826Z","iopub.execute_input":"2021-08-12T17:00:42.756205Z","iopub.status.idle":"2021-08-12T17:00:42.761841Z","shell.execute_reply.started":"2021-08-12T17:00:42.756173Z","shell.execute_reply":"2021-08-12T17:00:42.76091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Here, we drop those columns that are string in nature","metadata":{}},{"cell_type":"code","source":"df_num= df.select_dtypes(exclude='object')","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:00:44.08163Z","iopub.execute_input":"2021-08-12T17:00:44.081971Z","iopub.status.idle":"2021-08-12T17:00:44.096473Z","shell.execute_reply.started":"2021-08-12T17:00:44.081941Z","shell.execute_reply":"2021-08-12T17:00:44.095473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_num.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:00:44.730737Z","iopub.execute_input":"2021-08-12T17:00:44.731187Z","iopub.status.idle":"2021-08-12T17:00:44.754678Z","shell.execute_reply.started":"2021-08-12T17:00:44.731139Z","shell.execute_reply":"2021-08-12T17:00:44.753342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### As demonstrated, there are no missing data.","metadata":{}},{"cell_type":"markdown","source":"## Step 4: Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"fig= plt.figure(figsize=(10,10), dpi=500)\nplt.rcParams['font.size'] = '8'\nsns.heatmap(df_num.corr(), annot=True, cmap=\"YlGnBu\", vmin=-1, vmax=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:01:24.512981Z","iopub.execute_input":"2021-08-12T17:01:24.513378Z","iopub.status.idle":"2021-08-12T17:01:28.430735Z","shell.execute_reply.started":"2021-08-12T17:01:24.513345Z","shell.execute_reply":"2021-08-12T17:01:28.429555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### In the above heatmap,\n\n#### *  -1 indicates a perfectly negative linear correlation between two variables;\n#### *  0 indicates no linear correlation between two variables;\n#### *  1 indicates a perfectly positive linear correlation between two variables.","metadata":{}},{"cell_type":"code","source":"corr_matrix = df_num.corr()\nprint(corr_matrix[\"is_canceled\"].sort_values(ascending=False))","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:01:36.856916Z","iopub.execute_input":"2021-08-12T17:01:36.857273Z","iopub.status.idle":"2021-08-12T17:01:36.996273Z","shell.execute_reply.started":"2021-08-12T17:01:36.857242Z","shell.execute_reply":"2021-08-12T17:01:36.995187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Next, we drop useless features (with less than 0.01 correlation)","metadata":{}},{"cell_type":"code","source":"df_num = df_num.drop(['arrival_date_day_of_month', 'stays_in_weekend_nights', 'children', 'arrival_date_week_number'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:01:39.073299Z","iopub.execute_input":"2021-08-12T17:01:39.073796Z","iopub.status.idle":"2021-08-12T17:01:39.081871Z","shell.execute_reply.started":"2021-08-12T17:01:39.073764Z","shell.execute_reply":"2021-08-12T17:01:39.081137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig= plt.figure(figsize=(6,6), dpi=300)\n\nsns.countplot(data=df_num, x='is_canceled')","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:01:52.298224Z","iopub.execute_input":"2021-08-12T17:01:52.298715Z","iopub.status.idle":"2021-08-12T17:01:52.543365Z","shell.execute_reply.started":"2021-08-12T17:01:52.298682Z","shell.execute_reply":"2021-08-12T17:01:52.542381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_num['is_canceled'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:01:53.982807Z","iopub.execute_input":"2021-08-12T17:01:53.983164Z","iopub.status.idle":"2021-08-12T17:01:53.992856Z","shell.execute_reply.started":"2021-08-12T17:01:53.98313Z","shell.execute_reply":"2021-08-12T17:01:53.991774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Looks like we have imbalanced data. To put it in other words, the number of individuals that have canceled is way more than those who have not canceled.","metadata":{}},{"cell_type":"markdown","source":"# Step 5: Determining X (Features) and y (Target Variable)","metadata":{}},{"cell_type":"code","source":"y = df_num['is_canceled']\nX = df_num.drop('is_canceled', axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:01:55.598171Z","iopub.execute_input":"2021-08-12T17:01:55.59851Z","iopub.status.idle":"2021-08-12T17:01:55.606668Z","shell.execute_reply.started":"2021-08-12T17:01:55.598481Z","shell.execute_reply":"2021-08-12T17:01:55.605339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 6: Spliting Train and Test","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=101,test_size=0.3)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:01:56.918356Z","iopub.execute_input":"2021-08-12T17:01:56.918723Z","iopub.status.idle":"2021-08-12T17:01:57.112367Z","shell.execute_reply.started":"2021-08-12T17:01:56.918694Z","shell.execute_reply":"2021-08-12T17:01:57.11146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 7: Coping with imbalanced data applying SMOTE\n\n#### Imbalanced data profoundly affects our results. Thus, we employ SMOTE (synthetic minority oversampling technique) in order to cope with imbalanced data. SMOTE (Synthetic Minority Over-sampling Technique) is an over-sampling approach in which the minority class is over-sampled by creating “synthetic” examples <span style=\"color:crimson;\"> (Chawla et al., 2002).\n#### The SMOTE algorithm is a popular approach for oversampling the minority class. This technique can be used to reduce the imbalance or to make the class distribution even. This can be achieved by simply duplicating examples in the minority class, but these examples do not add any new information. Instead, new examples from the minority can be synthesized using existing examples in the training dataset. These new examples will be “close” to existing examples in the feature space, but different in small but random ways <span style=\"color:crimson;\">(Brownlee, 2021).\n\n### References\n\n#### * Chawla, N.V., Bowyer, K.W., Hall, L.O., Kegelmeyer, W.P. (2002). SMOTE: synthetic minority over-sampling technique. Journal of artificial intelligence research, 16, 321-357.\n#### * Brownlee, J. (2021). Imbalanced Classification With Python. https://machinelearningmastery.com/imbalanced-classification-with-python-7-day-mini-course/ (Accessed 12 August 2021)","metadata":{}},{"cell_type":"code","source":"# pip install imbalanced-learn\nfrom imblearn.over_sampling import SMOTE\nsmote = SMOTE(random_state=101, sampling_strategy='minority')\nX_sm, y_sm = smote.fit_resample(X, y)\ny_sm.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:01:57.675225Z","iopub.execute_input":"2021-08-12T17:01:57.67556Z","iopub.status.idle":"2021-08-12T17:01:58.846206Z","shell.execute_reply.started":"2021-08-12T17:01:57.675532Z","shell.execute_reply":"2021-08-12T17:01:58.845112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_sm, X_test_sm, y_train_sm, y_test_sm = train_test_split(X_sm, y_sm)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:01:58.847775Z","iopub.execute_input":"2021-08-12T17:01:58.848198Z","iopub.status.idle":"2021-08-12T17:01:58.882008Z","shell.execute_reply.started":"2021-08-12T17:01:58.848152Z","shell.execute_reply":"2021-08-12T17:01:58.881159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 8: Classification Machine Learning Algorithms","metadata":{}},{"cell_type":"markdown","source":"## 1. Decision Tree Algorithm\n\n#### A classification tree is used to predict a qualitative response rather than a quantitative one <span style=\"color:crimson;\">(James et al., 2021).\n\n### References\n\n#### * James, G., Witten, D., Hastie, T., Tibshirani, R. (2021). An introduction to statistical learning, 2nd Edition. New York: springer.","metadata":{}},{"cell_type":"markdown","source":"### Regular Decision Tree Algorithm ","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nModel_DecisionTree = DecisionTreeClassifier()\nModel_DecisionTree.fit(X_train, y_train)\ny_pred = Model_DecisionTree.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix,classification_report\nprint(classification_report(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:02:03.326106Z","iopub.execute_input":"2021-08-12T17:02:03.326593Z","iopub.status.idle":"2021-08-12T17:02:03.822371Z","shell.execute_reply.started":"2021-08-12T17:02:03.326561Z","shell.execute_reply":"2021-08-12T17:02:03.821183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Decision Tree Algorithm using SMOTE","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nModel_DecisionTree = DecisionTreeClassifier()\nModel_DecisionTree.fit(X_train_sm, y_train_sm)\nSMOTE_y_preds = Model_DecisionTree.predict(X_test_sm)\n\nfrom sklearn.metrics import confusion_matrix,classification_report\nprint(classification_report(y_test_sm,SMOTE_y_preds))","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:02:05.594117Z","iopub.execute_input":"2021-08-12T17:02:05.594476Z","iopub.status.idle":"2021-08-12T17:02:06.266241Z","shell.execute_reply.started":"2021-08-12T17:02:05.594444Z","shell.execute_reply":"2021-08-12T17:02:06.26535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import plot_tree\npruned_tree = DecisionTreeClassifier(max_leaf_nodes=7)\npruned_tree.fit(X_train_sm,y_train_sm)\nplt.figure(figsize=(6,10),dpi=200)\nplot_tree(pruned_tree,filled=True,feature_names=X.columns);","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:02:06.314443Z","iopub.execute_input":"2021-08-12T17:02:06.314766Z","iopub.status.idle":"2021-08-12T17:02:07.404525Z","shell.execute_reply.started":"2021-08-12T17:02:06.314738Z","shell.execute_reply":"2021-08-12T17:02:07.403582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. K Nearest Neighbors Algorithm\n\n#### K nearset neighbors (KNN) assigns a label to new data according to the distance between the old data and the new data.\n\n####  $Pr(Y=j|X=x_0) = 1/K \\times \\sum_{i \\in N_0} I(y_i = j)$\n\n#### *Given the fact that feature scaling is a compulsory task in the KNN algorithm, we do feature scaling before training the model.*","metadata":{}},{"cell_type":"markdown","source":"### Regular K Nearest Neighbors Algorithm","metadata":{}},{"cell_type":"code","source":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nscaler= StandardScaler()\nscaler.fit(X_train)\nscaled_X_train= scaler.transform(X_train)\nscaled_X_test= scaler.transform(X_test)\n\n# Training the model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_model= KNeighborsClassifier(n_neighbors=1)\nknn_model.fit(scaled_X_train, y_train)\ny_pred= knn_model.predict(scaled_X_test)\n\nfrom sklearn.metrics import confusion_matrix,classification_report\nprint(classification_report(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:02:09.651082Z","iopub.execute_input":"2021-08-12T17:02:09.651437Z","iopub.status.idle":"2021-08-12T17:02:29.919173Z","shell.execute_reply.started":"2021-08-12T17:02:09.651405Z","shell.execute_reply":"2021-08-12T17:02:29.918199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### K Nearest Neighbors Algorithm using SMOTE","metadata":{}},{"cell_type":"code","source":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nscaler= StandardScaler()\nscaler.fit(X_train_sm)\nscaled_X_train_sm= scaler.transform(X_train_sm)\nscaled_X_test_sm= scaler.transform(X_test_sm)\n\n# Training the model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_model= KNeighborsClassifier(n_neighbors=1)\nknn_model.fit(scaled_X_train_sm, y_train_sm)\ny_pred_sm= knn_model.predict(scaled_X_test_sm)\n\nfrom sklearn.metrics import confusion_matrix,classification_report\nprint(classification_report(y_test_sm,y_pred_sm))","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:02:38.080275Z","iopub.execute_input":"2021-08-12T17:02:38.080614Z","iopub.status.idle":"2021-08-12T17:04:04.440506Z","shell.execute_reply.started":"2021-08-12T17:02:38.080586Z","shell.execute_reply":"2021-08-12T17:04:04.439739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Logistic Regression Algorithm\n\n#### Logistic Regression transforms a Linear Regression into classification model using the below equation:\n\n#### $\\sigma (x) = 1/(1 + e^{-x})$\n\n#### Hence, the output always lays between 0 and 1.","metadata":{}},{"cell_type":"markdown","source":"### Regular Logistic Regression Algorithm","metadata":{}},{"cell_type":"code","source":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nscaler= StandardScaler()\nscaler.fit(X_train)\nscaled_X_train= scaler.transform(X_train)\nscaled_X_test= scaler.transform(X_test)\n\n# Training the model\nfrom sklearn.linear_model import LogisticRegression\nlog_model= LogisticRegression()\nlog_model.fit(scaled_X_train, y_train)\ny_pred= log_model.predict(scaled_X_test)\n\nfrom sklearn.metrics import confusion_matrix,classification_report\nprint(classification_report(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:04:07.471113Z","iopub.execute_input":"2021-08-12T17:04:07.471462Z","iopub.status.idle":"2021-08-12T17:04:08.465706Z","shell.execute_reply.started":"2021-08-12T17:04:07.471431Z","shell.execute_reply":"2021-08-12T17:04:08.464446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Discussion\n    \n#### As shown, we compared five different classification machine learning algorithms, i.e., Decision Tree, Decision Tree using SMOTE, K Nearest Neighbors, K Nearest Neighbors using SMOTE, and Logistic Regression, in order to predit cancelation. Eventually, results revealed that the Decision Tree using SMOTE outperformes other classification machine learning algorithms. According to outcomes, the Decision Tree using SMOTE brings about an accuracy of 0.84 for cancelation forecasting.    ","metadata":{}},{"cell_type":"code","source":"# creating the dataset\nfig= plt.figure(figsize=(4,2), dpi=250)\nplt.rcParams['font.size'] = '4'\ndata = {'Decision Tree Algorithm':0.82, 'Decision Tree Algorithm using SMOTE':0.84, 'K Nearest Neighbors':0.80,\n        'K Nearest Neighbors using SMOTE':0.82, 'Logistic Regression':0.74}\nAlgorithms= list(data.keys())\nAccuracy= list(data.values())\n\n \n# creating the bar plot\nplt.bar(Algorithms, Accuracy, color ='maroon',\n        width = 0.7)\n \nplt.xlabel(\"Classification machine learning algorithms\")\nplt.ylabel(\"Accuracy\")\nplt.xticks(rotation=90)\nplt.title(\"A comparison among various classification machine learning algorithms\")\n","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:04:12.139828Z","iopub.execute_input":"2021-08-12T17:04:12.140237Z","iopub.status.idle":"2021-08-12T17:04:12.362196Z","shell.execute_reply.started":"2021-08-12T17:04:12.140206Z","shell.execute_reply":"2021-08-12T17:04:12.361221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Results = {'Accuracy':[0.82, 0.84, 0.80, 0.82, 0.74]}  \nDataframe_of_results = pd.DataFrame(Results, index =['Decision Tree Algorithm', 'Decision Tree Algorithm using SMOTE', 'K Nearest Neighbors', 'K Nearest Neighbors using SMOTE', 'Logistic Regression'])\nDataframe_of_results.sort_values('Accuracy', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T17:04:14.001928Z","iopub.execute_input":"2021-08-12T17:04:14.002298Z","iopub.status.idle":"2021-08-12T17:04:14.02011Z","shell.execute_reply.started":"2021-08-12T17:04:14.002266Z","shell.execute_reply":"2021-08-12T17:04:14.018841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}