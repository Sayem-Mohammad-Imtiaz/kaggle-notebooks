{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nimport pydot\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom numpy.random import seed\nfrom sklearn import metrics\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD, NMF\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation,Dropout \nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.random import set_seed\npd.set_option('display.max_columns', None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/should-this-loan-be-approved-or-denied/SBAnational.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I think it will be interesting to predict MIS_Status value (Loan status charged off = CHGOFF, Paid in full = PIF)"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(data[data['MIS_Status'].isnull()].index, axis = 0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splits"},{"metadata":{"trusted":true},"cell_type":"code","source":"df, validation_df  = train_test_split(data,\n                                test_size=0.35,\n                                random_state = 101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train, df_test  = train_test_split(df,\n                                test_size=0.25,\n                                random_state = 101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"N\\A values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.countplot(x=\"NewExist\", hue=\"MIS_Status\", data=df_train)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.countplot(x=\"RevLineCr\", hue=\"MIS_Status\", data=df_train)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.countplot(x=\"LowDoc\", hue=\"MIS_Status\", data=df_train)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df_train, hue='MIS_Status', height = 7, aspect = 2)\ng.map(sns.kdeplot, 'Term')\nplt.legend()\nplt.title('Term factor')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12, 8))\njob_survey_data = df_train[['CreateJob', 'RetainedJob', 'MIS_Status']]\njob_survey_data[['CreateJob', 'RetainedJob']] = np.sqrt(job_survey_data[['CreateJob', 'RetainedJob']])\nsns.scatterplot(data = job_survey_data, x = 'CreateJob', y = 'RetainedJob', hue = 'MIS_Status', palette = 'magma')\nplt.show()\ndel job_survey_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = df_train['City'].value_counts()\nprint(f'Unique values: {len(count)}')\ncount_f = count[count>500]\nmore_popular_Cities = set(count_f.index)\nprint(f'Unique values after values grouped: {len(count_f)}')\ncount_f","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = df_train['Bank'].value_counts()\nprint(f'Unique values: {len(count)}')\ncount_f = count[count>500]\nmore_popular_Banks = set(count_f.index)\nprint(f'Unique values after values grouped: {len(count_f)}')\ncount_f","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['ChgOffDate'].isnull().value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like this column is better to ignore at all"},{"metadata":{"trusted":true},"cell_type":"code","source":"count = df_train['NAICS'].value_counts()\nprint(f'Unique values: {len(count)}')\ncount_f = count[count>500]\nmore_popular_NAICS = set(count_f.index)\nprint(f'Unique values after values grouped: {len(count_f)}')\ncount_f","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def unknown_filling_text(val):\n    if pd.isna(val):\n        return 'no data'\n    else:\n        return str(val)\n    \ndef proc_col_City(val):\n    if val not in more_popular_Cities:\n        return 'other'\n    else:\n        return val\n    \ndef proc_col_Bank(val):\n    if val not in more_popular_Banks:\n        return 'other'\n    else:\n        return val\n    \ndef proc_col_NAICS(val):\n    if val not in more_popular_NAICS:\n        return str(val)[:3]\n    else:\n        return str(val)\n\ndef proc_col_MIS_Status(val):\n    if val == 'CHGOFF':\n        return 1\n    elif val == 'P I F':\n        return 0\n    else:\n        raise ValueError('Incorrect MIS_Status value')\n    \ndef check_na(df):\n    if len(df[df.isnull().any(axis=1)])!= 0:\n        raise ValueError('N\\A in data')\n    \ndef pre_dumm_proc(df):\n    df = df.copy()\n    to_drop = [\n        'LoanNr_ChkDgt', 'ChgOffDate', 'Name', 'Zip', 'ApprovalDate',\n        'ApprovalFY', 'DisbursementDate', 'DisbursementGross',\n        'BalanceGross', 'ChgOffPrinGr', 'GrAppv', 'SBA_Appv'\n    ]\n    df.drop(columns = to_drop, axis = 1, inplace = True)\n    \n    df['City'] = df['City'].apply(proc_col_City)\n    df['State'] = df['State'].apply(unknown_filling_text)\n    df['Bank'] = df['Bank'].apply(proc_col_Bank)\n    df['BankState'] = df['BankState'].apply(unknown_filling_text)\n    df['NAICS'] = df['NAICS'].apply(proc_col_NAICS)\n    df['NewExist'] = df['NewExist'].apply(unknown_filling_text)\n    df['RevLineCr'] = df['RevLineCr'].apply(unknown_filling_text)\n    df['LowDoc'] = df['LowDoc'].apply(unknown_filling_text)\n    df['MIS_Status'] = df['MIS_Status'].apply(proc_col_MIS_Status)\n    check_na(df)\n    return df\n\ndef dummification(df):\n    dummy_df = pd.DataFrame()\n    object_cols = df.columns[df.dtypes == object]\n    for col in object_cols:\n        dummy_df = pd.concat([dummy_df, create_dummy(col, df)], axis = 1)\n    \n    df_out = pd.concat([df.drop(columns = object_cols), dummy_df], axis = 1)\n    return df_out.sort_index(ascending=False, axis=1)\n        \n\ndef create_dummy(col, df):\n    df_dummy = pd.get_dummies(df[col], drop_first = True)\n    df_dummy.columns = ['dum: ' + col + ': ' + str(name) for name in df_dummy.columns]\n    return df_dummy\n\ndef data_preparation(df):\n    return dummification(pre_dumm_proc(df))\n\ndf_train_d = data_preparation(df_train)\ndf_train_c = pre_dumm_proc(df_train)\ndf_train_d","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_c","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_needed = set(df_train_d.columns)\n\ndef columns_standardization(df):\n    df = df.copy()\n    for col in columns_needed:\n        if col not in set(df.columns):\n            df.insert(loc = len(df.columns), column = col, value = 0, allow_duplicates=False)\n    \n    for col in set(df.columns):\n        if col not in columns_needed:\n            df.drop(columns = col, axis = 1, inplace = True)\n    \n    return df.sort_index(ascending=False, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_d = columns_standardization(data_preparation(df_test))\ndf_test_c = pre_dumm_proc(df_test)\ndf_test_d","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_c.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_d[df_test_d.isnull().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_d[df_train_d.isnull().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No null value left"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = df_train_d.drop('MIS_Status', axis = 1)\ny_train = df_train_d['MIS_Status']\nX_test = df_test_d.drop('MIS_Status', axis = 1)\ny_test = df_test_d['MIS_Status']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del df, df_train_d, df_train_c, df_test_d, df_test_c","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler()\nscaler.fit(X_train)\nX_train_sc = scaler.transform(X_train.values)\nX_test_sc = scaler.transform(X_test.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dimensionality reduction using PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=2, random_state = 1)\ndf_pca_train = pca.fit_transform(X_train_sc)\ndf_pca_test = pca.transform(X_test_sc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pca_vis = pd.DataFrame(df_pca_train)\ndf_pca_vis['y'] = y_train.values\n\nplt.figure(figsize = (12, 8))\nsns.scatterplot(data = df_pca_vis, x = 0, y = 1, hue = 'y',  palette = 'magma')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_variance = pca.explained_variance_\n\nplt.figure(figsize=(6, 6))\nplt.bar(['0', '1'], pca_variance, align='center', label='individual variance')\nplt.legend()\nplt.ylabel('Variance ratio')\nplt.xlabel('Principal components')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbr_pca = LGBMClassifier(random_state = 1, n_jobs=- 1)\nlgbr_pca.fit(df_pca_train, y_train)\npred = lgbr_pca.predict(df_pca_test)\nprint(classification_report(y_test, pred))\nprint(confusion_matrix(y_test, pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dimensionality reduction using Singular Value Decomposition"},{"metadata":{"trusted":true},"cell_type":"code","source":"svd = TruncatedSVD(n_components=2, random_state = 1)\ndf_svd_train = svd.fit_transform(X_train_sc)\ndf_svd_test = svd.transform(X_test_sc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_svd_vis = pd.DataFrame(df_svd_train)\ndf_svd_vis['y'] = y_train.values\n\nplt.figure(figsize = (12, 8))\nsns.scatterplot(data = df_svd_vis, x = 0, y = 1, hue = 'y',  palette = 'magma')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbr_svd = LGBMClassifier(random_state = 1, n_jobs=- 1)\nlgbr_svd.fit(df_svd_train, y_train)\npred = lgbr_svd.predict(df_svd_test)\nprint(classification_report(y_test, pred))\nprint(confusion_matrix(y_test, pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dimensionality reduction using Non-Negative Matrix Factorization (NMF)"},{"metadata":{"trusted":true},"cell_type":"code","source":"nmf = NMF(n_components=2, random_state = 1)\ndf_nmf_train = nmf.fit_transform(X_train_sc, y_train)\ndf_nmf_test = nmf.transform(X_test_sc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_nmf_vis = pd.DataFrame(df_nmf_train)\ndf_nmf_vis['y'] = y_train.values\n\nplt.figure(figsize = (12, 8))\nsns.scatterplot(data = df_nmf_vis, x = 0, y = 1, hue = 'y',  palette = 'magma')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbr_nmf = LGBMClassifier(random_state = 1, n_jobs=- 1)\nlgbr_nmf.fit(df_nmf_train, y_train)\npred = lgbr_nmf.predict(df_nmf_test)\nprint(classification_report(y_test, pred))\nprint(confusion_matrix(y_test, pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dimensionality reduction using Linear Discriminant Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"lda = LinearDiscriminantAnalysis(n_components=1)\ndf_lda_train = lda.fit_transform(X_train_sc, y_train)\ndf_lda_test = lda.transform(X_test_sc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_lda_vis = pd.DataFrame(df_lda_train)\ndf_lda_vis['y'] = y_train.values\n\nplt.figure(figsize = (12, 8))\nsns.scatterplot(data = df_lda_vis, x = 0, y = 1, hue = 'y',  palette = 'magma')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbr_lda = LGBMClassifier(random_state = 1, n_jobs=- 1)\nlgbr_lda.fit(df_lda_train, y_train)\npred = lgbr_lda.predict(df_lda_test)\nprint(classification_report(y_test, pred))\nprint(confusion_matrix(y_test, pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dimensionality reduction using autoencoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_width = len(X_train.columns)\ninput_width","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dim_red_analysis(n_epochs = None):\n    seed(101)\n    set_seed(101)\n\n    encoder = Sequential()\n    encoder.add(Dense(units = 256, activation = 'relu', input_shape = [input_width]))\n    encoder.add(Dropout(0.2))\n    encoder.add(Dense(units = 16, activation = 'relu'))\n    encoder.add(Dense(units = 2, activation = 'relu'))\n\n    decoder = Sequential()\n    decoder.add(Dense(units = 16, activation = 'relu', input_shape = [2]))\n    decoder.add(Dense(units = 256, activation = 'relu'))\n    decoder.add(Dense(units = input_width, activation = 'relu'))\n\n    autoencoder = Sequential([encoder, decoder])\n\n    autoencoder.compile(loss = 'mse', optimizer = SGD(lr = 12))\n    \n    autoencoder.summary()\n    \n    if n_epochs is None:\n        es = [EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)]\n        n_epochs = 100\n    else:\n        es = []\n    \n    autoencoder.fit(\n            X_train_sc,\n            X_train_sc,\n            epochs = n_epochs,\n            validation_data=(X_test_sc, X_test_sc), \n            callbacks=[es]\n             )\n    \n    if n_epochs > 1:\n        histo = pd.DataFrame(autoencoder.history.history)\n        for metric in ['loss', 'val_loss']:\n            plt.title(metric)\n            histo[metric].plot()\n            plt.show()\n        \n    encoded_2dim = encoder.predict(X_train_sc)\n    encoded_2dim = pd.DataFrame(encoded_2dim)\n    encoded_2dim['y'] = y_train.values\n\n    plt.figure(figsize = (12, 8))\n    sns.scatterplot(data = encoded_2dim, x = 0, y = 1, hue = 'y', palette = 'magma')\n    plt.show()\n    \n    return encoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = dim_red_analysis(0)\n\nenc_train = encoder.predict(X_train_sc)\nenc_test = encoder.predict(X_test_sc)\n\nlgbr_enc = LGBMClassifier(random_state = 1, n_jobs=- 1)\nlgbr_enc.fit(enc_train, y_train)\npred = lgbr_enc.predict(enc_test)\nprint(classification_report(y_test, pred))\nprint(confusion_matrix(y_test, pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = dim_red_analysis(1)\n\nenc_train = encoder.predict(X_train_sc)\nenc_test = encoder.predict(X_test_sc)\n\nlgbr_enc = LGBMClassifier(random_state = 1, n_jobs=- 1)\nlgbr_enc.fit(enc_train, y_train)\npred = lgbr_enc.predict(enc_test)\nprint(classification_report(y_test, pred))\nprint(confusion_matrix(y_test, pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = dim_red_analysis(2)\n\nenc_train = encoder.predict(X_train_sc)\nenc_test = encoder.predict(X_test_sc)\n\nlgbr_enc = LGBMClassifier(random_state = 1, n_jobs=- 1)\nlgbr_enc.fit(enc_train, y_train)\npred = lgbr_enc.predict(enc_test)\nprint(classification_report(y_test, pred))\nprint(confusion_matrix(y_test, pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = dim_red_analysis(5)\n\nenc_train = encoder.predict(X_train_sc)\nenc_test = encoder.predict(X_test_sc)\n\nlgbr_enc = LGBMClassifier(random_state = 1, n_jobs=- 1)\nlgbr_enc.fit(enc_train, y_train)\npred = lgbr_enc.predict(enc_test)\nprint(classification_report(y_test, pred))\nprint(confusion_matrix(y_test, pred))\ndel encoder, enc_train, enc_test, lgbr_enc, pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = dim_red_analysis()\n\nenc_train = encoder.predict(X_train_sc)\nenc_test = encoder.predict(X_test_sc)\n\nlgbr_enc = LGBMClassifier(random_state = 1, n_jobs=- 1)\nlgbr_enc.fit(enc_train, y_train)\npred = lgbr_enc.predict(enc_test)\nprint(classification_report(y_test, pred))\nprint(confusion_matrix(y_test, pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling without DR"},{"metadata":{"trusted":true},"cell_type":"code","source":"dtc = DecisionTreeClassifier(random_state = 101)\ndtc.fit(X_train, y_train)\npred = dtc.predict(X_test)\nprint(classification_report(y_test, pred))\nprint(confusion_matrix(y_test, pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(random_state = 101, n_jobs = -1)\nrfc.fit(X_train, y_train)\npred = rfc.predict(X_test)\nprint(classification_report(y_test, pred))\nprint(confusion_matrix(y_test, pred))\ndisplay(pd.DataFrame({'Variable':X_train.columns,\n              'Importance':rfc.feature_importances_}).sort_values('Importance', ascending=False).head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbr = GradientBoostingClassifier(random_state = 101)\ngbr.fit(X_train, y_train)\npred = gbr.predict(X_test)\nprint(classification_report(y_test, pred))\nprint(confusion_matrix(y_test, pred))\ndisplay(pd.DataFrame({'Variable':X_train.columns,\n              'Importance':gbr.feature_importances_}).sort_values('Importance', ascending=False).head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbr = LGBMClassifier(random_state = 1, n_jobs=- 1)\nlgbr.fit(X_train.values, y_train)\npred = lgbr.predict(X_test.values)\nprint(classification_report(y_test, pred))\nprint(confusion_matrix(y_test, pred))\ndisplay(pd.DataFrame({'Variable':X_train.columns,\n              'Importance':lgbr.feature_importances_}).sort_values('Importance', ascending=False).head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgbr = XGBClassifier(random_state = 1, n_jobs=- 1)\nxgbr.fit(X_train, y_train)\npred = xgbr.predict(X_test)\nprint(classification_report(y_test, pred))\nprint(confusion_matrix(y_test, pred))\ndisplay(pd.DataFrame({'Variable':X_train.columns,\n              'Importance':xgbr.feature_importances_}).sort_values('Importance', ascending=False).head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ANN_model_classification(model, X_train_sc, y_train, X_test_sc, y_test):\n    \n    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n    \n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    \n    model.fit(\n        x = X_train_sc,\n        y = y_train,\n        epochs = 100,\n        validation_data=(X_test_sc, y_test), \n        batch_size = 128,\n        callbacks=[es]\n             )\n\n\n    histo = pd.DataFrame(model.history.history)\n    \n    for metric in ['loss', 'val_loss', 'accuracy', 'val_accuracy']:\n        plt.title(metric)\n        histo[metric].plot()\n        plt.show()\n    \n    pred_test_values = model.predict_classes(X_test_sc)\n\n    print('test')\n    print(classification_report(y_test,pred_test_values))\n    print(confusion_matrix(y_test,pred_test_values))\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed(101)\nset_seed(101)\n\nann_model1 = Sequential()\n\nann_model1.add(Dense(units=128, activation = 'relu'))\nann_model1.add(Dropout(0.3))\nann_model1.add(Dense(units=1,activation='sigmoid'))\nann_model1 = ANN_model_classification(ann_model1, X_train_sc, y_train, X_test_sc, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed(101)\nset_seed(101)\n\nann_model2 = Sequential()\n\nann_model2.add(Dense(units=128, activation = 'relu'))\nann_model2.add(Dropout(0.3))\nann_model2.add(Dense(units=16, activation = 'relu'))\nann_model2.add(Dense(units=1,activation='sigmoid'))\nann_model2 = ANN_model_classification(ann_model2, X_train_sc, y_train, X_test_sc, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like XGB model performs better then other."},{"metadata":{},"cell_type":"markdown","source":"# Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"val_data_for_model = columns_standardization(data_preparation(validation_df))\nval_data_for_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_data_for_model[val_data_for_model.isnull().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_validation = val_data_for_model.drop('MIS_Status', axis = 1)\ny_validation = val_data_for_model['MIS_Status']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = xgbr.predict(X_validation)\nprint(classification_report(y_validation, pred))\nprint(confusion_matrix(y_validation, pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model showed quite good result on validation data"},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_validation,pred)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}