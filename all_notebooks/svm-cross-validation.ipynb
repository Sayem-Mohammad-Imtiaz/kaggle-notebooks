{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### <h3>Attribute Information</h3>\n\n- Age: age of the patient: years\n- Sex: sex of the patient \n  - M: Male\n  - F: Female\n- ChestPainType: chest pain type \n  - TA: Typical Angina, \n  - ATA: Atypical Angina \n  - NAP: Non-Anginal Pain\n  - ASY: Asymptomatic\n- RestingBP: resting blood pressure: mm Hg\n- Cholesterol: serum cholesterol: mm/dl\n- FastingBS: fasting blood sugar \n  - 1: if FastingBS > 120 mg/dl\n  - 0: otherwise\n- RestingECG: resting electrocardiogram results \n  - Normal: Normal\n  - ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or - depression of > 0.05 mV)\n  - LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria\n- MaxHR: maximum heart rate achieved: Numeric value between 60 and 202\n- ExerciseAngina: exercise-induced angina \n  - Y: Yes\n  - N: No\n- Oldpeak: oldpeak ST: Numeric value measured in depression\n- ST_Slope: the slope of the peak exercise ST segment \n  - Up: upsloping \n  - Flat: flat\n  - Down: downsloping\n- HeartDisease: output class \n  - 1: heart disease\n  - 0: Normal","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix\nfrom sklearn.metrics import r2_score, roc_auc_score, roc_curve, classification_report\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import KFold\n\nDATASET_FILE_PATH = '/kaggle/input/heart-failure-prediction/heart.csv'\n","metadata":{"execution":{"iopub.status.busy":"2021-09-14T06:06:45.487286Z","iopub.execute_input":"2021-09-14T06:06:45.488205Z","iopub.status.idle":"2021-09-14T06:06:46.670743Z","shell.execute_reply.started":"2021-09-14T06:06:45.488143Z","shell.execute_reply":"2021-09-14T06:06:46.669838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(DATASET_FILE_PATH)\nprint(df.head(10))","metadata":{"execution":{"iopub.status.busy":"2021-09-14T06:06:47.556169Z","iopub.execute_input":"2021-09-14T06:06:47.556458Z","iopub.status.idle":"2021-09-14T06:06:47.590954Z","shell.execute_reply.started":"2021-09-14T06:06:47.556426Z","shell.execute_reply":"2021-09-14T06:06:47.589638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transform non-numerical labels to numerical labels\ncategory_encoders = {}\nstring_categories = [\"Sex\", \"ChestPainType\",\n                     \"RestingECG\", \"ExerciseAngina\", \"ST_Slope\"]\n\nfor category in string_categories:\n    category_encoders[category] = LabelEncoder()\n    df[f\"{category}_encoded\"] = category_encoders[category].fit_transform(\n        df[category])\n\n# rename output (HeartDisease) column to target\ndf = df.rename(columns={\"HeartDisease\": \"target\"})\n\n# Drop columns which contain non-numerical labels\ndf_processed = df.drop(string_categories, axis=1)\nprint(df_processed.head(10))\n","metadata":{"execution":{"iopub.status.busy":"2021-09-14T06:06:50.271452Z","iopub.execute_input":"2021-09-14T06:06:50.272547Z","iopub.status.idle":"2021-09-14T06:06:50.303723Z","shell.execute_reply.started":"2021-09-14T06:06:50.272502Z","shell.execute_reply":"2021-09-14T06:06:50.302795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = df_processed.corr()\nax, fig = plt.subplots(figsize=(15, 15))\nsns.heatmap(corr, vmin=-1, cmap=plt.cm.Blues, annot=True)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-09-14T06:06:53.374053Z","iopub.execute_input":"2021-09-14T06:06:53.374407Z","iopub.status.idle":"2021-09-14T06:06:54.587272Z","shell.execute_reply.started":"2021-09-14T06:06:53.37437Z","shell.execute_reply":"2021-09-14T06:06:54.586628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr[abs(corr['target']) < 0.3]['target']\n","metadata":{"execution":{"iopub.status.busy":"2021-09-14T06:06:56.657621Z","iopub.execute_input":"2021-09-14T06:06:56.658774Z","iopub.status.idle":"2021-09-14T06:06:56.669519Z","shell.execute_reply.started":"2021-09-14T06:06:56.658724Z","shell.execute_reply":"2021-09-14T06:06:56.668451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split dataset into X and Y\ndf_x = df_processed.iloc[:, df_processed.columns != 'target']\ndf_y = df_processed['target']\n\n# Standardize features\nscaler = StandardScaler()\ndf_x = scaler.fit_transform(df_x)\n\n# Split dataset into train and test\nX_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.2)\n\nprint(f\"X_train shape {X_train.shape}\")\nprint(f\"Y_train shape {y_train.shape}\")\nprint(f\"X_test shape {X_test.shape}\")\nprint(f\"Y_test shape {y_test.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2021-09-14T06:06:58.616157Z","iopub.execute_input":"2021-09-14T06:06:58.616506Z","iopub.status.idle":"2021-09-14T06:06:58.636399Z","shell.execute_reply.started":"2021-09-14T06:06:58.61646Z","shell.execute_reply":"2021-09-14T06:06:58.635404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SVC(tol=1e-4, verbose=1, max_iter=2500).fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nprint('Accuracy Score: {:.4f}'.format(accuracy_score(y_test, y_pred)))\nprint('SVC f1-score  : {:.4f}'.format(f1_score(y_pred, y_test)))\nprint('SVC precision : {:.4f}'.format(precision_score(y_pred, y_test)))\nprint('SVC recall    : {:.4f}'.format(recall_score(y_pred, y_test)))\nprint(\"\\n\", classification_report(y_pred, y_test))\ncnf_matrix = confusion_matrix(y_test, y_pred, labels=[1, 0])\nnp.set_printoptions(precision=2)\nplt.figure()\n","metadata":{"execution":{"iopub.status.busy":"2021-09-14T06:07:00.855909Z","iopub.execute_input":"2021-09-14T06:07:00.856499Z","iopub.status.idle":"2021-09-14T06:07:00.907308Z","shell.execute_reply.started":"2021-09-14T06:07:00.856445Z","shell.execute_reply":"2021-09-14T06:07:00.906353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Search best hyperparameters: GridSearchCV\n# GridSearchCV accepts dictionary where different hyper-parameters we want to try on the SVM model. \n\nkernels = list(['linear', 'rbf', 'poly', 'sigmoid'])\nc = list([1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1, 10, 1e2, 1e3, 1e4,1e5])\ngammas = list([0.1, 1, 10, 100])\n\nclf = SVC()\nclf.fit(X_train, y_train)\nparam_grid = dict(kernel=kernels, C=c, gamma=gammas)\ngrid = GridSearchCV(clf, param_grid, cv=10, n_jobs=-1)\ngrid.fit(X_train, y_train)\ngrid.best_params_\n","metadata":{"execution":{"iopub.status.busy":"2021-09-14T06:07:03.237406Z","iopub.execute_input":"2021-09-14T06:07:03.237721Z","iopub.status.idle":"2021-09-14T06:07:08.996939Z","shell.execute_reply.started":"2021-09-14T06:07:03.237687Z","shell.execute_reply":"2021-09-14T06:07:08.995781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since in this dataset False Negatives and False postives are quite important:\n - False Negatives (predicting negative to a positive heart failure),  failed task over here on maybe saving someone.\n - False positives (predicting positive to a negative heart failure), it's a bad joke to tell a person and it's family that someone will die. Stop him/her from going to Las Vegas for to spend all his money in at the casino.\n\nF1 Score it's an important scoring measure to take in consideration. In order to calculate the F1, we also need: recall and precision scores.  \n - Recall  \n<img src= \"https://lawtomated.com/wp-content/uploads/2019/10/Recall_1.png\" alt =\"Precision\" style='width: 200px;'>  \n\n - Precision  \n <img src= \"https://anchormen.nl/wp-content/uploads/2020/02/precision-formula.png\" alt =\"Precision\" style='width: 200px;'>  \n \n- F1 score  \n<img src= \"https://miro.medium.com/max/752/1*UJxVqLnbSj42eRhasKeLOA.png\n\" alt =\"F1 score\" style='width: 200px;'>  \n\nA presence of a smaller recall than the precision, means that the proposed model is more likely to miss classify a heart failure pixel as a nnegative. Whereas, a higher precision value explains that the model is more accurate on classifying correctly a heart failure as positive","metadata":{}},{"cell_type":"code","source":"# Best hyper-parameters are  C:1.0 (default), gamma: 0.1, kernel: rbf\n\nmodel_ = SVC(kernel='rbf',gamma=0.1, C=1.0, tol=1e-5, verbose=1,max_iter=2500).fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nprint('Accuracy Score: {:.4f}'.format(accuracy_score(y_test, y_pred)))\nprint('SVC f1-score  : {:.4f}'.format(f1_score(y_pred, y_test)))\nprint('SVC precision : {:.4f}'.format(precision_score(y_pred, y_test)))\nprint('SVC recall    : {:.4f}'.format(recall_score(y_pred, y_test)))\nprint(\"\\n\", classification_report(y_pred, y_test))\n\ncnf_matrix = confusion_matrix(y_test, y_pred, labels=[1, 0])\nsns.heatmap((cnf_matrix / np.sum(cnf_matrix)*100),\n            annot=True, fmt=\".2f\", cmap=\"Blues\")\n","metadata":{"execution":{"iopub.status.busy":"2021-09-14T06:21:49.38143Z","iopub.execute_input":"2021-09-14T06:21:49.384127Z","iopub.status.idle":"2021-09-14T06:21:49.689916Z","shell.execute_reply.started":"2021-09-14T06:21:49.38404Z","shell.execute_reply":"2021-09-14T06:21:49.688797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While doing tests, the different sets of train/test where giving different results\non the F1 score and accuraccy. Thus and to avoid overfitting cross-validation is used in\nthis experiment to avoid previouse mentioned problems. Cross-validation splits a dataset \ninto k parts, where\n\nSee: https://machinelearningmastery.com/k-fold-cross-validation/","metadata":{}},{"cell_type":"code","source":"kf = KFold(n_splits=10, shuffle=True)\n\nacc_arr = np.empty((10, 1))\nf1_arr = np.empty((10, 1))\ncnf_arr= []\nx = 0\nfor train_index, test_index in kf.split(df_x, df_y):\n    X_train, X_test = df_x[train_index], df_x[test_index]\n    y_train, y_test = df_y[train_index], df_y[test_index]\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print('Accuracy Score: {:.4f}'.format(accuracy_score(y_test, y_pred)))\n    print('SVC f1-score  : {:.4f}'.format(f1_score(y_pred, y_test)))\n    print('SVC precision : {:.4f}'.format(precision_score(y_pred, y_test)))\n    print('SVC recall    : {:.4f}'.format(recall_score(y_pred, y_test)))\n    print(\"\\n\", classification_report(y_pred, y_test))\n    \n    cnf_matrix = confusion_matrix(y_test, y_pred)\n    acc_arr[x] = accuracy_score(y_test, y_pred)\n    f1_arr[x] = f1_score(y_test, y_pred)\n\n    x = x+ 1\n\nprint(\"%0.2f f1 score with a standard deviation of %0.2f\" %\n      (f1_arr.mean(), f1_arr.std()))\nprint(\"%0.2f accuracy with a standard deviation of %0.2f\" %\n      (acc_arr.mean(), acc_arr.std()))\n","metadata":{"execution":{"iopub.status.busy":"2021-09-13T23:57:57.031597Z","iopub.execute_input":"2021-09-13T23:57:57.031892Z","iopub.status.idle":"2021-09-13T23:57:57.380184Z","shell.execute_reply.started":"2021-09-13T23:57:57.03186Z","shell.execute_reply":"2021-09-13T23:57:57.379262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}