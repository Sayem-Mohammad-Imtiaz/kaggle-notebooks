{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"Data = pd.read_csv('/kaggle/input/california-housing-prices/housing.csv')\nData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data['ocean_proximity'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.hist(bins = 50, figsize = (20,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we split the data using unique hash values. If we randomly generate the train and test set then, for every time we restart the data will again get mixed up and maybe the test set may contain values used previously in the training data. This is just a practice code for train Test split without using the sklearn train_test_split library.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from zlib import crc32\ndef test_set_check(identifier, test_ratio):\n    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n\ndef split_train_test_by_id(data, test_ratio, id_column):\n    ids = data[id_column]\n    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n    return data.loc[~in_test_set], data.loc[in_test_set]\n\nData_with_id = Data.reset_index() # adds an `index` column\ntrain_set, test_set = split_train_test_by_id(Data_with_id, 0.2, \"index\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data_with_id.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting the data into test set and train Set using sklearn train_test_split.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_set, test_set = train_test_split(Data, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we observe that the income is an important attribute. To avoid biasing we devide the important attribute into homogeneous groups called strata and then the right number of samples are distributed from each stratum to the training and the testing data. Further we are storing it into Income Category column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Data['Income Category'] = pd.cut(Data[\"median_income\"],bins=[0., 1.5, 3.0, 4.5, 6., np.inf],labels=[1, 2, 3, 4, 5])\nData['Income Category'].hist()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# StratifiedShuffleSplit\nis a sampling method in sklearn used for sampling based on equally distributed starta of a particular Attribute which is/are important. Here we are just giving a single split to the data since we want the data to be equally distributed in the test set and the train set.\n***After the split we use the hashed index to distribute the data to the train set and test set.***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(Data, Data[\"Income Category\"]):\n    strat_train_set = Data.loc[train_index]\n    strat_test_set = Data.loc[test_index]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This Shows how the Income Category is properly distributed according to their proportions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"strat_train_set['Income Category'].value_counts()/len(strat_train_set)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, there is no need of Income Category and hence we drop the same.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for set_ in (strat_train_set, strat_test_set):\n    set_.drop(\"Income Category\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Visualizing the locations of the Districts. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Data = strat_train_set.copy()\nData.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Changing the alpha from Default to 0.1 gives us the densed areas.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"There are a some ***high-density areas***, namely the Bay Area and around Los Angeles and San Diego, plus a long line of fairly high density in the Central Valley, in particular around Sacramento and Fresno.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\",alpha = 0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The radius of each circle represents the districtâ€™s population (option s ), and the color represents the price (option c ). jet is a predefined color map (option cmap ), which ranges from blue (low values) to red (high prices).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,s=Data[\"population\"]/100, label=\"population\", figsize=(10,7),c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Correlations\nSince the dataset is not too large, we can compute a correlation between the features using the standard correlation coeffecient**(Pearson's r)**.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = Data.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Closeness to +1 refers to higher (Directly proportional) dependency of the attribute with the target attribute. Negative correlation(closeness to -1) indicates the lower (inversely proportional) dependency. While closeness to zero shows no linear dependecy of the two features.\n***The correlation coeffecients only shows the linear dependencies.***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix['median_house_value'].sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The pandas.plotting.scatter_matrix gives the visualization of Data dependencies.\nPlots of some important feature dependencies - ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.plotting import scatter_matrix\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\"housing_median_age\"]\nscatter_matrix(Data[attributes], figsize=(12, 8))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the below plot we can clearly see the higher dependency of median_income to median_house values.\n*The Horizontal line clearly shows the capping of values.*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Extraction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Data['rooms_per_house'] = Data['total_rooms']/Data['households']\nData['bedrooms_per_room'] = Data['total_bedrooms']/Data['total_rooms']\nData['population_per_household'] = Data['population']/Data['households']\n\ncorr_matrix = Data.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reverting to a clean training set (by copying strat_train_set once again).\nSeparating the predictors and the labels(drop() creates a copy of the data and does not affect strat_train_set )","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Data = strat_train_set.drop(\"median_house_value\", axis=1)\nData_labels = strat_train_set[\"median_house_value\"].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* '''*Used to drop the rows with null values*''' - \nhousing.dropna(subset=[\"total_bedrooms\"])\n* '''*Used to drop the entire column, also used earlier to drop the labels from the training data*''' - \nhousing.drop(\"total_bedrooms\", axis=1)\n* '''*Used to compute the median of all the values which are not null*''' - \nmedian = housing[\"total_bedrooms\"].median()\n* '''*Further the missing values are filled with the median valuer of the attribute*''' - \nhousing[\"total_bedrooms\"].fillna(median, inplace=True)\n* If we are using the median to replace the na values then, we will save the median value to replace the na values in the test set.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Handling Missing Values**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy = 'median')\ndata_num = Data.drop('ocean_proximity',axis = 1)\nimputer.fit(data_num)\n''' simple imputer has an attribute statistics_ in which it saves the median values (imputer.statistics_)'''\ndata_num.median().values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Passing the numerical data to the imputer so that it fills the missing values with the learned median that it has computed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = imputer.fit(data_num)\nData_tr = pd.DataFrame(X, columns = data_num.columns, index = data_num.index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In sklearn any object that can estimate some parameters based on some attributes is called the estimator(eg. imputer). The estimation is performed by the fit method. Any parameter other than the training and test dataset(such as imputer strategy) needed to guide the estimation process is calleed a hyperparamter, and it must be set as an instance parameter.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cat = Data[['ocean_proximity']]\ndata_cat.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Converting Categorical Data to Numeric Data**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder()\nencoded_categories = encoder.fit_transform(data_cat)\nencoded_categories","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder.categories_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here there is only a single categorical attribute, but when dealing with multiple categorical values we will get a large number of training features which might slow down the training. If this happens we can replace the categories with useful numerical data. Example, we can replace the ocean_proximity feature with the distance to the ocean. Alternatively, we can replace each category with a learnable, low-dimensional vector called an embedding. This is an example of representation learning.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded_categories.toarray()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**custom transformer to form the combined attributes(cell 21).**\n* TransformerMixin - provides the fit_transform method, provided we have to create the fit method and the transform method. \n* BaseEstimator - used as a base class (avoids \\*args and \\*\\*kargs in the constructor), we will also get two extra methods ( get_params() and set_params() ) that will be useful for automatic hyperparameter tuning.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nrooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n        '''we are gateing this parameter since in future we can find out whether this parameter helps the algo'''\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n        population_per_household = X[:, population_ix] / X[:, households_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household,bedrooms_per_room]\n        return np.c_[X, rooms_per_household, population_per_household]\n\nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\nhousing_extra_attribs = attr_adder.transform(Data.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# creating a pipeline for all data transformation steps","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\n#StandardScaler - feature scaling\nfrom sklearn.preprocessing import StandardScaler\nnum_pipeline = Pipeline([('imputer', SimpleImputer(strategy=\"median\")),('attribs_adder', CombinedAttributesAdder()),('std_scaler', StandardScaler()),])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calling of the pipelineâ€™s fit() method, it calls fit_transform() sequentially on all transformers, passing the output of each call as the parameter to the next call until it reaches the final estimator, for which it calls the fit() method. The pipeline exposes the same methods as the final estimator.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_num_tr = num_pipeline.fit_transform(data_num)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The column transformer transforms both the numerical and categorical column at the same time.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nnum_attribs = list(data_num)\ncat_attribs = [\"ocean_proximity\"]\nfull_pipeline = ColumnTransformer([(\"num\", num_pipeline, num_attribs),(\"cat\", OneHotEncoder(), cat_attribs),])\nhousing_prepared = full_pipeline.fit_transform(Data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we are finished with the exploration and preprocessing of the data. Finally, it's time to select a model and train it.\n# **Linear Regressor Model**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlr_model = LinearRegression()\nlr_model.fit(housing_prepared,Data_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we predict some random data from the Dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"some_data = Data.iloc[:5]\nsome_labels = Data_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(\"Predictions:\", lr_model.predict(some_data_prepared))\nprint(\"Labels:\", list(some_labels))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the below cell we are calculating the error of our model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nhousing_predictions = lr_model.predict(housing_prepared)\nlr_mse = mean_squared_error(Data_labels, housing_predictions)\nlr_rmse = np.sqrt(lr_mse)\nlr_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will not touch the test set until we are ready to launch a confident model, so we need to use part of the training set for training and part of it for model validation.<br/>\nWe use the sklearn k fold cross validation to divide the training Data into 10 folds and training on 9 parts while evaluating on the 10th part. We are computing the sqrt of negative scores since cross validation is a utility function which should be maximized while MSE is a cost function which should be minimized.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nlin_scores = cross_val_score(lr_model, housing_prepared, Data_labels,scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since in the Linear Regression model is under fitting as we can see, we can simply look for more powerful Models such as Decision Tree Regressor. Because an error of 69052.46 is not at all good when the highest and the lowest Data labels are of value 265000 and 120000 respectively.\n# **Decision Tree Regressor**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\ndt_model = DecisionTreeRegressor()\ndt_model.fit(housing_prepared, Data_labels)\n\nhousing_predictions = dt_model.predict(housing_prepared)\ntree_mse = mean_squared_error(Data_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since in the above step, we can see that the model has 0 loss. Here, the model is badly overfitting the Data. For the assurance that the model is overfitting we can simply evaluate it using cross validation like we did Earlier.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_scores = cross_val_score(dt_model, housing_prepared, Data_labels,scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-dt_scores)\ndisplay_scores(tree_rmse_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, the score is even worse than the Linear Regression Score. Let's try random forest regressor.\n# **Random Forest Regressor**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrf_model = RandomForestRegressor()\nrf_model.fit(housing_prepared, Data_labels)\n\nhousing_predictions = rf_model.predict(housing_prepared)\nforest_mse = mean_squared_error(Data_labels, housing_predictions)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_scores = cross_val_score(rf_model, housing_prepared, Data_labels,scoring=\"neg_mean_squared_error\", cv=10)\nforest_rmse_scores = np.sqrt(-rf_scores)\ndisplay_scores(forest_rmse_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Random forest regresssor gives the minimum of the MSE among the three models that we have used.<br/>\n*We can further use many other models. But for time being we will use these 3 models.*\n\n# Fine-Tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nparam_grid = [{'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},{'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},]\nforest_reg = RandomForestRegressor()\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,scoring='neg_mean_squared_error',return_train_score=True)\ngrid_search.fit(housing_prepared, Data_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When we have no idea what value a hyperparameter should have,a simple approach is to try out consecutive powers of 10 (or a smaller number if you want a more fine-grained search, as shown above with the n_estimators hyperparameter).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also print all the evaluation scores by using the cv_results_ instance variable.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have fine tuned our best MSE giving model, and set the max_features hyperparameter to 6 and the n_estimators hyperparameter to 30. The results are however only Slightly better than the previous result.<br/>\n*Further we will analyze our best model with the  best hyperparameters.*\n# Analyzing the best Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances = grid_search.best_estimator_.feature_importances_\nextra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\ncat_encoder = full_pipeline.named_transformers_[\"cat\"]\ncat_one_hot_attribs = list(cat_encoder.categories_[0])\nattributes = num_attribs + extra_attribs + cat_one_hot_attribs\nsorted(zip(feature_importances, attributes), reverse=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it's time to evaluate our Model.\n# Model evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model = grid_search.best_estimator_\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\nX_test_prepared = full_pipeline.transform(X_test)\n\nfinal_predictions = final_model.predict(X_test_prepared)\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}