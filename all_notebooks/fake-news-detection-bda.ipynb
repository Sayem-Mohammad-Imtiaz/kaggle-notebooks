{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install pyspark","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pyspark\nfrom pyspark import SQLContext\n\nsc = pyspark.SparkContext(appName='Fake News Detection')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sql = pyspark.SQLContext(sc)\n\ntrue_df = sql.read.format(\"com.databricks.spark.csv\").option(\"header\", \"True\").load(\"../input/fake-and-real-news-dataset/True.csv\")\nfalse_df = sql.read.format(\"com.databricks.spark.csv\").option(\"header\", \"True\").load(\"../input/fake-and-real-news-dataset/Fake.csv\")\n\ntrue_df.show()\nfalse_df.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport string\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.pipeline import Pipeline\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom keras import Sequential\nfrom keras.layers import Embedding, LSTM, Dense, Dropout\nfrom keras.models import load_model\n\n\nseed = 4353","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/True.csv')\nfake = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/Fake.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fake.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Introducing new column in both dataframes\n\ntrue['category']=1\nfake['category']=0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Concatenating them using pandas concatenate to form a single dataframe\n\ndata_raw = pd.concat([true, fake], axis=0)\ndata_raw.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combining title and text to obtain a single string\n# dropping title and\n\ndata_raw['fulltext'] = data_raw.title + ' ' + data_raw.text\ndata_raw.drop(['title','text'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extracting a new dataframe using features fulltext and category\ndata = data_raw[['fulltext', 'category']]\ndata = data.reset_index()\ndata.drop(['index'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for missing values\n\ndata_raw.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The dataset contans {} rows and {} columns'.format(data.shape[0], data.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter Subject\nplt.figure(figsize =(15,10))\nsns.countplot(data_raw['subject'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Word extraction from true and fake texts\n\ntrue_text = data[data.category==1]['fulltext']\nfake_text = data[data.category==0]['fulltext']\nfake_text = fake_text.reset_index().drop(['index'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to extract major words from true and fake news\n\ndef wordcloud_words(X_data_full):\n    \n    # function for removing punctuations\n    def remove_punct(X_data_func):\n        string1 = X_data_func.lower()\n        translation_table = dict.fromkeys(map(ord, string.punctuation),' ')\n        string2 = string1.translate(translation_table)\n        return string2\n    \n    X_data_full_clear_punct = []\n    for i in range(len(X_data_full)):\n        test_data = remove_punct(X_data_full[i])\n        X_data_full_clear_punct.append(test_data)\n        \n    # function to remove stopwords\n    def remove_stopwords(X_data_func):\n        pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n        string2 = pattern.sub(' ', X_data_func)\n        return string2\n    \n    X_data_full_clear_stopwords = []\n    for i in range(len(X_data_full)):\n        test_data = remove_stopwords(X_data_full[i])\n        X_data_full_clear_stopwords.append(test_data)\n        \n    # function for tokenizing\n    def tokenize_words(X_data_func):\n        words = nltk.word_tokenize(X_data_func)\n        return words\n    \n    X_data_full_tokenized_words = []\n    for i in range(len(X_data_full)):\n        test_data = tokenize_words(X_data_full[i])\n        X_data_full_tokenized_words.append(test_data)\n        \n    # function for lemmatizing\n    lemmatizer = WordNetLemmatizer()\n    def lemmatize_words(X_data_func):\n        words = lemmatizer.lemmatize(X_data_func)\n        return words\n    \n    X_data_full_lemmatized_words = []\n    for i in range(len(X_data_full)):\n        test_data = lemmatize_words(X_data_full[i])\n        X_data_full_lemmatized_words.append(test_data)\n        \n    return X_data_full_lemmatized_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true_words = wordcloud_words(true_text)\nfake_words = wordcloud_words(fake_text.fulltext)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_wordcloud(text):\n    wordcloud = WordCloud(background_color = 'black',\n                         max_words = 3000,\n                         width=1600,\n                         height=800).generate(text)\n    plt.clf()\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,18))\nplot_wordcloud(' '.join(true_words))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,18))\nplot_wordcloud(' '.join(fake_words))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datas=pd.concat([true,fake])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datas['fulltext'] = datas.title + ' ' + datas.text\ndatas.drop(['title','text'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final = data[['fulltext', 'category']]\nfinal = data.reset_index()\nfinal.drop(['index'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#PREPROCESSING\n\n#Removing the repeated data.\n#Removing Stop-words\n#Remove any punctuations or limited set of special characters like , or . or # etc.\n#Snowball Stemming the word\n#Convert the word to lowercase.\n\nimport re\ni=0;\nfor sent in final['fulltext'].values:\n    if (len(re.findall('<.*?>', sent))):\n        print(i)\n        print(sent)\n        break;\n    i += 1; ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nstop = set(stopwords.words('english')) \nsno = nltk.stem.SnowballStemmer('english')\n\ndef cleanhtml(sentence): #function to clean the word of any html-tags\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, ' ', sentence)\n    return cleantext\ndef cleanpunc(sentence): #function to clean the word of any punctuation or special characters\n    cleaned = re.sub(r'[?|!|\\'|\"|#.|,|)|(|\\|/]',r'',sentence)\n    \n    return  cleaned","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PREPROCESSING STEP BY STEP\n# this code takes a while to run as it needs to run on 500k sentences.\nimport re\ni=0\nstr1=' '\nfinal_string=[]\nall_true_words=[] # store words from +ve reviews here\nall_fake_words=[] # store words from -ve reviews here.\ns=''\nfor sent in final['fulltext'].values:\n    filtered_sentence=[]\n    #print(sent);\n    sent=cleanhtml(sent) # remove HTMl tags\n    for w in sent.split():\n        for cleaned_words in cleanpunc(w).split():\n            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n                if(cleaned_words.lower() not in stop):\n                    s=(sno.stem(cleaned_words.lower())).encode('utf8')\n                    filtered_sentence.append(s)\n                    if (final['category'].values)[i] == '1': \n                        all_true_words.append(s) #list of all words used to describe positive reviews\n                    if(final['category'].values)[i] == '0':\n                        all_fake_words.append(s) #list of all words used to describe negative reviews reviews\n                else:\n                    continue\n            else:\n                continue \n    #print(filtered_sentence)\n    str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n    #print(\"***********************************************************************\")\n    \n    final_string.append(str1)\n    i+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final['CleanedText']=final_string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label=final[\"category\"]\nsample=final['CleanedText']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TRAIN and TEST Split\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(sample, label, test_size=0.30, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#IMPORT\n\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sb\nfrom sklearn.metrics import classification_report\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nimport gensim\nfrom gensim.models import Word2Vec, KeyedVectors\nfrom sklearn.metrics import f1_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TFIDF Vectorizer\n\ntf_idf_vect = TfidfVectorizer(ngram_range=(1,2))\nX_train = tf_idf_vect.fit_transform(X_train)\nX_test= tf_idf_vect.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model Multinomial NB\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import cross_val_score\n# Creating alpha values in the range from 10^-4 to 10^4\nneighbors = []\ni = 0.0001\nwhile(i<=10000):\n    neighbors.append(np.round(i,3))\n    i *= 3\n\n\n# empty list that will hold cv scores\ncv_scores = []\n\n# perform 10-fold cross validation\nfor k in neighbors:\n    bn = MultinomialNB(alpha = k)\n    scores = cross_val_score(bn, X_train, Y_train, cv=10, scoring='f1_macro', n_jobs=-1)\n    cv_scores.append(scores.mean())  \n    \n# determining best value of alpha\noptimal_alpha = neighbors[cv_scores.index(max(cv_scores))]\nprint('\\nThe optimal value of alpha is %.3f.' % optimal_alpha)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bn_optimal = MultinomialNB(alpha = optimal_alpha)\nbn_optimal.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bn_optimal.classes_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we can find log probabilities of different features for both the classes\nclass_features = bn_optimal.feature_log_prob_\n\n#  row_0 is for 'Fake' class and row_1 is for 'True' class\nFake_features = class_features[0]\nTrue_features = class_features[1]\n\n# Getting all feature names\nfeature_names = tf_idf_vect.get_feature_names()\n\n# Sorting 'Fake_features' and 'True_features' in descending order using argsort() function\nsorted_Fake_features = np.argsort(Fake_features)[::-1]\nsorted_True_features = np.argsort(True_features)[::-1]\n\nprint(\"Top 20 Important Features and their log probabilities For Fake News :\\n\\n\")\nfor i in list(sorted_Fake_features[0:20]):\n    print(\"%s\\t -->\\t%f  \"%(feature_names[i],Fake_features[i]))\n    \nprint(\"\\n\\nTop 20 Important Features and their log probabilities For true news :\\n\\n\")\nfor i in list(sorted_True_features[0:20]):\n    print(\"%s\\t -->\\t%f  \"%(feature_names[i],True_features[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom scipy.stats import randint as sp_randint\n\ndepths=[1,5,50,100]\nestimators=[1,5,50,100]\nclf = RandomForestClassifier()\n\nparams = {'max_depth' : depths,\n          'n_estimators':estimators  \n          }\n\ngrid = GridSearchCV(estimator = clf,param_grid=params ,cv = 2,n_jobs = 3,scoring='roc_auc')\ngrid.fit(X_train, Y_train)\nprint(\"best depth = \", grid.best_params_)\nprint(\"AUC value on train data = \", grid.best_score_*100)\na1 = grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimal_depth1 = a1.get('max_depth')\noptimal_bases1 = a1.get('n_estimators')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier(max_depth=optimal_depth1,n_estimators=optimal_bases1) \n\nclf.fit(X_train,Y_train)\n\npred = clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Code for drawing seaborn heatmaps\nclass_names = ['Fake','True']\ndf_heatmap = pd.DataFrame(confusion_matrix(Y_test, pred), index=class_names, columns=class_names )\nfig = plt.figure(figsize=(10,7))\nheatmap = sb.heatmap(df_heatmap, annot=True, fmt=\"d\")\n\n# Setting tick labels for heatmap\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=14)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=0, ha='right', fontsize=14)\nplt.ylabel('Predicted label',size=18)\nplt.xlabel('True label',size=18)\nplt.title(\"Confusion Matrix\\n\",size=24)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import confusion_matrix\nacc1 = accuracy_score(Y_test, pred) * 100\npre1 = precision_score(Y_test, pred) * 100\nrec1 = recall_score(Y_test, pred) * 100\nf11 = f1_score(Y_test, pred) * 100\nprint('\\nAccuracy=%f%%' % (acc1))\nprint('\\nprecision=%f%%' % (pre1))\nprint('\\nrecall=%f%%' % (rec1))\nprint('\\nF1-Score=%f%%' % (f11))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate feature importances from decision trees\nimportances = clf.feature_importances_\n\n# Sort feature importances in descending order\nindices = np.argsort(importances)[::-1][:25]\n\n# Rearrange feature names so they match the sorted feature importances\nnames = tf_idf_vect.get_feature_names()\n\nsb.set(rc={'figure.figsize':(11.7,8.27)})\n\n# Create plot\nplt.figure()\n\n# Create plot title\nplt.title(\"Feature Importance\")\n\n# Add bars\nplt.bar(range(25), importances[indices])\n\n# Add feature names as x-axis labels\nnames = np.array(names)\nplt.xticks(range(25), names[indices], rotation=90)\n\n# Show plot\nplt.show()\n# uni_gram.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=names[indices]\nprint(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\n\nwordcloud = WordCloud(width = 800, height = 600,background_color ='white').generate(str(df))\nplt.imshow(wordcloud)\nplt.title(\"Frequent words\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}