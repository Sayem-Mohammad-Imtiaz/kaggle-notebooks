{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#importing necessary libraries\nimport re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nimport seaborn\nimport warnings\nwarnings.filterwarnings(\"ignore\",category= DeprecationWarning)\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reading the data\ndata = pd.read_csv('/kaggle/input/reddit-india-flair-detection/datafinal.csv')\n\ndataf = data.copy()\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning"},{"metadata":{},"cell_type":"markdown","source":"#### Dropping unwanted columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(['score','url','comms_num','author','timestamp'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['title'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['body'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['combined_features'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['comments'][0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Note that combined features column is actually combination text of three columns as title,comments, url and combined_features."},{"metadata":{},"cell_type":"markdown","source":"I am dropping that column too for ease of analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(['combined_features'],axis=1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 18 unique flairs."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['flair'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that some of the flairs are date values. Let's explore it a bit to see what it is..."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby('flair')['title'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby('flair')['id'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby('flair')['body'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby('flair')['comments'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems like there is no significant details present related to those 'date-time' flairs. So let's drop those entries."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping the rows corresponding to date-time flairs\nf = data['flair'].dropna()\nregx = re.compile(r\"[\\d]{1,2}-[\\d]{1,2}-[\\d]{4} [\\d]{1,2}:[\\d]{1,2}\")\nfor __ in f:\n    #print(flair)\n    x = regx.search(__)\n    if x is not None:\n        #print(x.group())\n        d = data[data.flair == x.group()]\n        #print(d)\n        data = data.drop(d.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['flair'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are some nan values in flair. Exploring it...."},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data['flair'] == np.nan].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Note: There are no significant data related to nan flair value. Hence we can drop it."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.dropna(subset=['flair'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Combine [title,body and comments] for text-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['text'] = data['title'].astype(str) + data['body'].astype(str) + data['comments'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_final = data[['flair','id','text']]\ndata_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_final.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_final.groupby('flair')['text'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Removing Special characters"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_final['text'] = data_final['text'].str.replace(\"[^a-zA-Z0-9 \\n.]\",\" \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Now we have clean data!!!\"\"\"\ndata_final.head(10) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"### Removing Punctuation, Stopwords and Tokenization"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n    1. Removing all punctuation\n    2. Removing stop-words\n    3. Returns a clean text\n\"\"\"\ndef clean_txt(mess):\n    \n    nonpunc = [char for char in mess if char not in string.punctuation] #list of strings which are non-punc\n    \n    nonpunc = \"\".join(nonpunc) #join back to form the whole string\n    \n    return [word for word in nonpunc.split() if word.lower() not in stopwords.words('english')]    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_final['text'] = data_final['text'].apply(clean_txt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lemmatization"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import WordNetLemmatizer\n\nle = WordNetLemmatizer()\n\ndata_final['text'] = data_final['text'].apply(lambda x : [le.lemmatize(word) for word in x])\ndata_final['text'] = data_final['text'].apply(lambda x : \" \".join(x))\ndata_final['text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"qwe = data_final['text'].copy()\nqwe","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Vectorization"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bow_transformer = CountVectorizer(analyzer = clean_txt).fit(data_final['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_bow = bow_transformer.transform(data_final['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Shape of sparse matrix is {text_bow.shape}')\nprint(f'Length of dictionary is {len(bow_transformer.vocabulary_)}')\nprint(f'Number of non=zero occusrances is {text_bow.nnz}')\nsparsity = (text_bow.nnz/(text_bow.shape[0]*text_bow.shape[1]))*100\nprint('Sparsity :', sparsity)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### TfIdf"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_transformer = TfidfTransformer().fit(text_bow)\ntext_tfidf = tfidf_transformer.transform(text_bow)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Shape of tfidf of text is {text_tfidf.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Traning a Model (Naive Bayes)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = MultinomialNB().fit(text_tfidf,data_final['flair'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(text_tfidf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(classification_report(data_final['flair'],predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction \n\n## Pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = Pipeline([\n    ('bow', CountVectorizer(analyzer=clean_txt)),\n    ('tfidf',TfidfTransformer()),\n    ('classifier',MultinomialNB()),\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train-test split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_train, text_test, flair_train, flair_test = train_test_split(data_final['text'],data_final['flair'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline.fit(text_train,flair_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = pipeline.predict(text_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(flair_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Conclusion: Devised a model which can predict flair if given a text."},{"metadata":{},"cell_type":"markdown","source":"### Input "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(text_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = [data_final.iloc[int(i)]['id'] for i in text_test.index]\nPredicted_df = pd.DataFrame({'ID':ids,'Text':text_test,'PredictedFlair':predictions}).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Predicted_df","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}