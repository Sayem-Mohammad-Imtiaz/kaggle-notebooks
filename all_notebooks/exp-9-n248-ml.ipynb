{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Boosting and Stacking Exercises"},{"metadata":{},"cell_type":"markdown","source":"## Introduction\nWe will be using the Pima Indians Diabetes database, \n\nFor each record in the dataset it is provided:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd  # data processing\nimport numpy as np   # linear algebra\nimport matplotlib.pyplot as plt  #Plotting\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import print_function\nimport os\ndata_path = ['..', '..', 'data']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Question 1\n\nImport the data from the file diabetes.csv and examine the shape and data types.For the data types, there will be too many to list each column separately. Rather, aggregate the types by count.\n\nDetermine if the float columns need to be scaled."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfilepath = '../input/pima-indians-diabetes-database/diabetes.csv'\ndata = pd.read_csv(filepath, sep=',')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data has quite a few predictor columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mask to select float columns\nfloat_columns = (data.dtypes == np.float)\n\n# Verify that the maximum of all float columns is 1.0\nprint( (data.loc[:,float_columns].max()==1.0).all() )\n\n# Verify that the minimum of all float columns is -1.0\nprint( (data.loc[:,float_columns].min()==-1.0).all() )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.rename(columns={'Outcome':'Diabetic'})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Question 2\nInteger encode the activities.\nSplit the data into train and test data sets. Decide if the data will be stratified or not during the train/test split."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\ndata['Diabetic'] = le.fit_transform(data['Diabetic'])\n\nle.classes_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.Diabetic.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"NOTE: We are about to create training and test sets from data. On those datasets, we are going to run grid searches over many choices of parameters. This can take some time. In order to shorten the grid search time, feel free to downsample data and create X_train, X_test, y_train, y_test from the downsampled dataset.\n\nNow split the data into train and test data sets. A stratified split was not used here. If there are issues with any of the error metrics on the test set, it can be a good idea to start model fitting over using a stratified split. Boosting is a pretty powerful model, though, so it may not be necessary in this case."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Alternatively, we could stratify the categories in the split, as was done previously\nfeature_columns = [x for x in data.columns if x != 'Diabetic']\n\nX_train, X_test, y_train, y_test = train_test_split(data[feature_columns], data['Diabetic'],\n                 test_size=0.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Question 3\nFit gradient boosted tree models with all parameters set to their defaults the following tree numbers (n_estimators = [25, 50, 100, 200, 400]) and evaluate the accuracy on the test data for each of these models.\nPlot the accuracy as a function of estimator number.\nNote: This question may take some time to execute, depending on how many different values are fit for estimators. Setting max_features=4 in the gradient boosting classifier will increase the convergence rate.\n\nAlso, this is similar to question 3 from week 9, except that there is no such thing as out-of-bag error for boosted models. And the warm_flag=True setting has a bug in the gradient boosted model, so don't use it. Simply create the model inside the for loop and set the number of estimators at this time. This will make the fitting take a little longer. Additionally, boosting models tend to take longer to fit than bagged ones because the decision stumps must be fit successively."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\n\nerror_list = list()\n\n# Iterate through all of the possibilities for number of estimators\ntree_list =  [125, 58, 106, 20, 100]\nfor n_trees in tree_list:\n    \n    # Initialize the gradient boost classifier\n    GBC = GradientBoostingClassifier(n_estimators=n_trees, \n                                     subsample=0.5,\n                                     max_features=4,\n                                     random_state=32)\n\n    # Fit the model\n    GBC.fit(X_train.values, y_train.values)\n    y_pred = GBC.predict(X_test)\n\n    # Get the error\n    error = 1. - accuracy_score(y_test, y_pred)\n    \n    # Store it\n    error_list.append(pd.Series({'n_trees': n_trees, 'error': error}))\n\nerror_df = pd.concat(error_list, axis=1).T.set_index('n_trees')\n\nerror_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\nsns.set_context('talk')\nsns.set_style('white')\nsns.set_palette('dark')\n\n# Create the plot\nax = error_df.plot(marker='o')\n\n# Set parameters\nax.set(xlabel='n_trees', ylabel='error')\nax.set_xlim(0, max(error_df.index)*1.1);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Question 4Â¶\nUsing a grid search with cross-validation, fit a new gradient boosted classifier with the a list of estimators, similar to question 3. Also consider varying the learning rates (0.1, 0.01, 0.001, etc.), the subsampling value (1.0 or 0.5), and the number of maximum features (1, 2, etc.).\nExamine the parameters of the best fit model.\nCalculate relevant error metrics on this model and examine the confusion matrix.\nNote: this question may take some time to execute, depending on how many features are associated with the grid search. It is recommended to start with only a few to ensure everything is working correctly and then add more features. Setting max_features=4 in the gradient boosting classifier will increase the convergence rate."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# The parameters to be fit--only n_estimators and learning rate\n# have been varied here for simplicity\nparam_grid = {'n_estimators': [20, 100],\n              'learning_rate': [0.1, 0.01]}\n\n# The grid search object\nGV_GBC = GridSearchCV(GradientBoostingClassifier(subsample=0.5,\n                                                 max_features=4,\n                                                 random_state=32), \n                      param_grid=param_grid, \n                      scoring='accuracy',\n                      n_jobs=-1)\n\n# Do the grid search\nGV_GBC = GV_GBC.fit(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The best model\nGV_GBC.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\n\ny_pred = GV_GBC.predict(X_test)\nprint(classification_report(y_pred, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\nsns.set_context('talk')\ncm = confusion_matrix(y_test, y_pred)\nax = sns.heatmap(cm, annot=True, fmt='d')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Question 5\nCreate an AdaBoost model and fit it using grid search, much like question 4. Try a range of estimators between 100 and 200.\nCompare the errors from AdaBoost to those from the GradientBoostedClassifier.\nNOTE: Setting max_features=4 in the decision tree classifier used as the base classifier for AdaBoost will increase the convergence rate."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nABC = AdaBoostClassifier(DecisionTreeClassifier(max_features=4))\n\nparam_grid = {'n_estimators': [10, 120, 167],\n              'learning_rate': [0.01, 0.001]}\n\nGV_ABC = GridSearchCV(ABC,\n                      param_grid=param_grid, \n                      scoring='accuracy',\n                      n_jobs=-1)\n\nGV_ABC = GV_ABC.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The best model\nGV_ABC.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = GV_ABC.predict(X_test)\nprint(classification_report(y_pred, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_context('talk')\ncm = confusion_matrix(y_test, y_pred)\nax = sns.heatmap(cm, annot=True, fmt='d')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Question 6\nFit a logistic regression model with regularization. This can be a replica of a model that worked well in the exercises from week 4.\nUsing VotingClassifier, fit the logistic regression model along with either the GratientBoostedClassifier or the AdaBoost model (or both) from questions 4 and 5.\nDetermine the error as before and compare the results to the appropriate gradient boosted model(s).\nPlot the confusion matrix for the best model created in this set of exercises."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegressionCV\n\n# L2 regularized logistic regression\nLR_L2 = LogisticRegressionCV(Cs=5, cv=4, penalty='l2').fit(X_train, y_train)\ny_pred = LR_L2.predict(X_test)\nprint(classification_report(y_pred, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_context('talk')\ncm = confusion_matrix(y_test, y_pred)\nax = sns.heatmap(cm, annot=True, fmt='d')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\n# The combined model--logistic regression and gradient boosted trees\nestimators = [('LR_L2', LR_L2), ('GBC', GV_GBC)]\n\n# Though it wasn't done here, it is often desirable to train \n# this model using an additional hold-out data set and/or with cross validation\nVC = VotingClassifier(estimators, voting='soft')\nVC = VC.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = VC.predict(X_test)\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_context('talk')\ncm = confusion_matrix(y_test, y_pred)\nax = sns.heatmap(cm, annot=True, fmt='d')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}