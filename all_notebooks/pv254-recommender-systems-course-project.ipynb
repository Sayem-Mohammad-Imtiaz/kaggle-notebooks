{"cells":[{"metadata":{"_uuid":"5a57f3e9bcf0172b6d2928944b043423bccca629"},"cell_type":"markdown","source":"\n# Movielens 20M dataset offline analysis\n\n###### PV254 Recommender systems\n\nThis is a notebook for a project of a PV254 course on Masaryk University Brno, autumn 2018\n\n###### Oliver Velich, 409776"},{"metadata":{"_uuid":"443d332a94acbf5c1f89d5244cbb61f9ccd06bac"},"cell_type":"markdown","source":"## Introduction\n\nMovielens 20M is a\n> stable benchmark dataset. 20 million ratings and 465,000 tag applications applied to 27,000 movies by 138,000 users. Includes tag genome data with 12 million relevance scores across 1,100 tags [1].\n\nIt was last updated on 10/2016. It was created and maintained by GroupLens, a research lab in the Department of Computer Science and Engineering at the University of Minnesota, Twin Cities [2]. Dataset was obtained from an existing and active system [movielens.org](https://movielens.org/), that uses item-item collaborative filtering techniques to recommend movies based on past user's ratings [3].\n\n[1] [grouplens.org/datasets/movielens/20m](http://https://grouplens.org/datasets/movielens/20m/)\n[2] [grouplens.org/about/what-is-grouplens](https://grouplens.org/about/what-is-grouplens/)\n[3] [movielens.org/info/about](https://movielens.org/info/about/)"},{"metadata":{"_uuid":"2bd85718fecb7e990b512a845b96f59cae9c80da"},"cell_type":"markdown","source":"## Dataset description\n\nDataset contains six data tables."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"b66ed372935e5632b86e83e32ea94dffe754ebc6"},"cell_type":"code","source":"# imports\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\nimport time\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.metrics.pairwise import cosine_similarity as cos_sim\nfrom sklearn.neighbors import NearestNeighbors\nfrom scipy.sparse import csr_matrix\nfrom pandas.api.types import CategoricalDtype","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"42350ceef0131fb1ead2c514606ce359efaaa51b"},"cell_type":"code","source":"# data loading\nratings = pd.read_csv('../input/movielens-20m-dataset/rating.csv', parse_dates=[3])\nmovies = pd.read_csv('../input/movielens-20m-dataset/movie.csv')\ntags = pd.read_csv('../input/movielens-20m-dataset/tag.csv', parse_dates=[3])\nrelevances = pd.read_csv('../input/movielens-20m-dataset/genome_scores.csv')\ntagIDs = pd.read_csv('../input/movielens-20m-dataset/genome_tags.csv')\n\nratings100k = pd.read_csv('../input/movielens-latest-small/ratings.csv')\ntags100k = pd.read_csv('../input/movielens-latest-small/tags.csv')\nratings100k.timestamp = pd.to_datetime(ratings100k.timestamp, unit='s')\ntags100k.timestamp = pd.to_datetime(tags100k.timestamp, unit='s')\n\nmovies100k = pd.read_csv('../input/movielens-latest-small/movies.csv')\nmovies = pd.merge(movies, movies100k, how='outer')\nmovies = movies[~movies.duplicated(subset='movieId')]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true,"_uuid":"114150e000dddb05c817c105dad12d84d6ece41e"},"cell_type":"code","source":"import os\nprint(os.listdir('../input/movielens-20m-dataset'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ab44ae4e20d52bb0a476c9ac5bfb9f4dbee6913"},"cell_type":"markdown","source":"### Ratings\nRatings from users are stored in `rating.csv`.  It contains 4 columns: `userId`, `movieId`, `rating` and `timestamp`. "},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"59dd599853876842f4d61037a7f8f0d1795e9596"},"cell_type":"code","source":"print('Dataset contains {:,} ratings from {:,} distinct users applied to {:,} movies.'\\\n      .format(len(ratings), ratings['userId'].nunique(), ratings['movieId'].nunique()))\nprint('Dataset contains data since {} until {}.'\\\n      .format(ratings['timestamp'].min().date(), ratings['timestamp'].max().date()))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"markdown","source":"Here is a distribution of number of ratings given by each user. Histogram is truncated for users that rated less than 500 movies. "},{"metadata":{"trusted":true,"_uuid":"3bd36f9ab51b18c1c07da56fffd32dc9a85fb951","_kg_hide-input":true},"cell_type":"code","source":"user_ratings = ratings.groupby(by='userId')\nd = user_ratings['rating'].count()\nlimit = 500\nplt.hist(d[d<=limit], bins='fd')\nplt.xlabel('number of rated movies')\nplt.ylabel('number of users')\nprint(f'Only users with less than {limit} ratings are displayed ({len(user_ratings) - len(d[d<=limit]):,} users omitted).')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dce155d5baf54e200d487a2712394cbcc93a0395","_kg_hide-input":true},"cell_type":"code","source":"users_average = ratings.groupby('userId')['rating'].mean()\nitems_average = ratings.groupby('movieId')['rating'].mean()\nplt.hist([users_average, items_average], histtype='step', density=True)\nplt.xlabel('average rating for a movie / by a user')\nplt.ylabel('number of movies / users')\nplt.legend(['average rating given by a user', 'average rating of a movie'], loc=2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f78638c76d6b44dab0d63811be19e3af98792a91"},"cell_type":"markdown","source":"### Movies\nMovies are described in `movie.csv` table. It contains `movieId`, `title` and `genres` columns. One movie can contain several genres that are delimited by `|`."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"0be32b8fd64708c979d8466e8bcd36770d1d1ae6"},"cell_type":"code","source":"genres = Counter()\nfor g in movies['genres']:\n    genres.update(g.split('|'))\nprint('List of 10 most common genres: ', genres.most_common(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df97790818b5eb2656998458c0e043a0f59a7c24"},"cell_type":"markdown","source":"This is a list of the most rated and top rated movies (among movies with at least 20 ratings):"},{"metadata":{"trusted":true,"_uuid":"044e939fe6519fbe07617b7acc401fce62bce52e","_kg_hide-input":true},"cell_type":"code","source":"movie_ratings = ratings.groupby(by='movieId')\nmost_rated = movie_ratings['rating'].count().sort_values(ascending=False).head(10)\ntop_rated = movie_ratings['rating'].mean().where(movie_ratings['rating'].count() > 20).sort_values(ascending=False).head(10)\nprint(pd.merge(pd.DataFrame(most_rated), movies, on='movieId')[['title','rating']].rename(index=lambda x: x+1, columns={'rating': 'n. of ratings'}),'\\n')\nprint(pd.merge(pd.DataFrame(top_rated), movies, on='movieId')[['title','rating']].rename(index=lambda x: x+1, columns={'rating': 'average rating'}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0186aa518fe1c5d6fe9ed847a008194b6f226cc5"},"cell_type":"markdown","source":"### Tags\nTags given by users are stored in `tag.csv`. 4 columns are: `userId`,  `movieId`,  `tag` and  `timestamp`. "},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"f77dea157e2e4bbff6d4bb3c67396dc9704ee6f3"},"cell_type":"code","source":"print(f\"Tags were given by {tags['userId'].nunique():,} users to {tags['movieId'].nunique():,} movies.\")\nprint(f\"Together, {tags['tag'].nunique():,} unique tags were given in a period since {tags['timestamp'].min().date()} until {tags['timestamp'].max().date()}.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0acab16065e49b090436422eb9a7db4331d860b2","_kg_hide-input":true},"cell_type":"code","source":"tag_tags = tags.groupby(by='tag')\ntag_tags['movieId'].count().sort_values(ascending=False).head(10).rename('n. of movies')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f581481c727f6b09bf2ea1c2d18d82eaac56fbcb"},"cell_type":"markdown","source":"User added tags were evaluated for relevance by other users. Relevance of a tag is in `genome_scores.csv` table with columns: `movieId`, `tagId` and `relevance`. `tagId` is mapped to a `tag` in `genome_tags.csv` table."},{"metadata":{"trusted":true,"_uuid":"632fee2d4f36817d562524683517146acd7011bc"},"cell_type":"markdown","source":" 20 least relevant movie-tag pairs are:"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"9fa5e22f00aae5a031984e3a751540b8482a6595"},"cell_type":"code","source":"relevances.sort_values(by='relevance').head(20).merge(movies, on='movieId', how='left').merge(tagIDs, on='tagId', how='left').rename(index=lambda x: x+1)[['tag', 'title', 'relevance']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c85223c5f7881d35497db8423f69ae2eb18c0bf3"},"cell_type":"markdown","source":"## Dataset sampling\nI will use subsampling for performance reasons as well as train/test division for evaluation.\n\n### Subsampling\n#### MovieLens 20M `full`\nNo subsampling\n#### MovieLens Latest Small 100k `official_small`\nOfficial release of subsampled current MovieLens database with 100,000 ratings.\n#### 500 Most rated movies `most_rated`\nMovieLens 20M subsampled for 500 most rated movies.\n#### 1K Most active users `most_active`\nMovieLens 20M subsampled for 1,000 most active users.\n#### Random subsample 100K `random`\nMovieLens 20M randomly subsampled, not stratified. Will resample every time it is demanded.\n\nSampling methods are stored  in `Sampler` class with `sampler` instance. "},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"6714ebbb1b2f4ade79a9fb16b5d6fa7470955cda"},"cell_type":"code","source":"class Sampler():\n    def sample_full(self):\n        return ratings\n    def sample_latest(self):\n        return ratings100k\n    ratings_rated = None\n    def sample_rated(self):\n        if self.ratings_rated is None:\n            most_rated = ratings.groupby(by='movieId').count().sort_values(by='userId').tail(500).index\n            self.ratings_rated = ratings[ratings.movieId.isin(most_rated)]\n        return self.ratings_rated\n    ratings_active = None\n    def sample_active(self):\n        if self.ratings_active is None:\n            most_active = ratings.groupby(by='userId').count().sort_values(by='movieId').tail(1000).index\n            self.ratings_active = ratings[ratings.userId.isin(most_active)]\n        return self.ratings_active\n    def sample_random(self, sample_size=100000):\n        return ratings.sample(sample_size)\n    samplers = {'full': sample_full, 'official_small': sample_latest, 'most_rated': sample_rated, \\\n                'most_active': sample_active, 'random': sample_random}\n\nsampler = Sampler()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"9369bb6e2e9d2e03b90b97ad2fa4130b3279b9fc"},"cell_type":"code","source":"print(f\"'rated' dataset contains {len(sampler.samplers['most_rated'](sampler)):,} ratings.\")\nprint(f\"'active' dataset contains {len(sampler.samplers['most_active'](sampler)):,} ratings.\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6c1069ec5d437421b65f551d2bfb1b352b3e00c"},"cell_type":"markdown","source":"### Train/Test division\n#### Time-based division `time`\nDivide dataset by a timestamp with 80/20 train/test ratio\n#### Last 10 ratings `last_ratings`\nTest set contains last 10 ratings for each user.\n#### User-based division `users`\nTest set contains randomly selected 20% of users.\n\nDivision methods are stored in `Divider` class as static methods."},{"metadata":{"trusted":true,"_uuid":"c391e21caae3b02798586e4419805ea0e83d392f","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"class Divider():\n    @staticmethod\n    def divide_time(data):\n        div_time = data['timestamp'].quantile(0.8)\n        train = data[data['timestamp'] <= div_time]\n        test = data[data['timestamp'] > div_time].copy()\n        return train, test\n    @staticmethod\n    def divide_users(data):\n        u = np.random.choice(data.userId.unique(), int(data.userId.nunique()*0.2))\n        train = data[~data.userId.isin(u)]\n        test = data[data.userId.isin(u)].copy()\n        return train, test\n    @staticmethod\n    def divide_ratings(data):\n        rank = data.groupby('userId').timestamp.rank(method='first', ascending=False)\n        train = data[rank > 10]\n        test = data[rank <= 10].copy()\n        return train, test\n    dividers = {'time': divide_time.__func__, 'users': divide_users.__func__, 'last_ratings': divide_ratings.__func__}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af7ec01aaebbd7ff694bb420d615695d894597ad","_kg_hide-input":true},"cell_type":"code","source":"print('Division by time.')\nfor s in sampler.samplers:\n    print(f'Sampling type: {s}')\n    data = sampler.samplers[s](sampler)\n    train, test = Divider.dividers['time'](data)\n    new_users = set(test['userId']) - set(train['userId'])\n    new_items = set(test['movieId']) - set(train['movieId'])\n    print('Test subset is {:,} ratings long. It contains {} days of data.\\n\\\nTest subset contains {:,} new users who created {:.4} % of all test ratings.\\n\\\nTest subset contains {:,} new movies that correspond to {:.4} % of all test ratings.'\\\n          .format(len(test), (test['timestamp'].max() - test['timestamp'].min()).days, \\\n                  len(new_users), len(test[test['userId'].isin(new_users)])/len(test)*100,\\\n                  len(new_items), len(test[test['movieId'].isin(new_items)])/len(test)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e6d9686a5537d52622296d345a3c5a1d9c6fdff","_kg_hide-input":true},"cell_type":"code","source":"test_set_sizes = pd.DataFrame(index=sampler.samplers, columns=Divider.dividers)\nfor s in sampler.samplers:\n    for d in Divider.dividers:\n        data = sampler.samplers[s](sampler)\n        train, test = Divider.dividers[d](data)\n        test_set_sizes.at[s,d] = len(test)/len(data)*100\ntest_set_sizes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d7ef548b812844052b301f6aafddc140f5eca14"},"cell_type":"markdown","source":"## Evaluation methods\n#### Timing\nI will use Python's built-in time.time() method on start and end of every execution to track time elapsed when executing.\n#### Metric\nI will use RMSE metric to evaluate models. RMSE = numpy.sqrt(sklearn.metrics.mean_squared_error(true, predicted)). \nScikit-learn implementation is about 2 times faster than simple np.sqrt(((true - predicted) ** 2).mean()) for my test data, even though times are small anyway.\n"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"6bee517506267d113bd707f42ee2821c087224d4"},"cell_type":"code","source":"r = (4.5 * np.random.random_sample((len(test),))) + 0.5\ndef RMSE(true, predicted):\n    return np.sqrt(MSE(true, predicted))\ndef RMSE1(true, predicted):\n    return np.sqrt(((true - predicted) ** 2).mean())\nstart = time.time()\nrmse = RMSE1(test['rating'], r)\nprint(f'took {time.time()-start} seconds for numpy. RMSE = {rmse}')\nstart = time.time()\nrmse = RMSE(test['rating'], r)\nprint(f'took {time.time()-start} seconds for sklearn. RMSE = {rmse}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c37eb68a1bc17e4bb7b8285668011980214080f"},"cell_type":"markdown","source":"## Recommendation models\nAll results (RMSE scores) will be stored in `results` DataFrame with sampling and division methods as index and recommendation models as columns."},{"metadata":{"trusted":true,"_uuid":"33f20397865844cf53caa36729445663b128a5bd"},"cell_type":"code","source":"methods = pd.MultiIndex.from_product([sampler.samplers, Divider.dividers], names=['sampling methods', 'division methods'])\nmodels = ['random', 'user_avg', 'item_avg', 'item_CF', 'item_CF_lenskit']\nresults = pd.DataFrame(index=models, columns=methods)\nresults","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85907914c9f30907a7f546a054ef1a9c0f7653a5"},"cell_type":"markdown","source":"### Random rating\nSimplest model, predict random rating from [0.5, 5) interval."},{"metadata":{"trusted":true,"_uuid":"73677a0827416585098daa0004417463b366724f","_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"start = time.time()\nfor s in sampler.samplers:\n    for d in Divider.dividers:\n        print(f'Computing {s} sampler and {d} divider.')\n        data = sampler.samplers[s](sampler)\n        train, test = Divider.dividers[d](data)\n        r = (4.5 * np.random.random_sample((len(test),))) + 0.5\n        rmse = RMSE(test['rating'], r)\n        results.loc['random',(s,d)] = rmse\nprint(f\"Computation took {time.time() - start:.6} seconds.\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"9cc74f949dd6aaf9afeb795d34bfc6619b412204"},"cell_type":"code","source":"results","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"498e009aa3bdf0937a31a6e3d3aea0cbbd0fd2a6"},"cell_type":"markdown","source":"### User average\n\nPredict average rating per user. Predict overall average rating for new users."},{"metadata":{"trusted":true,"_uuid":"b41688e5f6e81ab8298ff6444f18e054f7f0592a","_kg_hide-output":true,"scrolled":false},"cell_type":"code","source":"start = time.time()\nfor s in sampler.samplers:\n    if s != 'official_small':  # for performance reasons when submitting\n        continue\n    for d in Divider.dividers:\n        print(f'Computing {s} sampler and {d} divider.')\n        data = sampler.samplers[s](sampler)\n        train, test = Divider.dividers[d](data)\n        global_average = train['rating'].mean()\n        users_average = train.groupby(by='userId')['rating'].mean()\n        test['predicted'] = np.repeat(global_average, len(test))  # global average fallback\n        users = np.intersect1d(users_average.index, test.userId.unique(), assume_unique=True)  # will be empty for users divider\n        c = 0\n        p = 0\n        step = len(users)/10\n        print('[__________]')\n        for u in users:\n            c += 1\n            if c >= (p+1)*step:\n                p += 1\n                print('[' + '#'*p + '_'*(10-p) + ']')\n            test.loc[test['userId'] == u,'predicted'] = users_average[u]\n        rmse = RMSE(test['rating'], test['predicted'])\n        results.loc['user_avg',(s,d)] = rmse\nprint(f\"Computation took {time.time() - start:.6} seconds.\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"f5baac6cb97727e91e2bfda675c204f548b7ca06"},"cell_type":"code","source":"results","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4076b3b9531851f8d00e5ca596dcb9135399f18a"},"cell_type":"markdown","source":"Best score was achieved with `last_ratings` division methods, which is expeceted due to the fact that we have all users from train set in test set."},{"metadata":{"trusted":true,"_uuid":"a444d92f5d8d9f29ea0f3e62fd87522cbb6dfba5"},"cell_type":"markdown","source":"### Item average\n\nPredict average rating per item. Predict genre average rating for new items. Predict overall average rating for unknown genres."},{"metadata":{"trusted":true,"_uuid":"72e795dd0808e94537faa1df8ca190113f1b7cfc","_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"start = time.time()\nfor s in sampler.samplers:\n    if s != 'official_small':\n        continue\n    for d in Divider.dividers:\n        print(f'Computing {s} sampler and {d} divider.')\n        data = sampler.samplers[s](sampler)\n        train, test = Divider.dividers[d](data)\n        global_average = train['rating'].mean()\n        genres_average = train.merge(movies[['movieId', 'genres']]).groupby('genres').rating.mean()\n        items_average = train.groupby(by='movieId')['rating'].mean()\n        test['predicted'] = np.repeat(global_average, len(test))  # second fallback for unknown genres\n        test_items = test.movieId.unique()\n        c = 0\n        p = 0\n        step = len(test_items)/10\n        print('[__________]')\n        for i in np.intersect1d(test_items, items_average.index, assume_unique=True):  # predict item average\n            c+=1\n            if c >= (p+1)*step:\n                p += 1\n                print('[' + '#'*p + '_'*(10-p) + ']')\n            test.loc[test['movieId'] == i,'predicted'] = items_average[i]\n        for i in np.setdiff1d(test_items, items_average.index, assume_unique=True):  # predict genre average for new items\n            c+=1\n            if c >= (p+1)*step:\n                p += 1\n                print('[' + '#'*p + '_'*(10-p) + ']')\n            g = movies.loc[movies.movieId == i, 'genres']\n            if g.empty:  # unknown genre\n                continue\n            try:\n                a = genres_average[g.values[0]]\n            except KeyError:  # not a perfect genres match\n                a = genres_average.filter(like=g.values[0])  # try any more specific genres\n                if a.empty:\n                    a = global_average\n                    for j in g.values[0].split('|'):  # try subgenres\n                        try:\n                            a = (a + genres_average[j]) / 2\n                        except KeyError:\n                            continue\n                else:\n                    a = a.mean()\n            test.loc[test['movieId'] == i, 'predicted'] = a\n        rmse = RMSE(test['rating'], test['predicted'])\n        results.loc['item_avg',(s,d)] = rmse\nprint(f\"Computation took {time.time() - start:.6} seconds.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89adfe437ee16feae786189a219df84ad2995ed2","_kg_hide-input":true},"cell_type":"code","source":"results","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af014b84c2916c2e9fe0e6cbdb3e65bafdc21d64"},"cell_type":"markdown","source":"Highest improvement was achieved with `users` division method. It is expected thanks to the fact that users division contains radically smaller portion of test data created by new items (see list below). On the other hand with `last_ratings` division we completely ignore user's preference and even though there are even less new items, score is worse than with `user_avg` method."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"7f3b4e4704f7d19b0a210811ba013b6a767bdb25"},"cell_type":"code","source":"for s in sampler.samplers:\n    if s != 'official_small':\n        continue\n    for d in Divider.dividers:\n        print(f'Computing {s} sampler and {d} divider.')\n        data = sampler.samplers[s](sampler)\n        train, test = Divider.dividers[d](data)\n        new_items = set(test['movieId']) - set(train['movieId'])\n        print(\"Number of new items:{}\\nData from test set created by new items: {:.4} %\".format(len(new_items), len(test[test['movieId'].isin(new_items)])/len(test)*100))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"016e7a084167da3da06f665292ed6df1234d4b41"},"cell_type":"markdown","source":"### Collaborative filtering\nFirst create user-item matrix. This is very sparse matrix therefore I use pandas.SparseDataFrame data structure created from scipy.sparse.csr_matrix (see e.g. [Stack Overflow thread](https://stackoverflow.com/questions/31661604/efficiently-create-sparse-pivot-tables-in-pandas) and [scipy docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix))\n\n#### Item-based CF\nUsing basic prediction furmula from slides \n![alt text](https://is.muni.cz/www/bielovico/pv254/itemCF.PNG \"Item-based Collaborative Filtering prediction\")\n\nI set `K=50` as a size of the neighborhood that should be checked and use cosine similarity measure.\n\nFirst fallback for unseen item is users' average. For unseen user, genre average is used. Last fallback is general average."},{"metadata":{"trusted":true,"_uuid":"2d14bfbe9a173f888ac54320e07d0c2fc8f0e39f","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"def itemCFpred(user, item, user_c, item_c, item_user, matrix_full):\n    i = np.argwhere(item_c.categories == item)[0][0]\n    u = np.argwhere(user_c.categories == user)[0][0]\n    rated = matrix_full.loc[:,matrix_full.loc[user,:].notna()].columns\n    irated = np.argwhere(item_c.categories.isin(rated)).flatten()\n    a = cos_sim(item_user[i,:],item_user[irated,:])[0]\n    k = min(len(rated), K)\n    ind = np.argpartition(a, -k)[-k:]\n    similarities = a[ind]\n    s = similarities.sum()\n    r = np.multiply(item_user[irated, u][ind].todense(), similarities.reshape(k,1)).sum()\n    return r/s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd8432c6c94f9df38be8dee9bd60bdfa171df8c4","_kg_hide-output":true,"scrolled":false},"cell_type":"code","source":"K = 50\n\nstart = time.time()\nfor s in sampler.samplers:\n    if s != 'official_small':\n        continue\n    for d in Divider.dividers:\n        print(f'Computing {s} sampler and {d} divider.')\n        data = sampler.samplers[s](sampler)\n        train, test = Divider.dividers[d](data)\n        global_average = train['rating'].mean()\n        users_average = train.groupby(by='userId')['rating'].mean()\n        items_average = train.groupby(by='movieId')['rating'].mean()\n        genres_average = train.merge(movies[['movieId', 'genres']]).groupby('genres').rating.mean()\n        # matrix_full = train.pivot(index='userId', columns='movieId', values='rating')  # too large\n        user_c = CategoricalDtype(sorted(train['userId'].unique()), ordered=True)\n        item_c = CategoricalDtype(sorted(train['movieId'].unique()), ordered=True)\n\n        row = train['userId'].astype(user_c).cat.codes\n        col = train['movieId'].astype(item_c).cat.codes\n        user_item = csr_matrix((train[\"rating\"], (row, col)), \\\n                                   shape=(user_c.categories.size, item_c.categories.size))\n        item_user = csr_matrix((train[\"rating\"], (col, row)), \\\n                                   shape=(item_c.categories.size, user_c.categories.size))\n        matrix_full = pd.SparseDataFrame(user_item, index=user_c.categories, columns=item_c.categories)\n        c = 0\n        p = 0\n        step = len(test)/50\n        print('[__________________________________________________]')\n        for row in test.itertuples():\n            c += 1\n            if c >= (p+1)*step:\n                p += 1\n                print('[' + '#'*p + '_'*(50-p) + ']')\n            if row.movieId not in matrix_full.columns:  # unseen item\n                try:\n                    ua = users_average.at[row.userId]\n                    test.at[row.Index, 'predicted'] = ua\n                except KeyError:  # and unseen user\n                    g = movies.loc[movies.movieId == i, 'genres']\n                    if g.empty:  # unknown genre\n                        test.at[row.Index, 'predicted'] = global_average                        \n                    try:\n                        a = genres_average[g.values[0]]\n                    except KeyError:  # not a perfect genres match\n                        a = genres_average.filter(like=g.values[0])  # try any more specific genres\n                        if a.empty:\n                            a = global_average\n                            for j in g.values[0].split('|'):  # try subgenres\n                                try:\n                                    a = (a + genres_average[j]) / 2\n                                except KeyError:\n                                    continue\n                        else:\n                            a = a.mean()\n                    test.at[row.Index, 'predicted'] = a\n            elif row.userId not in matrix_full.index:  #seen item but unseen user\n                test.at[row.Index, 'predicted'] = items_average[row.movieId]\n            else:  # seen both user and item; use CF\n                test.loc[row.Index, 'predicted'] = itemCFpred(row.userId, row.movieId, user_c, item_c, item_user, matrix_full)\n        test.predicted.fillna(global_average, inplace=True)  # to be fail-proof\n        rmse = RMSE(test['rating'], test['predicted'])\n        results.loc['item_CF',(s,d)] = rmse\nprint(f\"Computation took {time.time() - start:.6} seconds.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dda181bc7a796844a15b7327e6017ad962a13c76","_kg_hide-input":true,"scrolled":true},"cell_type":"code","source":"results","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c96ff32092efd86dac9f32d4cb8ac46c7671df4"},"cell_type":"markdown","source":"We can see improvement in both `time` and `last_ratings` division methods. No improvement was achieved with `users` method but this is dependant on random user selection. Cross-fold validation could help. "},{"metadata":{"trusted":true,"_uuid":"901c3fe12b0658066206077237bf8aa1bdd0f2c4"},"cell_type":"markdown","source":"#### lenskit\nlenskit is a python package created by the same group that created MovieLens. It consist of a few algorithms so far, started about half a year ago but is actively developed. [docs](https://lkpy.lenskit.org/en/latest/) and [GitHub](https://github.com/lenskit/lkpy)"},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"3695789f2c2be3df7d3ad3f777ec0d06233d015a"},"cell_type":"code","source":"from lenskit import batch\nfrom lenskit.algorithms import item_knn as knn\n\nalgo = knn.ItemItem(K)\n\nstart = time.time()\nfor s in sampler.samplers:\n    if s != 'official_small':\n        continue\n    for d in Divider.dividers:\n        print(f'Computing {s} sampler and {d} divider.')\n        data = sampler.samplers[s](sampler)\n        train, test = Divider.dividers[d](data)\n        train.rename(columns={'userId': 'user', 'movieId': 'item'}, inplace=True)\n        test.rename(columns={'userId': 'user', 'movieId': 'item'}, inplace=True)\n        global_average = train['rating'].mean()\n        print('training')\n        model = algo.train(train)\n        print('recommending')\n        recs = batch.predict(algo, test[['user', 'item']], model)\n        res = pd.merge(recs, test, how='left', on=('user', 'item'))\n        res.prediction.fillna(global_average, inplace=True)\n        rmse = RMSE(test['rating'], res['prediction'])\n        results.loc['item_CF_lenskit',(s,d)] = rmse\nprint(f\"Computation took {time.time() - start:.6} seconds.\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"7c5373befb83ced7374b989d8820b4894d927be2"},"cell_type":"code","source":"results","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"655a21a6bb4dd0de81b1ba374a15a64a7c315f5a"},"cell_type":"markdown","source":"Best performance was achieved with `last_ratings` method even though lenskit doesn't use users average. Performance with fallbacks is better on both `time` and `users` methods but lacks with `last_ratings` method. As seen with (almost) full table below, overall best performance was achieved when original dataset was subsampled only for most active users and put their last ratings into test set."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true,"_uuid":"6b21d60dd5bc71206de07964174403caf3560859"},"cell_type":"markdown","source":"![](https://is.muni.cz/www/bielovico/pv254/final_table1.PNG)\n![item_average](https://is.muni.cz/www/bielovico/pv254/itemAVG.PNG)\n![](https://is.muni.cz/www/bielovico/pv254/resources.PNG)"},{"metadata":{"_uuid":"1b2be6f1b855307ed0d26c36f9ce8bff0379dcae"},"cell_type":"markdown","source":"## Possible future work:\n* More exploratory analysis\n* Latent factors\n* Make use of tags\n* Cross-validation\n* Top n recommendation (nDCG metric)"},{"metadata":{"trusted":true,"_uuid":"8e8f742fca66c82bb329f2f217c5cd349b96bcce"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}