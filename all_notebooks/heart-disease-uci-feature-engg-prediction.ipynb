{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":28,"outputs":[{"output_type":"stream","text":"['heart.csv']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# 1.2 Feature creation libraries\nfrom sklearn.random_projection import SparseRandomProjection as sr  # Projection features\nfrom sklearn.cluster import KMeans                    # Cluster features\nfrom sklearn.preprocessing import PolynomialFeatures  # Interaction features\n\n# 1.3 For feature selection\n# Ref: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_classif  # Selection criteria\n\n# 1.4 Data processing\n# 1.4.1 Scaling data in various manner\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, scale\n# 1.4.2 Transform categorical (integer) to dummy\nfrom sklearn.preprocessing import OneHotEncoder\n\n# 1.5 Splitting data\nfrom sklearn.model_selection import train_test_split\n\n# 1.6 Decision tree modeling\n# http://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree\n# http://scikit-learn.org/stable/modules/tree.html#tree\nfrom sklearn.tree import  DecisionTreeClassifier as dt\n\n# 1.7 RandomForest modeling\nfrom sklearn.ensemble import RandomForestClassifier as rf\n\n# 1.8 Plotting libraries to plot feature importance\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 1.9 Misc\nimport os, time, gc","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2.1 Read train/test files\nheart = pd.read_csv(\"../input/heart.csv\") #Loading of Data\n\n# 2.2 Look at data\nheart.head(2)\nheart.shape                        \n\n","execution_count":30,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"(303, 14)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3. split the data\nX_train, X_test, y_train, y_test = train_test_split(heart.drop('target', 1), heart['target'], test_size = 0.3, random_state=10) \n\n# Check the splits\nX_train.shape       # 212 X 13\nX_test.shape        #  91 X 13\ny_train.shape       # (212,)\ny_test.shape        # ( 91,)\n\n\n# Check if there are Missing values? None\nX_train.isnull().sum().sum()  # 0\nX_test.isnull().sum().sum()   # 0","execution_count":31,"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"0"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  4. Feature 1: Row sums of features 1:13. More successful\n#                when data is binary.\n\nX_train['sum'] = X_train.sum(numeric_only = True, axis=1)  # numeric_only= None is default\nX_test['sum'] = X_test.sum(numeric_only = True,axis=1)\n\n# 4.1 Assume that value of '0' in a cell implies missing feature\n#     Transform train and test dataframes\n#     replacing '0' with NaN\n#     Use pd.replace()\ntmp_train = X_train.replace(0, np.nan)\ntmp_test = X_test.replace(0,np.nan)\n\n# 4.2 Check if tmp_train is same as train or is a view\n#     of train? That is check if tmp_train is a deep-copy\n\ntmp_train is X_train                # False\n#tmp_train is train.values.base    # False\ntmp_train._is_view                # False","execution_count":32,"outputs":[{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"False"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4.3 Check if 0 has been replaced by NaN\ntmp_train.head(1)\ntmp_test.head(1)","execution_count":33,"outputs":[{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"     age  sex  cp  trestbps  chol  ...    oldpeak  slope   ca  thal    sum\n246   56  NaN NaN       134   409  ...        1.9    1.0  2.0     3  757.9\n\n[1 rows x 14 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>sex</th>\n      <th>cp</th>\n      <th>trestbps</th>\n      <th>chol</th>\n      <th>fbs</th>\n      <th>restecg</th>\n      <th>thalach</th>\n      <th>exang</th>\n      <th>oldpeak</th>\n      <th>slope</th>\n      <th>ca</th>\n      <th>thal</th>\n      <th>sum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>246</th>\n      <td>56</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>134</td>\n      <td>409</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>150</td>\n      <td>1.0</td>\n      <td>1.9</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>3</td>\n      <td>757.9</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 5. Feature 2 : For every row, how many features exist\n#                that is are non-zero/not NaN.\n#                Use pd.notna()\ntmp_train.notna().head(1)\nX_train[\"count_not0\"] = tmp_train.notna().sum(axis = 1)\nX_test['count_not0'] = tmp_test.notna().sum(axis = 1)","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 6. Similary create other statistical features\n#    Feature 3\n#    Pandas has a number of statistical functions\n#    Ref: https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#computations-descriptive-stats\n\nfeat = [ \"var\", \"median\", \"mean\", \"std\", \"max\", \"min\"]\nfor i in feat:\n    X_train[i] = tmp_train.aggregate(i,  axis =1)\n    X_test[i]  = tmp_test.aggregate(i,axis = 1)","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 7 Delete not needed variables and release memory\ndel(tmp_train)\ndel(tmp_test)\ngc.collect()\n\n\n# 8. So what do we have finally\nX_train.shape                # 212 X (13 + 8)\nX_train.head(1)\nX_test.shape                 #  91 X (13 + 8)\nX_test.head(2)","execution_count":36,"outputs":[{"output_type":"execute_result","execution_count":36,"data":{"text/plain":"     age  sex  cp  trestbps ...         mean         std    max  min\n246   56    0   0       134 ...   151.580000  248.448308  757.9  1.0\n183   58    1   2       112 ...   104.636364  175.162509  575.5  1.0\n\n[2 rows x 21 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>sex</th>\n      <th>cp</th>\n      <th>trestbps</th>\n      <th>chol</th>\n      <th>fbs</th>\n      <th>restecg</th>\n      <th>thalach</th>\n      <th>exang</th>\n      <th>oldpeak</th>\n      <th>slope</th>\n      <th>ca</th>\n      <th>thal</th>\n      <th>sum</th>\n      <th>count_not0</th>\n      <th>var</th>\n      <th>median</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>max</th>\n      <th>min</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>246</th>\n      <td>56</td>\n      <td>0</td>\n      <td>0</td>\n      <td>134</td>\n      <td>409</td>\n      <td>0</td>\n      <td>0</td>\n      <td>150</td>\n      <td>1</td>\n      <td>1.9</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>757.9</td>\n      <td>10</td>\n      <td>61726.561778</td>\n      <td>29.5</td>\n      <td>151.580000</td>\n      <td>248.448308</td>\n      <td>757.9</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>183</th>\n      <td>58</td>\n      <td>1</td>\n      <td>2</td>\n      <td>112</td>\n      <td>230</td>\n      <td>0</td>\n      <td>0</td>\n      <td>165</td>\n      <td>0</td>\n      <td>2.5</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>575.5</td>\n      <td>11</td>\n      <td>30681.904545</td>\n      <td>3.0</td>\n      <td>104.636364</td>\n      <td>175.162509</td>\n      <td>575.5</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 8. Store column names of our data somewhere\n#     We will need these later (at the end of this code)\ncolNames = X_train.columns.values\ncolNames","execution_count":37,"outputs":[{"output_type":"execute_result","execution_count":37,"data":{"text/plain":"array(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',\n       'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'sum',\n       'count_not0', 'var', 'median', 'mean', 'std', 'max', 'min'],\n      dtype=object)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"################ Feature creation Using Random Projections ##################\n# 9. Random projection is a fast dimensionality reduction feature\n#     Also used to look at the structure of data\n\n# 10. Generate features using random projections\n#     First stack train and test data, one upon another\ntmp = pd.concat([X_train,X_test],\n                axis = 0,            # Stack one upon another (rbind)\n                ignore_index = True\n                )\n\n\n# 10.1\ntmp.shape     # 303 X 21","execution_count":38,"outputs":[{"output_type":"execute_result","execution_count":38,"data":{"text/plain":"(303, 21)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# 10.2 Transform tmp t0 numpy array\n#      Henceforth we will work with array only\ntmp = tmp.values\ntmp.shape       # (303, 21)","execution_count":41,"outputs":[{"output_type":"execute_result","execution_count":41,"data":{"text/plain":"(303, 21)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 11. Let us create 5 random projections/columns\n#     This decision, at present, is arbitrary\nNUM_OF_COM = 5\n\n# 11.1 Create an instance of class\nrp_instance = sr(n_components = NUM_OF_COM)\n\n# 11.2 fit and transform the (original) dataset\n#      Random Projections with desired number\n#      of components are returned\nrp = rp_instance.fit_transform(tmp[:, :13])\n\n# 11.3 Look at some features\nrp[: 5, :  3]\n\n\n# 11.4 Create some column names for these columns\n#      We will use them at the end of this code\nrp_col_names = [\"r\" + str(i) for i in range(5)]\nrp_col_names","execution_count":42,"outputs":[{"output_type":"execute_result","execution_count":42,"data":{"text/plain":"['r0', 'r1', 'r2', 'r3', 'r4']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"############################ Feature creation using kmeans ####################\n######################Can be skipped without loss of continuity################\n\n# 12. Before clustering, scale data\n# 12.1 Create a StandardScaler instance\nse = StandardScaler()\n# 12.2 fit() and transform() in one step\ntmp = se.fit_transform(tmp)\n# 12.3\ntmp.shape               # 303 X 21 (an ndarray)\n","execution_count":43,"outputs":[{"output_type":"execute_result","execution_count":43,"data":{"text/plain":"(303, 21)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 13. Perform kmeans using 13 features.\n#     No of centroids is no of classes in the 'target'\ncenters = y_train.nunique()    # 2 unique classes\ncenters ","execution_count":44,"outputs":[{"output_type":"execute_result","execution_count":44,"data":{"text/plain":"2"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 14.1 Begin clustering\nstart = time.time()\n\n# 14.2 First create object to perform clustering\nkmeans = KMeans(n_clusters=centers, # How many\n                n_jobs = 2)         # Parallel jobs for n_init\n\n\n\n# 14.3 Next train the model on the original data only\nkmeans.fit(tmp[:, : 13])\n\nend = time.time()\n(end-start)/60.0 ","execution_count":45,"outputs":[{"output_type":"execute_result","execution_count":45,"data":{"text/plain":"0.03480234940846761"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 15 Get clusterlabel for each row (data-point)\nkmeans.labels_\nkmeans.labels_.size   # 303","execution_count":46,"outputs":[{"output_type":"execute_result","execution_count":46,"data":{"text/plain":"303"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 16. Cluster labels are categorical. So convert them to dummy\n\n# 16.1 Create an instance of OneHotEncoder class\nohe = OneHotEncoder(sparse = False)\n\n# 16.2 Use ohe to learn data\n#      ohe.fit(kmeans.labels_)\nohe.fit(kmeans.labels_.reshape(-1,1))     # reshape(-1,1) recommended by fit()\n                                          # '-1' is a placeholder for actual\n# 16.3 Transform data now\ndummy_clusterlabels = ohe.transform(kmeans.labels_.reshape(-1,1))\ndummy_clusterlabels\ndummy_clusterlabels.shape    # 303 X 2 (as many as there are classes)\n\n# 16.4 We will use the following as names of new nine columns\n#      We need them at the end of this code\n\nk_means_names = [\"k\" + str(i) for i in range(2)]\nk_means_names","execution_count":47,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\nIf you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\nIn case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n  warnings.warn(msg, FutureWarning)\n","name":"stderr"},{"output_type":"execute_result","execution_count":47,"data":{"text/plain":"['k0', 'k1']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"############################ Interaction features #######################\n# 15. Will require lots of memory if we take large number of features\n#     Best strategy is to consider only impt features\n\ndegree = 2\npoly = PolynomialFeatures(degree,                 # Degree 2\n                          interaction_only=True,  # Avoid e.g. square(a)\n                          include_bias = False    # No constant term\n                          )\n\n\n# 15.1 Consider only first 5 features\n#      fit and transform\ndf =  poly.fit_transform(tmp[:, : 5])\n\n\ndf.shape     # 303 X 15","execution_count":48,"outputs":[{"output_type":"execute_result","execution_count":48,"data":{"text/plain":"(303, 15)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 15.2 Generate some names for these 15 columns\npoly_names = [ \"poly\" + str(i)  for i in range(15)]\npoly_names","execution_count":49,"outputs":[{"output_type":"execute_result","execution_count":49,"data":{"text/plain":"['poly0',\n 'poly1',\n 'poly2',\n 'poly3',\n 'poly4',\n 'poly5',\n 'poly6',\n 'poly7',\n 'poly8',\n 'poly9',\n 'poly10',\n 'poly11',\n 'poly12',\n 'poly13',\n 'poly14']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"################# concatenate all features now ##############################\n\n# 16 Append now all generated features together\n# 16 Append random projections, kmeans and polynomial features to tmp array\n\ntmp.shape          # 303 X 21\n\n#  16.1 If variable, 'dummy_clusterlabels', exists, stack kmeans generated\n#       columns also else not. 'vars()'' is an inbuilt function in python.\n#       All python variables are contained in vars().\n\nif ('dummy_clusterlabels' in vars()):               #\n    tmp = np.hstack([tmp,rp,dummy_clusterlabels, df])\nelse:\n    tmp = np.hstack([tmp,rp, df])       # No kmeans      <==\n\n\ntmp.shape          # 303 X 43   If no kmeans: (303, 21)\n\n# 16.2 Separate train and test\nX = tmp[: X_train.shape[0], : ]\nX.shape                             # 212 X 43 if no kmeans: (212, 21)\n\n# 16.3\ntest = tmp[X_train.shape[0] :, : ]\ntest.shape                         #  91 X 43; if no kmeans: (91, 21)\n\n# 16.4 Delete tmp\ndel tmp\ngc.collect()","execution_count":50,"outputs":[{"output_type":"execute_result","execution_count":50,"data":{"text/plain":"493"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"################## Model building #####################\n\n\n# 17. Split train into training and validation dataset\nX_train, X_test, y_train, y_test = train_test_split(\n                                                    X,\n                                                    y_train,\n                                                    test_size = 0.3)","execution_count":51,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape ","execution_count":52,"outputs":[{"output_type":"execute_result","execution_count":52,"data":{"text/plain":"(148, 43)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape ","execution_count":53,"outputs":[{"output_type":"execute_result","execution_count":53,"data":{"text/plain":"(64, 43)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 18 Decision tree classification\n# 18.1 Create an instance of class\nclf = dt(min_samples_split = 5,\n         min_samples_leaf= 5\n        )\n\nstart = time.time()\n# 18.2 Fit/train the object on training data\n#      Build model\nclf = clf.fit(X_train, y_train)\nend = time.time()\n(end-start)/60                     # 1 minute\n\n# 18.3 Use model to make predictions\nclasses = clf.predict(X_test)\n\n# 18.4 Check accuracy\n(classes == y_test).sum()/y_test.size ","execution_count":54,"outputs":[{"output_type":"execute_result","execution_count":54,"data":{"text/plain":"0.78125"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 19. Instantiate RandomForest classifier\nclf = rf(n_estimators=50)\n\n# 19.1 Fit/train the object on training data\n#      Build model\n\nstart = time.time()\nclf = clf.fit(X_train, y_train)\nend = time.time()\n(end-start)/60\n\n# 19.2 Use model to make predictions\nclasses = clf.predict(X_test)\n# 19.3 Check accuracy\n(classes == y_test).sum()/y_test.size ","execution_count":55,"outputs":[{"output_type":"execute_result","execution_count":55,"data":{"text/plain":"0.890625"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"################## Feature selection #####################\n\n##****************************************\n## Using feature importance given by model\n##****************************************\n\n# 20. Get feature importance\nclf.feature_importances_        # Column-wise feature importance\nclf.feature_importances_.size   # 43\n\n\n# 20.1 To our list of column names, append all other col names\n#      generated by random projection, kmeans (onehotencoding)\n#      and polynomial features\n#      But first check if kmeans was used to generate features\n\nif ('dummy_clusterlabels' in vars()):       # If dummy_clusterlabels labels are defined\n    colNames = list(colNames) + rp_col_names+ k_means_names + poly_names\nelse:\n    colNames = colNames = list(colNames) + rp_col_names +  poly_names      # No kmeans      <==\n\n# 20.1.1 So how many columns?\nlen(colNames)           # 43 with kmeans else 21\n\n# 20.2 Create a dataframe of feature importance and corresponding\n#      column names. Sort dataframe by importance of feature\nfeat_imp = pd.DataFrame({\n                   \"importance\": clf.feature_importances_ ,\n                   \"featureNames\" : colNames\n                  }\n                 ).sort_values(by = \"importance\", ascending=False)\n\n\nfeat_imp.shape                   # 43 X 2 ; without kmeans: (21,2)\nfeat_imp.head(20)\n\n\n# 20.3 Plot feature importance for first 20 features\ng = sns.barplot(x = feat_imp.iloc[  : 20 ,  1] , y = feat_imp.iloc[ : 20, 0])\ng.set_xticklabels(g.get_xticklabels(),rotation=90)","execution_count":56,"outputs":[{"output_type":"execute_result","execution_count":56,"data":{"text/plain":"[Text(0,0,'k1'),\n Text(0,0,'k0'),\n Text(0,0,'thalach'),\n Text(0,0,'poly2'),\n Text(0,0,'poly5'),\n Text(0,0,'cp'),\n Text(0,0,'r2'),\n Text(0,0,'oldpeak'),\n Text(0,0,'r0'),\n Text(0,0,'mean'),\n Text(0,0,'ca'),\n Text(0,0,'poly8'),\n Text(0,0,'poly13'),\n Text(0,0,'poly9'),\n Text(0,0,'r1'),\n Text(0,0,'poly6'),\n Text(0,0,'poly10'),\n Text(0,0,'poly12'),\n Text(0,0,'var'),\n Text(0,0,'age')]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"######### All the code below works  ###################\n\n\n# 21   Select top 13 columns and get their indexes\n#      Note that in the selected list few kmeans\n#      columns also exist\nnewindex = feat_imp.index.values[:13]\nnewindex\n\n\n# 22 Use these top 13 columns for classification\n# 22.1  Create classifier object\nclf = dt(min_samples_split = 5, min_samples_leaf= 5)\n# 22.2 Traion the object on data\nstart = time.time()\nclf = clf.fit(X_train[: , newindex], y_train)\nend = time.time()\n(end-start)/60                     # 1 minute\n\n\n# 22.3  Make prediction\nclasses = clf.predict(X_test[: , newindex])\n# 22.4 Accuracy?\n(classes == y_test).sum()/y_test.size","execution_count":57,"outputs":[{"output_type":"execute_result","execution_count":57,"data":{"text/plain":"0.875"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}