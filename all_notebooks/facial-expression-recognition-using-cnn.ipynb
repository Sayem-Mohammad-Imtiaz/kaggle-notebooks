{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom keras.utils import to_categorical\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom keras.losses import categorical_crossentropy\nfrom sklearn.metrics import accuracy_score\nfrom keras.optimizers import Adam, SGD\nfrom keras.regularizers import l2\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/facialexpressionrecognition/fer2013.csv')\n#check data shape\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check usage values\n#80% training, 10% validation and 10% test\ndata.Usage.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check target labels\nemotion_map = {0: 'Angry', 1: 'Digust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}\nemotion_counts = data['emotion'].value_counts(sort=False).reset_index()\nemotion_counts.columns = ['emotion', 'number']\nemotion_counts['emotion'] = emotion_counts['emotion'].map(emotion_map)\nemotion_counts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting a bar graph of the class distributions\nplt.figure(figsize=(6,4))\nsns.barplot(emotion_counts.emotion, emotion_counts.number)\nplt.title('Class distribution')\nplt.ylabel('Number', fontsize=12)\nplt.xlabel('Emotions', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def row2image(row):\n    pixels, emotion = row['pixels'], emotion_map[row['emotion']]\n    img = np.array(pixels.split())\n    img = img.reshape(48,48)\n    image = np.zeros((48,48,3))\n    image[:,:,0] = img\n    image[:,:,1] = img\n    image[:,:,2] = img\n    return np.array([image.astype(np.uint8), emotion])\n\nplt.figure(0, figsize=(16,10))\nfor i in range(1,8):\n    face = data[data['emotion'] == i-1].iloc[0]\n    img = row2image(face)\n    plt.subplot(2,4,i)\n    plt.imshow(img[0])\n    plt.title(img[1])\n\nplt.show()  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split data into training, validation and test set\ndata_train = data[data['Usage']=='Training'].copy()\ndata_val   = data[data['Usage']=='PublicTest'].copy()\ndata_test  = data[data['Usage']=='PrivateTest'].copy()\nprint(\"Train shape: {}, \\nValidation shape: {}, \\nTest shape: {}\".format(data_train.shape, data_val.shape, data_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# barplot class distribution of train, val and test\nemotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n\ndef setup_axe(axe,df,title):\n    df['emotion'].value_counts(sort=False).plot(ax=axe, kind='bar', rot=0)\n    axe.set_xticklabels(emotion_labels)\n    axe.set_xlabel(\"Emotions\")\n    axe.set_ylabel(\"Number\")\n    axe.set_title(title)\n    \n    # set individual bar lables using above list\n    for i in axe.patches:\n        # get_x pulls left or right; get_height pushes up or down\n        axe.text(i.get_x()-.05, i.get_height()+120, \\\n                str(round((i.get_height()), 2)), fontsize=14, color='dimgrey',\n                    rotation=0)\n\n   \nfig, axes = plt.subplots(1,3, figsize=(20,8), sharey=True)\nsetup_axe(axes[0],data_train,'train')\nsetup_axe(axes[1],data_val,'validation')\nsetup_axe(axes[2],data_test,'test')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#initilize parameters\nnum_classes = 7 \nwidth, height = 48, 48\nnum_epochs = 100\nbatch_size = 64\nnum_features = 64","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CRNO stands for Convert, Reshape, Normalize, One-hot encoding\n# (i) convert strings to lists of integers\n# (ii) reshape and normalise grayscale image with 255.0\n# (iii) one-hot encoding label, e.g. class 3 to [0,0,0,1,0,0,0]\n\ndef CRNO(df, dataName):\n    df['pixels'] = df['pixels'].apply(lambda pixel_sequence: [int(pixel) for pixel in pixel_sequence.split()])\n    data_X = np.array(df['pixels'].tolist(), dtype='float32').reshape(-1,width, height,1)/255.0   \n    data_Y = to_categorical(df['emotion'], num_classes)  \n    print(dataName, \"_X shape: {}, \", dataName, \"_Y shape: {}\".format(data_X.shape, data_Y.shape))\n    return data_X, data_Y\n\n    \ntrain_X, train_Y = CRNO(data_train, \"train\") #training data\nval_X, val_Y     = CRNO(data_val, \"val\") #validation data\ntest_X, test_Y   = CRNO(data_test, \"test\") #test data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Conv2D(num_features, kernel_size=(3, 3), activation='relu', input_shape=(width, height, 1), data_format='channels_last', kernel_regularizer=l2(0.01)))\nmodel.add(Conv2D(num_features, kernel_size=(3, 3), activation='relu', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\nmodel.add(Dropout(0.5))\n\nmodel.add(Conv2D(2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\nmodel.add(Dropout(0.5))\n\nmodel.add(Conv2D(2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\nmodel.add(Dropout(0.5))\n\nmodel.add(Conv2D(2*2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(2*2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\nmodel.add(Dropout(0.5))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(2*2*2*num_features, activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(2*2*num_features, activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(2*num_features, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', \n              optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7), \n              metrics=['accuracy'])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data generator\ndata_generator = ImageDataGenerator(\n                        featurewise_center=False,\n                        featurewise_std_normalization=False,\n                        rotation_range=10,\n                        width_shift_range=0.1,\n                        height_shift_range=0.1,\n                        zoom_range=.1,\n                        horizontal_flip=True)\ndatagenerator = data_generator.flow(train_X, train_Y, batch_size)\n\n\n# train_generator=data_generator.flow_from_dataframe(\n#     dataframe=data, \n#     directory=\"..output/working/train_images\", \n#     x_col=\"pixels\", \n#     y_col=\"emotion\", \n#     class_mode=\"raw\", \n#     target_size=(48,48), batch_size=64)\nprint(len(datagenerator.y), len(train_X))\n# print('Num of samples: ', datagenerator.samples)\n# print('Number of classes', len(datagenerator.class_indices))\n# print('Number of samples per class: ', int(datagenerator.samples / len(datagenerator.class_indices)))\n\n\nes = EarlyStopping(monitor='val_loss', patience = 100, mode = 'min', restore_best_weights=True)\n\nhistory = model.fit_generator(data_generator.flow(train_X, train_Y, batch_size),\n                                steps_per_epoch=len(train_X) / batch_size,\n                                epochs=num_epochs,\n                                verbose=2, \n                                callbacks = [es],\n                                validation_data=(val_X, val_Y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1,2, figsize=(18, 6))\n# Plot training & validation accuracy values\naxes[0].plot(history.history['accuracy'])\naxes[0].plot(history.history['val_accuracy'])\naxes[0].set_title('Model accuracy')\naxes[0].set_ylabel('Accuracy')\naxes[0].set_xlabel('Epoch')\naxes[0].legend(['Train', 'Validation'], loc='upper left')\n\n# Plot training & validation loss values\naxes[1].plot(history.history['loss'])\naxes[1].plot(history.history['val_loss'])\naxes[1].set_title('Model loss')\naxes[1].set_ylabel('Loss')\naxes[1].set_xlabel('Epoch')\naxes[1].legend(['Train', 'Validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_true = np.argmax(test_Y, axis=1)\ntest_pred = np.argmax(model.predict(test_X), axis=1)\nprint(\"CNN Model Accuracy on test set: {:.4f}\".format(accuracy_score(test_true, test_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = classes\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        #print(\"Normalized confusion matrix\")\n    #else:\n        #print('Confusion matrix, without normalization')\n\n    #print(cm)\n\n    fig, ax = plt.subplots(figsize=(12,6))\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot normalized confusion matrix\nplot_confusion_matrix(test_true, test_pred, classes=emotion_labels, normalize=True, title='Normalized confusion matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing import image\nimport numpy as np\n\nimg = image.load_img('../input/abcccc/download (1).jpg', grayscale=True, target_size=(48, 48))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis = 0)\n\nx /= 255\n\ncustom = model.predict(x)\n\nx = np.array(x, 'float32')\nx = x.reshape([48, 48]);\n\nplt.gray()\nplt.imshow(img)\nplt.show()\n\nm = 0.000000000000000000001\na = custom[0]\nfor i in range(0,len(a)):\n    if a[i] > m:\n        m = a[i]\n        ind = i\n        \nprint('Expression Prediction:',emotion_map[ind])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n# serialize weights to HDF5\nmodel.save_weights(\"model.h5\")\nprint(\"Saved model to disk\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"json_file = open('./model.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n# load weights into new model\nloaded_model.load_weights(\"./model.h5\")\nprint(\"Loaded model from disk\")\n\nimport cv2\n\nlabels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n\n#loading image\n#full_size_image = cv2.imread(\"../input/abcccc/download (1).jpg\")\nfull_size_image = cv2.imread(\"../input/test-image/wallpaper2you_443897.jpg\")\nprint(\"Image Loaded\")\ngray=cv2.cvtColor(full_size_image,cv2.COLOR_RGB2GRAY)\nface = cv2.CascadeClassifier('../input/fronttalface/haarcascade_frontalface_default.xml')\nfaces = face.detectMultiScale(gray, 1.3  , 4)\n\n#detecting faces\nfor (x, y, w, h) in faces:\n        roi_gray = gray[y:y + h, x:x + w]\n        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray, (48, 48)), -1), 0)\n        cv2.normalize(cropped_img, cropped_img, alpha=0, beta=1, norm_type=cv2.NORM_L2, dtype=cv2.CV_32F)\n        cv2.rectangle(full_size_image, (x, y), (x + w, y + h), (0, 255, 0), 3)\n        #predicting the emotion\n        yhat= loaded_model.predict(cropped_img)\n        #cv2.putText(full_size_image, labels[int(np.argmax(yhat))], (x, y), cv2.FONT_HERSHEY_SIMPLEX, 10, (255, 0, 0),10, cv2.LINE_AA)\n        print(\"Emotion: \"+labels[int(np.argmax(yhat))])\nplt.imshow(full_size_image)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}