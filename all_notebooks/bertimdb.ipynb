{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport transformers\nimport torch\nimport torch.nn as nn\nfrom tqdm import tqdm\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection\nfrom sklearn import metrics\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"DEVICE = \"cuda\"\nMAX_LEN = 64\nTRAIN_BATCH_SIZE = 8\nVALID_BATCH_SIZE = 4\nEPOCHS = 10\nBERT_PATH = \"bert-base-uncased\"\nMODEL_PATH = \"../input/bert-base-uncased/pytorch_model.bin\"\nTRAINING_FILE = \"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\"\nTOKENIZER = transformers.BertTokenizer.from_pretrained(BERT_PATH, do_lower_case=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertBaseUncased(nn.Module):\n    def __init__(self):\n        super (BertBaseUncased,self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(BERT_PATH)\n        self.drop_out = nn.Dropout(0.2)\n        self.out = nn.Linear(768,1)\n        \n    def forward(self,ids,mask,token_type_ids):\n        _,o2 = self.bert(ids, attention_mask = mask, token_type_ids = token_type_ids)\n        bo = self.drop_out(o2)\n        output = self.out(bo)\n        \n        return output\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DataLoader","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class BERTDataset:\n    def __init__(self, review, target):\n        self.review = review\n        self.target = target\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n        \n    def __len__(self):\n        return len(self.review)\n    \n    def __getitem__(self,item):\n        review = str(self.review[item])\n        review = \" \".join(review.split())\n        \n        inputs = self.tokenizer.encode_plus(\n            review,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True)\n        \n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs['token_type_ids']\n        \n        \n        return {\n            \"ids\" : torch.tensor(ids, dtype=torch.long),\n            \"mask\" : torch.tensor(mask,dtype=torch.long),\n            \"token_type_ids\" : torch.tensor(token_type_ids,dtype=torch.long),\n            \"targets\" : torch.tensor(self.target[item], dtype=torch.float)\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_fn(outputs, targets):\n    return nn.BCEWithLogitsLoss()(outputs, targets.view(-1,1))\n\ndef train_fn(data_loader, model, optimizer, device, schdeular):\n    model.train()\n    \n    for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        targets = d[\"targets\"]\n        \n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets = targets.to(device, dtype=torch.float)\n        \n        optimizer.zero_grad()\n        outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n        \n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        \n        optimizer.step()\n        schdeular.step()\n        \ndef eval_fun(data_loader, model, device):\n    model.eval()\n    fin_targets = []\n    fin_outputs = []\n    \n    with torch.no_grad(): \n        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n            ids = d[\"ids\"]\n            token_type_ids = d[\"token_type_ids\"]\n            mask = d[\"mask\"]\n            targets = d[\"targets\"]\n        \n            ids = ids.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            targets = targets.to(device, dtype=torch.float)\n        \n            outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n        \n            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n    return fin_outputs, fin_targets","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(TRAINING_FILE).fillna(\"none\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sentiment = df.sentiment.apply(lambda x: 1 if x == \"positive\" else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train, df_valid = model_selection.train_test_split(\n        df, test_size=0.1, random_state=42, stratify=df.sentiment.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train.reset_index(drop=True)\ndf_valid = df_valid.reset_index(drop=True)\n\ntrain_dataset = BERTDataset(review = df_train.review.values, target = df_train.sentiment.values)\ntrain_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size= TRAIN_BATCH_SIZE, num_workers = 4)\n\nvalid_dataset = BERTDataset(review = df_valid.review.values, target = df_valid.sentiment.values)\nvalid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=VALID_BATCH_SIZE, num_workers = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(DEVICE)\nmodel = BertBaseUncased()\nmodel.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_optimizer = list(model.named_parameters())\nno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\noptimizer_parameters = [\n        {\n            \"params\": [\n                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n            ],\n            \"weight_decay\": 0.001,\n        },\n        {\n            \"params\": [\n                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n            ],\n            \"weight_decay\": 0.0,\n        },\n    ]\n\n\nnum_train_steps = int(len(df_train)) / TRAIN_BATCH_SIZE * EPOCHS\noptimizer = AdamW(optimizer_parameters, lr = 3e-5)\nschedular = get_linear_schedule_with_warmup(\n                optimizer,num_warmup_steps=0,num_training_steps=num_train_steps)\n\nbest_accurcy = 0\n\nfor epoch in range(2):\n    train_fn(train_data_loader, model, optimizer, device, schedular)\n    outputs, targets = eval_fun(valid_data_loader, model, device)\n    outputs = np.array(outputs) >= 0.5\n    accuracy = metrics.accuracy_score(targets, outputs)\n    print(f\"Accuracy Score = {accuracy}\")\n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}