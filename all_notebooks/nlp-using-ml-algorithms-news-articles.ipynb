{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Classification of News Articles \n\nIt is a notebook for multiclass classification of News articles which are having classes numbered 1 to 4, where \n>1 is \"World News\" <br>\n>2 is \"Sports News\" <br>\n>3 is \"Business News\" <br>\n>4 is \"Science-Technology News\"\n\nI have used various models for classification of the News articles. The classification algorithms used are:-\n\n1. Multinomial Naive Bayes\n2. Decision Tree \n3. Gaussian Naive Bayes\n4. Stochastic Gradient Descent Classifier\n5. LGBM (light gradient boosting machine) Classifier\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Importing libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport nltk\nimport string as s\nimport re\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom wordcloud import WordCloud,STOPWORDS\n\nfrom sklearn.feature_extraction.text  import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics  import f1_score,accuracy_score\nfrom sklearn.metrics import  confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier\nfrom lightgbm import LGBMClassifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data=pd.read_csv(\"/kaggle/input/ag-news-classification-dataset/train.csv\",header=0,names=['classid','title','desc'])\ntest_data=pd.read_csv(\"/kaggle/input/ag-news-classification-dataset/test.csv\",header=0,names=['classid','title','desc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Splitting Data into Input and Label ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x=train_data.desc[:70000]\ntest_x=test_data.desc\ntrain_y=train_data.classid[:70000] \ntest_y=test_data.classid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=train_data[:70000]\nsns.countplot(df.classid);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## WordCloud of News Articles of Different Types","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\nc_mask = np.array(Image.open(\"/kaggle/input/masks/comment.png\"))\nu_mask = np.array(Image.open(\"/kaggle/input/masks/upvote.png\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwordset= set(STOPWORDS)\nmorestop={'lt','gt','href','HREF','quot','aspx'}\nstopwordset= stopwordset.union(morestop)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### World News","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"world = df.desc[df.classid[df.classid==1].index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,20)) ;\nwordcloud = WordCloud(min_font_size = 3,  max_words = 500 ,background_color=\"white\",contour_width=3, contour_color='firebrick',mask=c_mask, stopwords=stopwordset,colormap='Dark2').generate(\" \".join(world))\nplt.imshow(wordcloud,interpolation = 'bilinear');\nplt.axis(\"off\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sports News","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sports = df.desc[df.classid[df.classid==2].index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,20)) ;\nwordcloud = WordCloud(min_font_size = 3,  max_words = 500 ,background_color=\"white\",contour_width=3, contour_color='skyblue',mask=u_mask, stopwords=stopwordset,colormap='Dark2').generate(\" \".join(sports))\nplt.imshow(wordcloud,interpolation = 'bilinear');\nplt.axis(\"off\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Business News","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"biz = df.desc[df.classid[df.classid==3].index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,20)) ;\nwordcloud = WordCloud(min_font_size = 3,  max_words = 500 ,background_color=\"white\",contour_width=3, contour_color='firebrick',mask=c_mask, stopwords=stopwordset,colormap='Dark2').generate(\" \".join(biz))\nplt.imshow(wordcloud,interpolation = 'bilinear');\nplt.axis(\"off\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Science and Technology News","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sci = df.desc[df.classid[df.classid==4].index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,20)) ;\nwordcloud = WordCloud(min_font_size = 3,  max_words = 500 ,background_color=\"white\",contour_width=3, contour_color='skyblue',mask=u_mask, stopwords=stopwordset,colormap='Dark2').generate(\" \".join(sci))\nplt.imshow(wordcloud,interpolation = 'bilinear');\nplt.axis(\"off\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing of Data\n\nThe data is preprocessed, in NLP it is also known as text normalization. Some of the most common methods of text normalization are \n* Tokenization\n* Lemmatization\n* Stemming\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Removal of URLs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_urls(text):\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)\n\ntrain_x=train_x.apply(remove_urls)\ntest_x=test_x.apply(remove_urls)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Removal of HTML tags","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_html(text):\n    html_pattern = re.compile('<.*?>')\n    return html_pattern.sub(r'', text)\n\ntrain_x=train_x.apply(remove_html)\ntest_x=test_x.apply(remove_html)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tokenization of Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def word_tokenize(txt):\n    tokens = re.findall(\"[\\w']+\", txt)\n    return tokens\ntrain_x=train_x.apply(word_tokenize)\ntest_x=test_x.apply(word_tokenize)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conversion of Data to Lowercase","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def lowercasing(lst):\n    new_lst=[]\n    for  i in  lst:\n        i=i.lower()\n        new_lst.append(i) \n    return new_lst\ntrain_x=train_x.apply(lowercasing)\ntest_x=test_x.apply(lowercasing)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Removal of Stopwords","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stopwords(lst):\n    stop=stopwords.words('english')\n    new_lst=[]\n    for i in lst:\n        if i not in stop:\n            new_lst.append(i)\n    return new_lst\n\ntrain_x=train_x.apply(remove_stopwords)\ntest_x=test_x.apply(remove_stopwords)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Removal of Punctuation Symbols","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_punctuations(lst):\n    new_lst=[]\n    for i in lst:\n        for  j in  s.punctuation:\n            i=i.replace(j,'')\n        new_lst.append(i)\n    return new_lst\ntrain_x=train_x.apply(remove_punctuations) \ntest_x=test_x.apply(remove_punctuations)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Removal of Numbers(digits)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_numbers(lst):\n    nodig_lst=[]\n    new_lst=[]\n\n    for i in  lst:\n        for j in  s.digits:\n            i=i.replace(j,'')\n        nodig_lst.append(i)\n    for i in  nodig_lst:\n        if  i!='':\n            new_lst.append(i)\n    return new_lst\ntrain_x=train_x.apply(remove_numbers)\ntest_x=test_x.apply(remove_numbers)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stemming of Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\n\ndef stemming(text):\n    porter_stemmer = nltk.PorterStemmer()\n    roots = [porter_stemmer.stem(each) for each in text]\n    return (roots)\n\ntrain_x=train_x.apply(stemming)\ntest_x=test_x.apply(stemming)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lemmatization of Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmatizer=nltk.stem.WordNetLemmatizer()\ndef lemmatzation(lst):\n    new_lst=[]\n    for i in lst:\n        i=lemmatizer.lemmatize(i)\n        new_lst.append(i)\n    return new_lst\ntrain_x=train_x.apply(lemmatzation)\ntest_x=test_x.apply(lemmatzation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_extrawords(lst):\n    stop=['href','lt','gt','ii','iii','ie','quot','com']\n    new_lst=[]\n    for i in lst:\n        if i not in stop:\n            new_lst.append(i)\n    return new_lst\n\ntrain_x=train_x.apply(remove_extrawords)\ntest_x=test_x.apply(remove_extrawords)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x=train_x.apply(lambda x: ''.join(i+' ' for i in x))\ntest_x=test_x.apply(lambda x: ''.join(i+' '  for i in x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Extraction\n \n Features are extracted from the dataset and TF-IDF(Term Frequency - Inverse Document Frequency) is used for this purpose.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf=TfidfVectorizer(min_df=3)\ntrain_1=tfidf.fit_transform(train_x)\ntest_1=tfidf.transform(test_x)\nprint(\"No. of features extracted\")\nprint(len(tfidf.get_feature_names()))\n\n\ntrain_arr=train_1.toarray()\ntest_arr=test_1.toarray()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training of Model\n\n### Model 1- Multinomial Naive Bayes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"NB_MN=MultinomialNB(alpha=0.16)\nNB_MN.fit(train_arr,train_y)\npred=NB_MN.predict(test_arr)\n\nprint(\"first 20 actual labels\")\nprint(test_y.tolist()[:20])\nprint(\"first 20 predicted labels\")\nprint(pred.tolist()[:20])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluation of Results","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Function for evaluation of model**\n\nThis function finds the F1-score and Accuracy of the trained model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_model(y,y_pred):\n    print(\"F1 score of the model\")\n    print(f1_score(y,y_pred,average='micro'))\n    print(\"Accuracy of the model\")\n    print(accuracy_score(y,y_pred))\n    print(\"Accuracy of the model in percentage\")\n    print(round(accuracy_score(y,y_pred)*100,3),\"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Function for Displaying the Confusion Matrix**\n\nThis function displays the confusion matrix of the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def confusion_mat(color):\n    cof=confusion_matrix(test_y, pred)\n    cof=pd.DataFrame(cof, index=[i for i in range(1,5)], columns=[i for i in range(1,5)])\n    sns.set(font_scale=1.5)\n    plt.figure(figsize=(8,8));\n\n    sns.heatmap(cof, cmap=color,linewidths=1, annot=True,square=True, fmt='d', cbar=False,xticklabels=['World','Sports','Business','Science'],yticklabels=['World','Sports','Business','Science']);\n    plt.xlabel(\"Predicted Classes\");\n    plt.ylabel(\"Actual Classes\");\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_model(test_y,pred)\n    \na=round(accuracy_score(test_y,pred)*100,3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_mat('YlGnBu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 2 - Decision Tree Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"DT=DecisionTreeClassifier()\nDT.fit(train_arr,train_y)\npred=DT.predict(test_arr)\n\nprint(\"first 20 actual labels\")\nprint(test_y.tolist()[:20])\nprint(\"first 20 predicted labels\")\nprint(pred.tolist()[:20])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluation of Results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_model(test_y,pred)\n    \nb=round(accuracy_score(test_y,pred)*100,3)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_mat('Blues')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 3 - Gaussian Naive Bayes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"NB=GaussianNB(var_smoothing=0.1)\nNB.fit(train_arr,train_y)\npred=NB.predict(test_arr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluation of Results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_model(test_y,pred)\n    \nc=round(accuracy_score(test_y,pred)*100,3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_mat('Greens')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 4 - Stochastic Gradient Descent Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"SGD=SGDClassifier(early_stopping=True,penalty='l2',alpha=0.00001)\nSGD.fit(train_arr,train_y)\npred=SGD.predict(test_arr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluation of Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_model(test_y,pred)\n    \nd=round(accuracy_score(test_y,pred)*100,3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_mat('Reds')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 5 - Light Gradient Boosting Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm=LGBMClassifier(learning_rate=0.35)\nlgbm.fit(train_arr,train_y)\npred=lgbm.predict(test_arr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluation of Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_model(test_y,pred)\n\ne=round(accuracy_score(test_y,pred)*100,3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_mat('YlOrBr')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Comparison of Accuracies of Different Models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nModels = ['MultinomialNB', 'DecisionTree', 'GaussianNB', 'SGD','LGBM']\nAccuracy=[a,b,c,d,e]\nax.bar(Models,Accuracy,color=['#702963','#8a2be2','#9966cc','#df73ff','#702763']);\nfor i in ax.patches:\n    ax.text(i.get_x()+.1, i.get_height()-5.5, str(round(i.get_height(),2))+'%', fontsize=15, color='white')\nplt.title('Comparison of Different Classification Models');\nplt.ylabel('Accuracy');\nplt.xlabel('Classification Models');\n\nplt.show();","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}