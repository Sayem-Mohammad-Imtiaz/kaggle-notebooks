{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Time, Series Analysis To Predict Sunspots"},{"metadata":{},"cell_type":"markdown","source":"<img src='https://www.almanac.com/sites/default/files/styles/primary_image_in_article/public/image_nodes/sunspots.jpg?itok=6Fx0Px0U' alt='Sunspots' width='500' height='500'>\n<br><br>\n<b>Sunspots</b> are areas that appear dark on the surface of the Sun. They appear dark because they are cooler than other parts of the Sun’s surface. The temperature of a sunspot is still very hot though — around 6,500 degrees Fahrenheit!<br>\nSunspots are used to keep track of the solar cycle. The solar cycle is the cycle that the Sun’s magnetic field goes through approximately every 11 years.","attachments":{}},{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nfrom statsmodels.graphics.tsaplots import plot_acf,plot_pacf\nimport csv\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import EarlyStopping,LearningRateScheduler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,LSTM,Bidirectional,Lambda,Conv1D,Dropout\nfrom tensorflow.keras.optimizers import Adam,SGD\nfrom tensorflow.keras.metrics import mean_absolute_error,mean_squared_error\nfrom tensorflow.keras.losses import Huber\nfrom tensorflow.keras.utils import plot_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"time=[]\nsunspots=[]\nwith open(\"../input/sunspots/Sunspots.csv\") as f:\n    reader = csv.reader(f,delimiter=',')\n    next(reader)\n    for row in reader:\n        time.append(row[0])\n        sunspots.append(row[2])\n\nseries = np.array(sunspots).astype(float)\ntime = np.array(time).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"**Time Series** is the ordered sequnce of values spaced over equal interval of time."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot Time vs Series\ndef plot_series(time,series):\n    plt.title(\"Variation of Sunspots with Time\")\n    sns.lineplot(time,series)\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Value\")\nplt.figure(figsize=(12,6))\nplot_series(time,series)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Autocorrelation Plot\nfig,ax = plt.subplots(1,2,figsize=(15,6))\nauto = plot_acf(series,ax=ax[0])\npartial = plot_pacf(series,ax=ax[1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**ACF** : It is a auto-correlation function which gives us values of auto-correlation of any series with its lagged values.It describes how well the present value of the series is related with its past values.\nHere ACF is significant for about **30 values**.This means value depends on previous 30 values.<br>\n**PACF** : The \"partial\" correlation between two variables is the amount of correlation between them which is not explained by their mutual correlations with a specified set of other variables.Here PACF is significant for about **6 values.**"},{"metadata":{},"cell_type":"markdown","source":"# Preparing Test and Val Data"},{"metadata":{},"cell_type":"markdown","source":"We have to split our time series into training and validation period. The split time is 3000 means from 0 to 3000 will be for training and 3000 till the end is for validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"split_time = 3000\ntime_train = time[:split_time]\nx_train = series[:split_time]\ntime_valid = time[split_time:]\nx_valid = series[split_time:]\nsplit_time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Parameters\nwindow_size = 60\nbatch_size = 100\nshuffle_buffer = 1000","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will define a function to create a windowed dataset. In a window dataset, the previous n values could be seen as the input features. And the current value with any timestamp is the output label. Window dataset consconsists of fixed window size."},{"metadata":{"trusted":true},"cell_type":"code","source":"def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n    d = tf.data.Dataset.from_tensor_slices(series)\n    d = d.window(window_size + 1, shift=1, drop_remainder=True)\n    d = d.flat_map(lambda w: w.batch(window_size + 1))\n    d = d.shuffle(shuffle_buffer)\n    d = d.map(lambda w: (w[:-1], w[1:]))\n    d = d.batch(batch_size).prefetch(1)\n    return d","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will also define a function to make a forecast based on our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_forecast(model,series,batch_size,window_size):\n    d = tf.data.Dataset.from_tensor_slices(series)\n    d = d.window(window_size, shift=1, drop_remainder=True)\n    d = d.flat_map(lambda w: w.batch(window_size))\n    d = d.batch(batch_size).prefetch(1)\n    forecast = model.predict(d)\n    return forecast","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Time Series Prediction Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.backend.clear_session()\n\ntrain = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer)\nval = windowed_dataset(x_valid, window_size, batch_size, shuffle_buffer)\n\nmodel = Sequential()\nmodel.add(Lambda(lambda x:tf.expand_dims(x,axis=-1),input_shape=[None]))\nmodel.add(Conv1D(filters=60,kernel_size=5,strides=1,padding='causal',activation='relu'))\nmodel.add(LSTM(120,return_sequences=True))\nmodel.add(LSTM(120,return_sequences=True))\nmodel.add(Dense(60,activation='relu'))\nmodel.add(Dense(30,activation='relu'))\nmodel.add(Dense(1))\nmodel.add(Lambda(lambda x:x*400))\n\nlr_schedule = LearningRateScheduler(lambda epoch : 1e-8 * 10**(epoch / 20))\nmodel.compile(loss=Huber(),optimizer=SGD(lr=1e-8,momentum=0.9),metrics=['mae'])\nhistory = model.fit(train, epochs=100,validation_data=val,callbacks=[lr_schedule])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After trainig the model using Learning Rate Scheduler, lets plots the grpah of \"learning rate\" vs \"loss\". This will help us to select the best learning rate of all."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot for selecting learning rate\nplt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\nplt.axis([1e-8, 1e-3, 0, 100])\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" From this we select learning rate to be **8e-6**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Final Model with lr=8e-6\n\ntf.keras.backend.clear_session()\n\ntrain = windowed_dataset(x_train,window_size,batch_size,shuffle_buffer)\nval = windowed_dataset(x_valid,window_size,batch_size,shuffle_buffer)\n\nmodel = Sequential()\nmodel.add(Lambda(lambda x:tf.expand_dims(x,axis=-1),input_shape=[None]))\nmodel.add(Conv1D(filters=60,kernel_size=5,strides=1,padding='causal',activation='relu'))\nmodel.add(LSTM(120,return_sequences=True))\nmodel.add(LSTM(120,return_sequences=True))\nmodel.add(Dense(60,activation='relu'))\nmodel.add(Dense(30,activation='relu'))\nmodel.add(Dense(1))\nmodel.add(Lambda(lambda x:x*400))\n\nmodel.compile(loss=Huber(),optimizer=SGD(lr=8e-6,momentum=0.9),metrics=['mae'])\nhistory = model.fit(train, epochs=200,validation_data=val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets plot the graph between :\n* \"mae\" vs \"validation mae\"\n* \"loss\" vs \"validation loss\""},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting graphs for mae and loss\ndef plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history[\"val_\"+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string,\"val_\"+string])\n    plt.show()\n\nplt.figure(figsize=(12,6))\nplot_graphs(history,'mae')\nplt.figure(figsize=(12,6))\nplot_graphs(history,'loss')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Forecast\nforecast = model_forecast(model,series[..., np.newaxis],batch_size,window_size)\nforecast = forecast[split_time - window_size:-1,-1,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predicted Plot\nplt.figure(figsize=(12, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, forecast)\nplt.legend([\"Actual\",\"Forecast\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Result"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Mean Absolute Error: \",mean_absolute_error(x_valid,forecast).numpy())\nprint(\"Mean Squared Error:\",mean_squared_error(x_valid,forecast).numpy())","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}