{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Credit Cards - Predicition of churning customers"},{"metadata":{},"cell_type":"markdown","source":"#### Step by step guide to create a Random Forest Classifier to predict the Customer Churning\n\n1. Data Understanding\n2. Data Preparation\n3. Feature Engineering\n4. Feature Extraction\n5. Training\n6. Evaluation"},{"metadata":{},"cell_type":"markdown","source":"# Data Understanding"},{"metadata":{},"cell_type":"markdown","source":"**Importing the necessary libraries**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Loading the dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '/kaggle/input/credit-card-customers/'\n\ndata = pd.read_csv(path + 'BankChurners.csv')\n\n# drop last two columns\ndata = data.iloc[:, :-2]\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Preliminary Data Description**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"\"\"\n    No. of samples  : {data.shape[0]}\n    No. of features : {data.shape[1]}\n    Missing values  : {data.isnull().sum().sum()}\n\"\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"**Categorical features present in the dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.select_dtypes(include='object').columns.to_list()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Label Encoding the ordinal features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Attrition_Flag'].replace({'Existing Customer': 0, 'Attrited Customer': 1}, inplace=True)\n\ndata['Gender'].replace({'M': 1, 'F': 0})\n\neducation_mapping = {\n    'Unknown': 0, 'Uneducated': 1, 'High School': 2, 'College': 3, \n    'Graduate': 4, 'Post-Graduate': 5, 'Doctorate': 6\n}\ndata['Education_Level'].replace(education_mapping, inplace=True)\n\nincome_mapping= {\n    'Less than $40K': 'less_than_40k', '$40K - $60K': '40k_60k', '$60K - $80K': '60k_80k', \n    '$80K - $120K': '80k_120k', '$120K +': 'greater_than_120k',\n}\ndata['Income_Category'].replace(income_mapping, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**One-Hot encoding the nominal features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dummies = pd.get_dummies(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Drop the below features**\n\n1. `CLIENTNUM` - Since, it is an ID parameter and not relevant for our analysis\n2. We can drop one feature from each of the `one-hot encoded` features. In our case, we are dropping -\n    * `Gender_F`\n    * `Marital_Status_Unknown`\n    * `Income_Category_Unknown`"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop('CLIENTNUM', axis=1, inplace=True)\n\ndata_dummies.drop(['Gender_F', 'Marital_Status_Unknown', 'Income_Category_Unknown'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Dataset description after data preparation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"\"\"\n    No. of samples  : {data.shape[0]}\n    No. of features : {data.shape[1]}\n    Missing values  : {data.isnull().sum().sum()}\n\"\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"**Checking `skewness` of the numeric features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_skew = pd.DataFrame(data_dummies.skew(), columns=['Skewness']).sort_values(by='Skewness')\ndf_skew.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Applying `log transformation` to the highly skewed features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Get all the numeric features in out dataset\nnumeric_features = data_dummies.skew().index\n\n## We do not want to touch our target feature\nif 'Attrition_Flag' in numeric_features:\n    numeric_features = numeric_features.drop('Attrition_Flag')\n    \n## Getting all the skewed features (skew > 0.5 or skew < -0.5)\nskewed_features = data_dummies[numeric_features].skew()[np.abs(data_dummies[numeric_features].skew()) > 0.5].index\n\n## Performing log(1+x) transformation\ndata_dummies[skewed_features] = np.log1p(data_dummies[skewed_features])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Check the correlation among features using the heatmap**"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Get the correlation dataframe\ndf_corr = data.corr()\n\n# Plot the heatmap\nfig, ax = plt.subplots(figsize=(10, 8))\nmask    = np.triu(np.ones_like(df_corr, dtype=np.bool))\nsns.heatmap(\n    df_corr, mask=mask, annot=True, fmt=\".2f\", vmin = -1, vmax = 1,\n    cmap=sns.diverging_palette(150, 275, s=80, l=55, n=9)\n)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Extraction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separate the independent and dependent variable\nX = data_dummies.drop(\"Attrition_Flag\", axis = 1)\ny = data_dummies[\"Attrition_Flag\"]\n\n# Get the training and testing pairs\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Chi Square test for feature extraction**\n\n[Reference link](https://machinelearningmastery.com/feature-selection-with-categorical-data/)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nfs = SelectKBest(score_func=chi2, k='all')\nfs.fit(X_train, y_train)\nX_train_fs = fs.transform(X_train)\nX_test_fs = fs.transform(X_test)\n\nfor i in range(len(fs.scores_)):\n    print('Feature %d: %f' % (i, fs.scores_[i]))\n# plot the scores\nplt.bar([i for i in range(len(fs.scores_))], fs.scores_)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above results give us the feature index value and their corresponding importance. Higher the value, more important the feature is."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.iloc[:, [5, 6, 7, 9, 12, 13, 14, 15]]\nX_test = X_test.iloc[:, [5, 6, 7, 9, 12, 13, 14, 15]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{},"cell_type":"markdown","source":"**Using strong ensemble model classifier - Random Forest**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nregressor = RandomForestClassifier(n_estimators=20, random_state=0)\nregressor.fit(X_train, y_train)\ny_pred = regressor.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation"},{"metadata":{},"cell_type":"markdown","source":"**Confusion Matrix**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\narr_cm = confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(arr_cm, index=[0, 1], columns=[0, 1])\nfig = plt.figure(figsize=(4,3), dpi=120)\nsns.heatmap(df_cm, annot=True, fmt=\"d\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Classification Report**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Accuracy Score**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Model Accuracy:', round(accuracy_score(y_test, y_pred)*100, 3), '%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Do not forget to upvote : ) if you liked this notebook.!!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}