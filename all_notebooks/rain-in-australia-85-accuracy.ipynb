{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Packages","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\n\nimport pandas as pd\nimport missingno as mno\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset analysis","metadata":{}},{"cell_type":"markdown","source":"### Reading CSV Dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/weather-dataset-rattle-package/weatherAUS.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking first 5 rows","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking shape of Dataframe","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Variable Identification","metadata":{}},{"cell_type":"markdown","source":"## Checking Dataframe Info","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 16 *float64* column (numerical) and 7 *object* column (categorical).\n\n","metadata":{}},{"cell_type":"markdown","source":"## Checking null value count for each column","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are lot of null values in each column, except for date and location.","metadata":{}},{"cell_type":"markdown","source":"## Visualizing Null values using missingno","metadata":{}},{"cell_type":"code","source":"mno.matrix(df, figsize = (15, 6))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It can be visually seen that Evaporation, Sunshine, Cloud9am and Cloud3pm has lot of missing values.","metadata":{}},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"## Filtering numerical and categorical columns","metadata":{}},{"cell_type":"code","source":"numerical_df_cols = df.columns[df.dtypes != object]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_df_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_df_cols = df.columns[df.dtypes == object]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_df_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Filetring Null value column dataframe","metadata":{}},{"cell_type":"code","source":"null_cols_df =  df[df.columns[df.isnull().any()]].copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"null_cols_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing for Numberical Columns","metadata":{}},{"cell_type":"markdown","source":"### Checking if Central tendencies can be used for imputing missing values.","metadata":{}},{"cell_type":"code","source":"def draw_histplot_between_num_features():\n    fig, axes = plt.subplots(4, len(numerical_df_cols) // 4, figsize=(50, 50))\n    row_idx = 0\n    col_idx = 0\n    for col in numerical_df_cols:\n        if col_idx > 1 and col_idx % 3 == 1:\n            row_idx += 1\n            col_idx = 0\n        sns.histplot(x=col, data=df, ax = axes[row_idx, col_idx])\n        col_idx += 1\n    plt.show()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"draw_histplot_between_num_features()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Though most of the features seems to follow normal distribution, we cannot use mean to impute missing value because outliers. We can verify outliers using Boxplot","metadata":{}},{"cell_type":"markdown","source":"### Boxplot to visualize outlier","metadata":{}},{"cell_type":"code","source":"def draw_boxplot_for_num_features():\n    fig, axes = plt.subplots(4, len(numerical_df_cols) // 4, figsize=(50, 50))\n    row_idx = 0\n    col_idx = 0\n    for col in numerical_df_cols:\n        if col_idx > 1 and col_idx % 3 == 1:\n            row_idx += 1\n            col_idx = 0\n        sns.boxplot(x=col, data=df, ax = axes[row_idx, col_idx])\n        col_idx += 1\n    plt.show()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"draw_boxplot_for_num_features()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It can be seen that most outliers are in **Rainfall** and **Evaporation**.","metadata":{}},{"cell_type":"markdown","source":"### Scatter plot between numerical features","metadata":{}},{"cell_type":"code","source":"sns.pairplot(df[numerical_df_cols])","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It can be seen that mostly all features all follow linear relationship with other features. So we can try using Linear Regression to impute missing value.","metadata":{}},{"cell_type":"markdown","source":"#### Numerical Null value columns","metadata":{}},{"cell_type":"code","source":"numberical_null_value_cols = null_cols_df.columns[null_cols_df.dtypes != object]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numberical_null_value_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Regression to impute missing value","metadata":{}},{"cell_type":"markdown","source":"Our dataset has lot of missing value, even dependent variable will also have missing values, So we can't use them directly. \n* First we have to create a copy of all numberical missing column.\n* Then we can fill some random values using Simple Random Imputation. *(Simple Random Imputation)* Check [this](#Filling-NaN-with-Simple-Random-Imputation) section.\n* Then we will use the regression to fill the value in actual columns, one by one. *(Deterministic Regression Imputation)*. Check [this](#Applying-Deterministic-Regression-Imputation) section.\n\n**Before using Regression, we have to find the higly correlated column for each feature. So that we use only higly correlated column to impute the values.**","metadata":{}},{"cell_type":"markdown","source":"### Simple Random Imputation","metadata":{}},{"cell_type":"markdown","source":"It is used to fill missing values in dependent variable,so that we can use regression models. We will suffix the column with *'_imp'*","metadata":{}},{"cell_type":"code","source":"def simple_random_imputation(df):\n    dataset = df.copy()\n    for feature in numberical_null_value_cols:\n        dataset[feature] = dataset[feature]\n        number_missing = dataset[feature].isnull().sum()\n        observed_values = dataset.loc[dataset[feature].notnull(), feature]\n        dataset.loc[df[feature].isnull(), feature] = np.random.choice(observed_values, number_missing, replace = True)\n            \n    return dataset\n       ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Deterministic Regression Imputation","metadata":{}},{"cell_type":"markdown","source":"It is used to replace the missing value with exact regression output without considering the error term. It may result in overfitting because error term is not considered. **We will prefix the column with 'det_' imputed with this regression**. Later in this notebook we will implement stochastic regression imputation that will overcome the issue of Deterministic Regression Imputation.","metadata":{}},{"cell_type":"code","source":"def deterministic_regression_imputation(df, feature, correlated_cols):\n    dataset = df.copy()\n    dataset[\"det_\" + feature] = df_imp[feature]\n    parameters = list(correlated_cols) \n\n    #linear Regression model to estimate the missing data\n    model = LinearRegression()\n    model.fit(X = df_imp[parameters], y = df_imp[feature])\n    dataset.loc[dataset[feature].isnull(), \"det_\" + feature] = model.predict(df_imp[parameters])[dataset[feature].isnull()]\n    \n    dataset[feature] = dataset[\"det_\" + feature]\n    dataset.drop(\"det_\" + feature, axis = 1, inplace = True)\n    return dataset\n\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Filling NaN with Simple Random Imputation","metadata":{}},{"cell_type":"code","source":"df_imp = simple_random_imputation(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Checking newly added Columns","metadata":{}},{"cell_type":"code","source":"df_imp.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_imp.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Applying Deterministic Regression Imputation","metadata":{}},{"cell_type":"markdown","source":"* First we will find out higly positively and negatively correlated columns.\n* Use those copy of correlated columns that we created in [simple random imputation](#Filling-NaN-with-Simple-Random-Imputation) for Regression Imputation.","metadata":{}},{"cell_type":"code","source":"def find_correlated_cols(df):\n    dataset = df.copy()\n    corr = dataset.corr()\n    corr_col_arr = []\n    for col in numberical_null_value_cols:\n        correlated_cols = []\n\n        # find correlated columns\n        for rel_col, rel_col_corr in corr[col].iteritems():\n            if abs(rel_col_corr) >= 0.2 and abs(rel_col_corr) <= 0.9 and \"_imp\" not in rel_col  and \"det_\" not in rel_col:\n                correlated_cols.append(rel_col)\n        corr_col_arr.append({'col':col, 'correlated_cols': correlated_cols})\n    return corr_col_arr\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_col_arr = find_correlated_cols(df_imp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for v in corr_col_arr:\n    print(\"Column:\", v['col'])\n    print(\"Correlated Column:\", v['correlated_cols'])\n    print(\"\\n\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def draw_scatter_plot(x, correlated_cols, df):\n    fig, axes = plt.subplots(1, len(correlated_cols), figsize=(50, 8))\n    print(\"\\n\") \n    fig.suptitle(x + ' vs Correlated Columns:' + ', '.join(correlated_cols))\n    for idx, rel_col in enumerate(correlated_cols):\n        sns.scatterplot(x = x, y = rel_col, data= df, ax = axes[idx])\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def draw_scatter_plot_between_correlated_feature(df, corr_col_arr):\n    dataset = df.copy()\n    for v in corr_col_arr:\n        draw_scatter_plot(v['col'], v['correlated_cols'], dataset)\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"draw_scatter_plot_between_correlated_feature(df_imp, corr_col_arr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def appy_deterministic_imp(df):\n    dataset = df.copy()\n    for v in corr_col_arr:\n        print(\"Column:\", v['col'])\n        print(\"Correlated Column:\",v['correlated_cols'])\n        if len(v['correlated_cols']) > 0: \n            dataset = deterministic_regression_imputation(dataset, v['col'], v['correlated_cols'])\n            print(dataset[v['col']].isnull().sum())\n        else:\n            print(\"No Correlated Column for\", v['col'])\n        print(\"\\n\")\n    return dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_det = appy_deterministic_imp(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_det.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing for Categorical Columns","metadata":{}},{"cell_type":"markdown","source":"### Filtering Categorical Null value columns","metadata":{}},{"cell_type":"code","source":"categorical_null_value_cols = null_cols_df.columns[null_cols_df.dtypes == object]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_null_value_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in categorical_null_value_cols:\n    print(\"Unique value of column:\", col)\n    print(df[col].unique())\n    print(\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Imputing Categotical value","metadata":{}},{"cell_type":"markdown","source":"We are using mode of a particular location to impute missing values.","metadata":{}},{"cell_type":"code","source":"def impute_categorical_variable(df, cat_features):\n    for cat_feature in cat_features:\n        print(\"Imputing Column:\", cat_feature)\n        df[cat_feature] = df[cat_feature].fillna((df.groupby('Location')[cat_feature].transform(lambda x:  next(iter(x.mode()), np.nan))))\n        print(df[[cat_feature]].isnull().sum())\n        print(\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"impute_categorical_variable(df_det, categorical_null_value_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It can be seen that there are still some missing values in WindGustDir. It means some location have no value at all. We wll using mode of complete dataset.","metadata":{}},{"cell_type":"code","source":"df_det['WindGustDir']=df_det['WindGustDir'].fillna(df_det['WindGustDir'].mode().max())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_det['WindGustDir'].isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Encode Categorical Value","metadata":{}},{"cell_type":"markdown","source":"First we will encode RainToday and RainTomorrow Column. \"Yes\" => 1 and \"No\" => 0","metadata":{}},{"cell_type":"markdown","source":"#### Encoding Column: RainToday ","metadata":{}},{"cell_type":"code","source":"df_det['RainToday'] = df_det['RainToday'].apply(lambda x: 1 if x == 'Yes' else 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_det['RainToday'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Encoding Column: RainTomorrow","metadata":{}},{"cell_type":"code","source":"df_det['RainTomorrow'] = df_det['RainTomorrow'].apply(lambda x: 1 if x == 'Yes' else 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_det['RainTomorrow'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Encoding using get_dummies","metadata":{}},{"cell_type":"code","source":"df_det = pd.get_dummies(data=df_det, columns=['WindGustDir','WindDir9am','WindDir3pm'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_det.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_det.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Scaling using Standard scaler","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.preprocessing import StandardScaler\n\ndef scale_dataset(df):\n    dataset = df.copy()\n    dataset.drop(['Date', 'Location', 'RainToday', 'RainTomorrow'], axis = 1, inplace = True)\n    scaler = StandardScaler()\n    return pd.DataFrame(scaler.fit_transform(dataset),columns = dataset.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_det_scaled = pd.merge(scale_dataset(df_det), df_det[['Date', 'Location', 'RainToday', 'RainTomorrow']],left_index=True, right_index=True )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_det_scaled.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_det_scaled.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"## Highest and Lowest MinTemp by Location","metadata":{}},{"cell_type":"code","source":"gp_min_temp = df_det.groupby('Location')['MinTemp'].agg(['min', 'max']).reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gp_min_temp.sort_values('min').head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"MonutGinni has the lowest MinTemp.","metadata":{}},{"cell_type":"code","source":"gp_min_temp.sort_values('max', ascending=False).head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Adelaide has the highest MinTemp ","metadata":{}},{"cell_type":"markdown","source":"### Highest and Lowest MaxTemp by Location","metadata":{}},{"cell_type":"code","source":"gp_max_temp = df_det.groupby('Location')['MaxTemp'].agg(['min', 'max']).reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gp_max_temp.sort_values('min').head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"MountGinini has the lowest MaxTemp","metadata":{}},{"cell_type":"code","source":"gp_max_temp.sort_values('max', ascending=False).head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Woomera has the highest MaxTemp.","metadata":{}},{"cell_type":"code","source":"corr = df_det_scaled.corr().round(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(40,30))\nheatmap = sns.heatmap(corr, vmin=-1, vmax=1, annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **det_Temp9am and det_Temp3pm has high correlation with det_MinTemp and det_MaxTemp. So we will drop those column. To avoid overfitting**","metadata":{}},{"cell_type":"code","source":"def drop_column_to_avoid_overfitting(df):\n    dataset = df.copy()\n    dataset.drop('Temp9am',axis=1,inplace=True)\n    dataset.drop('Temp3pm',axis=1,inplace=True)\n    return dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_det_scaled_new = drop_column_to_avoid_overfitting(df_det_scaled)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Column with Zero Correlation","metadata":{}},{"cell_type":"code","source":"zreo_corr_col =  corr[corr['RainTomorrow'] == 0].index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"zreo_corr_col","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Dropping column with Zero correlation","metadata":{}},{"cell_type":"code","source":"df_det_scaled_new.drop(zreo_corr_col, axis = 1, inplace = True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_det_scaled_new.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_dp = len(df['RainTomorrow'])\ndf_det_scaled_new.groupby('RainTomorrow')['RainTomorrow'].count().apply( lambda x: (x/total_dp) * 100  )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dataset is higly imbalance. 78% contains No, and 22% contains Yes.","metadata":{}},{"cell_type":"markdown","source":"# Feature Selection","metadata":{}},{"cell_type":"markdown","source":"We will use lasso regression for feature selection.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import SelectFromModel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = df_det_scaled_new.copy()\ndataset.drop(['Date', 'Location'], axis = 1, inplace =True)\nY_det = dataset['RainTomorrow']\nX_det = dataset.drop('RainTomorrow', axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_sel_model = SelectFromModel(Lasso(alpha=0.005, random_state = 0))\nfeature_sel_model.fit(X_det, Y_det)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_sel_model.get_support()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_feat = X_det.columns[(feature_sel_model.get_support())]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_feat","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Selection and Cross Validation","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report,accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def apply_stratified_k_fold_validation( model, x = df_det_scaled_new[selected_feat], y = Y_det ):\n    stratified_acc = []\n    for train_index, test_index in skf.split(x , y):\n        x_train_fold, x_test_fold = x.iloc[train_index.tolist()], x.iloc[test_index.tolist()]\n        y_train_fold, y_test_fold = y.iloc[train_index.tolist()], y.iloc[test_index.tolist()]\n        model.fit(x_train_fold, y_train_fold)\n        stratified_acc.append(model.score(x_test_fold, y_test_fold))\n    \n    print(\"\\n\")\n    print('List of possible accuracy:', stratified_acc)\n    \n    print(\"\\n\")\n    print('Maximum Accuracy That can be obtained from this model is:', max(stratified_acc)*100, '%')\n    \n    print(\"\\n\")\n    print('Minimum Accuracy:', min(stratified_acc)*100, '%')\n    \n    print(\"\\n\")\n    print('Overall Accuracy:', np.mean(stratified_acc)*100, '%')\n    \n    print(\"\\n\")\n    print('Standard Deviation is:', np.std(stratified_acc)*100, '%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistics Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\napply_stratified_k_fold_validation(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## k-Nearest Neighbors","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier(n_neighbors=5)    \napply_stratified_k_fold_validation(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Decision Trees","metadata":{}},{"cell_type":"code","source":"from sklearn import tree\n\nmodel = tree.DecisionTreeClassifier()\napply_stratified_k_fold_validation(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XBoost Classifier","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\nmodel = XGBClassifier(max_depth=12,random_state = 42, use_label_encoder =False)\napply_stratified_k_fold_validation(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Naive Bayes Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\nmodel = GaussianNB()\napply_stratified_k_fold_validation(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(random_state=0)\napply_stratified_k_fold_validation(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Best Models are Logistic Regression, XGBoost and Random forest with Maximum Accuracy of 85%.**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}