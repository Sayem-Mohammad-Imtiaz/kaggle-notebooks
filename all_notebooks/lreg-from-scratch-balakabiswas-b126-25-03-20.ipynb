{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib as ml\nimport matplotlib.pyplot as plt\n%matplotlib inline\nml.style.use('fivethirtyeight')\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/iris/Iris.csv')\ndata.head(60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(columns=['Id'],inplace=True)\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.boxplot(data=data,palette='Set2')\nplt.title('Checking for the feature with least outliers')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(data.corr(),annot=True,linewidth=0.1,linecolor='white')\nplt.title(\"Checking for the most correlated features\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Clearly, PetalLengthCm and PetalWidthCm are the most related.**"},{"metadata":{},"cell_type":"markdown","source":"## Separating X and Y"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data['PetalLengthCm'].values\nY = data['PetalWidthCm'].values\nX.shape,Y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x=X,y=Y,hue=data.Species)\nplt.xlabel('PetalLengthCm')\nplt.ylabel('PetalWidthCm')\nplt.title(\"Raw plotting of the data\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Implementing Linear Regression from scratch"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mean X and Y\nmn_x = np.mean(X)\nmn_y = np.mean(Y)\ntotal = len(X)    # Total number of values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Calculation of Coefficients"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using the formula to calculate b0 and b1\nn = 0\nd = 0\nfor i in range(total):\n    n = n + ((X[i] - mn_x) * (Y[i] - mn_y))\n    d = d + ((X[i] - mn_x) ** 2)\nb1 = n / d                               # B1 = (summation([x[i] - x_mean]*(y[i] - y_mean)))/((summation(x[i] - x_mean))^2)\nb0 = mn_y - (b1 * mn_x)                  # B0 = y_mean - (B1*x_mean)\n\n# Print coefficients\nprint(b1, b0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Getting the equation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Y = B0 + B1*X\n# PetalWidthCm = 325.573421049 + (0.263429339489 * PetalLengthCm)         \n\n# This is our linear model\n\n# Now plotting it to obtain best fit curve\n\nmax_x = np.max(X)\nmin_x = np.min(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generating unseen data for analyzing best fit curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generating unseen data\n# Calculating line values x and y\nx = np.linspace(min_x, max_x, 1000)\ny = b0 + b1 * x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final comparative plot : Visualizing the best fit curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting\n\nplt.figure(figsize=(10,5))\n# Ploting Line based on linear model made from scratch : The unseen data\nplt.plot(x, y, color='blue', label='Regression Line')\n\n# Ploting Scatter Points of our actual dataset to see how well the model performs : The predefined data\nplt.scatter(X, Y, c='red', label='Scatter Plot')\nplt.xlabel('Petal Length in cm')\nplt.ylabel('Petal Width in cm')\nplt.title(\"Implentation of Linear Regression from scratch visualized\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## So, by visually analyzing the distances of the data points from the best fit curve, we can say this is a fairly good fit."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}