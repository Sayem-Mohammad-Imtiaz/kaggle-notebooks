{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Image Captioning with Tensorflow\n\n**Resources:**\n- https://medium.com/analytics-vidhya/image-captioning-with-tensorflow-2d72a1d9ffea\n- https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/\n\n**Code Contributions:**\n - Got working for TF 2\n - Debugged original code from Medium post which didn't work with copy/paste alone\n - Included inference code in demo\n - Included more output along the way for better code transparency","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, RepeatVector, Dense, LSTM\nfrom tensorflow.keras.layers import Embedding, Dropout, TimeDistributed, Concatenate\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.layers.merge import add\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport os\nimport numpy as np\nfrom PIL import Image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image Feature Extraction\n\n- Extract image features by applying ResNet50 (pretrainted on imagenet).\n    - Note:  We also omit the last layer (which is the softmax layer) because we only need to extract the features, not to classify the images.\n- After executing the code, the output would be a 1x2048 vector for each image, containing the features from the images.","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Get all filenames of the images\nfolder = \"../input/flickr8k/Flickr_Data/Flickr_Data/Images/\"\nimages = os.listdir(folder)\n\n# Load the CNN Architecture with Imagenet as weights\nimage_model = ResNet50(weights='../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels.h5')\nmodel_new = tf.keras.Model(image_model.input,image_model.layers[-2].output)\n\n# Store image features in dictionary\nimg_features = dict()  \nfor img in images: \n    img1 = image.load_img(folder + img, target_size=(224, 224))\n    x = image.img_to_array(img1)\n    x = np.expand_dims(x, axis=0)\n    x = preprocess_input(x)\n    fea_x = model_new.predict(x)\n    fea_x1 = np.reshape(fea_x , fea_x.shape[1])\n    img_features[img] = fea_x1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Caption Pre-Processing\n- The first thing we have to do is gather all of the captions from Flickr8k.Token.txt and group them by a single key, which is the filename. \n- After that, we split the captions for train, validation and test sets according to Flickr_8k.trainImages.txt, Flickr_8k.devImages.txt, and Flickr_8k.testImages.txt.  Those three files only contain the filenames for respective dataset. \n- While splitting the captions based on those files, we also add ‘Startseq’ at the beginning and ‘Endseq’ at the end of the sentence to each caption. This is to signal when the model should start writing or stop predicting next word at training phase.","metadata":{}},{"cell_type":"code","source":"# Get All Captions\nfn = \"../input/flickr8k/Flickr_Data/Flickr_Data/Flickr_TextData/Flickr8k.token.txt\"\nf = open(fn, 'r')\ncapts = f.read()\n#Group all captions by filename, for references\ncaptions = dict()\ni = 0\n\ntry:\n    for line in capts.split(\"\\n\"):\n        txt = line.split('\\t')\n        fn = txt[0].split('#')[0]\n        if fn not in captions.keys():\n            captions[fn] = [txt[1]]\n        else:\n            captions[fn].append(txt[1])\n        i += 1\nexcept:\n    pass #pass Model\n    \n\ndef getCaptions(path):\n    \n    f = open(path, 'r')\n    capts = f.read()\n    desc = dict()\n\n    try:\n        for line in capts.split(\"\\n\"):\n            image_id = line\n            image_descs = captions[image_id]\n\n            for des in image_descs:\n                ws = des.split(\" \")\n                w = [word for word in ws if word.isalpha()]\n                des = \"startseq \" + \" \".join(w) + \" endseq\"\n                if image_id not in desc:\n                    desc[image_id] = list()\n                desc[image_id].append(des)\n    except:\n        pass\n    \n    return desc\n\n# Split captions\ntrain_caps = getCaptions(\"../input/flickr8k/Flickr_Data/Flickr_Data/Flickr_TextData/Flickr_8k.trainImages.txt\")\nval_caps = getCaptions(\"../input/flickr8k/Flickr_Data/Flickr_Data/Flickr_TextData/Flickr_8k.devImages.txt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Review the train_caps & val_caps objects","metadata":{}},{"cell_type":"code","source":"print(\"train_caps type: \", type(train_caps))\nprint(\"First few (key, value) paris of train_caps:\") \nfor i, (k, v) in enumerate(train_caps.items()):\n    print(k, v)\n    if i>2:\n        break\nprint(\"\")\nprint(\"val_caps type: \", type(val_caps))\nprint(\"First few (key, value) pairs of val_caps:\") \nfor i, (k, v) in enumerate(val_caps.items()):\n    print(k, v)\n    if i>2:\n        break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Tokenization\n- After that, we have to tokenize the captions from the train set and get the word-index & index-word dictionaries from that. \n    - The word-index dictionary is meant to represent the captions as a number to be inputted to the model\n    - The index-word is to convert the next word/prediction to the word form, as we know it.","metadata":{}},{"cell_type":"code","source":"# Preparing to make word-index and index-word\n# (adding all training captions to a list)\ntrain_captions = []\nfor key, desc_list in train_caps.items():\n    for i in range(len(desc_list)):\n        train_captions.append(desc_list[i])\n\n# Tokenize top 5000 words in Train Captions\nvocab_size = 5000\ntokenizer = Tokenizer(num_words=vocab_size,\n                      oov_token=\"<unk>\",\n                      filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\ntokenizer.fit_on_texts(train_captions)\nword_index = tokenizer.word_index\nindex_word = tokenizer.index_word","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### View Tokenizer Indexes","metadata":{}},{"cell_type":"code","source":"vocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary Size: {}'.format(vocab_size))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"word_index type: \", type(word_index))\nprint(\"First few (key, value) paris of word_index:\") \nfor i, (k, v) in enumerate(word_index.items()):\n    print(k, v)\n    if i>2:\n        break\nprint(\"\")\nprint(\"index_word type: \", type(index_word))\nprint(\"First few (key, value) pairs of index_word:\") \nfor i, (k, v) in enumerate(index_word.items()):\n    print(k, v)\n    if i>2:\n        break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Images + Image Features","metadata":{}},{"cell_type":"code","source":"%%time\n\ntrain_fns = list(train_caps.keys())\ntrain_set = dict((k, img_features[k]) for k in train_fns)\nval_fns = list(val_caps.keys())\nval_set = dict((k, img_features[k]) for k in val_fns)\nfn_test = \"../input/flickr8k/Flickr_Data/Flickr_Data/Flickr_TextData/Flickr_8k.testImages.txt\"\nf = open(fn_test, 'r')\nt = f.read()\n\ntest_fns= t.split(\"\\n\")\ntest_set = dict((k, img_features[k]) for k in list(test_fns[:-1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### View train_set, val_set, test_set","metadata":{}},{"cell_type":"code","source":"print(\"train_set type: \", type(train_set))\nprint(\"First few (key, value) paris of train_set:\") \nfor i, (k, v) in enumerate(train_set.items()):\n    print(k, v)\n    if i>2:\n        break\nprint(\"\")\nprint(\"val_set type: \", type(val_set))\nprint(\"First few (key, value) pairs of val_set:\") \nfor i, (k, v) in enumerate(val_set.items()):\n    print(k, v)\n    if i>2:\n        break\nprint(\"\")\nprint(\"test_set type: \", type(test_set))\nprint(\"First few (key, value) pairs of test_set:\") \nfor i, (k, v) in enumerate(test_set.items()):\n    print(k, v)\n    if i>2:\n        break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final Training & Validation Data","metadata":{}},{"cell_type":"code","source":"# create sequences of images, input sequences and output words for an image\ndef create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n    X1, X2, y = list(), list(), list()\n    # walk through each description for the image\n    for desc in desc_list:\n        # encode the sequence\n        seq = tokenizer.texts_to_sequences([desc])[0]\n        # split one sequence into multiple X,y pairs\n        for i in range(1, len(seq)):\n            # split into input and output pair\n            in_seq, out_seq = seq[:i], seq[i]\n            # pad input sequence\n            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n            # encode output sequence\n            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n            # store\n            X1.append(photo)\n            X2.append(in_seq)\n            y.append(out_seq)\n    return np.array(X1), np.array(X2), np.array(y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length = 34","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data generator, intended to be used in a call to model.fit_generator()\ndef data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n    # loop for ever over images\n    while 1:\n        for key, desc_list in descriptions.items():\n            # retrieve the photo feature\n            photo = photos[key]\n            in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n            yield [in_img, in_seq], out_word","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test the data generator\ngenerator = data_generator(train_caps, train_set, tokenizer, max_length, vocab_size)\ninputs, outputs = next(generator)\nprint(inputs[0].shape)\nprint(inputs[1].shape)\nprint(outputs.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word Embedding Matrix\n- For this project, we used GloVe Word Embedding for the captions. Every word in the dictionary will be mapped into a vector from the pre-trained GloVe model. The dimension of GloVe we used is 200.\n- According to GloVe website, GloVe has been modeled by how frequently a word is coming up after another word. So, we expect that the predicted caption can be improved with this.\n- First, we collect all the words in GloVe and their coefficients. Then, we map all the words in our dictionaries before to a vector and collect all of them into a matrix (embedding matrix)","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Load Glove vectors\nembeddings_index = {} # empty dictionary\nf = open(\"../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt\", encoding=\"utf-8\")\n\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))\n# Get 200-dim dense vector for each of the 10000 words in out vocabulary\nvocab_size = len(word_index) + 1\nembedding_dim = 200\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\n\nfor word, i in word_index.items():\n    #if i < max_words:\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # Words not found in the embedding index will be all zeros\n        embedding_matrix[i] = embedding_vector","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Architecture\n- For the model architecture, there are two parts at the beginning. \n- The first one is handling the image feature (image_model), which consists a Dense layer and RepeatVector. \n- The second part is the one handling the captions (language_model). \n    - At the first layer of language_model, there is an Embedding layer, which weight will be assigned by the GloVe coefficients we have gathered before in embedding_matrix. We also set that layer to be untrainable, since it has been pre-trained before.\n- Next, the image_model and language_model are concatenated to predict the next word (output). \n    - For the next-word prediction part, there are 2 layers of LSTM followed by a Dense and Softmax layer for classification (since this case is a multi-class classification anyway).","metadata":{}},{"cell_type":"code","source":"image_model = tf.keras.models.Sequential()\n\nimage_model.add(Dense(embedding_dim, input_shape=(2048,), activation='relu'))\nimage_model.add(RepeatVector(max_length))\n\nlanguage_model = tf.keras.models.Sequential()\n\nlanguage_model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\nlanguage_model.add(LSTM(256, return_sequences=True))\nlanguage_model.add(TimeDistributed(Dense(embedding_dim)))\n\nconca = Concatenate()([image_model.output, language_model.output])\nx = LSTM(128, return_sequences=True)(conca)\nx = LSTM(512, return_sequences=False)(x)\nx = Dense(vocab_size)(x)\nout = Activation('softmax')(x)\nmodel_1 = Model(inputs=[image_model.input, language_model.input], outputs = out)\n\nmodel_1.layers[2].set_weights([embedding_matrix])\nmodel_1.layers[2].trainable = False\n\nmodel_1.compile(loss='categorical_crossentropy', optimizer = Adam(learning_rate = 0.0001), metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"%%time\n\n# train the model, run epochs manually and save after each epoch\nepochs = 50\nsteps = len(train_caps)\nfor i in range(epochs):\n    # create the data generator\n    generator = data_generator(train_caps, train_set, tokenizer, max_length, vocab_size)\n    # fit for one epoch\n    model_1.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference\n\n**Pseudo-Code:**\n- For Each Image\n    - KeepAddingWords = True\n    - While KeepAddingWords:\n        - yhat = model.predict([image_features,caption_sequence_so_far])\n        - if yhat is Endseq:\n            - KeepAddingWords = False\n        - Else:\n            - append yhat to caption_sequence_so_far","metadata":{}},{"cell_type":"code","source":"# map an integer to a word\ndef word_for_id(integer, tokenizer):\n    return tokenizer.index_word.get(integer)\n\n# generate a description for an image\ndef generate_desc(model, tokenizer, photo, max_length):\n    # seed the generation process\n    in_text = 'startseq'\n    # iterate over the whole length of the sequence\n    for i in range(max_length):\n        # integer encode input sequence\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        #print(\"sequence after tok: \", sequence)\n        # pad input\n        sequence = pad_sequences([sequence], maxlen=max_length)\n        # predict next word\n        if i==0:\n            photo = np.expand_dims(photo, axis=0)\n        #print(\"photo: \", photo)\n        #print(\"sequence: \", sequence)\n        yhat = model.predict([photo, sequence], verbose=0)\n        # convert probability to integer\n        yhat = np.argmax(yhat)\n        # map integer to word\n        word = word_for_id(yhat, tokenizer)\n        # stop if we cannot map the word\n        if word is None:\n            break\n        # append as input for generating the next word\n        in_text += ' ' + word\n        # stop if we predict the end of the sequence\n        if word == 'endseq':\n            break\n    return in_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test the predict function\ntmpimg1 = np.expand_dims(np.array(photo), axis=0)\nprint(tmpimg1.shape)\ntmpcap1 = pad_sequences([[3]], maxlen=max_length)\nprint(tmpcap1.shape)\ntmpout1 = model_1.predict([tmpimg1, tmpcap1], verbose=0)\nprint(tmpout1.shape)\nprint(tmpout1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def image_to_feat_vec(imagePath):\n    img1 = image.load_img(imagePath, target_size=(224, 224))\n    x = image.img_to_array(img1)\n    x = np.expand_dims(x, axis=0)\n    x = preprocess_input(x)\n    fea_x = model_new.predict(x)\n    fea_x1 = np.reshape(fea_x , fea_x.shape[1])\n    return fea_x1\n\nimagePath = \"../input/garage-detection-unofficial-ssl-challenge/GarageImages/GarageImages/image1086.jpg\"\nphoto = image_to_feat_vec(imagePath)\nprint(\"Predicted Caption:\", generate_desc(model_1, tokenizer, photo, max_length))\nImage.open(imagePath)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imagePath = \"../input/garage-detection-unofficial-ssl-challenge/GarageImages/GarageImages/image1185.jpg\"\nphoto = image_to_feat_vec(imagePath)\nprint(\"Predicted Caption:\", generate_desc(model_1, tokenizer, photo, max_length))\nImage.open(imagePath)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imagePath = \"../input/garage-detection-unofficial-ssl-challenge/GarageImages/GarageImages/image1396.jpg\"\nphoto = image_to_feat_vec(imagePath)\nprint(\"Predicted Caption:\", generate_desc(model_1, tokenizer, photo, max_length))\nImage.open(imagePath)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}