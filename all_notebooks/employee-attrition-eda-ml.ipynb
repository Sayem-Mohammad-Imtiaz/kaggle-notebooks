{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install pyforest   # it automatically imports few important libraries including numpy, pandas and matplotlib\nimport pyforest\nimport matplotlib\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import model_selection\nfrom sklearn.model_selection import KFold\n# matplotlib.rcParams['figure.figsize'] = (13.0, 7.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"../input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lets look at our dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(13,5))\nsns.heatmap(df.isna())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we will see if data is balanced or not?**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\ndf.Attrition.value_counts().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes =plt.subplots(3,3, figsize=(15,15))\naxes = axes.flatten()\nobject_bol = df.dtypes == 'object'\nfor ax, catplot in zip(axes, df.dtypes[object_bol].index):\n    sns.countplot(hue=catplot,x=df.Attrition ,data=df, ax=ax,)\n\nplt.tight_layout()  \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(5,5, figsize=(15,15))\naxes = axes.flatten()\nobject_bol = df.dtypes == 'int'\nfor ax, catplot in zip(axes, df.dtypes[object_bol].index):\n    sns.boxplot(y=df[catplot],x= df[\"Attrition\"],ax=ax)\n\nplt.tight_layout()  \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=df[['Attrition','Age','BusinessTravel', 'DistanceFromHome', 'MonthlyIncome','PercentSalaryHike','TotalWorkingYears']]\nsns.pairplot(df1,hue='Attrition',)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.distplot(df.Age,bins=20, kde=False)\nplt.xlabel(\"Age\")\nplt.ylabel(\"Counts\")\nplt.title(\"Age Counts\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s = (df.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Make copy to avoid changing original data \nlabel_data = df.copy()\n\n# Apply label encoder to each column with categorical data\nlabel_encoder = LabelEncoder()\nfor col in object_cols:\n    label_data[col] = label_encoder.fit_transform(df[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Build a forest to predict attrition and compute the feature importances**"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(class_weight=\"balanced\", n_estimators=500) \nrf.fit(label_data.drop(['Attrition'],axis=1), hr.Attrition)\nimportances = rf.feature_importances_\nnames = label_data.columns\nimportances, names = zip(*sorted(zip(importances, names)))\n\n# Lets plot this\nplt.figure(figsize=(15,8))\nplt.barh(range(len(names)), importances, align = 'center')\nplt.yticks(range(len(names)), names)\nplt.xlabel('Importance of features')\nplt.ylabel('Features')\nplt.title('Importance of each feature')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Spliting the data for training and testing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(label_data.drop(['Attrition'],axis=1), label_data.Attrition,\n                                                    test_size=0.25, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Creating a function for KFold Cross-Validation and Confusion Matrix(using heatmap)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def kfold_and_confusion_matrix(model, t_model):\n    kfold = KFold(n_splits=5)\n    model_kfold = model\n    results_kfold = model_selection.cross_val_score(model_kfold, X_train, y_train,  cv=kfold)\n    print(\"K Fold Accuracy: %.2f%%\" % (results_kfold.mean()*100.0)) \n    \n    plt.figure(figsize=(15,5))\n    y_pred=t_model.predict(X_test)\n    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Trying Logistic Regression with default parameters**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nmodel=LogisticRegression()\nmodel.fit(X_train, y_train)\nmodel.score(X_test, y_test)\n\nprint(\"Normal Accuracy:\",model.score(X_test, y_test))\nkfold_and_confusion_matrix(LogisticRegression(), model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We will use Oversampling technique SMOTE to match the number of target in both classes**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# SMOTE for imbalanced data\noversampler=SMOTE(random_state=0)\nX, y = oversampler.fit_sample(label_data.drop(['Attrition'],axis=1), label_data.Attrition,)   #label_data[list(names)[14:], using less features are reducing the accuracy\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.25)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Logistic Regression with SMOTEd data and default parameters**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model=LogisticRegression()\nmodel.fit(X_train, y_train, )\nprint(\"Normal Accuracy: %.2f%%\" % ( model.score(X_test, y_test)*100))\n\nkfold_and_confusion_matrix(LogisticRegression(), model)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Logistic Regression with SMOTEd data and GridSearch**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\n\nlogregpipe = Pipeline([('scale', StandardScaler()),\n                   ('logreg',LogisticRegression())])\n\n# Gridsearch to determine the value of C\nparam_grid = {'logreg__C':np.arange(0.01,100,10)}\nlogreg_cv = GridSearchCV(logregpipe,param_grid,cv=5,return_train_score=True)\nlogreg_cv.fit(X_train,y_train)\nprint(logreg_cv.best_params_)\n\n\nbestlogreg = logreg_cv.best_estimator_\nbestlogreg.fit(X_train,y_train)\nbestlogreg.coef_ = bestlogreg.named_steps['logreg'].coef_\nprint(\"Normal Accuracy: %.2f%%\" % (bestlogreg.score(X_train,y_train)*100))\n\nkfold_and_confusion_matrix(logreg_cv.best_estimator_, bestlogreg)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(class_weight=\"balanced\", n_estimators=500)\nmodel.fit(X_train, y_train)\nprint(\"Normal Accuracy:\",(model.score(X_test, y_test)*100))\n\nkfold_and_confusion_matrix(RandomForestClassifier(), model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import  GradientBoostingClassifier\nmodel = GradientBoostingClassifier()\nmodel.fit(X_train, y_train)\nprint(\"Normal Accuracy:\",(model.score(X_test, y_test)*100))\n\nkfold_and_confusion_matrix(GradientBoostingClassifier(), model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn import metrics\n# Parameter Tuning\nmodel = xgb.XGBClassifier()\nparam_dist = {\"max_depth\": [10,30,50],\n              \"min_child_weight\" : [1,3,6],\n              \"n_estimators\": [200],\n              \"learning_rate\": [0.05, 0.1,0.16],}\ngrid_search = GridSearchCV(model, param_grid=param_dist, cv = 3, \n                                   verbose=10, n_jobs=-1)\ngrid_search.fit(X_train, y_train)\nmodel=grid_search.best_estimator_\n# model = xgb.XGBClassifier(max_depth=50, min_child_weight=1,  n_estimators=200,\\\n#                           n_jobs=-1 , verbose=1,learning_rate=0.16)\nmodel.fit(X_train,y_train)\nprint(\"Normal Accuracy:\",(model.score(X_test, y_test)*100))\n\nkfold_and_confusion_matrix(grid_search.best_estimator_, model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn import metrics\n\n\nlg = lgb.LGBMClassifier(silent=False)\nparam_dist = {\"max_depth\": [25,50, 75],\n              \"learning_rate\" : [0.01,0.05,0.1],\n              \"num_leaves\": [300,900,1200],\n              \"n_estimators\": [200]\n             }\ngrid_search = GridSearchCV(lg, n_jobs=-1, param_grid=param_dist, cv = 3, scoring=\"roc_auc\", verbose=5)\ngrid_search.fit(X_train,y_train)\ngrid_search.best_estimator_\nmodel= grid_search.best_estimator_\nmodel.fit(X_train,y_train)\nprint(\"Normal Accuracy:\",(model.score(X_test, y_test)*100))\n\nkfold_and_confusion_matrix(grid_search.best_estimator_, model)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}