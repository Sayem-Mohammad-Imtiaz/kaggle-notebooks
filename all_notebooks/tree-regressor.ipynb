{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nLibrairies nécessaires\n'''\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom datetime import datetime\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom math import radians, cos, sin, asin, sqrt\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import RFECV\nfrom haversine import haversine, Unit \nimport csv as csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nChargement dataset\n'''\n#Chargement du dataset complet\ndataset_original = pd.read_csv('/kaggle/input/train.csv')\n\n# Création d'un copie\ndataset = dataset_original.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nTraitement des données\n'''\n# Vecteurs Coords pour calculer la Distance directe\npick_coords = dataset.loc[:, [\"pickup_latitude\",\"pickup_longitude\"]] \ndrop_coords = dataset.loc[:, [\"dropoff_latitude\",\"dropoff_longitude\"]] \n\n#Conversion de coords à tuples pour nourrir la fonction haversine\npick_coords_tup = [tuple(x) for x in pick_coords.values]\ndrop_coords_tup = [tuple(x) for x in drop_coords.values]\n\n# Calcul de la distance directe en KMS et conversion à MÉTRES\ntrip_dist = [haversine(pick_coords_tup[i], drop_coords_tup[i]) for i in range(len(pick_coords_tup))]\ntrip_dist = pd.Series(trip_dist)*1000\n\n# Conversion de types (object -> datetime) \ndataset['pickup_datetime'] = pd.to_datetime(dataset['pickup_datetime'])\ndataset['dropoff_datetime'] = pd.to_datetime(dataset['dropoff_datetime'])\n\n# Création de nouvelles variables à partir de 'pickup_datime' \nweek_day = dataset['pickup_datetime'].dt.weekday   # day of the week (0=Monday ... 6=Sunday)\nmonth = dataset['pickup_datetime'].dt.month   # month (0=Jan ... 11=Dec)\nyear = dataset['pickup_datetime'].dt.year   # year\ntime = dataset['pickup_datetime'].dt.time   # time\ndate = dataset['pickup_datetime'].dt.date   # date \n\n# Création du dataset avec les variables pertinentes\ndataset = dataset[['vendor_id','passenger_count','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','trip_duration']]\ndataset.insert(2, 'week_day', week_day, True)\ndataset.insert(3, 'month', month, True)\ndataset.insert(4, 'trip_dist', trip_dist, True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nNettoyage de données\n''' \n# Constraintes à appliquer:\n#   -trip_duration < 30000  ( 8 heures)\n#   -trip_duration < 45)\n#   -trip_dist < 300 kms\n\n \n#Mise en place du netoyage selon constraints\ndataset = dataset.drop(dataset[dataset.trip_duration > 30000].index)\ndataset = dataset.drop(dataset[dataset.trip_duration < 45].index)\ndataset = dataset.drop(dataset[dataset.trip_dist >= 300000].index)\n\ndataset = dataset.drop(dataset[dataset.pickup_latitude < 40.1].index)\ndataset = dataset.drop(dataset[dataset.pickup_latitude > 41.4].index)\ndataset = dataset.drop(dataset[dataset.pickup_longitude < -86].index)\ndataset = dataset.drop(dataset[dataset.pickup_longitude > -73].index)\n\ndataset = dataset.drop(dataset[dataset.dropoff_latitude < 39].index)\ndataset = dataset.drop(dataset[dataset.dropoff_latitude > 42].index)\ndataset = dataset.drop(dataset[dataset.dropoff_longitude < -75].index)\ndataset = dataset.drop(dataset[dataset.dropoff_longitude > -30].index)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nCreation du Train Set et du Test Set\n'''\n#Fonction pour diviser le dataset complet en Train set et Test set\ndef split_train_test(data, test_ratio):\n    np.random.seed(42)\n    shuffled_indices = np.random.permutation(len(data))\n    test_set_size = int(len(data) * test_ratio)\n    test_indices = shuffled_indices[:test_set_size]\n    train_indices = shuffled_indices[test_set_size:]\n    return data.iloc[train_indices], data.iloc[test_indices]\n\n#Division du dataset\nnew_train, new_test = split_train_test(dataset, 0.2)\n\n# Variables objetives (trip_duration)\ny = new_train[['trip_duration']]\ny_test = new_test[['trip_duration']]\n\n# Suppression de la valeur objetive du train et test set\nnew_train.drop('trip_duration', axis=1, inplace=True)\nnew_test.drop('trip_duration', axis=1, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nCreation de l'arbre Régresseur sans régulariser\n'''\n# Création de l'arbre régresseur sans régularisation\ntree_reg_nr = DecisionTreeRegressor(random_state=42)   \n\n# Entrainement \ntree_reg_nr.fit(new_train, y)\n\n# Afficher les hyperparamètres de l'arbre\ntree_reg_nr.get_params()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nEvaluation de l'arbre entrainé sur le training set\n'''\n# Modèle sans Régularisation\npredictions_nr = tree_reg_nr.predict(new_train)\ntree_mse = mean_squared_error(y, predictions_nr)\ntree_rmse_nr = np.sqrt(tree_mse)\nprint(\"\")\nprint(\"Error in Train set = \" + str(tree_rmse_nr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nAfficher la rélation entre valeurs réelles et calculées sans régularisation\n'''\ndef plot_predictions_vs_val_reelles(tree_reg, y, title, ylabel=\"$variableObjetive$\"):\n    y_pred = tree_reg.predict(new_train)[:100]\n    y_pred = np.asarray(y_pred, dtype='float64')\n    y_orig = y.iloc[:100]\n    y_orig = np.asarray(y_orig, dtype='float64')\n    x1 = np.linspace(0, 100, 100)#.reshape(-1, 1)\n    plt.axis([0, 100, 0, 4000])\n    plt.xlabel(\"$Instances (train set)$\", fontsize=12)\n    if ylabel:\n        plt.ylabel(ylabel, fontsize=12)\n    plt.plot(x1, y_orig, \"bo\", label='valeur_réelle')\n    plt.plot(x1, y_pred, \"r.-\", label='valeur_calculée')\n    plt.legend(loc='upper right')\n    plt.title(title, fontsize=14)\n    plt.show()\n\nplot_predictions_vs_val_reelles(tree_reg_nr, y,\"Prédiction modèle libre\")    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nEvaluation avec K-fold Cross-Validation (Pas de régularisation)\n'''\n# fonction auxiliaire pour afficher les résultats de K-fold Cross-Validation\ndef display_scores(scores):\n    print(\"\")\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n    print(\"\")\n    \n# K-fold Cross-Validation\nscores_nr = cross_val_score(tree_reg_nr, new_train, y, scoring=\"neg_mean_squared_error\", cv=5)\nrmse_scores_nr = np.sqrt(-scores_nr)\n\ndisplay_scores(rmse_scores_nr) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nOptimisation du modèle\n'''\n# Différentes ensembles de paramètres à essayer\nparam_grid_1 = [{'max_depth': [2, 4, 6], 'max_leaf_nodes': [2, 20, 200]}]   #best_params = max_depth: 6, max_leaf_nodes: 200\nparam_grid_2 = [{'max_depth': [6, 7, 8], 'max_leaf_nodes': [20, 200, 500]}]     #best_params = max_depth: 8, max_leaf_nodes: 500\nparam_grid_3 = [{'max_depth': [8, 9, 10], 'max_leaf_nodes': [450, 500, 550]}]     #best_params =  max_depth: 9, max_leaf_nodes: 450\n\n# Bucle pour afficher les meilleurs résultats de chaque essai  \nscore_list = [] \nfor param in list([param_grid_1, param_grid_2, param_grid_3]) :\n    \n    grid_search = GridSearchCV(tree_reg_nr, param, cv=5, n_jobs=-1, \n                           scoring='neg_mean_squared_error', \n                           return_train_score=True)\n    grid_search.fit(new_train, y)\n    cv_results = grid_search.cv_results_\n    score_list.append(np.sqrt(-cv_results[\"mean_test_score\"]))\n    \n    for mean_score, params in zip(cv_results[\"mean_test_score\"], cv_results[\"params\"]):\n        print(np.sqrt(-mean_score), params)\n\n    print(\"\")\n    print(\"Best paramateres: \", grid_search.best_params_)\n    print(\"\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nAfficher les performances de différents sets de paramètres\n'''\ndef plot_erreur_par_config(score_list,):\n    x1 = np.linspace(0, 9, 9)\n    plt.ylabel(\"$Erreur (RMSE)$\", fontsize=12)\n    plt.title(\"Influence Hyper-paramètres\", fontsize=14)\n    for resultat, style, label_ in ((score_list[0],\"b.-\", \"config_1\"),\n                            (score_list[1],\"r.-\", \"config_2\"),\n                            (score_list[2],\"g.-\", \"config_3\")):\n        plt.plot(x1, resultat, style, label=label_)\n    plt.legend(loc='upper right')\n    plt.show()\n\nplot_erreur_par_config(score_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nCréation Arbre Régressor avec Régularisation\n'''\n# Meilleurs paramètres: max_depth= 9, max_leaf_nodes= 450\n\n# Création de l'arbre régresseur avec régularisation\ntree_reg_r = DecisionTreeRegressor(max_depth = 9, max_leaf_nodes = 450, random_state=42)   \n\n# Entrainement \ntree_reg_r.fit(new_train, y)\n\n# Montrer hyperparamètres de l'arbre\ntree_reg_r.get_params()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nEvaluation de l'arbre entrainé sur training set\n'''\n# Modèle avec Régularisation\npredictions_r = tree_reg_r.predict(new_train)\ntree_mse = mean_squared_error(y, predictions_r)\ntree_rmse_r = np.sqrt(tree_mse)\nprint(\"\")\nprint(\"Error in Train set (Regularized) = \" + str(tree_rmse_r))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nAfficher la rélation entre valeurs réelles et calculées avec régularisation\n'''\nplot_predictions_vs_val_reelles(tree_reg_r, y, \"Prédiction modèle limité\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nEvaluation avec K-fold Cross-Validation (Régularisation)\n'''\n#  K-fold Cross-Validation\nscores_r = cross_val_score(tree_reg_r, new_train, y, scoring=\"neg_mean_squared_error\", cv=5)\nrmse_scores_r = np.sqrt(-scores_r)\n\ndisplay_scores(rmse_scores_r) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nEvaluation de l'arbre sur TESTING set\n'''\n# Modèle sans Régularisation\npredictions_nr = tree_reg_nr.predict(new_test)\ntree_mse = mean_squared_error(y_test, predictions_nr)\ntree_rmse_nr_test = np.sqrt(tree_mse)\nprint(\"\")\nprint(\"Error in Test set = \" + str(tree_rmse_nr_test))\n\n# Modele avec Régularisation\npredictions_r = tree_reg_r.predict(new_test)\ntree_mse = mean_squared_error(y_test, predictions_r)\ntree_rmse_r_test = np.sqrt(tree_mse)\nprint(\"\")\nprint(\"Error in Test set (Regularized) = \" + str(tree_rmse_r_test))\n\n#%%\n#'''\n#Affichage des erreurs finales\n#'''\nerr_train = (tree_rmse_nr, tree_rmse_r)\nerr_test = (tree_rmse_nr_test, tree_rmse_r_test)\nx1 = np.linspace(0, 2, 2)#.reshape(-1, 1)\nplt.axis([0, 3, 0, 600])\n#plt.xlabel(\"$Instances (train set)$\", fontsize=12)\nplt.ylabel(\"$Erreur(RMSE)$\", fontsize=12)\nplt.plot(x1, err_train, \"bo-\", label='erreur_train_set')\nplt.plot(x1, err_test, \"ro-\", label='erreur_test_set')\nplt.legend(loc='upper right')\nplt.title(\"$Evolution-de-l'erreur-total$\", fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nAffichage de l'arbre final\n'''\nfrom sklearn.tree import export_graphviz\nimport graphviz\n\n# Création d'une représentation visuelle de l'arbre\nexport_graphviz(tree_reg_r,out_file='duration_tree.dot',\n                feature_names=new_train.columns,\n                class_names=y.columns,\n                rounded=True,\n                filled=True)    \n\n#Conversion du fichier (.dot -> png)\n!dot -Tpng duration_tree.dot -o duration_tree.png -Gdpi=50\n\n# Afficher graphique\nfrom IPython.display import Image \nImage(filename = 'duration_tree.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nCreation d'un arbre avec régularisation NON OPTIMALE juste pour visualisation\n'''\n# Création de l'arbre régresseur sans régularisation\ntree_reg_r_ale = DecisionTreeRegressor(max_depth = 3, random_state=42)   \n\n# Entrainement \ntree_reg_r_ale.fit(new_train, y)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}