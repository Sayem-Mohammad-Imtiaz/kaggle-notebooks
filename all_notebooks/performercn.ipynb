{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install performer-pytorch","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Performer Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Try enwik dataset\nfrom performer_pytorch import PerformerLM\n# Calculates loss\nfrom performer_pytorch.autoregressive_wrapper import AutoregressiveWrapper\n\nimport random\n#import tqdm\nfrom tqdm.notebook import tqdm\nimport gzip\nimport numpy as np\nimport torch\nimport torch.optim as optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, Dataset\n\n# constants\nNUM_BATCHES = 10#int(1e5)\nBATCH_SIZE = 4\nGRADIENT_ACCUMULATE_EVERY = 4\nLEARNING_RATE = 1e-4\nVALIDATE_EVERY  = 100\nGENERATE_EVERY  = 500\nGENERATE_LENGTH = 2048\nSEQ_LEN = 4096\n\n# helpers\ndef cycle(loader):\n    while True:\n        for data in loader:\n            yield data\n\ndef decode_token(token):\n    \"\"\"\n    chr: returns character from string; e.g. chr(97)) > a; chr of <=32 > whitespace\n    \"\"\"\n    return str(chr(max(32, token)))\n\ndef decode_tokens(tokens):\n    return ''.join(list(map(decode_token, tokens)))\n\n\n# instantiate model\n\nmodel = PerformerLM(\n    num_tokens = 22000,          # As many tokens as we tokenize to (21128 if we use transformers voc)\n    dim = 512,\n    depth = 6,\n    max_seq_len = SEQ_LEN,\n    heads = 8,\n    causal = True,\n    reversible = True,\n    nb_features = 256,\n    use_scalenorm = True,\n    local_attn_heads = (8, 8, 8, 6, 4, 2) # Attention Heads per layer\n)\n\nmodel = AutoregressiveWrapper(model)\nmodel.cuda()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 新闻 Data Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"news_df = pd.read_csv(\"../input/chinese-official-daily-news-since-2016/chinese_news.csv\")\n# Concat all content together\n# TODO check if concatting is how it's done in practice\nfull_text = ''.join([str(i) for i in news_df[\"content\"]])\n\n# Note: In reality we need better encoding for CN chars, as eg 中 is encoded as [228 184 173])\n# We will probably just use a vocab.txt with 40K CN chars, like for BERT\nX = np.fromstring(full_text, dtype=np.uint8)\ntrX, vaX = np.split(X, [int(len(full_text)*0.9)])\ndata_train, data_val = torch.from_numpy(trX), torch.from_numpy(vaX)\n\nclass TextSamplerDataset(Dataset):\n    def __init__(self, data, seq_len):\n        super().__init__()\n        self.data = data\n        self.seq_len = seq_len\n\n    def __getitem__(self, index):\n        rand_start = torch.randint(0, self.data.size(0) - self.seq_len - 1, (1,))\n        full_seq = self.data[rand_start: rand_start + self.seq_len + 1].long()\n        return full_seq.cuda()\n\n    def __len__(self):\n        return self.data.size(0) // self.seq_len\n    \ntrain_dataset = TextSamplerDataset(data_train, SEQ_LEN)\nval_dataset   = TextSamplerDataset(data_val, SEQ_LEN)\ntrain_loader  = cycle(DataLoader(train_dataset, batch_size = BATCH_SIZE))\nval_loader    = cycle(DataLoader(val_dataset, batch_size = BATCH_SIZE))\n\n# optimizer\noptim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Alternative - Tokenize with huggingface; Runs in 2min\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n\nprint(\"Vocab Length: \", len(tokenizer.get_vocab()))\n\n# Example:\ntoken_ex = tokenizer.tokenize(\"中\")\nid_ex = tokenizer.convert_tokens_to_ids(token_ex)\nprint(token_ex, id_ex)\n\n# Load & Tokenize data\n\nnews_df = pd.read_csv(\"../input/chinese-official-daily-news-since-2016/chinese_news.csv\")\n# Concat all content together\n# TODO check if concatting is how it's done in practice\nfull_text = ''.join([str(i) for i in news_df[\"content\"]])\n\nX = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(full_text))\n# Note: In reality we need better encoding for CN chars, as eg 中 is encoded as [228 184 173])\n# We will probably just use a vocab.txt with 40K CN chars, like for BERT\nX = np.fromstring(full_text, dtype=np.uint8)\ntrX, vaX = np.split(X, [int(len(full_text)*0.9)])\ndata_train, data_val = torch.from_numpy(trX), torch.from_numpy(vaX)\n\nclass TextSamplerDataset(Dataset):\n    def __init__(self, data, seq_len):\n        super().__init__()\n        self.data = data\n        self.seq_len = seq_len\n\n    def __getitem__(self, index):\n        rand_start = torch.randint(0, self.data.size(0) - self.seq_len - 1, (1,))\n        full_seq = self.data[rand_start: rand_start + self.seq_len + 1].long()\n        return full_seq.cuda()\n\n    def __len__(self):\n        return self.data.size(0) // self.seq_len\n    \ntrain_dataset = TextSamplerDataset(data_train, SEQ_LEN)\nval_dataset   = TextSamplerDataset(data_val, SEQ_LEN)\ntrain_loader  = cycle(DataLoader(train_dataset, batch_size = BATCH_SIZE))\nval_loader    = cycle(DataLoader(val_dataset, batch_size = BATCH_SIZE))\n\n# optimizer\noptim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor i in tqdm(range(NUM_BATCHES), desc='training'):\n    model.train()\n\n    for __ in range(GRADIENT_ACCUMULATE_EVERY):\n        \n        loss = model(next(train_loader), return_loss = True)\n        loss.backward()\n        \n    print(f'training loss: {loss.item()}')\n\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n    optim.step()\n    \n    optim.zero_grad()\n\n    if i % VALIDATE_EVERY == 0:\n        model.eval()\n        with torch.no_grad():\n            loss = model(next(val_loader), return_loss = True)\n            print(f'validation loss: {loss.item()}')\n\n    if i % GENERATE_EVERY == 0 and i != 0:\n        model.eval()\n        inp = random.choice(val_dataset)[:-1]\n        prime = decode_tokens(inp)\n        print(f'%s \\n\\n %s', (prime, '*' * 100))\n\n        sample = model.generate(inp, GENERATE_LENGTH)\n        output_str = decode_tokens(sample)\n        print(output_str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Discard the LM & train body on CL task"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the model\nfrom copy import deepcopy\ntransfer = deepcopy(model.net)\ntransfer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Freeze some layers if nec.\n#for param in transfer.parameters():\n#    param.requires_grad = False\n    \n# Load model.net \n# Replace final Linear(512, 20000) with a Linear(512, 3)\ntransfer.to_out = torch.nn.Linear(512, 3, bias=True)    \ntransfer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(sents, max_seq_len, tokenizer):\n    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n    features = []\n    for sent in sents:\n        # Remove double whitespaces\n        tokens = tokenizer.tokenize(sent)\n\n        if len(tokens) > max_seq_len:\n            tokens = tokens[:(max_seq_len)]\n            #print(\"Too long: \", tokens)\n\n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n        # Zero-pad up to the sequence length.\n        padding = [0] * (max_seq_len - len(input_ids))\n        input_ids += padding\n        \n        assert len(input_ids) == max_seq_len\n        \n        features.append(input_ids)\n        \n    return features\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n\n# Load & Tokenize data\n\nnews_df = pd.read_csv(\"../input/chinese-official-daily-news-since-2016/chinese_news.csv\")\n\n# Drop ~5 NAN content rows\nnews_df = news_df.dropna()\n\n\n#X = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(x)) for x in news_df[\"content\"]]\n#X = np.array(X, dtype=np.uint8)\n\nX = np.array(preprocess(news_df[\"content\"], SEQ_LEN, tokenizer), dtype=np.uint8)\n\n# 详细全文 > 0\n# 国内 > 1\n# 国际 > 2\n\ndef convert_cl(str_lab):\n    if str_lab == \"详细全文\":\n        return 0\n    elif str_lab == \"国内\":\n        return 1\n    elif str_lab == \"国际\":\n        return 2\n    print(\"Unknown Label: \", str_lab)\n    \n\nY = np.array([convert_cl(y) for y in news_df[\"tag\"]], dtype=np.uint8)\n\ntrX, vaX = np.split(X, [int(len(X)*0.9)])\ntrY, vaY = np.split(Y, [int(len(Y)*0.9)])\n\ndata_trainX, data_trainY = torch.from_numpy(trX), torch.from_numpy(trY)\ndata_valX, data_valY = torch.from_numpy(vaX), torch.from_numpy(vaY)\n\nprint(len(X), len(Y))\nprint(len(trX), len(trY))\nprint(trY[1])\n\n\nclass TextClassficationDataset(Dataset):\n    def __init__(self, dataX, dataY, seq_len):\n        super().__init__()\n        self.dataX = dataX\n        self.dataY = dataY\n        self.seq_len = seq_len\n\n    def __getitem__(self, index):\n        X = self.dataX[index][:self.seq_len].long()\n        Y = self.dataY[index].long()\n        return (X.cuda(), Y.cuda())\n        #rand_start = torch.randint(0, self.dataX.size(0) - self.seq_len - 1, (1,))\n        #full_seq = self.data[rand_start: rand_start + self.seq_len + 1].long()\n        #return full_seq.cuda()\n\n    def __len__(self):\n        return self.dataX.size(0) #// self.seq_len\n    \ntrain_dataset = TextClassficationDataset(data_trainX, data_trainY, SEQ_LEN)\nval_dataset   = TextClassficationDataset(data_valX, data_valY, SEQ_LEN)\ntrain_loader  = cycle(DataLoader(train_dataset, batch_size = BATCH_SIZE))\nval_loader    = cycle(DataLoader(val_dataset, batch_size = BATCH_SIZE))\n\n# optimizer\noptim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Additional preparation\n\ncriterion = torch.nn.CrossEntropyLoss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transfer.cuda()\n\nfor i in tqdm(range(NUM_BATCHES), desc='training'):\n    transfer.train()\n\n    for __ in range(GRADIENT_ACCUMULATE_EVERY):\n        \n        x, y = next(train_loader)\n        out = transfer(x)\n        # Take first token as CLS\n        out = out[:, 0, :]\n        \n        loss = criterion(out, y)\n        loss.backward()\n        \n    print(f'training loss: {loss.item()}')\n\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n    optim.step()\n    \n    optim.zero_grad()\n\n    if i % VALIDATE_EVERY == 0:\n        model.eval()\n        with torch.no_grad():\n            x, y = next(val_loader)\n            out = transfer(x)\n            out = out[:, 0, :]\n            loss = criterion(out, y)\n            print(f'validation loss: {loss.item()}')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}