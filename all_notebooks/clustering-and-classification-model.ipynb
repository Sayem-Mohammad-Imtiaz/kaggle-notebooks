{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df= pd.read_csv('../input/glass/glass.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Their are 214 data points and 10 columns in the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Their are no null values present in the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.cluster.hierarchy import linkage,cophenet,dendrogram","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Checking the number of classes in the actual data\ndf['Type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=df.drop('Type',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Standardizing the data before clustering\nfrom scipy.stats import zscore\ndf1=df1.apply(zscore)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### KMeans clustering"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_no = range(1,15)\nwcss=[]\nfor no in cluster_no:\n    km = KMeans(no,random_state=1)\n    km.fit(df1)\n    wcss.append(km.inertia_)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Elbow curve to identify the appropriate no of clusters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.plot(cluster_no,wcss,marker='o')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As seen above  the optimum number of features is 6 hence we can do clustering using 6 as the number of clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"km = KMeans(n_clusters=6,random_state=1)\nkm.fit(df1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"km.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1['Class']= km.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1['Class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" # plot of the clusters using two features\nfig, (ax1, ax2) = plt.subplots(1,2,figsize=(10,8))\n\nax1 = plt.subplot(1,2,1)\nplt.title('Original Classes')\nsns.scatterplot(x='Ca', y='Fe', hue='Type', style='Type', data=df, ax=ax1)\n\nax2 = plt.subplot(1,2,2)\nplt.title('Predicted Classes')\nsns.scatterplot(x='Ca', y='Fe', hue='Class', style='Class', data=df1, ax=ax2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Agglomerative CLusering"},{"metadata":{"trusted":true},"cell_type":"code","source":"df2=df.drop('Type',axis=1)\ndf2=df2.apply(zscore)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.spatial.distance import pdist","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Calculating the Cophenet Distance"},{"metadata":{"trusted":true},"cell_type":"code","source":"Z = linkage(df2, method='complete')\nc, coph_dists = cophenet(Z , pdist(df2))\nc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Z = linkage(df2, method='single')\nc, coph_dists = cophenet(Z , pdist(df2))\nc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Z = linkage(df2, method='ward')\nc, coph_dists = cophenet(Z , pdist(df2))\nc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Z = linkage(df2, method='average')\nc, coph_dists = cophenet(Z , pdist(df2))\nc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Average linkage is better"},{"metadata":{},"cell_type":"markdown","source":"###### Making of Dendogram"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.cluster.hierarchy import linkage, dendrogram\nplt.figure(figsize=[10,10])\nmerg = linkage(df2, method='average')\ndendrogram(merg, leaf_rotation=90)\nplt.title('Dendrogram')\nplt.xlabel('Data Points')\nplt.ylabel('Euclidean Distances')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.cluster.hierarchy import linkage, dendrogram\nplt.figure(figsize=[10,10])\nmerg = linkage(df2, method='ward')\ndendrogram(merg, leaf_rotation=90)\nplt.title('Dendrogram')\nplt.xlabel('Data Points')\nplt.ylabel('Euclidean Distances')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Average method is better but here ward gives a better dendogram for interpretation."},{"metadata":{},"cell_type":"markdown","source":"If we draw a horizontal line at 15 on the y axis the optimal number of cluster is 6 hence making a hierarchical model with 6 clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ac = AgglomerativeClustering(n_clusters=6, affinity ='euclidean',linkage='ward')\nac.fit(df2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2['label']=ac.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2['label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" # plot of the clusters using two features\nfig, (ax1, ax2) = plt.subplots(1,2,figsize=(10,8))\n\nax1 = plt.subplot(1,2,1)\nplt.title('Original Classes')\nsns.scatterplot(x='Ca', y='Fe', hue='Type', style='Type', data=df, ax=ax1)\n\nax2 = plt.subplot(1,2,2)\nplt.title('Predicted Classes')\nsns.scatterplot(x='Ca', y='Fe', hue='label', style='label', data=df2, ax=ax2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Original Classes')\nsns.scatterplot(x='Mg', y='Al', hue='Type', style='Type', data=df)\nplt.show()\nplt.title('K-Means Classes')\nsns.scatterplot(x='Mg', y='Al', hue='Class', style='Class', data=df1)\nplt.show()\nplt.title('Hierarchical Classes')\nsns.scatterplot(x='Mg', y='Al', hue='label', style='label', data=df2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Original Data Classes:')\nprint(df.Type.value_counts())\nprint('-' * 30)\nprint('K-Means Predicted Data Classes:')\nprint(df1.Class.value_counts())\nprint('-' * 30)\nprint('Hierarchical Predicted Data Classes:')\nprint(df2.label.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###### Calculating cohen_kappa_score to check the aggrement","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### For KMeans Label\nfrom sklearn.metrics import cohen_kappa_score\ncohen_kappa_score(df['Type'],df1['Class'] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##### For Agglomerative Label\ncohen_kappa_score(df['Type'],df2['label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3=df1.drop('Class',1)\ndf3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##### Finding Silhouette Score ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import silhouette_score\nsilhouette_score ( df3 , df1['Class']  )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"silhouette_score ( df3 , df2['label']  )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A high value of silhouette score  indicates that the object is well matched to its own cluster\nand poorly matched to neighboring clusters. \nHence here we can say that Kmeans is performing better than Agglomerative Clustering."},{"metadata":{},"cell_type":"markdown","source":"##### Bulding models using  KMeans outcomes"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Decision Tree\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score,roc_auc_score\ndt = DecisionTreeClassifier()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x= df1.drop('Class',axis=1)\ny=df1['Class']\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt.fit(x_train,y_train)\ny_pred=dt.predict(x_test)\ny_prob = dt.predict_proba(x_test)[:,1]\nprint('accuracy_score fot test:',accuracy_score(y_test,y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## KNN model\n\nfrom sklearn.neighbors import KNeighborsClassifier\nkn = KNeighborsClassifier()\nkn.fit(x_train,y_train)\ny_pred=kn.predict(x_test)\ny_prob = kn.predict_proba(x_test)[:,1]\nprint('accuracy_score fot test:',accuracy_score(y_test,y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Logistic regression\n\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\ny_pred=lr.predict(x_test)\ny_prob = lr.predict_proba(x_test)[:,1]\nprint('accuracy_score fot test:',accuracy_score(y_test,y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## SVC\n\nfrom sklearn.svm import SVC\nsvc= SVC(probability=True)\nsvc.fit(x_train,y_train)\ny_pred=svc.predict(x_test)\ny_prob = svc.predict_proba(x_test)[:,1]\nprint('accuracy_score fot test:',accuracy_score(y_test,y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above models we can see that the best model is Logistic Regression and the worst performing model is SVC"},{"metadata":{"trusted":true},"cell_type":"code","source":"###### Using PC for model building ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()\ndf4=df.drop('Type',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_std = StandardScaler().fit_transform(df4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# covariance matrix\ncov_matrix = np.cov(X_std.T)\nprint('Covariance Matrix \\n', cov_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eig_values,eig_vect = np.linalg.eig(cov_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Eigen Vectors \\n', eig_vect)\nprint('\\n Eigen Values \\n', eig_values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tot = sum(eig_values)\nvar_exp = [( i /tot ) * 100 for i in sorted(eig_values, reverse=True)]\ncum_var = np.cumsum(var_exp)\nprint(\"Cumulative Variance Explained\", cum_var)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hence from above we can see that the first 6 principal components can explain 95% of the variablility in the data hence we can use only 6 principal components instead of all the features."},{"metadata":{"trusted":true},"cell_type":"code","source":"pca1 = PCA(n_components=6).fit_transform(X_std)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## KNN model\n\nfrom sklearn.neighbors import KNeighborsClassifier\nkn = KNeighborsClassifier()\nkn.fit(pca1,y)\ny_pred=kn.predict(pca1)\nprint('accuracy_score fot test:',accuracy_score(y,y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic regression\nlr = LogisticRegression()\nlr.fit(pca1,y)\ny_pred=lr.predict(pca1)\nprint('accuracy_score fot test:',accuracy_score(y,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Decision Tree\ndt =DecisionTreeClassifier()\ndt.fit(pca1,y)\ny_pred=dt.predict(pca1)\nprint('accuracy_score fot test:',accuracy_score(y,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## SVM\nsvc= SVC(probability=True)\nsvc.fit(pca1,y)\ny_pred=svc.predict(pca1)\nprint('accuracy_score fot test:',accuracy_score(y,y_pred))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above accuracy scores we can see that Decision tree works really well with the PCA  and it has outperformed Logistic regression. But there might be overfitting issue."},{"metadata":{},"cell_type":"markdown","source":"##### Performing modeling using the labels given by Agglomorative clustering"},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x= df2.drop('label',axis=1)\ny=df2['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## KNN model\n\nfrom sklearn.neighbors import KNeighborsClassifier\nkn = KNeighborsClassifier()\nkn.fit(x_train,y_train)\ny_pred=kn.predict(x_test)\ny_prob = kn.predict_proba(x_test)[:,1]\nprint('accuracy_score fot test:',accuracy_score(y_test,y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Logistic model\n\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\ny_pred=lr.predict(x_test)\ny_prob = lr.predict_proba(x_test)[:,1]\nprint('accuracy_score fot test:',accuracy_score(y_test,y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Decision tree\n\ndt =DecisionTreeClassifier()\ndt.fit(x_train,y_train)\ny_pred=dt.predict(x_test)\ny_prob = dt.predict_proba(x_test)[:,1]\nprint('accuracy_score fot test:',accuracy_score(y_test,y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## SVM\n\nsvc= SVC(probability=True)\nsvc.fit(x_train,y_train)\ny_pred=svc.predict(x_test)\nprint('accuracy_score fot test:',accuracy_score(y_test,y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the labels predicted by hierarchical clustering also Logistic regression performs well ."},{"metadata":{"trusted":true},"cell_type":"code","source":"### with pca ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca2 = PCA()\nx_train_2 = pca2.fit_transform(x_train)\nx_test_2 = pca2.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Decision tree\ndt=DecisionTreeClassifier()\ndt.fit(x_train_2,y_train)\ny_pred = dt.predict(x_test_2)\nprint(\"Accuracy Score:\",accuracy_score(y_test, y_pred) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## SVM\n\nsvc=SVC()\nsvc.fit(x_train_2,y_train)\ny_pred = svc.predict(x_test_2)\nprint(\"Accuracy Score:\",accuracy_score(y_test, y_pred) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## logistic\nlr=LogisticRegression()\nlr.fit(x_train_2,y_train)\ny_pred = lr.predict(x_test_2)\nprint(\"Accuracy Score:\",accuracy_score(y_test, y_pred) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## KNN\nkn=KNeighborsClassifier()\nkn.fit(x_train_2,y_train)\ny_pred = kn.predict(x_test_2)\nprint(\"Accuracy Score:\",accuracy_score(y_test, y_pred) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With pca also logistic regression seems to work better than other models ."},{"metadata":{},"cell_type":"markdown","source":" ##### Overall observation\n    \n-The clustering done by KMeans seems to be better than Hierarchical clustering and aslo its easy to interpret and find the number of cluster.\n-Analysing the number of clusters becomes difficult with dendogram as the number of observation increases.\n-Overall the Logistic regression model performs better than all the other models , with pca and without pca.\n-Decision tree performs well with pca using KMeans  labels but might have overfitting issue ."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nglass = pd.read_csv(\"../input/glass/glass.csv\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}