{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nAn insurance company that has provided Health Insurance to its customers want to predict which customers may be interest in Vehicle Insurance provided by the company. Besides we have some informations about these customers and our goal is that will try to predict potential customers with using data.\n\nIn this kernel, we'll use some of the widely used supervised learning algorithms.\n\n<font color = 'blue'>\nContent:\n    \n1. [Load and Check Data](#1)\n1. [Variable Description](#2)\n1. [Take a Look Data](#3)\n1. [Feature Engineering](#4)\n    * [Drop ID](#5)\n1. [Modeling - Supervised Learning](#6)\n    * [Normalization](#7)\n    * [Train Test Split](#8)\n    * [Simple Logistic Regression](#9)\n    * [Hyperparameter Tuning - Grid Search - Cross Validation](#10)\n    * [Comparison of Accuracy](#11)\n1. [Prediction and Submission](#12)\n1. [Conclusion](#13) "},{"metadata":{},"cell_type":"markdown","source":"<a id = '1'></a><br>\n# Load and Check Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/health-insurance-cross-sell-prediction/train.csv')\ntest_df = pd.read_csv('../input/health-insurance-cross-sell-prediction/test.csv')\n\ntrain_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = '2'></a><br>\n# Variable Description"},{"metadata":{},"cell_type":"markdown","source":"- **id:** Unique ID for the customer \n- **Gender:** Gender of the customer\n- **Age:** Age of the customer\n- **Driving_License:** 0 = Customer does not have DL, 1 = Customer already has DL\n- **Region_Code:** Unique code for the region of the customer\n- **Previously_Insured:** 1 = Customer already has Vehicle Insurance, 0 = Customer doesn't have Vehicle Insurance\n- **Vehicle_Age:** Age of the Vehicle\n- **Vehicle_Damage:** 1 = Customer got his/her vehicle damaged in the past. 0 = Customer didn't get his/her vehicle damaged in the past.\n- **Annual_Premium:** : The amount customer needs to pay as premium in the year\n- **PolicySalesChannel:** Anonymised Code for the channel of outreaching to the customer ie. Different Agents, Over Mail, Over Phone, In Person, etc.\n- **Vintage:** : Number of Days, Customer has been associated with the company\n- **Response:** : 1 = Customer is interested, 0 = Customer is not interested"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Besides, we don't have missing data."},{"metadata":{},"cell_type":"markdown","source":"<a id = '3'></a><br>\n# Take a Look Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# libraries for Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(train_df.Age,train_df.Gender).plot(kind=\"bar\",figsize=(30,8))\nplt.title('Age Frequency for Genders')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplot\ng = sns.FacetGrid(train_df, col = \"Response\", height = 6)\ng.map(sns.distplot, \"Age\", bins = 50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"def bar_plot(variable):\n    \"\"\"\n        input: variable ex: \"Vehicle_Age\"\n        output: bar plot & value count\n    \"\"\"\n    # get feature\n    var = train_df[variable]\n    \n    # count number of categorical variable(value/sample)\n    varValue = var.value_counts()   \n    \n    #visualize\n    plt.figure(figsize =(6,6))\n    labels = varValue.index\n    colors = ['#2C4447','#F3EC86','#679B75','red','green','brown']\n    plt.pie(varValue, labels=labels, colors=colors, autopct='%1.1f%%')\n    plt.ylabel(\"Rate\")\n    plt.title(variable)\n    plt.show()\n    \n    #print(\"{}: \\n {}\".format(variable,varValue))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"category1 = [\"Gender\",\"Vehicle_Age\",\"Vehicle_Damage\"]\nfor c in category1:\n    bar_plot(c)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = '4'></a><br>\n# Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At the same time, we will do same process for test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.get_dummies(train_df, columns = [\"Driving_License\"])\ntest_df = pd.get_dummies(test_df, columns = [\"Driving_License\"])\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,8))\nsns.countplot(x=\"Region_Code\", data = train_df)\nplt.xticks(rotation = 60)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 52 different Region Codes. I think we don't change it."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.get_dummies(train_df, columns = [\"Region_Code\"], prefix = \"RC\")\ntest_df = pd.get_dummies(test_df, columns = [\"Region_Code\"], prefix = \"RC\")\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,8))\nsns.countplot(x=\"Policy_Sales_Channel\", data = train_df)\nplt.xticks(rotation = 60)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"train_df.Policy_Sales_Channel.value_counts().head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a big difference among the Sales Channels here. So we will keep top 8 Sales Channels that sold  most insurances."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"Policy_Sales_Channel\"] = [i if i == 152.0 or i == 26.0 or i == 124.0 or i == 160.0 or i == 156.0 or i==122.0 or i == 157.0 or i == 154.0 else 200 for i in train_df.Policy_Sales_Channel]\ntest_df[\"Policy_Sales_Channel\"] = [i if i == 152.0 or i == 26.0 or i == 124.0 or i == 160.0 or i == 156.0 or i==122.0 or i == 157.0 or i == 154.0 else 200 for i in test_df.Policy_Sales_Channel]\ntrain_df.Policy_Sales_Channel.value_counts().head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,8))\nsns.countplot(x=\"Policy_Sales_Channel\", data = train_df)\nplt.xticks(rotation = 60)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we have less category for Policy Sales Channel but this might work too."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"train_df = pd.get_dummies(train_df, columns = [\"Policy_Sales_Channel\"], prefix = \"SC\")\ntest_df = pd.get_dummies(test_df, columns = [\"Policy_Sales_Channel\"], prefix = \"SC\")\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"train_df = pd.get_dummies(train_df, columns = [\"Vehicle_Damage\"], prefix = \"VD\")\ntest_df = pd.get_dummies(test_df, columns = [\"Vehicle_Damage\"], prefix = \"VD\")\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"train_df = pd.get_dummies(train_df, columns = [\"Vehicle_Age\"], prefix = \"VA\")\ntest_df = pd.get_dummies(test_df, columns = [\"Vehicle_Age\"], prefix = \"VA\")\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"train_df = pd.get_dummies(train_df, columns = [\"Gender\"], prefix = \"G\")\ntest_df = pd.get_dummies(test_df, columns = [\"Gender\"], prefix = \"G\")\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"test_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = '5'></a><br>\n## Drop ID"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.drop(labels = [\"id\"], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = '6'></a><br>\n# Modeling - Supervised Learning\n\nIn this section, we'll use some of the widely used supervised learning classification algorithms. But first we'll prepare data."},{"metadata":{},"cell_type":"markdown","source":"<a id = '7'></a><br>\n## Normalization "},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_df.Response.values\nx_data = train_df.drop([\"Response\"],axis=1)\n\n# normalization \nx = ( x_data - np.min(x_data) ) / ( np.max(x_data) - np.min(x_data) ).values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = '8'></a><br>\n## Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"# %% split data\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_val, y_train, y_val = train_test_split(x,y,test_size = 0.1, random_state = 42) # validation data = 0.1 data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"X_train\",len(x_train))\nprint(\"x_val\",len(x_val))\nprint(\"y_train\",len(y_train))\nprint(\"y_val\",len(y_val))\n\nprint(\"test\",len(test_df))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<a id = '9'></a><br>\n# Simple Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train,y_train)\nacc_log_train = round(logreg.score(x_train, y_train)*100,2)\nacc_log_val = round(logreg.score(x_val, y_val)*100,2)\nprint(\"Training Accuracy: % {}\".format(acc_log_train))\nprint(\"Testing Accuracy: % {}\".format(acc_log_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = '10'></a><br>\n# Hyperparameter Tuning - Grid Search - Cross Validation\n\nWe will compare 6 ml classifier and evaluate mean accuracy of each of them by stratified cross validation. Therefore, we will create subset training data.\n\n* Decision Tree\n* SVM\n* Random Forest\n* KNN\n* Logistic Regression\n* Naive Bayes Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import models\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import GaussianNB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %% split data\n\nx_train1, x_ss, y_train1, y_ss = train_test_split(x_train,y_train,test_size = 0.01, random_state = 42) # subset data = 0.01 training data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_state = 42\nclassifier = [DecisionTreeClassifier(random_state = random_state),\n             SVC(random_state = random_state),\n             RandomForestClassifier(random_state = random_state),\n             LogisticRegression(random_state = random_state),\n             KNeighborsClassifier(),\n             GaussianNB()]\n\ndt_param_grid = {\"min_samples_split\" : range(10,100,20),\n                \"max_depth\": range(1,20,4)}\n\nsvc_param_grid = {\"kernel\" : [\"rbf\"],\n                 \"gamma\": [ 0.01, 0.1, 1],\n                 \"C\": [1,10,50,100,500]}\n\nrf_param_grid = {\"max_features\": [1,3,10],\n                \"min_samples_split\":[2,3,10],\n                \"min_samples_leaf\":[1,3,10],\n                \"bootstrap\":[False],\n                \"n_estimators\":[100,300],\n                \"criterion\":[\"gini\"]}\n\nlogreg_param_grid = {\"C\":np.logspace(-3,3,7),\n                    \"penalty\": [\"l1\",\"l2\"]}\n\nknn_param_grid = {\"n_neighbors\": np.linspace(1,19,10, dtype = int).tolist(),\n                 \"weights\": [\"uniform\",\"distance\"],\n                 \"metric\":[\"euclidean\",\"manhattan\"]}\n\nnaive_param_grid = {}\n\nclassifier_param = [dt_param_grid,\n                   svc_param_grid,\n                   rf_param_grid,\n                   logreg_param_grid,\n                   knn_param_grid,\n                   naive_param_grid]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_result = []\n\nbest_estimators = []\n\nfor i in range(len(classifier)):\n    clf = GridSearchCV(classifier[i], param_grid=classifier_param[i], cv = StratifiedKFold(n_splits = 10), scoring = \"accuracy\", n_jobs = -1,verbose = 1)\n    clf.fit(x_ss,y_ss)     \n    cv_result.append(clf.best_score_) # save best scores\n    best_estimators.append(clf.best_estimator_) # save best estimators\n    print(cv_result[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = '11'></a><br>\n# Comparison of Accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_results = pd.DataFrame({\"Cross Validation Means\":cv_result, \"ML Models\":[\"DecisionTreeClassifier\", \"SVM\",\"RandomForestClassifier\",\n             \"LogisticRegression\",\n             \"KNeighborsClassifier\",\"GaussianNB\"]})\n\ng = sns.barplot(\"Cross Validation Means\", \"ML Models\", data = cv_results)\ng.set_xlabel(\"Mean Accuracy\")\ng.set_title(\"Cross Validation Scores\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will choose 3 ML algortihms.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"votingC = VotingClassifier(estimators = [(\"dt\",best_estimators[0]),\n                                        (\"rfc\",best_estimators[2]),\n                                        (\"lr\",best_estimators[3])],\n                                        voting = \"soft\", n_jobs = -1)\n\nvotingC = votingC.fit(x_train, y_train) \nprint(accuracy_score(votingC.predict(x_val),y_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = '12'></a><br>\n# Prediction and Submission\nActually, we've achieved almost same score with Logistic Regression. Although I will use last model to make prediction. "},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df_id = test_df.id\ntest_df.drop(labels = [\"id\"], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df_response = pd.Series(votingC.predict(test_df), name = \"Response\").astype(int)\n\nresults = pd.concat([test_df_id, test_df_response],axis = 1)\n\nresults.to_csv(\"cross_sell_prediction.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = '13'></a><br>\n## Conclusion\nPlease let me know if you have any suggestions or ideas on how to improve the model and results. Thanks for reading."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}