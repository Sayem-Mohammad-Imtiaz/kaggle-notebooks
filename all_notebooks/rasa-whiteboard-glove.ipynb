{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"id":"P-TZJ-GsQ6Ux","outputId":"c1124759-414e-4284-b0f2-2c4872cdacc5","trusted":true},"cell_type":"code","source":"try:\n  # %tensorflow_version only exists in Colab.\n  %tensorflow_version 2.x\nexcept Exception:\n  pass","execution_count":null,"outputs":[]},{"metadata":{"id":"jP3k0ljEQgp1","trusted":true},"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport matplotlib.pylab as plt","execution_count":null,"outputs":[]},{"metadata":{"id":"mqLHeTh_UpTJ"},"cell_type":"markdown","source":"## Fetching the Data \n\nThis is a bit annoying. But to download from kaggle we need to upload the kaggle API key here. Then we need to move the file to the correct folder after which we need to change the permissions. The error messages will not provide super helpful information so I've added the correct code here. \n\nYou can also upload the dataset from kaggle manually or you can download all of this locally. The kaggle dataset can be found [here](https://www.kaggle.com/therohk/million-headlines).\n\nThen again, this code works;","execution_count":null},{"metadata":{"id":"PVSVaGOzLpmF"},"cell_type":"markdown","source":"The idea is to build something like this; \n\n```\nword_i -> together <- word_j \n```\n\nWe will merely predict if these two words share context. In my book they will share context if they are in the same title. Later we will try to construct document embeddings as well.","execution_count":null},{"metadata":{"id":"ttXDwNveVRt6"},"cell_type":"markdown","source":"## Sequence of Letters \n\nLet's now take these headlines and grab sequences of letters out of them.","execution_count":null},{"metadata":{"id":"banoSM0VWuRG","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport itertools as it \nfrom collections import Counter \nfrom functools import reduce \n\nn_documents = 20000\n\nheadlines = pd.read_csv('/kaggle/input/million-headlines/abcnews-date-text.csv')['headline_text'][:n_documents]","execution_count":null,"outputs":[]},{"metadata":{"id":"qUDQ8LqfdzmJ","outputId":"a6cfd8b5-d1b1-41c5-d0a1-a8a6d8e8876c","trusted":true},"cell_type":"code","source":"headlines","execution_count":null,"outputs":[]},{"metadata":{"id":"YpiCTpOEApPG"},"cell_type":"markdown","source":"The next codeblock looks strange but it is much faster to split the counters up.","execution_count":null},{"metadata":{"id":"kEVtKPBBUjmV","outputId":"b0fccd0f-a708-41d4-8946-9225b49ccf85","trusted":true},"cell_type":"code","source":"import tqdm\n\ncombs = []\nbunchsize = 500\nfor i in tqdm.tqdm(range(round(n_documents/bunchsize))):\n  g = (Counter(it.combinations(i.split(\" \"), 2)) for i in headlines[bunchsize*i:bunchsize*(i+1)])\n  combs.append(reduce(lambda x,y : x + y, g))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combs[0][('act', 'fire')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combs[1][('act', 'fire')]","execution_count":null,"outputs":[]},{"metadata":{"id":"ZA7uzZ8kV0d8","trusted":true},"cell_type":"code","source":"big_word_count = reduce(lambda x,y : x + y, combs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"big_word_count[('abalone', 'penalties')]","execution_count":null,"outputs":[]},{"metadata":{"id":"2QkD8a7YWajd","trusted":true},"cell_type":"code","source":"word_count = Counter([i[0] for i in big_word_count.keys()]) + Counter([i[1] for i in big_word_count.keys()])\nuniq_words = Counter(word_count.keys()).keys()\nnum_words = len(uniq_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(big_word_count.keys())[0]","execution_count":null,"outputs":[]},{"metadata":{"id":"EwVCjxOPV1Hi","trusted":false},"cell_type":"code","source":"m = {c: i for i, c in enumerate(uniq_words)}\n\ndef gen_rand_tok(n):\n  t1 = np.random.choice(list(word_count.keys()), size=n, replace=True)\n  t2 = np.random.choice(list(word_count.keys()), size=n, replace=True)\n  return np.array([[m[w1], m[w2]] for w1, w2 in zip(t1, t2) if (w1, w2) not in big_word_count.keys()])","execution_count":null,"outputs":[]},{"metadata":{"id":"-vQvbovkavSe"},"cell_type":"markdown","source":"This is where we generate the training labels. ","execution_count":null},{"metadata":{"id":"jKX3ib72Y0Pl","trusted":false},"cell_type":"code","source":"positive_integers = np.array([[m[w1], m[w2]] for w1, w2 in big_word_count.keys()])\nnegative_integers = gen_rand_tok(n=positive_integers.shape[0])\n\nintegers_in = np.concatenate([positive_integers, negative_integers])\nlabels_in = np.concatenate([list(big_word_count.values()), np.zeros(negative_integers.shape[0])])","execution_count":null,"outputs":[]},{"metadata":{"id":"Pmt1k0YhQi0m","trusted":false},"cell_type":"code","source":"from tensorflow.keras.layers import Embedding, Dense, Flatten, Input, Dot\nfrom tensorflow.keras.models import Sequential, Model\n\ndim_words = 5\n\n# this one is so we might grab the embeddings\nmodel_emb = Sequential()\nembedding = Embedding(num_words, dim_words, input_length=1)\nmodel_emb.add(embedding)\nmodel_emb.add(Flatten())\n\nword_one = Input(shape=(1,))\nword_two = Input(shape=(1,))\n\ncross_prod = Dot(axes=1)([model_emb(word_one), model_emb(word_two)])\nout = Dense(1, activation=\"relu\")(cross_prod)\n\nglovelike = Model(inputs=[word_one, word_two], outputs=out)","execution_count":null,"outputs":[]},{"metadata":{"id":"pEmpigx8aNEu","trusted":false},"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"id":"R7giKj1SBZpB","outputId":"a95af3a4-25ab-44ec-cdf1-d88cae3f5416","trusted":false},"cell_type":"code","source":"for lr, epo in zip([0.02, 0.01, 0.002], [5, 10, 10]):\n  print(f\"stepsize={lr}\")\n  adam = Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999, amsgrad=False)\n  glovelike.compile(adam, 'mse', metrics=['accuracy'])\n  glovelike.fit(x=[integers_in[:, 0], integers_in[:, 1]], y=labels_in, epochs=epo, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"rnPMKZnZWbMw","trusted":false},"cell_type":"code","source":"# for i in range(100, 200):\n#   print(headlines[i])","execution_count":null,"outputs":[]},{"metadata":{"id":"15mlBdDOcmNx","trusted":false},"cell_type":"code","source":"words = ['violence', 'arrested', 'murder', 'police', 'jury', 'minister', 'health', 'finance', 'banks', 'wildlife', 'doctor',]\nemb = model_emb.predict([m[i] for i in words])","execution_count":null,"outputs":[]},{"metadata":{"id":"QQp8gi3fftUz","outputId":"f1079479-d9fc-433f-9f31-1994edb3a7d2","trusted":false},"cell_type":"code","source":"plt.scatter(emb[:, 0], emb[:, 1], alpha=0)\nfor i, w in enumerate(words):\n  plt.text(emb[i, 0], emb[i, 1], w)","execution_count":null,"outputs":[]},{"metadata":{"id":"D0FtS29oxF1U"},"cell_type":"markdown","source":"## Cosine Distances \n\nLet's compare some of these distances now.","execution_count":null},{"metadata":{"id":"E4MR8YzYmZY7","trusted":false},"cell_type":"code","source":"from scipy.spatial import distance\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"id":"t9FO7BBurF2f","outputId":"0826537f-6902-4de6-ac92-f79117531b9f","trusted":false},"cell_type":"code","source":"df = pd.DataFrame(distance.cdist(emb, emb, 'cosine'), columns=words)\ndf.index = words\nq1, q2 = df.quantile([0.2, 0.8]).mean(axis=1).values\n\ndef color(val):\n    if val < q1:\n        color = 'green'\n    elif val > q2:\n        color = 'red'\n    else:\n        color = 'yellow'\n    return 'background-color: %s' % color\n\ndf.style.applymap(color)","execution_count":null,"outputs":[]},{"metadata":{"id":"s7JW-jkpw9kC"},"cell_type":"markdown","source":"## Euclidean Distances \n\nLet's also have a peek at another type of distance.","execution_count":null},{"metadata":{"id":"CIxg8TrgWJ8_","outputId":"de14bc6c-6599-4a15-ab20-d14d41307d83","trusted":false},"cell_type":"code","source":"df = pd.DataFrame(distance.cdist(emb, emb, 'euclidean'), columns=words)\ndf.index = words\nq1, q2 = df.quantile([0.2, 0.8]).mean(axis=1).values\ndf.style.applymap(color)","execution_count":null,"outputs":[]},{"metadata":{"id":"hg1Cr6XkwyLd"},"cell_type":"markdown","source":"Why is `arrested` so different from `murder`? It feels like they should be similar. \n\nLet's find out.","execution_count":null},{"metadata":{"id":"YMCRilY1AlXA","outputId":"906d3b16-4ead-4901-8cee-a4d425b5ec98","trusted":false},"cell_type":"code","source":"n_arrested = (headlines.loc[lambda d: d.str.contains('arrested')].shape)\nn_murder = (headlines.loc[lambda d: d.str.contains('murder')].shape)\n\nn_both = (headlines\n          .loc[lambda d: d.str.contains('arrested')]\n          .loc[lambda d: d.str.contains('murder')]\n          .shape)\n\nn_arrested, n_murder, n_both","execution_count":null,"outputs":[]},{"metadata":{"id":"paYBcGTgyudu"},"cell_type":"markdown","source":"And what about `money`, `cash`, `zealand` and `domestic`?","execution_count":null},{"metadata":{"id":"_NoBarxAy1DE","outputId":"4a70459d-6b63-433b-e429-a8f9f53fadc5","trusted":false},"cell_type":"code","source":"n_arrested = (headlines.loc[lambda d: d.str.contains('money')].shape)\nn_murder = (headlines.loc[lambda d: d.str.contains('zealand')].shape)\n\nn_both = (headlines\n          .loc[lambda d: d.str.contains('money')]\n          .loc[lambda d: d.str.contains('zealand')]\n          .shape)\n\nn_arrested, n_murder, n_both","execution_count":null,"outputs":[]},{"metadata":{"id":"wEQzSllrxkEM"},"cell_type":"markdown","source":"Note how there is no overlap! \n\n## Conclusion \n\nNotice the tricky business here with word embeddings. They depend on a lot of things; \n\n- the size of the embeddig $k$ \n- the choice of algorithm \n- the choice of the data going in \n\nWe need to be careful that we don't cherry pick our results. The output of the model may not generalise the language well if we pick a bad language task, a bad dataset or if the hyperparameters of the model are wrong. \n\nThe hope though is that if you train on a large corpus you do get something that might be general. It's not always the case though that a word embedding that is general will be appropriate for the virtual assistant setting.\n\n## In Real Life\n\nYou won't need to train these yourself. In fact, it's I'll advised. Best to use the ones trained by others. \n","execution_count":null},{"metadata":{"id":"jfilV_CMzBzF","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}