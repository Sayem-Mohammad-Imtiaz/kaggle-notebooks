{"cells":[{"metadata":{"_cell_guid":"650ba9c0-fbd9-458d-b010-f48134ea3de2","_uuid":"fa212d9829b4078ac80b6c9a9bfcd4343c7b8259"},"cell_type":"markdown","source":"My Aim is to find the best possible models that give a high prediction accuracy in predicting the quality of White wine\nThis also happens to be my first indendent project , so any feedback is highly appreciated "},{"metadata":{"_cell_guid":"b4d7102c-52c0-4b1c-8ed0-f9f87e1daec3","_uuid":"3049f80b8eaebdc124a455a509f46165dffbfafd"},"cell_type":"markdown","source":"1. **Data pre processing **\n1. Lets start with importing the necessary libraries and try to understand the data set "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport os\n","execution_count":2,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"6d934750-76a1-4c82-9ec4-2fc17a08cd44","_uuid":"70079f067b4506ab66ac0d24d05cc8edfdec3563","trusted":true},"cell_type":"code","source":"\ndataset = pd.read_csv('../input/winequality_white.csv',header = 0)\ndataset.head()\n","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"841cb45f-baee-4943-9053-1cf19599b99b","_uuid":"46f67cb02abd361827f3b3eb3b7817017eb82c1f"},"cell_type":"markdown","source":"we can see that the features are continous variables and in different scales , lets try a few visualizations to understand the data set further "},{"metadata":{"_cell_guid":"7eedc643-1e62-425c-bed8-35022ee6f824","_uuid":"ca5678f1ab78f55d0f7f8f828001971184c75c92","trusted":true},"cell_type":"code","source":"dataset.describe()","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"30ea83d7-d0eb-4b3b-8de9-32b6ff6fcf4d","_uuid":"612aebeab950238f5898a030f76ada094e13df28","trusted":true},"cell_type":"code","source":"#checking for null values\ndataset.isnull().sum()","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"351a0f45-402b-4cae-bd95-43b159595a5f","_uuid":"6de174689ffc047e17693c7b250060bde50c279b"},"cell_type":"markdown","source":"Clearly there are no null values , so there we wont have any problems associated with missing values\n\nscatter plots helps us to understand how features vary for different class and the correlation between the features  "},{"metadata":{"_cell_guid":"60648588-6d6a-4cbb-a2e5-68c64cc3e45b","_uuid":"b327dbd4d5dc6d5beb8d6fc49105f8d760067876","trusted":true},"cell_type":"code","source":"sns.pairplot(dataset,vars = ['fixed acidity','volatile acidity','citric acid'],hue = 'quality')\nsns.pairplot(dataset,vars = ['residual sugar','chlorides','free sulfur dioxide'],hue = 'quality')\nsns.pairplot(dataset,vars = ['total sulfur dioxide','density','pH'],hue = 'quality')\n","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"a830b521f427e20006e6d6a1cd21e2c652185736"},"cell_type":"markdown","source":"another way of chekcing correlation when having many variables is correlation matric. when using regression models we might need to drop variables which have high correlation. "},{"metadata":{"trusted":true,"_uuid":"722f86347687d12ac287925dac9c76f2afd62b27"},"cell_type":"code","source":"\n# Compute the correlation matrix\ncorr = dataset.corr()\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n#for custom colors\ncmap = sns.diverging_palette(350, 69, as_cmap=True)\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap,center=0,square=True, linewidths=.5) \nplt.show()\n","execution_count":81,"outputs":[]},{"metadata":{"_cell_guid":"9c9eb059-4a4a-471a-a1c4-da726b46fb5c","_uuid":"c3d107c585f0e39b0b3c12cd290e2037071b746c"},"cell_type":"markdown","source":"\nWe see a high positive correlation between residual sugar and density.\nLets us check the count for each class variable to see if there is any imbalance with the data set"},{"metadata":{"_cell_guid":"58ece4bf-5e3b-42e0-a18f-ef1745ae661a","_uuid":"fa259d46e92950de2cdc150989d9a11235947bf8","trusted":true},"cell_type":"code","source":"\nsns.set_style(\"whitegrid\")\nax = sns.countplot(x=\"quality\", data=dataset)","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"31bd5096-aec6-42bf-abad-ef37a9bc8ee2","_uuid":"7040e17dcfb37378416ae967334e15ce1249b320"},"cell_type":"markdown","source":"**Building Model**\n1. We can clearly see that the quality 5,6 and 7 are the majority and 3,4,8,9 are very negligble.\n1. We will proceed with this imbalance for now , so lets do some feature scaling and proceed with building the models"},{"metadata":{"_cell_guid":"d850290b-e867-4d77-b534-760dedeeb95f","_uuid":"d37d25804c1a083db3b2c7e89d529360db7092e9","trusted":true},"cell_type":"code","source":"# seperate features from class variable\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n#test train split to split the data set \nfrom sklearn.model_selection import train_test_split\n# 10 % of data is my test set \nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.10, random_state=120)\n\n# feature scaling to normalize all the features in one scale\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_train_sc = sc_X.fit_transform(X_train)\nX_test_sc = sc_X.transform(X_test)\n\n","execution_count":53,"outputs":[]},{"metadata":{"_cell_guid":"26fbd62c-c8ee-4ffb-a5f7-b9f81fa1380b","_uuid":"bb72bf7fe4070694bb17b71a49b6eee286087a89","trusted":true},"cell_type":"code","source":"# Got a warning for setting cv value to 5 since i have only 3 values for class variable 3 , so had to reduce the k-fold value to 3 or less \nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {\"max_depth\": [25,26,27,28,30,32],\n              \"max_features\": [3, 5,10],\n              \"min_samples_split\": [2, 3,7, 10],\n              \"min_samples_leaf\": [6,7,10],\n              \"bootstrap\": [True, False],\n              \"criterion\": [\"gini\", \"entropy\"]}\n\nextra_clf = ExtraTreesClassifier()\ngrid_search = GridSearchCV(extra_clf,param_grid=param_grid,cv = 2)\ngrid_search.fit(X_train_sc, y_train)\nresult_Train_et = grid_search.predict(X_train_sc)\nresult_Test_et = grid_search.predict(X_test_sc)\n\n","execution_count":58,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9188250a3b775b8e6b1a2bed96a1ac42737ac3d"},"cell_type":"code","source":"# train accuracy is good but the test accuracy is very poor\nfrom sklearn.metrics import accuracy_score\nscore_et_test = accuracy_score(y_test,result_Test_et)\nscore_et_train = accuracy_score(y_train,result_Train_et)\nprint(\"train accuracy\",score_et_train)\nprint (\"test accuracy\",score_et_test )","execution_count":57,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a910c83eae1634b5340326b5344750f9829157b"},"cell_type":"code","source":"# lets try to balance the class variable and see if there is any change in accuracy \n# thanks to elite data science https://elitedatascience.com/imbalanced-classes\nfrom sklearn.utils import resample\n# Separate majority and minority classes\n\ndf_majority = dataset[dataset.quality==6]\ndf_minority1 = dataset[dataset.quality==9]\ndf_minority2 = dataset[dataset.quality==5]\ndf_minority3 = dataset[dataset.quality==7]\ndf_minority4 = dataset[dataset.quality==8]\ndf_minority5 = dataset[dataset.quality==4]\ndf_minority6 = dataset[dataset.quality==3]\n \n# Upsample minority class\n\ndf_minority1_upsampled = resample(df_minority1, \n                                 replace=True,     # sample with replacement\n                                 n_samples=2198,    # to match majority class\n                                 random_state=123) # reproducible results\ndf_minority2_upsampled = resample(df_minority2, \n                                 replace=True,     # sample with replacement\n                                 n_samples=2198,    # to match majority class\n                                 random_state=123) # reproducible resu\ndf_minority3_upsampled = resample(df_minority3, \n                                 replace=True,     # sample with replacement\n                                 n_samples=2198,    # to match majority class\n                                 random_state=123) # reproducible results                                 \ndf_minority4_upsampled = resample(df_minority4, \n                                 replace=True,     # sample with replacement\n                                 n_samples=2198,    # to match majority class\n                                 random_state=123) # reproducible results\ndf_minority5_upsampled = resample(df_minority5, \n                                 replace=True,     # sample with replacement\n                                 n_samples=2198,    # to match majority class\n                                 random_state=123) # reproducible results\ndf_minority6_upsampled = resample(df_minority6, \n                                 replace=True,     # sample with replacement\n                                 n_samples=2198,    # to match majority class\n                                 random_state=123) # reproducible results \n                                  \n                                  \n# Combine majority class with upsampled minority class\ndf_upsampled = pd.concat([df_majority, df_minority1_upsampled,df_minority2_upsampled,df_minority3_upsampled,df_minority4_upsampled,\n                           df_minority5_upsampled,df_minority6_upsampled])\nsns.set_style(\"whitegrid\")\nax = sns.countplot(x=\"quality\", data=df_upsampled)","execution_count":64,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5aa185fcf510f3851101a7c770d83e4e35e047ea"},"cell_type":"code","source":"# seperate features from class variable\nX = df_upsampled.iloc[:, :-1].values\ny = df_upsampled.iloc[:, -1].values\n#test train split to split the data set \nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.10, random_state=123)\n\n# feature scaling once again for our upsampled dataset \nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_train_sc = sc_X.fit_transform(X_train)\nX_test_sc = sc_X.transform(X_test)","execution_count":61,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7e817d58e3a82f1790aca6f81cc94bf70c6514d"},"cell_type":"code","source":"# now we can mention CV values greater than 3 cause all class variables now are greater than 2000\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {\"max_depth\": [25,26,27,28,30,32],\n              \"max_features\": [3, 5,10],\n              \"min_samples_split\": [2, 3,7, 10],\n              \"min_samples_leaf\": [6,7,10],\n              \"bootstrap\": [True, False],\n              \"criterion\": [\"gini\", \"entropy\"]}\n\nextra_clf = ExtraTreesClassifier()\ngrid_search = GridSearchCV(extra_clf,param_grid=param_grid,cv = 5,n_jobs = -1)\ngrid_search.fit(X_train_sc, y_train)\nresult_Train_et = grid_search.predict(X_train_sc)\nresult_Test_et = grid_search.predict(X_test_sc)","execution_count":68,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b82132d02f822b59a9ccdd95399923f11e29f29a"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nscore_et_test = accuracy_score(y_test,result_Test_et)\nscore_et_train = accuracy_score(y_train,result_Train_et)\nprint(\"train accuracy\",score_et_train)\nprint (\"test accuracy\",score_et_test )","execution_count":69,"outputs":[]},{"metadata":{"_uuid":"7225b8eacf180f2fa6af6ea274c0fb1ccedababb"},"cell_type":"markdown","source":"Now we can see immediate accuracy improvements for both train and test sets , lets try some other algorithms to see how they work"},{"metadata":{"trusted":true,"_uuid":"1f968a34da55a9d991e06292573a8a0bb0b0282a"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nparam_grid = {\"n_neighbors\": [2,5,8],\n              \"algorithm\": [\"ball_tree\", \"kd_tree\", \"brute\"],\n           \"leaf_size\":[10,20,30 ]}\n\nknn = KNeighborsClassifier()\ngrid_search_knn = GridSearchCV(knn,param_grid=param_grid,cv = 5)\ngrid_search_knn.fit(X_train_sc, y_train)\n\nresult_Train_knn = grid_search_knn.predict(X_train_sc)\nresult_Test_knn = grid_search_knn.predict(X_test_sc)\n","execution_count":74,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a578ae336f2175dd19217251a97c976a6cf8a18d"},"cell_type":"code","source":"#calaculate accuracy\nscore_Knn_test = accuracy_score(y_test,result_Test_knn)\nscore_Knn_train = accuracy_score(y_train,result_Train_knn)\nprint(\"train accuracy\",score_Knn_train)\nprint (\"test accuracy\",score_Knn_test )","execution_count":75,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28373995068b253355b7d282e6195db4cf52cfc5"},"cell_type":"code","source":"# lets see how it works with SVM\nfrom sklearn import svm\nparam_grid = {\"C\":[0.8,1.0,2.0],\"kernel\" :[\"linear\",\"rbf\"],\"gamma\" :[0.60, 0.75],\"class_weight\":[\"balanced\"],\"probability\":[False,True]}\nsvm= svm.SVC()\ngrid_search_svm = GridSearchCV(svm,param_grid=param_grid,cv = 5,n_jobs = -1)\ngrid_search_svm.fit(X_train_sc,y_train)\nresult_Train_svm = grid_search_svm.predict(X_train_sc)\nresult_Test_svm = grid_search_svm.predict(X_test_sc)\n\n","execution_count":78,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e124d7cde0ca7e1216a742027a53709716cfaf5"},"cell_type":"code","source":"score_Svm_test = accuracy_score(y_test,result_Test_svm)\nscore_Svm_train = accuracy_score(y_train,result_Train_svm)\nprint(\"train accuracy\",score_Svm_train)\nprint (\"test accuracy\",score_Svm_test )","execution_count":80,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6891be5e77d3e9a8d22b56c57de298770737c807"},"cell_type":"code","source":"# lets try logistic regression \n\nfrom sklearn.linear_model import LogisticRegression\nparam_grid = {\"penalty\":[\"l2\"],\"solver\":[\"newton-cg\", \"sag\" ],\"multi_class\" : [\"ovr\", \"multinomial\"]}\nregr = LogisticRegression()\ngrid_search_regrr = GridSearchCV(regr,param_grid=param_grid,cv = 5,n_jobs = -1)\n                            \ngrid_search_regrr.fit(X_train_sc,y_train)\nresults_Train = grid_search_regrr.predict(X_train_sc)\nresults_Test = grid_search_regrr.predict(X_test_sc)\n\n","execution_count":85,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25f5fa670f00d9fcc24e44e3679d25307dcb3c1f"},"cell_type":"code","source":"#calculate accuracy\nfrom sklearn.metrics import accuracy_score\nscore_regression_test = accuracy_score(y_test,results_Test)\nscore_regression_train = accuracy_score(y_train,results_Train)\nprint(\"train accuracy\",score_regression_train)\nprint (\"test accuracy\",score_regression_test )","execution_count":86,"outputs":[]},{"metadata":{"_uuid":"78c897f7efdc4a6d3944b0551d995aa6f147e196"},"cell_type":"markdown","source":"**Conclusion :**\n1. Feature scaling imporve accuracy for classifiers. \n1. Class imbalance gives lower accuracy values as algorithm wont be trained with all the classes \n1. Grid search helps to get the best possible parameters for different algorithms.\n1. Regression doesnt seem to be good for our dataset \n1. Can try and see if parameters like F1-score and Recall vary before and after dataset is balanced."},{"metadata":{"_uuid":"601becf82bae3e4ac8449e392c74206f200a2eee"},"cell_type":"markdown","source":""}],"metadata":{"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}