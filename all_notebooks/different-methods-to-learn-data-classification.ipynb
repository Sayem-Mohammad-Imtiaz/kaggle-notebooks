{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"90718b4b-9faa-fe75-0f28-c4a780e232cf"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n\n#Import sklearn methods\nfrom sklearn.model_selection import StratifiedShuffleSplit, train_test_split, cross_val_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report, \\\n                            confusion_matrix \nfrom sklearn.feature_selection import GenericUnivariateSelect, f_classif, SelectFromModel\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\n#Import sklearn classifiers\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF, DotProduct\n\n#Import XGBOOST classifier and importance plot method\nfrom xgboost import XGBClassifier, plot_importance\n\n%matplotlib inline"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"976ee420-cf46-d3dd-087f-926b3252666a"},"outputs":[],"source":"#Import data\ndataframe = pd.read_csv(\"../input/Dataset_spine.csv\")\ndataframe = dataframe.drop('Unnamed: 13', axis=1)\ndataframe['Class_att'].replace({\"Abnormal\": 0, \"Normal\": 1}, inplace=True)\ndataframe['Class_att'].unique()\ndataframe.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"674b8e2f-4084-c287-554c-3673069b9911"},"outputs":[],"source":"#Dataframe description\ndataframe.describe()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aa7a963c-df08-6805-ca7c-430f9b980aab"},"outputs":[],"source":"#Check for damaged samples\ndataframe.info()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"98e0a21c-7e1d-5559-497b-c8789576b87d"},"outputs":[],"source":"# Split into data and target\ntarget = dataframe['Class_att']\ndata = dataframe.drop(\"Class_att\", axis=1)\ndel dataframe\ndata.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"08584de4-5c74-7cab-92e3-2430a235acf2"},"outputs":[],"source":"# Correlation matrix\nprint(data.shape)\ndata.corr()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b5a1416c-ffe5-1866-6abe-377ed8fd8912"},"outputs":[],"source":"#Plotted for better visualizing\nplt.figure(figsize=(9,9))\nsns.heatmap(data.corr(), cbar=True, square=True, annot=True,\n            xticklabels = data.columns.tolist(), yticklabels = data.columns.tolist())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7cbd1874-6fb6-527f-ad5b-4b6252ab05de"},"outputs":[],"source":"# Turn data and target values into numpy array\nnames = data.columns.get_values()\ndata = data.as_matrix()\ntarget = target.as_matrix()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c64f2321-6caa-dc51-cc3c-6d72ea195288"},"outputs":[],"source":"# Analysing data - feature selection - GenericUnivariateSelect -> using SelectKBest method \nselector = GenericUnivariateSelect(f_classif,mode=\"k_best\", param=4)\nfit = selector.fit(data,target)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1f11c738-6c3c-2529-d67e-e5fa59bb6d34"},"outputs":[],"source":"figsize = (9,9)\n\nweights = fit.scores_\nind = np.arange(len(names))\nplt.figure(figsize=figsize)\nplt.bar(ind, weights, orientation = 'vertical')\nplt.xticks(ind+0.4, names)\nplt.xlabel(\"Features names\")\nplt.ylabel(\"Weights\")\nplt.title(\"Feature importance\")\n\nfor i, w in enumerate(weights):\n    num = \"%.4f\" % w\n    plt.text(i+0.4, w + 1.5, num, ha='center', color='blue', fontweight='bold')\n\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"74b8f5de-3640-44ff-f9af-c22e0db7a800"},"outputs":[],"source":"# Feature selection - XGBoost feature importance method\nxgsel = XGBClassifier(n_estimators=400)\nxgsel.fit(data, target)\n\nplt.figure(figsize = figsize)\nplot_importance(xgsel)\nthreshold = xgsel.feature_importances_\nthreshold = sorted(threshold)\nprint(threshold)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"16af8883-3af8-28db-be2f-5aba5191e971"},"outputs":[],"source":"# Using feature importance form XGBoost classifier to classify dataset \n# with different algorithms\ncv = StratifiedShuffleSplit(test_size=0.2)\n\nnames = [\n    \"SVC\",\n    \"DecisionTreeClassifier\",\n    \"ExtraTreesClassifier\",\n    \"SGDClassifier\",\n    \"KNeighborsClassifier\",\n    \"GaussianProcessClassifier\",\n    \"AdaBoostClassifier\"\n]\n\nclassifiers = [\n    SVC(C=2.0, tol=1e-4),\n    DecisionTreeClassifier(max_features=\"log2\"),\n    ExtraTreesClassifier(n_estimators=100, bootstrap=True, n_jobs=-1),\n    SGDClassifier(loss=\"perceptron\", penalty=\"elasticnet\", n_iter=10, shuffle=True, n_jobs=-1),\n    KNeighborsClassifier(n_neighbors=2, n_jobs=-1),\n    GaussianProcessClassifier(),\n    AdaBoostClassifier(ExtraTreesClassifier())\n]\n\nclf_values = []\n\nprint_values = False\n\nfor name, clf in zip(names, classifiers):\n    bucket = []\n    print(\"\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n\")\n    print(\"Using classifier:\",name)\n    print(\"\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n\")\n    for thresh in threshold:\n        \n        data_point = {'name':name, \"n_features\":0, \"mean\":0, \"std\":0}\n        \n        selector = SelectFromModel(xgsel, threshold=thresh, prefit=True)\n        X = selector.transform(data)\n        X = StandardScaler().fit_transform(X)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.3)\n        \n        results = cross_val_score(clf, X, target, cv=cv)\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        accuracy = accuracy_score(y_test, y_pred)\n        delic = f1_score(y_test, y_pred)\n        \n        if print_values:\n        \n            print(\"Thresh: %.3f, Number of features: %d\"%(thresh, X.shape[1]))\n            print(\"Mean: %.3f, Std: %.3f\" %(results.mean()*100, results.std()))\n            print(\"Accuracy: %.3f, F1 Score: %.3f\"%(accuracy*100, delic))\n            print(clf)\n            print(\"\\n#######################################\\n\")\n        \n        data_point['n_features'] = X.shape[1]\n        data_point['mean'] = results.mean()*100\n        data_point['std'] = results.std()\n        bucket.append(data_point)\n        \n    clf_values.append(bucket)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2974c31c-5e6e-3f11-134a-6529ca6b54f2"},"outputs":[],"source":"# Using feature importance from GenericUnivariateSelect to classify dataset \n# with different algorithms \nclf_values_2 = []\n\nprint_values = False\n\nfor name, clf in zip(names, classifiers):\n    bucket = []\n    print(\"\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n\")\n    print(\"Using classifier:\",name)\n    print(\"\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n\")\n    for i in range(12):\n        \n        data_point = {'name':name, \"n_features\":0, \"mean\":0, \"std\":0}\n        \n        selector = GenericUnivariateSelect(f_classif, mode='k_best', param=i+1)\n        X = selector.fit_transform(data,target)\n        X = StandardScaler().fit_transform(X)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.3)\n        \n        results = cross_val_score(clf, X, target, cv=cv)\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        accuracy = accuracy_score(y_test, y_pred)\n        delic = f1_score(y_test, y_pred)\n        \n        if print_values:\n        \n            print(\"Number of features: %d\" %  X.shape[1])\n            print(\"Mean: %.3f, Std: %.3f\" %(results.mean()*100, results.std()))\n            print(\"Accuracy: %.3f, F1 Score: %.3f\"%(accuracy*100, delic))\n            #print(clf)\n            print(\"\\n#######################################\\n\")\n        \n        data_point['n_features'] = X.shape[1]\n        data_point['mean'] = results.mean()*100\n        data_point['std'] = results.std()\n        bucket.append(data_point)\n        \n    clf_values_2.append(bucket)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"be80f21c-7932-e1b8-b035-10c7293576a6"},"outputs":[],"source":"#Quick lookup at the data - GUS featuer selection\nfor i in range(len(clf_values_2[0])):\n    print(clf_values_2[0][i])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b0729c6e-786e-eada-14e1-5745a8ca272c"},"outputs":[],"source":"#GenericUnivariateSelect feature selecion - mean accuracy\nplot_data = []\nlegend = []\nx_axis = [i+1 for i in range(12)]\n\nplt.figure(figsize=(9,9))\n\nfor j in range(len(clf_values_2)):\n    #print(clf_values_2[j][0]['name'])\n    legend.append(clf_values_2[j][0]['name'])\n    for i in range(len(clf_values_2[j])):\n        plot_data.append(clf_values_2[j][i]['mean'])\n    plt.plot(x_axis, plot_data)\n    plot_data = []\n\nx_ticks = [i for i in range(14)]\n\nplt.xticks(x_ticks)\nplt.xlabel(\"Number of features\")\nplt.ylabel(\"Mean accuracy\")\nplt.title(\"GUS feature selection\\nMean accuracy plot\")\nplt.legend(legend, loc=0)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"21ed92e5-5d6c-6b8c-a2b7-1f466b0d12fd"},"outputs":[],"source":"#GenericUnivariateSelect feature selecion - standard deviation\nplot_data = []\nlegend = []\nx_axis = [i+1 for i in range(12)]\n\nplt.figure(figsize=(9,9))\n\nfor j in range(len(clf_values_2)):\n    #print(clf_values_2[j][0]['name'])\n    legend.append(clf_values_2[j][0]['name'])\n    for i in range(len(clf_values_2[j])):\n        plot_data.append(clf_values_2[j][i]['std'])\n    plt.plot(x_axis, plot_data)\n    plot_data = []\n\nx_ticks = [i for i in range(14)]\n\nplt.xticks(x_ticks)\nplt.xlabel(\"Number of features\")\nplt.ylabel(\"Standard deviation \")\nplt.title(\"GUS feature selection\\nStandard deviation accuracy plot\")\nplt.legend(legend, loc=0)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f6c84617-d7e0-b90c-3a1f-4cf8c6fd4368"},"outputs":[],"source":"#Quick lookup at the data - XGBoost selection\nfor i in range(len(clf_values[0])):\n    print(clf_values[0][i])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7e3f7f60-460b-8da1-5013-23db19a820ca"},"outputs":[],"source":"#Sort points by number of features\nfor i in range(len(clf_values)):\n    clf_values[i] = sorted(clf_values[i], key=lambda k: k['n_features'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8aa049ee-5eba-b2ea-f50d-f3719c6a9d3d"},"outputs":[],"source":"#Quick check\nfor i in range(len(clf_values[0])):\n    print(clf_values[0][i])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"187373a3-cbab-22d5-04c8-bb4fab1c64b5"},"outputs":[],"source":"#XGBoost feature selecion - mean accuracy\nplot_data = []\nlegend = []\nx_axis = [i+1 for i in range(12)]\n\nplt.figure(figsize=(9,9))\n\nfor j in range(len(clf_values)):\n    #print(clf_values[j][0]['name'])\n    legend.append(clf_values[j][0]['name'])\n    for i in range(len(clf_values[j])):\n        plot_data.append(clf_values[j][i]['mean'])\n    plt.plot(x_axis, plot_data)\n    plot_data = []\n\nx_ticks = [i for i in range(14)]\n\nplt.xticks(x_ticks)\nplt.xlabel(\"Number of features\")\nplt.ylabel(\"Mean accuracy\")\nplt.title(\"XGBoost feature selection\\nMean accuracy plot\")\nplt.legend(legend, loc=0)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5d365910-2dd1-92a1-17f3-866cc962d3ec"},"outputs":[],"source":"#XGBoost feature selecion - standard deviation\nplot_data = []\nlegend = []\nx_axis = [i+1 for i in range(12)]\n\nplt.figure(figsize=(9,9))\n\nfor j in range(len(clf_values_2)):\n    #print(clf_values_2[j][0]['name'])\n    legend.append(clf_values_2[j][0]['name'])\n    for i in range(len(clf_values_2[j])):\n        plot_data.append(clf_values_2[j][i]['std'])\n    plt.plot(x_axis, plot_data)\n    plot_data = []\n\nx_ticks = [i for i in range(14)]\n\nplt.xticks(x_ticks)\nplt.xlabel(\"Number of features\")\nplt.ylabel(\"Standard deviation\")\nplt.title(\"XGBoost feature selection\\nStandard deviation plot\")\nplt.legend(legend, loc=0)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"5b76e1de-1221-5daa-e494-221de95362b7"},"source":"That is it for now! Again sorry for not using XGBoost classifier. It couldn't execute few times(probably not enough memory or processor power). If you want anything more or you want to express your attitude towards this notebook send me a message or post comment. I will be very happy if this script would be helpful ;)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bcb065ef-7c72-85c4-54f4-4a42382402a4"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}