{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Universidad San Francisco de Quito\n## IIN-3007 Analítica de Datos\n### Paralelo: 1\n### NRC: 1635\n### Semestre: Verano 2019-2020 (201930)\n## Los Pollos Hermanos:\n### Ana Caamaño, 200229\n### Carlos Herrera, 200619\n### Yuvinne Guerrero, 201420\n### Arturo Romo Leroux, 201690\n\n## Avance Proyecto #2\n\n#### Fecha de entrega: 13-Julio-2020\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importacion de librerias\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\nimport statistics\nimport seaborn as sns\nimport sklearn as skl\nimport scipy.stats as st\nfrom sklearn.model_selection import cross_validate #importo libreria para cross validation\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\npd.set_option('display.float_format', lambda x: '%.2f' % x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importacion de datasets\nOriginal_Train=pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\nOriginal_Test=pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Original_Train.info() #exploracion inicial","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Original_Test.info() #exploracion inicial","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Adición de las nuevas variables y codificación de las existentes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Original_Train['ExterQual'].value_counts()\nexterqual_nums={'ExterQual': {\"Ex\":5, \"Gd\":4,'TA':3,'Fa':2}}\nOriginal_Train.replace(exterqual_nums,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Original_Test['ExterQual'].value_counts()\nexterqual_nums={'ExterQual': {\"Ex\":5, \"Gd\":4,'TA':3,'Fa':2}}\nOriginal_Test.replace(exterqual_nums,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"data = [Original_Train, Original_Test] #creacion de gurpo para el loop\nfor dataset in data:\n    dataset['BaseArea'] = dataset['GrLivArea'] + dataset['GarageArea'] #creacion de variable relatives\n    dataset['Qual_Time'] = (dataset['YearBuilt']-dataset['YearRemodAdd']) \n    dataset['Exqual_YearB'] = (dataset['YearBuilt']*dataset['ExterQual'])\nOriginal_Train.head() #visualizacion","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# establecer grupos de variables cuantitatvas y cualitativas\nquantitative_ori = [f for f in Original_Train.columns if Original_Train.dtypes[f] != 'object']\n#remover saleprice y id de las variable cualitativas\nquantitative_ori.remove('SalePrice')\nquantitative_ori.remove('Id')\nqualitative_ori = [f for f in Original_Train.columns if Original_Train.dtypes[f] == 'object']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# distribución de las variables caulitativas\n#Original_melt= pd.melt(Original_Train, value_vars=quantitative_ori) # melt el data set original para obtener las variables y sus valores\n#graph = sns.FacetGrid(Original_melt, col=\"variable\",  col_wrap=2, sharex=False, sharey=False) #establecer los ejes del grafico\n#graph = graph.map(sns.distplot, \"value\") #impirmir el graph y mapear las distribuciones de cada valor\n\n#referencia: https://www.kaggle.com/dgawlik/house-prices-eda","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cova=Original_Train.cov()\ncova['ExterQual'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr=Original_Train.corr()\ncorr_extq=corr['ExterQual'].sort_values(ascending=False)\ncorr_extq.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"saleprice_corr=pd.DataFrame(corr['SalePrice'])\nsaleprice_corr.sort_values('SalePrice',ascending=False, inplace=True)\nsaleprice_corr.reset_index(inplace=True)\nsaleprice_corr.rename(columns={'index':'Variable'}, inplace=True)\nsaleprice_corr.head(10)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"s=corr.abs().unstack()\nso =pd.DataFrame(s.sort_values())\nso","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches(14, 7)\nax.set_title('Features correlation', )\nsns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, vmin=-1, vmax=1, annot=False, fmt=\".2f\", cmap='RdBu', center=0, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Estadísticas set de datos originales","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Original_Esta=Original_Test.describe()\nOriginal_Esta","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#encontrar el total de valores perdidos de cada variable\ntrain_total = Original_Train.isnull().sum().sort_values(ascending=False)\n#expresar el porcentaje de valores perdidos por variable\ntrain_percent_1 = Original_Train.isnull().sum()/Original_Train.isnull().count()*100\n#redondear los decimales del porcentaje\ntrain_percent_2 = (round(train_percent_1, 1)).sort_values(ascending=False)\n#crear un nuevo dataframe que muestre el total y porcentaje\ntrain_missing_data = pd.DataFrame([train_total, train_percent_2], index=[\"Total\",'Porcentaje']).T\ntrain_missing_data #visualizacion","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_train = Original_Train.isnull().sum()\nmissing_train = missing_train[missing_train > 0]\nmissing_train.sort_values(inplace=True)\nmissing_train.plot.bar()\n#plt.set_title('Missing Values')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#encontrar el total de valores perdidos de cada variable para el set de test\nTest_total = Original_Test.isnull().sum().sort_values(ascending=False)\n#expresar el porcentaje de valores perdidos por variable\nTest_percent_1 = Original_Test.isnull().sum()/Original_Test.isnull().count()*100\n#redondear los decimales del porcentaje\nTest_percent_2 = (round(Test_percent_1, 1)).sort_values(ascending=False)\n#crear un nuevo dataframe que muestre el total y porcentaje\nTest_missing_data = pd.DataFrame([Test_total, Test_percent_2], index=[\"Total\",'Porcentaje']).T\nTest_missing_data.head(10) #visualizacion","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_test = Original_Test.isnull().sum()\nmissing_test = missing_test[missing_test > 0]\nmissing_test.sort_values(inplace=True)\nmissing_test.plot.bar()\n#plt.set_title('Missing Values')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Limpieza de Datos y asignación de valores perdidos\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> Los valores perdidos de las variables de interés serán reemplazados por la mediana del correspondiente set de datos. Según Schmueli, la mediana es una medida de tendencia central robusta hacia los datos atípicos, por lo que su uso para asignación de valores perdidos es recomendado sobre la media de la muestra.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# crear un nuevo dataframe de set de datos editados\nTrain_Edited=pd.DataFrame(Original_Train.copy())\nTest_Edited=pd.DataFrame(Original_Test.copy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_Edited['BsmtQual'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> #### La variable PoolQC cuenta con el 98% de datos perdidos; sin embargo, es posible corregir en su totalidad este error dado que el hecho de que no existan valores de esta variable para las distintas casas quiere decir que estas no tienen piscina. De manera adicional, se realizó una codificación de la variable categórica.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = [Train_Edited, Test_Edited] #creacion de gurpo para el loop\nfor dataset in data:\n    pool_qual = {\"Ex\": 4, \"Gd\": 3, \"TA\":2, \"Fa\":1} #crear un diccionario de correspondencia\n    dataset['PoolQC'] = dataset['PoolQC'].map(pool_qual)  #cambiar los valores origniales con los del dic\n    dataset['PoolQC']=dataset['PoolQC'].fillna(0) #cambiar los valores nulos por 0\n    dataset['PoolQC']=dataset['PoolQC'].astype(int)\nTest_Edited['PoolQC'].isnull().sum() #contar la cantidad de valores nulos","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_Edited['PoolQC'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> #### La variable Fence cuenta con el 81% de datos perdidos; sin embargo, es posible corregir en su totalidad este error dado que el hecho de que no existan valores de esta variable para las distintas casas quiere decir que estas no tienen fence. De manera adicional, se realizó una codificación de la variable categórica.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = [Train_Edited, Test_Edited] #creacion de gurpo para el loop\nfor dataset in data:\n    fence_qual = {\"GdPrv\": 4, \"MnPrv\": 3, \"GdWo\":2, \"MnWw\":1} #crear un diccionario de correspondencia\n    dataset['Fence'] = dataset['Fence'].map(fence_qual)  #cambiar los valores origniales con los del dic\n    dataset['Fence']=dataset['Fence'].fillna(0) #cambiar los valores nulos por 0\n    dataset['Fence']=dataset['Fence'].astype(int)\nTest_Edited['Fence'].isnull().sum() #contar la cantidad de valores nulos","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_Edited['Fence'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> #### La variable Alley cuenta con el 94% de datos perdidos; sin embargo, es posible corregir en su totalidad este error dado que el hecho de que no existan valores de esta variable para las distintas casas quiere decir que estas no tienen alley. De manera adicional, se realizó una codificación de la variable categórica.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = [Train_Edited, Test_Edited] #creacion de gurpo para el loop\nfor dataset in data:\n    alley_qual = {\"Grvl\": 2, \"Pave\": 1} #crear un diccionario de correspondencia\n    dataset['Alley'] = dataset['Alley'].map(alley_qual)  #cambiar los valores origniales con los del dic\n    dataset['Alley']=dataset['Alley'].fillna(0) #cambiar los valores nulos por 0\n    dataset['Alley']=dataset['Alley'].astype(int)\nTest_Edited['Alley'].isnull().sum() #contar la cantidad de valores nulos","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_Edited['Alley'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> #### La variable MiscFeature cuenta con el 97% de datos perdidos; sin embargo, es posible corregir en su totalidad este error creando dummy variables a partir de sus categorías\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_Edited['Shed']=np.where(Original_Train['MiscFeature']=='Shed',1,0)\nTrain_Edited['Gar2']=np.where(Original_Train['MiscFeature']=='Gar2',1,0)\nTrain_Edited['Othr']=np.where(Original_Train['MiscFeature']=='Othr',1,0)\nTrain_Edited['TenC']=np.where(Original_Train['MiscFeature']=='TenC',1,0)\nTrain_Edited\n#data = [Train_Edited, Test_Edited] #creacion de gurpo para el loop\n#for dataset in data:\n#    misc_fea = {\"Shed\": 4, \"Gar2\": 3, \"Othr\": 2, \"TenC\": 1} #crear un diccionario de correspondencia\n#    dataset['MiscFeature'] = dataset['MiscFeature'].map(misc_fea)  #cambiar los valores origniales con los del dic\n#    dataset['MiscFeature']=dataset['MiscFeature'].fillna(0) #cambiar los valores nulos por 0\n#    dataset['MiscFeature']=dataset['MiscFeature'].astype(int)\n#Train_Edited['MiscFeature'].isnull().sum() #contar la cantidad de valores nulos","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_Edited['Shed'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testcor=Train_Edited.corr()\ntestcor[['Shed', 'Gar2', 'Othr', 'TenC']].loc['SalePrice':].head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> En base al análisis de correlación realizado, es posible concluir que ninguna de las categorías de MiscFeature influye en el precio, por lo que la variable puede ser removida con seguridad","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> #### La variable MiscFeature cuenta con el 47% de datos perdidos; sin embargo, es posible corregir en su totalidad este error dado que el hecho de que no existan valores de esta variable para las distintas casas quiere decir que estas no tienen fire place. De manera adicional, se realizó una codificación de la variable categórica.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = [Train_Edited, Test_Edited] #creacion de gurpo para el loop\nfor dataset in data:\n    fire_qua = {\"Ex\": 5, \"Gd\": 4, \"TA\":3, \"Fa\":2, \"Po\":1}  #crear un diccionario de correspondencia\n    dataset['FireplaceQu'] = dataset['FireplaceQu'].map(fire_qua)  #cambiar los valores origniales con los del dic\n    dataset['FireplaceQu']=dataset['FireplaceQu'].fillna(0) #cambiar los valores nulos por 0\n    dataset['FireplaceQu']=dataset['FireplaceQu'].astype(int)\nTrain_Edited['FireplaceQu'].isnull().sum() #contar la cantidad de valores nulos","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_Edited['BsmtQual'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_Edited","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reemplazar los valores perdidos de LotFrontage con la mediana\nTrain_Edited['LotFrontage'].fillna(Original_Train['LotFrontage'].median(), inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reemplazar los valores perdidos de LotFrontage con la mediana\nTest_Edited['LotFrontage'].fillna(Original_Test['LotFrontage'].median(), inplace = True)\nTest_Edited['BsmtFullBath'].fillna(Original_Test['BsmtFullBath'].median(), inplace = True)\nTest_Edited['BsmtHalfBath'].fillna(Original_Test['BsmtHalfBath'].median(), inplace = True)\nTest_Edited['TotalBsmtSF'].fillna(Original_Test['TotalBsmtSF'].median(), inplace = True)\nTest_Edited['GarageCars'].fillna(Original_Test['GarageCars'].median(), inplace = True)\nTest_Edited['GarageArea'].fillna(Original_Test['GarageArea'].median(), inplace = True)\nTest_Edited['BsmtUnfSF'].fillna(Original_Test['BsmtUnfSF'].median(), inplace = True)\nTest_Edited['BsmtFinSF2'].fillna(Original_Test['BsmtFinSF2'].median(), inplace = True)\nTest_Edited['BsmtFinSF1'].fillna(Original_Test['BsmtFinSF1'].median(), inplace = True)\nTest_Edited['BaseArea'].fillna(Original_Test['BaseArea'].median(), inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_Edited.drop(columns=['Shed', 'Gar2', 'Othr', 'TenC', 'MiscFeature', 'GarageYrBlt', 'MasVnrArea'], axis=1, inplace=True) #eliminar la variable MiscFeature\n#encontrar el total de valores perdidos de cada variable en el nuevo dataframe de train\ntrain_total_edited = Train_Edited.isnull().sum().sort_values(ascending=False)\n#expresar el porcentaje de valores perdidos por variable\ntrain_percent_1_edited = Train_Edited.isnull().sum()/Train_Edited.isnull().count()*100\n#redondear los decimales del porcentaje\ntrain_percent_2_edited = (round(train_percent_1_edited, 1)).sort_values(ascending=False)\n#crear un nuevo dataframe que muestre el total y porcentaje\ntrain_missing_data_edited = pd.DataFrame([train_total_edited, train_percent_2_edited], index=[\"Total\",'Porcentaje']).T\ntrain_missing_data_edited.head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_Edited.drop(columns=['MiscFeature', 'GarageYrBlt', 'MasVnrArea'], axis=1, inplace=True) #eliminar la variable MiscFeature\n#encontrar el total de valores perdidos de cada variable en el nuevo dataframe de Test\nTest_total_edited = Test_Edited.isnull().sum().sort_values(ascending=False)\n#expresar el porcentaje de valores perdidos por variable\nTest_percent_1_edited = Test_Edited.isnull().sum()/Test_Edited.isnull().count()*100\n#redondear los decimales del porcentaje\nTest_percent_2_edited = (round(Test_percent_1_edited, 1)).sort_values(ascending=False)\n#crear un nuevo dataframe que muestre el total y porcentaje\nTest_missing_data_edited = pd.DataFrame([Test_total_edited, Test_percent_2_edited], index=[\"Total\",'Porcentaje']).T\nTest_missing_data_edited.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Original_Train['BsmtQual'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reducción de Dimensionalidad","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> Se usará el algoritmo predefinido de PCA para reducir la dimensionalidad de los set de datos","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# crear un nuevo dataframe \nTrain_pca=Train_Edited.drop(columns=['Id']) #eliminar las columnas Id y SalePrice\nTrain_pca","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# establecer grupos de variables cuantitatvas y cualitativas\nquantitative = [f for f in Train_pca.columns if Train_pca.dtypes[f] != 'object']\nqualitative = [f for f in Train_pca.columns if Train_pca.dtypes[f] == 'object']\nquantitative_SP = [f for f in Train_pca.columns if Train_pca.dtypes[f] != 'object']\nquantitative_SP.remove('SalePrice')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#normalizar datos\nTrain_N=skl.preprocessing.StandardScaler().fit(Train_pca[quantitative]).transform(Train_pca[quantitative].astype(float))\nTrain_N=pd.DataFrame(Train_N.copy(), columns=Train_pca[quantitative].columns) # renombrar columanas con los nombres del DF original\nTrain_N #visualizar DF","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#correr modelo PCA \nTrain_N_SP=Train_N.copy().drop(columns='SalePrice')\npcs = skl.decomposition.PCA()\npcs.fit(Train_N_SP)\n#obtener estadisticas de los componentes principales\npcsSummary_df = pd.DataFrame({'Standard deviation': np.sqrt(pcs.explained_variance_),\n                           'Proportion of variance': pcs.explained_variance_ratio_,\n                           'Cumulative proportion': np.cumsum(pcs.explained_variance_ratio_)})\npcsSummary_df = pcsSummary_df.transpose() #transponer DF\npcsSummary_df.columns = ['PC{}'.format(i) for i in range(1, len(pcsSummary_df.columns)+1)] #establecer el nombre de las columnas\npcsSummary_df.round(4) #precision de 4 decimales\npcsSummary_df.iloc[:,:20] #mostrar unicamente los 20 primeros PC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encontrar los pesos de cada PC\npcsComponents_df = pd.DataFrame(pcs.components_.transpose(), columns=pcsSummary_df.columns, \n                                index=Train_N_SP[quantitative_SP].columns)\npcsComponents_df.iloc[:10,:13] #mostrar los pesos de los 13 primeros componentes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"houses_red_df = Train_N_SP[quantitative_SP].dropna(axis=0)\nhouses_red_df = Train_N_SP[quantitative_SP].reset_index(drop=True)\n\nscores = pd.DataFrame(pcs.fit_transform(skl.preprocessing.scale(houses_red_df.dropna(axis=0))), \n                      columns=[f'PC{i}' for i in range(1, 43)])\nhouses_pca_df = pd.concat([houses_red_df['MSSubClass'].dropna(axis=0), scores[['PC1', 'PC2']]], axis=1)\nax = houses_pca_df.plot.scatter(x='PC1', y='PC2', figsize=(6, 6))\npoints = houses_pca_df[['PC1','PC2','MSSubClass']]\n\ntexts = []\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#obtener los valores transformados de cada uno de PC para todos los registros del set de datos\ntransform_df = pd.DataFrame(pcs.transform(Train_N_SP[quantitative_SP]), \n                      columns=pcsSummary_df.columns)\ntransform_df_13=transform_df.iloc[:,:13].copy() # DF con 13 componentes principales\ntransform_df_20=transform_df.iloc[:,:20].copy() # DF con 20 componentes principales\ntransform_df_20.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PCA para el set de test\n\n# crear un nuevo dataframe \nTest_pca=Test_Edited.drop(columns=['Id']) #eliminar las columnas Id y SalePrice\nTest_pca\n\n\n#normalizar datos\nTest_N=skl.preprocessing.StandardScaler().fit(Test_pca[quantitative_SP]).transform(Test_pca[quantitative_SP].astype(float))\nTest_N=pd.DataFrame(Test_N.copy(), columns=Test_pca[quantitative_SP].columns) # renombrar columanas con los nombres del DF original\nTest_N #visualizar DF\n\n\n#correr modelo PCA \nTest_N_SP=Test_N.copy()\npcs_test = skl.decomposition.PCA()\npcs_test.fit(Test_N_SP)\n#obtener estadisticas de los componentes principales\npcsSummary_test_df = pd.DataFrame({'Standard deviation': np.sqrt(pcs_test.explained_variance_),\n                           'Proportion of variance': pcs_test.explained_variance_ratio_,\n                           'Cumulative proportion': np.cumsum(pcs_test.explained_variance_ratio_)})\npcsSummary_test_df = pcsSummary_test_df.transpose() #transponer DF\npcsSummary_test_df.columns = ['PC{}'.format(i) for i in range(1, len(pcsSummary_test_df.columns)+1)] #establecer el nombre de las columnas\npcsSummary_test_df.round(4) #precision de 4 decimales\npcsSummary_test_df.iloc[:,:20] #mostrar unicamente los 20 primeros PC\n\npcsComponents_test_df = pd.DataFrame(pcs_test.components_.transpose(), columns=pcsSummary_test_df.columns, \n                                index=Test_N_SP[quantitative_SP].columns)\npcsComponents_test_df.iloc[:10,:13] #mostrar los pesos de los 13 primeros componentes\n\n\nhouses_red_df_test = Test_N_SP[quantitative_SP].dropna(axis=0)\nhouses_red_df_test = Test_N_SP[quantitative_SP].reset_index(drop=True)\n\nscores_test = pd.DataFrame(pcs_test.fit_transform(skl.preprocessing.scale(houses_red_df_test.dropna(axis=0))), \n                      columns=[f'PC{i}' for i in range(1, 43)])\nhouses_pca_df_test = pd.concat([houses_red_df_test['MSSubClass'].dropna(axis=0), scores_test[['PC1', 'PC2']]], axis=1)\nax = houses_pca_df_test.plot.scatter(x='PC1', y='PC2', figsize=(6, 6))\npoints = houses_pca_df_test[['PC1','PC2','MSSubClass']]\n\ntexts = []\nplt.show()\n\n#obtener los valores transformados de cada uno de PC para todos los registros del set de datos\ntransform_df_test = pd.DataFrame(pcs_test.transform(Test_N_SP[quantitative_SP]), \n                      columns=pcsSummary_test_df.columns)\ntransform_df_test_13=transform_df_test.iloc[:,:13].copy() # DF con 13 componentes principales\ntransform_df_test_20=transform_df_test.iloc[:,:20].copy() # DF con 20 componentes principales\ntransform_df_test_20.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelos","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> ### 13 PC (69 % de variabilidad explicada)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"PCA_X_train_13 = transform_df_13 #definimos los feautures del set train para los modelos de prediccion\nPCA_Y_train_13 = Train_Edited[\"SalePrice\"] #definimos el target\nPCA_X_test_13 = transform_df_test_13 #definimos sobre que set de datos se desean realizar las prediccione\n\n#random forest model\nPCA_RF_13 = RandomForestClassifier(n_estimators=100)  #llamamos al primer modelo definiendo el numero de estimadores\nPCA_RF_13.fit(PCA_X_train_13, PCA_Y_train_13) #asignamos el set de train\nPCA_RF_EXIST_13 = PCA_RF_13.predict(PCA_X_train_13)\nPCA_Y_prediction_13 = PCA_RF_13.predict(PCA_X_test_13) #asignamos sobre que deseamos calcular el predictor\nPCA_ACC_RF_13 = PCA_RF_13.score(PCA_X_train_13, PCA_Y_train_13) * 100 #sacamos el porcetaje de certeza del modelo\nPCA_MAE_RF_13= skl.metrics.mean_absolute_error(PCA_RF_EXIST_13, PCA_Y_train_13)\nPCA_MSE_RF_13= skl.metrics.mean_squared_error(PCA_RF_EXIST_13, PCA_Y_train_13)\nPCA_MSE_RF_13","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# adaboosreg model\nX_13=PCA_X_train_13\ny_13=PCA_Y_train_13\nPCA_AB_13=skl.ensemble.AdaBoostRegressor(random_state=0, n_estimators=100)\nPCA_AB_13.fit(X_13,y_13)\nPCA_Y_pred_ADAB_13=PCA_AB_13.predict(PCA_X_test_13)\nPCA_AB_EXIST_13 = PCA_AB_13.predict(PCA_X_train_13)\nPCA_ACC_AB_13= PCA_AB_13.score(PCA_X_train_13, PCA_Y_train_13) * 100 #sacamos el porcetaje de certeza del modelo\nPCA_MAE_AB_13= skl.metrics.mean_absolute_error(PCA_AB_EXIST_13, PCA_Y_train_13)\nPCA_MSE_AB_13= skl.metrics.mean_squared_error(PCA_AB_EXIST_13, PCA_Y_train_13)\n#referencia: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#linear regresion model\nPCA_LR_13=skl.linear_model.LinearRegression()\nPCA_LR_13.fit(X_13, y_13)\nPCA_Y_pred_LR_13= PCA_LR_13.predict(PCA_X_test_13)\nPCA_LR_EXIST_13 = PCA_LR_13.predict(PCA_X_train_13)\nPCA_ACC_LR_13 = PCA_LR_13.score(PCA_X_train_13, PCA_Y_train_13) * 100 #sacamos el porcetaje de certeza del modelo\nPCA_MAE_LR_13= skl.metrics.mean_absolute_error(PCA_LR_EXIST_13, PCA_Y_train_13)\nPCA_MSE_LR_13= skl.metrics.mean_squared_error(PCA_LR_EXIST_13, PCA_Y_train_13)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Intercept (b0):',PCA_LR_13.intercept_)\n# For retrieving the slope:\nprint('****************')\nprint('(bi):',PCA_LR_13.coef_)\nprint('****************')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### 20 PC (85 % de variabilidad explicada)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"PCA_X_train_20 = transform_df_20 #definimos los feautures del set train para los modelos de prediccion\nPCA_Y_train_20 = Train_Edited[\"SalePrice\"] #definimos el target\nPCA_X_test_20 = transform_df_test_20 #definimos sobre que set de datos se desean realizar las prediccione\n\n#random forest model\nPCA_RF_20 = RandomForestClassifier(n_estimators=100)  #llamamos al primer modelo definiendo el numero de estimadores\nPCA_RF_20.fit(PCA_X_train_20, PCA_Y_train_20) #asignamos el set de train\nPCA_RF_EXIST_20 = PCA_RF_20.predict(PCA_X_train_20)\nPCA_Y_prediction_20 = PCA_RF_20.predict(PCA_X_test_20) #asignamos sobre que deseamos calcular el predictor\nPCA_ACC_RF_20 = PCA_RF_20.score(PCA_X_train_20, PCA_Y_train_20) * 100 #sacamos el porcetaje de certeza del modelo\nPCA_MAE_RF_20= skl.metrics.mean_absolute_error(PCA_RF_EXIST_20, PCA_Y_train_20)\nPCA_MSE_RF_20= skl.metrics.mean_squared_error(PCA_RF_EXIST_20, PCA_Y_train_20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# adaboosreg model\nX_20=PCA_X_train_20\ny_20=PCA_Y_train_20\nPCA_AB_20=skl.ensemble.AdaBoostRegressor(random_state=0, n_estimators=100)\nPCA_AB_20.fit(X_20,y_20)\nPCA_Y_pred_ADAB_20=PCA_AB_20.predict(PCA_X_test_20)\nPCA_AB_EXIST_20 = PCA_AB_20.predict(PCA_X_train_20)\nPCA_ACC_AB_20= PCA_AB_20.score(PCA_X_train_20, PCA_Y_train_20) * 100 #sacamos el porcetaje de certeza del modelo\nPCA_MAE_AB_20= skl.metrics.mean_absolute_error(PCA_AB_EXIST_20, PCA_Y_train_20)\nPCA_MSE_AB_20= skl.metrics.mean_squared_error(PCA_AB_EXIST_20, PCA_Y_train_20)\n#referencia: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#linear regresion model\nPCA_LR_20=skl.linear_model.LinearRegression()\nPCA_LR_20.fit(X_20, y_20)\nPCA_Y_pred_LR_20= PCA_LR_20.predict(PCA_X_test_20)\nPCA_LR_EXIST_20 = PCA_LR_20.predict(PCA_X_train_20)\nPCA_ACC_LR_20 = PCA_LR_20.score(PCA_X_train_20, PCA_Y_train_20) * 100 #sacamos el porcetaje de certeza del modelo\nPCA_MAE_LR_20= skl.metrics.mean_absolute_error(PCA_LR_EXIST_20, PCA_Y_train_20)\nPCA_MSE_LR_20= skl.metrics.mean_squared_error(PCA_LR_EXIST_20, PCA_Y_train_20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Intercept (b0):',PCA_LR_20.intercept_)\n# For retrieving the slope:\nprint('****************')\nprint('(bi):',PCA_LR_20.coef_)\nprint('****************')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cross Validation for RL (13 PC)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#cross validate linear regression PCA\nPCA_LR_CV_SCORE = cross_validate(PCA_LR_13, PCA_X_train_13, PCA_Y_train_13, cv=4,scoring=['neg_mean_absolute_error','neg_mean_squared_error'],return_estimator=True) #llamo a cross validate definiendo feautures del dataframe auxiliar, la variable de salida, el valor de k, y las metricas deseadas\n\n\nPCA_LR_CV_METRICS=pd.DataFrame(PCA_LR_CV_SCORE, columns=['test_neg_mean_squared_error']) #almaceno las metricas obtenidas en un dataframe\nPCA_LR_CV_METRICS.rename(columns={'test_neg_mean_squared_error':'MSE'}, inplace=True) #cambio los nombres predeterminados por nombres mas simples\nPCA_LR_CV_METRICS_2=pd.DataFrame() #creo un nuevo dataframe\n\nfor i in PCA_LR_CV_SCORE['estimator']: \n    PCA_LR_CV_METRICS_2.loc[i,'MAPE']=((i.predict(PCA_X_train_13)-PCA_Y_train_13).abs()/PCA_Y_train_13).sum()/PCA_X_train_13['PC1'].count()*100 ##guardo en el dataframe, el MAPE ajustado de cada modelo a meida itera\nPCA_LR_CV_METRICS_2.reset_index(drop=True, inplace=True) #borro el indice para cambiar el codigo del modelo por numeros iniciando en 0\nPCA_LR_CV_METRICS_FINAL=pd.DataFrame(PCA_LR_CV_METRICS_2[['MAPE']].copy()) #genero un dataframe que copie las metricas del pirmer dataframe de metricas \nPCA_LR_CV_METRICS_FINAL[['MSE']]=PCA_LR_CV_METRICS[['MSE']].copy() #añado las columnas del segundo dataframe de metricas al dataframe final\n\nPCA_LR_CV_METRICS_FINAL=abs(PCA_LR_CV_METRICS_FINAL) #cambio todos los valores del dataframe final por sus valores absolutos para evitar negativos\n\nPCA_LR_CV_METRICS_FINAL.rename(index={0: 'Fold_1',1: 'Fold_2',2: 'Fold_3',3: 'Fold_4' }, inplace=True) #renombro los indices segun el numero de fold\nPCA_LR_CV_METRICS_FINAL #visualzo el datagrame final","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Guardado de Dataframes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#saving de dataframes en archivos csv excluyendo la columna de index para datos CON y SIN AGE\nPred_RF_PCA_13=pd.DataFrame()\nPred_RF_PCA_13['Id']=Original_Test['Id']\nPred_RF_PCA_13['SalePrice']=PCA_Y_prediction_13\nPred_RF_PCA_13.to_csv('Pred_RF_PCA_13.csv',index=False)\n\nPred_LR_PCA_13=pd.DataFrame()\nPred_LR_PCA_13['Id']=Original_Test['Id']\nPred_LR_PCA_13['SalePrice']=PCA_Y_pred_LR_13\nPred_LR_PCA_13.to_csv('Pred_LR_PCA_13.csv',index=False)\n\nPred_AB_PCA_13=pd.DataFrame()\nPred_AB_PCA_13['Id']=Original_Test['Id']\nPred_AB_PCA_13['SalePrice']=PCA_Y_pred_ADAB_13\nPred_AB_PCA_13.to_csv('Pred_AB_PCA_13.csv',index=False)\n\nPred_RF_PCA_20=pd.DataFrame()\nPred_RF_PCA_20['Id']=Original_Test['Id']\nPred_RF_PCA_20['SalePrice']=PCA_Y_prediction_20\nPred_RF_PCA_20.to_csv('Pred_RF_PCA_20.csv',index=False)\n\nPred_LR_PCA_20=pd.DataFrame()\nPred_LR_PCA_20['Id']=Original_Test['Id']\nPred_LR_PCA_20['SalePrice']=PCA_Y_pred_LR_20\nPred_LR_PCA_20.to_csv('Pred_LR_PCA_20.csv',index=False)\n\nPred_AB_PCA_20=pd.DataFrame()\nPred_AB_PCA_20['Id']=Original_Test['Id']\nPred_AB_PCA_20['SalePrice']=PCA_Y_pred_ADAB_20\nPred_AB_PCA_20.to_csv('Pred_AB_PCA_20.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Approach #2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"obj_df = Train_Edited.select_dtypes(include=['object']).copy() #analysis de object variables\nobj_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_2=Train_Edited.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"saleprice_corr_2=pd.DataFrame(corr_2['SalePrice'])\nsaleprice_corr_2.sort_values('SalePrice',ascending=False, inplace=True)\nsaleprice_corr_2.reset_index(inplace=True)\nsaleprice_corr_2.rename(columns={'index':'Variable'}, inplace=True)\nsaleprice_corr_2.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"qualitative_model=['MSZoning', 'Neighborhood', 'MasVnrType', 'BsmtQual','CentralAir', 'Electrical', 'KitchenQual', 'SaleType'] #seleccion de variables qualitativas en base a conocimientos previos\nquantitative_model=['OverallQual','BaseArea','GrLivArea','Exqual_YearB','ExterQual','GarageCars','GarageArea','TotalBsmtSF','1stFlrSF'] #seleccion de variables cuantitativas en funcion de su correlacion con sale price\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_Edited[qualitative_model].info() #analisis de mssing values para variables categoricas en set de entrenamiento","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_Edited[qualitative_model].info() #analisis de mssing values para variables categoricas en set de prueba","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_Edited[quantitative_model].info() #analisis de mssing values para variables cuantitativas en set de entrenamiento","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_Edited[quantitative_model].info() #analisis de mssing values para variables cuantitativas en set de entrenamiento","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_Edited_m1=pd.DataFrame()\nTrain_Edited_m1[qualitative_model]=Train_Edited[qualitative_model].copy()\nTrain_Edited_m1[quantitative_model]=Train_Edited[quantitative_model].copy()\nTrain_Edited_m1['SalePrice']=Train_Edited['SalePrice'].copy()\n\nTest_Edited_m1=pd.DataFrame()\nTest_Edited_m1[qualitative_model]=Test_Edited[qualitative_model].copy()\nTest_Edited_m1[quantitative_model]=Test_Edited[quantitative_model].copy()\n\nTrain_Edited_m1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Completado de set de entranamiento mediante llenado de nulos y codificacion de variables cualitativas","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_m1 = [Train_Edited_m1,Test_Edited_m1] #creacion de gurpo para el loop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in data_m1:\n    msz = {\"FV\": 5, \"RL\": 4, \"RH\":3, \"RM\":2, \"C (all)\":1}  #crear un diccionario de correspondencia\n    dataset['MSZoning'] = dataset['MSZoning'].map(msz)  #cambiar los valores origniales con los del dic\n    dataset['MSZoning']=dataset['MSZoning'].fillna(dataset['MSZoning'].median())\n    dataset['MSZoning']=dataset['MSZoning'].astype(int)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in data_m1:\n    NB = {\"CollgCr\": 25, \"OldTown\": 24, \"Edwards\":23, \"Somerst\":22, \"Gilbert\":21,\"NridgHt\": 20, \"Sawyer\": 19, \"NWAmes\":18, \"SawyerW\":17, \"BrkSide\":16,\"Crawfor\": 15, \"Mitchel\": 14, \"NoRidge\":13, \"Timber\":12, \"IDOTRR\":10,\"ClearCr\": 9, \"StoneBr\":8, \"SWISU\":7, \"Blmngtn\":6,\"MeadowV\": 5, \"BrDale\": 4, \"Veenker\":3, \"NPKVill\":2, \"Blueste\":1}  #crear un diccionario de correspondencia\n    dataset['Neighborhood'] = dataset['Neighborhood'].map(NB)  #cambiar los valores origniales con los del dic\n    dataset['Neighborhood']=dataset['Neighborhood'].fillna(dataset['Neighborhood'].median())\n    dataset['Neighborhood']=dataset['Neighborhood'].astype(int)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in data_m1:\n    MVT = {\"Stone\": 4, \"BrkFace\": 3, \"BrkCmn\":2, \"None\":1}  #crear un diccionario de correspondencia\n    dataset['MasVnrType'] = dataset['MasVnrType'].map(MVT)  #cambiar los valores origniales con los del dic\n    dataset['MasVnrType']=dataset['MasVnrType'].fillna(dataset['MasVnrType'].median())\n    dataset['MasVnrType']=dataset['MasVnrType'].astype(int)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in data_m1:\n    BMQ = {\"Ex\": 4, \"Gd\": 3, \"TA\":2, \"Fa\":1}  #crear un diccionario de correspondencia\n    dataset['BsmtQual'] = dataset['BsmtQual'].map(BMQ)  #cambiar los valores origniales con los del dic\n    dataset['BsmtQual']=dataset['BsmtQual'].fillna(dataset['BsmtQual'].median())\n    dataset['BsmtQual']=dataset['BsmtQual'].astype(int)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in data_m1:\n    CA = {\"Y\": 1, \"N\": 0}  #crear un diccionario de correspondencia\n    dataset['CentralAir'] = dataset['CentralAir'].map(CA)  #cambiar los valores origniales con los del dic\n    dataset['CentralAir']=dataset['CentralAir'].fillna(dataset['CentralAir'].median())\n    dataset['CentralAir']=dataset['CentralAir'].astype(int)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in data_m1:\n    EL = {\"SBrkr\": 5, \"FuseA\": 4,\"FuseF\":3,\"FuseP\":2,\"Mix\":1}  #crear un diccionario de correspondencia\n    dataset['Electrical'] = dataset['Electrical'].map(EL)  #cambiar los valores origniales con los del dic\n    dataset['Electrical']=dataset['Electrical'].fillna(dataset['Electrical'].median())\n    dataset['Electrical']=dataset['Electrical'].astype(int)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in data_m1:\n    KQ = {\"Ex\": 4,\"Gd\":3,\"TA\":2,\"Fa\":1}  #crear un diccionario de correspondencia\n    dataset['KitchenQual'] = dataset['KitchenQual'].map(KQ)  #cambiar los valores origniales con los del dic\n    dataset['KitchenQual']=dataset['KitchenQual'].fillna(dataset['KitchenQual'].median())\n    dataset['KitchenQual']=dataset['KitchenQual'].astype(int)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in data_m1:\n    KQ = {\"Con\": 9,\"New\":8,\"CWD\":7,\"WD\":6,\"ConLw\": 5,\"COD\":4,\"ConLD\":3,\"ConLI\":2,\"Oth\": 1}  #crear un diccionario de correspondencia\n    dataset['SaleType'] = dataset['SaleType'].map(KQ)  #cambiar los valores origniales con los del dic\n    dataset['SaleType']=dataset['SaleType'].fillna(dataset['SaleType'].median())\n    dataset['SaleType']=dataset['SaleType'].astype(int)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Completado de set de entranamiento y test mediante llenado de nulos de variables cuantitativas","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in data_m1:\n    dataset['OverallQual']=dataset['OverallQual'].fillna(dataset['OverallQual'].median())\n    dataset['OverallQual']=dataset['OverallQual'].astype(int)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in data_m1:\n    dataset['BaseArea']=dataset['BaseArea'].fillna(dataset['BaseArea'].median())\n    dataset['BaseArea']=dataset['BaseArea'].astype(int)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in data_m1:\n    dataset['GrLivArea']=dataset['GrLivArea'].fillna(dataset['GrLivArea'].median())\n    dataset['GrLivArea']=dataset['GrLivArea'].astype(int)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in data_m1:\n    dataset['Exqual_YearB']=dataset['Exqual_YearB'].fillna(dataset['Exqual_YearB'].median())\n    dataset['Exqual_YearB']=dataset['Exqual_YearB'].astype(int)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in data_m1:\n    dataset['ExterQual']=dataset['ExterQual'].fillna(dataset['ExterQual'].median())\n    dataset['ExterQual']=dataset['ExterQual'].astype(int)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in data_m1:\n    dataset['GarageCars']=dataset['GarageCars'].fillna(dataset['GarageCars'].median())\n    dataset['GarageCars']=dataset['GarageCars'].astype(int)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in data_m1:\n    dataset['GarageArea']=dataset['GarageArea'].fillna(dataset['GarageArea'].median())\n    dataset['GarageArea']=dataset['GarageArea'].astype(int)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in data_m1:\n    dataset['TotalBsmtSF']=dataset['TotalBsmtSF'].fillna(dataset['TotalBsmtSF'].median())\n    dataset['TotalBsmtSF']=dataset['TotalBsmtSF'].astype(int)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in data_m1:\n    dataset['1stFlrSF']=dataset['1stFlrSF'].fillna(dataset['1stFlrSF'].median())\n    dataset['1stFlrSF']=dataset['1stFlrSF'].astype(int)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_Train_Final_m1=pd.DataFrame()\nX_Train_Final_m1[qualitative_model]=Train_Edited_m1[qualitative_model].copy()\nX_Train_Final_m1[quantitative_model]=Train_Edited_m1[quantitative_model].copy()\nY_Train_Final_m1=Train_Edited_m1['SalePrice']\n\nX_Test_Final_m1=pd.DataFrame()\nX_Test_Final_m1[qualitative_model]=Test_Edited_m1[qualitative_model].copy()\nX_Test_Final_m1[quantitative_model]=Test_Edited_m1[quantitative_model].copy() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_Train_Final_m1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_Test_Final_m1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_Train_Final_m1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelos","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#random forest model\nAP2_RF = RandomForestClassifier(n_estimators=100)  #llamamos al primer modelo definiendo el numero de estimadores\nAP2_RF.fit(X_Train_Final_m1, Y_Train_Final_m1) #asignamos el set de train\nAP2_RF_EXIST= AP2_RF.predict(X_Train_Final_m1)\nAP2_RF_PRED = AP2_RF.predict(X_Test_Final_m1) #asignamos sobre que deseamos calcular el predictor\nAP2_ACC_RF = AP2_RF.score(X_Train_Final_m1, Y_Train_Final_m1) * 100 #sacamos el porcetaje de certeza del modelo\nAP2_MAE_RF= skl.metrics.mean_absolute_error(AP2_RF_EXIST, Y_Train_Final_m1)\nAP2_MSE_RF= skl.metrics.mean_squared_error(AP2_RF_EXIST, Y_Train_Final_m1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# adaboosreg model\nAP2_AB=skl.ensemble.AdaBoostRegressor(random_state=0, n_estimators=100)\nAP2_AB.fit(X_Train_Final_m1,Y_Train_Final_m1)\nAP2_AB_EXIST= AP2_AB.predict(X_Train_Final_m1)\nAP2_AB_PRED=AP2_AB.predict(X_Test_Final_m1)\nAP2_ACC_AB = AP2_AB.score(X_Train_Final_m1, Y_Train_Final_m1) * 100 #sacamos el porcetaje de certeza del modelo\nAP2_MAE_AB= skl.metrics.mean_absolute_error(AP2_AB_EXIST, Y_Train_Final_m1)\nAP2_MSE_AB= skl.metrics.mean_squared_error(AP2_AB_EXIST, Y_Train_Final_m1)\n#referencia: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#linear regresion model\nAP2_LR=skl.linear_model.LinearRegression()\nAP2_LR.fit(X_Train_Final_m1,Y_Train_Final_m1)\nAP2_LR_EXIST= AP2_LR.predict(X_Train_Final_m1)\nAP2_LR_PRED= AP2_LR.predict(X_Test_Final_m1)\nAP2_ACC_LR = AP2_LR.score(X_Train_Final_m1, Y_Train_Final_m1) * 100 #sacamos el porcetaje de certeza del modelo\nAP2_MAE_LR= skl.metrics.mean_absolute_error(AP2_LR_EXIST, Y_Train_Final_m1)\nAP2_MSE_LR= skl.metrics.mean_squared_error(AP2_LR_EXIST, Y_Train_Final_m1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Intercept (b0):',AP2_LR.intercept_)\n# For retrieving the slope:\nprint('(bi):',AP2_LR.coef_)\nprint('****************')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cross Validation for LR","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#cross validate para linear regression approach 2\nAP2_LR_CV_SCORE = cross_validate(AP2_LR, X_Train_Final_m1, Y_Train_Final_m1, cv=4,scoring=['neg_mean_absolute_error','neg_mean_squared_error'],return_estimator=True) #llamo a cross validate definiendo feautures del dataframe auxiliar, la variable de salida, el valor de k, y las metricas deseadas\n\n\nAP2_LR_CV_METRICS=pd.DataFrame(AP2_LR_CV_SCORE, columns=['test_neg_mean_squared_error']) #almaceno las metricas obtenidas en un dataframe\nAP2_LR_CV_METRICS.rename(columns={'test_neg_mean_squared_error':'MSE'}, inplace=True) #cambio los nombres predeterminados por nombres mas simples\nAP2_LR_CV_METRICS_2=pd.DataFrame() #creo un nuevo dataframe\n\nfor i in AP2_LR_CV_SCORE['estimator']: \n    AP2_LR_CV_METRICS_2.loc[i,'MAPE']=((i.predict(X_Train_Final_m1)-Y_Train_Final_m1).abs()/Y_Train_Final_m1).sum()/X_Train_Final_m1['Electrical'].count()*100 ##guardo en el dataframe, el MAPE ajustado de cada modelo a meida itera\nAP2_LR_CV_METRICS_2.reset_index(drop=True, inplace=True) #borro el indice para cambiar el codigo del modelo por numeros iniciando en 0\nAP2_LR_CV_METRICS_FINAL=pd.DataFrame(AP2_LR_CV_METRICS_2[['MAPE']].copy()) #genero un dataframe que copie las metricas del pirmer dataframe de metricas \nAP2_LR_CV_METRICS_FINAL[['MSE']]=AP2_LR_CV_METRICS[['MSE']].copy() #añado las columnas del segundo dataframe de metricas al dataframe final\n\nAP2_LR_CV_METRICS_FINAL=abs(AP2_LR_CV_METRICS_FINAL) #cambio todos los valores del dataframe final por sus valores absolutos para evitar negativos\n\nAP2_LR_CV_METRICS_FINAL.rename(index={0: 'Fold_1',1: 'Fold_2',2: 'Fold_3',3: 'Fold_4' }, inplace=True) #renombro los indices segun el numero de fold\nAP2_LR_CV_METRICS_FINAL #visualzo el datagrame final","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Guardado de Dataframes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":" #saving de dataframes en archivos csv\nPred_RF_AP2=pd.DataFrame()\nPred_RF_AP2['Id']=Original_Test['Id']\nPred_RF_AP2['SalePrice']=AP2_RF_PRED\nPred_RF_AP2.to_csv('Pred_RF_AP2.csv',index=False)\n\nPred_LR_AP2=pd.DataFrame()\nPred_LR_AP2['Id']=Original_Test['Id']\nPred_LR_AP2['SalePrice']=AP2_LR_PRED\nPred_LR_AP2.to_csv('Pred_LR_AP2.csv',index=False)\n\nPred_AB_AP2=pd.DataFrame()\nPred_AB_AP2['Id']=Original_Test['Id']\nPred_AB_AP2['SalePrice']=AP2_AB_PRED\nPred_AB_AP2.to_csv('Pred_AB_AP2.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Approach #3","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#crear un data frame de train con las variables categoricas codificadas y las variables cuantitativas\nX_Train_Final_m2=pd.DataFrame(X_Train_Final_m1[qualitative_model].copy())\ncolumns_3=Train_pca[quantitative_SP].columns\nfor i in columns_3:\n    X_Train_Final_m2[i]=Train_pca[i].copy()\nX_Train_Final_m2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#crear un data frame de test con las variables categoricas codificadas y las variables cuantitativas\nX_Test_Final_m2=pd.DataFrame(X_Test_Final_m1[qualitative_model].copy())\ncolumns_3=Test_pca[quantitative_SP].columns\nfor i in columns_3:\n    X_Test_Final_m2[i]=Test_pca[i].copy()\nX_Test_Final_m2.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# crear una variable con lo valores del outcome del set de train\nY_Train_Final_m2=Y_Train_Final_m1.copy()\n\n# normalizar datos\nX_Train_Final_m2_N=skl.preprocessing.StandardScaler().fit(X_Train_Final_m2).transform(X_Train_Final_m2.astype(float))\nX_Train_Final_m2_N=pd.DataFrame(X_Train_Final_m2_N.copy(), columns=X_Train_Final_m2.columns) # renombrar columanas con los nombres del DF original\n\nX_Test_Final_m2_N=skl.preprocessing.StandardScaler().fit(X_Test_Final_m2).transform(X_Test_Final_m2.astype(float))\nX_Test_Final_m2_N=pd.DataFrame(X_Test_Final_m2_N.copy(), columns=X_Test_Final_m2.columns) # renombrar columanas con los nombres del DF original","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#correr modelo PCA \npcs_train_3 = skl.decomposition.PCA()\npcs_train_3.fit(X_Train_Final_m2_N)\n#obtener estadisticas de los componentes principales\npcsSummary_train_3_df = pd.DataFrame({'Standard deviation': np.sqrt(pcs_train_3.explained_variance_),\n                           'Proportion of variance': pcs_train_3.explained_variance_ratio_,\n                           'Cumulative proportion': np.cumsum(pcs_train_3.explained_variance_ratio_)})\npcsSummary_train_3_df = pcsSummary_train_3_df.transpose() #transponer DF\npcsSummary_train_3_df.columns = ['PC{}'.format(i) for i in range(1, len(pcsSummary_train_3_df.columns)+1)] #establecer el nombre de las columnas\npcsSummary_train_3_df.round(4) #precision de 4 decimales\npcsSummary_train_3_df.iloc[:,:30] #mostrar unicamente los 20 primeros PC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pcsComponents_train_3_df = pd.DataFrame(pcs_train_3.components_.transpose(), columns=pcsSummary_train_3_df.columns, \n                                index=X_Train_Final_m2_N.columns)\npcsComponents_train_3_df.iloc[:10,:25] #mostrar los pesos de los 13 primeros componentes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"houses_red_df_train_3 = X_Train_Final_m2_N.dropna(axis=0)\nhouses_red_df_train_3 = X_Train_Final_m2_N.reset_index(drop=True)\nscores_train_3 = pd.DataFrame(pcs_train_3.fit_transform(skl.preprocessing.scale(houses_red_df_train_3.dropna(axis=0))), \n                      columns=[f'PC{i}' for i in range(1, 51)])\n#obtener los valores transformados de cada uno de PC para todos los registros del set de datos\ntransform_df_train_3 = pd.DataFrame(pcs_train_3.transform(X_Train_Final_m2_N), \n                      columns=pcsSummary_train_3_df.columns)\ntransform_df_train_3=transform_df_train_3.iloc[:,:25] #25 componentes principales\ntransform_df_train_3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#correr modelo PCA \npcs_test_3 = skl.decomposition.PCA()\npcs_test_3.fit(X_Test_Final_m2_N)\n#obtener estadisticas de los componentes principales\npcsSummary_test_3_df = pd.DataFrame({'Standard deviation': np.sqrt(pcs_test_3.explained_variance_),\n                           'Proportion of variance': pcs_test_3.explained_variance_ratio_,\n                           'Cumulative proportion': np.cumsum(pcs_test_3.explained_variance_ratio_)})\npcsSummary_test_3_df = pcsSummary_test_3_df.transpose() #transponer DF\npcsSummary_test_3_df.columns = ['PC{}'.format(i) for i in range(1, len(pcsSummary_test_3_df.columns)+1)] #establecer el nombre de las columnas\npcsSummary_test_3_df.round(4) #precision de 4 decimales\npcsSummary_test_3_df.iloc[:,:20] #mostrar unicamente los 20 primeros PC\npcsComponents_test_3_df = pd.DataFrame(pcs_test_3.components_.transpose(), columns=pcsSummary_test_3_df.columns, \n                                index=X_Test_Final_m2_N.columns)\npcsComponents_test_3_df.iloc[:10,:13] #mostrar los pesos de los 13 primeros componentes\nhouses_red_df_test_3 = X_Test_Final_m2_N.dropna(axis=0)\nhouses_red_df_test_3 = X_Test_Final_m2_N.reset_index(drop=True)\nscores_test_3 = pd.DataFrame(pcs_test_3.fit_transform(skl.preprocessing.scale(houses_red_df_test_3.dropna(axis=0))), \n                      columns=[f'PC{i}' for i in range(1, 51)])\n#obtener los valores transformados de cada uno de PC para todos los registros del set de datos\ntransform_df_test_3 = pd.DataFrame(pcs_test_3.transform(X_Test_Final_m2_N), \n                      columns=pcsSummary_test_3_df.columns)\ntransform_df_test_3=transform_df_test_3.iloc[:,:25] # 25 pc\ntransform_df_test_3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelos","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> ### Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"PCA_X_train_3 = transform_df_train_3 #definimos los feautures del set train para los modelos de prediccion\nPCA_Y_train_3 = Train_Edited[\"SalePrice\"] #definimos el target\nPCA_X_test_3 = transform_df_test_3 #definimos sobre que set de datos se desean realizar las prediccione\n\nPCA_RF_3 = RandomForestClassifier(n_estimators=100)  #llamamos al primer modelo definiendo el numero de estimadores\nPCA_RF_3.fit(PCA_X_train_3, PCA_Y_train_3) #asignamos el set de train\nPCA_RF_EXIST_3 = PCA_RF_3.predict(PCA_X_train_3)\nPCA_Y_prediction_3 = PCA_RF_3.predict(PCA_X_test_3) #asignamos sobre que deseamos calcular el predictor\nPCA_ACC_RF_3 = PCA_RF_3.score(PCA_X_train_3, PCA_Y_train_3) * 100 #sacamos el porcetaje de certeza del modelo\nPCA_MAE_RF_3= skl.metrics.mean_absolute_error(PCA_RF_EXIST_3, PCA_Y_train_3)\nPCA_MSE_RF_3= skl.metrics.mean_squared_error(PCA_RF_EXIST_3, PCA_Y_train_3)\nPCA_MSE_RF_3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> #### Refinamiento de n:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# partición de set de datos para poder obtener el mejor n\nPCA_split=PCA_X_train_3.copy()\nPCA_split['Sale Price']= Train_Edited[\"SalePrice\"]\nPCA_train_split, PCA_test_split = train_test_split(PCA_split,test_size=0.25, random_state=1)\nprint('Training : ', PCA_train_split.shape) #imprimir la dimensionalidad del set de entrenamiento\nprint('Test : ', PCA_test_split.shape) #imprimir dimensionalidad del set de test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_split=PCA_train_split.drop(columns='Sale Price')\ny_train_split=PCA_train_split['Sale Price']\nX_test_split=PCA_test_split.drop(columns='Sale Price')\ny_test_split=PCA_test_split['Sale Price']\nresults=[]\nfor k in range(1,100):\n    PCA_RF_3 =  RandomForestClassifier(n_estimators=k).fit(X_train_split,y_train_split)\n    results.append({\n        'k': k,\n        'MAE': skl.metrics.mean_absolute_error(y_test_split, PCA_RF_3.predict(X_test_split)),\n        'MSE': skl.metrics.mean_squared_error(y_test_split, PCA_RF_3.predict(X_test_split))\n    })\n# Convert results to a pandas data frame\nresults = pd.DataFrame(results)\nresults.sort_values([\"k\",'MAE','MSE'],ascending=True).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('El mejor valor de n es: \\n', results.sort_values(['MAE','MSE'],ascending=True).iloc[0,0:3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PCA_RF_3 = RandomForestClassifier(n_estimators=91)  #llamamos al primer modelo definiendo el numero de estimadores\nPCA_RF_3.fit(PCA_X_train_3, PCA_Y_train_3) #asignamos el set de train\nPCA_RF_EXIST_3 = PCA_RF_3.predict(PCA_X_train_3)\nPCA_Y_prediction_3 = PCA_RF_3.predict(PCA_X_test_3) #asignamos sobre que deseamos calcular el predictor\nPCA_ACC_RF_3 = PCA_RF_3.score(PCA_X_train_3, PCA_Y_train_3) * 100 #sacamos el porcetaje de certeza del modelo\nPCA_MAE_RF_3= skl.metrics.mean_absolute_error(PCA_RF_EXIST_3, PCA_Y_train_3)\nPCA_MSE_RF_3= skl.metrics.mean_squared_error(PCA_RF_EXIST_3, PCA_Y_train_3)\nPCA_MSE_RF_3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### AdaBoost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# adaboosreg model\nPCA_AB_3=skl.ensemble.AdaBoostRegressor(random_state=0, n_estimators=100)\nPCA_AB_3.fit(PCA_X_train_3,PCA_Y_train_3)\nPCA_Y_pred_ADAB_3=PCA_AB_3.predict(PCA_X_test_3)\nPCA_AB_EXIST_3 = PCA_AB_3.predict(PCA_X_train_3)\nPCA_ACC_AB_3= PCA_AB_3.score(PCA_X_train_3, PCA_Y_train_3) * 100 #sacamos el porcetaje de certeza del modelo\nPCA_MAE_AB_3= skl.metrics.mean_absolute_error(PCA_AB_EXIST_3, PCA_Y_train_3)\nPCA_MSE_AB_3= skl.metrics.mean_squared_error(PCA_AB_EXIST_3, PCA_Y_train_3)\n#referencia: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> #### Refinamiento de n:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"results_ab=[]\nfor b in range(1,100):\n    PCA_AB_rf =  skl.ensemble.AdaBoostRegressor(n_estimators=b).fit(X_train_split,y_train_split)\n    results_ab.append({\n        'b': b,\n        'MAE': skl.metrics.mean_absolute_error(y_test_split, PCA_AB_rf.predict(X_test_split)),\n        'MSE': skl.metrics.mean_squared_error(y_test_split, PCA_AB_rf.predict(X_test_split))\n    })\n# Convert results to a pandas data frame\nresults_ab = pd.DataFrame(results_ab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_ab.sort_values(['MAE'],ascending=True).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# adaboosreg model\nPCA_AB_3=skl.ensemble.AdaBoostRegressor(random_state=0, n_estimators=84)\nPCA_AB_3.fit(PCA_X_train_3,PCA_Y_train_3)\nPCA_Y_pred_ADAB_3=PCA_AB_3.predict(PCA_X_test_3)\nPCA_AB_EXIST_3 = PCA_AB_3.predict(PCA_X_train_3)\nPCA_ACC_AB_3= PCA_AB_3.score(PCA_X_train_3, PCA_Y_train_3) * 100 #sacamos el porcetaje de certeza del modelo\nPCA_MAE_AB_3= skl.metrics.mean_absolute_error(PCA_AB_EXIST_3, PCA_Y_train_3)\nPCA_MSE_AB_3= skl.metrics.mean_squared_error(PCA_AB_EXIST_3, PCA_Y_train_3)\n#referencia: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Linear Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#linear regresion model\nPCA_LR_3=skl.linear_model.LinearRegression()\nPCA_LR_3.fit(PCA_X_train_3, PCA_Y_train_3)\nPCA_Y_pred_LR_3= PCA_LR_3.predict(PCA_X_test_3)\nPCA_LR_EXIST_3 = PCA_LR_3.predict(PCA_X_train_3)\nPCA_ACC_LR_3 = PCA_LR_3.score(PCA_X_train_3, PCA_Y_train_3) * 100 #sacamos el porcetaje de certeza del modelo\nPCA_MAE_LR_3= skl.metrics.mean_absolute_error(PCA_LR_EXIST_3, PCA_Y_train_3)\nPCA_MSE_LR_3= skl.metrics.mean_squared_error(PCA_LR_EXIST_3, PCA_Y_train_3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Guardado de resultados","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#saving de dataframes en archivos csv excluyendo la columna de index para datos CON y SIN AGE\nPred_RF_PCA_3=pd.DataFrame()\nPred_RF_PCA_3['Id']=Original_Test['Id']\nPred_RF_PCA_3['SalePrice']=PCA_Y_prediction_3\nPred_RF_PCA_3.to_csv('Pred_RF_PCA_3.csv',index=False)\n\nPred_LR_PCA_3=pd.DataFrame()\nPred_LR_PCA_3['Id']=Original_Test['Id']\nPred_LR_PCA_3['SalePrice']=PCA_Y_pred_LR_3\nPred_LR_PCA_3.to_csv('Pred_LR_PCA_3.csv',index=False)\n\nPred_AB_PCA_3=pd.DataFrame()\nPred_AB_PCA_3['Id']=Original_Test['Id']\nPred_AB_PCA_3['SalePrice']=PCA_Y_pred_ADAB_3\nPred_AB_PCA_3.to_csv('Pred_AB_PCA_3.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Resultados Generales","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Accuracy_df = pd.DataFrame({'Model': ['Random Forest','Linear Reg','ADA Boost'],\n                        'AP-2 Acc':[AP2_ACC_RF, AP2_ACC_LR, AP2_ACC_AB],\n                        '13 PCA Acc':[PCA_ACC_RF_13,PCA_ACC_LR_13,PCA_ACC_AB_13],\n                        '20 PCA Acc':[PCA_ACC_RF_20,PCA_ACC_LR_20,PCA_ACC_AB_20],\n                        '25 PCA Acc':[PCA_ACC_RF_3,PCA_ACC_LR_3,PCA_ACC_AB_3],\n                        'AP-2 MAE':[AP2_MAE_RF,AP2_MAE_LR,AP2_MAE_AB],\n                        '13 PCA MAE':[PCA_MAE_RF_13,PCA_MAE_LR_13,PCA_MAE_AB_13],  \n                        '20 PCA MAE':[PCA_MAE_RF_20,PCA_MAE_LR_20,PCA_MAE_AB_20],\n                        '25 PCA MAE':[PCA_MAE_RF_3,PCA_MAE_LR_3,PCA_MAE_AB_3],\n                        'AP-2 MSE': [AP2_MSE_RF, AP2_MSE_LR, AP2_MSE_AB],\n                        '13 PCA MSE':[PCA_MSE_RF_13,PCA_MSE_LR_13,PCA_MSE_AB_13],\n                        '20 PCA MSE':[PCA_MSE_RF_20,PCA_MSE_LR_20,PCA_MSE_AB_20],\n                        '25 PCA MSE':[PCA_MSE_RF_3,PCA_MSE_LR_3,PCA_MSE_AB_3],   \n                        })\nAccuracy_df               ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Referencias\n\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html\n\nhttps://www.pluralsight.com/guides/different-ways-create-numpy-arrays\n\nhttps://www.guru99.com/numpy-statistical-function.html\n\nhttps://stackoverflow.com/questions/45416684/python-pandas-replace-multiple-columns-zero-to-nan?rq=1\n\nhttps://kanoki.org/2019/07/17/pandas-how-to-replace-values-based-on-conditions/\n\nhttps://seaborn.pydata.org/examples/many_pairwise_correlations.html\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html\n\nhttps://www.kaggle.com/dgawlik/house-prices-eda\n\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html\n\nhttps://www.pluralsight.com/guides/different-ways-create-numpy-arrays\n\nhttps://www.guru99.com/numpy-statistical-function.html\n\nhttps://stackoverflow.com/questions/45416684/python-pandas-replace-multiple-columns-zero-to-nan?rq=1\n\nhttps://kanoki.org/2019/07/17/pandas-how-to-replace-values-based-on-conditions/\n\nhttps://seaborn.pydata.org/examples/many_pairwise_correlations.html\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html\n\nhttps://www.kaggle.com/dgawlik/house-prices-eda","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}