{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![550,500](https://images.pexels.com/photos/149387/pexels-photo-149387.jpeg?auto=compress&cs=tinysrgb&dpr=1&w=800)","metadata":{}},{"cell_type":"markdown","source":"Normally for tabular dataset I would use decision trees , random forests , xgboost for a classification problem. How ever for this problem I decided to make a  neural network model using pytorch lightning. I wanted to get out of my comfort zone while making this project. Incase you want a explanation of the model building process I'd recommend checking out fast.ai course they do a better job of explaining than I do.","metadata":{}},{"cell_type":"markdown","source":"Also If you want to see the data visualised here's my [EDA](https://www.kaggle.com/aristotle609/eda-on-hr-dataset) since I won't be doing any EDA in this notebook","metadata":{}},{"cell_type":"markdown","source":"# Dependencies","metadata":{}},{"cell_type":"code","source":"#importing all the required dependencies\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as torch_optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\nfrom datetime import datetime\nimport pytorch_lightning as pl","metadata":{"execution":{"iopub.status.busy":"2021-06-20T01:58:09.719642Z","iopub.execute_input":"2021-06-20T01:58:09.720077Z","iopub.status.idle":"2021-06-20T01:58:09.726962Z","shell.execute_reply.started":"2021-06-20T01:58:09.720033Z","shell.execute_reply":"2021-06-20T01:58:09.725622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing Data","metadata":{}},{"cell_type":"code","source":"#lets take a look at the data\ndf = pd.read_csv(\"../input/hr-analytics-and-job-prediction/HR_comma_sep.csv\")\nprint(\"Shape:\", df.shape)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T01:58:09.742715Z","iopub.execute_input":"2021-06-20T01:58:09.743073Z","iopub.status.idle":"2021-06-20T01:58:09.774446Z","shell.execute_reply.started":"2021-06-20T01:58:09.743041Z","shell.execute_reply":"2021-06-20T01:58:09.773394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label = \"left\"\ncat_cols = ['Work_accident','number_project','promotion_last_5years','Department','salary']\nnum_cols = ['satisfaction_level', 'last_evaluation','average_montly_hours','time_spend_company']\nprint(\"Num of Categorical columns : \" ,len(cat_cols))\nprint(\"Number of numerical columns : \" , len(num_cols))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T01:58:09.776097Z","iopub.execute_input":"2021-06-20T01:58:09.77639Z","iopub.status.idle":"2021-06-20T01:58:09.782772Z","shell.execute_reply.started":"2021-06-20T01:58:09.77636Z","shell.execute_reply":"2021-06-20T01:58:09.781924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#splitting the data into train, val and test\ntest_size = 0.1\nval_size = 0.3\nrandom_state = 42\n\ndf_train , df_test = train_test_split(df,test_size = test_size,random_state = random_state,stratify = df[label])\n\ndf_train , df_val = train_test_split(df_train,test_size = val_size,random_state = random_state,stratify = df_train[label])\n\nprint(\"Shape:\", df.shape)\nprint(\"Shape of train:\", df_train.shape)\nprint(\"Shape of test:\", df_test.shape)\nprint(\"Shape of validation:\", df_val.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T01:58:09.788821Z","iopub.execute_input":"2021-06-20T01:58:09.789159Z","iopub.status.idle":"2021-06-20T01:58:09.81485Z","shell.execute_reply.started":"2021-06-20T01:58:09.789129Z","shell.execute_reply":"2021-06-20T01:58:09.813642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Numerating Categorical Columns","metadata":{}},{"cell_type":"code","source":"#numeralising the data\ncat_code_dict = {}\n\nfor col in cat_cols:\n    category_col = df[col].astype('category')\n    cat_code_dict[col] = {value : idx for idx,value in enumerate(category_col.cat.categories)}\ncat_code_dict","metadata":{"execution":{"iopub.status.busy":"2021-06-20T01:58:09.81676Z","iopub.execute_input":"2021-06-20T01:58:09.817031Z","iopub.status.idle":"2021-06-20T01:58:09.83232Z","shell.execute_reply.started":"2021-06-20T01:58:09.817006Z","shell.execute_reply":"2021-06-20T01:58:09.831367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# since the numerical columns have been scaled there's no need to scale them again\ndef preprocess(df,cat_code_dict,num_cols,cat_cols,label_col):\n    \"\"\"\n    df:DataFrame,\n    cat_code_dict : A dictionary of categorial columns ,\n    num_cols : the numerical columns,\n    cat_cols : the categorical columns,\n    label_col : the target column\n    \"\"\"\n    df = df.copy()\n    df[num_cols] = df[num_cols].astype(np.float32)\n    \n    for col in cat_cols:\n        col_dict = cat_code_dict[col]\n        df[col] = df [col].map(col_dict).astype(np.int64)\n        df[label_col] = df[label_col].astype(np.int64)\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-06-20T01:58:09.836805Z","iopub.execute_input":"2021-06-20T01:58:09.83713Z","iopub.status.idle":"2021-06-20T01:58:09.842398Z","shell.execute_reply.started":"2021-06-20T01:58:09.837103Z","shell.execute_reply":"2021-06-20T01:58:09.841545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#A look at the preprocessed data\npreprocess(df,cat_code_dict,num_cols,cat_cols,label)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T01:58:09.852497Z","iopub.execute_input":"2021-06-20T01:58:09.853198Z","iopub.status.idle":"2021-06-20T01:58:09.886309Z","shell.execute_reply.started":"2021-06-20T01:58:09.853157Z","shell.execute_reply":"2021-06-20T01:58:09.885115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#preprocessing  all of the dataframes\ndf_train = preprocess(df_train,cat_code_dict,num_cols,cat_cols,label)\ndf_test = preprocess(df_test,cat_code_dict,num_cols,cat_cols,label)\ndf_val = preprocess(df_val,cat_code_dict,num_cols,cat_cols,label)\ndisplay(df_train,df_test,df_val)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T01:58:09.887783Z","iopub.execute_input":"2021-06-20T01:58:09.888034Z","iopub.status.idle":"2021-06-20T01:58:09.966982Z","shell.execute_reply.started":"2021-06-20T01:58:09.88801Z","shell.execute_reply":"2021-06-20T01:58:09.965982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets make the dataset\nclass TabularDataset(Dataset):\n    def __init__(self,df,num_cols,cat_cols,label):\n        \"\"\"\n        df: Dataframe passed,\n        num_cols : Numerical Columns,\n        cat_cols : Categorical_columns,\n        label : target column\n        \"\"\"\n        self.df = df \n        self.num_cols = num_cols\n        self.cat_cols = cat_cols\n        self.label = label\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self,idx):\n        num_array = self.df[self.num_cols].iloc[idx].values\n        cat_array = self.df[self.cat_cols].iloc[idx].values\n        label_array = self.df[self.label].iloc[idx]\n        return num_array,cat_array,label_array","metadata":{"execution":{"iopub.status.busy":"2021-06-20T01:58:09.968726Z","iopub.execute_input":"2021-06-20T01:58:09.969136Z","iopub.status.idle":"2021-06-20T01:58:09.976409Z","shell.execute_reply.started":"2021-06-20T01:58:09.9691Z","shell.execute_reply":"2021-06-20T01:58:09.975595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking the dataset for sanity\ndataset = TabularDataset(df_train,num_cols,cat_cols,label)\ndataloader = DataLoader(dataset,batch_size = 1 ,  shuffle = True)\nnext(iter(dataloader))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T01:58:09.978476Z","iopub.execute_input":"2021-06-20T01:58:09.978843Z","iopub.status.idle":"2021-06-20T01:58:09.998639Z","shell.execute_reply.started":"2021-06-20T01:58:09.978802Z","shell.execute_reply":"2021-06-20T01:58:09.997479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pytorch Lightning","metadata":{}},{"cell_type":"code","source":"#Making the Pytorch Lightning DataModule recommended by Pytorch Lightning it makes the data more convinient to use\nclass TabularDatsetModule(pl.LightningDataModule):\n    def __init__(self,df_train,df_test,df_val,num_cols,cat_cols,label,test_batch = 64,train_batch = 64,val_batch = 64):\n        \"\"\"\n        df_train : Train Dataframe\n        df_test:test DataFrame\n        df_val : Validation Dataframe\n        num_cols : Numerical Columns\n        cat_cols : Categorical Columns\n        \"\"\"\n        super().__init__()\n        self.train = df_train\n        self.test = df_test\n        self.val = df_val\n        self.num_cols = num_cols\n        self.cat_cols = cat_cols\n        self.label = label\n        self.test_batch = test_batch\n        self.train_batch = train_batch\n        self.val_batch = val_batch\n    \n    def setup(self,stage):\n        self.train_loader = TabularDataset(self.train,self.num_cols,self.cat_cols,self.label)\n        self.test_loader = TabularDataset(self.test,self.num_cols,self.cat_cols,self.label)\n        self.val_loader = TabularDataset(self.val,self.num_cols,self.cat_cols,self.label)\n    \n    \"\"\"\n    These return the data to the neural network --->\n    \"\"\"\n    def train_dataloader(self):\n        return DataLoader(self.train_loader,batch_size = self.train_batch,shuffle = True)\n    \n    def test_dataloader(self):\n        return DataLoader(self.test_loader,batch_size = self.test_batch,shuffle = True)\n    \n    def val_dataloader(self):\n        return DataLoader(self.val_loader,batch_size = self.val_batch,shuffle = True)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-20T01:58:10.000154Z","iopub.execute_input":"2021-06-20T01:58:10.000468Z","iopub.status.idle":"2021-06-20T01:58:10.009934Z","shell.execute_reply.started":"2021-06-20T01:58:10.000436Z","shell.execute_reply":"2021-06-20T01:58:10.008807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Making the neural network\nclass TabularNet(pl.LightningModule):\n    def __init__(self,num_cols,cat_cols,embedding_size_dict,n_classes,embedding_dim_dict = None):\n        \"\"\"\n        num_cols: A list of the numerical columns\n        cat_cols: A list of cat_cols,\n        embedding_size_dict :  A dictionary of th columns and the number of categories,\n        n_classes = number of classes,\n        embedding_dim_dict: A dictionary of th columns and the dimensions of the ouput embedding\n        \"\"\"\n        super().__init__()\n        self.embeddings , total_embeddings_dim = self._create_embedding_layers(cat_cols,embedding_size_dict,embedding_dim_dict)\n        in_features = len(num_cols) + total_embeddings_dim\n        self.layers = nn.Sequential(\n        nn.Linear(in_features,128),\n            nn.ReLU(),\n            nn.Linear(128,100),\n            nn.ReLU(),\n            nn.Linear(100,n_classes),\n        )\n    @staticmethod\n    def _create_embedding_layers(cat_cols,embedding_size_dict,embeddind_dim_dict):\n        total_embedding_dim = 0\n        embedding_dim = 0\n        embeddings = {}\n        for col in cat_cols:\n            embedding_size = embedding_size_dict[col]\n            embedding_dim = embedding_dim_dict[col]\n            total_embedding_dim +=embedding_dim\n            embeddings[col] = nn.Embedding(embedding_size,embedding_dim)\n            \n        return nn.ModuleDict(embeddings),total_embedding_dim\n        \n    def forward(self,num_tensor,cat_tensor):\n        cat_outputs = []\n        for i,col in enumerate(cat_cols):\n            embedding = self.embeddings[col]\n            cat_output = embedding(cat_tensor[:,i])\n            cat_outputs.append(cat_output)\n        cat_outputs = torch.cat(cat_outputs,dim = 1)\n        all_outputs = torch.cat([num_tensor,cat_outputs],dim = 1)\n        final_output = self.layers(all_outputs).squeeze(dim=-1)\n        return final_output\n        \n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=1e-3)\n        \n    def training_step(self,batch,batch_idx):\n        num_tensor,cat_tensor,target = batch\n        y_pred = self(num_tensor,cat_tensor)\n        loss = F.cross_entropy(y_pred,target)\n        return loss\n    def val_step(self,batch,batch_idx):\n        num_tensor,cat_tensor,target = batch\n        y_pred = self(num_tensor,cat_tensor)\n        loss = F.cross_entropy(y_pred,target)\n        return loss\n    def test_step(self,batch,batch_idx):\n        num_tensor,cat_tensor,target = batch\n        y_pred = self(num_tensor,cat_tensor)\n        loss = F.cross_entropy(y_pred,target)\n        self.log(\"Loss:\",loss)\n        return loss","metadata":{"execution":{"iopub.status.busy":"2021-06-20T01:58:10.041317Z","iopub.execute_input":"2021-06-20T01:58:10.041626Z","iopub.status.idle":"2021-06-20T01:58:10.053722Z","shell.execute_reply.started":"2021-06-20T01:58:10.0416Z","shell.execute_reply":"2021-06-20T01:58:10.05259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nThis to determine the size of each embedding dimension check out fast.ai course to get a better understanding of this method\n\"\"\"\nn_classes = 2\nembedding_size_dict = {col: len(code) for col, code in cat_code_dict.items()}\nembedding_dim_dict = {col: embedding_size // 2 for col, embedding_size in embedding_size_dict.items()}\nembedding_dim_dict","metadata":{"execution":{"iopub.status.busy":"2021-06-20T01:58:10.059801Z","iopub.execute_input":"2021-06-20T01:58:10.060096Z","iopub.status.idle":"2021-06-20T01:58:10.069903Z","shell.execute_reply.started":"2021-06-20T01:58:10.06007Z","shell.execute_reply":"2021-06-20T01:58:10.068973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tabular_data_module = TabularDatsetModule(df_train,df_test,df_val, num_cols, cat_cols, label)\n\n# we can print out the network architecture for inspection\ntabular_model = TabularNet(num_cols, cat_cols, embedding_size_dict, n_classes, embedding_dim_dict)\ntabular_model","metadata":{"execution":{"iopub.status.busy":"2021-06-20T01:58:10.080584Z","iopub.execute_input":"2021-06-20T01:58:10.080881Z","iopub.status.idle":"2021-06-20T01:58:10.088706Z","shell.execute_reply.started":"2021-06-20T01:58:10.080856Z","shell.execute_reply":"2021-06-20T01:58:10.087701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrainer = pl.Trainer(max_epochs=1000)#chose the epochs wisely on a kaggle server 1 epoch takes 17s \ntrainer.fit(tabular_model, tabular_data_module)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T01:58:10.098713Z","iopub.execute_input":"2021-06-20T01:58:10.099171Z","iopub.status.idle":"2021-06-20T01:58:28.033183Z","shell.execute_reply.started":"2021-06-20T01:58:10.099132Z","shell.execute_reply":"2021-06-20T01:58:28.03236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.test()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T01:58:28.034188Z","iopub.execute_input":"2021-06-20T01:58:28.034444Z","iopub.status.idle":"2021-06-20T01:58:30.096864Z","shell.execute_reply.started":"2021-06-20T01:58:28.03442Z","shell.execute_reply":"2021-06-20T01:58:30.096112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Good Enough","metadata":{}}]}