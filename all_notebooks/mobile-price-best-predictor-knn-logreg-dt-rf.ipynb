{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Mobile Price Prediction\nDataset Courtesy of Abhishek Sharma (https://www.kaggle.com/iabhishekofficial/).\n\nExercise: \nRecently, I was gifted a smartphone. Now I want to know what the value of the phone I was gifted is likely worth.\n\nSo in this notebook, I will seek to find the best predictive model based on mobile phone features in this structured dataset. \n\nIn this exercise, we explore the algorithms:\n* Logistic Regression\n* KNN\n* Decision Tree\n* Random Forest\nand also:  a Linear Regression as a bonus!\n\nWe will then use the model with the highest score to predict the price-range of my own handset, let's hope I get a good price range.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read in CSV in train_df DataFrame\ntrain_df = pd.read_csv('../input/mobile-price-classification/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preview train_df's head\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[:50]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Explore dataset with .info()\ntrain_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use describe function to explore dataset\ntrain_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Creation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create X and y datasets\nX = train_df.drop('price_range', axis=1)\ny = train_df['price_range']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split dataset\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating and Training Logistic Regression Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import and instantiate Logistic Regression model\nfrom sklearn.linear_model import LogisticRegression\nlogmodel = LogisticRegression(max_iter=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit model\nlogmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logmodel.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import and instantiate Logistic Regression model (this time with more iterations - 10000)\nfrom sklearn.linear_model import LogisticRegression\nlogmodel_10k = LogisticRegression(max_iter=10000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit model\nlogmodel_10k.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logmodel_10k.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating & Training KNN Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Impport, instantiate and fit model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=10)\nknn.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check score of KNN model\nknn.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Elbow Method For optimum value of K"},{"metadata":{"trusted":true},"cell_type":"code","source":"error_rate = []\nfor i in range(1,20):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reveal plot of Error rate vs K Value (Elbow Method)\nplt.figure(figsize=(10,6))\nplt.plot(range(1,20),error_rate,color='red', linestyle='dashed', marker='o',\n         markerfacecolor='yellow', markersize=5)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the  Visualization of Error rate vs K Value (above), we can visually and intuitively conclude that the n_neighbors parameter equalling 10 is sufficiently optimal. "},{"metadata":{},"cell_type":"markdown","source":"# Creating & Training Decision Tree Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import, instantiate and fit Decision Tree Model\nfrom sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier()\ndtree.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check score of Decision Tree model\ndtree.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing the Decision Tree model"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names=['battery_power', 'blue', 'clock_speed', 'dual_sim', 'fc', 'four_g',\n       'int_memory', 'm_dep', 'mobile_wt', 'n_cores', 'pc', 'px_height',\n       'px_width', 'ram', 'sc_h', 'sc_w', 'talk_time', 'three_g',\n       'touch_screen', 'wifi']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating & Training Random Forest Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import, instantiate and fit model\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=200)\nrfc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check score of Random Forest Classifier model\nrfc.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# From the findings, we can rank the models by their score. The KNN model performed the best, followed by Random Forest Classifier, then Decision Tree and finally the Logistic Regression Model."},{"metadata":{},"cell_type":"markdown","source":"# Let's assess the classification report and confusion matrix of the KNN and Random Forest Classifier models."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import Classification Report and Confusion Matrix functions\nfrom sklearn.metrics import classification_report, confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's assess KNN's performance first"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate predictions from X_test dataset for KNN model and Random Forest Classifier models\nknn_pred = knn.predict(X_test)\nrfc_pred = rfc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reveal Classification Report for the KNN model\nprint(classification_report(y_test,knn_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reveal Confusion Matrix for KNN model\nprint(confusion_matrix(y_test, knn_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reveal Classification Report for the Random Forest Classifer model\nprint(classification_report(y_test,rfc_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reveal Confusion Matrix for Random Forest Classifier model\nprint(confusion_matrix(y_test, rfc_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing own mobile sample to test prediction\nown_mobile_test = pd.read_csv('/kaggle/input/mobilesample/own_mobile2.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show data sample\nown_mobile_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate single sample \nown_mobile_X_test = own_mobile_test.drop('price_range', axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# With the knowledge that my own mobile handset for sampling is a premium priced handset with high-end specifications but in a form-factor associated a lower price range. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create predictions from the 4 models and compare\n\n# KNN Model prediction\nknn_own_pred = knn.predict(own_mobile_X_test)\nknn_own_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Regression Model prediction\nlogmodel_own_pred = logmodel.predict(own_mobile_X_test)\nlogmodel_own_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decision Tree Model prediction\ndtree_own_pred = dtree.predict(own_mobile_X_test)\ndtree_own_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest Classifier Model prediction\nrfc_own_pred = rfc.predict(own_mobile_X_test)\nrfc_own_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# It seems all 4 models predicted that it was in the highest price range (which is true in real-life experience)."},{"metadata":{},"cell_type":"markdown","source":"# Creating & Training Linear Regression Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import, instantiate and fit Linear Regression Model\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_linear = lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RMSE: np.sqrt(mean_square_error()):\nfrom sklearn.metrics import mean_squared_error\nrmse = np.sqrt(mean_squared_error(y_test, y_pred_linear))\nprint(\"Root Mean Squared Error: {}\".format(rmse))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check score of Linear Regression model\nlr.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the prediction with the Linear Regression Model on my own mobile sample\nlr_own_pred = lr.predict(own_mobile_X_test)\nlr_own_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nIndeed the Linear Regression model shows a value that is a higher price range than the price range for the entire training dataset (whilst the earlier Classifier models predict the training dataset's maximum price_range of 3). \n\nThese results reflect the fact that the sample mobile phone was released many years after  date of the original dataset and thus has features that far exceed almost all of the phone specifications in the existing dataset. A prediction from linear regression that exceeds the price_range of 3 is entirely plausible. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}