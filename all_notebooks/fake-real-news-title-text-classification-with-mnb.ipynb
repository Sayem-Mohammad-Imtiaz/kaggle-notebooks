{"cells":[{"metadata":{},"cell_type":"markdown","source":"Text Classification based on Fake and Real News Dataset, titles only, using Multinomial Naive Bayes and TF.IDF. Any feedback is appreciated!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# EDA","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"fake = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/Fake.csv', usecols=['title', 'subject'])\nfake['label'] = 'fake'\n\ntrue = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/True.csv', usecols=['title', 'subject'])\ntrue['label'] = 'real'\n\ndata = pd.concat([fake, true])\ndata.reset_index(inplace=True)\ndata.drop(columns=['index'], inplace=True)\n\nfake = None\ntrue = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['title_word_count'] = data.title.apply(len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = plt.bar([0, 1], [data[data.label == 'fake'].shape[0], data[data.label == 'real'].shape[0]])\nplt.xticks([0,1], ['fake', 'real'])\nplt.xlabel('Label')\nplt.ylabel('Frequency')\nplt.title('Observations Per Class')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking in on the observations per class, they are almost balanced, so I will not take steps to balance the classes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax0, ax1) = plt.subplots(2, 1, sharex=True)\nax0.hist(data[data.label=='fake'].title_word_count)\nax1.hist(data[data.label=='real'].title_word_count)\n\nfig.suptitle('Distribution of Title Word Count Per Class')\nax0.set_xlabel('Fake Class')\nax1.set_xlabel('Real Class')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the distribution of the word count per class, it is clear that the titles for the Fake class tend to be longer, and have a wider distribution than the true class. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"subjects = set(data.subject)\n\nword_count_by_subject = {'fake': {}, 'real': {}}\n\nfor subject in subjects:\n    word_count_by_subject['fake'][subject] = data[(data.label=='fake') & (data.subject == subject)].title_word_count.median()\n    word_count_by_subject['real'][subject] = data[(data.label=='real') & (data.subject == subject)].title_word_count.median()\n\nxs = range(0, len(subjects))\n\n\nplt.bar(xs, [value for subject, value in word_count_by_subject['fake'].items()], width=.5, label='Fake')\nplt.bar([x +.2 for x in xs], [value for subject, value in word_count_by_subject['real'].items()], width=0.5, color='orange', label='Real')\n\nplt.legend(loc='lower right',framealpha=.75)\n\nplt.xticks(xs, subjects, rotation=90)\nplt.tick_params('both')\nplt.xlabel('Subject')\nplt.ylabel('Median Title Word Count')\nplt.title('Median Title Word Count by Subject and Class')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the median title word count by subject and class, it is clear that the Fake news also a wider variety of \"subjects\", but that the subjects also overlap (why are there subjects for \"politicsNews\" and \"politics\"?), so I am now very curious about the data collection methods used. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Text Processing\n\nHere I will not do too much preprocessing, as I will allow the TF.IDF vectorizer to do most of the heavy lifting. However, I do lowercase and remove stopwords and special characters.  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import regex as re\n\nimport nltk\nfrom nltk.corpus import stopwords\n\ncached_stopwords = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def replace_spec(text):\n    regex = r'[^a-zA-z0-9/s]'\n    text = re.sub(regex, ' ', text)\n    return text\n\ndef process_title(title):\n    title = title.lower()\n    title = replace_spec(title)\n        \n    title_list = str.split(title)\n    \n    final_title =[]\n    \n    for item in title_list:\n        if item not in cached_stopwords:\n            final_title.append(item)\n            \n    return \" \".join(final_title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['title_final'] = data.title.apply(process_title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# previewing the before and after of text processing\nprint(data.at[0, 'title'], '\\n', data.at[0, 'title_final'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# changing to a binary (0, 1) label\ndata.label = data.label.apply(lambda x: 1 if x == 'fake' else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(data[['title_final']], data.label, test_size=.1, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"docs = x_train.title_final.tolist()\n\nvectorizer = TfidfVectorizer(strip_accents='unicode', min_df=3)\n\ntime_start = time.time()\nvectorizer.fit(docs)\ntime_end = time.time()\n\n\nprint(f\"vectorizer fit in {(time_end-time_start)/60} mins\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tfidf = vectorizer.transform(x_train.title_final.tolist())\ntest_tfidf = vectorizer.transform(x_test.title_final.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.shape, train_tfidf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mnb = MultinomialNB()\n\ntime_start = time.time()\nmnb.fit(train_tfidf, y_train.values)\ntime_end = time.time()\n\nprint(f\"mnb trained in {(time_end-time_start)/60} mins\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for k, v in {'TRAIN': [train_tfidf, y_train], 'TEST': [test_tfidf, y_test]}.items():\n    preds = mnb.predict(v[0])\n    \n    print(f\"{k} RESULTS\\n\\n{classification_report(v[1],preds)}\\n\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With basic preprocessing steps and using a relatively simplistic model (Naive Bayes), the model achieves 94% accuracy on the test set and high precision and recall. Further, the model performance does not drop significantly between the train and test set, so it looks like we have avoided overfitting. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Feature Importance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting the indexes of the top features by class\nfake_prob_sorted = mnb.feature_log_prob_[1, :].argsort()\nreal_prob_sorted = mnb.feature_log_prob_[0, :].argsort()\n\n# getting the top feature names\nfake_top_features = np.take(vectorizer.get_feature_names(), fake_prob_sorted[:20])\nreal_top_features = np.take(vectorizer.get_feature_names(), real_prob_sorted[:20])\n\n# creating dictionaries for each class with the feature name and log probability\nreal_dict = {k:v for k, v in zip(\n                                real_top_features, \n                                np.take(mnb.feature_log_prob_[0, :], real_prob_sorted[:20])\n                                )\n            }\n\nfake_dict = {k:v for k, v in zip(\n                                fake_top_features, \n                                np.take(mnb.feature_log_prob_[1, :], fake_prob_sorted[:20])\n                                )\n            }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating a dataframe using the dictionaries of top terms per class\ntop_terms = pd.DataFrame.from_dict(real_dict, orient='index')\ntop_terms.reset_index(inplace=True)\ntop_terms.rename(columns={'index': 'term', 0: 'log_prob'}, inplace=True)\ntop_terms['label'] = 'Real'\n\n# add the top terms for the fake class\nfor term, log_prob in fake_dict.items():\n    top_terms = top_terms.append({'term': term, 'log_prob': log_prob, 'label': 'Fake'}, ignore_index=True)\n\n# convert log probability to odds ratio\ntop_terms['odds'] = top_terms.log_prob.apply(np.exp)\n\n# sort alphabetically\ntop_terms = top_terms.sort_values('term', ascending=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating dictionary to be used for plotting\ny_map = {term: y for y, term in zip(range(0, top_terms.shape[0]), top_terms.term)}\n\nplot_map = {} \nfor index, row in top_terms.iterrows():\n    term = row['term']\n    \n    plot_map[term] = {\n        'x': row['odds'],\n        'y': y_map[term],\n        'c': 'red' if row['label'] == 'Fake' else 'blue'\n    } ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting\nfig = plt.figure(figsize=(8, 8))\n_ = plt.scatter(\n            [value['x'] for key, value in plot_map.items()], \n            [value['y'] + 5 for key, value in plot_map.items()],\n            s=5,\n            color=[value['c'] for key, value in plot_map.items()],\n)\n\n_ = plt.scatter(\n    [0]* top_terms.shape[0],\n    [r + 5 for r in range(0, top_terms.shape[0])],\n    s=5,\n    color='grey'\n)\n\nplt.legend()\nplt.yticks(ticks=[r + 5 for r in range(0, top_terms.shape[0])], labels=y_map.keys())\nplt.xlabel('Odds Ratio')\nplt.ylabel('Term')\nplt.title('Top Important Terms by Class')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The top features for each class have very similar log probabilities which is kind of surprising, as is the list of terms. I think the next iteration of the model would need to lemmatize so that similar wors are mapped together (e.g., infiltrate and infiltrated).  ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}