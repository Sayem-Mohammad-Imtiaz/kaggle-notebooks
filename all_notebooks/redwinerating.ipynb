{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Red Wine Quality\n\nThis is a simple classifier, comparing Decision Tree Classifier and Random Forest Classifier methods for analyzing the dataset that contains 11 features and one rating for every wine type. \n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndf = pd.read_csv('../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# below table shows the first couple of features for different samples\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# following graph shows the range of quality and count of the target variable\nsns.countplot(x = \"quality\", data = df, palette = \"Set3\")\nplt.title(\"Quality Count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* In order to visualize the correlation between the target and features,we can plot \"boxen\" plots, which are similar to box plots but show more info about the distribution. Types of box plots also shows us the outliers clearly.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x = \"quality\", y = \"fixed acidity\", data = df, kind = \"boxen\")\nplt.title(\"Fixed Acidity vs. Quality\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x = \"quality\", y = \"volatile acidity\", data = df, kind = \"boxen\")\nplt.title(\"Volatile Acidity vs. Quality\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x = \"quality\", y = \"citric acid\", data = df, kind = \"boxen\")\nplt.title(\"Citric Acidity vs. Quality\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x = \"quality\", y = \"residual sugar\", data = df, kind = \"boxen\")\nplt.title(\"Residual Sugar vs. Quality\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x = \"quality\", y = \"chlorides\", data = df, kind = \"boxen\")\nplt.title(\"Chlorides vs. Quality\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x = \"quality\", y = \"free sulfur dioxide\", data = df, kind = \"boxen\")\nplt.title(\"Free Sulfur Dioxide vs. Quality\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x = \"quality\", y = \"total sulfur dioxide\", data = df, kind = \"boxen\")\nplt.title(\"Total Sulfur Dioxide vs. Quality\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x = \"quality\", y = \"density\", data = df, kind = \"boxen\")\nplt.title(\"Density vs. Quality\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x = \"quality\", y = \"pH\", data = df, kind = \"boxen\")\nplt.title(\"pH vs. Quality\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x = \"quality\", y = \"sulphates\", data = df, kind = \"boxen\")\nplt.title(\"Sulphates vs. Quality\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x = \"quality\", y = \"alcohol\", data = df, kind = \"boxen\")\nplt.title(\"Alcohol vs. Quality\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Later with this analysis, I have found out that in order to increase the prediction accuracy, I can re-label the target with a new range. Such as:\n\n* Quality 3-4 = Below Average = 1\n* Quality 5-6-7 = Average = 2\n* Quality 8 = Above Average = 3\n\nI've added another column to the dataset, similar to the quality.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# \"rating\" is the new column which is correlated with wine quality \nrating = []\nfor i in df['quality']:\n    if i >= 3 and i < 5:\n        rating.append('1')\n    elif i >= 5 and i < 8:\n        rating.append('2')\n    elif i == 8:\n        rating.append('3')\ndf['rating'] = rating\n\n\n# we can see the total count of elements in the new grading system below\nCounter(df['rating'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# defined target and feature arrays as 'rating' being the target and other 11 colums as features\n\nx = df.iloc[:,:11]\ny = df['rating']\n\nprint(x.head(5))\nprint(\"-----------------------------------------\")\nprint(y.head(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n* It is important to scale the data before processing it. This can be done with standardization, which means rescaling the features so that they have the properties of a standard normal distribution. This distribution has a mean of zero and a standard deviation of one.\n\n> More information can be found [here](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nx_std = sc.fit_transform(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Principle component analysis (PCA) is a method of analysis which involves finding the linear combination of a set of variables that has maximum variance and removing its effect, repeating this. This technique is reduces the dimensionality of datasets, increasing interpretability but at the same time minimizing information loss. If one feature varies less than another feature because of their respective scales, PCA might determine that the direction of maximal variance more closely.\n\n> More information can be found [here](https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0202).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA()\nx_pca = pca.fit_transform(x_std)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We can split our data into two subsets, training set and test set. Training set which includes known outputs will be given to the model to learn on. Test data will be later on used to measure the prediction on this set.\n\n> More information can be found: [Resource_1](https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6), [Resource_2](https://developers.google.com/machine-learning/crash-course/training-and-test-sets/splitting-data), [Video_Tutorial](https://www.youtube.com/watch?v=fwY9Qv96DJY).\n\n\n* In the code below, test_size = 0.3 means that %30 of the data is split into test set and the remainig is left for training.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://miro.medium.com/max/700/1*-8_kogvwmL1H6ooN1A1tsQ.png)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x_pca, y, test_size = 0.3,random_state = 16)\nprint(\"Training Feature Set:\", x_train.shape,\"\\nTraining Output Set:\", y_train.shape, \"\\n\\nTest Feature Set:\",x_test.shape, \"\\nTest Output Set\", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now, the model needs to be fit with the data we split, before making predictions. I've chosen Decision Tree Classifier for the model type.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nmodel = DecisionTreeClassifier(random_state = 16)\nmodel.fit(x_train,y_train)\nprediction = model.predict(x_test).reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Finally, now that the model is trained with the subsets x_train and y_train, the x_test data is used for predicting the corresponding outputs. The last thing that is left is to compare these predictions with the subset y_test. This comparison will yield the approxiamate accuracy of the test in general.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = accuracy_score(y_test, prediction)\nprint(\"The Decision Tree Classifier test is %\",accuracy*100, \"accurate.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Repeating the last step with Random Forest Classifier in order to compare the test acuracy with Decision Tree Classifier.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nmodel_2 = RandomForestClassifier(n_estimators = 100, random_state = 16)\nmodel_2.fit(x_train, y_train)\nprediction_2 = model_2.predict(x_test).reshape(-1,1)\n\naccuracy_2 = accuracy_score(y_test, prediction_2)\nprint(\"The Random Forest Classifier test is %\",accuracy_2*100, \"accurate.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reducing the target range from (3, 4, 5, 6, 7, 8) to (1, 2, 3) obviously increaseses the accuracy and it is a simple, straight forward way. Altough you miss somewhat of the data. But these methods can be improved by filtering and tuning the features as input parameters, and preprocessing the data better.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Thank you for your time.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}