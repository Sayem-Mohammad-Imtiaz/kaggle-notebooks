{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 align=\"center\" style=\"color:#6699ff\"> DataCamp IEEE Fraud Detection </h1>"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://github.com/DataCampM2DSSAF/suivi-du-data-camp-equipe-tchouacheu_toure_niang_chokki/blob/master/img/credit-card-fraud-detection.png?raw=true\" width=\"800\" align=\"center\">"},{"metadata":{},"cell_type":"markdown","source":"#  <a style=\"color:#6699ff\"> Team </a>\n- <a style=\"color:#6699ff\">Mohamed NIANG </a>\n- <a style=\"color:#6699ff\">Fernanda Tchouacheu </a>\n- <a style=\"color:#6699ff\">Sokhna Penda Toure </a>\n- <a style=\"color:#6699ff\">Hypolite Chokki </a>"},{"metadata":{},"cell_type":"markdown","source":"# <a style=\"color:#6699ff\">  Table of Contents</a> \n\n<a style=\"color:#6699ff\"> I. Introduction</a>\n\n<a style=\"color:#6699ff\"> II. Descriptive Statistics & Visualization</a>\n\n<a style=\"color:#6699ff\"> III. Preprocessing</a>\n\n<a style=\"color:#6699ff\"> IV. Machine Learning Models</a>"},{"metadata":{},"cell_type":"markdown","source":"# <a style=\"color:#6699ff\"> I. Introduction</a>"},{"metadata":{},"cell_type":"markdown","source":"**Pourquoi la détection de fraude ?**\n> La fraude est un commerce d'un milliard de dollars et elle augmente chaque année. L'enquête mondiale de PwC sur la criminalité économique de 2018 a révélé que la moitié (49 %) des 7 200 entreprises interrogées avaient été victimes d'une fraude quelconque. C'est une augmentation par rapport à l'étude PwC de 2016, dans laquelle un peu plus d'un tiers des organisations interrogées (36 %) avaient été victimes de la criminalité économique.\n\n\nCette compétition est un problème de **classification binaire** - c'est-à-dire que notre variable cible est un attribut binaire (l'utilisateur qui fait le clic est-il frauduleux ou non ?) et notre objectif est de classer les utilisateurs en \"frauduleux\" ou \"non frauduleux\" le mieux possible."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file \n\nimport datetime\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport matplotlib.gridspec as gridspec\n%matplotlib inline\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.metrics import roc_auc_score\n\nimport xgboost as xgb\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport gc\ngc.enable()\n\nimport os\nos.chdir('/kaggle/input/ieeedatapreprocessing') # Set working directory\nprint(os.listdir('/kaggle/input/ieeedatapreprocessing'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Load data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nX_train = pd.read_pickle('train_df.pkl')\nX_test = pd.read_pickle('test_df.pkl')\nprint (\"Data is loaded!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train_transaction shape is {}'.format(X_train.shape))\nprint('test_transaction shape is {}'.format(X_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a style=\"color:#6699ff\"> IV. Machine Learning Models</a>"},{"metadata":{},"cell_type":"markdown","source":"## XGBoost feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# NORMALIZE D COLUMNS\nfor i in [1,2,3,4,5,10,11,15]:\n    if i in [1,2,3,5]: continue\n    X_train['D'+str(i)] =  X_train['D'+str(i)] - X_train.TransactionDT/np.float32(24*60*60)\n    X_test['D'+str(i)] = X_test['D'+str(i)] - X_test.TransactionDT/np.float32(24*60*60) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# FREQUENCY ENCODE TOGETHER\ndef encode_FE(df1, df2, cols):\n    for col in cols:\n        df = pd.concat([df1[col],df2[col]])\n        vc = df.value_counts(dropna=True, normalize=True).to_dict()\n        vc[-1] = -1\n        nm = col+'_FE'\n        df1[nm] = df1[col].map(vc)\n        df1[nm] = df1[nm].astype('float32')\n        df2[nm] = df2[col].map(vc)\n        df2[nm] = df2[nm].astype('float32')\n        print(nm,', ',end='')\n        \n# LABEL ENCODE\ndef encode_LE(col,train=X_train,test=X_test,verbose=True):\n    df_comb = pd.concat([train[col],test[col]],axis=0)\n    df_comb,_ = df_comb.factorize(sort=True)\n    nm = col\n    if df_comb.max()>32000: \n        train[nm] = df_comb[:len(train)].astype('int32')\n        test[nm] = df_comb[len(train):].astype('int32')\n    else:\n        train[nm] = df_comb[:len(train)].astype('int16')\n        test[nm] = df_comb[len(train):].astype('int16')\n    del df_comb; x=gc.collect()\n    if verbose: print(nm,', ',end='')\n\n# LABEL ENCODE 2\ndef encode_LE2(df1,df2,col,verbose=True):\n    df_comb = pd.concat([df1[col],df2[col]],axis=0)\n    df_comb,_ = df_comb.factorize()\n    df1[col] = df_comb[:len(df1)].astype('int32')\n    df2[col] = df_comb[len(df1):].astype('int32')\n    if verbose: print(col,', ',end='')\n        \n# GROUP AGGREGATION MEAN AND STD\ndef encode_AG(main_columns, uids, aggregations=['mean'], train_df=X_train, test_df=X_test, \n              fillna=True, usena=False):\n    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS\n    for main_column in main_columns:  \n        for col in uids:\n            for agg_type in aggregations:\n                new_col_name = main_column+'_'+col+'_'+agg_type\n                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n                if usena: temp_df.loc[temp_df[main_column]==-1,main_column] = np.nan\n                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n                                                        columns={agg_type: new_col_name})\n\n                temp_df.index = list(temp_df[col])\n                temp_df = temp_df[new_col_name].to_dict()   \n\n                train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')\n                test_df[new_col_name]  = test_df[col].map(temp_df).astype('float32')\n                \n                if fillna:\n                    train_df[new_col_name].fillna(-1,inplace=True)\n                    test_df[new_col_name].fillna(-1,inplace=True)\n                \n                print(\"'\"+new_col_name+\"'\",', ',end='')\n                \n# COMBINE FEATURES\ndef encode_CB(col1,col2,df1=X_train,df2=X_test):\n    nm = col1+'_'+col2\n    df1[nm] = df1[col1].astype(str)+'_'+df1[col2].astype(str)\n    df2[nm] = df2[col1].astype(str)+'_'+df2[col2].astype(str) \n    encode_LE(nm,verbose=False)\n    print(nm,', ',end='')\n    \n# GROUP AGGREGATION NUNIQUE\ndef encode_AG2(main_columns, uids, train_df=X_train, test_df=X_test):\n    for main_column in main_columns:  \n        for col in uids:\n            comb = pd.concat([train_df[[col]+[main_column]],test_df[[col]+[main_column]]],axis=0)\n            mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n            train_df[col+'_'+main_column+'_ct'] = train_df[col].map(mp).astype('float32')\n            test_df[col+'_'+main_column+'_ct'] = test_df[col].map(mp).astype('float32')\n            print(col+'_'+main_column+'_ct, ',end='')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TRANSACTION AMT CENTS\nX_train['cents'] = (X_train['TransactionAmt'] - np.floor(X_train['TransactionAmt'])).astype('float32')\nX_test['cents'] = (X_test['TransactionAmt'] - np.floor(X_test['TransactionAmt'])).astype('float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# FREQUENCY ENCODE: ADDR1, CARD1, CARD2, CARD3, P_EMAILDOMAIN\nencode_FE(X_train,X_test,['addr1','card1','card2','card3','P_emaildomain'])\n# COMBINE COLUMNS CARD1+ADDR1, CARD1+ADDR1+P_EMAILDOMAIN\nencode_CB('card1','addr1')\nencode_CB('card1_addr1','P_emaildomain')\n# FREQUENCY ENOCDE\nencode_FE(X_train,X_test,['card1_addr1','card1_addr1_P_emaildomain'])\n# GROUP AGGREGATE\nencode_AG(['TransactionAmt','D10','D11'],['card1','card1_addr1','card1_addr1_P_emaildomain'],['mean','std'],usena=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ADD MONTH FEATURE\nSTART_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\nX_train['DT_M'] = X_train['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\nX_train['DT_M'] = (X_train['DT_M'].dt.year-2017)*12 + X_train['DT_M'].dt.month \n\nX_test['DT_M'] = X_test['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\nX_test['DT_M'] = (X_test['DT_M'].dt.year-2017)*12 + X_test['DT_M'].dt.month ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ADD UID FEATURE\nX_train['day'] = X_train.TransactionDT / (24*60*60)\nX_train['uid'] = X_train.card1_addr1.astype(str)+'_'+np.floor(X_train.day-X_train.D1).astype(str)\n\nX_test['day'] = X_test.TransactionDT / (24*60*60)\nX_test['uid'] = X_test.card1_addr1.astype(str)+'_'+np.floor(X_test.day-X_test.D1).astype(str)\n# LABEL ENCODE\nencode_LE2(X_train,X_test,'uid',verbose=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof = np.zeros(len(X_train))\npreds = np.zeros(len(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idxT = X_train.index[:4*len(X_train)//5]\nidxV = X_train.index[4*len(X_train)//5:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop = [\"TransactionID\", \"isFraud\", \"TransactionDT\"]\nuseful_cols = list(X_train.columns)\n\nfor col in cols_to_drop:\n    while True:\n        try:\n            useful_cols.remove(col)\n        except:\n            break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('NOW USING THE FOLLOWING',len(useful_cols),'FEATURES.')\nnp.array(useful_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost classifier with cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = X_train['isFraud'].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skf = GroupKFold(n_splits=6)\n\nfor i, (idxT, idxV) in enumerate( skf.split(X_train, y_train, groups=X_train['DT_M']) ):\n    month = X_train.iloc[idxV]['DT_M'].iloc[0]\n    \n    print('Fold',i,'withholding month',month)\n    print(' rows of train =',len(idxT),'rows of holdout =',len(idxV))\n    \n    xgboost_magic_classifier = xgb.XGBClassifier(\n            n_estimators=15000,\n            max_depth=20,\n            learning_rate=0.02,\n            subsample=0.8,\n            eval_metric='auc',\n            colsample_bytree=0.4,\n            missing=-999,\n            tree_method='gpu_hist' \n        )   \n    \n    xgboost_magic_classifier_fit = xgboost_magic_classifier.fit(X_train[useful_cols].iloc[idxT], y_train.iloc[idxT], \n            eval_set=[(X_train[useful_cols].iloc[idxV],y_train.iloc[idxV])],\n            verbose=100, early_stopping_rounds=500)\n    \n    oof[idxV] += xgboost_magic_classifier.predict_proba(X_train[useful_cols].iloc[idxV])[:,1]\n    preds += xgboost_magic_classifier.predict_proba(X_test[useful_cols])[:,1]/skf.n_splits\n    \n    del xgboost_magic_classifier_fit\n    x = gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_train, oof.round()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_train, oof.round()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature importance for XGBoost with cross validation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp = pd.DataFrame(sorted(zip(xgboost_magic_classifier.feature_importances_,useful_cols)), columns=['Value','Feature'])\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).iloc[:50])\nplt.title('XGBoost cross validation Most Important Features')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Seventh submission for XGBOOST with cross validation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('sample_submission.csv', index_col='TransactionID')\nsubmission.isFraud = preds\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(submission.isFraud,bins=100)\nplt.ylim((0,5000))\nplt.title('XGBoost cross validation submission')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('/kaggle/working/xgboost_cv_submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"}},"nbformat":4,"nbformat_minor":4}