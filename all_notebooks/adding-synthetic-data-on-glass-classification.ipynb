{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"3e702071-f4ea-9c14-6bb8-14a87a80e0a5"},"source":"# Studying glass classification"},{"cell_type":"markdown","metadata":{"_cell_guid":"d6f425b6-5930-55a8-cab2-654007081d1b"},"source":"I'm trying to learn a bit more on 2 topics:\n\n1. spotting and correcting skewed data\n2. tuning the Random Forest Classifier parameters to achieve better results\n\n## The data\nThis is the list of all the fields.\n\n* Id number: 1 to 214 (removed from CSV file)\n* RI: refractive index\n* Na: Sodium (unit measurement: weight percent in corresponding oxide, as are attributes 4-10)\n* Mg: Magnesium\n* Al: Aluminum\n* Si: Silicon\n* K: Potassium\n* Ca: Calcium\n* Ba: Barium\n* Fe: Iron\n* Type of glass: (class attribute)\n  *  1 building_windows_float_processed\n  * 2 building_windows_non_float_processed\n  * 3 vehicle_windows_float_processed\n  * 4 vehicle_windows_non_float_processed (none in this database)\n  * 5 containers\n  * 6 tableware\n  * 7 headlamps"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a4cfb2e0-b44a-0b07-ad5d-d304f9fb193a"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"markdown","metadata":{"_cell_guid":"c8de1f55-e577-ade5-e774-b8a14b8e6d01"},"source":"Loading dataset and showing some records"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e54dd6dd-c047-880a-de8f-df01a84b4b2c"},"outputs":[],"source":"df = pd.read_csv('../input/glass.csv')\n\ndf.sample(5)"},{"cell_type":"markdown","metadata":{"_cell_guid":"11d93fce-3693-e45c-838f-eb93876a90a5"},"source":"## Unbalanced classes\n\nThe classes in this dataset are not equally represented. Classes `3`, `5` and `6` are really poor and past analyses show that the algorithms struggle to classify data for them."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7d69a890-1ba4-989e-ecff-c1b50f99113f"},"outputs":[],"source":"df.groupby(by='Type').count()"},{"cell_type":"markdown","metadata":{"_cell_guid":"bc1e5ce0-8c06-a4c1-85ce-2ccd26fa2b7a"},"source":"I try to find some common values in records for those classes. The goal is to duplicate data including some variation in order to \"reinforce\" the classification.\n\n### Past results\nBefore doing it, I note here the past performances to see if there's an improvement.\n\nThis is the correlation between the features and the class:\n\n* Mg      0.744993\n* Al      0.598829\n* Ba      0.575161\n* Na      0.502898\n* Fe      0.188278\n* RI      0.164237\n* Si      0.151565\n* K       0.010054\n* Ca      0.000952\n\nThis is the best estimator perf for a Random Forest:\n\n`0.7890625`\n\n`{'n_estimators': 100, 'min_samples_split': 2, 'criterion': 'entropy', 'min_samples_leaf': 1}`\n\n\nAnd this is the cross-validation score:\n\n`Score: 0.756`\n\nLet's go.\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"979042be-9733-c4ec-8073-a6bb188af2a9"},"source":"### Generating random samples\n\nI create some useful function that helps me create new samples for the \"poor\" classes using their mean and standard deviation values and setting to 0 a set of labels"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4e32ac91-93a5-edb9-c7c7-341a98f2c801"},"outputs":[],"source":"\n# generate a -1 or +1 at random to add or subtract a random value from the mean\ndef random_sign():\n    return [-1 if np.random.random()<.5 else 1]\n\n# generate a new fake sample based on the passed means and standard deviations arrays,\n# setting the passed Type value and the passed labels at zero\ndef gen_sample(mm, ss, type, labels_at_zero=[]):\n    ix = ['RI', 'Na', 'Mg', 'Al', 'Si', 'K', 'Ca', 'Ba', 'Fe', 'Type']\n    new = np.abs(mm + (ss * np.random.random(len(ix)) * random_sign() ))\n    for l in labels_at_zero:\n        new[l] = 0\n    new['Type'] = type\n    return new\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d295382f-723e-a0d2-e609-1706417999d0"},"outputs":[],"source":"# Class 6\n# Selecting the 'mean' and 'std' columns from the describe() to generate random values for the class\nmeans = df[df['Type']==6].describe().loc['mean',:]\nstds = df[df['Type']==6].describe().loc['std',:]\n\n# this was the full table, I took only the second and third row\ndf[df['Type']==6].describe()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0b1139c1-eb19-d46d-abe6-f08f491046eb"},"outputs":[],"source":"# using a temp value\ndfnew = df\nfor i in range(0, 20):\n    dfnew = dfnew.append(gen_sample(means, stds, 6, ['Ba', 'Fe', 'K']), ignore_index=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"76779c74-bc18-8d0f-c320-7998fdf6dab0"},"outputs":[],"source":"# Let's check what happened\ndfnew[dfnew['Type']==6].describe()"},{"cell_type":"markdown","metadata":{"_cell_guid":"0862d595-abc6-1dc4-cd39-e8ad7e3441a5"},"source":"Count raised to 29, so correctly +20. Standard deviation obviously decreased because I was adding random values inside the stddev range. Maybe this helps prediction too but I feel I'm corrupting the data a bit more than I wanted.\n\nNow I do the same for the rest of the classes. BTW, I already checked that classes 3 and 5 do not have blank features."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"454b7fba-c357-e104-bbad-70ed7f980984"},"outputs":[],"source":"# Class 3\n# Selecting the 'mean' and 'std' columns from the describe() to generate random values for the class\nmeans = df[df['Type']==3].describe().loc['mean',:]\nstds = df[df['Type']==3].describe().loc['std',:]\n\nfor i in range(0, 13):\n    dfnew = dfnew.append(gen_sample(means, stds, 3, []), ignore_index=True)\n\n# Class 5\n# Selecting the 'mean' and 'std' columns from the describe() to generate random values for the class\nmeans = df[df['Type']==5].describe().loc['mean',:]\nstds = df[df['Type']==5].describe().loc['std',:]\nfor i in range(0, 17):\n    dfnew = dfnew.append(gen_sample(means, stds, 5, []), ignore_index=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"785528c0-7667-2787-52aa-ebbce4e500a5"},"source":"### A final check on the data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"41e7b0d9-3b9f-83a3-bbfa-42eeb7775619"},"outputs":[],"source":"dfnew.groupby(by='Type').count()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f620ae22-377f-ee3b-fe69-4605ead07b42"},"outputs":[],"source":"df = dfnew"},{"cell_type":"markdown","metadata":{"_cell_guid":"e194a722-c269-4bf9-c67a-0aedd8afee03"},"source":"### X and Y\nDropping the class (`Type` column) from the X set and moving it in the Y set"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"23722203-27b3-2785-6ec6-816a98a98e1b"},"outputs":[],"source":"X = df.drop(['Type'], axis=1)\nY = df['Type']"},{"cell_type":"markdown","metadata":{"_cell_guid":"f3c8088e-d892-3314-e4d3-c3814736ebfd"},"source":"How the features influence the classification"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2523b9e4-d9ae-9a22-9d3f-7272b23f9593"},"outputs":[],"source":"df.corr()['Type'].abs().sort_values(ascending=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"9988982a-ecc4-d323-2596-54f6bf425616"},"source":"### Calculating data skewness and possibly unskewing"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8587b0ba-812b-25bb-8c87-e71fdf11fe6b"},"outputs":[],"source":"import matplotlib.pylab as plt\nfrom sklearn import preprocessing\nfrom scipy.stats import skew\nfrom scipy.stats import boxcox\n\n# getting features names to loop\nclasses = X.columns.values\n\n# This will contain the unskewed features\nX_unsk = pd.DataFrame()\n\n# looping through the \nfor c in classes:\n    scaled = preprocessing.scale(X[c]) \n    boxcox_scaled = preprocessing.scale(boxcox(X[c] + np.max(np.abs(X[c]) +1) )[0])\n    \n    # Populating \n    X_unsk[c] = boxcox_scaled\n    \n    #Next We calculate Skewness using skew in scipy.stats\n    skness = skew(scaled)\n    boxcox_skness = skew(boxcox_scaled)\n    \n    #We draw the histograms \n    figure = plt.figure()\n    # First the original data shape\n    figure.add_subplot(121)   \n    plt.hist(scaled,facecolor='blue',alpha=0.55) \n    plt.xlabel(c + \" - Transformed\") \n    plt.title(\"Skewness: {0:.2f}\".format(skness)) \n    \n    # then the unskewed\n    figure.add_subplot(122) \n    plt.hist(boxcox_scaled,facecolor='red',alpha=0.55) \n    plt.title(\"Skewness: {0:.2f}\".format(boxcox_skness)) \n\n    plt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"ff60cf20-6999-e27d-7202-3ded7afbc018"},"source":"In most cases the BoxCox unskewing is successfully transforming the data"},{"cell_type":"markdown","metadata":{"_cell_guid":"3e901416-d0c4-01b2-0df2-2c0948aa48e2"},"source":"## Hyperparameters\n\nSearching the best parameters for the Random Forest Classifier "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"21ae28c1-569d-a67f-3da8-e388c737f9a4"},"outputs":[],"source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nimport math\n\n# Here I use the unskewed dataset\nX = X_unsk\nX_tr, X_ts, y_tr, y_ts = train_test_split(X, Y, test_size=0.40, random_state=42)\n\nrf = RandomForestClassifier(max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\nparam_grid = { \"criterion\" : [\"gini\", \"entropy\"]\n              , \"min_samples_leaf\" : [1, 5, 10]\n              , \"min_samples_split\" : [2, 4, 10, 12, 16]\n              , \"n_estimators\": [100, 125, 200]}\ngs = GridSearchCV(estimator=rf, param_grid=param_grid, scoring='accuracy', cv=3, n_jobs=-1)\ngs = gs.fit(X_tr, y_tr)"},{"cell_type":"markdown","metadata":{"_cell_guid":"cd44ce71-000a-d3be-3cd9-3cc5881e9c62"},"source":"### Score\n\nPrinting best score performance and algorithm parameters"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"573b89f2-62e7-873c-6948-ff3dce57e0b5"},"outputs":[],"source":"print(gs.best_score_)\nprint(gs.best_params_)"},{"cell_type":"markdown","metadata":{"_cell_guid":"721fd245-6596-d738-cebc-f912b31da533"},"source":"### Training\n\nFinal training with the best hyperparameters found by GridSearchCV"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7013de95-ad11-491f-4a99-eaf2c5dbfb36"},"outputs":[],"source":"bp = gs.best_params_\nrf = RandomForestClassifier( criterion=bp['criterion'], \n                             n_estimators=bp['n_estimators'],\n                             min_samples_split=bp['min_samples_split'],\n                             min_samples_leaf=bp['min_samples_leaf'],\n                             max_features='auto',\n                             oob_score=True,\n                             random_state=1,\n                             n_jobs=-1)\n\nrf.fit(X_tr, y_tr)\npred = rf.predict(X_ts)\n\nscore = rf.score(X_ts, y_ts)\nprint(\"Score: %.3f\" % (score))"},{"cell_type":"markdown","metadata":{"_cell_guid":"1ef6ace5-a7b8-0727-e17c-545553119108"},"source":"Works better on the training set but worse on the validation.\n\n### Features importance\nThis is the features importances for the algorithm"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ea4593e6-be08-f9ff-20ec-b5abaf6df6dc"},"outputs":[],"source":"pd.concat((pd.DataFrame(X.columns, columns = ['variable']), \n           pd.DataFrame(rf.feature_importances_, columns = ['importance'])), \n          axis = 1).sort_values(by='importance', ascending = False)[:20]"},{"cell_type":"markdown","metadata":{"_cell_guid":"3575c314-b157-7709-579a-9a6b7d7515b2"},"source":"Taking a look at the confusion matrix."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e9efadc5-fc39-d261-0081-36dbb4aaebd2"},"outputs":[],"source":"from sklearn.metrics import confusion_matrix\nimport itertools\n#print(y_ts.values)\n#print(pred)\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = (cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]).round(decimals=2)\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\ncnf_matrix = confusion_matrix(y_ts.values, pred)\n\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=np.sort(y_ts.unique()), normalize=False,\n                      title='Confusion matrix, without normalization')"},{"cell_type":"markdown","metadata":{"_cell_guid":"3b55bad0-100c-81f1-c40f-e86edfd0a101"},"source":"Classes 4 and 5 are ok now but I corrupted 1 and 2 and 3 is matching less then 50% of the times."},{"cell_type":"markdown","metadata":{"_cell_guid":"8e20c4c3-9cf6-264c-4001-26196c3592d6"},"source":"## Trying with XGBoost\n\nAs suggested in the comments, I try to compare this Random Forest model with an XGBoost to see how both perform with this dataset. [I fixed the values to the successful set calculated on my PC because it takes too much to run online]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"98ae8ef7-5bea-387e-8fa3-6092249966d9"},"outputs":[],"source":"from xgboost import XGBClassifier\n\n# Here I use the unskewed dataset\nX = X_unsk\nX_tr, X_ts, y_tr, y_ts = train_test_split(X, Y, test_size=0.4, random_state=42)\nxgb = XGBClassifier()\n\nparam_grid = { \"max_depth\" : [5]\n              , \"learning_rate\" : [0.125]\n              , \"n_estimators\": [50]\n              , \"reg_lambda\": [.1]}\ngs = GridSearchCV(estimator=xgb, param_grid=param_grid, scoring='accuracy', cv=3, n_jobs=-1, verbose=1)\ngs = gs.fit(X_tr, y_tr)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f2232a70-3ea8-b9fa-3c38-deb295b26dc0"},"outputs":[],"source":"print(gs.best_score_)\nprint(gs.best_params_)"},{"cell_type":"markdown","metadata":{"_cell_guid":"b6f49faa-55d0-a6a3-e020-e59f52f72c97"},"source":"The score on the TS is a bit worse than the RF even though I tried many parameters combination."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ac36248b-07b4-02aa-898a-f7a9a0b6a743"},"outputs":[],"source":"bp = gs.best_params_\nxgb = XGBClassifier( max_depth=bp['max_depth'], \n                             n_estimators=bp['n_estimators'],\n                             learning_rate=bp['learning_rate'],\n                   reg_lambda=bp['reg_lambda'])\n\nxgb.fit(X_tr, y_tr)\npred = xgb.predict(X_ts)\n\nscore = xgb.score(X_ts, y_ts)\nprint(\"Score: %.3f\" % (score))"},{"cell_type":"markdown","metadata":{"_cell_guid":"9304e4a9-d493-7093-98f0-2f674c0e39a9"},"source":"The feature importance matrix is slightly different from the RF's one."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dd3c020b-cd47-7109-ec70-4eabcaec61a0"},"outputs":[],"source":"pd.concat((pd.DataFrame(X.columns, columns = ['variable']), \n           pd.DataFrame(xgb.feature_importances_, columns = ['importance'])), \n          axis = 1).sort_values(by='importance', ascending = False)[:20]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aa58a244-13ac-507b-100f-3d72b712e073"},"outputs":[],"source":"cnf_matrix = confusion_matrix(y_ts.values, pred)\n\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=np.sort(y_ts.unique()), normalize=False,\n                      title='Confusion matrix, without normalization')"},{"cell_type":"markdown","metadata":{"_cell_guid":"74495e9f-648e-a4c0-2dcb-79a4309b7990"},"source":"Again the issue here are features 1 and 2 that worked a lot better before the fake data was introduced, and the 3 that is really mismatched."},{"cell_type":"markdown","metadata":{"_cell_guid":"3bedebd6-da3c-44bb-11d1-628cbda81859"},"source":"## Conclusions\n\nOLD COMMENT:\n> I'm really a novice in ML but I'm trying to apply all the interesting stuff I find in many awesome Kaggle kernels and discussions in order to slowly learn how work with data.\n> In this case I've learned a bit more about feature skewing, Random Forest parameters tuning and I did some experiment with XGBoost but the kernel is really far from defining a decent classifier for the glass classification problem. Maybe I need to study more the dataset and to try other classifiers.\n\nNEW COMMENT:\nI tried to fix the poor performance of this small dataset adding fake values, similar to the existing ones, for the less represented classes. It has not worked as I expected, maybe I added too much data or maybe someone has a better explanation out there!\n\nAnyway, any advice is welcome!"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}