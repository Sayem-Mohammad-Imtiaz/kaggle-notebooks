{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\nLeague of Legends (LoL) is just one of those games that really frustrating when you lose, but pretty fun overall - even more so with friends!  Here I will try to to map key features that can lead to a win.  Special thanks to [Dr. Penguin](https://www.kaggle.com/xiyuewang/lol-how-to-win) as I modified from his notebook.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load packages and dataset\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%matplotlib inline\nsns.set_style('darkgrid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/league-of-legends-diamond-ranked-games-10-min/high_diamond_ranked_10min.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# check missing values and data type\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dataset is pretty clean so, I'm not gonna remove any features for now except for gameId.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(columns='gameId')\n#Checking if removed\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Next let's check the relationship between parameters of blue team features\ng = sns.PairGrid(data=df, vars=['blueKills', 'blueAssists', 'blueWardsPlaced', 'blueTotalGold'], hue='blueWins', size=3, palette='Set1')\ng.map_diag(plt.hist)\ng.map_offdiag(plt.scatter)\ng.add_legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see co-linearity between variables.  I am going to go with a tree based method.  Luckily, tree algorithms are immune to multicollinearity by nature, since the tree will choose only one of the perfectly correlated features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can see that a lot of the features are highly correlated, let's get the correlation matrix\nplt.figure(figsize=(16, 12))\nsns.heatmap(df.drop('blueWins', axis=1).corr(), cmap='YlGnBu', annot=True, fmt='.2f', vmin=0);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Selection","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# train test split scale the set\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns='blueWins')\ny = df['blueWins']\n\n# I am chosing not to scale the data first, since I am going the tree route.\n#Decision trees and ensemble methods do not require feature scaling to be performed as they are not sensitive to the the variance in the data.\n\n##scaler = MinMaxScaler()\n##scaler.fit(X)\n##X = scaler.transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gradient Boosting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit the model\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import metrics\n\ngb = GradientBoostingClassifier()\n\n# search the best params\ngrid = {'n_estimators':[100,200,300,400,500], 'max_depth': [2, 5, 10]}\n\nclf_gb = GridSearchCV(gb, grid, cv=5)\nclf_gb.fit(X_train, y_train)\n\npred_gb = clf_gb.predict(X_test)\n\n# get the accuracy score\nacc_gb = accuracy_score(pred_gb, y_test)\nprint(acc_gb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confusion matrix\nfrom sklearn.metrics import plot_confusion_matrix\n# Plot non-normalized confusion matrix\ntitles_options = [(\"Confusion matrix, without normalization\", None),\n                  (\"Normalized confusion matrix\", 'true')]\nfor title, normalize in titles_options:\n    disp = plot_confusion_matrix(clf_gb, X_test, y_test,\n                                 #display_labels=class_names,\n                                 cmap=plt.cm.Blues,\n                                 normalize=normalize)\n    disp.ax_.set_title(title)\n\n    print(title)\n    print(disp.confusion_matrix)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance for Win","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting settings from GridCV\nclf_gb.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#inputting best settings from GridCV\ngb = GradientBoostingClassifier(n_estimators=100, max_depth=3)\nclf_gb.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.inspection import permutation_importance\n\nr = permutation_importance(clf_gb, X_test, y_test,\n                           n_repeats=20,\n                           random_state=0)\n\nfor i in r.importances_mean.argsort()[::-1]:\n    if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n        print(f\"{X_train.columns[i]:<10}\"\n              f\"{r.importances_mean[i]:.3f}\"\n              f\" +/- {r.importances_std[i]:.3f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Based on the minimally tuned gradient boosted trees, the prediction accuracy is roughly 73%.  I think that's not too bad, since most League win rates hover around 50% anyway.  Based on the feature importance, we can see that acquiring gold is the largest benefit to winning a game.  But that really isn't too suprising, since gold is an indirect measure of how well a team is doing.  \n\nThe next big advantage is getting dragons.  Now this makes sense due to dragons giving permanent buffs. Also having good dragon control from my experience means that you have a good jungler with a strong enough bot lane to secure the buff.  Closely tied to dragons is the jungle minions killed.  I think this makes a sense too due to a stronger jungler tends to have better items that lead to more successful ganks.\n\nThe last key feature I want to discuss is vision.  Vision is key, since if you know where the enemy is then you can make proper plans to avoid deaths and plan for objectives.\n\nIn summary, LoL is a pretty complex game and, based on this simple analysis, that there are many factors that come into play for a victory.  If I had to sum it up in three points to help you win more games is get dragons, CS well, and maintain vision control to reduce deaths.  Hope you liked it for my first shared kernel/notebook and comments are welcome!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}