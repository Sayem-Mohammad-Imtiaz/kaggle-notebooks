{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport string\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D, Bidirectional\nfrom sklearn.model_selection import train_test_split\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam\nfrom tqdm import tqdm\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nstop=set(stopwords.words('english'))\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#coachella = pd.read_csv('../input/coachella2015/Coachella-2015-2-DFE.csv', engine=\"python\")\n#coachella.head(3)\n#co_df = coachella[[\"coachella_sentiment\", \"text\"]]\n#co_df = co_df[co_df[\"text\"].str.contains(\"fuck\")]\n#co_df.head(10)\n#co_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#twitter = pd.read_csv('../input/twitter-sentiment-analysis-hatred-speech/train.csv')\n#twitter.head(3)\n#twitter = twitter[twitter[\"tweet\"].str.contains(\"fuck\")]\n#twitter.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATASET_ENCODING = \"ISO-8859-1\"\nDATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\nDATASET_PATH = \"../input/sentiment140/training.1600000.processed.noemoticon.csv\"\nCURSE_WORDS = [\"asshole\", \"bitch\", \"crap\", \"cunt\", \"damn\", \"fuck\", \"hell\", \"shit\", \"slut\", \"nigga\", \"prick\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter = pd.read_csv(DATASET_PATH, encoding = DATASET_ENCODING, names = DATASET_COLUMNS)\ntwitter.head(3)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter = twitter[[\"target\", \"text\"]]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter['text'] = twitter['text'].apply(lambda x : remove_URL(x))\ntwitter['text'] = twitter['text'].apply(lambda x : remove_html(x))\ntwitter['text'] = twitter['text'].apply(lambda x : remove_emoji(x))\ntwitter['text'] = twitter['text'].apply(lambda x : remove_punct(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"curse_words = ' ' + ' | '.join(CURSE_WORDS) + ' '\ntwitter['text'] = twitter['text'].apply(lambda x : ' ' + x + ' ')\ncurse_tweets = twitter[twitter[\"text\"].str.contains(curse_words, case=False)]\ncurse_tweets.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"curse_tweets.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"curse_tweets['target'] = curse_tweets['target'].apply(lambda x : 1 if x == 4 else x)\nx = curse_tweets.target.value_counts()\nsns.barplot(x.index, x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=curse_tweets[curse_tweets['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='red')\nax1.set_title('positive tweets')\ntweet_len=curse_tweets[curse_tweets['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='green')\nax2.set_title('negative tweets')\nfig.suptitle('Words in a tweet with curse words')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_corpus(df):\n    corpus=[]\n    for tweet in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n        corpus.append(words)\n    return corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus=create_corpus(curse_tweets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dict={}\nwith open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN=35\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    \n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Sequential()\n\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nforward_layer = LSTM(64, return_sequences=True)\nbackward_layer = LSTM(64, activation='relu', return_sequences=True,\n                       go_backwards=True)\nmodel.add(Bidirectional(forward_layer, backward_layer=backward_layer))\nmodel.add(Bidirectional(LSTM (64,dropout=0.2)))\n#model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=Adam(learning_rate=1e-3)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_val, X_test, y_train_val, y_test = train_test_split(tweet_pad, curse_tweets['target'].values, test_size = 0.15, random_state = 42, stratify = curse_tweets['target'])\nX_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size = 0.15/0.85, random_state = 42, stratify = y_train_val)\nprint('Shape of Train',X_train.shape)\nprint(\"Shape of Validation \",X_val.shape)\nprint(\"Shape of Test \",X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history=model.fit(X_train,y_train,batch_size=128,epochs=15,validation_data=(X_val,y_val),verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test)\nprint(y_pred.shape)\ny_pred=np.round(y_pred).astype(int).reshape(y_test.shape)\naccuracy = sum([p == y for p, y in zip(y_pred, y_test)]) / len(y_pred) * 100\nprint(accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BERT"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow_hub as hub\nimport bert\nfrom BertLibrary import BertFTModel\nfrom tensorflow.keras.models import Model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nTRAIN_SIZE = 0.7\nVAL_SIZE = 0.15\nTEST_SIZE = 1 - TRAIN_SIZE - VAL_SIZE\n\ntrain_val, test = train_test_split(curse_tweets, test_size = TEST_SIZE, random_state = 42)\ntrain, val = train_test_split(train_val, test_size = VAL_SIZE/(TRAIN_SIZE+VAL_SIZE), random_state = 42)\nprint(\"TRAIN size: \", len(train))\nprint(\"TEST size: \", len(test))\nprint(\"VAL size: \", len(val))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir dataset\ntrain.sample(frac=1.0).reset_index(drop=True).to_csv('dataset/train.tsv', sep='\\t', index=None, header=None)\nval.to_csv('dataset/dev.tsv', sep='\\t', index=None, header=None)\ntest.to_csv('dataset/test.tsv', sep='\\t', index=None, header=None)\n! cd dataset && ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_layer=hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",trainable=True)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}