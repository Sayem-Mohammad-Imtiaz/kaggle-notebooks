{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Загружаем наши данные"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/thoraric-surgery/ThoraricSurgery.csv', index_col = 'id')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nData Set Information:\n\nThe data was collected retrospectively at Wroclaw Thoracic Surgery Centre for patients who underwent major lung resections for primary lung cancer in the years 2007â€“2011. The Centre is associated with the Department of Thoracic Surgery of the Medical University of Wroclaw and Lower-Silesian Centre for Pulmonary Diseases, Poland, while the research database constitutes a part of the National Lung Cancer Registry, administered by the Institute of Tuberculosis and Pulmonary Diseases in Warsaw, Poland.\n\n\nAttribute Information:\n\n1. DGN: Diagnosis - specific combination of ICD-10 codes for primary and secondary as well multiple tumours if any (DGN3,DGN2,DGN4,DGN6,DGN5,DGN8,DGN1)\n2. PRE4: Forced vital capacity - FVC (numeric)\n3. PRE5: Volume that has been exhaled at the end of the first second of forced expiration - FEV1 (numeric)\n4. PRE6: Performance status - Zubrod scale (PRZ2,PRZ1,PRZ0)\n5. PRE7: Pain before surgery (T,F)\n6. PRE8: Haemoptysis before surgery (T,F)\n7. PRE9: Dyspnoea before surgery (T,F)\n8. PRE10: Cough before surgery (T,F)\n9. PRE11: Weakness before surgery (T,F)\n10. PRE14: T in clinical TNM - size of the original tumour, from OC11 (smallest) to OC14 (largest) (OC11,OC14,OC12,OC13)\n11. PRE17: Type 2 DM - diabetes mellitus (T,F)\n12. PRE19: MI up to 6 months (T,F)\n13. PRE25: PAD - peripheral arterial diseases (T,F)\n14. PRE30: Smoking (T,F)\n15. PRE32: Asthma (T,F)\n16. AGE: Age at surgery (numeric)\n17. Risk1Y: 1 year survival period - (T)rue value if died (T,F)\n\nПроизведем следующие преобразования для повышения читабельности датасета"},{"metadata":{"trusted":true},"cell_type":"code","source":"#T/F = 1/0\ndf[['PRE7', 'PRE8', 'PRE9', 'PRE10', 'PRE11', 'PRE17', 'PRE19', 'PRE25', 'PRE30', 'PRE32', 'Risk1Yr']] = \\\n(df[['PRE7', 'PRE8', 'PRE9', 'PRE10', 'PRE11', 'PRE17', 'PRE19', 'PRE25', 'PRE30', 'PRE32', 'Risk1Yr']] == 'T').astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#в ячейках содержащих числовые и строковые данные - оставим только цифры\ndf['DGN']   = df['DGN'].str[-1:].astype(int)\ndf['PRE6']  = df['PRE6'].str[-1:].astype(int)\ndf['PRE14'] = df['PRE14'].str[-1:].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#переименовываем\ncolumns = ['Diagnosis','Forced_Capacity','Forced_Expiration','Zubrod_scale','Pain',' Haemoptysis','Dyspnoea',\n       'Cough','Weakness','Size_of_tumor','diabetes','MI_6months','PAD','Smoker','Asthmatic','Age','Risk_1y']\ndf.columns = columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas_profiling import ProfileReport\nreport = ProfileReport(df, minimal = False, progress_bar=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"report.to_notebook_iframe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Данных c диагнозами очень очень мало\nОбъединим диагнозы 1,5,6,7,8 в один, т.к. по ним мало данных - и они существенного правильного прогноза не дадут (низкая точность)"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df.copy()\ndata['Diagnosis'] = np.where(df['Diagnosis'].isin([1,5,6,7,8]), 0, df['Diagnosis'])\ndata.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import learning_curve, GridSearchCV, train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import Ridge, LinearRegression, LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop(columns='Diagnosis')\ny = data.Diagnosis","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"пусть базовое предсказание будет без аугментации - обычная логистическая регрессия"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = LogisticRegression(class_weight = 'balanced')\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\nclf.fit(X_train, y_train)\npredictions = clf.predict(X_test)\nprint(metrics.classification_report(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"т.к по диагнозу 3 было больше всего наблюдений  там получилось более менее точные предсказания"},{"metadata":{},"cell_type":"markdown","source":"Проведем аугментацию данных"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE, ADASYN\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n\naugm = ADASYN()\nX_train_augm, y_train_augm = augm.fit_resample(np.array(X_train), np.array(y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#исходные данные\npd.Series(y_train).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6, 4))\nsns.countplot(y_train)\nplt.xlabel('Class')\nplt.ylabel('Count')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#данные после аугментации\npd.Series(y_train_augm).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6, 4))\nsns.countplot(y_train_augm)\nplt.xlabel('Class')\nplt.ylabel('Count')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import collections\ncollections.Counter(y_train_augm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Произведем прогноз с использованием логистической регрессии аугментированных данных"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = LogisticRegression()\nclf.fit(X_train_augm, y_train_augm)\npredictions = clf.predict(X_test)\nprint(metrics.classification_report(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_ground_truth_df = pd.DataFrame(list(zip(predictions, y_train_augm)))\npredictions_ground_truth_df.columns = ['Prediction', 'Ground_truth']\npredictions_ground_truth_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6, 4))\nplt.scatter(predictions_ground_truth_df.Prediction, predictions_ground_truth_df.Ground_truth, c = '#ad09a3')\nplt.xlabel('Predicted')\nplt.ylabel('Ground truth')\nplt.plot([0, 5], [0, 5], color=\"red\")\nplt.tight_layout()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Построим другие модели по аугментированным данным для получения улучшенного результата"},{"metadata":{"trusted":true},"cell_type":"code","source":"#KNN\nknn = KNeighborsClassifier()\nknn.fit(X_train_augm, y_train_augm)\npredictions = knn.predict(X_test)\nprint(metrics.classification_report(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#random forest\nrfc = RandomForestClassifier(random_state=666)\nrfc.fit(X_train_augm, y_train_augm)\npredictions = rfc.predict(X_test)\nprint(metrics.classification_report(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Метод опорных векторов (SVM)\nМы не можем заранее знать, какой тип ядра предсказывает наилучшие результаты - поэтому попробуем несколько различных типов ядер."},{"metadata":{"trusted":true},"cell_type":"code","source":"# linear svm:\nsvm = SVC(kernel='linear', probability=True)\nsvm.fit(X_train_augm, y_train_augm)\npredictions = svm.predict(X_test)\nprint(metrics.classification_report(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# linear rbf:\nsvm = SVC(kernel='rbf', probability=True, gamma=10)\nsvm.fit(X_train_augm, y_train_augm)\npredictions = svm.predict(X_test)\nprint(metrics.classification_report(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  poly svm:\nsvm = SVC(kernel='poly', probability=True)\nsvm.fit(X_train_augm, y_train_augm)\npredictions = svm.predict(X_test)\nprint(metrics.classification_report(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Лучший показатель при применении алгоритма SVM показало простейшее линейное ядро. \nДанный алгоритм хорошие резульататы показывает на небольших данных. В нашей модели он показал лучший результат т.к. данные наши линейно сепарабельные\nГаусовское и полиномиальное ядра хорошо себя показывают на больших данных и разнообразных признаках."},{"metadata":{},"cell_type":"markdown","source":"# Оптимизация гиперпараметров. RandomizedSearchCV ч.1\nна базе random forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\nmax_features = ['log2', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(start = 1, stop = 15, num = 15)]\nmin_samples_split = [int(x) for x in np.linspace(start = 2, stop = 50, num = 10)]\nmin_samples_leaf = [int(x) for x in np.linspace(start = 2, stop = 50, num = 10)]\nbootstrap = [True, False]\nparam_dist = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nrs = RandomizedSearchCV(rfc, \n                        param_dist, \n                        n_iter = 100, \n                        cv = 3, \n                        verbose = 1, \n                        n_jobs=-1, \n                        random_state=0)\nrs.fit(X_train_augm, y_train_augm)\nrs.best_params_\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"При значениях параметров n_iter = 100 и cv = 3, мы создали 300 RF-моделей, случайно выбирая комбинации представленных выше гиперпараметров. Мы можем обратиться к атрибуту best_params_ для получения сведений о наборе параметров, позволяющем создать самую лучшую модель. Но на данной стадии это может не дать нам наиболее интересных данных о диапазонах параметров, которые стоит изучить на следующем раунде оптимизации. Для того чтобы выяснить то, в каком диапазоне значений стоит продолжать поиск, мы легко можем получить датафрейм, содержащий результаты работы алгоритма RandomizedSearchCV."},{"metadata":{"trusted":true},"cell_type":"code","source":"rs_df = pd.DataFrame(rs.cv_results_).sort_values('rank_test_score').reset_index(drop=True)\nrs_df = rs_df.drop([\n            'mean_fit_time', \n            'std_fit_time', \n            'mean_score_time',\n            'std_score_time', \n            'params', \n            'split0_test_score', \n            'split1_test_score', \n            'split2_test_score', \n            'std_test_score'],\n            axis=1)\nrs_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(ncols=3, nrows=2)\nsns.set(style=\"whitegrid\", color_codes=True, font_scale = 2)\nfig.set_size_inches(30,25)\nsns.barplot(x='param_n_estimators', y='mean_test_score', data=rs_df, ax=axs[0,0], color='lightgrey')\nsns.barplot(x='param_min_samples_split', y='mean_test_score', data=rs_df, ax=axs[0,1], color='coral')\nsns.barplot(x='param_min_samples_leaf', y='mean_test_score', data=rs_df, ax=axs[0,2], color='lightgreen')\nsns.barplot(x='param_max_features', y='mean_test_score', data=rs_df, ax=axs[1,0], color='wheat')\nsns.barplot(x='param_max_depth', y='mean_test_score', data=rs_df, ax=axs[1,1], color='lightpink')\nsns.barplot(x='param_bootstrap',y='mean_test_score', data=rs_df, ax=axs[1,2], color='skyblue')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"используя то, что мы выяснили с помощью RandomizedSearchCV, исследуем значения гиперпараметров, которые лучше всего себя показали"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nn_estimators = [500,700]\nmax_features = ['sqrt']\nmax_depth = [7, 11, 15]\nmin_samples_split = [2,12,23,44]\nmin_samples_leaf = [2,7, 18]\nbootstrap = [False]\nparam_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\ngs = GridSearchCV(rfc, param_grid, cv = 3, verbose = 1, n_jobs=-1)\ngs.fit(X_train_augm, y_train_augm)\npredictions = gs.predict(X_test)\nprint(metrics.classification_report(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Вывод:**\nПри решении нашей задачи классификации на маленьких данных мы смогли сделать хорошие прогнозы по 3 и 4 диагнозу с помощью применения линейных моделей, т.к. категориальные признаки по данных диагнозам находятся в линейной зависимости, имеют низкий разброс и исходных данных для \"обучения с учителем\" было больше всего. Все остальные диагнозы получили практически случайное предсказание."},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}