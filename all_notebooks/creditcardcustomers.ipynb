{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np \nimport pandas as pd \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\n\"\"\"\nFollowing is the Data Dictionary for Credit Card dataset :-\n\nCUSTID : Identification of Credit Card holder (Categorical)\nBALANCE : Balance amount left in their account to make purchases \nBALANCEFREQUENCY : How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)\nPURCHASES : Amount of purchases made from account\nONEOFFPURCHASES : Maximum purchase amount done in one-go\nINSTALLMENTSPURCHASES : Amount of purchase done in installment\nCASHADVANCE : Cash in advance given by the user\nPURCHASESFREQUENCY : How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)\nONEOFFPURCHASESFREQUENCY : How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)\nPURCHASESINSTALLMENTSFREQUENCY : How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)\nCASHADVANCEFREQUENCY : How frequently the cash in advance being paid\nCASHADVANCETRX : Number of Transactions made with \"Cash in Advanced\"\nPURCHASESTRX : Numbe of purchase transactions made\nCREDITLIMIT : Limit of Credit Card for user\nPAYMENTS : Amount of Payment done by user\nMINIMUM_PAYMENTS : Minimum amount of payments made by user\nPRCFULLPAYMENT : Percent of full payment paid by user\nTENURE : Tenure of credit card service for user\n\nWe are going to utilise clustering techniques to segment the customers based on their credit card details\n\"\"\"\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A dataset for various credit card users,we can use this dataset to segment different types of customers."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/ccdata/CC GENERAL.csv\")\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Find all the categorical and non numeric varibles."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only Customer_id is string type,so we will drop it here."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop({'CUST_ID'},axis=1)\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let us convert all the int types to float types."},{"metadata":{"trusted":true},"cell_type":"code","source":"datatypes = {}\nfor column in df.columns:\n    datatypes[column] = float\n    \ndf = df.astype(datatypes)    \ndf.info()    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can see that some null values in the minimum_payments column.We will drop these rows here."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.dropna()\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We would like to drop the balance frequency(it doesn't relate to customer behaviour) and tenture as it is mostly 12 months."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop({'BALANCE_FREQUENCY','TENURE'},axis=1)\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will try to segemnt our customers based on these attributes."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.mean(axis=0) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We will drop the columns with very low mean values here as their significance is very less.\ndf = df.drop({\"PURCHASES_FREQUENCY\",\"ONEOFF_PURCHASES_FREQUENCY\",\"PURCHASES_INSTALLMENTS_FREQUENCY\",\"CASH_ADVANCE_FREQUENCY\",\"PRC_FULL_PAYMENT\",    \"CASH_ADVANCE_TRX\",\"PURCHASES_TRX\"},axis=1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nX = scaler.fit_transform(df)\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_clusters = 25\ninertia = []\nfor n in range(1,n_clusters):\n    km = KMeans(n)\n    km.fit(X)\n    inertia.append(km.inertia_)\n        \nplt.plot(inertia)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us choose 10 as the number of clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"km = KMeans(10)\nkm.fit(X)\nkm.inertia_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cluster = pd.concat([df, pd.DataFrame({'Cluster': km.labels_})],axis=1)\ndf_cluster = df_cluster.dropna()\ndf_cluster.info()\ndf_cluster.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"visualisation of clusters.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\narr_0 = df_cluster[df_cluster[\"Cluster\"] == 0.0]\narr_1 = df_cluster[df_cluster[\"Cluster\"] == 1.0]\narr_2 = df_cluster[df_cluster[\"Cluster\"] == 2.0]\narr_3 = df_cluster[df_cluster[\"Cluster\"] == 3.0]\narr_4 = df_cluster[df_cluster[\"Cluster\"] == 4.0]\narr_5 = df_cluster[df_cluster[\"Cluster\"] == 5.0]\narr_6 = df_cluster[df_cluster[\"Cluster\"] == 6.0]\narr_7 = df_cluster[df_cluster[\"Cluster\"] == 7.0]\narr_8 = df_cluster[df_cluster[\"Cluster\"] == 8.0]\narr_9 = df_cluster[df_cluster[\"Cluster\"] == 9.0]\nplt.figure(figsize=(10,15))\nfor c in df:\n    arr = []\n    arr.append(arr_0[c].mean())\n    arr.append(arr_1[c].mean())\n    arr.append(arr_2[c].mean())\n    arr.append(arr_3[c].mean())\n    arr.append(arr_4[c].mean())\n    arr.append(arr_5[c].mean())\n    arr.append(arr_6[c].mean())\n    arr.append(arr_7[c].mean())\n    arr.append(arr_8[c].mean())\n    arr.append(arr_9[c].mean())\n    \n    plt.plot(arr,label = c)\n    plt.xlabel(\"Clusters\")\n    plt.ylabel(\"Mean_values\")\n    plt.legend(loc=2,prop = {'size':8})\n    \n    \n    \n    \n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nTrends: Across all clusters.\nWe can see that credit_card limit(brown) and balances(blue) are directly linked.\nThe purchases(orange) and cash_advance(violet) are directly linked.\nNo clear trends between payments(pink) and purchases(orange).\nInstallments purchases(red) are not linked with balance or purchases,but are inversely linked with oneoff purchases(green).\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nX_PCA = scaler.fit_transform(df)\nX_PCA\n\nn_dim = 7\nexplained_variance  = []\nfor n in range(1,n_dim):\n    pca = PCA(n)\n    pca.fit_transform(X_PCA)\n    explained_variance.append(pca.explained_variance_ratio_.sum())\n    \nplt.plot(explained_variance)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will number of dimensions as 6 here.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(4)\nX_PCA = pca.fit_transform(X_PCA)\npca.explained_variance_ratio_\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.explained_variance_ratio_.sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Here we can see that the principal components have following variance explanation and overall the six components contribute to 82% of overall variance.Now we will cluster them based on the these components"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_clusters = 25\ninertia = []\nfor n in range(1,n_clusters):\n    km = KMeans(n)\n    km.fit_transform(X_PCA)\n    inertia.append(km.inertia_)\n    \nplt.plot(inertia)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again we are going to choose the value 10 as the number of clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"km = KMeans(10)\nkm.fit_transform(X_PCA)\nkm.inertia_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we see a substantial drop in inertia as compared to using 16 dimensions as in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cluster_1 = pd.concat([df,pd.DataFrame({\"Cluster_PCA\": km.labels_})],axis=1)\ndf_cluster_1 = df_cluster_1.dropna()\ndf_cluster_1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mean values of each attriubute with the number of clusters used."},{"metadata":{"trusted":true},"cell_type":"code","source":"\narr_0 = df_cluster_1[df_cluster_1[\"Cluster_PCA\"] == 0.0]\narr_1 = df_cluster_1[df_cluster_1[\"Cluster_PCA\"] == 1.0]\narr_2 = df_cluster_1[df_cluster_1[\"Cluster_PCA\"] == 2.0]\narr_3 = df_cluster_1[df_cluster_1[\"Cluster_PCA\"] == 3.0]\narr_4 = df_cluster_1[df_cluster_1[\"Cluster_PCA\"] == 4.0]\narr_5 = df_cluster_1[df_cluster_1[\"Cluster_PCA\"] == 5.0]\narr_6 = df_cluster_1[df_cluster_1[\"Cluster_PCA\"] == 6.0]\narr_7 = df_cluster_1[df_cluster_1[\"Cluster_PCA\"] == 7.0]\narr_8 = df_cluster_1[df_cluster_1[\"Cluster_PCA\"] == 8.0]\narr_9 = df_cluster_1[df_cluster_1[\"Cluster_PCA\"] == 9.0]\nplt.figure(figsize=(10,15))\nfor c in df:\n    arr = []\n    arr.append(arr_0[c].mean())\n    arr.append(arr_1[c].mean())\n    arr.append(arr_2[c].mean())\n    arr.append(arr_3[c].mean())\n    arr.append(arr_4[c].mean())\n    arr.append(arr_5[c].mean())\n    arr.append(arr_6[c].mean())\n    arr.append(arr_7[c].mean())\n    arr.append(arr_8[c].mean())\n    arr.append(arr_9[c].mean())\n    \n    plt.plot(arr,label = c)\n    plt.xlabel(\"Clusters\")\n    plt.ylabel(\"Mean_values\")\n    plt.legend(loc=2,prop = {'size':8})\n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nWe see the same trends here as above.\nOnly the correlations seem much sharper.\nAlso here purchases(pink) is much more correlated with balance(blue) and purchases(orange) here.\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus PCA improves the inertia of the kmeans clustering. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}