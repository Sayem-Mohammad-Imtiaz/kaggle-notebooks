{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Appliying ML Algorithms on Heart Disease Data\n* [Reading Data](#1)\n* [Understanding Data](#2)\n* [Normalize & Seperation of Data and Target](#3)\n* [Creating ML models](#4)\n    * [Logistics Regression](#5)\n    * [KNN Neighbours](#6)\n    * [Support Vector Machine](#7)\n    * [Naive Bayes](#8)\n    * [Decision Tree Classifier](#9)\n    * [Random Forest Classifier](#10)\n    * [Linear Discriminant Analysis](#11)\n* [Trying to Automate the Model Selection](#12)\n* [Conclusion](#13)\n* [Unsupervised Learning Work](#14)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, confusion_matrix,classification_report\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading Data  <a id = \"1\"></a><br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/heart-disease-uci/heart.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Understanding Data <a id = \"2\"></a><br>","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"display(df.head(),df.tail(),df.describe(),df.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There is no missing values.\n* Normalization is needed since feature means have big difference which can result in dominance of some fetures against others.\n* Target variable type is integer and in a binary form, which means no need for any actions\n* There is no object type variable ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Normalize & Seperation of Data and Target & Train Test Split<a id = \"3\"></a><br>\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Seperation of Data and Target\nx_data,y = df.loc[:,df.columns != 'target'],df.loc[:,'target'].values\n#normalize\nx = (x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data))\n#Train Test Split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3, random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Applying ML models<a id = \"4\"></a><br>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Logistics Regression<a id = \"5\"></a><br>\n* Logistic Regression resulted with 79% accuracy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\ny_pred_lr = lr.predict(x_test)\nprint('LR accuracy = {}'.format(lr.score(x_test,y_test)))\ncm_lr = confusion_matrix(y_test,y_pred_lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Logistic Regression Visiualization\nsns.set_palette('muted')\n\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm_lr,annot=True,linewidths=0.5,linecolor='black',fmt='.0f',ax=ax,)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## KNN Neighbours<a id = \"6\"></a><br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#K=3\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(x_train,y_train)\ny_pred_knn = knn.predict(x_test)\nprint('With K = {}, Accuracy is {}'.format(3,knn.score(x_test,y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#finding best K value\nscore_test = []\nscore_train =[]\nfor i in range(1,30):\n    knn2 = KNeighborsClassifier(i)\n    knn2.fit(x_train,y_train)\n    score_test.append(knn2.score(x_test,y_test))\n    score_train.append(knn2.score(x_train,y_train))\nf, ax = plt.subplots(figsize=(15,10))\nplt.plot(range(1,30),score_test,label='Test Data Accuracy')\nplt.plot(range(1,30),score_train,label='Train Data Accuracy')\nplt.legend()\nplt.xlabel('K values')\nplt.ylabel('Accuracy')\nplt.show()\nprint('Best Accuracy is = {} with the K Value of = {}'.format(np.max(score_test),+1+score_test.index(np.max(score_test))))\ny_pred_knn = knn2.predict(x_test)\ncm_knn = confusion_matrix(y_test,y_pred_knn)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f , ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm_knn,annot=True,linewidths=0.5,ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Machine <a id = \"7\"></a><br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvc = SVC(random_state=42)\nsvc.fit(x_train,y_train)\ny_pred_svc = svc.predict(x_test)\nprint('Svc score is = {}'.format(svc.score(x_test,y_test)) )\ncm_svc = confusion_matrix(y_test,y_pred_svc)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm_svc,annot=True,cmap='coolwarm',linewidth=0.5,ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Naive Bayes <a id = \"8\"></a><br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\ny_pred_nb = nb.predict(x_test)\nprint('Accuracy with Naive Bayes = {}'.format(nb.score(x_test,y_test)))\ncm_nb = confusion_matrix(y_test,y_pred_nb)\n\nf, ax= plt.subplots(figsize=(5,5))\nsns.heatmap(cm_nb,annot=True,linewidth=0.5,ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree Classifier<a id = \"9\"></a><br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\ndtc.fit(x_train,y_train)\ny_pred_dtc = dtc.predict(x_test)\ncm_dtc = confusion_matrix(y_test,y_pred_dtc)\n\nprint('Accuracy with Decision Tree Classifier {}'.format(dtc.score(x_test,y_test)))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax= plt.subplots(figsize=(5,5))\nsns.heatmap(cm_dtc,annot=True,linewidths=0.5,ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Classifier<a id = \"10\"></a><br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#finding best Estimator value\nfrom sklearn.ensemble import RandomForestClassifier\nscore_list = []\nfor i in range(1,101):\n    rfc2= RandomForestClassifier(n_estimators=i,random_state=42)\n    rfc2.fit(x_train,y_train)\n    score_list.append(rfc2.score(x_test,y_test))\n    print('Accuracy with Random forest is {} with {} Trees'.format(rfc2.score(x_test,y_test),i))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Best Accuracy with Random Forest Classifier is {} with the Decision Tree number of {} '.format(np.max(score_list),+1+score_list.index(np.max(score_list))))\nplt.plot(range(1,101),score_list,c='orange',label='RF Accuracy')\nplt.legend()\nplt.xlabel('# of Decision Trees')\nplt.ylabel('Accuracy')\nplt.show()\nbest_estimator = score_list.index(np.max(score_list))+1\nrfc = RandomForestClassifier(n_estimators=best_estimator,random_state=42)\nrfc.fit(x_train,y_train)\ny_pred_rfc = rfc.predict(x_test)\ncm_rfc = confusion_matrix(y_test,y_pred_rfc)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm_rfc, annot=True, linewidths=0.5,ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear Discriminant Analysis<a id = \"11\"></a><br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nlinr = LinearDiscriminantAnalysis()\nlinr.fit(x_train,y_train)\nprint('Accuracy with Linear Disc Analysis is {}'.format(linr.score(x_test,y_test)))\ny_pred_linr = linr.predict(x_test)\n\ncm_linr = confusion_matrix(y_test,y_pred_linr) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm_linr,annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Trying to Automate the Model Selection<a id = \"12\"></a><br>\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#ML models in Automation\n\nmodels = []\n\nmodels.append((\"LR\",LogisticRegression()))\nmodels.append((\"NB\",GaussianNB()))\nmodels.append((\"KNN\",KNeighborsClassifier(n_neighbors=5)))\nmodels.append((\"DT\",DecisionTreeClassifier()))\nmodels.append((\"SVM\",SVC()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('RF',RandomForestClassifier(n_estimators=13)))\nmodels\n#Results for ML Models\n\n\nfor name, model in models:\n    \n    clf=model\n\n    clf.fit(x_train, y_train)\n\n    y_pred =clf.predict(x_test)\n    print(10*\"=\",\"{} için Sonuçlar\".format(name).upper(),10*\"=\")\n    print(\"Accuracy Score:{:0.2f}\".format(accuracy_score(y_test, y_pred)))\n    print(\"Confusion Matrix:\\n{}\".format(confusion_matrix(y_test, y_pred)))\n    print(\"Classification Report:\\n{}\".format(classification_report(y_test,y_pred)))\n    print(30*\"=\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Logistic Regression Accuracy:\", 100*lr.score(x_test, y_test), \"%\")\nprint(\"KNN Prediction Accuracy:\", 100*knn.score(x_test, y_test), \"%\")\nprint(\"SVM Prediction Accuracy:\", 100*svc.score(x_test, y_test), \"%\")\nprint(\"Naive Bayes Prediction Accuracy:\", 100*nb.score(x_test, y_test), \"%\")\nprint(\"Decision Trees Prediction Accuracy:\", 100*dtc.score(x_test, y_test), \"%\")\nprint(\"Random Forest Prediction Accuracy:\", 100*rfc.score(x_test, y_test), \"%\")\nprint('Linear Disc Analysis Accuracy:',100*linr.score(x_test,y_test),'%')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion<a id = \"13\"></a><br>\n* In conclusion Random Forest Classifier is the best tool for Heart Disease UCI data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Unsupervised Learning Work<a id = \"14\"></a><br>\nTo practice Unsupervised Learning I will drop target column and try to apply Unsupervised Learning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dfu_data = df.loc[:, df.columns != 'target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfu_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#normalize data\ndfu = (dfu_data-np.min(dfu_data))/(np.max(dfu_data)-np.min(dfu_data))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Kmeans Clustering\nfrom sklearn.cluster import KMeans\ninertia_list = np.empty(10)\nfor i in range(1,10):\n    kmeans = KMeans(n_clusters = i)\n    kmeans.fit_predict(dfu)\n    inertia_list[i]= kmeans.inertia_\nplt.plot(range(0,10),inertia_list,'-o',c='r')\nplt.xlabel('# of clusters')\nplt.ylabel('Inertia')\nplt.grid(True)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#graph suggests Cluster number to be 2, as we know from the labeled data there were 2 labels\nkmeans2= KMeans(n_clusters=2)\n\n\nlabels = kmeans2.fit_predict(dfu)\ncheckdata = pd.DataFrame({'labels': labels,'target':df.target})\nct = pd.crosstab(checkdata.labels,checkdata.target)\nct\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Standartization and making pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nscalar = StandardScaler()\npipe = make_pipeline(scalar,kmeans2)\npipe.fit(dfu)\nlabels = pipe.predict(dfu)\npipedf = pd.DataFrame({'labels':labels,'target':df.target})\nct = pd.crosstab(pipedf.labels,pipedf.target)\nct\n\n#Hierarchical Clustering\nfrom scipy.cluster.hierarchy import linkage,dendrogram\nmerg = linkage(dfu,method='ward')\ndendrogram(merg,leaf_rotation=90)\nplt.show","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}