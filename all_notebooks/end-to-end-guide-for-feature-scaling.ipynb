{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#5642C5;\n           font-size:33px;\n           font-family:Nexa;\n           letter-spacing:0.5px\">\n        <p style=\"padding: 15px;\n              color:white;\">\n            <b>Feature Scaling</b>\n        </p>\n</div>","metadata":{"papermill":{"duration":0.019894,"end_time":"2021-08-06T01:24:47.130282","exception":false,"start_time":"2021-08-06T01:24:47.110388","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:16px; font-family:nexa;\">\n    Feature Engineering is a big part of Data Science and Machine Learning. Feature Scaling is one of the last steps in the whole life cycle of Feature Engineering. It is a technique to standardize the independent features in a data in a fixed range or scale. Thus the name Feature Scaling.<br>\n    In simple words, once we are done with all the other steps of feature engineering, like ecoding variables, handling missing values etc, then we scale all the variable to a very small range of say -1 to +1. So all the data gets squeezed to decimal points between -1 and +1. What it does is keep the distribution of the data, the correlation and covariance absolutely the same however scales every independent or the feature matrix columns to a smaller scale. We do this as most of the ML algorithms problems perform significantly better after scaling. \n</div>","metadata":{"papermill":{"duration":0.01825,"end_time":"2021-08-06T01:24:47.170202","exception":false,"start_time":"2021-08-06T01:24:47.151952","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"![Art of Scaling](https://webwisemedia.co.uk/wp-content/uploads/The-Art-of-Scaling.jpg)","metadata":{"papermill":{"duration":0.017892,"end_time":"2021-08-06T01:24:47.206747","exception":false,"start_time":"2021-08-06T01:24:47.188855","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"***\n### Types of Feature Scaling:\n   * <b>Standardization:</b>\n       - Standard Scaler\n   * <b>Normalization:</b>\n       - Min Max Scaling\n       - Mean Normalization\n       - Max Absolute Scaling\n       - Robust Scaling *etc.*\n***","metadata":{"papermill":{"duration":0.018006,"end_time":"2021-08-06T01:24:47.242966","exception":false,"start_time":"2021-08-06T01:24:47.22496","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#5F1666;\n           font-size:33px;\n           font-family:Nexa;\n           letter-spacing:0.5px\">\n        <p style=\"padding: 15px;\n              color:white;\">\n            <b>Standardization</b>\n        </p>\n</div>\n<div class=\"alert alert-block alert-info\" style=\"font-size:16px; font-family:nexa;\">\n   Standardization is a scaling technique where the values are centered around the mean with a unit standard deviation. This means that the mean of the attribute becomes zero and the resultant distribution has a unit standard deviation.\n</div>","metadata":{"papermill":{"duration":0.017912,"end_time":"2021-08-06T01:24:47.279056","exception":false,"start_time":"2021-08-06T01:24:47.261144","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Formula of Standardization:","metadata":{"papermill":{"duration":0.018975,"end_time":"2021-08-06T01:24:47.316321","exception":false,"start_time":"2021-08-06T01:24:47.297346","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"![formula of standardization](https://toptipbio.com/wp-content/uploads/2018/07/Z-score-formula.jpg)","metadata":{"papermill":{"duration":0.018054,"end_time":"2021-08-06T01:24:47.352546","exception":false,"start_time":"2021-08-06T01:24:47.334492","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"font-size:16px; \n            display:fill;\n            border-radius:5px;\n            font-family:Nexa; \n            line-height: 1.7em;\n            background-color:#FFD0D2\">\n    <p style=\"padding: 8px;\n              color:black;\">\n    We will now implement this and see the results for ourselves\n</div>","metadata":{"papermill":{"duration":0.017851,"end_time":"2021-08-06T01:24:47.388613","exception":false,"start_time":"2021-08-06T01:24:47.370762","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_theme(palette=\"rainbow\", style=\"darkgrid\")\n\n%matplotlib inline","metadata":{"papermill":{"duration":1.001506,"end_time":"2021-08-06T01:24:48.408327","exception":false,"start_time":"2021-08-06T01:24:47.406821","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-10T03:49:11.98551Z","iopub.execute_input":"2021-08-10T03:49:11.985885Z","iopub.status.idle":"2021-08-10T03:49:11.994176Z","shell.execute_reply.started":"2021-08-10T03:49:11.985855Z","shell.execute_reply":"2021-08-10T03:49:11.993133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/titanic-train-dataset/train.csv\", usecols=[\"Age\", \"Fare\"])\ndf[\"Fare\"].fillna(value=df[\"Fare\"].mean(), inplace=True)","metadata":{"papermill":{"duration":0.044065,"end_time":"2021-08-06T01:24:48.471969","exception":false,"start_time":"2021-08-06T01:24:48.427904","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-10T03:49:11.998915Z","iopub.execute_input":"2021-08-10T03:49:11.999421Z","iopub.status.idle":"2021-08-10T03:49:12.034442Z","shell.execute_reply.started":"2021-08-10T03:49:11.999387Z","shell.execute_reply":"2021-08-10T03:49:12.033442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# importing sklearn StandardScaler class which is for Standardization\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler() # creating an instance of the class object\ndf_new = pd.DataFrame(sc.fit_transform(df), columns=df.columns)  #fit and transforming StandardScaler the dataframe ","metadata":{"papermill":{"duration":0.16141,"end_time":"2021-08-06T01:24:48.65219","exception":false,"start_time":"2021-08-06T01:24:48.49078","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-10T03:49:12.035891Z","iopub.execute_input":"2021-08-10T03:49:12.036232Z","iopub.status.idle":"2021-08-10T03:49:12.181374Z","shell.execute_reply.started":"2021-08-10T03:49:12.036201Z","shell.execute_reply":"2021-08-10T03:49:12.180308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting the scatterplot of before and after Standardization\nplt.figure(figsize=(14,7))\nplt.subplot(1,2,1)\nplt.title(\"Scatterplot Before Standardization\", fontsize=18)\nsns.scatterplot(data = df, color=\"blue\")\nplt.subplot(1,2,2)\nplt.title(\"Scatterplot After Standardization\", fontsize=18)\nsns.scatterplot(data = df_new, color=\"red\")\nplt.tight_layout()\nplt.show()","metadata":{"papermill":{"duration":0.684879,"end_time":"2021-08-06T01:24:49.356626","exception":false,"start_time":"2021-08-06T01:24:48.671747","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-10T03:49:12.183175Z","iopub.execute_input":"2021-08-10T03:49:12.183454Z","iopub.status.idle":"2021-08-10T03:49:12.914677Z","shell.execute_reply.started":"2021-08-10T03:49:12.183428Z","shell.execute_reply":"2021-08-10T03:49:12.913903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting the scatterplot of before and after Standardization\nplt.figure(figsize=(14,7))\nplt.subplot(1,2,1)\nplt.title(\"PDF Before Standardization\", fontsize=18)\nsns.kdeplot(data = df, color=\"blue\")\nplt.subplot(1,2,2)\nplt.title(\"PDF After Standardization\", fontsize=18)\nsns.kdeplot(data = df_new, color=\"red\")\nplt.tight_layout()\nplt.show()","metadata":{"papermill":{"duration":0.742282,"end_time":"2021-08-06T01:24:50.123786","exception":false,"start_time":"2021-08-06T01:24:49.381504","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-10T03:49:12.915941Z","iopub.execute_input":"2021-08-10T03:49:12.916346Z","iopub.status.idle":"2021-08-10T03:49:13.639639Z","shell.execute_reply.started":"2021-08-10T03:49:12.916304Z","shell.execute_reply":"2021-08-10T03:49:13.638645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:16px; font-family:nexa;\">\n    Notice how the mean of the distribution is very close to 0 and stadard deviation is exactly 1. This is what Standardization does. We had some outliers in Fare and that's why mean didn't reduce down to 0.<br>\n    Also notice that how in the Scatterplot, the scale changed and the distribution came to the centre or 0. <br>\n    In the probability density function, the kde plot is exactly the same, so this shows how the distribution is not effected by standardization.\n</div>","metadata":{"papermill":{"duration":0.025654,"end_time":"2021-08-06T01:24:50.17545","exception":false,"start_time":"2021-08-06T01:24:50.149796","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#5F1666;\n           font-size:33px;\n           font-family:Nexa;\n           letter-spacing:0.5px\">\n        <p style=\"padding: 15px;\n              color:white;\">\n            <b>Normalization</b>\n        </p>\n</div>\n<div class=\"alert alert-block alert-info\" style=\"font-size:16px; font-family:nexa;\">\n    Normalization is a technique often applied as part of data preparation for machine learning. The goal of normalization is to change the values of numeric columns in the dataset to use a common scale, without distorting differences in the ranges of values or losing information.\n</div>","metadata":{"papermill":{"duration":0.025367,"end_time":"2021-08-06T01:24:50.22688","exception":false,"start_time":"2021-08-06T01:24:50.201513","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#26785F;\n           font-size:18px;\n           font-family:Nexa;\n           letter-spacing:0.5px\">\n        <p style=\"padding: 8px;\n              color:white;\">\n            <b>1 . Min Max Scaling</b>\n        </p>\n</div>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#A9C9A9;\n           font-size:16px;\n           font-family:Nexa;\n           letter-spacing:0.5px\">\n        <p style=\"padding: 8px;\n              color:black;\">\n            Min-max normalization is one of the most common ways to normalize data. For every feature, the minimum value of that feature gets transformed into a 0, the maximum value gets transformed into a 1, and every other value gets transformed into a decimal between 0 and 1.\n        </p>\n</div>","metadata":{"papermill":{"duration":0.026121,"end_time":"2021-08-06T01:24:50.279243","exception":false,"start_time":"2021-08-06T01:24:50.253122","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Formula of Min Max Scaling:","metadata":{"papermill":{"duration":0.038205,"end_time":"2021-08-06T01:24:50.352227","exception":false,"start_time":"2021-08-06T01:24:50.314022","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<img src=\"https://androidkt.com/wp-content/uploads/2020/10/Selection_060.png\" alt=\"min max formula\" width=\"500\" height=\"600\">","metadata":{"papermill":{"duration":0.028775,"end_time":"2021-08-06T01:24:50.412259","exception":false,"start_time":"2021-08-06T01:24:50.383484","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# importing sklearn Min Max Scaler class which is for Standardization\nfrom sklearn.preprocessing import MinMaxScaler\n\nmm = MinMaxScaler() # creating an instance of the class object\ndf_new_mm = pd.DataFrame(mm.fit_transform(df), columns=df.columns)  #fit and transforming MinMaxScaler the dataframe ","metadata":{"papermill":{"duration":0.038045,"end_time":"2021-08-06T01:24:50.477349","exception":false,"start_time":"2021-08-06T01:24:50.439304","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-10T03:49:13.640872Z","iopub.execute_input":"2021-08-10T03:49:13.641196Z","iopub.status.idle":"2021-08-10T03:49:13.651579Z","shell.execute_reply.started":"2021-08-10T03:49:13.641168Z","shell.execute_reply":"2021-08-10T03:49:13.650286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting the scatterplot of before and after Min Max Scaling\nplt.figure(figsize=(14,7))\nplt.subplot(1,2,1)\nplt.title(\"Scatterplot Before Min Max Scaling\", fontsize=18)\nsns.scatterplot(data = df, color=\"blue\")\nplt.subplot(1,2,2)\nplt.title(\"Scatterplot After Min Max Scaling\", fontsize=18)\nsns.scatterplot(data = df_new_mm, color=\"red\")\nplt.tight_layout()\nplt.show()","metadata":{"papermill":{"duration":0.608302,"end_time":"2021-08-06T01:24:51.111608","exception":false,"start_time":"2021-08-06T01:24:50.503306","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-10T03:49:13.653561Z","iopub.execute_input":"2021-08-10T03:49:13.654023Z","iopub.status.idle":"2021-08-10T03:49:14.315877Z","shell.execute_reply.started":"2021-08-10T03:49:13.653989Z","shell.execute_reply":"2021-08-10T03:49:14.314824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting the scatterplot of before and after Min Max Scaling\nplt.figure(figsize=(14,7))\nplt.subplot(1,2,1)\nplt.title(\"PDF Before Min Max Scaling\", fontsize=18)\nsns.kdeplot(data = df, color=\"blue\")\nplt.subplot(1,2,2)\nplt.title(\"PDF After Min Max Scaling\", fontsize=18)\nsns.kdeplot(data = df_new_mm, color=\"red\")\nplt.tight_layout()\nplt.show()","metadata":{"papermill":{"duration":0.669898,"end_time":"2021-08-06T01:24:51.814288","exception":false,"start_time":"2021-08-06T01:24:51.14439","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-10T03:49:14.317331Z","iopub.execute_input":"2021-08-10T03:49:14.317859Z","iopub.status.idle":"2021-08-10T03:49:15.010056Z","shell.execute_reply.started":"2021-08-10T03:49:14.317818Z","shell.execute_reply":"2021-08-10T03:49:15.009278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:16px; font-family:nexa;\">\n    Min Max Normalization will perform best when the maximum and minimum value is very distinct and known.\n</div>","metadata":{"papermill":{"duration":0.034128,"end_time":"2021-08-06T01:24:51.883088","exception":false,"start_time":"2021-08-06T01:24:51.84896","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#2A3162;\n           font-size:18px;\n           font-family:Nexa;\n           letter-spacing:0.5px\">\n        <p style=\"padding: 8px;\n              color:white;\">\n            <b>2 . Max Absolute Scaling</b>\n        </p>\n</div>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#D8E4FF;\n           font-size:16px;\n           font-family:Nexa;\n           letter-spacing:0.5px\">\n        <p style=\"padding: 8px;\n              color:black;\">\n            Scale each feature by its maximum absolute value.\nThis estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity.\nThis scaler can also be applied to sparse CSR or CSC matrices.\n        </p>\n</div>","metadata":{"papermill":{"duration":0.034049,"end_time":"2021-08-06T01:24:51.951681","exception":false,"start_time":"2021-08-06T01:24:51.917632","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Formula of Max Absolute Scaling:","metadata":{"papermill":{"duration":0.034211,"end_time":"2021-08-06T01:24:52.021191","exception":false,"start_time":"2021-08-06T01:24:51.98698","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<img src=\"https://www.oreilly.com/library/view/python-feature-engineering/9781789806311/assets/d35ab4dd-7642-4e76-8cbf-cdc37a7fb4d8.png\" alt=\"max abs formula\" width=\"500\" height=\"600\">","metadata":{"papermill":{"duration":0.035024,"end_time":"2021-08-06T01:24:52.091064","exception":false,"start_time":"2021-08-06T01:24:52.05604","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# importing sklearn Min Max Scaler class which is for Max Absolute Scaling\nfrom sklearn.preprocessing import MaxAbsScaler\n\nma = MaxAbsScaler() # creating an instance of the class object\ndf_new_ma = pd.DataFrame(ma.fit_transform(df), columns=df.columns)  #fit and transforming Max Absolute Scaling the dataframe ","metadata":{"papermill":{"duration":0.045916,"end_time":"2021-08-06T01:24:52.171896","exception":false,"start_time":"2021-08-06T01:24:52.12598","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-10T03:49:15.01183Z","iopub.execute_input":"2021-08-10T03:49:15.01227Z","iopub.status.idle":"2021-08-10T03:49:15.020811Z","shell.execute_reply.started":"2021-08-10T03:49:15.012238Z","shell.execute_reply":"2021-08-10T03:49:15.019814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting the scatterplot of before and after Max Absolute Scaling\nplt.figure(figsize=(14,7))\nplt.subplot(1,2,1)\nplt.title(\"Scatterplot Before Max Absolute Scaling\", fontsize=18)\nsns.scatterplot(data = df, color=\"blue\")\nplt.subplot(1,2,2)\nplt.title(\"Scatterplot After Max Absolute Scaling\", fontsize=18)\nsns.scatterplot(data = df_new_ma, color=\"red\")\nplt.tight_layout()\nplt.show()","metadata":{"papermill":{"duration":0.606823,"end_time":"2021-08-06T01:24:52.813495","exception":false,"start_time":"2021-08-06T01:24:52.206672","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-10T03:49:15.022149Z","iopub.execute_input":"2021-08-10T03:49:15.022635Z","iopub.status.idle":"2021-08-10T03:49:15.758858Z","shell.execute_reply.started":"2021-08-10T03:49:15.022602Z","shell.execute_reply":"2021-08-10T03:49:15.758149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting the scatterplot of before and after max Absolute Scaling\nplt.figure(figsize=(14,7))\nplt.subplot(1,2,1)\nplt.title(\"PDF Before Max Absolute Scaling\", fontsize=18)\nsns.kdeplot(data = df, color=\"blue\")\nplt.subplot(1,2,2)\nplt.title(\"PDF After Max Absolute Scaling\", fontsize=18)\nsns.kdeplot(data = df_new_ma, color=\"red\")\nplt.tight_layout()\nplt.show()","metadata":{"papermill":{"duration":0.557761,"end_time":"2021-08-06T01:24:53.412409","exception":false,"start_time":"2021-08-06T01:24:52.854648","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-10T03:49:15.759833Z","iopub.execute_input":"2021-08-10T03:49:15.760221Z","iopub.status.idle":"2021-08-10T03:49:16.381022Z","shell.execute_reply.started":"2021-08-10T03:49:15.760194Z","shell.execute_reply":"2021-08-10T03:49:16.380303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:16px; font-family:nexa;\">\n    Max Absolute scaling will perform a lot better in sparse data or when most of the values are 0. \n</div>","metadata":{"papermill":{"duration":0.042531,"end_time":"2021-08-06T01:24:53.498043","exception":false,"start_time":"2021-08-06T01:24:53.455512","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#4E3626;\n           font-size:18px;\n           font-family:Nexa;\n           letter-spacing:0.5px\">\n        <p style=\"padding: 8px;\n              color:white;\">\n            <b>1 . Robust Scaling</b>\n        </p>\n</div>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#EEC3B3;\n           font-size:16px;\n           font-family:Nexa;\n           letter-spacing:0.5px\">\n        <p style=\"padding: 8px;\n              color:black;\">\n           This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile).\n        </p>\n</div>\n","metadata":{"papermill":{"duration":0.042735,"end_time":"2021-08-06T01:24:53.583722","exception":false,"start_time":"2021-08-06T01:24:53.540987","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Formula of Robust Scaling:","metadata":{"papermill":{"duration":0.042304,"end_time":"2021-08-06T01:24:53.668853","exception":false,"start_time":"2021-08-06T01:24:53.626549","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<img src=\"https://miro.medium.com/max/1008/1*N0cPiRvreurI_rKU55g1Eg.png\"  alt=\"max abs formula\" width=\"500\" height=\"600\">","metadata":{"papermill":{"duration":0.041842,"end_time":"2021-08-06T01:24:53.752745","exception":false,"start_time":"2021-08-06T01:24:53.710903","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# importing sklearn Min Max Scaler class which is for Robust scaling\nfrom sklearn.preprocessing import RobustScaler\n\nrs = RobustScaler() # creating an instance of the class object\ndf_new_rs = pd.DataFrame(rs.fit_transform(df), columns=df.columns)  #fit and transforming Robust Scaling the dataframe ","metadata":{"papermill":{"duration":0.057096,"end_time":"2021-08-06T01:24:53.851918","exception":false,"start_time":"2021-08-06T01:24:53.794822","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-10T03:49:16.381998Z","iopub.execute_input":"2021-08-10T03:49:16.382382Z","iopub.status.idle":"2021-08-10T03:49:16.394661Z","shell.execute_reply.started":"2021-08-10T03:49:16.382355Z","shell.execute_reply":"2021-08-10T03:49:16.393539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting the scatterplot of before and after Robust Scaling\nplt.figure(figsize=(14,7))\nplt.subplot(1,2,1)\nplt.title(\"Scatterplot Before Robust Scaling\", fontsize=18)\nsns.scatterplot(data = df, color=\"blue\")\nplt.subplot(1,2,2)\nplt.title(\"Scatterplot After Robust Scaling\", fontsize=18)\nsns.scatterplot(data = df_new_rs, color=\"red\")\nplt.tight_layout()\nplt.show()","metadata":{"papermill":{"duration":0.602521,"end_time":"2021-08-06T01:24:54.497309","exception":false,"start_time":"2021-08-06T01:24:53.894788","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-10T03:49:16.395829Z","iopub.execute_input":"2021-08-10T03:49:16.396163Z","iopub.status.idle":"2021-08-10T03:49:17.084563Z","shell.execute_reply.started":"2021-08-10T03:49:16.396121Z","shell.execute_reply":"2021-08-10T03:49:17.083673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting the scatterplot of before and after Robust Scaling\nplt.figure(figsize=(14,7))\nplt.subplot(1,2,1)\nplt.title(\"PDF Before Robust Scaling\", fontsize=18)\nsns.kdeplot(data = df, color=\"blue\")\nplt.subplot(1,2,2)\nplt.title(\"PDF After Robust Scaling\", fontsize=18)\nsns.kdeplot(data = df_new_rs, color=\"red\")\nplt.tight_layout()\nplt.show()","metadata":{"papermill":{"duration":0.529986,"end_time":"2021-08-06T01:24:55.075103","exception":false,"start_time":"2021-08-06T01:24:54.545117","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-10T03:49:17.085816Z","iopub.execute_input":"2021-08-10T03:49:17.086103Z","iopub.status.idle":"2021-08-10T03:49:17.667521Z","shell.execute_reply.started":"2021-08-10T03:49:17.086077Z","shell.execute_reply":"2021-08-10T03:49:17.666602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:16px; font-family:nexa;\">\n    Robust Scaling is best for data that has outliers\n</div>","metadata":{"papermill":{"duration":0.049813,"end_time":"2021-08-06T01:24:55.175585","exception":false,"start_time":"2021-08-06T01:24:55.125772","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#58596D;\n           font-size:18px;\n           font-family:Nexa;\n           letter-spacing:0.5px\">\n        <p style=\"padding: 8px;\n              color:white;\">\n            <b>1 . Mean Normalization</b>\n        </p>\n</div>\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#D0D6EB;\n           font-size:16px;\n           font-family:Nexa;\n           letter-spacing:0.5px\">\n        <p style=\"padding: 8px;\n              color:black;\">\n            It is very similar to Min Max Scaling, just that we use mean to normalize the data. Removes the mean from the data and scales it into max and min values.\n        </p>\n</div>","metadata":{"papermill":{"duration":0.050698,"end_time":"2021-08-06T01:24:55.276454","exception":false,"start_time":"2021-08-06T01:24:55.225756","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Formula of Mean Normalization:","metadata":{"papermill":{"duration":0.05041,"end_time":"2021-08-06T01:24:55.377053","exception":false,"start_time":"2021-08-06T01:24:55.326643","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<img src=\"https://miro.medium.com/max/920/1*TKTgR3jJNsDd77tZ-fOhxg.png\" alt=\"max abs formula\" width=\"500\" height=\"600\">","metadata":{"papermill":{"duration":0.050006,"end_time":"2021-08-06T01:24:55.477642","exception":false,"start_time":"2021-08-06T01:24:55.427636","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:16px; font-family:nexa;\">\n    Scikitlearn does not have any specific class for mean normalization. However, you can do this very easily using numpy.<br><br>\n    <b>YOUR TASK:</b><br>\n    Go ahead and try to code mean normalization using any dataset. Share the code in comments for other to see and discuss. <br> \n    Try the other ones as well that I have shared above. \n</div>","metadata":{"papermill":{"duration":0.050111,"end_time":"2021-08-06T01:24:55.578039","exception":false,"start_time":"2021-08-06T01:24:55.527928","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#5642C5;\n           font-size:33px;\n           font-family:Nexa;\n           letter-spacing:0.5px\">\n        <p style=\"padding: 15px;\n              color:white;\">\n            <b>Conclusion and Tips!</b>\n        </p>\n</div>\n<div class=\"alert alert-block alert-info\" style=\"font-size:16px; font-family:nexa;\">\n    Don't get limited by your understanding while implementing these scalers. What I personally follow is trail and error method. In this world of ever changing tech, it is difficult to comprehend the mathematical nuiances of every bit of code that you write. It might be possible that you forget a certain formula a certain point of time. That should not effect your intuition though. That is the whole point of trail and error. Play with a single dataset and apply every scaler that I have shown you today. See the results for yourself. Go ahead and try it on other 5/6 datasets. Check the results and redo the models till the accuracy is good. This way you will have a very intuitive understanding of why a scaler performs better than the others in a very specific scenario. This intuition is hard to develop. <br><br>\n    <b>Below I will share some basic tips that you can use while trying to scale:</b><br>\n    1. If you do not know which scaler to use, apply all and check the effect on the models. <br> \n    2. If you do not understand the data, use standard scaler. It works most of the times. <br> \n    3. If you know the max and min values of the feature, then use min max scaler. Like in CNN. <br> \n    4. If most of the values in the feature column is 0 or sparce matrix, then use Max Absolute Scaling<br> \n    5. If the data has outliers, use Robust Scaling.<br> <br> <br> \n    <b>Thank you so much for reading till the end. You are a hero. Keep learning!<br>\n        Also, if you liked the notebook, an upvote will definitely keep my motivation up!\n</div>","metadata":{"papermill":{"duration":0.050023,"end_time":"2021-08-06T01:24:55.678332","exception":false,"start_time":"2021-08-06T01:24:55.628309","status":"completed"},"tags":[]}}]}