{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Ridge Regression\n\nThe aim is to find the coefficients that minimize the error sum of squares, by applying a penalty to these coefficients.\n\n<img src=\"https://datavedas.com/wp-content/uploads/2018/04/image001-1.png\" />\n\nThe first y value in the formula is the real values and the second y value is the predicted value. After this equation is opened and betas are written in my place and solved, what remains are the coefficients.\n\n- It is resistant to over learning.\n- It is biased but its variance is low.\n- Better than OLS when there are too many parameters.\n- Offers a solution to the problem of multidimensionality.\n- Effective when there is a problem of multiple linear connections. Multiple linear connection problem; It means that there is a high correlation between independent variables. In other words, it carries the same information that a variable carries in another variable.\n- Builds a model with all variables. It does not remove unrelated variables from the model, it brings their coefficients closer to zero.\n- λ is in the critical model. It allows to control the relative effects of two terms (in the formula).\n- It is important to find a good value for λ. For this, the CV method is used.\n\n<img src=\"https://i.ibb.co/2qMjXG8/Untitled.png\" />\n\n- The value in the left part of the formula is the classical recession.\n- where λ is zero is in OLS.\n- A set containing certain values ​​for λ is selected and the cross validation test error is calculated for each.\n- The λ which gives the smallest cross validation is chosen as the setting parameter.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# import the necessary packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection\nfrom scipy.stats import boxcox\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import RidgeCV","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# load data\ndata = \"../input/insurance/insurance.csv\"\ndf = pd.read_csv(data)\n\n# show data (6 row)\ndf.head(6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ridge Regression Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_encode = pd.get_dummies(data = df, columns = ['sex','smoker','region'])\ndf_encode.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalization\ny_bc,lam, ci= boxcox(df_encode['charges'],alpha=0.05)\ndf_encode['charges'] = np.log(df_encode['charges'])\n\ndf_encode.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_encode.drop(\"charges\",axis=1)\ny = df_encode[\"charges\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nridge_model = Ridge(alpha=0.1).fit(X_train, y_train)\nridge_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_model.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_model.intercept_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lambdas = 10**np.linspace(10,-2,100)*0.5 # Creates random numbers\nridge_model =  Ridge()\ncoefs = []\n\nfor i in lambdas:\n    ridge_model.set_params(alpha=i)\n    ridge_model.fit(X_train,y_train)\n    coefs.append(ridge_model.coef_)\n    \nax = plt.gca()\nax.plot(lambdas, coefs)\nax.set_xscale(\"log\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In contrast to the different beta values, the changes in the coefficients of the variables in our data set appear in the graph above. As can be seen, as the coefficients increase, it approaches zero.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Ridge Regression - Prediction\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_model = Ridge().fit(X_train,y_train)\n\ny_pred = ridge_model.predict(X_train)\n\nprint(\"Predict: \", y_pred[0:10])\nprint(\"Real: \", y_train[0:10].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RMSE = np.mean(mean_squared_error(y_train,y_pred)) # rmse = square root of the mean of error squares\nprint(\"train error: \", RMSE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Verified_RMSE = np.sqrt(np.mean(-cross_val_score(ridge_model, X_train, y_train, cv=20, scoring=\"neg_mean_squared_error\")))\nprint(\"Verified_RMSE: \", Verified_RMSE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are two values above. One of them is unverified, the other is the values ​​that represent the square root of the sum of the verified error squares. As you can see, the unverified value is almost half of the verified value. This result shows us that it is more correct to use the second method, not the first method, while taking the square root of the mean of the error squares.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Model Tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_model = Ridge(10).fit(X_train,y_train)\ny_pred = ridge_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_model = Ridge(30).fit(X_train,y_train)\ny_pred = ridge_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_model = Ridge(90).fit(X_train,y_train)\ny_pred = ridge_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can find out which value will work better by trial and error. But with the method we will use below, we can find the most appropriate value more easily and quickly.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lambdas1 = 10**np.linspace(10,-2,100)\nlambdas2 = np.random.randint(0,10000,100)\n\nridgeCV = RidgeCV(alphas = lambdas1,scoring = \"neg_mean_squared_error\", cv=10, normalize=True)\nridgeCV.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can use alpha_ feature to attract the most appropriate value.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ridgeCV.alpha_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# final model\nridge_tuned = Ridge(alpha = ridgeCV.alpha_).fit(X_train,y_train)\ny_pred = ridge_tuned.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred))*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for lambdas2\nridgeCV = RidgeCV(alphas = lambdas2,scoring = \"neg_mean_squared_error\", cv=10, normalize=True)\nridgeCV.fit(X_train,y_train)\nridge_tuned = Ridge(alpha = ridgeCV.alpha_).fit(X_train,y_train)\ny_pred = ridge_tuned.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred))*100","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}