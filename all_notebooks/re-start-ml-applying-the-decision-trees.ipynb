{"nbformat":4,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","version":"3.6.3","name":"python"}},"nbformat_minor":1,"cells":[{"source":"# Building Decision Trees for Classification","metadata":{"_uuid":"cf51e3bc1c9cc8fbaa33bf3e8be070c5b002300c","_cell_guid":"3f16bcdb-3a97-42fe-9b4e-cffec0a21ab5"},"cell_type":"markdown"},{"source":"We are going to discuss the process of building a decision tree classifier for the diabetes problem. The objective is to predict based on diagnostic measurements whether a patient has diabetes.\n\nLet us first import libraries that we are going to use during our work.","metadata":{"_uuid":"23ead179e9b5d50e85cee4081be9014da4eca437","_cell_guid":"35e17c07-218e-4f37-bb0a-3085dc7b2156"},"cell_type":"markdown"},{"source":"import numpy as np\nimport pandas as pd\nimport sklearn as sk\nfrom sklearn.model_selection import train_test_split","outputs":[],"metadata":{"_uuid":"4ec4584f9f16791431444c72b97ea566e18ab554","_cell_guid":"9d28de81-0b4e-4067-a5b1-358bae42290d","collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"We are building a model that is going to make predictions, so we need to find a way to evaluate the quality of these predictions in order to trust them. Since predictions by definition is for some unseen input, we cannot depend on the data that we used to create the model. We first need to divide the dataset into two non-intersecting parts: training data that is going to be used for building the model and test data for evaluating the model predictions.","metadata":{"_uuid":"5278ee30b8658de950dfb495e6d8c48aca071189","_cell_guid":"d572547e-c7f9-4ca5-af44-7b3d51a1f1b3"},"cell_type":"markdown"},{"source":"dataset = pd.read_csv('../input/diabetes.csv')\nX = dataset[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',\n'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']]\nY = dataset[['Outcome']]\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\nY_train.describe()","outputs":[],"metadata":{"_uuid":"4c6475f23c0f60fa33737f070f10e061c45d928d","_cell_guid":"4fb1e0b6-5755-45bc-8cc4-de9380866a16","collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"Y_test.describe()","outputs":[],"metadata":{"_uuid":"40bc8523cc6f6cbe466fa6a25fcbcde65e819cba","_cell_guid":"ff8d4a72-14d2-4364-a359-30be2758482c","collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"We are ready now to build our first classifier. We use the training data to build our decision tree model. Then we are going to evaluate its score using the test set. ","metadata":{"_uuid":"9a54f0641907fa5579eb5d20da06fbae81917681","_cell_guid":"3bdb494b-eaf3-47f8-8d7f-66d01ee7fb63"},"cell_type":"markdown"},{"source":"from sklearn.tree import DecisionTreeClassifier\n\n# Create the classifier\ndecision_tree_classifier = DecisionTreeClassifier(random_state = 0)\n\n# Train the classifier on the training set\ndecision_tree_classifier.fit(X_train, Y_train)\n\n# Evaluate the classifier on the testing set using classification accuracy\ndecision_tree_classifier.score(X_test, Y_test)","outputs":[],"metadata":{"_uuid":"1c372a4693a8c684bcb5b61ecd2aa7d3b0855a23","_cell_guid":"96a358fd-8d3d-4612-8de3-6c99a5e2ea71","collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"Congratualtions! We got around 77% accuracy on our first classifier. Let us first visualize the decision tree built.","metadata":{"_uuid":"bb83ef41fe43e4e36891e11f6af99af3dd57c61e","_cell_guid":"b3ce72b3-ad3b-451f-8da9-161538d1795a"},"cell_type":"markdown"},{"source":"from sklearn import tree\n\ndot_file = tree.export_graphviz(decision_tree_classifier, out_file='tree_a1.dot', \n                                feature_names = list(dataset)[0:-1],\n                                class_names = ['healthy', 'ill']) \n","outputs":[],"metadata":{"_uuid":"a2f220fed5e5f4181ad564da93e0fc60dbd8d506","_cell_guid":"531f0a4d-4d6f-46e6-9d0e-11ddbd71e9f1","collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"We noticed that the decision tree built is very deep and too complicated. This indicates that the model \nwill not be able to generalize well. This phenomenon is called overfitting. Mainly, the model memorizes\nthe training data and would have high accuracy on the training data but will perform badly on unseen ones.","metadata":{"_uuid":"422fb933f4971d3c2ec12b99f9e1a16589e056a7","_cell_guid":"0226a4a3-edd0-43db-bbc4-7d6797253283"},"cell_type":"markdown"},{"source":"print(\"Accuracy on training set: {:.3f}\".format(decision_tree_classifier.score(X_train, Y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(decision_tree_classifier.score(X_test, Y_test)))","outputs":[],"metadata":{"_uuid":"440bc7ed3a05a5767658f9ecc56e9d4632c266a4","_cell_guid":"5e8c1215-c5f5-4375-bee7-2ed163ceb800","collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"import graphviz\nwith open(\"tree_a1.dot\") as f:\n    dot_graph = f.read()\ngraphviz.Source(dot_graph)","outputs":[],"metadata":{"_uuid":"cb6a766ce2cdee71da3fe8cd93e16fffcb25300d","_cell_guid":"240575b9-2158-4f8f-9d2d-8f12f5480407","collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"To avoid overfitting, we can attempt to reduce the complexity of the model. This can be done during building the model\n(pre-pruning) or after building it (post-pruning). Sklearn provide built-in functions to control pre-pruning like\nlimiting the depth of the model. ","metadata":{"_uuid":"d2cb8618ec2937e95cd0a54b9837e0e0318965bd","_cell_guid":"4576047c-5b92-487d-a90f-5202031cf264"},"cell_type":"markdown"},{"source":"decision_tree_pruned = DecisionTreeClassifier(random_state = 0, max_depth = 2)\n\ndecision_tree_pruned.fit(X_train, Y_train)\ndecision_tree_pruned.score(X_test, Y_test)","outputs":[],"metadata":{"_uuid":"1d2264fa1fb9a92095be725663503cbd6ae19c98","_cell_guid":"d97bf5a3-c01c-4a4e-a470-6c8290a9296b","collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"pre_pruned_dot_file = tree.export_graphviz(decision_tree_pruned, out_file='tree_pruned.dot', \n                                feature_names = list(dataset)[0:-1],\n                                class_names = ['healthy', 'ill'])\nwith open(\"tree_pruned.dot\") as f:\n    dot_graph = f.read()\ngraphviz.Source(dot_graph)","outputs":[],"metadata":{"_uuid":"5b9b204e3d52401f693c4758cc96e4581331f189","_cell_guid":"574fd54b-effc-4c87-bea6-58b040219066","collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"What other parameters can be used for pre-pruning? Experiment with different parameters and check how the results vary.\n\nhint: consult http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html","metadata":{"_uuid":"272d811649e53b70736c45a91189b85e04f1e9b7","_cell_guid":"3d7e69c8-b854-4791-af73-d7609c3c67be"},"cell_type":"markdown"},{"source":"","outputs":[],"metadata":{"_uuid":"4c3e070a10cce1d0cc121cb254ffaf8af840dfdf","_cell_guid":"a7d612f0-293c-4ec4-8e90-285a96fab8ac","collapsed":true},"execution_count":null,"cell_type":"code"}]}