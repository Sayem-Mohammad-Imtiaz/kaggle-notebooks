{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Task\nCreate summary tables that address therapeutics, interventions, and clinical studies related to COVID-19. Specifically, the submission should focus on what the literature reports about:\n* What is the best method to combat the hypercoagulable state seen in COVID-19?\n* What is the efficacy of novel therapeutics being tested currently?\n\nWe describe our efforts in answering these two questions of this task. For each question, we create a csv file as our submission.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Approach Summary\nOur phase-1 submission for this task described CoNetz, a tool for text and network mining for COVID-19 research. The tool allows easy exploration and visualization of the COVID-19 corpus derived network for generation of leads. We now describe our phase-2 submission for this task, which uses a set of new techniques to specifically address the above two questions.\n\nOur fully automated phase-2 pipeline broadly consists of the following steps:\n1. **Corpus preprocessing (corpus augmentation, named entity annotation and cleansing)**:  \n  The CORD-19 corpus was augmented with additional COVID-19 related MEDLINE abstracts. Foreign (non-English) language articles were then removed. A lexicon (dictionary)-based NER was performed to annotate named entities in the articles. In particular, entities belonging to \"PHENOTYPE and SYMPTOM\" terms, \"CHEMICAL, DRUG and INTERVENTION\" terms, \"HYPERCOAGULATION\" terms and COVID-19 DISEASE terms were tagged. \n2. **Identification of relevant articles**:  \n Selection of relevant COVID-19 articles that are related to clinical intervention studies in the context of either COVID-19 therapeutics or handling hypercoagulable state.\n3. **Information extraction/prediction for populating the target fields**:  \n Applying automated information extraction and prediction on the final corpus based on various DL and ML models including the BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining) model and SVM models for populating the target fields in the csv output. \n4. **Post processing**:  \n Process the csv intermediate output from the previous step for handling duplicates, missing fields, noisy entries etc., and produce the final csv output.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Approach Details\n\nIn the following, we describe the above steps in detail. Implementation details including organization of supporting data/code and statistics of intermediate outputs are also discussed in the end (Additional Details Section). Most of the computational pipeline is identical for answering both the questions. Where ever applicable, we discuss the custom modifications that are applied to answer any of the specific questions. The pipeline is run on the latest CORD-19 corpus (uploaded on 9th June 2020). \n\n### 1. **Corpus preprocessing (corpus augmentation, named entity annotation and cleansing)**: \n* Using a JSON parser, the provided CORD-19 corpus was converted to a TSV file that consists of relevant fields.\n* The provided CORD-19 corpus is an excellent source of COVID-19 related articles. However, in order to improve the coverage, we also included additional COVID-19 related MEDLINE abstracts that are not present in the provided CORD-19 corpus. In order to do this, we searched for COVID-19 or any of its synonyms in the MEDLINE corpus and the compared their PubMed IDs with the PubMed IDs present in the CORD-19 corpus. \n* Foreign language (non-English) articles were removed from this augmented corpus. The \"FastText\" model based prediction was used for filtering out non-English articles. Please see the Additional Details section for code/data. The final filtered set of articles are saved in cord19_arts.tsv file.\n* We performed Named Entity Recognition (NER) on the filtered corpus to tag the entities of interest. Specifically, we tagged entities belonging to \"PHENOTYPE and SYMPTOM\" terms, \"CHEMICAL, DRUG and INTERVENTION\" terms, \"HYPERCOAGULATION\" terms and COVID-19 DISEASE terms. These entities are crucial for accurate extraction of relevant information.\n* For performing annotations, we used the TPX NER tool [1] which is an in-house lexicon (dictionary)-based NER module. The customized dictionaries consisted of a comprehensive set of terms belonging to the four entity categories. The NER module performs approximate string matching for tagging. It can also handle local abbreviations.\n* The input to the NER module is cord19_arts.tsv. The NER output consists of a) article level annotation capturing the tagged entities (annotations.inp) and b) corpus level aggregate data that captures pairwise co-occurrence and pairwise Pearson correlation between the tagged entities in the input corpus (pc_associations_master.txt). The output files are made available in the data folder. \n* The corpus preprocessing phase comprising of the above steps is not included in our Jupyter notebook. However, all the relevant code and the intermediate outputs are made available in the data/code folder (Details discussed in the Additional Details Section), with the exception of the NER module. The TPX NER code is not included in our submission code folder due to licensing constraints. In order to support future execution of our pipeline on newer articles, we would be providing a facility for web based remote execution of the NER module which takes a cord19_arts.tsv file as input and produces the two corresponding output files that feed into our next step.\n\n### 2. **Identification of relevant articles**: \n* In this step, the output files from the previous step are used to select the relevant articles for the subsequent information extraction steps. We apply multiple filters for relevant article identification.\n* The first filter uses entity annotations of the articles. An article qualifies this filter if the entity annotations in its title/abstract contain\n    * A COVID-19 (or its synonym) term \n    * at least one \"CHEMICAL, DRUG and INTERVENTION\" term\n    * at least one \"HYPERCOAGULATION\" term (only in the case of addressing the first question on hypercoagulable state).\n* The second filter is applied on the articles that qualify the first filter in order to identify articles that are related to clinical intervention studies. This is because, for both the questions in this task, the information extraction has to be performed from articles that are related to clinical intervention studies. The second filter applies a combination of pattern matching and  SVM based classification. An article that was classified as positive by either the pattern matching or the SVM classifier are included for further processing.\n    * In pattern matching, articles whose title/abstract has an occurrence of any of [\"patients\" or \"volunteers\" or \"participants\" or \"cases\" or \"COVID-19 case\" or \"cohort\" or [0-9]+ year old] are tagged as positive.\n    * A one-class SVM classifier (outlier detection) was trained on a positive training set of PubMed articles whose publication type metadata is any one of [Adaptive Clinical Trial, Case Reports, Clinical Conference, Clinical Study, Clinical Trial, Clinical Trial, Phase I, Clinical Trial, Phase II, Clinical Trial, Phase III, Clinical Trial, Phase IV, Clinical Trial Protocol, Controlled Clinical Trial, Pragmatic Clinical Trial and Randomized Controlled Trial]. A negative training set could further improve the quality by training a two-class SVM classifier. However, we have used a one-class classifier in this submission.\n* The output is a tsv file \"novel_th_ab.tsv\" (\"hgs_ab.tsv\" for the hypercoagulation question).  These files will be created in the Jupyter working folder.\n\n### 3. **Information extraction/prediction for populating the target fields**:\n* In this step, we apply information extraction and prediction on each article present in the output of the previous step. For this, we apply various DL and ML models including the BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining) model [2] and SVM models.\n* The target fields in the output are: Date | Study | Study Link | Journal | Study Type | Therapeutic method(s) utilized/assessed | Sample size| Severity of Symptoms | General Outcome/Conclusion Excerpt | Primary Endpoint(s) of Study | Clinical Improvement (Y/N)\n* The entity annotations of the articles are also used in conjunction with ML and DL models for populating the different fields.\n* **\"Date\", \"Study\", \"Study Link\" and \"Journal\" fields**: These are directly populated from the CORD-19 article metadata.\n* **\"Study Type\" field**:\n    * We use an SVM based classifier to populate this field. We train a multi-class SVM classifier using training data constructed from PubMed. For each class, we create approximately 2000 training articles by using related PubMed searches and related PubMed metadata. The training corpus, training code and the final trained SVM model are available in the data folder. \n* **\"Therapeutic method(s) utilized/assessed\" field**:\n    * We use pre-trained BioBERT model (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining) trained on Biomedical corpus to extract this field.\n    * In particular, we use the Question/Answer (Q&A) model of BioBERT. We frame multiple questions around Therapeutics such as \"What were the therapeutics used for treatment of covid-19?\" and scan the BioBERT returned text snippets from the article title/abstract. We then identify the snippet having the maximum prediction score by BioBERT.\n    * Subsequently, we then use the article annotation data and search for the presence of tagged therapeutics terms in the identified text snippet.\n    * If multiple tagged therapeutics are detected then we create separate rows in the output csv for each of these and extract the remaining fields for them separately.\n    * To address the question on handling hypercoagulation state, we use a set of modified questions such as \"What were the therapeutics used for treatment of Y ?\", where \"Y\" is the tagged HYPERCOAGULATION TERM present in the article abstract.\n* **\"Sample size\" field**:\n    * We use pre-trained BioBERT Q&A model along with multiple related questions followed by pattern matching to extract sample size.\n* **\"Severity of Symptoms\" field**:\n    * Here again, we use pre-trained BioBERT Q&A model along with multiple related questions followed by pattern matching to populate this field.\n* **\"General Outcome/Conclusion Excerpt\" field**: \n    * We use the article conclusion section in the case of full text articles to populate this field. For articles without full text, conclusions present in the article abstract are extracted using pattern matching. In the absence of a defined conclusion in the article abstract, we populate this field with the text snippet containing the tagged therapeutic term and COVID-19 mention.\n* **\"Primary Endpoint(s) of Study\" field**:\n    * Presence of primary end point related patterns (such as \"primary composite endpoints\") are searched in the full text articles. After identifying the relevant text portions, BioBERT based Q&A model along with related questions are used to identify high confidence text snippets that are then used to populate this field. For articles without full text, BioBERT Q&A model is run on the whole title/abstract section. \n* **\"Clinical Improvement (Y/N)\" field**:\n    * We use a combination of standard questions and a set of adaptive questions together with BioBERT to extract relevant text snippets with high prediction scores. For adaptive questions, we use the tagged therapeutics and frame tailored questions such as \"How effective was Y?\" where \"Y\" stands for a tagged therapeutic entity. The extracted text snippet is then fed to Vader sentiment analyzer to classify it as either Y or N. \n* In summary, to populate the above fields, we heavily use BioBERT Q&A model to identify high confidence text snippets by firing multiple related questions. To improve the precision further, we use the tagged entities in the article to both process these snippets and to construct adaptive queries for BioBERT that are tailored to the specific article. For some of the fields, we use additional trained SVM models that take these snippets as input and predict the field values. We believe that using a combination of BioBERT Q&A framework together with tailored questions, tagged annotations and additional SVM classifiers is a promising approach in achieving both good precision as well as recall.\n* The output of this step is novel_th_ab_wbert.tsv file (hgs_ab_wbert.tsv for the hypercoagulation question) containing all the target fields and this forms the input to the next step.\n* \n\n### 4. **Post processing**: \n\n* We perform a set of post processing steps to clean the csv file generated by the previous step.\n* Add missing Journals, if found in MEDLINE. All the available journal names in the corpus were mapped to their PubMed journal name abbreviation. Some were a direct match, while there were many which needed to be processed into their full names using rules created based on the patterns observed.\n* Add missing DOI, if found in MEDLINE.\n* Add missing/partial publication date, if found in MEDLINE.\n* Date: to be transformed to M/D/YYYY format. If the date is shown as just \"2020\", then it is left as-is.\n* Remove the word(s) \"Abstract\" or \"Background\", \"Full-length title\", \"News at a glance\", \"Letter to the Editor\" or \"commentary\" if the title begins with these in the Study field.\n\n\n### **The final output files**: \n* novel_th.csv file for the novel therapeutics question\n* hgs.csv for the hypercoagulation question","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Additional Details\n* The latest CORD-19 input corpus consisted of 1,38,794 articles.\n* A total of 2553 non-English articles were removed in the preprocessing step.\n* A total of 2420 filtered articles resulted after performing step 2 (identifying relevant articles).\n* The /kaggle/input/data folder contains:\n    * The BioBERT Q&A SQuAD model \n    * Combined corpus of CORD-19 articles and filtered MEDLINE\n    * Corpus annotations and Pearson correlation values\n    * SVM models for study type classification\n    * one-class SVM model for clinical study article classification\n* The /kaggle/input/code/python folder contains:\n    * CORD-19 JSON parser\n    * FastText foreign language classifier.\n    * BioBERT Q&A code\n    * SVM classifier (training and prediction) for study type classification in the utils sub folder\n    * one-class SVM classifier (training and prediction) for clinical study article classification in the utils sub folder\n* The /kaggle/input/code/java folder contains:\n    * java source code for novel therapuetics pipeline, BioBERT input/output processing and for the post processing step.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## References\n1. Joseph T, Saipradeep VG, Raghavan GS, Srinivasan R, Rao A, Kotte S, Sivadasan N. TPX: Biomedical literature search made easy. Bioinformation 8(12): 578-80 (2012).\n2. Lee, J; Yoon, W; Kim, S; Kim, D; Kim, S; Ho So, C; Kang, J. Bioinformatics, Volume 36, Issue 4, 15 February 2020, Pages 1234–1240.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -lrt /kaggle/input\n!java -version\n!javac -d /kaggle/working/  /kaggle/input/code/java/CORD19/src/DTBean.java\n!javac -d /kaggle/working/  /kaggle/input/code/java/CORD19/src/Segment.java\n!javac -d /kaggle/working/  /kaggle/input/code/java/CORD19/src/Text.java\n!javac -d /kaggle/working/  /kaggle/input/code/java/CORD19/src/Word2Num.java\n!javac -d /kaggle/working/ -cp /kaggle/working/ /kaggle/input/code/java/CORD19/src/SentenceSplitter.java\n!javac -d /kaggle/working/ -cp /kaggle/working/ /kaggle/input/code/java/CORD19/src/NovelTherapeuticsPipeline.java\n!javac -d /kaggle/working/ -cp /kaggle/working/ /kaggle/input/code/java/CORD19/src/BERTInputPreprocess.java\n!javac -d /kaggle/working/ -cp /kaggle/working/ /kaggle/input/code/java/CORD19/src/BERTPostProcessing.java\n!javac -d /kaggle/working/ -cp /kaggle/working/:/kaggle/input/code/java/CORD19/lib/* /kaggle/input/code/java/CORD19/src/BERTOutputProcessor.java\n\n!java -cp /kaggle/working/ NovelTherapeuticsPipeline\n!java -cp /kaggle/working/ BERTInputPreprocess\n!ls -lrt /kaggle/input\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Code for running BioBERT for novel therapeutics\n\n!pip install tensorflow-gpu==1.14.0\n!pip install bert-tensorflow\n\n! python /kaggle/input/code/python/run_factoid.py      --do_train=False      --do_predict=True      --vocab_file=/kaggle/input/data/data/BERT-pubmed-1000000-SQuAD/vocab.txt      --bert_config_file=/kaggle/input/data/data/BERT-pubmed-1000000-SQuAD/bert_config.json      --init_checkpoint=/kaggle/input/data/data/BERT-pubmed-1000000-SQuAD/model.ckpt-14599      --max_seq_length=384      --train_batch_size=12      --learning_rate=5e-6      --doc_stride=128      --num_train_epochs=5.0      --do_lower_case=False      --predict_file=/kaggle/working/novel_th_ab_bert.json      --output_dir=/kaggle/working/","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!java -cp /kaggle/working/:/kaggle/input/code/java/CORD19/lib/* BERTOutputProcessor\n!java -cp /kaggle/working/:/kaggle/input/code/java/CORD19/lib/* BERTPostProcessing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nimport re\nnltk.downloader.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport pandas as pd\n\nregex=r\"\\b(low|reduce|stop)(.*)(infection|fatal|mortal|risk|cytokine storm|concentration|death|adverse)+\"\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\ndef checkNegFP(text):\n    m=re.search(regex,text)\n    if(m!=None):\n        return True\n    else:\n        return False\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_file = '/kaggle/working/novel_th_ab_wbert_processed.tsv'\ntdf = pd.read_csv(input_file,sep='\\t',converters={\"Clinical Improvement (Y/N)\":str})\nfor i, r in tdf.iterrows():\n    r[1]=r[1].title() \n    score_dict = SentimentIntensityAnalyzer().polarity_scores(r[9]);\n    if(score_dict['neg']>score_dict['pos']):\n        if checkNegFP(r[9]):\n            r[9]='Y'\n        else:\n            r[9]='N'\n    elif(score_dict['neg']<score_dict['pos']):\n        r[9]='Y'\n    else:\n        r[9]='-'\ntdf.to_csv(\"/kaggle/working/novel_th.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Final output summary table for novel therapeutics","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tdf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!java -cp /kaggle/working/ NovelTherapeuticsPipeline q1\n!java -cp /kaggle/working/ BERTInputPreprocess q1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Code for running BioBERT for hypercoagulable state\n\n! python /kaggle/input/code/python/run_factoid.py      --do_train=False      --do_predict=True      --vocab_file=/kaggle/input/data/data/BERT-pubmed-1000000-SQuAD/vocab.txt      --bert_config_file=/kaggle/input/data/data/BERT-pubmed-1000000-SQuAD/bert_config.json      --init_checkpoint=/kaggle/input/data/data/BERT-pubmed-1000000-SQuAD/model.ckpt-14599      --max_seq_length=384      --train_batch_size=12      --learning_rate=5e-6      --doc_stride=128      --num_train_epochs=5.0      --do_lower_case=False      --predict_file=/kaggle/working/hgs_ab_bert.json      --output_dir=/kaggle/working/","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!java -cp /kaggle/working/:/kaggle/input/code/java/CORD19/lib/* BERTOutputProcessor q1\n!java -cp /kaggle/working/:/kaggle/input/code/java/CORD19/lib/* BERTPostProcessing q1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_file = '/kaggle/working/hgs_ab_wbert_processed.tsv'\ndf = pd.read_csv(input_file,sep='\\t',converters={\"Clinical Improvement (Y/N)\":str})\nfor i, r in df.iterrows():\n    r[1]=r[1].title() \n    score_dict = SentimentIntensityAnalyzer().polarity_scores(r[9]);\n    if(score_dict['neg']>score_dict['pos']):\n        if checkNegFP(r[9]):\n            r[9]='Y'\n        else:\n            r[9]='N'\n    elif(score_dict['neg']<score_dict['pos']):\n        r[9]='Y'\n    else:\n        r[9]='-'\ndf.to_csv(\"/kaggle/working/hgs.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Final output summary table for hypercoagulable state","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}