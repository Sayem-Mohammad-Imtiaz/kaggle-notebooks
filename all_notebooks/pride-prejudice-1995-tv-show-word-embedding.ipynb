{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import spacy\nimport pandas as pd\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-05-21T14:15:22.001944Z","iopub.execute_input":"2021-05-21T14:15:22.00253Z","iopub.status.idle":"2021-05-21T14:15:22.784318Z","shell.execute_reply.started":"2021-05-21T14:15:22.002445Z","shell.execute_reply":"2021-05-21T14:15:22.78322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/pride-prejudice-clean-dataset/pride_prejudice(1995).csv', delimiter = ',')\ndf","metadata":{"execution":{"iopub.status.busy":"2021-05-21T14:15:25.17403Z","iopub.execute_input":"2021-05-21T14:15:25.17443Z","iopub.status.idle":"2021-05-21T14:15:25.218441Z","shell.execute_reply.started":"2021-05-21T14:15:25.174392Z","shell.execute_reply":"2021-05-21T14:15:25.217485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cleaning(doc):\n    txt = [token.lemma_.lower() for token in doc if not token.is_stop and len(token) > 1] # this line tokenizes, takes out stopwords, and returns lemmas. We also lowercase everything (works for english, not necessarily other langauges) and also only take words with more than one letter to get rid of punctuation. There's smarter ways to do that -- but this is meant to be simplistic!!\n    if len(txt) > 2: # dump any sentences with less than 2 words -- word2vec is based on collocation, after all...\n        return \" \".join(txt)\n\ndef process(df):\n    docs = [row for row in df[\"Pride and Prejudice (1995)\"]] # get your documents as a list of individual texts -- if you get a KeyError you might have to check your dataframe\n    txts = [cleaning(doc) for doc in nlp.pipe(docs, batch_size=500, n_process = -1)] # nlp.pipe is some spacy magic -- this is where the processing happens\n    sentences = [row.split() for row in txts if row != None] # we need to return the cleaned data (from cleaning(doc)) as sentences for Gensim\n    return sentences # this is what we pass to Gensim","metadata":{"execution":{"iopub.status.busy":"2021-05-21T14:15:28.157393Z","iopub.execute_input":"2021-05-21T14:15:28.157904Z","iopub.status.idle":"2021-05-21T14:15:28.165577Z","shell.execute_reply.started":"2021-05-21T14:15:28.15787Z","shell.execute_reply":"2021-05-21T14:15:28.164351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_sm\", disable = [\"ner\",\"parser\"])\n\n## now process our data\nsentences = process(df)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T14:15:31.035821Z","iopub.execute_input":"2021-05-21T14:15:31.03637Z","iopub.status.idle":"2021-05-21T14:15:34.62021Z","shell.execute_reply.started":"2021-05-21T14:15:31.036318Z","shell.execute_reply":"2021-05-21T14:15:34.61831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## first we initialize the model\nfrom gensim.models import Word2Vec\n\nmodel = Word2Vec(\n    min_count = 1, ## you can mess around with these parameters (don't mess with workers though!); min_count refers to words that appear at least N times \n    window = 4, ## window refers to the size of the window +/- N words for something to count as a collocation\n    workers = 4 ## leave this one alone...refers to the number of CPUs/threads to use. This works with Google Colab so just leave it alone for now. \n)\n\nmodel.build_vocab(sentences, progress_per = 50) # remember that sentences is just what we output after spacy did its work in the process(df) function","metadata":{"execution":{"iopub.status.busy":"2021-05-21T14:15:37.299513Z","iopub.execute_input":"2021-05-21T14:15:37.299981Z","iopub.status.idle":"2021-05-21T14:15:38.53023Z","shell.execute_reply.started":"2021-05-21T14:15:37.299931Z","shell.execute_reply":"2021-05-21T14:15:38.529344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## now we train the model on our data\n\nmodel.train(sentences, total_examples = model.corpus_count, epochs = 30)\n\n## This might take quite some time again","metadata":{"execution":{"iopub.status.busy":"2021-05-21T14:15:42.084924Z","iopub.execute_input":"2021-05-21T14:15:42.085342Z","iopub.status.idle":"2021-05-21T14:15:42.787169Z","shell.execute_reply.started":"2021-05-21T14:15:42.085304Z","shell.execute_reply":"2021-05-21T14:15:42.786049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# first, save the model\n\nmodel_savename = \"model95.w2v\"\nmodel.wv.save(model_savename)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T14:15:45.400781Z","iopub.execute_input":"2021-05-21T14:15:45.401156Z","iopub.status.idle":"2021-05-21T14:15:45.410488Z","shell.execute_reply.started":"2021-05-21T14:15:45.401124Z","shell.execute_reply":"2021-05-21T14:15:45.409404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## ok now the model is saved, so lets reload it\n## we need to import a helper function from Gensim to reload the model\n\nfrom gensim.models import KeyedVectors\n\nmodel = KeyedVectors.load(\"model95.w2v\")","metadata":{"execution":{"iopub.status.busy":"2021-05-21T14:15:49.24463Z","iopub.execute_input":"2021-05-21T14:15:49.245061Z","iopub.status.idle":"2021-05-21T14:15:49.253082Z","shell.execute_reply.started":"2021-05-21T14:15:49.245027Z","shell.execute_reply":"2021-05-21T14:15:49.2518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.vocab # generate the list of all vocabulary within the dataset","metadata":{"execution":{"iopub.status.busy":"2021-05-21T13:29:33.750588Z","iopub.execute_input":"2021-05-21T13:29:33.751064Z","iopub.status.idle":"2021-05-21T13:29:33.815855Z","shell.execute_reply.started":"2021-05-21T13:29:33.751023Z","shell.execute_reply":"2021-05-21T13:29:33.813527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## let's make a table of our vocabulary with frequency\n\nrecords = list()\n\n## iterate over every word in the vocab and get its frequency and save it to records\nfor word in model.vocab:\n    records.append((word, model.vocab[word].count))\n\n## make a table with some pandas magic\nmodel_vocab_df = pd.DataFrame.from_records(records, columns = [\"lemma\",\"frequency\"])\nmodel_vocab_df.sort_values(\"frequency\", ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T13:29:36.820676Z","iopub.execute_input":"2021-05-21T13:29:36.821096Z","iopub.status.idle":"2021-05-21T13:29:36.845022Z","shell.execute_reply.started":"2021-05-21T13:29:36.821062Z","shell.execute_reply":"2021-05-21T13:29:36.842784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def naive_projection(x_axis, y_axis, test_words, model, plot_size=10):\n\n\n  \n    if len(x_axis) != 2:\n        print(\"You must only have two antonyms in your x-axis\")\n    elif len(y_axis) !=2:\n        print(\"You must only have two antonyms in your y-axis\")\n    else:\n\n\n        x = list() \n        y = list()\n\n    \n    for word in test_words:\n        x_val = model.distance(x_axis[0], word) - model.distance(x_axis[1], word) \n        y_val = model.distance(y_axis[0], word) - model.distance(y_axis[1], word) \n        x.append(x_val) \n        y.append(y_val)\n    \n   \n\n    fig, ax = plt.subplots(figsize=(plot_size,plot_size))\n\n    for i in range(len(x)):\n        ax.scatter(x[i], y[i])\n        ax.annotate(test_words[i], (x[i], y[i]))\n    \n    xlab = x_axis[0] + \" --- \" + x_axis[1]\n    ylab = y_axis[0] + \" --- \" + y_axis[1]\n    plt.xlabel(xlab)\n    plt.ylabel(ylab)\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T14:16:19.227145Z","iopub.execute_input":"2021-05-21T14:16:19.227523Z","iopub.status.idle":"2021-05-21T14:16:19.239195Z","shell.execute_reply.started":"2021-05-21T14:16:19.227489Z","shell.execute_reply":"2021-05-21T14:16:19.237785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_axis = [\"mr\", \"miss\"]\ny_axis = [\"rich\",\"poor\"]\ntest_words = [\"elizabeth\", \"bennet\", \"fitzwilliam\", \"darcy\", \"bourgh\", \"jane\", \"bingley\", \"caroline\", \"wickham\", \"lydia\", \"gardiner\", \"william\", \"charlotte\", \"georgiana\", \"mary\", \"catherine\"] \n\nnaive_projection(x_axis, y_axis, test_words, model)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T14:16:22.023682Z","iopub.execute_input":"2021-05-21T14:16:22.0241Z","iopub.status.idle":"2021-05-21T14:16:22.469733Z","shell.execute_reply.started":"2021-05-21T14:16:22.024065Z","shell.execute_reply":"2021-05-21T14:16:22.468567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_axis = [\"man\",\"woman\"]\ny_axis = [\"fortune\",\"debt\"]\ntest_words = [\"elizabeth\", \"bennet\", \"fitzwilliam\", \"darcy\", \"bourgh\", \"jane\", \"bingley\", \"caroline\", \"wickham\", \"lydia\", \"gardiner\", \"william\", \"charlotte\", \"georgiana\", \"mary\", \"catherine\"] \n\nnaive_projection(x_axis, y_axis, test_words, model)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T14:16:25.55889Z","iopub.execute_input":"2021-05-21T14:16:25.559438Z","iopub.status.idle":"2021-05-21T14:16:25.986561Z","shell.execute_reply.started":"2021-05-21T14:16:25.559402Z","shell.execute_reply":"2021-05-21T14:16:25.985584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_axis = [\"gentleman\",\"lady\"]\ny_axis = [\"lord\",\"servant\"]\ntest_words = [\"elizabeth\", \"bennet\", \"fitzwilliam\", \"darcy\", \"jane\", \"bingley\", \"caroline\", \"wickham\", \"lydia\", \"gardiner\", \"william\", \"charlotte\", \"georgiana\", \"mary\",  \"bourgh\",\"catherine\"] \n\nnaive_projection(x_axis, y_axis, test_words, model)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T14:16:28.995403Z","iopub.execute_input":"2021-05-21T14:16:28.995833Z","iopub.status.idle":"2021-05-21T14:16:29.412388Z","shell.execute_reply.started":"2021-05-21T14:16:28.99579Z","shell.execute_reply":"2021-05-21T14:16:29.411251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_axis = [\"sister\", \"brother\"]\ny_axis = [\"honour\", \"disgrace\"]\ntest_words = [\"elizabeth\", \"bennet\", \"fitzwilliam\", \"darcy\", \"bourgh\", \"jane\", \"bingley\", \"caroline\", \"wickham\", \"lydia\", \"gardiner\", \"william\", \"charlotte\", \"georgiana\", \"mary\", \"catherine\"] \n\nnaive_projection(x_axis, y_axis, test_words, model)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T14:16:32.051705Z","iopub.execute_input":"2021-05-21T14:16:32.052098Z","iopub.status.idle":"2021-05-21T14:16:32.492705Z","shell.execute_reply.started":"2021-05-21T14:16:32.052062Z","shell.execute_reply":"2021-05-21T14:16:32.491631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def advanced_projection(x_dimensions, y_dimensions, test_words, model, plot_size=10, xlab=\"label\", ylab = \"label\"):\n    x = list()\n    y = list()\n\n    for word in test_words:\n        x_vals = list()\n        y_vals = list()\n\n        for dim in x_dimensions:\n            xval = model.distance(dim[0], word) - model.distance(dim[1], word)\n            x_vals.append(xval)\n    ## repeat for y values\n        for dim in y_dimensions:\n            yval = model.distance(dim[0], word) - model.distance(dim[1], word)\n            y_vals.append(yval)\n\n    ## ok now we need to take the average of all the x_vals and y_vals we collected for this word\n        xavg = statistics.mean(x_vals)\n        yavg = statistics.mean(y_vals)\n\n    ## now lets save this to our x and y lists that we set up above (outside the for word in test_words loop) so that we can plot the word\n        x.append(xavg)\n        y.append(yavg)\n\n\n    \n    fig, ax = plt.subplots(figsize=(plot_size,plot_size))\n    for i in range(len(x)):\n        ax.scatter(x[i], y[i])\n        ax.annotate(test_words[i], (x[i], y[i])) \n    plt.xlabel(xlab)\n    plt.ylabel(ylab)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T14:16:36.576828Z","iopub.execute_input":"2021-05-21T14:16:36.577211Z","iopub.status.idle":"2021-05-21T14:16:36.587819Z","shell.execute_reply.started":"2021-05-21T14:16:36.577178Z","shell.execute_reply":"2021-05-21T14:16:36.586499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import statistics\n\nx_dimensions =  [\n                [\"husband\", \"wife\"],\n                [\"man\",\"woman\"],\n                [\"father\",\"mother\"],\n                [\"gentleman\", \"lady\"],\n                [\"mr\", \"miss\"],\n                [\"mr\", \"mrs\"],\n                [\"brother\", \"sister\"],\n                [\"sir\", \"madam\"]\n]\n\n\ny_dimensions = [\n                [\"rich\",\"poor\"],\n                [\"master\",\"servant\"],\n                [\"fortune\", \"misfortune\"]\n]\n\ntest_words = [\"elizabeth\", \"bennet\", \"fitzwilliam\", \"darcy\", \"jane\", \"bingley\",\"wickham\", \"lydia\", \"gardiner\", \"william\", \"charlotte\", \"georgiana\", \"mary\", \"caroline\", \"bourgh\",\"catherine\"] \nxlab = \"maleness -- femaleness\"\nylab = \"richness -- poorness\"\n\nadvanced_projection(\n    x_dimensions,\n    y_dimensions,\n    test_words,\n    model,\n    plot_size=10,\n    xlab = xlab,\n    ylab = ylab\n)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T14:16:39.01208Z","iopub.execute_input":"2021-05-21T14:16:39.012469Z","iopub.status.idle":"2021-05-21T14:16:39.412861Z","shell.execute_reply.started":"2021-05-21T14:16:39.012433Z","shell.execute_reply":"2021-05-21T14:16:39.412026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_dimensions =  [\n                [\"husband\", \"wife\"],\n                [\"man\",\"woman\"],\n                [\"father\",\"mother\"],\n                [\"gentleman\", \"lady\"],\n                [\"mr\", \"miss\"],\n                [\"mr\", \"mrs\"],\n                [\"brother\", \"sister\"],\n                [\"sir\", \"madam\"]\n]\n\n\ny_dimensions = [\n                [\"rich\",\"poor\"],\n                [\"master\",\"servant\"],\n                [\"fortune\", \"misfortune\"]\n]\n\ntest_words = [\"elizabeth\", \"bennet\", \"fitzwilliam\", \"darcy\", \"jane\", \"bingley\",\"wickham\", \"lydia\", \"gardiner\", \"william\", \"charlotte\", \"georgiana\", \"mary\", \"caroline\"] \nxlab = \"maleness -- femaleness\"\nylab = \"richness -- poorness\"\n\nadvanced_projection(\n    x_dimensions,\n    y_dimensions,\n    test_words,\n    model,\n    plot_size=10,\n    xlab = xlab,\n    ylab = ylab\n)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T14:16:52.587796Z","iopub.execute_input":"2021-05-21T14:16:52.58845Z","iopub.status.idle":"2021-05-21T14:16:52.961885Z","shell.execute_reply.started":"2021-05-21T14:16:52.588412Z","shell.execute_reply":"2021-05-21T14:16:52.960831Z"},"trusted":true},"execution_count":null,"outputs":[]}]}