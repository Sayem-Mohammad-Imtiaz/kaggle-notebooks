{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking if we have a working GPU\ntorch.cuda.device_count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#name of the available GPU\ntorch.cuda.get_device_name(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scp /Users/gigik/Desktop/en-ha.csv gigi@147.83.50.73:/veu4/usuaris29/gigi/translator","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !unzip englishhausa-corpus.zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import our dependencies","metadata":{}},{"cell_type":"code","source":"#numpy\nimport numpy as np\n#numpy\nimport pandas as pd\n#universal character encoding standard...assigns a number to every character\nimport unicodedata\n#string module\nimport string\n#RegEx\nimport re\n#random module\nimport random\n#import PyTorch\nimport torch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load our dataset.","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('../input/englishhausa-corpus/en-ha.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A little data exploration","metadata":{}},{"cell_type":"code","source":"#first 5 rows of the dataset\ndata.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = 42","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop duplicate translations\ndata = data.drop_duplicates()\n\n# Shuffle the data to remove bias in dev set selection.\ndata = data.sample(frac=1, random_state=seed).reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check dimensions of the dataset\ndata.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#I noticed there was an extra row of numbers, I renamed them and dropped the row\n#I think there's a better way to go about this though.\ndata.rename( columns={'Unnamed: 0':'numbers'}, inplace=True )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.drop('numbers', inplace=True, axis=1) #drop column with floats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#now we have 2 rows\ndata.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Create a Language class to store  util functions\n## such as index to word and word to index\nSOS_token = 0\nEOS_token = 1\n\n\nclass Lang:\n    def __init__(self, name):\n        self.name = name\n        self.word2index = {\"<blank>\":0, \"SOS\":1,\"EOS\":2}\n        self.word2count = {}\n        self.index2word = {0:\"<blank>\", 1: \"SOS\", 2: \"EOS\"}\n        self.n_words = 3  # Count SOS and EOS\n\n    def addSentence(self, sentence):\n      try:\n        for word in sentence.split(' '):\n          self.addWord(word)\n      except:\n        a = 1\n        # Do nothing\n\n    def addWord(self, word):\n        if word not in self.word2index:\n            self.word2index[word] = self.n_words\n            self.word2count[word] = 1\n            self.index2word[self.n_words] = word\n            self.n_words += 1\n        else:\n            self.word2count[word] += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Turn a Unicode string to plain ASCII, thanks to\n# https://stackoverflow.com/a/518232/2809427\ndef unicodeToAscii(s):\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn'\n    )\n\n# Lowercase, trim, and remove non-letter characters\ndef normalizeString(s):\n    s = s.strip()\n    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n    # s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n    return s.strip()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove empty rows\ndata = data.dropna()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## create lang\neng_lang = Lang(\"source_sentence\")\nhau_lang = Lang(\"hau_new\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index, (source_sentence, target_sentence) in data.iterrows():\n  # print(source_sentence, target_sentence)\n  eng_lang.addSentence(source_sentence)\n  hau_lang.addSentence(target_sentence)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hau_lang.n_words","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eng_lang.n_words","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(list(eng_lang.word2count))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## convert word to their index\ndef tokenize(Lang,sentence):\n    return np.array([Lang.word2index.get(word) for word in sentence.split(' ')])\n    #X = [[word2idx.get(token, None) for token in d.split()] for d in desc]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['source_index'] = data['source_sentence'].apply(lambda s: tokenize(eng_lang, s))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['target_index'] = data['target_sentence'].apply(lambda s: tokenize(hau_lang, s))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pairs2 = data[['source_index', 'target_index']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pairs2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pairs2.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create the input lan english tokenization","metadata":{}},{"cell_type":"code","source":"input_token = data['source_index']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_token.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_token = data['target_index']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_token.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(input_token), len(output_token)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len_arr = map(lambda x: len(x), input_token)\nmax_input_length = np.array(list(len_arr)).max()\nmax_input_length","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len_arr = map(lambda x: len(x), output_token)\nmax_output_length = np.array(list(len_arr)).max()\nmax_output_length","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"the input and output lang is padded and converted into a numpy array","metadata":{}},{"cell_type":"code","source":"MAX_LENGTH = (np.array([max_input_length, max_output_length])).max()\nMAX_LENGTH = min(256, MAX_LENGTH)\nMAX_LENGTH","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_tokenPad = np.zeros((len(input_token),MAX_LENGTH))\noutput_tokenPad = np.zeros((len(input_token),MAX_LENGTH))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(np.unique(input_tokenPad[2]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_token[0], output_token[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i,v in enumerate(input_token):\n    \n    for j, token in enumerate(v[:MAX_LENGTH]):\n        \n        input_tokenPad[i,j] = token","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i,v in enumerate(output_token):\n    \n    for j, token in enumerate(v[:MAX_LENGTH]):\n        \n        output_tokenPad[i,j] = token","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(input_tokenPad[1], output_tokenPad[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## we only want up to 20K rows------cause of batching\ninput_tokenPad1 = input_tokenPad[:1000]\noutput_tokenPad1 = output_tokenPad[:1000]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(input_tokenPad1.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#split the dataset into train, test and validation\ntrain_eng, valid_eng,train_hau,valid_hau = train_test_split(input_tokenPad1,output_tokenPad1,test_size=0.2,shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_eng[0], train_hau[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dataloader is created to make batching easy","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader\n\ntrain_data = TensorDataset(torch.from_numpy(train_eng).long(),torch.from_numpy(train_hau).long())\nvalid_data = TensorDataset(torch.from_numpy(valid_eng).long(),torch.from_numpy(valid_hau).long())\n\nbatch_size = 32\nbatch_size = 16\n\ntrain_loader= DataLoader(train_data,shuffle=True,batch_size=batch_size,)\nvalid_loader =DataLoader(valid_data,shuffle=True,batch_size=batch_size,)\n\n# print(train_eng[3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(list(train_loader))[0][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# obtain one batch of training data\ndataiter = iter(train_loader)\nsample_x, sample_y = dataiter.next()\n\nprint(\"shape of english\", sample_x.device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Transformer model","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nimport math, copy, time\nfrom torch.autograd import Variable\nimport matplotlib.pyplot as plt\nimport seaborn\nseaborn.set_context(context=\"talk\")\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Transformer","metadata":{}},{"cell_type":"code","source":"class EncoderDecoder(nn.Module):\n    \"\"\"\n    A standard Encoder-Decoder architecture. Base for this and many \n    other models.\n    \"\"\"\n    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n        super(EncoderDecoder, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.src_embed = src_embed\n        self.tgt_embed = tgt_embed\n        self.generator = generator\n        \n    def forward(self, src, tgt, src_mask, tgt_mask):\n        \"Take in and process masked src and target sequences.\"\n        return self.decode(self.encode(src, src_mask), src_mask,\n                            tgt, tgt_mask)\n    \n    def encode(self, src, src_mask):\n        return self.encoder(self.src_embed(src), src_mask)\n    \n    def decode(self, memory, src_mask, tgt, tgt_mask):\n        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Generator(nn.Module):\n    \"Define standard linear + softmax generation step.\"\n    def __init__(self, d_model, vocab):\n        super(Generator, self).__init__()\n        self.proj = nn.Linear(d_model, vocab)\n\n    def forward(self, x):\n        return F.log_softmax(self.proj(x), dim=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"encoder and decoder stacks","metadata":{}},{"cell_type":"code","source":"def clones(module, N):\n    \"Produce N identical layers.\"\n    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    \"Core encoder is a stack of N layers\"\n    def __init__(self, layer, N):\n        super(Encoder, self).__init__()\n        self.layers = clones(layer, N)\n        self.norm = LayerNorm(layer.size)\n        \n    def forward(self, x, mask):\n        \"Pass the input (and mask) through each layer in turn.\"\n        for layer in self.layers:\n            x = layer(x, mask)\n        return self.norm(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LayerNorm(nn.Module):\n    \"Construct a layernorm module (See citation for details).\"\n    def __init__(self, features, eps=1e-6):\n        super(LayerNorm, self).__init__()\n        self.a_2 = nn.Parameter(torch.ones(features))\n        self.b_2 = nn.Parameter(torch.zeros(features))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SublayerConnection(nn.Module):\n    \"\"\"\n    A residual connection followed by a layer norm.\n    Note for code simplicity the norm is first as opposed to last.\n    \"\"\"\n    def __init__(self, size, dropout):\n        super(SublayerConnection, self).__init__()\n        self.norm = LayerNorm(size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, sublayer):\n        \"Apply residual connection to any sublayer with the same size.\"\n        return x + self.dropout(sublayer(self.norm(x)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    \"Encoder is made up of self-attn and feed forward (defined below)\"\n    def __init__(self, size, self_attn, feed_forward, dropout):\n        super(EncoderLayer, self).__init__()\n        self.self_attn = self_attn\n        self.feed_forward = feed_forward\n        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n        self.size = size\n\n    def forward(self, x, mask):\n        \"Follow Figure 1 (left) for connections.\"\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n        return self.sublayer[1](x, self.feed_forward)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Decoder","metadata":{}},{"cell_type":"code","source":"class Decoder(nn.Module):\n    \"Generic N layer decoder with masking.\"\n    def __init__(self, layer, N):\n        super(Decoder, self).__init__()\n        self.layers = clones(layer, N)\n        self.norm = LayerNorm(layer.size)\n        \n    def forward(self, x, memory, src_mask, tgt_mask):\n        for layer in self.layers:\n            x = layer(x, memory, src_mask, tgt_mask)\n        return self.norm(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n        super(DecoderLayer, self).__init__()\n        self.size = size\n        self.self_attn = self_attn\n        self.src_attn = src_attn\n        self.feed_forward = feed_forward\n        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n \n    def forward(self, x, memory, src_mask, tgt_mask):\n        \"Follow Figure 1 (right) for connections.\"\n        m = memory\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n        return self.sublayer[2](x, self.feed_forward)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def subsequent_mask(size):\n    \"Mask out subsequent positions.\"\n    attn_shape = (1, size, size)\n    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n    return torch.from_numpy(subsequent_mask) == 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nplt.imshow(subsequent_mask(20)[0])\nNone","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Attention","metadata":{}},{"cell_type":"code","source":"def attention(query, key, value, mask=None, dropout=None):\n    \"Compute 'Scaled Dot Product Attention'\"\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n             / math.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    p_attn = F.softmax(scores, dim = -1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return torch.matmul(p_attn, value), p_attn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        \"Take in model size and number of heads.\"\n        super(MultiHeadedAttention, self).__init__()\n        assert d_model % h == 0\n        # We assume d_v always equals d_k\n        self.d_k = d_model // h\n        self.h = h\n        self.linears = clones(nn.Linear(d_model, d_model), 4)\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout)\n        \n    def forward(self, query, key, value, mask=None):\n        \"Implements Figure 2\"\n        if mask is not None:\n            # Same mask applied to all h heads.\n            mask = mask.unsqueeze(1)\n        nbatches = query.size(0)\n        \n        # 1) Do all the linear projections in batch from d_model => h x d_k \n        query, key, value = \\\n            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n             for l, x in zip(self.linears, (query, key, value))]\n        \n        # 2) Apply attention on all the projected vectors in batch. \n        x, self.attn = attention(query, key, value, mask=mask, \n                                 dropout=self.dropout)\n        \n        # 3) \"Concat\" using a view and apply a final linear. \n        x = x.transpose(1, 2).contiguous() \\\n             .view(nbatches, -1, self.h * self.d_k)\n        return self.linears[-1](x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Position-wise feedforward","metadata":{}},{"cell_type":"code","source":"class PositionwiseFeedForward(nn.Module):\n    \"Implements FFN equation.\"\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super(PositionwiseFeedForward, self).__init__()\n        self.w_1 = nn.Linear(d_model, d_ff)\n        self.w_2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.w_2(self.dropout(F.relu(self.w_1(x))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"embedding and softmax","metadata":{}},{"cell_type":"code","source":"class Embeddings(nn.Module):\n    def __init__(self, d_model, vocab):\n        super(Embeddings, self).__init__()\n        self.lut = nn.Embedding(vocab, d_model)\n        self.d_model = d_model\n\n    def forward(self, x):\n        return self.lut(x) * math.sqrt(self.d_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"positional encoding","metadata":{}},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    \"Implement the PE function.\"\n    def __init__(self, d_model, dropout, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        # Compute the positional encodings once in log space.\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0., max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0., d_model, 2) *\n                             -(math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n        \n    def forward(self, x):\n        x = x + Variable(self.pe[:, :x.size(1)], \n                         requires_grad=False)\n        return self.dropout(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 5))\npe = PositionalEncoding(20, 0)\ny = pe.forward(Variable(torch.zeros(1, 100, 20)))\nplt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\nplt.legend([\"dim %d\"%p for p in [4,5,6,7]])\nNone","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Full model","metadata":{}},{"cell_type":"code","source":"def make_model(src_vocab, tgt_vocab, N=6, \n               d_model=512, d_ff=2048, h=8, dropout=0.1):\n    \"Helper: Construct a model from hyperparameters.\"\n    c = copy.deepcopy\n    attn = MultiHeadedAttention(h, d_model)\n    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n    position = PositionalEncoding(d_model, dropout)\n    model = EncoderDecoder(\n        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n                             c(ff), dropout), N),\n        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n        Generator(d_model, tgt_vocab))\n    \n    # This was important from their code. \n    # Initialize parameters with Glorot / fan_avg.\n    for p in model.parameters():\n        if p.dim() > 1:\n            nn.init.xavier_uniform(p)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Small example model.\ntmp_model = make_model(10, 10, 2)\nNone","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training","metadata":{}},{"cell_type":"code","source":"class Batch:\n    \"Object for holding a batch of data with mask during training.\"\n    def __init__(self, src, trg=None, pad=0):\n        self.src = src\n        self.src_mask = (src != pad).unsqueeze(-2)\n        if trg is not None:\n            self.trg = trg[:, :-1]\n            self.trg_y = trg[:, 1:]\n            self.trg_mask = \\\n                self.make_std_mask(self.trg, pad)\n            self.ntokens = (self.trg_y != pad).data.sum()\n    \n    @staticmethod\n    def make_std_mask(tgt, pad):\n        \"Create a mask to hide padding and future words.\"\n        tgt_mask = (tgt != pad).unsqueeze(-2)\n        tgt_mask = tgt_mask & Variable(\n            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n        return tgt_mask","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_epoch(data_iter, model, loss_compute):\n    \"Standard Training and Logging Function\"\n    start = time.time()\n    total_tokens = 0\n    total_loss = 0\n    tokens = 0\n    for i, data in enumerate(data_iter):\n        \n        src, trg = data\n        batch = Batch(src.cuda(),trg.cuda())\n        out = model.forward(batch.src, batch.trg, \n                            batch.src_mask, batch.trg_mask)\n        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n        total_loss += loss\n        total_tokens += batch.ntokens\n        tokens += batch.ntokens\n        if i % 50 == 1:\n            elapsed = time.time() - start\n            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n                    (i, loss / batch.ntokens, tokens / elapsed))\n            start = time.time()\n            tokens = 0\n    return total_loss / total_tokens","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"optim","metadata":{}},{"cell_type":"code","source":"class NoamOpt:\n    \"Optim wrapper that implements rate.\"\n    def __init__(self, model_size, factor, warmup, optimizer):\n        self.optimizer = optimizer\n        self._step = 0\n        self.warmup = warmup\n        self.factor = factor\n        self.model_size = model_size\n        self._rate = 0\n        \n    def step(self):\n        \"Update parameters and rate\"\n        self._step += 1\n        rate = self.rate()\n        for p in self.optimizer.param_groups:\n            p['lr'] = rate\n        self._rate = rate\n        self.optimizer.step()\n        \n    def rate(self, step = None):\n        \"Implement `lrate` above\"\n        if step is None:\n            step = self._step\n        return self.factor * \\\n            (self.model_size ** (-0.5) *\n            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n        \ndef get_std_opt(model):\n    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Three settings of the lrate hyperparameters.\nopts = [NoamOpt(512, 1, 4000, None), \n        NoamOpt(512, 1, 8000, None),\n        NoamOpt(256, 1, 4000, None)]\nplt.plot(np.arange(1, 20000), [[opt.rate(i) for opt in opts] for i in range(1, 20000)])\nplt.legend([\"512:4000\", \"512:8000\", \"256:4000\"])\nNone","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"label smoothing","metadata":{}},{"cell_type":"code","source":"class LabelSmoothing(nn.Module):\n    \"Implement label smoothing.\"\n    def __init__(self, size, padding_idx, smoothing=0.0):\n        super(LabelSmoothing, self).__init__()\n        self.criterion = nn.KLDivLoss(size_average=False)\n        self.padding_idx = padding_idx\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.size = size\n        self.true_dist = None\n        \n    def forward(self, x, target):\n        assert x.size(1) == self.size\n        true_dist = x.data.clone()\n        true_dist.fill_(self.smoothing / (self.size - 2))\n        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        true_dist[:, self.padding_idx] = 0\n        mask = torch.nonzero(target.data == self.padding_idx)\n        if mask.dim() > 0:\n            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n        self.true_dist = true_dist\n        return self.criterion(x, Variable(true_dist, requires_grad=False))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"loss computation","metadata":{}},{"cell_type":"code","source":"class SimpleLossCompute:\n    \"A simple loss compute and train function.\"\n    def __init__(self, generator, criterion, opt=None):\n        self.generator = generator\n        self.criterion = criterion\n        self.opt = opt\n        \n    def __call__(self, x, y, norm):\n        x = self.generator(x)\n        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \n                              y.contiguous().view(-1)) / norm\n        loss.backward()\n        if self.opt is not None:\n            self.opt.step()\n            self.opt.optimizer.zero_grad()\n        return loss.item() * norm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def greedy_decode(model, src, src_mask, max_len, start_symbol):\n    memory = model.encode(src, src_mask)\n    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n    for i in range(max_len-1):\n        out = model.decode(memory, src_mask, \n                           Variable(ys), \n                           Variable(subsequent_mask(ys.size(1))\n                                    .type_as(src.data)))\n        prob = model.generator(out[:, -1])\n        _, next_word = torch.max(prob, dim = 1)\n        next_word = next_word.data[0]\n        ys = torch.cat([ys, \n                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n    return ys","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"training begins","metadata":{}},{"cell_type":"code","source":"pad_idx = hau_lang.word2index[\"<blank>\"]\nprint(eng_lang.n_words, hau_lang.n_words)\nmodel = make_model(eng_lang.n_words,hau_lang.n_words,N=6)\nmodel.cuda()\ncriterion = LabelSmoothing(size=hau_lang.n_words,padding_idx=pad_idx,smoothing=0.1)\ncriterion.cuda()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_opt = NoamOpt(model.src_embed[0].d_model, 1, 4000,\n            torch.optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.98), eps=1e-9))\n\nfor epoch in range(5000):\n  model.train()\n  \n  print(\"train\", run_epoch(train_loader,model,\n           SimpleLossCompute(model.generator,criterion,model_opt)))\n  model.eval()\n  print(\"test\", run_epoch(valid_loader,model,\n           SimpleLossCompute(model.generator,criterion,None)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, data in enumerate(list(valid_loader)[:5]):\n    # print(\"English\", data[0][0])\n    # print(\"Hausa\", data[0][1])\n    src,trg = data[:10][0], data[:10][1]\n    # src,trg = src.cuda(), trg.cuda()\n    # print(len(src), len(trg))\n    batch = Batch(src.cuda(),trg.cuda())\n    src = batch.src[:1]\n    # print(src.cuda(), trg.cuda())\n    src_mask = (src != eng_lang.word2index[\"<blank>\"]).unsqueeze(-2)\n    # print(len(src), len(src_mask))\n    out = greedy_decode(model.cuda(), src, src_mask, \n                        max_len=60, start_symbol=hau_lang.word2index[\"SOS\"])\n    \n    # print(eng_lang.word2index)\n    # print(eng_lang.index2word[src[0, 0].item()])\n\n    # print(out)\n\n    # print(batch.trg.data)\n\n    # print(src[0, 2].item())\n    for i in range(0, src.size(1)):\n        sym = eng_lang.index2word[src[0, i].item()]\n        if sym == \"<blank>\": break\n        print(sym, end =\" \")\n    print()\n    print(\"Translation:\", end=\"\\t\")\n    for i in range(1, out.size(1)):\n        sym = hau_lang.index2word[out[0, i].item()]\n        if sym == \"EOS\": break\n        print(sym, end =\" \")\n    print()\n    print(\"Target:\", end=\"\\t\")\n    for i in range(1, batch.trg.size(1)):\n        \n        sym = hau_lang.index2word[batch.trg.data[0,i].item()]\n        if sym == \"EOS\": break\n        print(sym, end =\" \")\n    print()\n    break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://heartbeat.fritz.ai/exploring-language-models-for-neural-machine-translation-part-one-from-rnn-to-transformers-3e53b7d8a01f","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}