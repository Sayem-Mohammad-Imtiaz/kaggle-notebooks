{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Basic Idea on Neural Network \n\nIn this notebook , we'll build neural network and use it to predict daily bike rental ridership.This notebook gives you a general idea of Neural Network e.g How they build and how they work.\n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%matplotlib inline\n\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load and Prepare Data\n\nA critical step in working with neural networks is preparing the data correctly. Variables on different scales make it difficult for the network to efficiently learn the correct weights.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rides_df = pd.read_csv(\"/kaggle/input/bike-sharing-dataset/hour.csv\")\nrides_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rides_df.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This dataset has the number of riders for each hour of each day from `January 1 2011` to `December 31 2012`. The number of riders is split between casual and registered, summed up in the cnt column. You can see the first few rows of the data above.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Below plot show the number of bike riders over the first 5(approx) days.\nThis data is pretty complicated! The weekends have lower over all ridership and there are spikes when people are biking to and from work during the week. Looking at the data above, we also have information about temperature, humidity, and windspeed, all of these likely affecting the number of riders.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rides_df[:125].plot(x='dteday', y='cnt')\nplt.xticks([]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dummy Variables\n\nHere we have some categorical variables like season, weather, month. To include these in our model, we'll need to make binary dummy variables.\n\nWe are using Pandas `get_dummies()` methode to make dummy variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy_variables = ['season', 'weathersit', 'mnth', 'hr', 'weekday']\nfor variable in dummy_variables:\n    dummies = pd.get_dummies(rides_df[variable], prefix=variable, drop_first=False)\n    rides_df = pd.concat([rides_df, dummies], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now remove the variables which are not useful in dataframe.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"variables_to_drop = ['instant', 'dteday', 'season', 'weathersit', \n                  'weekday', 'atemp', 'mnth', 'workingday', 'hr']\ndata = rides_df.drop(variables_to_drop, axis=1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Normalization\n\nTo make training the network easier, we'll standardize each of the continuous variables. That is, we'll shift and scale the variables such that they have zero mean and a standard deviation of 1.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#features which needs Normalization\nquant_features = ['casual', 'registered', 'cnt', 'temp', 'hum', 'windspeed']\n# Store scalings in a dictionary so we can convert back later\nscaled_features = {}\nfor feature in quant_features:\n    mean, std = data[feature].mean(), data[feature].std()\n    scaled_features[feature] = [mean, std]\n    data.loc[:, feature] = (data[feature] - mean)/std","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting the data into training, testing, and validation sets\n\nWe'll save the data for the last approximately 25 days to use as a test set after we've trained the network. We'll use this set to make predictions and compare them with the actual number of riders.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save data for approximately the last 25 days \ntest_data = data[-25*24:]\n\n# Now remove the test data from the data set \ndata = data[:-25*24]\n\n# Separate the data into features and targets\ntarget_fields = ['cnt', 'casual', 'registered']\n\nfeatures  = data.drop(target_fields, axis=1)\ntargets = data[target_fields]\n\ntest_features  = test_data.drop(target_fields, axis=1) \ntest_targets = test_data[target_fields]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll split the data into two sets, one for training and one for validating as the network is being trained. Since this is time series data, we'll train on historical data, then try to predict on future data (the validation set).\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hold out the last 60 days or so of the remaining data as a validation set\ntrain_features = features[:-60*24]\ntrain_targets =  targets[:-60*24]\n\nval_features = features[-60*24:]\nval_targets = targets[-60*24:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building The Network\n\nThe network has two layers, a hidden layer and an output layer. The hidden layer will use the sigmoid function for activations. The output layer has only one node and is used for the regression, the output of the node is the same as the input of the node. That is, the activation function is ùëì(ùë•)=ùë•.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class NeuralNetwork(object):\n    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n        # Set number of nodes in input, hidden and output layers.\n        self.input_nodes = input_nodes\n        self.hidden_nodes = hidden_nodes\n        self.output_nodes = output_nodes\n\n        # Initialize weights\n        self.weights_input_to_hidden = np.random.normal(0.0, self.input_nodes**-0.5, (self.input_nodes, self.hidden_nodes))\n        self.weights_hidden_to_output = np.random.normal(0.0, self.hidden_nodes**-0.5,(self.hidden_nodes, self.output_nodes))\n        self.lr = learning_rate\n        # Replace 0 with your sigmoid calculation.\n        self.activation_function = lambda x : 1 / (1 + np.exp(-x))                      \n\n    def train(self, features, targets):\n        \n        n_records = features.shape[0]\n        \n        delta_weights_i_h = np.zeros(self.weights_input_to_hidden.shape)\n        delta_weights_h_o = np.zeros(self.weights_hidden_to_output.shape)\n        \n        for X, y in zip(features, targets):\n            # Implement the forward pass function below\n            final_outputs, hidden_outputs = self.forward_pass_train(X)  \n            # Implement the backproagation function below\n            delta_weights_i_h, delta_weights_h_o = self.backpropagation(final_outputs, hidden_outputs, X, y,delta_weights_i_h, delta_weights_h_o)\n        self.update_weights(delta_weights_i_h, delta_weights_h_o, n_records)\n\n\n    def forward_pass_train(self, X):\n        # signals into hidden layer\n        hidden_inputs = np.dot(X,self.weights_input_to_hidden) \n        # signals from hidden layer\n        hidden_outputs = self.activation_function(hidden_inputs)        \n\n        # signals into final output layer\n        final_inputs = np.dot(hidden_outputs,self.weights_hidden_to_output) \n        # signals from final output layer\n        final_outputs = final_inputs \n        \n        return final_outputs, hidden_outputs\n\n    def backpropagation(self, final_outputs, hidden_outputs, X, y, delta_weights_i_h, delta_weights_h_o):\n        # Output layer error is the difference between desired target and actual output.\n        error = y-final_outputs \n        hidden_error = np.dot(self.weights_hidden_to_output, error)\n        output_error_term = error * 1.0\n        \n        hidden_error_term = hidden_error * hidden_outputs * (1 - hidden_outputs)\n        \n        delta_weights_i_h += hidden_error_term * X[:, None]\n        delta_weights_h_o += output_error_term * hidden_outputs[:, None]\n        return delta_weights_i_h, delta_weights_h_o\n\n    def update_weights(self, delta_weights_i_h, delta_weights_h_o, n_records):\n        # update hidden-to-output weights with gradient descent step\n        self.weights_hidden_to_output += self.lr * delta_weights_h_o / n_records\n        # update input-to-hidden weights with gradient descent step\n        self.weights_input_to_hidden += self.lr * delta_weights_i_h / n_records \n        \n    def run(self, features):\n        # signals into hidden layer\n        hidden_inputs = np.dot(features, self.weights_input_to_hidden)\n        # signals from hidden layer\n        hidden_outputs = self.activation_function(hidden_inputs) \n        \n        # signals into final output layer\n        final_inputs = np.dot(hidden_outputs, self.weights_hidden_to_output)\n         # signals from final output layer\n        final_outputs = final_inputs  \n        \n        return final_outputs\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Set Hyperparameters\n\n> ‚ÄúA good choice of hyperparameters can really make an algorithm shine‚Äù.\n\nChoosing appropriate hyperparameters plays a crucial role in the success of our neural network architecture. Since it makes a huge impact on the learned model.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#hyperparameters\niterations = 200\nlearning_rate = 0.1\nhidden_nodes = 5\noutput_nodes = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Mean Square Error\ndef MSE(y, Y):\n    return np.mean((y-Y)**2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training the network\n\nWe are using Stochastic Gradient Descent (SGD) to train the network. The idea is that for each training pass, you grab a random sample of the data instead of using the whole data set. You use many more training passes than with normal gradient descent, but each pass is much faster. This ends up training the network more efficiently.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\n\nN_i = train_features.shape[1]\n#Build a Network Object\nnetwork = NeuralNetwork(N_i, hidden_nodes, output_nodes, learning_rate)\n#Store Loss for training and validation.\nlosses = {'train':[], 'validation':[]}\nfor i in range(iterations):\n    # Go through a random batch of 128 records from the training data set\n    batch = np.random.choice(train_features.index, size=128)\n    X, y = train_features.loc[batch].values, train_targets.loc[batch]['cnt']\n                             \n    network.train(X, y)\n    \n    # Printing out the training progress\n    train_loss = MSE(network.run(train_features).T, train_targets['cnt'].values)\n    val_loss = MSE(network.run(val_features).T, val_targets['cnt'].values)\n    sys.stdout.write(\"\\rProgress: {:2.1f}\".format(100 * i/float(iterations)) \\\n                     + \"% ... Training loss: \" + str(train_loss)[:5] \\\n                     + \" ... Validation loss: \" + str(val_loss)[:5])\n    sys.stdout.flush()\n    \n    losses['train'].append(train_loss)\n    losses['validation'].append(val_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(losses['train'], label='Training loss')\nplt.plot(losses['validation'], label='Validation loss')\nplt.legend()\n_ = plt.ylim()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction\n\nHere, we use the test data to view how well our network is modeling the data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(8,4))\nmean, std = scaled_features['cnt']\n#predict on test data \npredictions = network.run(test_features).T*std + mean\nax.plot(predictions[0], label='Prediction')\nax.plot((test_targets['cnt']*std + mean).values, label='Data')\nax.set_xlim(right=len(predictions))\nax.legend()\n\ndates = pd.to_datetime(rides_df.loc[test_data.index]['dteday'])\ndates = dates.apply(lambda d: d.strftime('%b %d'))\nax.set_xticks(np.arange(len(dates))[12::24])\n_ = ax.set_xticklabels(dates[12::24], rotation=45)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's all . Change hyperparameters and see the change in accuracy of model.\n\n# Thanks !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}