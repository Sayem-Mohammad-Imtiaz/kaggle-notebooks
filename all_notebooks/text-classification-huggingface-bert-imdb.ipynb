{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 参考 https://www.topbots.com/fine-tune-transformers-in-pytorch/\n\nimport io\nimport os\nimport sys\nimport random\nimport time\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom collections import OrderedDict\nimport re, string\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom transformers import (AutoConfig, \n                          AutoModelForSequenceClassification,\n                          AutoTokenizer, AdamW,\n                          get_linear_schedule_with_warmup,\n                          set_seed,\n                         )\n\ndef seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    # some cudnn methods can be random even after fixing the seed\n    # unless you tell it to be deterministic\n    torch.backends.cudnn.deterministic = True\n    \n!mkdir ./model_bakup/\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('device is', device)\n\nclass CFG:\n    batch_size = 8\n    lr = 0.01\n    eval_step_num = 20\n    mid_eval = False\n    best_eval_acc = 0.0\n    model_output_dir = './model_bakup/'\n    seed = 2032\n    use_ema = False\n    use_adversial_training = True\n    use_lr_scheduler = True\n    model_name_or_path = 'bert-base-cased'\n    #model_name_or_path = 'bert-large-cased'\n    \nDEBUG_RUN = False\n\nglobal_start_t = time.time()\nprint('ok')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-16T08:54:05.534321Z","iopub.execute_input":"2021-06-16T08:54:05.534736Z","iopub.status.idle":"2021-06-16T08:54:13.611752Z","shell.execute_reply.started":"2021-06-16T08:54:05.534655Z","shell.execute_reply":"2021-06-16T08:54:13.610402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_everything(seed=42)\n\nimdb_data = pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\nimdb_data['sentiment'] = imdb_data['sentiment'].map({'positive': 1, 'negative': 0})\nprint('before drop_duplicates, imdb_data.shape: ', imdb_data.shape)\nimdb_data = imdb_data.drop_duplicates()\nprint('after drop_duplicates, imdb_data.shape: ', imdb_data.shape)\nimdb_data = imdb_data.sample(30000)\nprint('after sample, imdb_data.shape: ', imdb_data.shape)\nimdb_data = imdb_data.sample(len(imdb_data)).reset_index(drop=True)  # shuffle\n\nimdb_data.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T08:54:13.613785Z","iopub.execute_input":"2021-06-16T08:54:13.614142Z","iopub.status.idle":"2021-06-16T08:54:15.079516Z","shell.execute_reply.started":"2021-06-16T08:54:13.614099Z","shell.execute_reply":"2021-06-16T08:54:15.078734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TRAIN_NUM = 15000\nTRAIN_NUM = 2500\nimdb_data_test = imdb_data.iloc[:5000]\nimdb_data_valid = imdb_data.iloc[5000:10000]\nimdb_data_train = imdb_data.iloc[10000:TRAIN_NUM+10000]\n\nif DEBUG_RUN:\n    SAMPLE_NUM = 300\n    imdb_data_test = imdb_data_test.sample(SAMPLE_NUM)\n    imdb_data_valid = imdb_data_valid.sample(SAMPLE_NUM)\n    #imdb_data_train = imdb_data_train.sample(2*SAMPLE_NUM)\n\nprint(f'imdb_data_train.shape: {imdb_data_train.shape}, imdb_data_valid.shape: {imdb_data_valid.shape}, '\n      f'imdb_data_test.shape: {imdb_data_test.shape}')\n\n# imdb_data_train.head(10)\nimdb_data_test.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T08:54:15.08112Z","iopub.execute_input":"2021-06-16T08:54:15.081364Z","iopub.status.idle":"2021-06-16T08:54:15.095243Z","shell.execute_reply.started":"2021-06-16T08:54:15.081339Z","shell.execute_reply":"2021-06-16T08:54:15.094203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data_test.tail(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T08:54:15.097248Z","iopub.execute_input":"2021-06-16T08:54:15.09774Z","iopub.status.idle":"2021-06-16T08:54:15.108679Z","shell.execute_reply.started":"2021-06-16T08:54:15.097702Z","shell.execute_reply":"2021-06-16T08:54:15.107718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg = CFG()\nseed_everything(seed=cfg.seed)\n\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-06-16T08:54:15.110194Z","iopub.execute_input":"2021-06-16T08:54:15.1106Z","iopub.status.idle":"2021-06-16T08:54:15.119924Z","shell.execute_reply.started":"2021-06-16T08:54:15.110561Z","shell.execute_reply":"2021-06-16T08:54:15.118838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MovieReviewsDataset(Dataset):\n    def __init__(self, data_df, use_tokenizer, max_sequence_len=None):\n        self.data_df = data_df\n        max_sequence_len = use_tokenizer.max_len if max_sequence_len is None else max_sequence_len\n        texts = list(data_df['review'].values)\n        labels = list(data_df['sentiment'].values)\n        self.n_examples = len(labels)\n        self.inputs = use_tokenizer(texts, add_special_tokens=True, truncation=True, padding=True, \n                                    return_tensors='pt', max_length=max_sequence_len)\n        self.sequence_len = self.inputs['input_ids'].shape[-1]\n        self.inputs.update({'labels': torch.tensor(labels)})\n    \n    def __len__(self):\n        return self.n_examples\n    \n    def __getitem__(self, item):\n        return {key: self.inputs[key][item] for key in self.inputs.keys()}\n    \ndef train(dataloader, optimizer, scheduler, device):\n    global model, global_step_num, global_best_val_acc, cfg\n    predictions_labels = []\n    true_labels = []\n    total_loss = 0\n    model.train()\n    for batch in dataloader:\n        global_step_num += 1\n        true_labels += batch['labels'].numpy().flatten().tolist()\n        batch = {k: v.type(torch.long).to(device) for k, v in batch.items()}\n        model.zero_grad()\n        outputs = model(**batch)\n        loss, logits = outputs[:2]\n        total_loss += loss.item()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        if scheduler is not None:\n            scheduler.step()\n        logits = logits.detach().cpu().numpy()\n        predictions_labels += logits.argmax(axis=-1).flatten().tolist()\n        \n        if cfg.mid_eval and (global_step_num % cfg.eval_step_num == 0):\n            valid_labels, valid_predict, val_loss = validation(dataloader_valid, device)\n            val_acc = accuracy_score(valid_labels, valid_predict)\n            print(f'step_num: {global_step_num}, val_acc: {val_acc:.5f}')\n            if val_acc > global_best_val_acc:\n                global_best_val_acc = val_acc\n                print(f'step_num: {global_step_num}, get new best val_acc: {val_acc:.5f}, save the model now!')                \n                torch.save(model.state_dict(), os.path.join(cfg.model_output_dir, 'best_step_model.pth'))\n        \n    avg_epoch_loss = total_loss / len(dataloader)\n    \n    return true_labels, predictions_labels, avg_epoch_loss\n\ndef validation(dataloader, device):\n    global model\n    predictions_labels, true_labels = [], []\n    total_loss = 0\n    model.eval()\n    for batch in dataloader:\n        true_labels += batch['labels'].numpy().flatten().tolist()\n        batch = {k: v.type(torch.long).to(device) for k,v in batch.items()}\n        with torch.no_grad():\n            outputs = model(**batch)\n            loss, logits = outputs[:2]\n            logits = logits.detach().cpu().numpy()\n            total_loss += loss.item()\n            predict_content = logits.argmax(axis=-1).flatten().tolist()\n            predictions_labels += predict_content\n            \n    model.train()  # 将模型重新置为训练状态\n    avg_epoch_loss = total_loss / len(dataloader)\n    return true_labels, predictions_labels, avg_epoch_loss\n\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2021-06-16T08:54:15.121586Z","iopub.execute_input":"2021-06-16T08:54:15.121966Z","iopub.status.idle":"2021-06-16T08:54:15.142548Z","shell.execute_reply.started":"2021-06-16T08:54:15.121928Z","shell.execute_reply":"2021-06-16T08:54:15.141552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Loading configuration...')\nmax_length = 500\n\nmodel_config = AutoConfig.from_pretrained(pretrained_model_name_or_path=cfg.model_name_or_path,\n                                          num_labels=2)\ntokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=cfg.model_name_or_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path=cfg.model_name_or_path,\n                                                           config=model_config)\nmodel.to(device)\n\nmodel_param_num = sum(p.numel() for p in model.parameters())\nmodel_trainable_param_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint('model_param_num: ', model_param_num, 'model_trainable_param_num: ', \n      model_trainable_param_num)\n\ndataset_train = MovieReviewsDataset(imdb_data_train,\n                                    use_tokenizer=tokenizer,\n                                    max_sequence_len=max_length)\ndataloader_train = DataLoader(dataset_train, batch_size=cfg.batch_size, shuffle=True)\n\ndataset_valid = MovieReviewsDataset(imdb_data_valid,\n                                    use_tokenizer=tokenizer,\n                                    max_sequence_len=max_length)\ndataloader_valid = DataLoader(dataset_valid, batch_size=2*cfg.batch_size, shuffle=False)\n\ndataset_test = MovieReviewsDataset(imdb_data_test,\n                                    use_tokenizer=tokenizer,\n                                    max_sequence_len=max_length)\ndataloader_test = DataLoader(dataset_test, batch_size=2*cfg.batch_size, shuffle=False)\n\nprint('len of dataloader_train: ', len(dataloader_train),\n      'len of dataloader_valid: ', len(dataloader_valid),\n      'len of dataloader_test: ', len(dataloader_test))\n\nprint('ok')\n\n# bert-base-cased model \n# model_param_num:  108311810 model_trainable_param_num:  108311810\n# len of dataloader_train:  188 len of dataloader_valid:  47 len of dataloader_test:  47\n\n# bert-large-cased model\n# model_param_num:  333581314 model_trainable_param_num:  333581314\n# len of dataloader_train:  188 len of dataloader_valid:  47 len of dataloader_test:  47","metadata":{"execution":{"iopub.status.busy":"2021-06-16T08:54:15.144024Z","iopub.execute_input":"2021-06-16T08:54:15.144479Z","iopub.status.idle":"2021-06-16T08:54:56.239526Z","shell.execute_reply.started":"2021-06-16T08:54:15.144439Z","shell.execute_reply":"2021-06-16T08:54:56.238082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 3\noptimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8, weight_decay=1e-5)\ntotal_steps = len(dataloader_train) * epochs\nlr_scheduler = None\nif cfg.use_lr_scheduler:\n    lr_scheduler = get_linear_schedule_with_warmup(optimizer, \n                        num_warmup_steps=int(0.1 * total_steps), \n                        num_training_steps=total_steps)\n    \nglobal_step_num = 0\nglobal_best_val_acc = 0.0\nall_loss = {'train_loss': [], 'val_loss': []}\nall_acc = {'train_acc': [], 'val_acc': []}\n\nfor epoch in range(epochs):\n    train_labels, train_predict, train_loss = train(dataloader_train, optimizer, lr_scheduler, device)\n    train_acc = accuracy_score(train_labels, train_predict)\n    print(f'epoch: {epoch}, train_acc: {train_acc:.5f}')\n    \n    valid_labels, valid_predict, val_loss = validation(dataloader_valid, device)\n    val_acc = accuracy_score(valid_labels, valid_predict)\n    \n    test_labels, test_predict, test_loss = validation(dataloader_test, device)\n    test_acc = accuracy_score(test_labels, test_predict)\n    \n    print(f'epoch: {epoch}, train_acc: {train_acc:.5f}, val_acc: {val_acc:.5f}, test_acc: {test_acc:.5f}')\n    if val_acc > global_best_val_acc:\n        global_best_val_acc = val_acc\n        print(f'at the end of epoch, step_num: {global_step_num}, get new best val_acc: {val_acc:.5f}, save the model now!')                \n        torch.save(model.state_dict(), os.path.join(cfg.model_output_dir, 'best_step_model.pth'))\n    \nprint('ok')\n\n# epoch: 0, train_acc: 0.46000\n# epoch: 1, train_acc: 0.46000\n# epoch: 2, train_acc: 0.64000\n\n###########################################################\n\n# epoch: 0, train_acc: 0.46000\n# epoch: 0, train_acc: 0.46000, val_acc: 0.53140, test_acc: 0.51260\n# at the end of epoch, step_num: 7, get new best val_acc: 0.53140, save the model now!\n# epoch: 1, train_acc: 0.60000\n# epoch: 1, train_acc: 0.60000, val_acc: 0.54500, test_acc: 0.53620\n# at the end of epoch, step_num: 14, get new best val_acc: 0.54500, save the model now!\n# epoch: 2, train_acc: 0.74000\n# epoch: 2, train_acc: 0.74000, val_acc: 0.53600, test_acc: 0.53240\n\n###########################################################","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-16T08:54:56.241658Z","iopub.execute_input":"2021-06-16T08:54:56.242003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del model  # 删除原来的模型，以减少GPU内存占用\nmodel = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path=cfg.model_name_or_path,\n                                                           config=model_config)\nmodel.to(device)\nmodel.load_state_dict(torch.load(os.path.join(cfg.model_output_dir, 'best_step_model.pth')))\n\ntest_labels, test_predict, test_loss = validation(dataloader_test, device)\ntest_acc = accuracy_score(test_labels, test_predict)\nvalid_labels, valid_predict, valid_loss = validation(dataloader_valid, device)\nvalid_acc = accuracy_score(valid_labels, valid_predict)\nprint(f'TRAIN_NUM: {TRAIN_NUM} final test_acc: {test_acc:.5f}, valid_acc: {valid_acc:.5f}, global_best_val_acc: {global_best_val_acc:.5f}')\n\nprint('total finished, cost time: ', time.time() - global_start_t)\n\n# final test_acc: 0.52200, valid_acc: 0.51940\n\n#######################################################\n# TRAIN_NUM: 50 final test_acc: 0.53620, valid_acc: 0.54500, global_best_val_acc: 0.54500\n# total finished, cost time:  301.0138831138611\n\n# TRAIN_NUM: 200 final test_acc: 0.70560, valid_acc: 0.70340, global_best_val_acc: 0.70340  no_mid_eval\n# TRAIN_NUM: 200 final test_acc: 0.71860, valid_acc: 0.71540, global_best_val_acc: 0.71540  mid_eval\n\n# TRAIN_NUM: 500 final test_acc: 0.85720, valid_acc: 0.84540, global_best_val_acc: 0.84540  no_mid_eval","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}