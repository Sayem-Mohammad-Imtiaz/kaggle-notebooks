{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#    Previsão de *Default* em Financiamento Bancário"},{"metadata":{},"cell_type":"markdown","source":"## Por aluna matrícula 1931133087 "},{"metadata":{},"cell_type":"markdown","source":"O presente trabalho utiliza o dataset Home Equity DAtaset(HMEQ) disponível no Kaggle para, mediante modelo de Machine Learning, prever o Default ou não do financiamento bancário contratado pelo cliente.\n\nO Dataset HMEQ apresenta informações de 5960 contratantes e de seus financiamentos bancários distribuídas nas 12 variáveis abaixo e também a variável resposta \"BAD\" que informa se o cliente deixou de honrar seu empréstimo (default) ou não.\n\nAs variáveis preditoras constantes da base de dados são:\n\n* LOAN: Valor da parcela (valor $);\n\n* MORTDUE: Valor devido na hipoteca existente (valor $);\n\n* VALUE: Valor corrente da propriedade financiada (valor $);\n\n* REASON: Destinação do crédito (se debt consolidatio= DebtCon ou home improvement= HomeImp;\n\n* JOB: Ocupação. Possui sex classes: Other, ProfExe, Office, Mgr, Self e Sales;\n\n* YOJ: Anos no emprego atual (em anos);\n\n* DEROG: Número de relatórios depreciativos;\n\n* DELINQ: Número de linhas de crédito inadimplentes;\n\n* CLAGE: Idade da linha de crédito mais antiga;\n\n* NINQ: Número de linhas de crédito recentes;\n\n* CLNO: Número de linhas de crédito;\n\n* DEBTIC : Razão Dívida/Rendimento.\n\nA variável resposta BAD é binária e apresenta as seguintes classes: 0 - o cliente não ficou inadimplente e 1 - o cliente apresentou Default. Logo, é esta variável que conterá as previsões realizadas.\n"},{"metadata":{},"cell_type":"markdown","source":"# 1. Carregamento da Base de Dados HMEQ"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Carregamento do Dataset:\ndf = pd.read_csv('/kaggle/input/hmeq-data/hmeq.csv')\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A Base de Dados possui 5960 observações e 13 colunas"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizando os dados:\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando uma amostra aleatória\ndf.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando os tipos dos dados e os tamanhos\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nesta visualização, verifica-se grande quantidade de colunas com dados 'missing'. Aliás, somente a coluna com a variável resposta 'BAD e a 'VALUE' não demandam tratamento.\nAlém disso, há duas variáveis do tipo 'object', ou seja, não numéricas. \nLogo, o tratamento dos dados se destinará a solucionar estas duas situações para que seja viável a aplicação do modelo escolhido."},{"metadata":{},"cell_type":"markdown","source":"# 2. Tratamento dos Dados"},{"metadata":{},"cell_type":"markdown","source":"## 2.1. Tratamento dos dados Faltantes"},{"metadata":{},"cell_type":"markdown","source":"### 2.1.1. Exclusão de observações com informações sobre o financiamento nulas.\n\nEstudando os dados, entendeu-se que as observações onde as variáveis 'MORTDUE' (valor devido) e 'VALUE' (valor da propriedade) estão com dados nulos (em conjunto), devem ser excluídas pois não agregam informações relevantes para o estudo de predição, podendo até distorcer, caso haja imputação.\n\nDesta forma, inicialmente será feito a verificação das observações com esta característica e após, sua exclusão."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando quantas observações possuem esta característica: \ndf[df['MORTDUE'].isnull() & df['VALUE'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Serão excluídos os 27 registros que possuem esta caracteristica:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Usando o comando drop para excluir os registros que possuem valores nulos nas 2 varíaveis com dados \n#sobre o atual financiamento.\ndf.dropna(how='all', subset = ['MORTDUE', 'VALUE'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# conferindo se o comando de exclusão deu certo: \ndf[df['MORTDUE'].isnull() & df['VALUE'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.1.2 Imputação de dados faltantes\n\n"},{"metadata":{},"cell_type":"markdown","source":"#### VARIÁVEIS MORTDUE E VALUE\nAinda existem muitos casos de valores nulos nas colunas 'MORTDUE' e 'VALUE'. Para imputar os valores destas colunas adotaremos o seguinte:\nCaso de nulo no valor da propriedade (VALUE): Será imputado o valor da dívida atual (MORTDUE), garantindo assim que o valor da propriedade será ao menos o valor da dívida.\nCaso de nulo no valor da dívida autal (MORTDUE): Será imputado o valor da propriedade garantidora da operação, considerando assim o valor da propriedade como valor estimado do financiamento."},{"metadata":{"trusted":true},"cell_type":"code","source":"# verificando a quantidade de casos enquadrados nesta situação (nulo em MORTDUE ou VALUE:\ndf[df['MORTDUE'].isnull() | df['VALUE'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aqui se verifica que quase 10% da base de dados encontra-se nesta situação (576 registros)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# imputando o valor da Propriedade pelo valor da dívida:\ndf['VALUE'] = df.apply(lambda row: row['MORTDUE'] if np.isnan(row['VALUE']) else row['VALUE'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# imputando o valor da dívida pelo valor da propriedade: \ndf['MORTDUE'] = df.apply(lambda row: row['VALUE'] if np.isnan(row['MORTDUE']) else row['MORTDUE'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Conferindo que não existem mais registros com valores nulos nas colunas MORTDUE e VALUE:\ndf[df['MORTDUE'].isnull() | df['VALUE'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando os primeiros dados após efeito da imputação\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando como ficou o Dataset após esta primeira imputação:\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A base de dados ficou com 27 registros a menos, com nenhum valor nulo nas quatro primeiras colunas."},{"metadata":{},"cell_type":"markdown","source":"VARIÁVEL 'REASON'"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando a quantidade de dados nulos:\ndf[df['REASON'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Verifica-se 242 com dado nulo na variável 'REASON'."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando as classes da variável 'REASON' e a frequência de cada um delas.\ndf['REASON'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aqui verifica-se que a maior frequência é da classe 'DebtCon'."},{"metadata":{},"cell_type":"markdown","source":"Como apoio à decisão sobre imputar os dados missing da variável 'REASON' pelo valor da classe com maior frequência, decidiu-se verificar em uma tabela cruzada se as classes desta variável apresentam grande diferença de proporção na variável resposta. Um desproporção indicaria tentar outro método para imputação visto a possibilidade de distorção na variável resposta.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando as proporção dos dados distribuídos entre as variáveis 'REASON' e 'BAD':\ntotals=pd.crosstab(df['REASON'],df['BAD'],margins=True).reset_index()\npercentages = pd.crosstab(df['REASON'],\n   df['BAD']).apply(lambda row: row/row.sum(),axis=1).reset_index()\ntotals\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando os percentuais cruzados entre 'REASON' e 'BAD':\npercentages","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A análise da tabela cruzada mostrou pouco desbalanceamento entre classes da variável resposta (0,18 e 0,21), então optou-se pela imputação usando a classe de maior frequência, 'DebtCon', para o tratamento dos dados missing."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Realizando a imputação da variável 'REASON':\ndf['REASON'].fillna('DebtCon', inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando como ficou a distribuição de frequências da variável 'REASON':\ndf['REASON'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando como ficou o Dataset:\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"VARIÁVEL 'JOB'"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando as classes e frequência da variável 'JOB':\ndf['JOB'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A classe 'Other' possui a maioria dos registros válidos."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando os registros nulos na variável 'JOB':\ndf[df['JOB'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Verifica-se 267 registros com dado faltante na variável 'JOB'."},{"metadata":{},"cell_type":"markdown","source":"Visto a classe 'Other' possuir a maior parte dos registros, optou-se pela imputação dos valores 'missing' pelo valor desta classe."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Imputação dos valores nulos da variável 'JOB'pelo valor da classe 'Other':\ndf['JOB'].fillna('Other', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando como ficou a distribuição das classes da variável 'JOB':\ndf['JOB'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Verificando como ficou o Dataset:\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"VARIÁVEL 'YOJ'"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando a distribuição da variável:\ndf['YOJ'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando os dados nulos da variável 'YOJ':\ndf[df['YOJ'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Constata-se 505 registros com dados nulos na variável 'YOJ';\nVisto a variável ser contínua, verificou-se a existência de vários domínios, sem predominância de um deles. Desta forma partiu-se para a análise do seu resumo estatístico e de sua distribuição:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando o resumo de medidas estísiticas da variável 'YOJ':\ndf['YOJ'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando uma medida estatística extra, a mediana:\ndf['YOJ'].median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando a distribuição dos dados por meio do Histograma:\ndf['YOJ'].plot.hist(bins=50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Considerando os dados estarem mais distribuídos à esquerda, usaremos a Mediana ao invés da Média para a imputação dos valores missing da coluna \"YOJ\". "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Efetuando a imputação pela Mediana: \ndf['YOJ'].fillna(7, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando o dataset:\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"VARIÁVEIS 'DEROG', 'DELINQ, 'CLAGE', 'NINQ', 'CLNO', DEBTINC':"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando as classes e distribuição da 'DEROG':\ndf['DEROG'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando a quantidade de registros com dados missing na variável 'DEROG': \ndf[df['DEROG'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tendo em vista a quase totalidade dos registros estarem com valor zero para a variável 'DEROG' usaremos a imputação por zero."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando as classes e distribuição da 'DELINQ':\ndf['DELINQ'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando a quantidade de registros com dados missing na variável 'DELINQ': \ndf[df['DELINQ'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tendo em vista, assim como a 'DEROG, que a quase totalidade dos registros estão com valor zero para a variável 'DELINQ' usaremos a imputação por zero"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando as classes e frequência da variável 'CLAGE':\ndf['CLAGE'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['CLAGE'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aqui também trata-se de variável contínua sem predominância de classe. É necessária a análise de suas medidas estatísticas para a tomada de decisão quanto a imputação de dados nulos."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando o resumo de medidas estísiticas da variável 'CLAGE':\ndf['CLAGE'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando a Mediana:\ndf['CLAGE'].median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando a distribuição dos dados por meio do Histograma:\ndf['CLAGE'].plot.hist(bins=50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tendo em vista a distribuição dos dados, para CLAGE usaremos a imputação pela mediana"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando as classes e distribuição entre estas para a variável 'NINQ': \ndf['NINQ'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando os dados nulos para a variável 'NINQ':\ndf[df['NINQ'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sumário estatístico para 'NINQ':\ndf['NINQ'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando a Mediana:\ndf['NINQ'].median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando o Histograma:\ndf['CLAGE'].plot.hist(bins=50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Para NINQ usaremos a mediana, visto os dados estarem fortemente distribuidos à esquerda da curva."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando classes e frequência para variável 'CLNO':\ndf['CLNO'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando os valores Nulos: \ndf[df['CLNO'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando as medidas estatísticas:\ndf['CLNO'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando a Mediana:\ndf['CLNO'].median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando o Histograma:\ndf['CLNO'].plot.hist(bins=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Para CLNO usaremos a mediana. Concentração maior à esquerda."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando as classes e distribuição da variável 'DEBTINC':\ndf['DEBTINC'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A variável DEBTINC é contínua sem classes predominante."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando os valores Nulos: \ndf[df['DEBTINC'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando o resumo estatístico:\ndf['DEBTINC'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando a Mediana: \ndf['DEBTINC'].median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando o histograma: \ndf['DEBTINC'].plot.hist(bins=50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visto os dados não estarem muito dispersos, para a variável DEBTINC usaremos a média, como critério para a imputação dos dados faltantes."},{"metadata":{},"cell_type":"markdown","source":"Finalmente, após o término da análise das seis últimas variáveis, procederemos a imputação conforme a seguir: \nDEROG: por zero;\nDELINQ: por zero;\nCLAGE: pela Mediana;\nNINQ: pela Mediana;\nCLNO: pela Mediana:\nDEBTINC: pela Média."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Imputação dos dados nulos:\ndf['DEROG'].fillna(0, inplace=True)\ndf['DELINQ'].fillna(0, inplace=True)\ndf['CLAGE'].fillna(173.48, inplace=True)\ndf['NINQ'].fillna(1, inplace=True)\ndf['CLNO'].fillna(20, inplace=True)\ndf['DEBTINC'].fillna(33.79, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando o dataset após todo o tratamento de Dados 'Missing's':\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2. ANÁLISE EXPLORATÓRIA DAS VARIÁVEIS EXPLICATIVAS E SUAS RELAÇÕES COM A VARIÁVEL RESPOSTA 'BAD'"},{"metadata":{},"cell_type":"markdown","source":"### 2.2.1. Verificando graficamente a Distribuição da Variável Resposta 'BAD': "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importando \nimport seaborn as sns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando inicialmente a Tabela Cruzada\n\ny = df['BAD'].astype(object) \ncount = pd.crosstab(index = y, columns=\"count\")\npercentage = pd.crosstab(index = y, columns=\"frequency\")/pd.crosstab(index = y, columns=\"frequency\").sum()\npd.concat([count, percentage], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotando o gráfico da Frequência da Variável Resposta 'BAD'\nax = sns.countplot(x=y, data=df).set_title(\"Distribuição da Variável Resposta 'BAD'\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pode-se perceber que o índice de inadimplência ('default') é de 19,70%"},{"metadata":{},"cell_type":"markdown","source":"### 2.2.2. Verificando graficamente a relação da variável resposta 'BAD' e algumas variáveis explicativas"},{"metadata":{},"cell_type":"markdown","source":"'BAD' versus 'JOB'\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotando o Gráfico de Barras Empilhadas mostrando a relação entre a variável 'BAD' e \"JOB\":\nJOB=pd.crosstab(df['JOB'],df['BAD'])\nJOB.div(JOB.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, title='JOB x BAD', figsize=(4,4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No gráfico acima é confrontada a inadimplência (variável 'BAD') com o tipo de ocupação ('JOB'). Verifica-se que o segmento voltado à Vendas (Sales) possui o maior índice de inadimplência, cerca de 30%, sendo que o menor índice de default foi da classe 'Oficce'."},{"metadata":{},"cell_type":"markdown","source":"'BAD' versus 'REASON': \n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"REASON=pd.crosstab(df['REASON'],df['BAD'])\nREASON.div(REASON.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, title='REASON x BAD', figsize=(4,4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"O gráfico acima mostra a relação entre a inadimplência e a finalidade do crédito. Verifica-se que a inadimplência é ligeiramente menor para a finalidade \"DebtConf\".\n"},{"metadata":{},"cell_type":"markdown","source":"'BAD' versus 'MORTDUE'\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.stripplot(x='BAD', y='MORTDUE', data=df, linewidth=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"O gráfico acima mostra a relação entre a inadimplência e o valor do financiamento. Pode-se constatar que com a exceção de 4 outlies com valores elevados de dívida, não existe grande diferença no valor médio do crédito do cliente insolvente para aquele que conseguiu honrá-lo;\n\n"},{"metadata":{},"cell_type":"markdown","source":"'BAD'versus 'DEBTINC':"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.stripplot(x='BAD', y='DEBTINC', data=df, linewidth=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Este gráfico relaciona a variável 'BAD' com a 'DEBTINC'. A variável 'DEBTINC recebe a razão entre o valor dos débitos e os rendimentos do financiado, ou seja, quanto maior o seu valor, maior é endividamento do cliente. Graficamente percebe-se que nenhum clientes na classe que dos que conseguiram pagar seu empréstimo possuem DEBTINC maior que 50. Por outro lado, é grande a quantidade de inadimplentes com valores elevados nesta variável. Portanto, provavelmente esta variável terá grande impacto no modelo de predição. "},{"metadata":{},"cell_type":"markdown","source":"## 2.3. Tranformando as variáveis categóricas em 'dummies'"},{"metadata":{},"cell_type":"markdown","source":"A base de dados conta com duas variáveis do tipo 'object' e que deverão ser tratadas para que se possa utilizá-las no modelo de predição: \"REASON\" e \"JOB\". \nA opção escolhida foi a transformação em 'dummies, conforme é efetuado a seguir:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando as variáveis do dataset:\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transformando as variáveis 'REASON' e 'JOB' em 'dummies':\ndf = pd.get_dummies(df, columns=['REASON', 'JOB'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando como ficou o dataset após o processo de transformações em 'dummies':\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conforme verifica-se acima, o dataset final, tratado e pronto para ser utilizado no modelo, contará com 19 variáveis, todas numéricas. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizando o dataset após o processo de 'dummies':\ndf.head().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Verificação das Correlações entre as variáveis"},{"metadata":{},"cell_type":"markdown","source":"Previamente à aplicação dos modelos de Machine Learning escolhidos para a predição, é importante verificar a colinearidade entre as variáveis explicativas, para que, com a devida exclusão das variáveis correlacionadas, podermos evitar o overfitting do modelo. É o que será realizado neste tópico. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importando o pacote matplotlib\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create Correlation matrix\ncorr = df.corr()\n#Plot figsize\nfig, ax = plt.subplots(figsize=(10,8))\n#Generate Color Map\ncolormap = sns.diverging_palette(220, 10, as_cmap=True)\n#Generate Heat Map, allow annotations and place floats in map\nsns.heatmap(corr, cmap=colormap, annot=True, fmt=\".2f\")\n#Apply xticks\nplt.xticks(range(len(corr.columns)), corr.columns);\n#Apply yticks\nplt.yticks(range(len(corr.columns)), corr.columns)\n#show plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Analisando o Gráfico da Matriz de Correlação, verifica-se que as Variáveis 'MORTDUE' e 'VALUE' possuem forte correlação positiva (0,89). Para evitar distorçoes no modelo, optou-se pela exclusão de uma delas: a 'VALUE'.\n\nDa mesma forma, será excluída da análise da variável dummificada REASON_HomeImp."},{"metadata":{},"cell_type":"markdown","source":"# 4. APLICAÇÃO DO MODELO\n"},{"metadata":{},"cell_type":"markdown","source":"A fim de conseguirmos previsões acerca da possibilidade de 'default' ou não a partir dos dados do dataset, vamos utilizar dois algoritmos diferentes de árvores de decisão: Random Forest e o XGBoost, ambos modelos de Machine Learning. "},{"metadata":{},"cell_type":"markdown","source":"## 4.1. Aplicação do Modelo de Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importando o pacote do Sklearn:\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separando os dados de Treino, Validação e Teste, usando a proporção 80/20:\ntrain, test = train_test_split(df, test_size=0.20, random_state=42)\ntrain, valid = train_test_split(train, test_size=0.20, random_state=42)\ntrain.shape, valid.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Definindo colunas de entrada. Excluiremos a Variável Resposta e as duas com colinearidade:\nfeats = [c for c in df.columns if c not in ['BAD', 'VALUE', 'REASON_HomeImp']]\nfeats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importando o pacote RandomForestClassifier necessário para rodar o modelo:\n\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instanciando o modelo com 200 árvores de decisão\nrf = RandomForestClassifier(n_estimators=200, random_state=42) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Treinando o Modelo: \nrf.fit(train[feats], train['BAD'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fazendo as previsões para os dados de Validação:\npreds_val= rf.predict(valid[feats])\npreds_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importando o pacote necessário para verificarmos a acurácio do modelo\n\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando a predição nos dados de Validação:\naccuracy_score(valid['BAD'], preds_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Encontramos uma acurácia de 0,93 nos dados de validação."},{"metadata":{},"cell_type":"markdown","source":"APURANDO A ACURÁCIA NOS DADOS DE TESTE:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando a acurácia do modelo nos dados de teste:\npreds_test = rf.predict(test[feats])\naccuracy_score(test['BAD'], preds_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A Acurácia encontrada nos dados de teste foi de 0,915754."},{"metadata":{},"cell_type":"markdown","source":"VARIÁVEIS MAIS IMPORTANTES PARA O MODELO"},{"metadata":{"trusted":true},"cell_type":"code","source":"# avaliando a importância de cada coluna (cada variável de entrada)\n\npd.Series(rf.feature_importances_, index=feats).sort_values().plot.barh()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Este gráfico demonstra-se a importância de cada uma das variáveis para a predição realizada. Verifica-se que a variável que teve mais peso no modelo foi a DEBTINC, seguida de MORTDUE, DELINQ e LOAN."},{"metadata":{},"cell_type":"markdown","source":"MATRIZ DE CONFUSÃO"},{"metadata":{"trusted":true},"cell_type":"code","source":"#importando a biblioteca necessária para plotar o gráfico de Matriz de Confusão\n\nimport scikitplot as skplt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gerando a Matriz de Confusão \nskplt.metrics.plot_confusion_matrix(valid['BAD'], preds_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Na diagonal principal da Matriz de Confusão encontram-se as predições corretas realizadas pelo modelo aplicado. Verifica-se 760 acertos de não evento, ou seja, aqueles onde o modelo acertou que não haveria o Default e 126 acertos de evento, quando o modelo acertou a incorrência do default. "},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## 4.2. Aplicação do Modelo XGBoost com Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fazendo uma cópia do Dataset para a aplicação do outro modelo. Chamaremos de df1: \ndf1 = df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separando o dataframe em dados de treino e Teste. Não será apartado dados para validação visto que usaremos\n# a validação cruzada onde faremos várias validações com dados aleatórios do dataset de treino.\n\n# Importando o train_test_split\nfrom sklearn.model_selection import train_test_split\n\n# Separando treino e teste\ntrain, test = train_test_split(df1, test_size=0.20, random_state=42)\n\n# Não vamos mais usar o dataset de validação\n\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# definindo colunas de entrada\n\nfeats = [c for c in df1.columns if c not in ['BAD', 'VALUE', 'REASON_HomeImp']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"APLICAÇÃO DO XGBoost: Árvores encadeadas"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importar o modelo\nfrom xgboost import XGBClassifier\n\n# Instanciar o modelo\nxgb = XGBClassifier(n_estimators=200, n_jobs=-1, random_state=42, learning_rate=0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Usando o Cross validation\n\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(xgb, train[feats], train['BAD'], n_jobs=-1, cv=5) # estimator= xgb\n\n# Definiremos 5 splits para realizar a validação cruzada:\nscores, scores.mean() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"O array acima mostra a acurácia obtida em cada split de validação. O último valor é a média encontrada: 0,912"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Usando o XGB para treinamento e predição \n\nxgb.fit(train[feats], train['BAD'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fazendo predições\npreds = xgb.predict(test[feats])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Medir o desempenho do modelo\nfrom sklearn.metrics import accuracy_score\n\naccuracy_score(test['BAD'], preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comparando o modelo de Random Forest e o XGBoost para os dados estudados, verifica-se que o primeiro possui leve vantagem em termos de acurácia: 0,9157 contra 0,9115"},{"metadata":{},"cell_type":"markdown","source":"VARIÁVEIS MAIS IMPORTANTES PARA O MODELO XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(xgb.feature_importances_, index=feats).sort_values().plot.barh()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Com o algoritmo XGBoost percebemos diferenças quanto à importância de cada uma das variáveis para a predição realizada. Verifica-se que a variável que teve mais peso no modelo também foi a DEBTINC, porém no XGBoost esta vem seguida DELINQ e DEROG."},{"metadata":{},"cell_type":"markdown","source":"# 5.CONCLUSÃO"},{"metadata":{},"cell_type":"markdown","source":"Com a realização do presente estudo, verificou-se que, após a análise e tratamento realizados nos dados originais e aplicação do modelo de Machine Learnig Random Forest (com os parâmetros utilizados acima) foi possível encontrar uma acurácia na predição nos dados de teste de 0,9154, ligeiramente melhor do que o modelo XGBoost usando com validação cruzada. Desta forma, para os dados estudados, o modelo escolhido foi o Random Forest."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}