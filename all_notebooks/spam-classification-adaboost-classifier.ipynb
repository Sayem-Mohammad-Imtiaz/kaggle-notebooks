{"cells":[{"metadata":{"collapsed":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57d5bc841683b7796decb0d7e2124bbf10b37deb","_cell_guid":"c1b71a63-3efc-4236-bba2-d89f2ea97a22"},"cell_type":"markdown","source":"**Importing required packages.**"},{"metadata":{"collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"import itertools\nimport nltk\nimport re\nimport numpy as np\nfrom nltk.stem.snowball import SnowballStemmer,PorterStemmer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize\nimport math\nimport re, string\nfrom numpy.random import multivariate_normal\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.linear_model import Perceptron, SGDClassifier\nfrom sklearn.neighbors import NearestCentroid\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.externals import joblib\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"19e9de896c778abb5686131e6aa8dfb86029519f","_cell_guid":"2e1da8cf-83bd-4b0d-966e-cb43925d6ab5","trusted":false},"cell_type":"code","source":"# Defining runtime parameters\nstemmer = SnowballStemmer(\"english\")\nfeature = 1000\nestimator_number = 1000\ncomponent = 30","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45688f865816525d43a61bd1170cf10c46781a15","_cell_guid":"1787e8b9-4dc1-4a7a-a2da-9241f7e2d600"},"cell_type":"markdown","source":"**Functions which will be required:**"},{"metadata":{"collapsed":true,"_uuid":"13d396550dcb8853a4f0b7f4def81263d723b3f7","_cell_guid":"77ccd678-cb16-4303-809c-e4acd137855b","trusted":false},"cell_type":"code","source":"def resample(mean, cov, number, tag):\n    array = multivariate_normal(mean=mean, cov=cov, size=number)\n    response_new = np.repeat(tag, number)\n    return array, response_new\ndef ReplaceNumbertoWords(text):\n    p = inflect.engine()\n    s = text\n    num = re.findall('\\d+', s)\n    l = text.split(\" \")\n    if len(num) >= 1:\n        # listreplace = [p.number_to_words(n) for n in num]\n        for n in num:\n            replacenumlist = np.where(np.array(l) == n)[0].tolist()\n            for j in replacenumlist:\n                l[j] = p.number_to_words(n)\n        l1 = ' '.join(l)\n    else:\n        l1 = s\n    return l1\ndef transform(sentiment):\n    if sentiment == 'spam':\n        return 1\n    elif sentiment == 'ham':\n        return 0\n\ndef toLower(texts):\n    tokens = sent_tokenize(texts)\n    words = [w.lower() for w in tokens]\n    return \" \".join(words)\n\ndef removeStopWords(text):\n    stop_words = set(stopwords.words('english'))\n    tokens = sent_tokenize(text)\n    words = [w.lower() for w in tokens]\n    words = [w for w in words if not w in stop_words]\n    return \" \".join(words)\n\n\ndef Fit(modelname, name, docs_train, docs_test, y_train, y_test):\n    model = modelname\n    model.fit(docs_train, y_train)\n    y_pred = model.predict(docs_test)\n    y_train_pred = model.predict(docs_train)\n    score = accuracy_score(y_test, y_pred)\n    trainScore = accuracy_score(y_train, y_train_pred)\n    c = confusion_matrix(y_test, y_pred)\n    c = np.round(c / c.sum(1).astype(float).reshape(-1, 1), 3)\n    print (\"ModelName:\", name, \"TestScore:\", score, \"TrainScore:\",trainScore)\n    return [model,name, feature, component, estimator_number, c[0, 0], c[0, 1], c[1, 0], c[1, 1], score,trainScore]\ndef BiasedSamplesBinary(dataset,Responsename,category):\n    X_1 = dataset[dataset[Responsename] == category]     #dataset containing category with max count\n    X_2 = dataset[dataset[Responsename] != category]     #dataset containing category with min count\n    X_1.index = range(0,X_1.shape[0])\n    X_2.index = range(0,X_2.shape[0])\n    smpl = np.random.choice(range(0,X_1.shape[0]),2*X_2.shape[0],replace=False)\n    X = pd.concat([X_1.loc[smpl],X_2],axis=0)\n    X = X.sample(frac=1)\n    return X\n\ndef Bias_Sampling_Check(dataset,Responsename):\n    # global biased_dataset\n    if len(dataset[Responsename].unique()) == 2:\n        max_category = np.argmax(dataset[Responsename].value_counts())\n        ratio = dataset[Responsename].value_counts()[np.argmax(dataset[Responsename].value_counts())]/float(dataset[Responsename].value_counts()[np.argmin(dataset[Responsename].value_counts())])\n        if ratio > 2.0:\n            biased_dataset = BiasedSamplesBinary(dataset,Responsename,max_category)\n            biased_dataset.reset_index(drop = True, inplace = True)\n    return biased_dataset[[Responsename]].values,biased_dataset.drop(Responsename,1).values\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5893e6287bf8cee21c27644f29cc355770ac34eb","_cell_guid":"6ad9c978-730e-4e34-9c79-bcde4ebf7241"},"cell_type":"markdown","source":"**Data Import**"},{"metadata":{"collapsed":true,"_uuid":"2405f4e881402184de3966f6dd39971b240c59fa","_cell_guid":"5108af91-e960-4ca0-b7cd-b6c57df64575","trusted":false},"cell_type":"code","source":"tweet = pd.read_csv('../input/spam.csv', encoding='latin-1')\ntweet = tweet[['v1','v2']]\ntweet.columns = ['sentiment','text']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7eec99c6d2c89d8596f14ccd9e097a6b8aee1ea9","_cell_guid":"467aad4e-65bf-4f01-9b43-c545960f2527"},"cell_type":"markdown","source":" **Preprocessing**"},{"metadata":{"collapsed":true,"_uuid":"29c21e3939b714de14f8e58014587d5b644e9ccb","_cell_guid":"d8946fa5-09ed-4d51-94d7-1baad9a633a2","trusted":false},"cell_type":"code","source":"tweet['text']=[t.replace('@', 'at') for t in tweet['text'].values]\ntweet['text']=[t.replace('$', '') for t in tweet['text'].values]\ntweet['text']=[t.replace('%', ' percent') for t in tweet['text'].values]\ntweet['text']=[t.replace('.', '') for t in tweet['text'].values]\ntweet['text']=[t.replace(',', '') for t in tweet['text'].values]\ntweet['text']=[t.replace('!', ' surprisesituationtag ') for t in tweet['text'].values]\ntweet['text']=[t.replace('#', '') for t in tweet['text'].values]\ntweet['text']=[re.sub(r'https?:\\/\\/.*\\/\\w*','',t) for t in tweet['text'].values]\ntweet['text']=[re.sub(r'['+string.punctuation+']+', ' ',t) for t in tweet['text'].values]\ntweet['text']=[re.sub(r'\\$\\w*','',t) for t in tweet['text'].values]\ntweet['text']=[t.replace('/', '') for t in tweet['text'].values]\ntweet['text']=[t.replace('$', ' dollar ') for t in tweet['text'].values]\ntweet['text']=[t.replace(\"?\", '') for t in tweet['text'].values]\ntweet['text']=[t.replace(\"&\", '') for t in tweet['text'].values]\ntweet['text']=[t.replace('~', 'nearly ') for t in tweet['text'].values]\ntweet['text']=[t.replace('+', ' grow up higher high ') for t in tweet['text'].values]\ntweet['text']=[t.replace('-', ' decline down lower less low') for t in tweet['text'].values]\ntweet['text']=[stemmer.stem(t) for t in tweet['text'].values]\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a050e63e1e7d62bd76389fb258ea5db75705e9ad","_cell_guid":"4760a671-e0a4-46b0-b42d-f1f714dd39b2"},"cell_type":"markdown","source":"**Response creation:**"},{"metadata":{"collapsed":true,"_uuid":"508dc616f203d81830ecf1a1f249cd2c48ec07c7","_cell_guid":"d6fcb19f-4a6e-40a5-bb83-7f54bc13068c","trusted":false},"cell_type":"code","source":"Response_array=np.array([transform(t) for  t in tweet['sentiment'].values])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab2983ce04dcf0216e52c1a293195e4859e87bcb","_cell_guid":"6f0b78b3-80eb-480e-a829-58e2b6bc9116"},"cell_type":"markdown","source":"**Data transformation for modeling**"},{"metadata":{"collapsed":true,"_uuid":"195793dee5495e1ad22601e69f2f1c5cccd1aa15","_cell_guid":"2af24b4a-b3aa-458f-a256-1b8ee8ae9d88","trusted":false},"cell_type":"code","source":"vectorizer = TfidfVectorizer(stop_words='english',smooth_idf=True, max_features = feature)\nvectorizer.fit(tweet['text'].values)\ncorpus_vec = vectorizer.transform(tweet['text'].values).toarray()\ndocs_train, docs_test, y_train, y_test = train_test_split(corpus_vec, Response_array, test_size = 0.20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d848ceb5c272a47b7546cd7e31d94365a0cd8739","_cell_guid":"3b941078-2d81-47be-a62d-c4936fa4c42d"},"cell_type":"markdown","source":"**Model fitting:**"},{"metadata":{"collapsed":true,"_uuid":"d5f8a05272081cd2fa965976c4f86bd5a3b0dbcb","_cell_guid":"2c768455-84f9-40b9-8e89-548520029536","trusted":false},"cell_type":"code","source":"model, name, feature, component, estimator_number, c00, c01, c10, c11, score,trainScore = Fit(AdaBoostClassifier(n_estimators=200,learning_rate=0.9), 'Adaboost Classification', docs_train, docs_test, y_train, y_test)\nprint( \" Actual Ham - Predicted Ham\",c00)\nprint( \" Actual Ham - Predicted Spam\",c01)\nprint( \" Actual Spam - Predicted Ham\",c10)\nprint( \" Actual Spam - Predicted Spam\",c11)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"5f746d7e81555289dd2058a0c563f8cef348eac2","_cell_guid":"2834a25d-9dd4-4bf2-95a2-b4dfb41e184d","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}