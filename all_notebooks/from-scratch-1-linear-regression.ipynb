{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from statistics import stdev, mean\n\nimport pandas as pd\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nfrom IPython.display import Image","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear Regression explanation"},{"metadata":{},"cell_type":"markdown","source":"During Linear Regression we try to find a line that fits a set of data the best. As we try to fit a line to a dataset, many data points will be far away from points of the line.\n\nEquation for the line is $y = mx + b$."},{"metadata":{},"cell_type":"markdown","source":"### Loss"},{"metadata":{},"cell_type":"markdown","source":"Hence we need to compute error (called *Loss*) measuring how bad our line is. We want the distances between point of the line and data point from out set to contribute in the same way, so the errors need to by squared. As we are interested in total value of all distances, they need to be summed.\n\nThe formula is Loss = $\\frac{1}{N}\\sum\\limits_{i=1}^{N}(y_i-(mx_i-b))^2$, where $(mx_i-b)$ is predicted $y_i$.\n\nLet's consider an one line and two points."},{"metadata":{"trusted":true},"cell_type":"code","source":"line_x = [i for i in range(1, 11)]\nline_y = [i*2 for i in range(1, 11)]\nplt.plot(line_x, line_y)\nplt.plot(5,18, \"o\", color=\"red\") \nplt.plot(8,12, \"o\", color=\"green\") \n\n\nx_values = [5, 5]\ny_values = [18, 5*2]\nplt.plot(x_values, y_values, color=\"red\", linestyle='dashed')\n\n\nx_values = [8, 8]\ny_values = [12, 8*2]\nplt.plot(x_values, y_values, color=\"green\", linestyle='dashed')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"error_red = 18-(5*2)\nerror_green = 12-(8*2)\nprint(f'Error for red dot is {error_red} and for green one: {error_green}. Total error is {error_red + error_green}.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"line_x = [i for i in range(1, 11)]\nline_y = [i*2 for i in range(1, 11)]\nplt.plot(line_x, line_y)\nplt.plot(5,18, \"o\", color=\"red\") \nplt.plot(8,12, \"o\", color=\"green\") \n\n\nx_values = [5, 5]\ny_values = [18, 5*2]\nplt.plot(x_values, y_values, color=\"red\", linestyle='dashed')\nplt.annotate(f'dist = {error_red}', xy=(5, 19))\n\n\nx_values = [8, 8]\ny_values = [12, 8*2]\nplt.plot(x_values, y_values, color=\"green\", linestyle='dashed')\nplt.annotate(f'dist = {error_green}', xy=(8, 10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that negative value of second error is dicreasing of importance of total error. To avoid such situations, it's better to take into account squared values."},{"metadata":{"trusted":true},"cell_type":"code","source":"sq_error_red = (18-(5*2))**2\nsq_error_green = (12-(8*2))**2\nprint(f'Squared error for red dot is {sq_error_red} and for green one: {sq_error_green}. Total error is {sq_error_red + sq_error_green}.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gradient descent"},{"metadata":{},"cell_type":"markdown","source":"Gradient descent is an algorithm for finding local minimum of continuous and differentiable function. In case of linear regression, this function is Loss, since we want to have as low errors as possible. In order to use gradient descent algorithm:\n1. We choose supossed b and m.\n2. We find partial derivative for slope, and for intercept (we need to find the derivative to get to know direction in which we will move through Loss function). \n3. Then, according to whether the derivative is negative or positive, we will move through Loss function upward or downward by updating slope and intercept values.\n4. This update looks like: supposed b = supossed b - (learning rate * gradient at b). Respectively for m.\n5. We stop once Loss function stops changing or changes very slow."},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"../input/gradient-chart/gradient.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculating partial derivatives:\n\n$F=\\frac{1}{N}\\sum\\limits_{i=1}^{N}(y_i-(m*x_i+b))^2$\n\ngradient at b = $(\\frac{\\partial F}{\\partial b}=\\frac{1}{N}\\sum\\limits_{i=1}^{N}(y_i-(mx_i+b))^2)^{'}=\\frac{1}{N}\\sum\\limits_{i=1}^{N}2(y_i-(mx_i+b))(-1)=-\\frac{2}{N}\\sum\\limits_{i=1}^{N}(y_i-(mx_i+b))$\n\ngradient at m = $(\\frac{\\partial F}{\\partial m}=\\frac{1}{N}\\sum\\limits_{i=1}^{N}(y_i-(mx_i+b))^2)^{'}=\\frac{1}{N}\\sum\\limits_{i=1}^{N}2(y_i-(mx_i+b))(-x_i)=-\\frac{2}{N}\\sum\\limits_{i=1}^{N}x_i(y_i-(mx_i+b))$"},{"metadata":{},"cell_type":"markdown","source":"### Learning rate"},{"metadata":{},"cell_type":"markdown","source":"It's a rate determining how slow or fast the minimum of Loss function will be found. It can be very small like 0.000001. Then we will have a lot of itreations (steps through Loss function in direction of Loss's minimum), execution will take more time, but it will result in a good accuracy. Reversely for high learning rate."},{"metadata":{},"cell_type":"markdown","source":"## Choose a dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/google-stock-prices/googl_prices.csv', engine='python', sep=r'\\s*,\\s*') # sep=r'\\s*,\\s*' to remove spaces\ndf = df.iloc[::-1] # as we want to start from the earliest dates\ndf.index = df.index[::-1]\ndf = df.iloc[524:] # from 2018 to 2020\ndf.reset_index(inplace=True)\ndf.set_index('index')\ndf = df.drop(columns=['index'])\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As none of variables has missing values nor any significant outliers I may choose whatever variable. Let it be a Close/Last."},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Close/Last\"] = df[\"Close/Last\"].str.replace('$', '') \ndf[\"Close/Last\"] = df[\"Close/Last\"].str.replace(' ', '') # to convert str into float\ndf[\"Close/Last\"] = df[\"Close/Last\"].astype(float)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Date = pd.to_datetime(df.Date) # change to dataframe to have dates visible on plots","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"date = df.Date\nprice = df[\"Close/Last\"]\nplt.figure(figsize=(10,8)) # custom size to improve visibility\nplt.scatter(date, price, s=5) # size of dots = 5 to have them smaller ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Write algorithms from scratch"},{"metadata":{},"cell_type":"markdown","source":"On the basis of above explanation, we will write functions for:\n* loss function --> get_total_loss(x, y, m, b)\n* gradients for m and b --> get_gradient_at_b(x, y, m, b) and get_gradient_at_m(x, y, m, b)\n* b and m update --> update_b_and_m(b, m, x, y, learn_rate)\n* returning the most optimal b and m at minimal loss --> gradient_descent(x, y, learn_rate, num_of_iterations)"},{"metadata":{},"cell_type":"markdown","source":"Let's create some linear function that could approximate (poorly) Google stock prices."},{"metadata":{"trusted":true},"cell_type":"code","source":"index = df.index.tolist()\nm=0.8\nb=800\ny_predicted = [m*x+b for x in index] # y=mx+b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"price = df[\"Close/Last\"]\nplt.figure(figsize=(10,8)) \nplt.scatter(index, price, s=5)  \nplt.plot(index, y_predicted, 'g')\nplt.xlabel('2018-2020')\nplt.ylabel('Google prices in USD')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calculating Loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_total_loss(x, y, m, b, is_standarized):\n    \n    # since we don't want to multiple by datetimes, \n    # we will convert X datapoints into integers and call it x_set\n    if is_standarized == False:\n        x_set = [i for i in range(len(x))][1:]\n        x_set.append(len(x))\n    else:\n        x_set = x\n    y_predicted = [m*x+b for x in x_set]\n    loss = 0\n    \n    for i in range(len(x_set)):\n        # we iterate through each price and calculate distance \n        # between real price and predicted one determined by the straight line\n        loss += (y[i] - y_predicted[i])**2 \n        \n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = date\ny = price\nm=0.8\nb=800\n\nprint(f'Total loss is: {get_total_loss(x, y, m, b,0)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calculating both gradients"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_gradient_at_b(x, y, m, b, is_standarized):\n    \n    if is_standarized == False:\n        x_set = [i for i in range(len(x))][1:]\n        x_set.append(len(x))\n    else:\n        x_set = x\n        \n    y_set = y\n    gradient = 0\n    \n    for i in range(len(x_set)):\n        x = x_set[i]\n        y = y_set[i]\n        y_predicted = m*x+b\n        gradient += y-y_predicted\n        \n    return gradient*(-2/len(x_set))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_gradient_at_m(x, y, m, b, is_standarized):\n    \n    if is_standarized == False:\n        x_set = [i for i in range(len(x))][1:]\n        x_set.append(len(x))\n    else:\n        x_set = x\n        \n    y_set = y\n    gradient = 0\n    \n    for i in range(len(x_set)):\n        x = x_set[i]\n        y = y_set[i]\n        y_predicted = m*x+b\n        gradient += x*(y-y_predicted)\n        \n    return gradient*(-2/len(x_set))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = date\ny = price\nm=0.8\nb=800\n\nprint(f'gradient at b : {get_gradient_at_b(x, y, m, b, 0)}')\nprint(f'gradient at m : {get_gradient_at_m(x, y, m, b, 0)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Updating b and m"},{"metadata":{"trusted":true},"cell_type":"code","source":"def update_b_and_m(b, m, x, y, learn_rate, is_standarized):\n    \n    gradient_b = get_gradient_at_b(x, y, m, b, is_standarized)\n    gradient_m = get_gradient_at_m(x, y, m, b, is_standarized)\n    \n    b = b-(learn_rate*gradient_b)\n    m = m-(learn_rate*gradient_m)\n    \n    return (b, m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = date\ny = price\nm=0.8\nb=800\nlearn_rate = 0.01\nis_standarized = 0\nb, m = update_b_and_m(b, m, x, y, learn_rate, is_standarized)\nprint(f'b: {b}, m: {m}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Combining all and getting the most optimal b and m "},{"metadata":{"trusted":true},"cell_type":"code","source":"def gradient_descent(x, y, learn_rate, num_of_iterations, is_standarized):\n    \n    \n    outcome = pd.DataFrame(data={'b': [], 'm': [], 'loss_set': []})\n    m = 0 # initial value\n    b = 0\n    \n    # fill in the dict with b, m, and loss values\n    for i in range(num_of_iterations):\n        b, m = update_b_and_m(b, m, x, y, learn_rate, is_standarized)\n        loss = get_total_loss(x, y, m, b, is_standarized)\n        outcome = outcome.append({'loss_set': loss, 'b': b, 'm': m}, ignore_index=1)\n    \n    loss = min(outcome.loss_set)\n    \n    b = outcome.b[outcome['loss_set'] == loss].values[0]\n    m = outcome.m[outcome['loss_set'] == loss].values[0]\n    \n    return [b, m, loss]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = date\ny = price\nlearn_rate = 0.01\nnum_of_iterations = 100\nis_standarized =  0 \nb, m, loss = gradient_descent(x, y, learn_rate, num_of_iterations, is_standarized)\nprint(f'The most optimal: b: {b}, m: {m}, loss: {loss}.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can notice that above algorithm doesn't output relevant results.\n\nLet's execute above function for more proportional values from x and y datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"x = [1,2,3]\ny = [i*2 for i in x]\nlearn_rate = 0.01\nnum_of_iterations = 1000\nis_standarized = 0\n\nb, m, loss = gradient_descent(x, y, learn_rate, num_of_iterations, is_standarized)\nprint(f'The most optimal: b: {b}, m: {m}, loss: {loss}.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how it looks on the graph."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = [m*i+b for i in x]\nplt.plot(x, y_pred)\nplt.scatter(x, y, s=20, c='r')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's easy to realize that b and m as the outputs of gradient_descent() function approximate linear function y=mx+b very well."},{"metadata":{},"cell_type":"markdown","source":"## Fix unrelevant output for Googl stock prices"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Min Googl price is {min(price)} and max: {max(price)} of dollars.')\nprint(f'Length of X dataset: {len(date)}.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's wonder why gradient_descent() output looks fine for our second dummy dataset, but looks quite strange for Googl stock prices.\n\nOur dummy dataset contains proportional data - y value is always 2 times greater that corresponding x value.\nIn the case of Googl, prices fall into [681.14, 1795.36] of dollars. Our X dataset that is taken into account while get_descent() execution takes in values from 1 to 1259. "},{"metadata":{},"cell_type":"markdown","source":"### Normalization"},{"metadata":{},"cell_type":"markdown","source":"The solution to have lesser spread is **normalization**. Let's transform our data so they fall into range [0,1].\nThere are a lot of normalization algorithms that fit to specific tasks. \n\nI will choose **min-max** algorithm, since we don't have any outliers (if we had them, we could use Z-score algorithm), and we want to obtain exact same scale.\n* min-max normalization = $\\frac{value - min}{max - min}$, where *value* is an each value from dataset that will be standarize and *min* is minimum value from dataset (inversely for *max*)."},{"metadata":{},"cell_type":"markdown","source":"#### Min-max normalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"def min_max(data):\n    \n    minimum = min(data)\n    maximum = max(data)\n    \n    normalized = []\n    \n    for value in data:\n        normalized.append((value-minimum)/(maximum-minimum))\n        \n    return normalized","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = date\nx_set = [i for i in range(len(x))][1:]\nx_set.append(len(x))\n\nmin_max_df = pd.DataFrame(data={\n    'Price before': price, \n    'Price after': min_max(price),\n    'X before': x_set,\n    'X after': min_max(x_set),\n})\n\nmin_max_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's run get_descent() again."},{"metadata":{"trusted":true},"cell_type":"code","source":"x = min_max_df['X after']\ny = min_max_df['Price after']\nlearn_rate = 0.01\nnum_of_iterations = 100\nis_standarized = 1\nb, m, loss = gradient_descent(x, y, learn_rate, num_of_iterations, is_standarized)\nprint(f'The most optimal: b: {b}, m: {m}, loss: {loss}.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = [m*i+b for i in x]\nplt.plot(x, y_pred)\nplt.scatter(x, y, s=5) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The last thing is transforming *m*, *b* and *loss* back."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}