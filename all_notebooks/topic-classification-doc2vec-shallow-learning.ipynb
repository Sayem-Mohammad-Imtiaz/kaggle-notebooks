{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ipython-autotime\n%load_ext autotime","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/topic-balaned-dataset/topic_balanced_aug.csv')\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['topic'].value_counts() # checking data balance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nfrom gensim.parsing.preprocessing import remove_stopwords\n# remove stopwords and punctuation\ndef preprocess_sentence(sentence):\n    return remove_stopwords(sentence.lower()).translate(str.maketrans('', '', string.punctuation)).strip()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# testing the string preprocessing function\ndic = {'title': [\"Kareem-buggy sentence . , large full of punct?u@ations\", 'another given?.,*^% sentence! is am are']}\npd.DataFrame(dic)['title'].apply(preprocess_sentence)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['title'] = df['title'].apply(preprocess_sentence)\ndf['topic'] = df['topic'].apply(str.lower) \ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Doc2vec embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\ndef cosine(u, v):\n    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = df['title'].tolist()\n# Tokenization of each document\ntokenized_sentences = []\nfor s in sentences:\n    tokenized_sentences.append(word_tokenize(s.lower()))\ntokenized_sentences[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models.doc2vec import Doc2Vec, TaggedDocument\ntagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_sentences)]\ntagged_data[0:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ## Train doc2vec model\n# model = Doc2Vec(tagged_data, vector_size = 20, window = 2, min_count = 1, epochs = 100, workers=4)\n\n# '''\n# vector_size = Dimensionality of the feature vectors.\n# window = The maximum distance between the current and predicted word within a sentence.\n# min_count = Ignores all words with total frequency lower than this.\n# alpha = The initial learning rate.\n# '''\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.save('topic_doc2vec.model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load pretrained model made using cells above in some older notebook\nmodel = Doc2Vec.load('/kaggle/input/news-topic-classification-doc2vec-model/topic_doc2vec.model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#transform training data\ntrain_vectors = []\nfor t in tokenized_sentences:\n    train_vectors.append(model.infer_vector(t))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_vectors[0:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings = pd.DataFrame(train_vectors)\nembeddings['topic'] = df['topic']\nembeddings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save embeddings to be able to use them later\nembeddings.to_csv('topic_doc2vec_embeddings.csv', index=False, header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import linear_model, datasets\nlogreg = linear_model.LogisticRegression()\nlogreg.fit(train_vectors, df['topic'])\nprint(\n    \"Logistic Regression classification accuracy on training data:\\n\",\n    logreg.score(train_vectors,df['topic'])*100,\"%\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}