{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Import Libraries**","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport time\nimport shutil\n\nimport random\nimport pickle\n\nfrom tqdm import tqdm as print_progress\nfrom glob import glob\n\nimport dask.dataframe as dd\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom IPython.display import display, HTML","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ['TF_KERAS'] = '1'\n\nimport tensorflow as tf\n\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.metrics import TopKCategoricalAccuracy\nfrom tensorflow.keras.losses import categorical_crossentropy\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler, Callback\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import (\n    Layer, \n    Input, InputLayer, Embedding, \n    Dropout, Dense, \n    Dot, Concatenate, Average, Add,\n    Bidirectional, LSTM,\n    Lambda, Reshape\n)\nfrom tensorflow.keras.activations import softmax, sigmoid\nfrom tensorflow.keras.initializers import Identity, GlorotNormal\nfrom tensorflow.keras.utils import plot_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # detect and init the TPU\n# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n# tf.config.experimental_connect_to_cluster(tpu)\n# tf.tpu.experimental.initialize_tpu_system(tpu)\n\n# # instantiate a distribution strategy\n# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install stellargraph","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from stellargraph.layer import GraphAttention\nfrom stellargraph.utils import plot_history","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install gradient-centralization-tf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Load data**","metadata":{}},{"cell_type":"code","source":"datasets_path = '../input/hotel-comment'\nsample_dfs = dict()\nfor dataset in ['training', 'valuating', 'testing']:\n    print(f'\\n\\n\\nProcessing {dataset}-set ...')\n    sample_dfs[dataset] = dd.read_csv(\n        os.path.join(datasets_path, f'{dataset}_data*.csv')).compute()\n    print(f\"{dataset}-set contains {len(sample_dfs[dataset])} samples\")\n    print(sample_dfs[dataset].sample(n=3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename = os.path.join(datasets_path, 'label_encoder.pkl')\nlabel_encoder = pickle.load(open(filename, 'rb'))\nlabels = list(label_encoder.classes_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Pretrained BERT**","metadata":{}},{"cell_type":"code","source":"pip install keras-bert","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import backend as K\n\nfrom keras_bert import (\n    PretrainedList, \n    get_pretrained, \n    get_checkpoint_paths,\n    load_trained_model_from_checkpoint, \n    load_vocabulary,\n    extract_embeddings,\n    Tokenizer\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_version = 'uncased_L-4_H-512_A-8'\nbert_model_path = os.path.join('../input/bert-pretrained', bert_version)\npaths = get_checkpoint_paths(bert_model_path)\n\nbert_model = load_trained_model_from_checkpoint(\n    config_file=paths.config,\n    checkpoint_file=paths.checkpoint,\n    output_layer_num=1,\n)\n\nvocabs = load_vocabulary(paths.vocab)\ntokenizer = Tokenizer(vocabs, cased=False if 'uncased' in bert_version else True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_word_embeddings(sentences: list, tokenizer, bert_model,\n                            seq_len: int=512, n_pads=10, reduce_output: bool=False):\n    tokens, segments, n_tokens = [], [], []\n    # Tokenize and numberize tokens\n    for sentence in sentences:\n        token, segment = tokenizer.encode(sentence, max_len=seq_len)\n        tokens.append(token)\n        segments.append(segment)\n        n_tokens.append(min(seq_len, np.count_nonzero(token)+n_pads))\n                \n    # 0-padding\n    for i in range(len(tokens)):\n        tokens[i].extend([0] * (seq_len-len(tokens[i])))\n        segments[i].extend([0] * (seq_len-len(segments[i])))\n        \n    # Get predictions by batch\n    tokens, segments = np.array(tokens), np.array(segments)\n    predictions = bert_model.predict([tokens, segments])\n    if not reduce_output:\n        return predictions\n    \n    # Clip predictions for less memory storage\n    outputs = []\n    for prediction, len_pred in zip(list(predictions), n_tokens):\n        outputs.append(prediction[:len_pred, :])\n    return outputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_vector = process_word_embeddings(labels, tokenizer, bert_model, reduce_output=True)\nlabels_vector = [np.mean(l[~np.all(l==0, axis=1)], axis=0) for l in labels_vector]\nlabels_matrix = np.vstack(labels_vector)\nlabels_matrix = np.expand_dims(labels_matrix, axis=0)\n# np.save(os.path.join(datasets_path, 'labels_embeddings.npy'), labels_matrix)\nlabels_matrix.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Generator**","metadata":{}},{"cell_type":"code","source":"import sklearn\nfrom ast import literal_eval\nfrom tensorflow.keras.utils import Sequence, to_categorical\n\nclass DataGenerator(Sequence):\n\n    def __init__(self, data_df: pd.DataFrame,\n                       tokenizer,\n                       word_embedder,\n                       labels_fixed: np.array,\n                       batch_size: int = 64, \n                       shuffle: bool = True):\n        self.data_df = data_df\n        if len(labels_fixed.shape) == 2:\n            labels_fixed = np.expand_dims(labels_fixed, axis=0)\n        elif len(labels_fixed.shape) != 3:\n            raise ValueError(\"Shape of `labels_fixed` must be 2D or 3D\")\n        self.labels_fixed = labels_fixed\n        self.tokenizer = tokenizer\n        self.word_embedder = word_embedder\n        self.max_seq_length, \\\n        self.embedding_dim = K.int_shape(self.word_embedder.outputs[0])[1:]\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.indices = np.array(list(self.data_df.index))\n        self.on_epoch_end()\n\n    def __len__(self):\n        \" Denotes the number of batches per epoch \"\n        return int(len(self.data_df) // self.batch_size)\n\n    def __getitem__(self, index):\n        \" Generate single batch of data \"\n        # Generate indexes of the batch\n        start_index = self.batch_size * index\n        end_index = self.batch_size * (index+1)\n        indices = self.indices[start_index:end_index]\n\n        # Generate data\n        samples = self.data_df.loc[indices, ['Comment', 'label_encoder']].copy()\n        labels = samples.label_encoder.values.tolist()\n        texts = samples.Comment.values.tolist()\n        embeddings = process_word_embeddings(texts, self.tokenizer, self.word_embedder, reduce_output=False)\n\n        # Encoding multi-class labels\n        mClss_labels = []\n        for l in labels:\n            l = literal_eval(l) if ',' in l else [int(ch) for ch in l[1:-1].split()]\n                \n            # Build multi-class labels\n            mtc = np.sum(to_categorical(l, num_classes=self.labels_fixed.shape[-2]), axis=0)\n            mClss_labels += [self.smooth_labels(mtc)]\n        mClss_labels = np.array(mClss_labels)\n\n        del samples, labels, texts\n        _ = gc.collect()\n        return [embeddings, self.labels_fixed], mClss_labels\n\n    def smooth_labels(self, labels, factor=0.1):\n        \" Smooth the labels \"\n        labels *= (1 - factor)\n        labels += (factor / labels.shape[-1])\n        return labels\n\n    def on_epoch_end(self):\n        \" Update indices after each epoch \"\n        if self.shuffle:\n            self.indices = sklearn.utils.shuffle(self.indices)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_generator = dict()\nfor dataset in ['training', 'valuating', 'testing']:\n    dset_fn = os.path.join(datasets_path, f'{dataset}_data*.csv')\n    dset = dd.read_csv(dset_fn).compute()\n    data_generator[dataset] = DataGenerator(dset, \n                                            tokenizer=tokenizer,\n                                            word_embedder=bert_model, \n                                            labels_fixed=labels_matrix, \n                                            batch_size=64, \n                                            shuffle=True if dataset=='training' else False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data_generator['training']), len(data_generator['valuating'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Load Model**","metadata":{}},{"cell_type":"code","source":"class CyclicLR(Callback):\n    \"\"\"\n    This callback implements a cyclical learning rate policy (CLR).\n    The method cycles the learning rate between two boundaries with some constant frequency, \n        as detailed in this paper (https://arxiv.org/abs/1506.01186).\n    The amplitude of the cycle can be scaled on a per-iteration or per-cycle basis.\n    \n    This class has three built-in policies, as put forth in the paper.\n    \"triangular\":\n        A basic triangular cycle w/ no amplitude scaling.\n    \"halving\":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    \"exponential\":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n        cycle iteration.\n\n    For more detail, please read the paper.\n    \n    # Example\n        ```python\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., mode='triangular')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```\n    \n    Class also supports custom scaling functions:\n        ```python\n            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., scale_fn=clr_fn,\n                                scale_mode='cycle')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```    \n    # Arguments\n        base_lr: initial learning rate which is the\n            lower boundary in the cycle.\n        max_lr: upper boundary in the cycle. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore \n            max_lr may not actually be reached depending on\n            scaling function.\n        step_size: number of training iterations per\n            half cycle. Authors suggest setting step_size\n            2-8 x training iterations in epoch.\n        mode: one of {original, halving, exponential}.\n            Default 'original'.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n        gamma: constant in 'exp_range' scaling function:\n            gamma**(cycle iterations)\n        scale_fn: Custom scaling policy defined by a single\n            argument lambda function, where \n            0 <= scale_fn(x) <= 1 for all x >= 0.\n            mode paramater is ignored \n        scale_mode: {'cycle', 'iterations'}.\n            Defines whether scale_fn is evaluated on \n            cycle number or cycle iterations (training\n            iterations since start of cycle). Default is 'cycle'.\n    \"\"\"\n    def __init__(self, base_lr=0.001, max_lr=0.1, step_size=2000., mode='original',\n                 gamma=1., scale_fn=None, scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn == None:\n            if self.mode == 'halving':\n                self.scale_fn = lambda x: 1/(2.**(x-1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exponential':\n                self.scale_fn = lambda x: gamma**(x)\n                self.scale_mode = 'iterations'\n            else:\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None, new_step_size=None):\n        \"\"\"\n        Resets cycle iterations.\n            Optional boundary/step size adjustment.\n        \"\"\"\n        if new_base_lr is not None:\n            self.base_lr = new_base_lr\n        if new_max_lr is not None:\n            self.max_lr = new_max_lr\n        if new_step_size is not None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n        \n    def clr(self):\n        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n        \n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())        \n            \n    def on_batch_end(self, epoch, logs=None):        \n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        \n        new_lr = self.clr()\n        K.set_value(self.model.optimizer.lr, new_lr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Adjacency(Layer):\n\n    def __init__(self, nodes=1, weights=None, init_method='identity'):\n        super(Adjacency, self).__init__()\n\n        self.shape = (1, nodes, nodes)\n\n        if weights is not None:\n            assert weights.shape==(nodes, nodes), \\\n                f'Adjacency Matrix must have shape ({nodes}, {nodes})' + \\\n                f' while its shape is {weights.shape}'\n            w_init = tf.convert_to_tensor(weights)\n        else:\n            init_method = init_method.lower()\n            if init_method == 'identity':\n                initializer = tf.initializers.Identity()\n            elif init_method in ['xavier', 'glorot']:\n                initializer = tf.initializers.GlorotNormal()\n            w_init = initializer(shape=(nodes, nodes))\n\n        self.w = tf.Variable(\n            initial_value=tf.expand_dims(w_init, axis=0), \n            dtype=\"float32\", trainable=True\n        )\n\n    def call(self, inputs):\n        return tf.convert_to_tensor(self.w)\n\n    def compute_output_shape(self):\n        return self.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def buil_MAGNET(n_labels,\n                embedding_dim: int,\n                sequence_length: int=512, \n                lstm_units: int=64,\n                dropout_rates=[0.2, 0.3],\n                attention_heads=[4, 2],\n                adjacency_matrix=None,\n                adjacency_generation='xavier', # 'identity' or 'xavier' or 'glorot'\n                feed_text_embeddings=True, # if False, add additional Embedding layer\n                text_embeddings_matrix=None, # initialized weights for text Embedding layer\n                feed_label_embeddings=True, # if False, add additional Embedding layer\n                label_embeddings_matrix=None, # initialized weights for label Embedding layer\n                ) -> Model:\n\n    if isinstance(attention_heads, int):\n        attention_heads = [attention_heads, attention_heads]\n    if not isinstance(attention_heads, (list, tuple)):\n        raise ValueError('`attention_heads` must be INT, LIST or TUPLE')\n\n    # 1. Sentence Representation\n    if feed_text_embeddings:\n        sentence_model = Sequential(name='sentence_model')\n        sentence_model.add(Dropout(dropout_rates[0], input_shape=(sequence_length, embedding_dim), name='word_embeddings'))\n        word_inputs, word_embeddings = sentence_model.inputs, sentence_model.outputs\n    else:\n        word_inputs = Input(shape=(sequence_length, ), name='word_inputs')\n        embedding_args = {\n            'input_dim': sequence_length,\n            'output_dim': embedding_dim,\n            'name': 'word_embeddings'\n        }\n        if text_embeddings_matrix is not None \\\n            and text_embeddings_matrix.shape==(sequence_length, embedding_dim):\n            embedding_args['weights'] = [text_embeddings_matrix]\n        word_embeddings = Embedding(**embedding_args)(word_inputs)\n        word_embeddings = Dropout(dropout_rates[0], name='WE_dropout')(word_embeddings)\n\n    forward_rnn = LSTM(units=lstm_units, return_sequences=True, name='forward_rnn')\n    backward_rnn = LSTM(units=lstm_units, return_sequences=True, name='backward_rnn', go_backwards=True)\n    bidir_rnn = Bidirectional(layer=forward_rnn, backward_layer=backward_rnn, merge_mode=\"concat\", name='bidir_rnn')\n    \n    sentence_repr = bidir_rnn(word_embeddings)\n    sentence_repr = K.mean(sentence_repr, axis=1)\n    # print(f\"sentence_repr: {K.int_shape(sentence_repr)}\")\n\n    # 2. Labels Representation\n    if feed_label_embeddings:\n        label_inputs = Input(batch_shape=(1, n_labels, embedding_dim), name='label_embeddings')\n        label_embeddings = label_inputs\n    else:\n        label_inputs = Input(batch_shape=(1, n_labels), name='label_inputs')\n        embedding_args = {'input_dim': n_labels,\n                          'output_dim': embedding_dim,\n                          'name': 'label_embeddings'}\n        if label_embeddings_matrix is not None \\\n            and label_embeddings_matrix.shape==(n_labels, embedding_dim):\n            embedding_args['weights'] = [label_embeddings_matrix]\n        label_embeddings = Embedding(**embedding_args)(label_inputs)\n        label_embeddings = Dropout(rate=dropout_rates[0], name='LE_dropout')(label_embeddings)\n    label_embeddings = Dense(units=embedding_dim//4, name='label_embeddings_reduced')(label_embeddings)\n    # print(f\"label_inputs: {K.int_shape(label_inputs)}\")\n\n    label_correlation = Adjacency(nodes=n_labels, \n                                  weights=adjacency_matrix,\n                                  init_method=adjacency_generation)(label_inputs)\n    # print(f\"label_correlation: {K.int_shape(label_correlation)}\")\n\n    label_attention = GraphAttention(units=embedding_dim//4//attention_heads[0],\n                                     activation='tanh',\n                                     attn_heads=attention_heads[0],\n                                     in_dropout_rate=dropout_rates[1],\n                                     attn_dropout_rate=dropout_rates[1], )([label_embeddings, label_correlation])\n    # print(f\"label_attention: {K.int_shape(label_attention)}\")\n\n    label_residual = Add(name='label_residual')([label_attention, label_embeddings])\n    # print(f\"label_residual: {K.int_shape(label_residual)}\")\n\n    label_repr = GraphAttention(units=2*lstm_units,\n                                activation='tanh',\n                                attn_heads_reduction='average',\n                                attn_heads=attention_heads[1],\n                                in_dropout_rate=dropout_rates[1],\n                                attn_dropout_rate=dropout_rates[1], )([label_residual, label_correlation])\n\n    label_repr = K.sum(label_repr, axis=0, keepdims=False)\n    # print(f\"label_repr: {K.int_shape(label_repr)}\")\n\n    # 3. Prediction\n    prediction = tf.einsum('Bk,Nk->BN', sentence_repr, label_repr)\n    prediction = sigmoid(prediction)\n    # print(f\"prediction: {K.int_shape(prediction)}\")\n\n    return Model(inputs=[word_inputs, label_inputs], outputs=prediction, name='MAGNET')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gctf\n\nclass MAGNET:\n\n    def __init__(self, n_labels: int, embedding_dim: int):\n\n        self.embedding_dim = embedding_dim\n\n        # Build model(s)\n        print(f\"\\n\\n\\nBuilding MAGNET ...\\n\\n\\n\")\n        self.model = buil_MAGNET(n_labels, embedding_dim=embedding_dim, sequence_length=512, lstm_units=32)\n        self.model.summary()\n\n    def compile(self, model_saved: str, logs_path: str, schedule_step: int, verbose: int=1):\n                    \n        # Compile optimizer, loss & metric functions\n        print(f\"Compiling MAGNET using \\n\\tgrad-centralized ADAM, \\n\\ttop-k Accuracy, \\n\\tweighted Cross-Entropy \\n...\")\n        self.model.compile(optimizer=gctf.optimizers.adam(learning_rate=0.001), \n                           # optimizer=Adam(learning_rate=0.001), \n                           metrics=[\"accuracy\", TopKCategoricalAccuracy(k=3)],\n                           loss=categorical_crossentropy)\n\n        # Define Callbacks\n        return [\n            TensorBoard(log_dir=logs_path),\n            # ReduceLROnPlateau(monitor='loss', factor=0.1, patience=3, verbose=verbose),\n            CyclicLR(mode='exponential', base_lr=1e-7, max_lr=1e-3, step_size=schedule_step),\n            ModelCheckpoint(filepath=model_saved, monitor='accuracy', save_weights_only=True, save_best_only=False, save_freq='epoch'),\n            # LearningRateScheduler(noam_scheme),\n            # EarlyStopping(monitor='val_accuracy', mode='max', restore_best_weights=True, min_delta=1e-7, patience=7, verbose=verbose),\n        ]\n\n    def finetune(self, train_generator, val_generator, model_saved: str, logs_path: str, n_loops: int=3, verbose: int=1):\n        # Compile\n        schedule_step = len(train_generator) // 2\n        custom_callbacks = self.compile(model_saved, logs_path, schedule_step, verbose)\n\n        # Define part(s) of layers for fine-tuning\n        label_layers = ['adjacency', 'graph_attention', 'graph_attention_1']\n        sentence_layers = ['bidir_rnn', 'label_embeddings_reduced']\n        train_histories = []\n\n        ######################################\n        #             FINE-TUNING            #\n        ######################################\n\n        print(f\"[Fine-tuning MAGNET]\")\n        train_args = {\n            'generator': train_generator,\n            'steps_per_epoch': len(train_generator),\n            'validation_data': val_generator,\n            'validation_steps': len(val_generator),\n            'callbacks': custom_callbacks\n        }\n        for l in range(n_loops):\n            \n            print(f\"Training loop {l+1}\")\n\n            # Step 1: Train ALL layers\n            for layer in self.model.layers:\n                layer.trainable = True\n\n            print(f\"\\tStep 1: Training ALL layers ...\")\n            train_history = self.model.fit_generator(initial_epoch=l*20, epochs=l*20+5, **train_args)\n            train_histories.append(train_history)\n\n            # Step 2: Train LABEL-ATTENTION layers\n            for layer in self.model.layers:\n                layer.trainable = True if layer.name in label_layers else False\n\n            print(f\"\\tStep 2: Training LABEL-ATTENTION layers ...\")\n            train_history = self.model.fit_generator(initial_epoch=l*20+5, epochs=l*20+10, **train_args)\n            train_histories.append(train_history)\n\n            # Step 3: Train SENTENCE layers\n            for layer in self.model.layers:\n                layer.trainable = True if layer.name in sentence_layers else False\n\n            print(f\"\\tStep 3: Training SENTENCE layers ...\")\n            train_history = self.model.fit_generator(initial_epoch=l*20+10, epochs=l*20+15, **train_args)\n            train_histories.append(train_history)\n\n            # Step 4: Train ALL layers\n            for layer in self.model.layers:\n                layer.trainable = True\n\n            print(f\"\\tStep 4: Training ALL layers ...\")\n            train_history = self.model.fit_generator(initial_epoch=l*20+15, epochs=l*20+20, **train_args)\n            train_histories.append(train_history)\n\n            # Reduce learning rate\n            # custom_callbacks[0].base_lr /= 1.69\n            # custom_callbacks[0].max_lr /= 1.69\n\n        return train_histories\n\n    def train(self, train_generator, val_generator, \n                    model_saved: str, logs_path: str,\n                    max_epochs: int=50, verbose: int=1):\n        # Compile\n        schedule_step = len(train_generator)*2\n        custom_callbacks = self.compile(model_saved, logs_path, schedule_step, verbose)\n\n        # Training\n        train_history = self.model.fit_generator(generator=train_generator,\n                                                 steps_per_epoch=len(train_generator),\n                                                 validation_data=val_generator,\n                                                 validation_steps=len(val_generator),\n                                                 callbacks=custom_callbacks, \n                                                 epochs=max_epochs,\n                                                 initial_epoch=0)\n        return train_history\n\n    def load_weights(self, weight_path: str):\n        self.model.load_weights(weight_path)\n\n    def predict(self, label_embeddings: np.array, sent_embeddings: np.array):\n        sent_embeddings = np.reshape(sent_embeddings, (1, 512, self.embedding_dim))\n        preds = self.model.predict([sent_embeddings, label_embeddings]).tolist()\n        return preds[0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def weighted_cross_entropy(y_true, y_pred, pos_weight=1.618):\n    losses = y_true * -K.log(y_pred) * pos_weight + (1-y_true) * -K.log(1-y_pred)\n    losses = K.clip(losses, 0.0, 9.7)\n    return K.mean(losses)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_LABELS = labels_matrix.shape[1]\nmax_seq_len, embedding_dim = K.int_shape(bert_model.outputs[0])[1:]\n\nmodel = MAGNET(n_labels=N_LABELS, embedding_dim=embedding_dim)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def noam_scheme(global_step, init_lr, warmup_steps=16):\n    \"\"\"\n    Noam scheme learning rate decay\n        init_lr: (scalar) initial learning rate. \n        global_step: (scalar) current training step\n        warmup_steps: (scalar) During warmup_steps, learning rate increases until it reaches init_lr.\n    \"\"\"\n    step = tf.cast(global_step+1, dtype=tf.float32, name=\"global_step\")\n    return init_lr * (warmup_steps**0.5) * tf.minimum(step*(warmup_steps**-1.5), step**-0.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Train**","metadata":{}},{"cell_type":"code","source":"models_path = '/kaggle/working/models'\nif not os.path.isdir(models_path):\n    os.makedirs(models_path)\n\nlogs_path = '/kaggle/working/logs'\nif not os.path.isdir(logs_path):\n    os.makedirs(logs_path)\n    \npred_dir = '/kaggle/working/predictions'\nif not os.path.isdir(pred_dir):\n    os.makedirs(pred_dir)\n    \nmodel_format = 'ep={epoch:03d}_acc={accuracy:.3f}_val_acc={val_accuracy:.3f}_topk={top_k_categorical_accuracy:.3f}_val_topk={val_top_k_categorical_accuracy:.3f}.h5'\nmodel_saved =  os.path.join(models_path, model_format)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_history = model.finetune(data_generator['training'], data_generator['valuating'], \n                               model_saved=model_saved, logs_path=logs_path, n_loops=3, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir(r'/kaggle/working')\ndir_path = '/kaggle/working/'\nshutil.make_archive(dir_path+\"data\", 'zip', dir_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}