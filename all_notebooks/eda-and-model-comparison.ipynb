{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        data = pd.read_csv(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## In this Notebook we are going to perform some EDA and then come with a cool package called Lazypredict which can give us a detailed report of comparisons of various models","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Let's take a look at the dataset first of all\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make all the necessary imports\n\nimport matplotlib.pyplot as plt\n!pip install lazypredict \nfrom lazypredict.Supervised import LazyClassifier\n!pip install plotly\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom plotly.subplots import make_subplots","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting a outlook of our dataset\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**So our data has 303 samples, Good news is that no column has null values so phew! one less thing to worry about**\n\nThe following is the description of dataset as given in the data descrption page\n\nfeatures = \n\n1) age\n\n2) sex\n\n3) chest pain type (4 values)\n\n4) resting blood pressure\n\n5) serum cholestoral in mg/dl\n\n6) fasting blood sugar > 120 mg/dl\n7) resting electrocardiographic results (values 0,1,2)\n\n8) maximum heart rate achieved\n\n9) exercise induced angina\n\n10) oldpeak = ST depression induced by exercise relative to rest\n\n11) the slope of the peak exercise ST segment\n\n12) number of major vessels (0-3) colored by flourosopy\n\n13) thal: 0 = normal; 1 = fixed defect; 2 = reversable defect\n\n14) target: 0= less chance of heart attack 1= more chance of heart attac","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking this out to determine which are to be left as numeric and which to categorical\ndata.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separating out categorical and numerical columns\n\n\n#categorical columns\ncat = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\nnum = [l for l in list(data.columns) if l not in cat]\nprint(\"Numerical columns:\",num)\nprint(\"Categorical columns:\",cat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We will use boxplots to visualize the categorical data's influence on our target, and we will use the plotly package as it makes interactive plots","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n\n\nfig = make_subplots(rows = 2, cols = 3)\n\nfor i, n in enumerate(num):\n    if n != 'target':\n        fig.add_trace(go.Box(x = list(data['target']), y = list(data[n]), name = str(n)),\n                     row = (i//3+1), col = i%3 + 1)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### First thing we can see from the boxplots is that surprisingly enough cholestrol is not a good indicator for our model, Secondly we see too many outliers, so we are going to remove them ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('no of outliers in the thalach', sum(data['thalach']<90))\nprint('no of outliers in the oldpeak', sum(data['oldpeak']>5))\nprint('no of outliers in the chol', sum(data['chol']>400))\nprint('no of outliers in the trestbps', sum(data['trestbps']>190))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Now that we know about the outlier we are going to remove them","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[data['thalach']>90]\ndata = data[data['oldpeak']<5]\ndata = data[data['chol'] < 400]\ndata = data[data['trestbps']<190]\nprint(\"No of rows after removing the outliers\", len(data))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now extracting the categorical and the numerical columns separately so that we can perform the preprocessing accordingly","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dfcat = data[cat]\ndfcat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfnum = data[num]\ndfnum.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#One hot encoding the categorical data\n\nonehot = OneHotEncoder()\nxcat = onehot.fit_transform(dfcat.iloc[:, :].values).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling the numerical features\n\nfor col in dfnum.columns:\n    dfnum[col] = (dfnum[col] - dfnum[col].mean())/dfnum[col].std()\n    \nxnum = dfnum.iloc[:, :-1].values\n\nx = np.concatenate((xcat, xnum), axis = 1)\ny = data.iloc[:, -1].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lazypredict's Model report\n\nxtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.2)\nclf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\nmodels,predictions = clf.fit(xtrain, xtest, ytrain, ytest)\nmodels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am going to update the notebook with some cool charts for categorical features and some feature engineering in a short while... Mean while if you liked the notebook do give an upvote.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}