{"cells":[{"metadata":{"id":"t6_tEx4e_HhS"},"cell_type":"markdown","source":"# **TripAdvisor Hotel Reviews**"},{"metadata":{"id":"6s5gujua-QiV"},"cell_type":"markdown","source":"## Importing Libraries"},{"metadata":{"id":"TPeG3lea8va3","trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom nltk.tag import UnigramTagger\nfrom nltk.corpus import treebank\nimport nltk\nimport re\nimport matplotlib.pyplot as plt\n\nreplacement_patterns = [\n    (r'won\\'t', 'will not'),\n    (r'can\\'t', 'cannot'),\n    (r'i\\'m', 'i am'),\n    (r'ain\\'t', 'is not'),\n    (r'(\\w+)\\'ll', '\\g<1> will'),\n    (r'(\\w+)n\\'t', '\\g<1> not'),\n    (r'(\\w+)\\'ve', '\\g<1> have'),\n    (r'(\\w+)\\'s', '\\g<1> is'),\n    (r'(\\w+)\\'re', '\\g<1> are'),\n    (r'(\\w+)\\'d', '\\g<1> would'),\n]\n\nclass RegexpReplacer(object):\n    def __init__(self, patterns=replacement_patterns): \n        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n    def replace(self, text):\n        s = text\n        for (pattern, repl) in self.patterns:\n            s = re.sub(pattern, repl, s) \n        return s\n\nreplacer=RegexpReplacer()\nreplacer.replace(\"Don't hesistate to ask questions\")\n\nfrom sklearn.model_selection import train_test_split\n\nimport math\nimport random\nfrom collections import defaultdict\nfrom pprint import pprint\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nimport re\nimport string\nfrom sklearn.metrics import mean_squared_error\nimport nltk\n\n# Prevent future/deprecation warnings from showing in output\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\nimport numpy as np\nimport pandas as pd\n\n\nimport matplotlib.pyplot as plt\nfrom nltk.tokenize import TreebankWordTokenizer\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import sentiwordnet as swn\nfrom bs4 import BeautifulSoup             \nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom nltk.corpus import stopwords \nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n\n#Visualization packages\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import mean_squared_error\nimport random\n","execution_count":null,"outputs":[]},{"metadata":{"id":"jXYsyLQI-BqI","outputId":"8f81fc62-df91-46da-a2e9-953049769610","trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('treebank')\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')","execution_count":null,"outputs":[]},{"metadata":{"id":"EYx_aZRS-W-E"},"cell_type":"markdown","source":"## Importing dataset"},{"metadata":{"id":"oXz9776e81WM","trusted":true},"cell_type":"code","source":"#file_id='1DiCkP6qwCxIPK2TuK47US_QTVEminv5T'\n#link='https://drive.google.com/uc?export=download&id={FILE_ID}'\n#csv_url=link.format(FILE_ID=file_id)\n\n#original_dataset = pd.read_csv(csv_url, sep=';', index_col='Unnamed: 0')\n\ncolumn_names = ['reviews.rating','reviews.text']\noriginal_dataset = pd.read_csv('http://christophe-rodrigues.fr/eval_reviews.csv', usecols=column_names, sep=\";\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"zMSdDIE2Z5Dg","outputId":"fbfa6e9c-f7ed-42f3-b232-16595be2ca03","trusted":true},"cell_type":"code","source":"original_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"HvKj4CRc85lk","outputId":"fa4bfcb7-5e82-4250-a477-390fc7c2418f","trusted":true},"cell_type":"code","source":"#Dimension of the dataset\noriginal_dataset.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"WDKYdaUIAE_w","outputId":"ae51ae27-6765-41a4-acf8-cf3406df1b30","trusted":true},"cell_type":"code","source":"original_dataset.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"fljeOpG7AmqG"},"cell_type":"markdown","source":"Let's look at the missing values"},{"metadata":{"id":"t0dxkzu0AGzD","outputId":"06d901a7-0017-42fa-8858-1c4e1d4f63f5","trusted":true},"cell_type":"code","source":"original_dataset.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"i88jdcI9A3_s"},"cell_type":"markdown","source":"##### Let's delete all MoreMore"},{"metadata":{"id":"827DAm7uAMT8","trusted":true},"cell_type":"code","source":"original_dataset = original_dataset[original_dataset['reviews.text']!='MoreMore']","execution_count":null,"outputs":[]},{"metadata":{"id":"3g3SXwFAAQZe","outputId":"04caddf5-1dd1-4c85-d21f-9f1c23e370a3","trusted":true},"cell_type":"code","source":"original_dataset.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"OGUkSvU7A9iX"},"cell_type":"markdown","source":"### Let's look at the distribution of the different grades"},{"metadata":{"id":"lU3ph58QA_r0","outputId":"b21f44b5-6d47-4fed-81a4-642f1106f81c","trusted":true},"cell_type":"code","source":"original_dataset['reviews.rating'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"DXcyRGH5BJWX"},"cell_type":"markdown","source":"We can see that the distribution is unbalanced and  there is a very large part of 5 unlike the note 1 and 2. The opinions are therefore positive in the majority\n"},{"metadata":{"id":"GrfKGhXvE3Ei","outputId":"2ea110fe-2636-4a19-ff12-aa3acaf343e6","trusted":true},"cell_type":"code","source":"original_dataset['reviews.rating'].value_counts().plot.bar(color='blue')","execution_count":null,"outputs":[]},{"metadata":{"id":"Z2wAtz4BBq5r"},"cell_type":"markdown","source":"### Preprocessing function for reviews. We will go through the various stages in order to clean the reviews in the best way.\n- Convert the text to lowercase\n- Removing Numbers\n- Removing white spaces\n- Replacer replace\n- Tokenize into sentences\n- Tokenize into words\n- Remove stop words\n- Lemmatize\n\n\n"},{"metadata":{"id":"Ukz21yL1BCou","trusted":true},"cell_type":"code","source":" \ndef preprocess_text(test):\n\n    #Convert the text to lowercase\n    test = test.lower()\n\n    #Removing Numbers\n    test=re.sub(r'\\d+','',test)\n\n\n    \n    #Removing white spaces\n    test=test.strip()\n    \n    #Replacer replace\n    text_replaced = replacer.replace(test)\n    \n\n    \n    #Tokenize\n    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n    sentences = tokenizer.tokenize(text_replaced)\n\n    #Tokenize words\n    from nltk.tokenize import RegexpTokenizer\n    tokenizer=RegexpTokenizer(\"[\\w]+\")\n\n    for i in range(len(sentences)):\n        sentences[i] = tokenizer.tokenize(sentences[i])\n\n    #Remove stop words\n\n    from nltk.corpus import stopwords\n    stops=set(stopwords.words('english'))\n\n    for i in range(len(sentences)):\n        sentences[i] = [word for word in sentences[i] if word not in stops]\n\n    #Lemmatize\n\n    from nltk.stem import WordNetLemmatizer\n    lemmatizer_output=WordNetLemmatizer()\n\n    for i in range(len(sentences)):\n        for j in range(len(sentences[i])):\n            sentences[i][j] = lemmatizer_output.lemmatize(sentences[i][j])\n\n\n    #Join the words back into a sentence.\n    a=[' '.join(s) for s in sentences]\n    b=['. '.join(a)]\n\n    return b \n","execution_count":null,"outputs":[]},{"metadata":{"id":"6-Eo-T-2B2WO"},"cell_type":"markdown","source":"#### Let's apply this function on reviews"},{"metadata":{"id":"P4SPLvXoBv3S","trusted":true},"cell_type":"code","source":"review_clean = [preprocess_text(doc) for doc in original_dataset['reviews.text']]\nsentences = [' '.join(r) for r in review_clean]\n","execution_count":null,"outputs":[]},{"metadata":{"id":"ot5uBnxCA-PU"},"cell_type":"markdown","source":"Creating a column with cleaned reviews"},{"metadata":{"id":"iC9YKQxIB-SN","outputId":"c1e71faa-5a7b-422d-c780-5c4ce12ab0d0","trusted":true},"cell_type":"code","source":"original_dataset['text_cleaned']=sentences\noriginal_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"rRNQv5aYD-rL"},"cell_type":"markdown","source":"### We will make a copy of the dataset in order to try a binary approach\n"},{"metadata":{"id":"a5KbPNZ5D-Mf","trusted":true},"cell_type":"code","source":"dataset = original_dataset.copy()","execution_count":null,"outputs":[]},{"metadata":{"id":"7ztH69bVEG2B"},"cell_type":"markdown","source":"### We apply the following principle to be able to have a binary model\n- If the score is less than 3 then the score becomes 0\n- Otherwise the note becomes 1"},{"metadata":{"id":"G8IGpj0lCUEE","trusted":true},"cell_type":"code","source":"dataset[dataset['reviews.rating'] != 3]\ndataset['labels'] = np.where(dataset['reviews.rating'] > 2, 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"id":"WuNPiGKmENoJ","outputId":"9e7554bd-7b10-4c8f-f6f2-fb3e82047b7c","trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"2tHobhaPEPAB","outputId":"25a2172c-d7ac-45fa-9fa6-2aa3570ee102","trusted":true},"cell_type":"code","source":"dataset['labels'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"NRB138-BEmWK","outputId":"1861580d-74ee-4307-e12a-7f869abc1aa2","trusted":true},"cell_type":"code","source":"dataset['labels'].value_counts().plot.bar(color='green')","execution_count":null,"outputs":[]},{"metadata":{"id":"W5klujb0FY6b"},"cell_type":"markdown","source":"###### We will therefore use a classification model to predict class 0 or 1"},{"metadata":{"id":"qbrcEa4mMtEY"},"cell_type":"markdown","source":"* We are going to split the dataset into training and test *"},{"metadata":{"id":"ARTrUrhAFX_b","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = dataset.text_cleaned\ny = dataset.labels\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"id":"XVoepJ4fFrZb","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer(max_features=15000, binary=True)\n\nX_train_vect = vectorizer.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"qVG7ET_sQXxp","trusted":true},"cell_type":"code","source":"#Utilisation de smote pour les dataset déséquilibrés\nfrom imblearn.over_sampling import SMOTE\n\nsm = SMOTE()\n\nX_train_res, y_train_res = sm.fit_sample(X_train_vect, y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"YWUxFGtlS2Iy","trusted":true},"cell_type":"code","source":"X_test_vect = vectorizer.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"-u9-znpPCoF6"},"cell_type":"markdown","source":"#### Utilisation du modele Naive bayes"},{"metadata":{"id":"oQRZvtirSVW4","trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nnb = MultinomialNB()\n\nnb.fit(X_train_res, y_train_res)\n\ny_pred = nb.predict(X_test_vect)","execution_count":null,"outputs":[]},{"metadata":{"id":"sNbA7eOgQXtH","outputId":"9648fa38-a221-4662-8150-ce0c3baa64f8","trusted":true},"cell_type":"code","source":"print(\"Accuracy: {:.2f}%\".format(accuracy_score(y_test, y_pred) * 100))","execution_count":null,"outputs":[]},{"metadata":{"id":"RSJMuDbmC2Ti"},"cell_type":"markdown","source":"Faisons quelques tests sur differentes phrases"},{"metadata":{"id":"kK3_ciKvQXrJ","outputId":"cf56ee66-c1e4-4b1b-ebdb-cbb6d9287d2c","trusted":true},"cell_type":"code","source":"print(nb.predict(vectorizer.transform(['this hotel was amazing'])))","execution_count":null,"outputs":[]},{"metadata":{"id":"d15qXMijQXoh","outputId":"3e390895-9376-4daa-c03e-8d29df3c577d","trusted":true},"cell_type":"code","source":"print(nb.predict(vectorizer.transform(['This hotel was a fucking joke, have you ever seen a housekipper that doesn\\'t clean room? '])))","execution_count":null,"outputs":[]},{"metadata":{"id":"snWfsyRwYltL"},"cell_type":"markdown","source":"However we want to predict the grades and not a binary class.\n\nAfter testing a binary model we will make it more complex by going with the real notes and not the binary labels"},{"metadata":{"id":"WO30O_b7Ykgl","outputId":"35887940-6a31-4ec6-b811-0a82418bec00","trusted":true},"cell_type":"code","source":"original_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"2OYNeUZ3Ys7v","trusted":true},"cell_type":"code","source":"x1 = original_dataset['text_cleaned']\ny1 = original_dataset['reviews.rating']","execution_count":null,"outputs":[]},{"metadata":{"id":"CrjqWV74ZEtx"},"cell_type":"markdown","source":"We will create a vectorizer to split the text into unigram and bigrams"},{"metadata":{"id":"IF3V_WeRZD0b","trusted":true},"cell_type":"code","source":"vect = TfidfVectorizer(ngram_range = (1,2))\nx_vect1 = vect.fit_transform(x1)","execution_count":null,"outputs":[]},{"metadata":{"id":"Lo1xqY_nZMqO","trusted":true},"cell_type":"code","source":"x_train_c, x_test_c, y_train_c, y_test_c = train_test_split(x_vect1, y1, test_size=0.15, random_state = 10, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"xCDoJOZlZmry"},"cell_type":"markdown","source":"### Linear SVM pour une classification multiclasse"},{"metadata":{"id":"MS07H9vZZh1o","outputId":"22bb3149-c11a-44ca-c627-703033f0df1c","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nlin_svc_mod = LinearSVC(C=0.13, class_weight=None, dual=True, fit_intercept=True,\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n     multi_class='ovr', penalty='l2', random_state=0, tol=1e-05, verbose=0)\nlin_svc_mod.fit(x_train_c, y_train_c)\npred = lin_svc_mod.predict(x_test_c)\nprint(\"Linear SVC:\",accuracy_score(y_test_c, pred))\nprint(\"MSE: \",mean_squared_error(y_test_c,pred))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"E-LtcO8bhljI","outputId":"720e970b-922a-49d2-8949-1b3e1b820d52","trusted":true},"cell_type":"code","source":"print(lin_svc_mod.predict(vect.transform(['this hotel was horrible'])))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"1FyVTvt4D0wH"},"cell_type":"markdown","source":"Let's take a sentence from a review that does not appear in the dataset and that was also rated on TipAdvisor"},{"metadata":{"id":"Tqztl_JFhRXg","outputId":"ff2270e0-2bc5-414c-e727-384d47c0add2","trusted":true},"cell_type":"code","source":"print(\"Score supposé : 4\")\nprint(\"Score predit : \")\nprint(lin_svc_mod.predict(vect.transform(['loved \tstayed warwick overnight getway enjoy christmas shopping \twarwick exceeded expectations \tstaff wonderful extrememly friendly room clean service lounge wonderful \tcame contact hotel friendly \twomen bathroom lever lounge well.. think haunted totally creepy vibe lights anywho \treally enjoyed stay going couple days \t '])))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"8exHeVvSFV3D"},"cell_type":"markdown","source":"We are approaching the desired class, in fact our model has an accuracy of 60%."},{"metadata":{"id":"usTzv-BZbqJm","outputId":"ba0b203b-a445-4105-9f99-f8d1c17abfd8","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrmfr = RandomForestClassifier()\nrmfr.fit(x_train_c, y_train_c)\npredrmfr = rmfr.predict(x_test_c)\nprint(\"Score:\",round(accuracy_score(y_test_c,predrmfr)*100,2))\nprint(\"MSE: \",mean_squared_error(y_test_c,predrmfr))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"olHPeK7_F0Ga"},"cell_type":"markdown","source":"Let's do a grid search to find out which are the best parameters"},{"metadata":{"id":"n0C94znDKEGV","outputId":"843e0271-7fc5-4ce4-9808-01581ee236b2","trusted":true},"cell_type":"code","source":"\nparameters = {\n    \"n_estimators\":[5,10,50,100,250],\n    \"max_depth\":[2,4,8,16,32,64]\n    \n}\nfrom sklearn.model_selection import GridSearchCV\ncv = GridSearchCV(rmfr,parameters,cv=5)\ncv.fit(x_train_c, y_train_c)","execution_count":null,"outputs":[]},{"metadata":{"id":"OrF72qvUNXrC","trusted":true},"cell_type":"code","source":"def display(results):\n    print(f'Best parameters are: {results.best_params_}')\n    print(\"\\n\")\n    mean_score = results.cv_results_['mean_test_score']\n    std_score = results.cv_results_['std_test_score']\n    params = results.cv_results_['params']\n    for mean,std,params in zip(mean_score,std_score,params):\n        print(f'{round(mean,3)} + or -{round(std,3)} for the {params}')","execution_count":null,"outputs":[]},{"metadata":{"id":"H_EFiwk-NgOA","outputId":"2f2ec0d1-209b-49c6-f18c-e854b0bc7524","trusted":true},"cell_type":"code","source":"display(cv)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"WIXxC-Ph1doB","outputId":"8fdcb113-7d1c-413e-979a-8283004baed0","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrmfrclass = RandomForestClassifier(max_depth = 64,n_estimators = 10 )\nrmfrclass.fit(x_train_c, y_train_c)\npredrmfrclass = rmfrclass.predict(x_test_c)\nprint(\"Score:\",round(accuracy_score(y_test_c,predrmfrclass)*100,2))\nprint(\"MSE: \",mean_squared_error(y_test_c,predrmfrclass))","execution_count":null,"outputs":[]},{"metadata":{"id":"saVzFd-JRGOS"},"cell_type":"markdown","source":"#### Support Vector Machine"},{"metadata":{"id":"KRBC9c5ieDh4","outputId":"180fae85-c36b-4c3d-c21a-5bb32592ff18","trusted":true},"cell_type":"code","source":"\nfrom sklearn.svm import SVC\nsvm = SVC(random_state=101)\nsvm.fit(x_train_c,y_train_c)\npredsvm = svm.predict(x_test_c)\nprint(\"Score:\",round(accuracy_score(y_test_c,predsvm)*100,2))\nprint(\"MSE: \",mean_squared_error(y_test_c,predsvm))","execution_count":null,"outputs":[]},{"metadata":{"id":"pWyRBvmH73zC"},"cell_type":"markdown","source":"## Regression lineaire"},{"metadata":{"id":"IQ8isPDQ77x-","outputId":"ceb4aedd-c0ff-41c3-f849-9b850c940034","trusted":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression()\nreg.fit(x_train_c,y_train_c)\nprint(\"Score: \", reg.score(x_test_c, y_test_c))\npred_lin_reg = reg.predict(x_test_c)\nprint(\"MSE: \",mean_squared_error(y_test_c,pred_lin_reg))","execution_count":null,"outputs":[]},{"metadata":{"id":"YecSPxn0y2hq","outputId":"a6202c6e-fecd-4d49-d023-22262a99d958","trusted":true},"cell_type":"code","source":"#Supposé 2\nprint(reg.predict(vect.transform([' \t1st time seattle delayed anniversary trip wanted stay nicer hotels room reminded holiday inn level hotel \tplain room extra pillows \tbathroom ordinary corian sink ordinary bathroom \troom higher floor looking freeway loud \treason earplugs sleep cd \tasked switch rooms told probably stay way stay 2 nights staying hotel different area town \tluggage room decided eat \tstopped concierge asked good place walk rudely told just walk area \tnot sure concierge doorman just sitting desk expected help \tdecided night hotel come day earlier happily said \tused club points crowne rooms maybe lousy experience opted leave pay room luxury hotel hotel 1000'])))","execution_count":null,"outputs":[]},{"metadata":{"id":"qlnHm9Qw-f0U","trusted":true},"cell_type":"code","source":"#from sklearn import datasets,linear_model\n#from sklearn.model_selection import GridSearchCV\n#parameters = {'kernel':('linear', 'rbf')}\n#svc=linear_model.ARDRegression(n_iter=300,tol=0.001)\n#clf = GridSearchCV(svc, parameters, cv=5)\n#clf","execution_count":null,"outputs":[]},{"metadata":{"id":"3FkOjnedlqwz"},"cell_type":"markdown","source":"## Resume des differents modele de machine learning"},{"metadata":{"id":"T3hk0QYYmaRB"},"cell_type":"markdown","source":"#### The linear SVC has the best precision but the linear regression gives us the smallest MSE"},{"metadata":{"id":"G8kEQqImmpOK"},"cell_type":"markdown","source":"# Improve the model with Deep Learning"},{"metadata":{"id":"9bnBeRAFf5_b","outputId":"281aeb43-fc98-41f8-a883-82cc23b181d5","trusted":true},"cell_type":"code","source":"import keras\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom keras import Sequential\nfrom keras.layers import Embedding\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import Lambda\nfrom keras.layers import LSTM\nimport keras.backend as K\n\nimport nltk\nnltk.download('treebank')\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')","execution_count":null,"outputs":[]},{"metadata":{"id":"CvCGyTmmoPNF"},"cell_type":"markdown","source":"## Data Preprocessing"},{"metadata":{"id":"htSO91UAm3Vp","trusted":true},"cell_type":"code","source":"def create_dataset(num_words, max_text_len, data):\n  res = []\n  for i in tqdm(range(len(data))):\n    res.append([preprocess_text(data.iloc[i][\"reviews.text\"]), data.iloc[i][\"reviews.rating\"]])\n  inp, targ = zip(*res)\n  print(\"Tokenizing the data ...\")\n  tokenizer = keras.preprocessing.text.Tokenizer(num_words = num_words)\n  tokenizer.fit_on_texts([i[0] for i in inp])\n  tensor = [ tokenizer.texts_to_sequences(i)[0] for i in inp]\n  tensor = keras.preprocessing.sequence.pad_sequences(tensor, padding='post',   value=0, maxlen=max_text_len)\n  print(\"Splitting the data into train/val datasets (0.2) ...\")\n  input_tensor_train, input_tensor_test, target_tensor_train, target_tensor_test = train_test_split(tensor, targ, test_size=0.1)\n\n  return (input_tensor_train, np.array(target_tensor_train)-1), (input_tensor_test, target_tensor_test),  tokenizer","execution_count":null,"outputs":[]},{"metadata":{"id":"EgslIYBVoejH"},"cell_type":"markdown","source":"## Hyperparams"},{"metadata":{"id":"cZwqkLrknIM_","trusted":true},"cell_type":"code","source":"vocab_size = 5000\nembed_size = 300\nmax_text_len = 200\nlearning_rate = 0.001\nbatch_size = 128\nn_epochs = 4","execution_count":null,"outputs":[]},{"metadata":{"id":"9RBdUFoynNUs","outputId":"8c6d037c-b441-474e-dd6e-78800e5bb061","trusted":true},"cell_type":"code","source":"train_set, test_set, dictionary = create_dataset(vocab_size, max_text_len, original_dataset)","execution_count":null,"outputs":[]},{"metadata":{"id":"DRgR6DhbotRj"},"cell_type":"markdown","source":"## Model"},{"metadata":{"id":"KgET9W9SnRTD","outputId":"c5e1c40b-4426-4fb2-de1c-dc42774beac2","trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(vocab_size, embed_size, input_shape=(max_text_len,), mask_zero=True))\nmodel.add(LSTM(512, return_sequences=True))\nmodel.add(LSTM(512, return_sequences=True))\nmodel.add(Lambda(lambda x : K.mean(x, axis=1)))\nmodel.add(Dense(256, activation=\"relu\"))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(100, activation=\"relu\"))\nmodel.add(Dropout(0.2))\n#model.add(Dense(50, activation=\"relu\"))\nmodel.add(Dense(5, activation=\"softmax\"))","execution_count":null,"outputs":[]},{"metadata":{"id":"pbiEV9tqnU8o","trusted":true},"cell_type":"code","source":"import tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"id":"PiBSBjIlnm1b","outputId":"83a0e936-ed49-491b-c91d-574c4ccdac6e","trusted":true},"cell_type":"code","source":"# Variable-length int sequences.\nquery_input = tf.keras.Input(shape=(max_text_len,), dtype='int32')\n\n# Embedding lookup.\ntoken_embedding = tf.keras.layers.Embedding(vocab_size, embed_size)\n# Query embeddings of shape [batch_size, Tq, dimension].\nquery_embeddings = token_embedding(query_input)\n# Value embeddings of shape [batch_size, Tv, dimension].\nvalue_embeddings = token_embedding(query_input)\n\n# CNN layer.\ncnn_layer = tf.keras.layers.Conv1D(\n    filters=100,\n    kernel_size=4,\n    # Use 'same' padding so outputs have the same shape as inputs.\n    padding='same')\n# Query encoding of shape [batch_size, Tq, filters].\nquery_seq_encoding = cnn_layer(query_embeddings)\n# Value encoding of shape [batch_size, Tv, filters].\nvalue_seq_encoding = cnn_layer(value_embeddings)\n\n# Query-value attention of shape [batch_size, Tq, filters].\nquery_value_attention_seq = tf.keras.layers.AdditiveAttention()(\n    [query_seq_encoding, value_seq_encoding])\n\n# Reduce over the sequence axis to produce encodings of shape\n# [batch_size, filters].\nquery_encoding = tf.keras.layers.GlobalAveragePooling1D()(\n    query_seq_encoding)\nquery_value_attention = tf.keras.layers.GlobalAveragePooling1D()(\n    query_value_attention_seq)\n\nfcn1 = tf.keras.layers.Dense(256)(query_value_attention)\ndropout1 = tf.keras.layers.Dropout(0.5)(fcn1)\nfcn2 = tf.keras.layers.Dense(100)(dropout1)\ndropout2 = tf.keras.layers.Dropout(0.2)(fcn2)\n\noutput = tf.keras.layers.Dense(5, activation=\"softmax\")(dropout2)\n\nmodel = tf.keras.Model(query_input, output)","execution_count":null,"outputs":[]},{"metadata":{"id":"l8wUzeFBnoB2","outputId":"63acb39c-d4a4-4fc0-c310-6e71d22f9d14","trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"8ZjsOjFlnsST","trusted":true},"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam()","execution_count":null,"outputs":[]},{"metadata":{"id":"EYzIrB6qnyDz","trusted":true},"cell_type":"code","source":"model.compile(loss=\"sparse_categorical_crossentropy\", optimizer = optimizer, metrics=[\"accuracy\"])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}