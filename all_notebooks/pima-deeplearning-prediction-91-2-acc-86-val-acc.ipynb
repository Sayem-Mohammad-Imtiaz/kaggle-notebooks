{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Before starting I would like to give credit to \"Pima Indians Diabetes - EDA & Prediction (0.906) by Vincent Lugat\" for inspiring me to add some of his beautiful EDA in this notebook. \n[https://www.kaggle.com/vincentlugat/pima-indians-diabetes-eda-prediction-0-906/notebook](http://) \n\n### If you like my work please upvote it. Thank You!!","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Data viz. and EDA\nimport matplotlib.pyplot as plt \n%matplotlib inline  \nimport plotly.offline as py\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs,init_notebook_mode,plot, iplot\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\npy.init_notebook_mode(connected=True)\n\n## For scaling data \nfrom mlxtend.preprocessing import minmax_scaling \n\n# Tensorflow \nimport tensorflow as tf\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/pima-indians-diabetes-database/diabetes.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking missing values if any\ndisplay(data.info(),data.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see 9 columns where 'Outcome' specifies whether the person is diabetic or not. \n\nIts great to see that there is no null element present. Thus we do not need to fill or drop empty cells.\nHowever on close inspection I found that there are many '0' values that doesn't make anysense. So we are considering them as null values.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Affected People from Diabeties. ","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"## lets see how many are affected by diabeties \nD = data[data['Outcome'] == 1]\nH = data[data['Outcome'] == 0]\n\n## here I am using graph_obs as I am not able to costimize px. \n\ndef target_count():\n    trace = go.Bar( x = data['Outcome'].value_counts().values.tolist(), \n                    y = ['healthy','diabetic' ], \n                    orientation = 'h', \n                    text=data['Outcome'].value_counts().values.tolist(), \n                    textfont=dict(size=15),\n                    textposition = 'auto',\n                    opacity = 0.5,marker=dict(\n                    color=['lightskyblue', ' indigo'],\n                    line=dict(color='#000000',width=1.5)))\n\n    layout = dict(title =  'Count of affectes females')\n\n    fig = dict(data = [trace], layout=layout)\n    py.iplot(fig)\n\n# --------------- donut chart to show there percentage -------------------- # \n\ndef target_per():\n    trace = go.Pie(labels=['healthy','diabetic' ],values=data['Outcome'].value_counts(),\n                   textfont=dict(size=15),\n                   opacity = 0.5,marker=dict(\n                   colors=['lightskyblue','indigo'],line=dict(color='#000000', width=1.5)),\n                   hole=0.6\n                  )\n    layout = dict(title='Donut chart to see the %age of affected.')\n    fig = dict(data=[trace],layout=layout)\n    py.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_count()\ntarget_per()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## As seen earlier there is no null value. However on close inspection we find that null values are filled with '0'\n\ndata[['Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']] = data[['Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']].replace(0,np.NaN)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Checking the new null values found.\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define missing plot to detect all missing values in dataset\ndef missing_plot(dataset, key) :\n    null_feat = pd.DataFrame(len(dataset[key]) - dataset.isnull().sum(), columns = ['Count'])\n    percentage_null = pd.DataFrame((dataset.isnull().sum())/len(dataset[key])*100, columns = ['Count'])\n    percentage_null = percentage_null.round(2)\n\n    trace = go.Bar(x = null_feat.index, y = null_feat['Count'] ,opacity = 0.8, text = percentage_null['Count'],  textposition = 'auto',marker=dict(color = '#7EC0EE',\n            line=dict(color='#000000',width=1.5)))\n\n    layout = dict(title =  \"Missing Values (count & %)\")\n\n    fig = dict(data = [trace], layout=layout)\n    py.iplot(fig)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_plot(data,'Outcome')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lets first fill null values and then find relations.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## to find the median for filling null values\n\ndef find_median(var):\n    temp = data[data[var].notnull()]\n    temp = data[[var,'Outcome']].groupby('Outcome')[[var]].median().reset_index()\n    return temp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def density_plot(var,size_bin):\n    tmp1 = D[var]\n    tmp2 = H[var]\n    \n    hist_data = [tmp1,tmp2]\n    labels = ['Diabeties','Healthy']\n    color = ['skyblue','indigo']\n    fig = ff.create_distplot(hist_data,labels,colors = color,show_hist=True,bin_size=size_bin,curve_type='kde')\n    \n    fig['layout'].update(title = var)\n\n    py.iplot(fig, filename = 'Density plot')\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"density_plot('Insulin',0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"find_median('Insulin')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Now we will be filling these values instead of null values\n\ndata.loc[(data['Outcome'] == 0) & (data['Insulin'].isnull()), 'Insulin'] = 102.5\ndata.loc[(data['Outcome'] == 1) & (data['Insulin'].isnull()), 'Insulin'] = 169.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SkinThickness density plot \n\ndensity_plot('SkinThickness',0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"find_median('SkinThickness')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Now we will be filling these values instead of null values\n\ndata.loc[(data['Outcome'] == 0) & (data['SkinThickness'].isnull()), 'SkinThickness'] = 27.0\ndata.loc[(data['Outcome'] == 1) & (data['SkinThickness'].isnull()), 'SkinThickness'] = 32.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"density_plot('BloodPressure',0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"find_median('BloodPressure')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[(data['Outcome'] == 0) & (data['BloodPressure'].isnull()), 'BloodPressure'] = 27.0\ndata.loc[(data['Outcome'] == 1) & (data['BloodPressure'].isnull()), 'BloodPressure'] = 32.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"density_plot('BMI',0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"find_median('BMI')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[(data['Outcome'] == 0) & (data['BMI'].isnull()), 'BMI'] = 30.1\ndata.loc[(data['Outcome'] == 1) & (data['BMI'].isnull()), 'BMI'] = 34.3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"density_plot('Glucose',0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"find_median('Glucose')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[(data['Outcome'] == 0) & (data['Glucose'].isnull()) , 'Glucose'] = 107.0\ndata.loc[(data['Outcome'] == 1) & (data['Glucose'].isnull()) , 'Glucose'] = 140.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## lets check if any null value is still left\n\ndisplay(data.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now that we do not have any null values we can start with finding some correlations between the data presents ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def correlation_plot():\n    #correlation\n    correlation = data.corr()\n    #tick labels\n    matrix_cols = correlation.columns.tolist()\n    #convert to array\n    corr_array  = np.array(correlation)\n    trace = go.Heatmap(z = corr_array,\n                       x = matrix_cols,\n                       y = matrix_cols,\n                       colorscale='Viridis',\n                       colorbar   = dict() \n                      )\n    layout = go.Layout(dict(title = 'Correlation Matrix for variables',\n                            #autosize = False,\n                            #height  = 1400,\n                            #width   = 1600,\n                            margin  = dict(r = 0 ,l = 100,\n                                           t = 0,b = 100,\n                                         ),\n                            yaxis   = dict(tickfont = dict(size = 9)),\n                            xaxis   = dict(tickfont = dict(size = 9)),\n                           )\n                      )\n    fig = go.Figure(data = [trace],layout = layout)\n    py.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation_plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## We find 3 pairs which were having good correlations. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_feat1_feat2(feat1, feat2) :  \n    D = data[(data['Outcome'] != 0)]\n    H = data[(data['Outcome'] == 0)]\n    trace0 = go.Scatter(\n        x = D[feat1],\n        y = D[feat2],\n        name = 'diabetic',\n        mode = 'markers', \n        opacity=0.8,\n        marker = dict(color = 'lightskyblue',\n            line = dict(\n                width = 1)))\n\n    trace1 = go.Scatter(\n        x = H[feat1],\n        y = H[feat2],\n        name = 'healthy',\n        opacity=0.8,\n        mode = 'markers',\n        marker = dict(color = 'indigo',\n            line = dict(\n                width = 1)))\n\n    layout = dict(title = feat1 +\" \"+\"vs\"+\" \"+ feat2,\n                  yaxis = dict(title = feat2,zeroline = False),\n                  xaxis = dict(title = feat1, zeroline = False)\n                 )\n\n    plots = [trace0, trace1]\n\n    fig = dict(data = plots, layout=layout)\n    py.iplot(fig)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_feat1_feat2('Pregnancies', 'Age')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Here we can see that people with Age < 35 and Pragnancies < 6 are less likly to be affected with diageties.  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_feat1_feat2('Glucose', 'Insulin')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Here we can see that people with Glucose> 100 and Insulin > 180 are more likly to be affected with diageties.  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_feat1_feat2('SkinThickness', 'BMI')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Here we can see that people with SkinThickness< 30 and BMI <45 are less likly to be affected with diageties.  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Scaling data using minmax_scaling[](http://)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We are doing this step as to to load the data in the model and avoid multi-variable multi-output data problem. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_data = minmax_scaling(data,columns=['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating the binary classifier model.  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model():\n    model = tf.keras.Sequential([\n    tf.keras.layers.Dense(8, activation='relu', input_shape=[len(scaled_data.keys())]),\n    tf.keras.layers.Dense(4, activation='relu'),\n    tf.keras.layers.Dense(1,activation='sigmoid')\n  ])\n\n    optimizer = tf.keras.optimizers.RMSprop(0.01)\n\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return model\n\nmodel = build_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 1000\n\nhistory = model.fit(scaled_data, data['Outcome'],epochs=EPOCHS, validation_split=0.2, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = pd.DataFrame(history.history)\nhist['epoch'] = history.epoch\nhist.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final Training and Validation Accuracy ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = (hist['accuracy'].tail().sum())*100/5 \nval_acc = (hist['val_accuracy'].tail().sum())*100/5 \n\nprint(\"Training Accuracy = {}% and Validation Accuracy= {}%\".format(acc,val_acc))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}