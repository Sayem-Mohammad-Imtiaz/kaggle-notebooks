{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Bayesian Optimization","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import pandas as pd\ndf=pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\ndf['Glucose']=np.where(df['Glucose']==0,df['Glucose'].median(),df['Glucose'])\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=df.drop('Outcome',axis=1)\ny=df['Outcome']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(X,columns=df.columns[:-1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from hyperopt import hp,fmin,tpe,STATUS_OK,Trials","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"space = {'criterion': hp.choice('criterion', ['entropy', 'gini']),\n        'max_depth': hp.quniform('max_depth', 10, 1200, 10),\n        'max_features': hp.choice('max_features', ['auto', 'sqrt','log2', None]),\n        'min_samples_leaf': hp.uniform('min_samples_leaf', 0, 0.5),\n        'min_samples_split' : hp.uniform ('min_samples_split', 0, 1),\n        'n_estimators' : hp.choice('n_estimators', [10, 50, 300, 750, 1200,1300,1500])\n    }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"space","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\ndef objective(space):\n    model = RandomForestClassifier(criterion = space['criterion'], max_depth = space['max_depth'],\n                                 max_features = space['max_features'],\n                                 min_samples_leaf = space['min_samples_leaf'],\n                                 min_samples_split = space['min_samples_split'],\n                                 n_estimators = space['n_estimators'], \n                                 )\n    \n    accuracy = cross_val_score(model, X_train, y_train, cv = 5).mean()\n\n    # We aim to maximize accuracy, therefore we return it as a negative value\n    return {'loss': -accuracy, 'status': STATUS_OK }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\ntrials = Trials()\nbest = fmin(fn= objective,\n            space= space,\n            algo= tpe.suggest,\n            max_evals = 80,\n            trials= trials)\nbest","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ncrit = {0: 'entropy', 1: 'gini'}\nfeat = {0: 'auto', 1: 'sqrt', 2: 'log2', 3: None}\nest = {0: 10, 1: 50, 2: 300, 3: 750, 4: 1200,5:1300,6:1500}\n\n\nprint(crit[best['criterion']])\nprint(feat[best['max_features']])\nprint(est[best['n_estimators']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best['min_samples_leaf']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\ntrainedforest = RandomForestClassifier(criterion = crit[best['criterion']], max_depth = best['max_depth'], \n                                       max_features = feat[best['max_features']], \n                                       min_samples_leaf = best['min_samples_leaf'], \n                                       min_samples_split = best['min_samples_split'], \n                                       n_estimators = est[best['n_estimators']]).fit(X_train,y_train)\npredictionforest = trainedforest.predict(X_test)\nprint(confusion_matrix(y_test,predictionforest))\nprint(accuracy_score(y_test,predictionforest))\nprint(classification_report(y_test,predictionforest))\nacc5 = accuracy_score(y_test,predictionforest)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize = (10,8))\nplt.xlabel('Log  Bins')\nplt.ylabel('Frequency')\nplt.title('Histogram of Hyperopt Solution Scores')\nplt.hist(y, 30, density=True, facecolor='r', alpha=0.75)\nplt.show","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}