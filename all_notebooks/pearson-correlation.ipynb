{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Feature Selection-How To Drop Features Using Pearson Correlation"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Import libraries.\nfrom sklearn.datasets import load_boston\nimport pandas as pd\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loading the dataset\ndata = load_boston()\ndf = pd.DataFrame(data.data,columns =data.feature_names)\ndf['MEDV'] = data.target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.feature_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X =df.drop('MEDV', axis =1)\ny = df[\"MEDV\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Separate data into train and test\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test= train_test_split(X,y,test_size=0.3,random_state = 0)\nX_train.shape , X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we will use Pearson Correlation to remove the features which are correlated. Below is the example:\nimport seaborn as sns\nplt.figure(figsize=(12,10))\ncor = X_train.corr()\nsns.heatmap(cor,annot=True,cmap= plt.cm.CMRmap_r)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from the above figure we can conclude that, TAX and  RAD is highly correlated.i.e 0.91(91%) \n#So,we can use any one if it. It all depends on how much threshold you are keeping for correlation.\n#If it is 70%. then, we can see AGE and NOX also.\n# I know you dont have enough time to watch the figure and write down the correlated feature.\n#Lets Write some code.\n\ndef correlation(dataset,threshold):\n    col_corr = set() # set all the names of correlated columns\n    corr_matrix = dataset.corr() # hey just execute  dataset.corr() to understand this line.\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if(corr_matrix.iloc[i,j])> threshold:\n                colname= corr_matrix.columns[i]  #getting column names\n                col_corr.add(colname)\n    return col_corr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_features = correlation(X_train,0.7)\nlen(set(corr_features))\ncorr_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" We can remove AGE, NOX and TAX from the dataset.\nLet's take one more example where we have lot of features."},{"metadata":{"trusted":true},"cell_type":"code","source":"df= pd.read_csv(\"../input/santander-customer-satisfaction-traincsv/Santander Customer Satisfaction - TRAIN.csv\", nrows =10000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# OMG Here we have 371 features.Now in this case we can use the pearson correlation to remove the features\nX = df.drop(labels = [\"TARGET\"],axis=1)\ny= df['TARGET']\n# separating dataset into training and test set\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we will use Pearson Correlation to remove the features which are correlated. Below is the example:\nimport seaborn as sns\nplt.figure(figsize=(12,10))\ncorrmat = X_train.corr()\nfig,ax = plt.subplots()\nfig.set_size_inches(11,11)\nsns.heatmap(corrmat)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Hurrah, Not able to understand which feature to remove, But have an idea , Let's do it******"},{"metadata":{"trusted":true},"cell_type":"code","source":"cor_features = correlation(X_train,0.8)\nlen(set(cor_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ok so we can delete 191 columns.\ncor_features\n#Below are the names.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Deleting those features\nX_train.drop(cor_features,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"370-191","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Perfect Now we can handle easily 179 features instead of 370.\n# Thanks if you liked this notebook and have any suggestion, let me know. Kindly check my other notebooks for more basics on machine learning."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}