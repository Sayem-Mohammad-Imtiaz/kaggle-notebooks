{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 align=\"center\" style=\"color:#6699ff\"> DataCamp IEEE Fraud Detection </h1>"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://github.com/DataCampM2DSSAF/suivi-du-data-camp-equipe-tchouacheu_toure_niang_chokki/blob/master/img/credit-card-fraud-detection.png?raw=true\" width=\"800\" align=\"center\">"},{"metadata":{},"cell_type":"markdown","source":"#  <a style=\"color:#6699ff\"> Team </a>\n- <a style=\"color:#6699ff\">Mohamed NIANG </a>\n- <a style=\"color:#6699ff\">Fernanda Tchouacheu </a>\n- <a style=\"color:#6699ff\">Sokhna Penda Toure </a>\n- <a style=\"color:#6699ff\">Hypolite Chokki </a>"},{"metadata":{},"cell_type":"markdown","source":"# <a style=\"color:#6699ff\">  Table of Contents</a> \n\n<a style=\"color:#6699ff\"> I. Introduction</a>\n\n<a style=\"color:#6699ff\"> II. Descriptive Statistics & Visualization</a>\n\n<a style=\"color:#6699ff\"> III. Preprocessing</a>\n\n<a style=\"color:#6699ff\"> IV. Machine Learning Models</a>"},{"metadata":{},"cell_type":"markdown","source":"# <a style=\"color:#6699ff\"> I. Introduction</a>"},{"metadata":{},"cell_type":"markdown","source":"**Pourquoi la détection de fraude ?**\n> La fraude est un commerce d'un milliard de dollars et elle augmente chaque année. L'enquête mondiale de PwC sur la criminalité économique de 2018 a révélé que la moitié (49 %) des 7 200 entreprises interrogées avaient été victimes d'une fraude quelconque. C'est une augmentation par rapport à l'étude PwC de 2016, dans laquelle un peu plus d'un tiers des organisations interrogées (36 %) avaient été victimes de la criminalité économique.\n\n\nCette compétition est un problème de **classification binaire** - c'est-à-dire que notre variable cible est un attribut binaire (l'utilisateur qui fait le clic est-il frauduleux ou non ?) et notre objectif est de classer les utilisateurs en \"frauduleux\" ou \"non frauduleux\" le mieux possible."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file \n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn import preprocessing\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.gridspec as gridspec\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport gc\ngc.enable()\n\nimport os\nos.chdir('/kaggle/input/ieeecis-fraud-detection') # Set working directory\nprint(os.listdir('/kaggle/input/ieeecis-fraud-detection'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Load data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_transaction = pd.read_csv('train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('test_transaction.csv', index_col='TransactionID')\ntrain_identity = pd.read_csv('train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('test_identity.csv', index_col='TransactionID')\nprint (\"Data is loaded!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train_transaction shape is {}'.format(train_transaction.shape))\nprint('test_transaction shape is {}'.format(test_transaction.shape))\nprint('train_identity shape is {}'.format(train_identity.shape))\nprint('test_identity shape is {}'.format(test_identity.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a style=\"color:#6699ff\"> III. Preprocessing</a>"},{"metadata":{},"cell_type":"markdown","source":"## Merge transaction & identity "},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_df = pd.merge(train_transaction, train_identity, on = \"TransactionID\", how = \"left\")\nprint(\"Tain: \",train_df.shape)\ndel train_transaction, train_identity\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntest_df = pd.merge(test_transaction, test_identity, on = \"TransactionID\", how = \"left\")\nprint(\"Test: \",test_df.shape)\ntest_df[\"isFraud\"] = 0\ndel test_transaction, test_identity\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pipeline of preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"emails = {\n'gmail': 'google', \n'att.net': 'att', \n'twc.com': 'spectrum', \n'scranton.edu': 'other', \n'optonline.net': 'other', \n'hotmail.co.uk': 'microsoft',\n'comcast.net': 'other', \n'yahoo.com.mx': 'yahoo', \n'yahoo.fr': 'yahoo',\n'yahoo.es': 'yahoo', \n'charter.net': 'spectrum', \n'live.com': 'microsoft', \n'aim.com': 'aol', \n'hotmail.de': 'microsoft', \n'centurylink.net': 'centurylink',\n'gmail.com': 'google', \n'me.com': 'apple', \n'earthlink.net': 'other', \n'gmx.de': 'other',\n'web.de': 'other', \n'cfl.rr.com': 'other', \n'hotmail.com': 'microsoft', \n'protonmail.com': 'other', \n'hotmail.fr': 'microsoft', \n'windstream.net': 'other', \n'outlook.es': 'microsoft', \n'yahoo.co.jp': 'yahoo', \n'yahoo.de': 'yahoo',\n'servicios-ta.com': 'other', \n'netzero.net': 'other', \n'suddenlink.net': 'other',\n'roadrunner.com': 'other', \n'sc.rr.com': 'other', \n'live.fr': 'microsoft',\n'verizon.net': 'yahoo', \n'msn.com': 'microsoft', \n'q.com': 'centurylink', \n'prodigy.net.mx': 'att', \n'frontier.com': 'yahoo', \n'anonymous.com': 'other', \n'rocketmail.com': 'yahoo',\n'sbcglobal.net': 'att',\n'frontiernet.net': 'yahoo', \n'ymail.com': 'yahoo',\n'outlook.com': 'microsoft',\n'mail.com': 'other', \n'bellsouth.net': 'other',\n'embarqmail.com': 'centurylink',\n'cableone.net': 'other', \n'hotmail.es': 'microsoft', \n'mac.com': 'apple',\n'yahoo.co.uk': 'yahoo',\n'netzero.com': 'other', \n'yahoo.com': 'yahoo', \n'live.com.mx': 'microsoft',\n'ptd.net': 'other',\n'cox.net': 'other',\n'aol.com': 'aol',\n'juno.com': 'other',\n'icloud.com': 'apple'\n}\n\n# number types for filtering the columns\nint_types = [\"int8\", \"int16\", \"int32\", \"int64\", \"float\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check how many missing values has each column.\n\ndef check_nan(df, limit):\n    '''\n    Check how many values are missing in each column.\n    If the number of missing values are higher than limit, we drop the column.\n    '''\n    \n    total_rows = df.shape[0]\n    total_cols = df.shape[1]\n    \n    total_dropped = 0\n    col_to_drop = []\n    \n    for col in df.columns:\n\n        null_sum = df[col].isnull().sum()\n        perc_over_total = round((null_sum/total_rows), 2)\n        \n        if perc_over_total > limit:\n            \n            print(\"The col {} contains {} null values.\\nThis represents {} of total rows.\"\\\n                  .format(col, null_sum, perc_over_total))\n            \n            print(\"Dropping column {} from the df.\\n\".format(col))\n            \n            col_to_drop.append(col)\n            total_dropped += 1            \n    \n    df.drop(col_to_drop, axis = 1, inplace = True)\n    print(\"We have dropped a total of {} columns.\\nIt's {} of the total\"\\\n          .format(total_dropped, round((total_dropped/total_cols), 2)))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def binarizer(df_train, df_test):\n    '''\n    Work with cat features and binarize the values.\n    Works with 2 dataframes at a time and returns a tupple of both.\n    '''\n    cat_cols = df_train.select_dtypes(exclude=int_types).columns\n\n    for col in cat_cols:\n        \n        # creating a list of unique features to binarize so we dont get and value error\n        unique_train = list(df_train[col].unique())\n        unique_test = list(df_test[col].unique())\n        unique_values = list(set(unique_train + unique_test))\n        \n        enc = LabelEncoder()\n        enc.fit(unique_values)\n        \n        df_train[col] = enc.transform((df_train[col].values).reshape(-1 ,1))\n        df_test[col] = enc.transform((df_test[col].values).reshape(-1 ,1))\n    \n    return (df_train, df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cathegorical_imputer(df_train, df_test, strategy, fill_value):\n    '''\n    Replace all cathegorical features with a constant or the most frequent strategy.\n    '''\n    cat_cols = df_train.select_dtypes(exclude=int_types).columns\n    \n    for col in cat_cols:\n        print(\"Working with column {}\".format(col))\n        \n        # select the correct inputer\n        if strategy == \"constant\":\n            # input a fill_value of -999 to all nulls\n            inputer = SimpleImputer(strategy=strategy, fill_value=fill_value)\n        elif strategy == \"most_frequent\":\n            inputer = SimpleImputer(strategy=strategy)\n        \n        # replace the nulls in train and test\n        df_train[col] = inputer.fit_transform(X = (df_train[col].values).reshape(-1, 1))\n        df_test[col] = inputer.transform(X = (df_test[col].values).reshape(-1, 1))\n        \n    return (df_train, df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def numerical_inputer(df_train, df_test, strategy, fill_value):\n    '''\n    Replace NaN in the numerical features.\n    Works with 2 dataframes at a time (train & test).\n    Return a tupple of both.\n    '''\n    \n    # assert valid strategy\n    message = \"Please select a valid strategy (mean, median, constant (and give a fill_value) or most_frequent)\"\n    assert strategy in [\"constant\", \"most_frequent\", \"mean\", \"median\"], message\n    \n    # int_types defined earlier in the kernel\n    num_cols = df_train.select_dtypes(include = int_types).columns\n    \n    for col in num_cols:\n\n        print(\"Working with column {}\".format(col))\n\n        # select the correct inputer\n        if strategy == \"constant\":\n            inputer = SimpleImputer(strategy=strategy, fill_value=fill_value)\n        elif strategy == \"most_frequent\":\n            inputer = SimpleImputer(strategy=strategy)\n        elif strategy == \"mean\":\n            inputer = SimpleImputer(strategy=strategy)\n        elif strategy == \"median\":\n            inputer = SimpleImputer(strategy=strategy)\n\n        # replace the nulls in train and test\n        try:\n            df_train[col] = inputer.fit_transform(X = (df_train[col].values).reshape(-1, 1))\n            df_test[col] = inputer.transform(X = (df_test[col].values).reshape(-1, 1))\n        except:\n            print(\"Col {} gave and error.\".format(col))\n            \n    return (df_train, df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pipeline(df_train, df_test):\n    '''\n    We define a personal pipeline to process the data and fill with processing functions.\n    NOTE: modifies the df in place.\n    '''\n    print(\"Shape of train is {}\".format(df_train.shape))\n    print(\"Shape of test is {}\".format(df_test.shape))\n    # We have set the limit of 70%. If a column contains more that 70% of it's values as NaN/Missing values we will drop the column\n    # Since it's very unlikely that it will help our future model.\n    print(\"Checking for nan values\\n\")\n    df_train = check_nan(df_train, limit=0.7)\n    \n    # Select the columns from df_train with less nulls and asign to test.\n    df_test = df_test[list(df_train.columns)]\n          \n    print(\"Shape of train is {}\".format(df_train.shape))\n    print(\"Shape of test is {}\".format(df_test.shape))\n          \n    # mapping emails\n    print(\"Mapping emails \\n\")\n    df_train[\"EMAILP\"] = df_train[\"P_emaildomain\"].map(emails)\n    df_test[\"EMAILP\"] = df_test[\"P_emaildomain\"].map(emails)\n\n    print(\"Shape of train is {}\".format(df_train.shape))\n    print(\"Shape of test is {}\".format(df_test.shape))\n          \n    # replace nulls from the train and test df with a value of \"Other\"\n    print(\"Working with cathegorical values\\n\")\n    df_train, df_test = cathegorical_imputer(df_train, df_test, strategy = \"constant\", fill_value = \"Other\")\n    \n    print(\"Shape of train is {}\".format(df_train.shape))\n    print(\"Shape of test is {}\".format(df_test.shape))\n          \n    # now we will make a one hot encoder of these colums\n    print(\"Binarazing values\\n\")\n    df_train, df_test = binarizer(df_train, df_test)\n    \n    print(\"Shape of train is {}\".format(df_train.shape))\n    print(\"Shape of test is {}\".format(df_test.shape))\n          \n    # working with null values in numeric columns\n    print(\"Working with numerical columns. NAN values\\n\")\n    df_train, df_test = numerical_inputer(df_train, df_test, strategy = \"constant\", fill_value=-999)\n        \n    print(\"Shape of train is {}\".format(df_train.shape))\n    print(\"Shape of test is {}\".format(df_test.shape))\n          \n    return (df_train, df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# before preprocesing\nprint(\"Train before preprocesing: \",train_df.shape)\nprint(\"Test before preprocesing: \",test_df.shape)\n\ntrain_df, test_df = pipeline(train_df, test_df)\n\n# after preprocesing\nprint(\"Train after preprocesing: \",train_df.shape)\nprint(\"Test after preprocesing: \",test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for null values\ncolumns = train_df.columns\nfor col in  columns:\n    total_nulls = train_df[col].isnull().sum()\n    if total_nulls > 0:\n        print(col, total_nulls)\n        \ncolumns = test_df.select_dtypes(exclude=int_types).columns\ntrain_df[columns]\n\ncolumns = test_df.select_dtypes(include=int_types).columns\ntrain_df[columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.to_pickle('/kaggle/working/train_df.pkl')\ntest_df.to_pickle('/kaggle/working/test_df.pkl')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"}},"nbformat":4,"nbformat_minor":4}