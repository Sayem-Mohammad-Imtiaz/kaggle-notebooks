{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Deep Learning Model and Shapley Values for Worst Predictions","metadata":{}},{"cell_type":"code","source":"# Load libraries\nimport numpy as np\nnp.random.seed(1)\nimport tensorflow as tf\n\nfrom tensorflow.keras.models import Sequential, save_model, model_from_json\nfrom tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimport os\nimport time\nimport matplotlib.pyplot as plt\n\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc, accuracy_score\nfrom keras.preprocessing import image\n\nimport matplotlib.image as mpimg\n\nplt.rcParams.update({'figure.max_open_warning': 0})\n\n# Required line to avoid issue issue https://github.com/slundberg/shap/issues/1694#issue-773518362\ntf.compat.v1.disable_v2_behavior()\n","metadata":{"_uuid":"e4aeeaa9-36be-4d1d-9e75-94106dde3bfa","_cell_guid":"1a916e8e-5553-406b-b3e7-6bf2f7907454","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-30T06:29:15.305271Z","iopub.execute_input":"2021-07-30T06:29:15.305575Z","iopub.status.idle":"2021-07-30T06:29:20.119799Z","shell.execute_reply.started":"2021-07-30T06:29:15.305509Z","shell.execute_reply":"2021-07-30T06:29:20.118918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Time measure\ntotal_start_time = time.time()\n# Cofig variables\nLOAD_MODEL = True\nSAVE_MODEL = False\nTEST_MODEL = True\nRUN_CONFUSION_MATRIX = True\nRUN_TENSORBOARD = False\nCLEAR_TENSORBOARD_LOGS = False\nZIP_RESULTS = False\nRUN_SHAP = True\nRUN_SALIENCY_MAPS = False\nFREE_GPU_MEMORY = False\n","metadata":{"_uuid":"9185c47e-253b-4cbb-9149-a044a09e2846","_cell_guid":"e2288477-0133-4fd8-96bd-27b7101595ad","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-30T06:29:20.123053Z","iopub.execute_input":"2021-07-30T06:29:20.123312Z","iopub.status.idle":"2021-07-30T06:29:20.129944Z","shell.execute_reply.started":"2021-07-30T06:29:20.123285Z","shell.execute_reply":"2021-07-30T06:29:20.128995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Point to the 3 directories\ntrain_dir = '../input/gravity-spy-gravitational-waves/train/train/'\nvalidation_dir = '../input/gravity-spy-gravitational-waves/validation/validation/'\ntest_dir = '../input/gravity-spy-gravitational-waves/test/test/'","metadata":{"_uuid":"b6859c00-c192-49ee-b05b-beedcd82a455","_cell_guid":"4c7f2cf0-753b-4767-bec4-1ca9f3988801","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-30T06:29:20.131613Z","iopub.execute_input":"2021-07-30T06:29:20.131995Z","iopub.status.idle":"2021-07-30T06:29:20.139311Z","shell.execute_reply.started":"2021-07-30T06:29:20.131957Z","shell.execute_reply":"2021-07-30T06:29:20.13846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the data generators\ntrain_datagen = ImageDataGenerator(rescale=1. / 255)\nvalidation_datagen = ImageDataGenerator(rescale=1. / 255)  \ntest_datagen = ImageDataGenerator(rescale=1. / 255)","metadata":{"_uuid":"5f1def53-398c-4931-8e40-79aed4e07040","_cell_guid":"f83cf74e-06e2-4042-b43e-eb68599bd612","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-30T06:29:20.142752Z","iopub.execute_input":"2021-07-30T06:29:20.143012Z","iopub.status.idle":"2021-07-30T06:29:20.149486Z","shell.execute_reply.started":"2021-07-30T06:29:20.142986Z","shell.execute_reply":"2021-07-30T06:29:20.148577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test if GPU is available\ndevice_name = tf.test.gpu_device_name()\nprint('GPU avaliable: ', device_name)\nif device_name != '/device:GPU:0':\n  print(\n      '\\n\\nThis error most likely means that this notebook is not '\n      'configured to use a GPU.  Change this in Notebook Settings via the '\n      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n  raise SystemError('GPU device not found')","metadata":{"execution":{"iopub.status.busy":"2021-07-30T06:29:20.152629Z","iopub.execute_input":"2021-07-30T06:29:20.152929Z","iopub.status.idle":"2021-07-30T06:29:21.774601Z","shell.execute_reply.started":"2021-07-30T06:29:20.152903Z","shell.execute_reply":"2021-07-30T06:29:21.773671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Analysis","metadata":{}},{"cell_type":"code","source":"# Take a look at the dataset\ndf = pd.read_csv(\"../input/gravity-spy-gravitational-waves/trainingset_v1d1_metadata.csv\")\npd.set_option('display.max_rows', None)\n# Print number of rows per label in dataset\nprint(df['label'].value_counts().sort_index(ascending=True), '\\n')\n# Print number of rows per sample_type in dataset\nprint(df['sample_type'].value_counts().sort_index(ascending=True), '\\n')\n# Print count of rows per label and sample_type\nprint(df.groupby(['label', 'sample_type']).size())\npd.set_option('display.max_rows', 10)","metadata":{"_uuid":"f0bb0ba7-332d-4492-8f93-fd446e92db48","_cell_guid":"a9ccf755-0de7-4891-a2d1-fdc4950018ad","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-30T06:29:21.775825Z","iopub.execute_input":"2021-07-30T06:29:21.776157Z","iopub.status.idle":"2021-07-30T06:29:21.968653Z","shell.execute_reply.started":"2021-07-30T06:29:21.776121Z","shell.execute_reply":"2021-07-30T06:29:21.967812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The most common type is 'Blip'. Here is what such an image looks like\nfullpath = train_dir + 'Blip/' + os.listdir(train_dir + 'Blip')[0]\nimg=mpimg.imread(fullpath)\nplt.imshow(img)","metadata":{"_uuid":"ecae9c1c-19e7-4239-a089-5cdba83933f6","_cell_guid":"e40afc32-a131-49a1-936e-4f27354f06bc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-30T06:29:21.969946Z","iopub.execute_input":"2021-07-30T06:29:21.970451Z","iopub.status.idle":"2021-07-30T06:29:22.341479Z","shell.execute_reply.started":"2021-07-30T06:29:21.97041Z","shell.execute_reply":"2021-07-30T06:29:22.34059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the class names\nclasses_list = df.label.value_counts().index\nclasses_list = list(classes_list)","metadata":{"_uuid":"c4ff9e6e-196b-4f41-ab72-e77e3a0d3eb6","_cell_guid":"16066d0a-4d08-43cd-874a-ab45a9180b3d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-30T06:29:22.344054Z","iopub.execute_input":"2021-07-30T06:29:22.344391Z","iopub.status.idle":"2021-07-30T06:29:22.350104Z","shell.execute_reply.started":"2021-07-30T06:29:22.344361Z","shell.execute_reply":"2021-07-30T06:29:22.349326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Examples of all the 22 classes\nplt.figure(figsize=(25, 25))\n    \nfor i in range(0, 22):\n    plt.subplot(8, 3, i+1)\n    fullpath = train_dir + classes_list[i] + '/' + os.listdir(train_dir + classes_list[i])[0]\n    img=mpimg.imread(fullpath)\n    plt.title(classes_list[i])\n    plt.tight_layout()\n    plt.imshow(img)","metadata":{"_uuid":"799470c1-aae7-47f8-9666-89febca0ebc3","_cell_guid":"7cad7e5a-c5df-488b-9387-64bbc809fd2d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-30T06:29:22.351711Z","iopub.execute_input":"2021-07-30T06:29:22.352252Z","iopub.status.idle":"2021-07-30T06:29:30.153063Z","shell.execute_reply.started":"2021-07-30T06:29:22.352216Z","shell.execute_reply":"2021-07-30T06:29:30.152278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Sources","metadata":{}},{"cell_type":"code","source":"# Data sources\ntraining_batch_size = 64\nvalidation_batch_size = 32\nimg_dim = 250\n\ntrain_generator = train_datagen.flow_from_directory(\n  train_dir,                                                  \n  classes = classes_list,\n  target_size = (img_dim, img_dim),            \n  batch_size = training_batch_size,\n  class_mode = \"categorical\",\n  shuffle = True,\n  seed = 123)\n\nvalidation_generator = validation_datagen.flow_from_directory(\n  validation_dir,\n  classes = classes_list,\n  target_size = (img_dim, img_dim),\n  batch_size = validation_batch_size,\n  class_mode = \"categorical\",\n  shuffle = True,\n  seed = 123)\n\ntest_size = !find '../input/gravity-spy-gravitational-waves/test/test/' -type f | wc -l\ntest_size = int(test_size[0])\ntest_batch_size = 1\n\ntest_generator = test_datagen.flow_from_directory(\n  test_dir,\n  classes = classes_list,\n  target_size = (img_dim, img_dim),\n  batch_size = test_batch_size,\n  class_mode = \"categorical\",\n  shuffle = False,\n  seed = 3)","metadata":{"_uuid":"fe71f01b-6d30-486d-b2a2-e61ee5787cf6","_cell_guid":"aca6d2e2-31bb-42f1-9922-3d697ee92d0f","execution":{"iopub.status.busy":"2021-07-30T06:29:30.154354Z","iopub.execute_input":"2021-07-30T06:29:30.15489Z","iopub.status.idle":"2021-07-30T06:29:40.128325Z","shell.execute_reply.started":"2021-07-30T06:29:30.15485Z","shell.execute_reply":"2021-07-30T06:29:40.127313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"# CNN\ninput_shape = (img_dim, img_dim, 3)\nmodel = None\n\n# Define and train model or load a pretrained model from a different session to save time\nif LOAD_MODEL:\n    # Load json and create model\n    json_file = open('../input/saved-model/model_kaggle.json', 'r')\n    loaded_model_json = json_file.read()\n    json_file.close()\n    loaded_model = model_from_json(loaded_model_json)\n    # Load weights into new model\n    loaded_model.load_weights(\"../input/saved-model/model_kaggle.h5\")\n    model = loaded_model\nelse:\n    tf.keras.backend.clear_session()\n    model = Sequential()\n    model.add(tf.keras.layers.Conv2D(32, (10, 10), input_shape=input_shape))\n    model.add(tf.keras.layers.Activation('relu'))\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n    model.add(tf.keras.layers.Dropout(0.25))\n\n    model.add(tf.keras.layers.Conv2D(32, (5, 5)))\n    model.add(tf.keras.layers.Activation('relu'))\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n    model.add(tf.keras.layers.Dropout(0.25))\n\n    model.add(tf.keras.layers.Conv2D(64, (3, 3)))\n    model.add(tf.keras.layers.Activation('relu'))\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n    model.add(tf.keras.layers.Dropout(0.25))\n\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(img_dim, activation=\"relu\"))\n    model.add(tf.keras.layers.Dropout(0.25))\n    model.add(tf.keras.layers.Dense(22, activation=\"softmax\"))\n\n    # Now, train the model\n    model.compile(loss = \"categorical_crossentropy\",  \n                  optimizer = 'adam', \n                  metrics=[\"accuracy\"])\n\n    training_step_size = 32\n    validation_step_size = 32\n    \n    # Run tensorboard for graph inspection\n    if RUN_TENSORBOARD:\n        if CLEAR_TENSORBOARD_LOGS:\n            # Clear any logs from previous runs\n            !rm -rf ./logs/ \n            !mkdir ./logs/\n        \n        !rm -rf ./ngork\n        !rm -rf ./ngork-stable-linux-amd64.*\n\n        # Download Ngrok to tunnel the tensorboard port to an external port\n        !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n        !unzip -o ngrok-stable-linux-amd64.zip\n\n        # Run tensorboard as well as Ngrox (for tunneling as non-blocking processes)\n        import os\n        import datetime\n        import multiprocessing\n\n\n        pool = multiprocessing.Pool(processes = 10)\n        results_of_processes = [pool.apply_async(os.system, args=(cmd, ), callback = None )\n                                for cmd in [\n                                f\"tensorboard --logdir ./logs/ --host 0.0.0.0 --port 6006 &\",\n                                \"./ngrok http 6006 &\"\n                                ]]\n\n        # Get the url to access tensorboad\n        ! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n            \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n                \n        log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n\n    history = model.fit(\n        train_generator,\n        steps_per_epoch = training_step_size,\n        epochs = 1,\n        validation_data = validation_generator,\n        validation_steps = validation_step_size,\n        verbose = 1)\n    \n    # Take a look at the accuracy and loss\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n    # Summarize history for loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()","metadata":{"_uuid":"a478466b-99ce-46fc-8963-4af72e403f75","_cell_guid":"253d5755-a785-4f22-ad87-4c5d400ca0e6","execution":{"iopub.status.busy":"2021-07-30T06:29:40.130059Z","iopub.execute_input":"2021-07-30T06:29:40.130662Z","iopub.status.idle":"2021-07-30T06:29:42.352222Z","shell.execute_reply.started":"2021-07-30T06:29:40.130616Z","shell.execute_reply":"2021-07-30T06:29:42.351221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save model for future use\nif SAVE_MODEL:\n    # Serialize model to JSON\n    model_json = model.to_json()\n    with open(\"model.json\", \"w\") as json_file:\n        json_file.write(model_json)\n    # Serialize weights to HDF5\n    model.save(\"model.h5\")","metadata":{"_uuid":"4b3f3b1e-223c-4899-85fc-ce519c3bfcf6","_cell_guid":"e215597d-5ca9-49fd-ae87-331ab3712117","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-30T06:29:42.353589Z","iopub.execute_input":"2021-07-30T06:29:42.353986Z","iopub.status.idle":"2021-07-30T06:29:42.361518Z","shell.execute_reply.started":"2021-07-30T06:29:42.353947Z","shell.execute_reply":"2021-07-30T06:29:42.360634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make the predictions on the test set\n# Line declared for use in the next code cells\ndf = None\nif TEST_MODEL:\n    # Test model\n    print('Testing model...', end='')\n    predictions = model.predict(test_generator, steps = test_size, verbose = 1)\n    print('completed.')","metadata":{"_uuid":"4cec3d28-e618-483f-9065-36c4bccdcf94","_cell_guid":"bbdea673-4ca4-4ae4-abd4-c34127c9dd72","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-30T06:29:42.36296Z","iopub.execute_input":"2021-07-30T06:29:42.363508Z","iopub.status.idle":"2021-07-30T06:30:46.864134Z","shell.execute_reply.started":"2021-07-30T06:29:42.363468Z","shell.execute_reply":"2021-07-30T06:30:46.862999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TEST_MODEL:\n    # Accuracy\n    df = pd.DataFrame(predictions)\n    df['filename'] = test_generator.filenames\n    df['truth'] = np.nan\n    df['truth'] = df['filename'].str.split('/', 1, expand = True)\n    df['prediction_index'] = df[[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21]].idxmax(axis=1).copy()\n    df['prediction'] = np.nan\n\n    for i in range(0,22):\n        df['prediction'][df['prediction_index'] == i] = classes_list[i]\n\n    accuracy = accuracy_score(df['truth'], df['prediction'])\n    print(accuracy)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-07-30T06:30:46.865675Z","iopub.execute_input":"2021-07-30T06:30:46.866076Z","iopub.status.idle":"2021-07-30T06:30:46.962057Z","shell.execute_reply.started":"2021-07-30T06:30:46.866027Z","shell.execute_reply":"2021-07-30T06:30:46.96122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Confusion matrix","metadata":{}},{"cell_type":"code","source":"if TEST_MODEL:\n    # Create a confusion matrix\n    if RUN_CONFUSION_MATRIX:    \n        cm = confusion_matrix(df['truth'], df['prediction'])\n        cm_df = pd.DataFrame(cm)\n        cm_df.columns = classes_list\n        cm_df['signal'] = classes_list\n\n        # Plot the confusion matrix as a correlation plot\n        import seaborn as sns\n\n        plt.figure(figsize=(12, 12))\n\n        corr = cm_df.corr()\n        # Warning raised in this line.\n        ax = sns.heatmap(\n            corr, \n            vmin=0, vmax=1, center=0.5,\n            cmap=sns.diverging_palette(0, 200, n=200),\n            square=True\n        )\n        ax.set_xticklabels(\n            ax.get_xticklabels(),\n            rotation=45,\n            horizontalalignment='right'\n        )\n        \n        # Calculate the N max values and their indexes in confusion matrix\n        df_data = {\n            'value': [],\n            'row_index': [],\n            'col_index': []\n        }\n        num_max_values = 5\n        for i in range(len(corr)):\n            for j in range(len(corr.iloc[i])):\n                end_col = (corr.iloc[i,j] == 1)\n                if end_col:\n                    break\n                df_data['value'].append(corr.iloc[i,j])\n                df_data['row_index'].append(corr.iloc[i].index[i])\n                df_data['col_index'].append(corr.iloc[i].index[j])\n        corr_df = pd.DataFrame(df_data)\n        print('Top values in confusion matrix:\\n', corr_df.nlargest(10, ['value']))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T06:30:46.963287Z","iopub.execute_input":"2021-07-30T06:30:46.96379Z","iopub.status.idle":"2021-07-30T06:30:47.898318Z","shell.execute_reply.started":"2021-07-30T06:30:46.963749Z","shell.execute_reply":"2021-07-30T06:30:47.897364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Worst Predictions","metadata":{}},{"cell_type":"code","source":"# Line declared for use in the next code cells\npivot_table_df = None\nworst_predictions_df = None\nif TEST_MODEL:\n    # Check the number of errors in the model and which predictions the least accurate.\n    df['prediction_is_correct'] = df.apply(lambda x : True if x['truth'] == x['prediction'] else False, axis = 1)\n    pd.set_option('display.max_rows', None)\n    pivot_table_df = pd.pivot_table(df.loc[:,['truth', 'prediction_is_correct']], index=['truth'], \\\n        columns=['prediction_is_correct'], aggfunc=len, fill_value=0).sort_values(\\\n        by=False, ascending=False).head()\n    print(pivot_table_df, '\\n')\n    pivot_table_df = pd.pivot_table(df.loc[:,['truth', 'prediction', 'prediction_is_correct']], \\\n        index=['truth', 'prediction'], columns=['prediction_is_correct'], aggfunc=len, \\\n        fill_value=0).sort_values(by='truth', ascending=True)\n    print(pivot_table_df, '\\n')\n    pivot_table_df = pd.pivot_table(df.loc[:,['truth', 'prediction', 'prediction_is_correct']], \\\n        index=['truth', 'prediction'], columns=['prediction_is_correct'], aggfunc=len, \\\n        fill_value=0).sort_values(by=False, ascending=False).head()\n    print(pivot_table_df, '\\n')\n    \n    # Collect x images from the top worst prediction pairs\n    num_images = 5\n    df_aux_0 = df[(df.truth == pivot_table_df.index[0][0]) & \\\n        (df.prediction == pivot_table_df.index[0][1]) & \\\n        (df.prediction_is_correct == False) & \\\n        (df.filename.str.contains('4.0.png'))].head(num_images)\n    df_aux_1 = df[(df.truth == pivot_table_df.index[1][0]) & \\\n        (df.prediction == pivot_table_df.index[1][1]) & \\\n        (df.prediction_is_correct == False) & \\\n        (df.filename.str.contains('4.0.png'))].head(num_images)\n    worst_predictions_df = pd.concat([df_aux_0,df_aux_1], ignore_index=True)\n    print(worst_predictions_df[['filename', 'truth', 'prediction', 'prediction_is_correct']])","metadata":{"execution":{"iopub.status.busy":"2021-07-30T06:30:47.899661Z","iopub.execute_input":"2021-07-30T06:30:47.900023Z","iopub.status.idle":"2021-07-30T06:30:48.111788Z","shell.execute_reply.started":"2021-07-30T06:30:47.899984Z","shell.execute_reply":"2021-07-30T06:30:48.110103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Worst Predictions Shapley Values","metadata":{}},{"cell_type":"code","source":"# Lines declared for use in the next code cells\n\nif RUN_SHAP:\n    # Now explainability analysis\n    import shap\n    \n    # Preparation\n    shap_values = []\n    test = []\n    shap_train_batch_size = 64\n    shap_test_batch_size = 5\n\n    shap_background_generator = train_datagen.flow_from_directory(\n            train_dir,\n            classes = classes_list,\n            target_size = (img_dim, img_dim),\n            batch_size = shap_train_batch_size,\n            class_mode = \"categorical\",\n            shuffle = True,\n            seed = 3)\n    shap_test_generator = test_datagen.flow_from_dataframe(\n            worst_predictions_df,\n            directory = test_dir,\n            classes = classes_list,\n            y_col = 'truth',\n            target_size = (img_dim, img_dim),\n            batch_size = shap_test_batch_size,\n            class_mode = \"categorical\",\n            shuffle = False,\n            seed = 3)\n    \n    \n    # Select a set of background examples to take an expectation over\n    shap_background = shap_background_generator.next()[0]\n    \n    # Select a set to calculate shapley values\n    shap_test = shap_test_generator.next()[0]\n\n    # Explain predictions of the model on four images\n    print('Creating DeepExplainer...', end='')\n    tensor_input = model.layers[0].input\n    tensor_output = model.layers[-1].output\n    e = shap.DeepExplainer((tensor_input, tensor_output), shap_background)\n    print('completed.')\n    print('Calculating shapley values...', end='')\n    for i in range(0, len(shap_test)):\n        shap_values.append(e.shap_values(shap_test[i:(i+1)]))\n    print('completed.')\n    test = test.append(e.shap_values(shap_test[0:2]))","metadata":{"_uuid":"b4dc8f03-cef3-44b2-bb1d-0383cf73841a","_cell_guid":"91a577f0-cd62-4aba-8440-45f965c76a90","execution":{"iopub.status.busy":"2021-07-30T06:30:48.113139Z","iopub.execute_input":"2021-07-30T06:30:48.113472Z","iopub.status.idle":"2021-07-30T06:32:02.514935Z","shell.execute_reply.started":"2021-07-30T06:30:48.113434Z","shell.execute_reply":"2021-07-30T06:32:02.513985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if RUN_SHAP:\n    # Plot the feature attributions across classes\n    for i, row in enumerate(shap_values):\n        pixel_values = shap_test[(i):(i+1)]\n        labels = [classes_list]\n        shap.image_plot(row, pixel_values=pixel_values, labels=labels, width=300, show=False)\n        plt.savefig('scratch_' + str(i) + '.png', dpi=300)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T06:32:02.516289Z","iopub.execute_input":"2021-07-30T06:32:02.516643Z","iopub.status.idle":"2021-07-30T06:32:30.508669Z","shell.execute_reply.started":"2021-07-30T06:32:02.516602Z","shell.execute_reply":"2021-07-30T06:32:30.507575Z"},"trusted":true},"execution_count":null,"outputs":[]}]}