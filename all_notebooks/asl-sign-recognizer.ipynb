{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Importing libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# for preprocessing\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\n\n# for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# for CNN\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n\n# for image/video recording/loading\nimport cv2\n\n# for Uniform Manifold Approximation and Projections\nfrom umap import UMAP\n\n# for saving file as json\nimport json\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pwd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importing train dataset and separating labels and pixel values from it"},{"metadata":{"trusted":true},"cell_type":"code","source":"# raw_train_data = pd.read_csv(\"../input/sign-language-mnist/sign_mnist_train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# raw_train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# labels = raw_train_data['label'].values #saving labels to a numpy ndarray before dropping them\n# train_data = raw_train_data.drop(['label'],axis=1) #dropping labels (target value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_data = train_data.values # converting to numpy ndarray","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Binary encoding for multiple classes (labels) **"},{"metadata":{"trusted":true},"cell_type":"code","source":"# label_binarizer = LabelBinarizer()\n# labels_encoded = label_binarizer.fit_transform(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# labels_encoded # binarized labels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualizing an instance present in the `train_data`**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.imshow(train_data[0].reshape(28,28), cmap='binary') #showing the first row in train_data as an image\n# plt.grid(False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"**labels to alphabets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# labels_alpha = []\n# def labels_to_alpha(labels):\n#     for x in range(labels.size):\n#             labels_alpha.append(chr(labels[x]+65))\n#     return labels_alpha","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# np.unique(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# np.unique(labels_to_alpha(labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# num_of_classes = np.unique(labels).size\n# num_of_classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.figure(figsize = (18,8))\n# plt.title=(\"count plot of signs\") # TODO this is not showing up in plot\n# sns.countplot(x = labels_alpha, order = np.unique(labels_alpha))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Spliting into test and train sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train, X_test, Y_train, Y_test = train_test_split(train_data, labels_encoded, test_size = 0.25, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Normalizing pixel values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train_normalized = X_train/255\n# X_test_normalized = X_test/255","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Displaying Normalized and UnNormalized Images**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# fig, (ax1, ax2) = plt.subplots(1, 2)\n# ax1.imshow(X_train_normalized[0].reshape(28,28),cmap='binary')\n# ax1.set_title(\"Normalized\")\n# ax1.grid(False)\n# ax2.imshow(X_train[0].reshape(28,28), cmap='binary')\n# ax2.set_title(\"Un-normalized\")\n# ax2.grid(False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/sign-language-mnist/sign_mnist_train.csv')\ntest = pd.read_csv('../input/sign-language-mnist/sign_mnist_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = train['label'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_val = np.array(labels)\nnp.unique(unique_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop('label', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images = train.values\nimages = np.array([np.reshape(i, (28, 28)) for i in images])\nimages = np.array([i.flatten() for i in images])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelBinarizer\nlabel_binrizer = LabelBinarizer()\nlabels = label_binrizer.fit_transform(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size = 0.3, random_state = 101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 128\nnum_classes = 24\nepochs = 50","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = x_train / 255\nx_test = x_test / 255","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_t = np.stack([x_train.reshape(x_train.shape[0],28,28)]*3, axis=3).reshape(x_train.shape[0],28,28,3)\nx_test_t = np.stack([x_test.reshape(x_test.shape[0],28,28)]*3, axis=3).reshape(x_test.shape[0],28,28,3)\nx_train_t.shape, x_test_t.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\nx_test = x_test.reshape(x_test.shape[0], 28, 28, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Making images as per the size required by VGG16"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Resize the images 48*48 as required by VGG16\nfrom keras.preprocessing.image import img_to_array, array_to_img\nx_train_tt = np.asarray([img_to_array(array_to_img(im, scale=True).resize((48,48))) for im in x_train_t])/225\nx_test_tt = np.asarray([img_to_array(array_to_img(im, scale=True).resize((48,48))) for im in x_test_t])/225\nx_train_tt.shape, x_test_tt.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### displaying the image sample "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(x_test_tt[0].reshape(48,48,3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using VGG16 architecture"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications import VGG16\nfrom keras.applications.vgg16 import preprocess_input\nmodelVGG = Sequential()\n#  Create base model of VGG16\nmodelVGG.add(VGG16(weights='imagenet',\n                  include_top=False, pooling = 'avg',  \n                  input_shape=(48, 48, 3)\n                 ))\n# 2nd layer as Dense \nmodelVGG.add(Dense(num_classes, activation = 'softmax'))\n\n# Say not to train first layer (ResNet) model as it is already trained\nmodelVGG.layers[0].trainable = False\nmodelVGG.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The CNN Architecture "},{"metadata":{"trusted":true},"cell_type":"code","source":"# CNN_model = Sequential()\n# CNN_model.add(Conv2D(64,kernel_size=(3,3), activation='relu', input_shape=(28,28,1)))\n# CNN_model.add(MaxPooling2D(pool_size=(2,2)))\n\n# CNN_model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n# CNN_model.add(MaxPooling2D(pool_size = (2,2)))\n\n# CNN_model.add(Conv2D(64, kernel_size = (3,3), activation = 'relu'))\n# CNN_model.add(MaxPooling2D(pool_size = (2,2)))\n\n# CNN_model.add(Flatten())\n\n# CNN_model.add(Dense(128, activation='relu'))\n# CNN_model.add(Dropout(0.20))\n# CNN_model.add(Dense(num_of_classes, activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Reshaping `X_train_normalized` and `X_test_normalized` for input to Conv Layers**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train_reshaped = X_train_normalized.reshape(X_train_normalized.shape[0], 28, 28, 1)\n# X_test_reshaped = X_test_normalized.reshape(X_test_normalized.shape[0],28,28,1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Compiling the `CNN_model`**. Using *Adam* optimizer, *categorical_crossentropy* loss and *accuracy* metric "},{"metadata":{"trusted":true},"cell_type":"code","source":"# CNN_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelVGG.compile(loss = keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Specifying other parameters to start training of `CNN_model`**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# batch_size = 128\n# epochs = 50","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Training the `CNN_model`** adn saving the results of each epoch into `history` dictionary"},{"metadata":{"trusted":true},"cell_type":"code","source":"#history = CNN_model.fit(X_train_reshaped, Y_train, validation_data=(X_test_reshaped,Y_test),epochs=epochs,batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"historyVGG = modelVGG.fit(x_train_tt, y_train, validation_data = (x_test_tt, y_test), epochs=epochs, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Saving the weights and the architecture of CNN model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# CNN_model.save_weights(\"cnn_weights_50epochs.h5\")\n# CNN_model.save(\"cnn_architecture_weights_50epoch\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelVGG.save_weights(\"modelvgg_weights_50epochs.h5\")\nmodelVGG.save(\"modelvgg_architecture_weights_50epoch\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pwd\n!ls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### CNN_model metrics"},{"metadata":{},"cell_type":"markdown","source":"**Plotting Accuracy vs No. of Epochs for training and testing data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# history.history.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.figure(figsize = (18,8))\n# plt.plot(history.history['accuracy'])\n# plt.plot(history.history['val_accuracy'])\n# plt.xlabel('epoch')\n# plt.ylabel('accuracy')\n# plt.legend(['train','test'])\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(historyVGG.history['accuracy'])\nplt.plot(historyVGG.history['val_accuracy'])\nplt.title(\"Accuracy\")\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.legend(['train','test'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Plotting Loss vs No. of Epochs for training and testing data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.figure(figsize = (18,8))\n# plt.plot(history.history['loss'])\n# plt.plot(history.history['val_loss'])\n# plt.xlabel('epoch')\n# plt.ylabel('loss')\n# plt.legend(['train','test'])\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (18,8))\nplt.plot(historyVGG.history['loss'])\nplt.plot(historyVGG.history['val_loss'])\nplt.title(\"Loss\")\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['train','test'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Last epoch metrics**"},{"metadata":{},"cell_type":"markdown","source":"1. Train Set Accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"# history.history['accuracy'][-1]*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"historyVGG.history['accuracy'][-1]*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. Test Set Accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"# history.history['val_accuracy'][-1]*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"historyVGG.history['val_accuracy'][-1]*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. Train Set Loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"# history.history['loss'][-1]*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"historyVGG.history['loss'][-1]*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4. Test Set Loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"# history.history['val_loss'][-1]*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"historyVGG.history['val_loss'][-1]*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO save history.history dict which has a list of floats and floats should be converted to str before saving\n# with open('history_CNN_model_50epochs.json', 'w') as fp:\n#     json.dump(history.history, fp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Accuracy on test data (the one which is another csv file, not the one which is splitted from train data)**"},{"metadata":{},"cell_type":"markdown","source":"Preapring the test data in the same format as we did for train data so that the input to the `CNN_model` remains in the same format"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_labels = test['label']\ntest.drop('label', axis = 1, inplace = True)\ntest_images = test.values\ntest_images = np.array([np.reshape(i, (28, 28)) for i in test_images])\n#test_images = np.array([i.flatten() for i in test_images])\ntest_images_t = np.stack([test_images.reshape(test_images.shape[0],28,28)]*3, axis=3).reshape(test_images.shape[0],28,28,3)\n\n# Resize the images 48*48 as required by VGG16\nfrom keras.preprocessing.image import img_to_array, array_to_img\ntest_images_tt = np.asarray([img_to_array(array_to_img(im, scale=True).resize((48,48))) for im in test_images_t])/225\ntest_images_tt.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(test_images_tt[0].reshape(48,48,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_labels = label_binrizer.fit_transform(test_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_images = test_images_tt.reshape(test_images.shape[0], 48, 48, 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_images.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Predicting the test data using `CNN_model`"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Y_pred = CNN_model.predict(test_data)\n# accuracy_score(test_labels_encoded,Y_pred.round())*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_vgg = modelVGG.predict(test_images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(test_labels, y_pred_vgg.round())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !ls ../input/asl-sign-recognizer/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CNN_model = keras.models.load_model('../input/asl-sign-recognizer/cnn_architecture_weights_50epoch')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CNN_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Y_pred.round()[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# unique_labels = [x for x in \"ABCDEFGHIKLMNOPQRSTUVWXY\"]\n# unique_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Y_pred_list_alpha = []\n# for x in range(Y_pred.round().shape[0]):\n#     Y_pred_list_alpha.append(unique_labels[np.argmax(Y_pred.round()[x])])\n# Y_pred_list_alpha","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# len(Y_pred_list_alpha)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# len(test_labels_encoded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_labels_encoded_alpha = []\n# for x in range(test_labels_encoded.shape[0]):\n#     test_labels_encoded_alpha.append(unique_labels[np.argmax(test_labels_encoded[x])])\n# test_labels_encoded_alpha","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# if Y_pred_list_alpha == test_labels_encoded_alpha:\n#     print(\"d\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cm = confusion_matrix(test_labels_encoded_alpha,Y_pred_list_alpha)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df = pd.DataFrame(cm ,index = unique_labels, columns = unique_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.figure(figsize = (24,24))\n# sns.heatmap(df, annot=True,cmap=\"YlGnBu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}