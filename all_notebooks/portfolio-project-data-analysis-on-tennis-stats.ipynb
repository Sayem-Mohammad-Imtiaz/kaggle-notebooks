{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Portfolio Project: Data Analysis on Tennis stats\n\n## TABLE OF CONTENTS\n1. Introduction\n2. Imports and Functions\n3. The Data\n4. Data Preparation\n5. Data Processing\n6. Data Analysis and Visualization\n7. Conclusions\n8. Improvements & Next Steps\n9. References\n\n## 1. INTRODUCTION\nFor those who don't know, I spent more than one year working on a predictive system able to make money consistently and profitably by betting on NBA games. I started this journey on September 2019 and, as of December 2020, I had found an approach that way better than what I had hoped to find.\n\nThat's when Netty was born and, an entire regular season after, I can proudly say that it yielded a 9.14% ROI betting mostly on underdogs (average odds were 2.15) and made more than 51 units in profits just in 280 games. To put an example, someone with a unit of 100€ would have finished with **over 5100€ in less than 5 months**. \n\nBeing the ambitious person that I am, I wasn't going to settle with something that was profitable only during 6 months. I want to earn money consistently and regularly, that's why I need another model able to work the entire year.\n\nThere were different options: tennis, horse racing, greyhound racing... I ended up choosing tennis because I'm not particularly a fan of animal racing and tennis is a much more famous sport (and that usually means more data).\n\nNow, I won't be creating the model and doing all the work that comes after. This project will consist in all the previous phases: from data manipulation to data analysis, to data visualization, to getting data almost ready for modelling time. So **expect the analysis to move towards the direction of finding those relevant features** to use in a predictive model. \n\nI'll then, privately, use the insights I'll discover to create the model and hopefully make it profitable enough.\n\n## 2. IMPORTS AND FUNCTIONS\n\nThis might not be the most efficient practice but I like to keep things organized and I can't think of a better way to do so.\n\nBelow, you'll be able to see the packages I'm using throughout the entire document and also some external functions that will help me make this path easier.","metadata":{}},{"cell_type":"code","source":"#######################################################################################\n# IMPORTS\n#######################################################################################\n\n# Math\nimport numpy as np\nimport math, statistics\n\n# Data manipulation\nimport pandas as pd\nfrom fuzzywuzzy import fuzz, process\n!pip install xlrd\n!pip install openpyxl\n\n# Data visualization\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nplt.style.use('ggplot')\n%matplotlib inline\nmatplotlib.rcParams['figure.figsize'] = (12,8)\n\n# Machine learning\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\n\n# Extras\nfrom tqdm import tqdm\nimport time\n\n# Shutting down warnings, just to make things cleaner\nimport warnings\nwarnings. simplefilter(action='ignore', category=Warning)\npd.set_option(\"display.max_columns\", None)\npd.set_option(\"display.max_rows\", 20)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T08:35:59.386292Z","iopub.execute_input":"2021-07-22T08:35:59.386747Z","iopub.status.idle":"2021-07-22T08:36:14.049134Z","shell.execute_reply.started":"2021-07-22T08:35:59.386703Z","shell.execute_reply":"2021-07-22T08:36:14.048044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#######################################################################################\n# FUNCTIONS\n#######################################################################################\n\ndef divider(ini, stop, denominator):\n    '''Acts as a range() but instead of adding up or subtracting, it multiplies/divides'''\n    l = []\n    if ini > stop:\n        while ini > stop:\n            l.append(int(ini))\n            ini /= denominator\n    else:\n        while ini < stop:\n            l.append(int(ini))\n            ini *= denominator\n        \n    return l\n\ndef compute_diff(df, cols):\n    '''This computes the difference of certain features between player1 and player2'''\n    # ALWAYS player 1 - player 2\n    # cols shouldn't have player suffixes\n    for col in cols:\n        df[col + \"_diff\"] = df['p1_' + col] - df['p2_' + col]\n    return df\n\ndef myround(x, base=.5):\n    '''This rounds to the desired base (default to .5)'''\n    return base * round(x/base)\n\ndef f(x):\n    '''This will be used to display information when grouping by odds or confidence'''\n    d = {}\n    d[\"Games\"] = x[\"Right\"].count()\n    d['Accuracy'] = x['Right'].mean()\n    d['Mean Odds'] = x['Odd'].mean()\n    d['Profits (unit)'] = x['Profits_unit'].sum()\n    d['Profits (odd)'] = x['Profits_odd'].sum()\n    d['ROI (unit)'] = x['Profits_unit'].sum() / x['Odd'].count()\n    d['ROI (odd)'] = x['Profits_odd'].sum() / x['Odd'].sum()\n    return pd.Series(d, index=d.keys())","metadata":{"execution":{"iopub.status.busy":"2021-07-22T08:26:55.669593Z","iopub.execute_input":"2021-07-22T08:26:55.670071Z","iopub.status.idle":"2021-07-22T08:26:55.680793Z","shell.execute_reply.started":"2021-07-22T08:26:55.670025Z","shell.execute_reply":"2021-07-22T08:26:55.679428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. THE DATA\nThe available data consists in two different types of files, both types extracted from different sources, and as many files per type as years between 2005 and 2021 (both included). \n\n* **data/atp_matches_{year}.csv**: they contain valuable qualitative information (like each player's hand, surface, tourney level...) as well as quantitative data from that particular event (typical match stats).\n* **data/{year}.xls[x]**: these mainly contain information related to odds as well as the match results and metadata. \n\nLet's quickly see a sneak peek of both types:","metadata":{}},{"cell_type":"code","source":"pd.read_csv(\"/kaggle/input/atp-masters-tennis-dataset/atp_matches_2021.csv\").head()","metadata":{"execution":{"iopub.status.busy":"2021-07-22T08:35:18.934623Z","iopub.execute_input":"2021-07-22T08:35:18.935069Z","iopub.status.idle":"2021-07-22T08:35:18.998271Z","shell.execute_reply.started":"2021-07-22T08:35:18.935034Z","shell.execute_reply":"2021-07-22T08:35:18.997251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_excel(\"/kaggle/input/tennis-2021/2021.xlsx\").head()","metadata":{"execution":{"iopub.status.busy":"2021-07-22T08:36:22.462327Z","iopub.execute_input":"2021-07-22T08:36:22.4627Z","iopub.status.idle":"2021-07-22T08:36:23.391304Z","shell.execute_reply.started":"2021-07-22T08:36:22.462664Z","shell.execute_reply":"2021-07-22T08:36:23.390532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. DATA PREPARATION\n\nThis will be a pretty mechanical process which will consist in a series of steps:\n\n1. Create the necessary transformations to join datasets (that is, normalize player and tournament names, dates...)\n2. Join datasets, create one per year.\n3. Get rid of unnecessary columns.\n4. Reorganize dataset: from winner and loser to player1 and player2, evenly distributed (that means, half the time player1 won and the other half was won by player2). Also create a new column names \"Player1Win\".\n5. Create new features?\n\n\n### 4.1. Transform identifier columns\nThe plan is to join both types of dataframes and, to do so, I'll be using the following columns as identifiers (format: column_name_df1 = column_name_df2), some of which I'll have to create:\n* Month = month\n* Year = year\n* winner = winner_name\n* loser = loser_name\n\nIf we look closer to both types of dataframes, one has full names for both player names while the other has just the last name followed by the initial letter of the player's name. I'll be using the fuzzywuzzy package to perform some string matching.","metadata":{}},{"cell_type":"code","source":"years = [y for y in range(2005, 2022)]\nd = {}\n\nfor year in tqdm(years):\n    # Type of XLS file changed in 2013. We'll read them accordingly.\n    if year < 2013:\n        df1 = pd.read_excel(\"/kaggle/input/tennis-data-atp/{}.xls\".format(year))\n    elif year < 2019:\n        df1 = pd.read_excel(\"/kaggle/input/tennis-data-atp/{}.xlsx\".format(year))\n    elif year < 2021:\n        df1 = pd.read_excel(\"/kaggle/input/atp-mens/ATP_Data/{}.xlsx\".format(year))\n    else:\n        df1 = pd.read_excel(\"/kaggle/input/tennis-2021/2021.xlsx\")\n    df2 = pd.read_csv(\"/kaggle/input/atp-masters-tennis-dataset/atp_matches_{}.csv\".format(year))\n    \n    # Converting dates into two columns: year and month -> year will always be the same as the iteration variable \n    # but let's do things right just in case\n    df1[\"Year\"] = df1[\"Date\"].dt.year\n    df1[\"Month\"] = df1[\"Date\"].dt.month\n    df2[\"year\"] = df2[\"tourney_date\"].astype(str).str[:4].astype(int)\n    df2[\"month\"] = df2[\"tourney_date\"].astype(str).str[4:6].astype(int)\n    \n    # Formatting strings in player names to match the other df\n    l = []\n    w = []\n    players_w = pd.unique(df2[\"winner_name\"])\n    players_l = pd.unique(df2[\"loser_name\"])\n    for i,row in df1.iterrows():\n        winner = row[\"Winner\"]\n        loser = row[\"Loser\"]\n        \n        w1 = process.extract(winner, players_w)\n        l1 = process.extract(loser, players_l)\n        \n        if len(w1) > 0:\n            w.append(w1[0][0])\n        else:\n            w.append(\"\")\n            \n        if len(l1) > 0:\n            l.append(l1[0][0])\n        else:\n            l.append(\"\")\n            \n    df1[\"winner\"] = w\n    df1[\"loser\"] = l\n    \n    d[year] = [df1, df2]","metadata":{"execution":{"iopub.status.busy":"2021-07-22T08:37:38.097787Z","iopub.execute_input":"2021-07-22T08:37:38.098356Z","iopub.status.idle":"2021-07-22T08:42:35.288504Z","shell.execute_reply.started":"2021-07-22T08:37:38.098318Z","shell.execute_reply":"2021-07-22T08:42:35.28666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2. Join datasets","metadata":{}},{"cell_type":"code","source":"for year in years:\n    d[year] = d[year][0].merge(d[year][1], left_on=[\"Month\", \"Year\", \"winner\", \"loser\"], right_on=[\"month\", \"year\", \"winner_name\", \"loser_name\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As expected, we've lost some matches in the process and it's an average 13.24%. Is that a lot? Is it not? I'd say it's higher than the ideal but still good enough. \n\n### 4.3. Remove unwanted columns\n\nFirst of all, not all dataframes have the same columns, so we must add consistency here. Let's quickly go over all of them to see which has the lowest number of columns.","metadata":{}},{"cell_type":"code","source":"for year in d:\n    print(\"{} has {} columns\".format(year, len(d[year].columns)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2021 is the one with the fewest columns. Let's use them as a basis and see if the other years have the same ones.\n\nThen, of all the columns available right now, I'll be removing some which I think won't have any relevance. I'll be keeping some of these though for better understanding of the data.\n\nI'll make sure all available games have been completed (not finished by injury or other causes), also to keep those rows in which ATP ranks and ATP points coincide in both dfs and to remove those rows without odds.\n\nLastly, I'll be creating one single dataframe from them all, so we have all the data in one single data structure.","metadata":{}},{"cell_type":"code","source":"cols = d[2021].columns.tolist() # Choosing 2021 because it's the one with less columns\n\nfor year in d:\n    cols = [col for col in cols if col in d[year].columns]\nprint(\"Final number of columns: {}\".format(len(cols)))\n\n# Unwanted columns\nremove = [\"ATP\", \"Location\", \"Tournament\", \"Date\", \"Series\", \"Best of\", \"Winner\", \"Loser\", \"Year\", \"Month\", \n          \"tourney_id\", \"tourney_name\", \"surface\", \"draw_size\", \"tourney_level\", \"tourney_date\", \"match_num\",\n          \"winner_id\", \"winner_entry\", \"winner_seed\", \"winner\", \"loser\", \"winner_ioc\", \"loser_id\", \"loser_seed\",\n          \"loser_entry\", \"loser_ioc\", \"score\", \"best_of\", \"round\", \"winner_rank\", \"loser_rank\", \"winner_rank_points\",\n          \"loser_rank_points\", \"Comment\"\n         ]\nwanted_cols = [col for col in cols if col not in remove]\n\n# Making sure ranks and points coincide, also checking that the game had been completed and odds are not missing.\nfor year in d:\n    d[year] = d[year][(d[year][\"WRank\"] == d[year][\"winner_rank\"]) & (d[year][\"LRank\"] == d[year][\"loser_rank\"]) & \n            (d[year][\"WPts\"] == d[year][\"winner_rank_points\"]) & (d[year][\"LPts\"] == d[year][\"loser_rank_points\"])\n            & (d[year][\"Comment\"] == \"Completed\") & (~d[year][\"B365W\"].isna()) & (~d[year][\"B365L\"].isna())\n           ][wanted_cols]\n\n# Creating a single dataframe for all data, using just the desired columns\ndf = pd.concat([d[year] for year in d])\ndf.reset_index(drop=True, inplace=True)\ndf = df[['Court', 'Surface', 'Round', 'year', 'month', 'minutes', \n         'winner_name', 'WRank', 'WPts', 'W1', 'W2', 'W3', 'W4', 'W5', 'Wsets', 'B365W', 'winner_hand', 'winner_ht',\n         'winner_age', 'w_ace', 'w_df', 'w_svpt', 'w_1stIn', 'w_1stWon', 'w_2ndWon', 'w_SvGms', 'w_bpSaved', \n         'w_bpFaced',\n         'loser_name', 'LRank', 'LPts', 'L1', 'L2', 'L3', 'L4', 'L5', 'Lsets', 'B365L', 'loser_hand', 'loser_ht', \n         'loser_age', 'l_ace', 'l_df', 'l_svpt', 'l_1stIn', 'l_1stWon', 'l_2ndWon', 'l_SvGms', 'l_bpSaved', \n         'l_bpFaced']]\n\n# I'll keep working with the same variable, df, all the time (aesthetics). It's a good practice to save \n# the current work in another variable, just in case I mess up later.\ndf_v1 = df.copy()\n\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.4. Reorganize dataset\nI think it's a good time to start shaping the data as we will need it. Instead of having winner and loser, I'll create player1 and player2. For future purposes, I'll make that half the time will win player1, the other half will be for player2 (keeping it as it is now, the output would learn that the first player always wins and that would be a huge mistake).\n\nObviously, I'll need an extra column telling which player won: \"p1_win\".","metadata":{}},{"cell_type":"code","source":"data = []\np1_wins = []\n\ninverted_cols = df.columns[:6].tolist() + df.columns[28:].tolist() + df.columns[6:28].tolist()\n\nfinal_cols = ['Court', 'Surface', 'Round', 'year', 'month', 'minutes', \n              'p1_name', 'p1_Rank', 'p1_Pts', 'p1_1', 'p1_2', 'p1_3', 'p1_4', 'p1_5', 'p1_sets', 'p1_B365', \n              'p1_hand', 'p1_ht', 'p1_age', 'p1_ace', 'p1_df', 'p1_svpt', 'p1_1stIn', 'p1_1stWon', 'p1_2ndWon', \n              'p1_SvGms', 'p1_bpSaved', 'p1_bpFaced',\n              'p2_name', 'p2_Rank', 'p2_Pts', 'p2_1', 'p2_2', 'p2_3', 'p2_4', 'p2_5', 'p2_sets', 'p2_B365', \n              'p2_hand', 'p2_ht', 'p2_age', 'p2_ace', 'p2_df', 'p2_svpt', 'p2_1stIn', 'p2_1stWon', 'p2_2ndWon', \n              'p2_SvGms', 'p2_bpSaved', 'p2_bpFaced']\n\nfor i,row in tqdm(df.iterrows()):\n    if len(p1_wins) == 0 or statistics.mean(p1_wins) <= 0.5: # More 0 than 1 (or same) -> we add 1 (that means, player1 is the one who won)\n        p1_wins.append(1)\n        data.append(row.tolist())\n    else:\n        p1_wins.append(0)\n        data.append(row[inverted_cols].tolist())\n        \ndf = pd.DataFrame(data, columns = final_cols)\ndf[\"p1_2\"] = df[\"p1_2\"].astype(float)\ndf[\"p1_3\"] = df[\"p1_3\"].replace(\" \", np.nan)\ndf[\"p1_3\"] = df[\"p1_3\"].astype(float)\ndf[\"p2_2\"] = df[\"p2_2\"].astype(float)\ndf[\"p2_3\"] = df[\"p2_3\"].replace(\" \", np.nan)\ndf[\"p2_3\"] = df[\"p2_3\"].astype(float)\ndf[\"p1_win\"] = p1_wins\ndf","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. DATA PROCESSING\n\nThis is where stuff starts getting interesting. In a bit, you'll see me cleaning data, creating new features...\n\n### 5.1. Checking consistency in Odds","metadata":{}},{"cell_type":"code","source":"# Visualize them\nfor i,row in df.iterrows():\n    if (1/row[\"p2_B365\"]) + (1/row[\"p1_B365\"]) < 1:\n        print(row[\"p2_B365\"], row[\"p1_B365\"])\n    if row[\"p2_B365\"]<1 or row[\"p1_B365\"]<1:\n        print(row[\"p2_B365\"], row[\"p1_B365\"])\n        \n# Drop them\ndf = df[(df[\"p2_B365\"]>1) & (df[\"p1_B365\"]>1)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All good now. Let's generate some, more advanced, features.\n\n### 5.2. Feature generation\n\nIf you're someone who loves tennis and loves stats, I'm sure you're familiarized with the website Ultimate Tennis Statistics. If you're not, feel free to check it out (it's a website full of tennis stats, as you already intuited).\n\nI'm referring to this service because it has advanced stats I can use for this analysis. Concretely, there's this [glossary](https://www.ultimatetennisstatistics.com/glossary) containing the formulas, some of which I'll be creating right now. Don't worry if something's not clear, it's just code performing simple mathematical computations to generate these advanced features.","metadata":{}},{"cell_type":"code","source":"# First, turn the missing values into 0 within the games-won-per-set columns\ndf[\"p1_3\"].fillna(0, inplace=True)\ndf[\"p1_4\"].fillna(0, inplace=True)\ndf[\"p1_5\"].fillna(0, inplace=True)\ndf[\"p2_3\"].fillna(0, inplace=True)\ndf[\"p2_4\"].fillna(0, inplace=True)\ndf[\"p2_5\"].fillna(0, inplace=True)\n\n# 1st Serve Effectiveness\ndf[\"p1_1stWon%\"] = df[\"p1_1stWon\"] / df[\"p1_1stIn\"]\ndf[\"p1_2ndWon%\"] = df[\"p1_2ndWon\"] / (df[\"p1_svpt\"] - df[\"p1_1stIn\"])\ndf[\"p1_1stServeEffectiveness\"] = df[\"p1_1stWon%\"]/df[\"p1_2ndWon%\"]\n\ndf[\"p2_1stWon%\"] = df[\"p2_1stWon\"] / df[\"p2_1stIn\"]\ndf[\"p2_2ndWon%\"] = df[\"p2_2ndWon\"] / (df[\"p2_svpt\"] - df[\"p2_1stIn\"])\ndf[\"p2_1stServeEffectiveness\"] = df[\"p2_1stWon%\"]/df[\"p2_2ndWon%\"]\n\n# Return to Service Points Ratio \ndf[\"p1_Ret2ServPtsRatio\"] = df[\"p2_svpt\"] / df[\"p1_svpt\"]\ndf[\"p2_Ret2ServPtsRatio\"] = df[\"p1_svpt\"] / df[\"p2_svpt\"]\n\n# Point Dominance Ratio\ndf[\"p1_ServeWon%\"] = (df[\"p1_1stWon\"] + df[\"p1_2ndWon\"]) / df[\"p1_svpt\"]\ndf[\"p1_ReturnWon%\"] = 1 - df[\"p1_ServeWon%\"]\n\ndf[\"p2_ServeWon%\"] = (df[\"p2_1stWon\"] + df[\"p2_2ndWon\"]) / df[\"p2_svpt\"]\ndf[\"p2_ReturnWon%\"] = 1 - df[\"p2_ServeWon%\"]\n\ndf[\"p1_PtsDominanceRatio\"] = df[\"p1_ReturnWon%\"] / df[\"p2_ReturnWon%\"]\ndf[\"p2_PtsDominanceRatio\"] = df[\"p2_ReturnWon%\"] / df[\"p1_ReturnWon%\"]\n\n# Break Points Ratio\ndf[\"p1_BPConverted%\"] = (df[\"p2_bpFaced\"] - df[\"p2_bpSaved\"]) / df[\"p2_bpFaced\"]\ndf[\"p2_BPConverted%\"] = (df[\"p1_bpFaced\"] - df[\"p1_bpSaved\"]) / df[\"p1_bpFaced\"]\n\ndf[\"p1_BPRatio\"] = df[\"p1_BPConverted%\"] / df[\"p2_BPConverted%\"]\ndf[\"p2_BPRatio\"] = df[\"p2_BPConverted%\"] / df[\"p1_BPConverted%\"]\n\n# Points to Sets Over-Performing Ratio\ndf[\"p1_SetWon%\"] = df[\"p1_sets\"] / (df[\"p1_sets\"] + df[\"p2_sets\"])\ndf[\"p1_PtsWon%\"] = (df[\"p1_1stWon\"] + df[\"p1_2ndWon\"] + df[\"p2_1stIn\"] - df[\"p2_1stWon\"] + (df[\"p2_svpt\"] - df[\"p2_1stIn\"]) - df[\"p2_2ndWon\"]) / (df[\"p1_svpt\"] + df[\"p2_svpt\"])\ndf[\"p1_Pts2Sets_OP_Ratio\"] = df[\"p1_SetWon%\"] / df[\"p1_PtsWon%\"]\n\ndf[\"p2_SetWon%\"] = df[\"p2_sets\"] / (df[\"p1_sets\"] + df[\"p2_sets\"])\ndf[\"p2_PtsWon%\"] = (df[\"p2_1stWon\"] + df[\"p2_2ndWon\"] + df[\"p1_1stIn\"] - df[\"p1_1stWon\"] + (df[\"p1_svpt\"] - df[\"p1_1stIn\"]) - df[\"p1_2ndWon\"]) / (df[\"p1_svpt\"] + df[\"p2_svpt\"])\ndf[\"p2_Pts2Sets_OP_Ratio\"] = df[\"p2_SetWon%\"] / df[\"p2_PtsWon%\"]\n\n# Points to Games Over-Performing Ratio\ndf[\"p1_GmsWon%\"] = (df[\"p1_1\"] + df[\"p1_2\"] + df[\"p1_3\"] + df[\"p1_4\"] + df[\"p1_5\"]) / (df[\"p1_1\"] + df[\"p1_2\"] + df[\"p1_3\"] + df[\"p1_4\"] + df[\"p1_5\"] + df[\"p2_1\"] + df[\"p2_2\"] + df[\"p2_3\"] + df[\"p2_4\"] + df[\"p2_5\"])\ndf[\"p1_Pts2Gms_OP_Ratio\"] = df[\"p1_GmsWon%\"] / df[\"p1_PtsWon%\"]\n\ndf[\"p2_GmsWon%\"] = (df[\"p2_1\"] + df[\"p2_2\"] + df[\"p2_3\"] + df[\"p2_4\"] + df[\"p2_5\"]) / (df[\"p1_1\"] + df[\"p1_2\"] + df[\"p1_3\"] + df[\"p1_4\"] + df[\"p1_5\"] + df[\"p2_1\"] + df[\"p2_2\"] + df[\"p2_3\"] + df[\"p2_4\"] + df[\"p2_5\"])\ndf[\"p2_Pts2Gms_OP_Ratio\"] = df[\"p2_GmsWon%\"] / df[\"p2_PtsWon%\"]\n\n# Games to Sets Over-Performing Ratio\ndf[\"p1_Gms2Sets_OP_Ratio\"] = df[\"p1_SetWon%\"] / df[\"p1_GmsWon%\"]\ndf[\"p2_Gms2Sets_OP_Ratio\"] = df[\"p2_SetWon%\"] / df[\"p2_GmsWon%\"]\n\n# Break Points Over-Performing Ratio\ndf[\"p1_BPWon%\"] = (df[\"p2_bpFaced\"] - df[\"p2_bpSaved\"] + df[\"p1_bpSaved\"]) / (df[\"p1_bpFaced\"] + df[\"p2_bpFaced\"])\ndf[\"p1_BP_OP_Ratio\"] = df[\"p1_BPWon%\"] / df[\"p1_PtsWon%\"]\n\ndf[\"p2_BPWon%\"] = (df[\"p1_bpFaced\"] - df[\"p1_bpSaved\"] + df[\"p2_bpSaved\"]) / (df[\"p1_bpFaced\"] + df[\"p2_bpFaced\"])\ndf[\"p2_BP_OP_Ratio\"] = df[\"p2_BPWon%\"] / df[\"p2_PtsWon%\"]\n\n# Break Points Saved Over-Performing Ratio\ndf[\"p1_BPSaved%\"] = df[\"p1_bpSaved\"] / df[\"p1_bpFaced\"]\ndf[\"p1_BPSaved_OP_Ratio\"] = df[\"p1_BPSaved%\"] / df[\"p1_ServeWon%\"]\n\ndf[\"p2_BPSaved%\"] = df[\"p2_bpSaved\"] / df[\"p2_bpFaced\"]\ndf[\"p2_BPSaved_OP_Ratio\"] = df[\"p2_BPSaved%\"] / df[\"p2_ServeWon%\"]\n\n# Break Points Converted Over-Performing Ratio\ndf[\"p1_BPConverted_OP_Ratio\"] = df[\"p1_BPConverted%\"] / df[\"p1_ReturnWon%\"]\ndf[\"p2_BPConverted_OP_Ratio\"] = df[\"p2_BPConverted%\"] / df[\"p2_ReturnWon%\"]\n\n # Extras I might need\ndf[\"p1_Ace%\"] = df[\"p1_ace\"]/df[\"p1_svpt\"]\ndf[\"p1_DF%\"] = df[\"p1_df\"]/df[\"p1_svpt\"]\ndf[\"p1_1stServe%\"] = df[\"p1_1stIn\"] / df[\"p1_svpt\"]\ndf[\"p1_1stReturnWon%\"] = (df[\"p2_1stIn\"] - df[\"p2_1stWon\"]) / df[\"p2_1stIn\"]\n\ndf[\"p2_Ace%\"] = df[\"p2_ace\"]/df[\"p2_svpt\"]\ndf[\"p2_DF%\"] = df[\"p2_df\"]/df[\"p2_svpt\"]\ndf[\"p2_1stServe%\"] = df[\"p2_1stIn\"] / df[\"p2_svpt\"]\ndf[\"p2_1stReturnWon%\"] = (df[\"p1_1stIn\"] - df[\"p1_1stWon\"]) / df[\"p1_1stIn\"]\n\n# Upsets\ndf[\"p1_UpsetScored\"] = [1 if (row[\"p1_Rank\"] < row[\"p2_Rank\"] and row[\"p1_win\"] == 1) else 0 for i,row in df.iterrows()]\ndf[\"p2_UpsetScored\"] = [1 if (row[\"p1_Rank\"] > row[\"p2_Rank\"] and row[\"p1_win\"] == 0) else 0 for i,row in df.iterrows()]\ndf[\"p1_UpsetAgainst\"] = df[\"p2_UpsetScored\"]\ndf[\"p2_UpsetAgainst\"] = df[\"p1_UpsetScored\"]\n\n# Rank variation\nr1, r2 = [], []\nfor i,row in tqdm(df.iterrows()):\n    year = row[\"year\"]\n    month = row[\"month\"]\n    p1 = row[\"p1_name\"]\n    p2 = row[\"p2_name\"]\n    p1_rank = row[\"p1_Rank\"]\n    p2_rank = row[\"p2_Rank\"]\n    \n    if month < 7:\n        year -= 1\n        month = 12 + month - 6\n    else:\n        month -= 6\n        \n    try:\n        aux1 = df[((df[\"year\"] == year) & (df[\"month\"] <= month)) | (df[\"year\"] < year)].loc[(df[\"p1_name\"] == p1) | (df[\"p2_name\"] == p1)].iloc[-1]\n        prev_rank1 = aux1[\"p1_Rank\"] if aux1[\"p1_name\"] == p1 else aux1[\"p2_Rank\"]\n        r1.append(prev_rank1 - p1_rank)\n    except:\n        r1.append(0)\n        \n    try:\n        aux2 = df[((df[\"year\"] == year) & (df[\"month\"] <= month)) | (df[\"year\"] < year)].loc[(df[\"p1_name\"] == p2) | (df[\"p2_name\"] == p2)].iloc[-1]\n        prev_rank2 = aux2[\"p1_Rank\"] if aux2[\"p1_name\"] == p2 else aux2[\"p2_Rank\"]\n        r2.append(prev_rank2 - p2_rank)\n    except:\n        r2.append(0)\n    \ndf[\"p1_RankVariation\"] = r1\ndf[\"p2_RankVariation\"] = r2\n\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.3. Handling Null Values\nLet's examine the proportion of missing values in the entire dataset:","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_rows', 50)\ndf.isnull().sum().sort_values(ascending=False).head(50)/df.shape[0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Columns corresponding to sets 3, 4 and 5 would have a lot of missing values if I hadn't converted them into 0. That makes sense, since some tournaments are \"best of 3\" and a 2-0 would actually finish the match, without the need of playing set 3). I don't really need them from now on, so I think I'll be getting rid of them.\n\nSome heights are also missing. Around 21% of the games lack this feature (for either winner and/or loser players). I could use an imputation but I prefer not to, it just doesn't sound natural to me. For now, I won't be removing it, but I will if it proves non or low-correlated with the chances of winning.\n\nAlso, there's the 0.1016% of games missing the BPRatio feature. This is because they depend on percentage features which could be NaN due to a 0 by 0 division. These are demanding a clear dropna(). Same with other advanced features I've created.\n\nI'll also drop those rows with missing values in the traditional stats (ace, double faults...) which all are missing in the same rows.\n\nApart from these, the rest of missing values (minutes column) can be imputed using Sklearn.","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_rows', 20) # Get it back to the default we established\n\n# 1. drop set columns\ndf.drop(columns = [\"p1_1\", \"p2_1\", \"p1_2\", \"p2_2\", \"p1_3\", \"p2_3\", \"p1_4\", \"p2_4\", \"p1_5\", \"p2_5\"], inplace=True)\n\n# 2. drop na\ndf.dropna(subset = [\"p1_BPRatio\", \"p2_BPRatio\", \"p1_Gms2Sets_OP_Ratio\", \"p2_Gms2Sets_OP_Ratio\",\n                    \"p1_Pts2Gms_OP_Ratio\", \"p2_Pts2Gms_OP_Ratio\", \"p1_GmsWon%\", \"p1_2ndWon%\",\n                    \"p1_sets\"], inplace=True)\n\n# 3. Impute using sklearn\nsi = SimpleImputer(strategy = \"median\")\nsi.fit(df[[\"minutes\"]])\ndf[[\"minutes\"]] = si.transform(df[[\"minutes\"]])\n\n# I'll keep working with the same variable, df, all the time (aesthetics). It's a good practice to save \n# the current work in another variable, just in case I mess up later.\ndf_v2 = df.copy()\n\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Someone could think we've done enough. Data is apparently clean and is ready to be analyzed, that is true. But let's go back to the goals or the motivations I had to create this analysis: forecasting a match-winner.\n\nIf I happened to analyze all these features and how they correlated with player 1 winning the match, that'd be an error and could lead to misleading conclusions. Why? Because each row contains the data from that match, which is information we obviously don't have before the event, when we want to make the prediction.\n\nWe should, somehow, do something to use the information previous to match-time to analyze its effects on the outcome of the game. How? **Rolling averages**\n\n### 5.4. Rolling Averages\n\nCreating a rolling average simply consists in imputing the average of the previous X rows in the current row. We want to make it in a way that it doesn't take into account the actual row for the average, and the number of games I'll be using is 30.\n\nFurthermore, I'll add the Win% feature.","metadata":{}},{"cell_type":"code","source":"# 1. Transform to long-format table (two rows per match, one per each player)\np1 = [col for col in df.columns if \"p1_\" in col and col != \"p1_win\"]\np2 = [col for col in df.columns if \"p2_\" in col]\ninfo = [col for col in df.columns if \"p1_\" not in col and \"p2_\" not in col]\n\nnew_cols = [\"Win\"] + info + [col[3:] for col in p1]\nl = []\nfor i,row in df.iterrows():\n    l.append([row[\"p1_win\"]] + row[info + p1].tolist())\n    l.append([abs(1-row[\"p1_win\"])] + row[info + p2].tolist())\n    \ndf = pd.DataFrame(l, columns = new_cols)\nplayers = pd.unique(df[\"name\"])\ndf[\"Win%\"] = df[\"Win\"]\n\n# Columns to average\nnums_avg = [\n    \"minutes\", \"sets\", \"ace\", \"df\", \"svpt\", \"1stIn\", \"1stWon\", \"2ndWon\", \"SvGms\",\n    \"bpSaved\", \"bpFaced\", \"1stWon%\", \"2ndWon%\", \"1stServeEffectiveness\", \"Ret2ServPtsRatio\", \"ServeWon%\",\n    \"ReturnWon%\", \"PtsDominanceRatio\", \"BPConverted%\", \"BPRatio\", \"SetWon%\", \"PtsWon%\", \"Pts2Sets_OP_Ratio\",\n    \"GmsWon%\", \"Pts2Gms_OP_Ratio\", \"Gms2Sets_OP_Ratio\", \"BPWon%\", \"BP_OP_Ratio\", \"BPSaved%\", \"BPSaved_OP_Ratio\",\n    \"BPConverted_OP_Ratio\", \"Ace%\", \"DF%\", \"1stServe%\", \"1stReturnWon%\", \"UpsetScored\", \"UpsetAgainst\", \"Win%\"\n           ]\n# Rolling averages \nwindow = 30\nfor player in tqdm(players):\n    for col in nums_avg:\n        df.loc[df[\"name\"] == player, col] = (\n            df.loc[df[\"name\"] == player, col].shift(1).rolling(window, min_periods = 10).mean()\n        )\n        \ndf.rename(columns={\"UpsetScored\": \"UpsetsScored%\", \"UpsetAgainst\": \"UpsetsAgainst%\"}, inplace=True)\ndf","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Instead of re-converting it into a wider format, I actually think keeping it like this could be benefitial in analysis time (we don't have to differ between player 1 and player 2, so we have a clearer picture of how stats correlate with other features).\n\n\n## 6. DATA ANALYSIS AND VISUALIZATION","metadata":{}},{"cell_type":"markdown","source":"### 6.1. Summary\nLet's quickly run a describe() to see our data briefly summarized.","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.2. Categorical data\n\nI want to start visualizing data. Starting simple, let's just see how categorical data is distributed by plotting the value-counts of each type.","metadata":{}},{"cell_type":"code","source":"categorical = [\"Court\", \"Surface\", \"hand\", \"win\", \"Round\"]\nnumerical = [ \n    'Rank','Pts', 'sets', 'B365','ht', 'age', 'ace', 'df', 'svpt', \n    '1stIn', '1stWon', '2ndWon', 'SvGms', 'bpSaved', 'bpFaced', \"1stServeEffectiveness\", \n    \"Ret2ServPtsRatio\", \"PtsDominanceRatio\", \"BPRatio\", \"Pts2Sets_OP_Ratio\", \"Pts2Gms_OP_Ratio\", \n    \"Gms2Sets_OP_Ratio\", \"BP_OP_Ratio\", \"BPSaved_OP_Ratio\", \"BPConverted_OP_Ratio\", \"Ace%\", \n    \"DF%\", \"1stServe%\", \"1stReturnWon%\"\n]\nother = [\"year\", \"month\", \"name\"]\n\n# Useful lists\nres = [\"Win\"]\n\nfig, axs = plt.subplots(2,2, constrained_layout=True)\nfig.suptitle(\"Value Counts of Court, Surface, Playing Hand and Favorite player winning the game\")\nsns.countplot(x=\"Court\", data=df, palette=\"Set2\", ax = axs[0,0])\nsns.countplot(x=\"Surface\", data=df, palette=\"Set2\", ax = axs[0,1])\nsns.countplot(x=\"hand\", data=df, palette=\"Set2\", ax = axs[1,0])\nsns.countplot(x=\"Win\", data=df, palette=\"Set2\", ax = axs[1,1])\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcParams[\"figure.figsize\"] = [12,5]\nfig, axs = plt.subplots(1, 3, constrained_layout=True)\nfig.suptitle(\"Value Counts of Court, Surface, Playing Hand for both winner and loser.\")\nsns.countplot(x=\"Court\", data=df, hue = \"Win\", palette=\"Set2\", ax = axs[0])\nsns.countplot(x=\"Surface\", data=df, hue = \"Win\", palette=\"Set2\", ax = axs[1])\nsns.countplot(x=\"hand\", data=df, hue = \"Win\", palette=\"Set2\", ax = axs[2])\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Obvious observations:\n1. Most of the tournaments are played **outdoors** and the favorite has higher chances of winning when it's that way.\n2. Hard is the most common surface, followed by clay, grass and carpet (in order). \n3. Most of the players are right-handed, just like in real workd.\n\nNothing surprising here, we're just getting to know the data better!\n\n### 6.3. Numerical data\n\nTime for a heatmap? Let's see if there's correlation between numeric variables and the target feature by taking a look at the correlation matrix.","metadata":{}},{"cell_type":"code","source":"# Pearson Correlation\ncorr_matrix = df[numerical + res].corr().sort_values(by=[\"Win\"])\ncorr_matrix","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating a heatmap for all the variables we have would create an ugly plot with a lot of numbers overlapping each other. That's not nice to see... I'll be instead showing the most-correlated with player 1 actually winning the game.","metadata":{}},{"cell_type":"code","source":"plt.rcParams[\"figure.figsize\"] = [12,8]\nmatrix = pd.concat([corr_matrix.iloc[:3], corr_matrix.iloc[-9:]])\n\nsns.heatmap(matrix[matrix.index], cmap='Blues', annot=True)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is interesting. None of the correlations is huge, but some are above the 20% threshold, which makes them worth looking at! \n\nWe can get good insights from here, like the fact that the **Points to Games Over-Performance Ratio**, the **Points to Sets Over-Performance Ratio**, the **Games to Sets Over-Performance Ratio** and the average number of sets won are somewhat correlated as well as the **Points Dominance Ratio** and, obviously, the **odds**. We can tell bookies are doing a good job if they are the number-one feature in terms of correlation. \n\nWhat's surprising, at least it shocks me, is that the Win% doesn't seem much relevant, nor any other advanced feature which is not a ratio. Curious.\n\nMost of those stats being highly correlated between themselves. And it makes sense, they share a linear relationship. Should we study that?","metadata":{}},{"cell_type":"code","source":"sns.pairplot(data = df[[\"Win\", \"PtsDominanceRatio\", \"Gms2Sets_OP_Ratio\", \"sets\", \"Pts2Gms_OP_Ratio\", \"Pts2Sets_OP_Ratio\"]], diag_kind = 'kde', hue = \"Win\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well that's very clear right? They all show a pretty strong linear relationship and it's a signal of dependence. That's normal, as they were built as a combination of other basic features.\n\nI've also added this hue which indicates if a certain point resulted in a win or loss by that player, just to see how colors were distributed throughout the graphs. To be honest, I expected it to be more distinguishible. If we look at the numbers as well as the plots, the pattern exists: in a general way, orange points tend to occupy more space in one extreme and blue in the other. But, again, it's not extremely clear.\n\nI'd now move to feature importance but I'd first like to start encoding categorical data, so I can take them into account too.\n\n### 6.4. Feature Importance","metadata":{}},{"cell_type":"code","source":"#One hot encoding\nsurface=pd.get_dummies(df[\"Surface\"], prefix='surface_')\ndf = pd.concat([df,surface],axis=1)\ndf.drop(columns='Surface', inplace=True)\n\nhand=pd.get_dummies(df[\"hand\"], prefix='hand_')\ndf = pd.concat([df,hand],axis=1)\ndf.drop(columns='hand', inplace=True)\n\nplaying_round=pd.get_dummies(df[\"Round\"])\ndf = pd.concat([df,playing_round],axis=1)\ndf.drop(columns='Round', inplace=True)\n\ndf[\"Court\"].replace(to_replace=['Outdoor','Indoor'],value=[1,0], inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have all relevant data in numeric format, it's time to decide upon the features we'll be using for the model. There's several ways to do so, but I'm going to use one of the easiest and simplest because that can probably be enough. \n\nI'll be using sklearn's RandomForestClassifier, which could serve as the predictive model in and of itself, but it also internally ranks features in terms of importance. That's why I'm choosing this method. Again, there are way more ways to do it and are probably way better.","metadata":{}},{"cell_type":"code","source":"df.dropna(subset=[\"minutes\", \"ht\"], inplace=True)\ndf.fillna(0, inplace=True)\ndf.replace([np.inf, -np.inf], 0, inplace=True)\n\nfeature_names = [feature for feature in df.drop(columns=[\"Win\", \"name\", \"year\", \"month\"]).columns]\nforest = RandomForestClassifier(random_state=0)\nforest.fit(df.drop(columns=[\"Win\", \"name\", \"year\", \"month\"]), df[[\"Win\"]])\n\nimportances = forest.feature_importances_\nstd = np.std([\n    tree.feature_importances_ for tree in forest.estimators_], axis=0)\n\nforest_importances = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n\nplt.rcParams[\"figure.figsize\"] = [15,7]\nfig, ax = plt.subplots()\nforest_importances.plot.bar(yerr=std, ax=ax)\nax.set_title(\"Feature importances\")\nax.set_ylabel(\"Mean decrease in impurity\")\nfig.tight_layout()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This plot is ordered in terms of importance. We see the odds being the most important by far, and then the vast majority have pretty much the same relevance. I'm glad 7 of the top 10 are the advanced, created features.\n\nLower imporance features are those that correspond to the one-hot encoded features. That makes sense and this shouldn't be a reason to leave them out. Even so, I'm planning on dropping the columns corresponding to the round being played, I don't like this feature.\n\nI'll also drop height.\n\nApart from that, I think we actually have enough! The data observed in our correlation visualization and what this random forest model provides is insightful and helps us determine what stats are important.\n\n## 7. CONCLUSIONS\n\nThis project has given us some useful insights in terms of which stats and features may be the most important when it comes to forecasting a match-winner in tennis. Most of the tasks were focused on the data preparation and processing, but we also used some visualizations to understand better our data and get the results we were looking for.\n\nOdds are, by far, the most important feature. And it makes sense, bookies are right around 70% of the time! It also surprised me the relevance the player's rank and points have. I conclude from this that ATP points are solid and significant, pretty well computed.\n\nApart from that, we've already seen some more advanced ratios and how they correlate with the player's chances of winning, as well as some other more advanced stats. \n\nAge seems to be a considerable factor, more than a lot more features which someone could have thought were important (like Ace%, Double Faults...).\n\nLastly, I want to highlight my surprise to see that the Win% within the last 30 games seems negligible. Also, upset-related stats aren't much important either.\n\n\n## 8. IMPROVEMENTS & NEXT STEPS\nThere's several things I could have done. I wanted to keep this as simple as possible, even though it ended up being quite long. \n\nWhat I've restricted myself from doing is adding a lot more new features, like the **implied probability** (which comes from the odds), **player fatigue**...\n\nI could have also studied the number of previous games we'd be performing the rolling averages over. I chose 30 but it could have been any other number. For better results, we should aim to find the number that optimizes our future results.\n\nThe next steps are clear: build the model and find a betting strategy to make it profitable.\n\n## 9. REFERENCES\nThe data comes from two sources:\n* [Jeff Sackmann](https://github.com/JeffSackmann/tennis_atp): he has an amazing set of files with almost everything one would need. I used some of his files to get the stats per match.\n* [Tennis-data.co.uk](http://www.tennis-data.co.uk/alldata.php): It was used to get the information related to odds as well as the match results and other qualitative information.\n\nI also used [Ultimate Tennis Statistics](https://www.ultimatetennisstatistics.com/glossary) to create new features and learn more about tennis itself.\n","metadata":{}}]}