{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfrom collections import Counter\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/pokemon/Pokemon.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Firstly we need get rid of the spaces between column names to be easier, you can do it like that or you can use split() method also","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.rename(columns={\"Type 1\":\"Type1\", \"Type 2\":\"Type2\", \"Sp. Atk\":\"Sp.Atk\", \"Sp. Def\":\"Sp.Def\"},inplace=True)\ndata.columns = data.columns.str.strip()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Types:\",data[\"Type1\"].unique().tolist())\nprint(\"Amount of types:\",len(data[\"Type1\"].unique().tolist()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's drop the Type2 column \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(\"Type2\",axis = 1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"Type1\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# objects = (\"Water\",\"Normal\",\"Grass\",\"Bug\",\"Psychic\",\"Fire\",\"Rock\",\"Electric\",\"Dragon\",\"Ground\",\"Ghost\",\"Dark\",\"Poison\",\"Fighting\",\"Steel\",\"Ice\",\"Fairy\",\"Flying\")\n# y_pos = np.arange(len(objects))\n# performance = [112,98,70,69,57,52,44,44,32,32,32,31,28,27,27,24,17,4]\n# \n# plt.figure(figsize=(22,10))\n# plt.bar(y_pos, performance, align='center', alpha=0.6)\n# plt.xticks(y_pos, objects)\n# plt.xlabel(\"Types\")\n# plt.ylabel('Frequency')\n# plt.title('Pokemon Types')\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CATEGORİCAL PLOTTING\nvar= data[\"Type1\"]\n#count number of categorical variable(value/sample)\nvarValue = var.value_counts()\n\n#visualize\nplt.figure(figsize=(20,10))\nplt.bar(varValue.index,varValue)\nplt.xticks(varValue.index,varValue.index.values) #With this command we can limit the ticks  \nplt.ylabel(\"Frequency\")\nplt.title(\"Pokemon Types\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_bar(variable):\n    \n    var = data[variable]\n    varValue = var.value_counts()\n    \n    plt.figure(figsize=(15,10))\n    plt.bar(varValue.index,varValue)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} distrubution with barplot\".format(variable))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numericVar = [\"HP\",\"Attack\",\"Total\",\"Defense\",\"Sp.Atk\",\"Sp.Def\",\"Speed\"]\nfor n in numericVar:\n    plot_bar(n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Artifical Neural Network (ANN)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"-  Neural Network is one of the provided version of Logistic Regression \n-  Remember!!! In the Logistic Regression there are input and output layers \n-  In ANN there must be minimum 1 hidden layer between input and output layer. When hidden layer amount increase model will be more complicated.\n-  When model getting more complicated our model would be more succesfull but more slow then we need find the optimum option \n-  Most of steps are same and there is just one difference that is Layer Term, we going to see the details (\"What is Layer?\") \n-  In other words ANN is applying Logistic Regression minimum 2 times (if you apply 2 times called as 2-Layer, more >> L-Layer ANN)\n-  Okay how will we choose the exact number of layer, what is the optimum layer amount for models ? \n-  Actually it depends your hardware. For instance in my device 5 - 6 hidden layer force the hardware. For commercial projects 120-130 hidden layer might optimum value\n-  When we say 2 layer network means 1 Hidden + 1 Output Layer >>(Input layer doesn't count as layer) \n- ## Let's Start ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a href=\"http://ibb.co/eF315x\"><img src=\"http://preview.ibb.co/dajVyH/9.jpg\" alt=\"9\" border=\"0\"></a>\n* Step by step we will learn this image.\n    * Our hyperparameters : Number of Hidden Layer , Node ( Number of element in hidden layer - in example there are 3), Learning Rate \n    * Input and output layers do not change. They are same like logistic regression.\n    * In image, there is a tanh function that is unknown for you. It is a activation function like sigmoid function. Tanh activation function is better than sigmoid for hidden units bacause mean of its output is closer to zero so it centers the data better for the next layer. Also tanh activation function increase non linearity that cause our model learning better.\n    * As you can see with purple color there are two parts. Both parts are like logistic regression. The only difference is activation function, inputs and outputs.\n        * In logistic regression: input => output\n        * In 2 layer neural network: input => hidden layer => output. You can think that hidden layer is output of part 1 and input of part 2.\n* Thats all. We will follow the same path like logistic regression for 2 layer neural network.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1-) 2 Layer Neural Network ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop([\"Name\"],axis=1,inplace=True)\ndata.drop([\"#\"],axis =1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#List Comprehension \ndata[\"Type1\"] = [0 if i == \"Water\" else 1 if i == \"Normal\" else 2 if i== \"Grass\" else 3 if i==\"Bug\" else 4 if i==\"Fire\" or i==\"Psychic\" else 5 if i==\"Electric\" or i==\"Rock\"\n                 else 6 if i==\"Dragon\" or i==\"Ground\" else 7 if i==\"Ghost\" or i==\"Dark\" else 8 if i==\"Poison\" or i==\"Fighting\" else 9 if  i==\"Steel\" or i==\"Ice\" else 10 for i in data[\"Type1\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, all of the columns are int64 okey let's start the modelling ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"TYPE 1 Categorization\n- 0 - Water \n- 1 - Normal\n- 2 - Grass \n- 3 - Bug\n- 4 - Fire + Psychic\n- 5 - Electric + Rock \n- 6 - Dragon + Ground\n- 7 - Ghost + Dark \n- 8 - Poison + Fighting \n- 9 - Steel + Ice \n- 10 - Fairy + Flying\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#CATEGORİCAL PLOTTING\nvar= data[\"Type1\"]\n#count number of categorical variable(value/sample)\nvarValue = var.value_counts()\n\n#visualize\nplt.figure(figsize=(20,10))\nplt.bar(varValue.index,varValue)\nplt.xticks(varValue.index,varValue.index.values) #With this command we can limit the ticks  \nplt.ylabel(\"Frequency\")\nplt.title(\"Pokemon Types\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.1) Train & Test Split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[:,\"Type1\"].value_counts()\ny = data.Type1.values\nx_data = data.iloc[:,2:]\nx = (x_data - np.min(x_data))/(np.max(x_data)-np.min(x_data)) #Normalization\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.2) Initialize Weight & Bias and Sigmoid Function \n<img src=\"https://miro.medium.com/max/4000/1*JHWL_71qml0kP_Imyx4zBg.png\" alt=\"3\" border=\"0\">","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_weights_and_bias_NN(x_train,y_train):\n    parameters = {\"weight1\":np.random.randn(3,x_train.shape[0])*0.1, #3 x 600\n                  \"bias1\":np.zeros((3,1)),\n                  \"weight2\":np.random.randn(y_train.shape[0],3)*0.1, #600 x 3\n                  \"bias2\":np.zeros((y_train.shape[0],1))} \n        # We define 3x1 because z2 = w2*A1 + b2 then A1 is 3x1 and\n        # when we want to matris product w2 must be 1x3 \n    return parameters \n\ndef sigmoid(z):\n    y_head = 1/(1+np.exp(-z))\n    return y_head","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.3) Forward Propagation \n<img src=\"http://preview.ibb.co/dajVyH/9.jpg\" alt=\"9\" border=\"0\">","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def forward_propagation_NN(x_train,parameters):\n    Z1 = np.dot(parameters[\"weight1\"],x_train) + parameters[\"bias1\"]\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(parameters[\"weight2\"],A1) + parameters[\"bias2\"]\n    A2 = sigmoid(Z2)\n    \n    cache={\"Z1\":Z1,\n           \"A1\":A1,\n           \"Z2\":Z2,\n           \"A2\":A2}\n    return A2,cache","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.4) Loss and Cost Function \n<img src=\"https://cdn-images-1.medium.com/max/800/1*EJPT0utTkQ2qrHfjDID5RA.png\" alt=\"9\" border=\"0\">","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_cost_NN(A2,y,parameters): #A2 is an output actually but we gonna use it input for cost function\n    logaritmicprobs = np.multiply(np.log(A2),y)\n    cost = -np.sum(logaritmicprobs)/y.shape[1]\n    return cost","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.5) Backward Propagation \n<img src=\"https://www.guru99.com/images/1/030819_0937_BackPropaga1.png\" alt=\"9\" border=\"0\">","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def backward_propagation_NN(cache,parameters,x,y): #Derivative Part\n    dZ2 = cache[\"A2\"]-y\n    dW2 = np.dot(dZ2,cache[\"A1\"].T)/x.shape[1]\n    db2 = np.sum(dZ2,axis =1,keepdims=True)/x.shape[1]\n    dZ1 = np.dot(parameters[\"weight2\"].T,dZ2)*(1 - np.power(cache[\"A1\"], 2))\n    dW1 = np.dot(dZ1,x.T)/x.shape[1]\n    db1 = np.sum(dZ1,axis =1,keepdims=True)/x.shape[1]\n    grads = {\"dweight1\": dW1,\n             \"dbias1\": db1,\n             \"dweight2\": dW2,\n             \"dbias2\": db2}\n    return grads","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.6) Updating Parameters \n- Weight = Weight - (learning rate x Derivative Weight)\n- Bias = Bias - (learning rate x Derivative Bias)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def update_parameters_NN(parameters,grads,learning_rate = 0.01):\n    parameters = {\"weight1\": parameters[\"weight1\"]-learning_rate*grads[\"dweight1\"],\n                      \"bias1\": parameters[\"bias1\"]-learning_rate*grads[\"dbias1\"],\n                      \"weight2\": parameters[\"weight2\"]-learning_rate*grads[\"dweight2\"],\n                      \"bias2\": parameters[\"bias2\"]-learning_rate*grads[\"dbias2\"]}\n    \n    return parameters","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.7) Prediction Part ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def prediction_NN(parameters,x_test): #Remember predictions always apply w/test data\n    A2,cache = forward_propagation_NN(x_test,parameters)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5 our prediction is sign one (y_head=1),\n    # if z is equal or smaller than 0.5, our prediction is sign zero (y_head=0) >> Sigmoid func\n    \n    for i in range(A2.shape[1]):\n        if A2[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n            \n    return Y_prediction","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.8) Creating Artificial Neural Network Model - 2 Layer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def two_layer_NN(x_train,y_train,x_test,y_test,num_iterations):\n    cost_list = []\n    index_list = []\n    #initializing parameters and layer size\n    parameters = initialize_weights_and_bias_NN(x_train,y_train)\n    \n    for i in range(0,num_iterations):\n        #Forward propagation \n        A2,cache = forward_propagation_NN(x_train,parameters)\n        #Computing Cost \n        cost = loss_cost_NN(A2,y_train,parameters)\n        #Backward Propagation\n        grads = backward_propagation_NN(parameters,cache,x_train,y_train)\n        #Updating Parameters \n        parameters = update_parameters_NN(parameters,grads)\n        \n        if i % 50 == 0: #Per 50 iterations\n            cost_list.append(cost)\n            index_list.append(i)\n            print(\"Cost after Iteration %i: %f\" %(i,cost))\n            \n    plt.plot(index_list,cost_list)\n    plt.xticks(index_list,rotation = 45)\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    \n    #Prediction \n    y_prediction_test = prediction_NN(parameters,x_test)\n    y_prediction_train  = prediction_NN(parameters,x_train)\n    \n    #Printing Train & Test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    return parameters\n\nparameters = two_layer_NN(x_train,y_train,x_test,y_test,num_iterations=2500)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2) L Layer Neural Networks w/Keras Library \n<img src=\"https://pskp-95.github.io/public/images/multiple_layers.png\" alt=\"9\" border=\"0\">\n* There are some hyperparameters we need to choose like learning rate, number of iterations, number of hidden layer, number of hidden units, type of activation functions. Woww it is too much :)\n* These hyperparameters can be chosen intiutively if you spend a lot of time in deep learning world.\n* However, if you do not spend too much time, the best way is to google it but it is not necessary. You need to try hyperparameters to find best one.\n* In this tutorial our model will have 2 hidden layer with 8 and4 nodes, respectively. Because when number of hidden layer and node increase, it takes too much time. \n* As a activation function we will use *elu(first hidden layer), relu(second hidden layer) and sigmoid(output layer) respectively.\n* Number of iteration will be 100.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- units = node amount in hidden layer\n- kernel_initializer = initialize weight\n- activation = relu,tanh,sigmoid >> relu limits the values 0 by n if input <0 output is 0 but if input >0 output is input for ex : -1 == 0 , 7 == 7\n\n- Ps: Keras requres the defined input and output dimension\n- input_dim = input dimension \n- loss = \"binary_crossentropy\" is cost func same in logistic regression \n- optimizer =\"adam\" adam means adaptive momentum and \"adam\" provides us adaptive learning rate, with adam optimizer we can run our model more faster and effectively \n- metrics is our criteria to rating model performance\n- cross_val_score is cross validation gives us many accuracies and get average of them\n- epochs = number of iteration \n- cv = 3 means model will do cross validation 3 times and get average of them\n\nThis formula similar to Gradient Descent \n\n$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$ \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#reshape - keras requires tranpose of data\nx_train,x_test,y_train,y_test = x_train.T, x_test.T ,y_train.T, y_test.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# L-layer NN w/Keras\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score #cross_val_score = 2 means 4 part train 1 part test\nfrom keras.models import Sequential # Initializing NN Library\nfrom keras.layers import Dense #Layer Builder\n\ndef build_classifier():\n    classifier = Sequential() #İnitializing NN\n    classifier.add(Dense(units = 16, kernel_initializer = \"uniform\", activation =\"relu\", input_dim = x_train.shape[1])) # First Hidden layer, we defined the input dimension 600\n    classifier.add(Dense(units = 12, kernel_initializer = \"uniform\", activation =\"relu\"))\n    classifier.add(Dense(units = 8, kernel_initializer = \"uniform\", activation =\"relu\"))\n    classifier.add(Dense(units = 6, kernel_initializer = \"uniform\", activation =\"relu\"))\n    classifier.add(Dense(units = 4, kernel_initializer = \"uniform\", activation =\"relu\"))\n    classifier.add(Dense(units = 2, kernel_initializer = \"uniform\", activation =\"relu\"))\n    classifier.add(Dense(units = 1, kernel_initializer = \"uniform\", activation = \"sigmoid\")) #means 1 output (1 node) and if we want to create model after hidden layer to output activation = sigmoid\n    classifier.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n    return classifier\n\n#lets call our classifier\nclassifier = KerasClassifier(build_fn = build_classifier, epochs = 500)\naccuracies = cross_val_score(estimator = classifier, X=x_train, y=y_train, cv = 3)\nmean = accuracies.mean()\nvariance = accuracies.std()\nprint(\"Accuracy Mean :\"+str(mean))\nprint(\"Accuracy Variance :\"+str(variance))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}