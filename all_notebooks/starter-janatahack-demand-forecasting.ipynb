{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <center> Demand Forecasting</center>\n________","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<center><img src='https://datahack-prod.s3.ap-south-1.amazonaws.com/__sized__/contest_cover/cover_1_3vEBqwk-thumbnail-1200x1200.png'/></center>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## About","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Demand Forecasting is the pivotal business process around which strategic and operational plans of a company are devised. Based on the Demand Forecast, strategic and long-range plans of a business like budgeting, financial planning, sales and marketing plans, capacity planning, risk assessment and mitigation plans are formulated.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Problem Statement\n\nOne of the largest retail chains in the world wants to use their vast data source to __build an efficient forecasting model__ to predict the sales for each SKU in its portfolio at its __76 different stores__ using historical sales data for the __past 3 years__ on a week-on-week basis. Sales and promotional information is also available for each week - product and store wise. \n\nHowever, no other information regarding stores and products are available. Can you still forecast accurately the sales values for every such product/SKU-store combination for the __next 12 weeks accurately__? \n\n- If yes, then dive right in! Let's Play","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data Description","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<center><img src='https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1279142%2F4f09fa27a17b01fa40700e7b80d87add%2Fdataset_description.jpg?generation=1594430740572308&alt=media'/></center>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Evaluation Metric\n- The evaluation metric for this competition is 100*RMSLE (Root Mean Squared Log Error).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Let's Begin\n\n> In this notebook i provide you some hints if you implement them in your notebook,Surely gonna get better results.(because i already implemented and got much better results).\n\n> - So keep your eye on given hints.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# To print multiple output in a cell\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Import all the required libraries\n\nimport pandas as pd\nimport numpy as np\nimport os\n\nimport matplotlib.pyplot as plt\n# % matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To begin this exploratory analysis, first import libraries and define functions for plotting the data using `matplotlib`. Depending on the data, not all plots will be made. (Hey, I'm just a simple kerneling bot, not a Kaggle Competitions Grandmaster!)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"There are 3 csv files in the current version of the dataset:\n","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv('../input/janatahack-demand-forecasting-analytics-vidhya/train.csv')\n\ntest=pd.read_csv('../input/janatahack-demand-forecasting-analytics-vidhya/test.csv')\n\nsample=pd.read_csv('../input/janatahack-demand-forecasting-analytics-vidhya/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(10)\nprint('Shape of training data is {}'.format(train.shape))\n\nprint('-------------'*5)\n\ntest.head()\nprint('Shape of test data is {}'.format(test.shape))\n\nprint('--------------'*5)\n\nsample.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Visualization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['week'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of units sold in accordance with the week\n\ntrain.groupby('week').sum()['units_sold'].plot(figsize=(12,8))\nfont = {'family': 'serif',\n        'color':  'darkred',\n        'weight': 'normal',\n        'size': 26,\n        }\nplt.xlabel('Week',fontdict=font)\nplt.ylabel('units_sold',fontdict=font)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# amount earned through sales in each week\n\ntrain.groupby('week').sum()['total_price'].plot(figsize=(12,8))\nfont = {'family': 'serif',\n        'color':  'darkred',\n        'weight': 'normal',\n        'size': 26,\n        }\nplt.xlabel('Week',fontdict=font)\nplt.ylabel('total_price',fontdict=font)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['store_id'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## product sold by each of the store\n\n\ntrain.groupby('store_id').sum()['units_sold'].plot(figsize=(15,8),kind='bar')\nfont = {'family': 'serif',\n        'color':  'darkred',\n        'weight': 'normal',\n        'size': 26,\n        }\nplt.xlabel('store_id',fontdict=font)\nplt.ylabel('units_sold',fontdict=font)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Max number of product are sold by ```8023``` store id.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Product was on display at a prominent place at the store\n\n# Impact on sales on the basis of display\ntrain.groupby(['is_display_sku','store_id']).sum()['units_sold']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# join test and train data\n\ntrain['train_or_test']='train'\ntest['train_or_test']='test'\ndf=pd.concat([train,test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating Time Based Features, So further we can convert this time series problem in to a regression one.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to utilize date time column i.e '''week'''\n\ndef create_week_date_featues(dataframe):\n\n    df['Month'] = pd.to_datetime(df['week']).dt.month\n\n    df['Day'] = pd.to_datetime(df['week']).dt.day\n\n    df['Dayofweek'] = pd.to_datetime(df['week']).dt.dayofweek\n\n    df['DayOfyear'] = pd.to_datetime(df['week']).dt.dayofyear\n\n    df['Week'] = pd.to_datetime(df['week']).dt.week\n\n    df['Quarter'] = pd.to_datetime(df['week']).dt.quarter \n\n    df['Is_month_start'] = pd.to_datetime(df['week']).dt.is_month_start\n\n    df['Is_month_end'] = pd.to_datetime(df['week']).dt.is_month_end\n\n    df['Is_quarter_start'] = pd.to_datetime(df['week']).dt.is_quarter_start\n\n    df['Is_quarter_end'] = pd.to_datetime(df['week']).dt.is_quarter_end\n\n    df['Is_year_start'] = pd.to_datetime(df['week']).dt.is_year_start\n\n    df['Is_year_end'] = pd.to_datetime(df['week']).dt.is_year_end\n\n    df['Semester'] = np.where(df['week'].isin([1,2]),1,2)\n\n    df['Is_weekend'] = np.where(df['week'].isin([5,6]),1,0)\n\n    df['Is_weekday'] = np.where(df['week'].isin([0,1,2,3,4]),1,0)\n\n    df['Days_in_month'] = pd.to_datetime(df['week']).dt.days_in_month\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=create_week_date_featues(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(5)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encode the categorical variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col=['store_id','sku_id','Is_month_start','Is_month_end','Is_quarter_start','Is_quarter_end','Is_year_start','Is_year_end']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in col:\n    df = pd.get_dummies(df, columns=[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# col2=['Is_month_start','Is_month_end','Is_quarter_start','Is_quarter_end','Is_year_start','Is_year_end']\n\n# for i in col2:\n#     df = pd.get_dummies(df, columns=[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop the columns\ndf.drop(['record_ID','week'],inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Treating skewed features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Total price columns\n\ndf['total_price'].plot(kind='hist')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['total_price']=np.log1p(df['total_price'])\ndf['total_price'].plot(kind='hist')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['base_price'].plot(kind='hist')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['base_price']=np.log1p(df['base_price'])\ndf['base_price'].plot(kind='hist')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hint1:  \n\nTreat the skewness of the target variable too. \n- In my final notebook i am using it and getting a better score.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_1=df.loc[df.train_or_test.isin(['train'])]\ntest_1=df.loc[df.train_or_test.isin(['test'])]\ntrain_1.drop(columns={'train_or_test'},axis=1,inplace=True)\ntest_1.drop(columns={'train_or_test'},axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_1.head()\ntrain_1.shape\ntest_1.shape\ntest_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_1.drop(['units_sold'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_1.shape\ntest_1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=train_1.drop(['units_sold'],axis=1)\ny=train_1['units_sold']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=x.values\ntest_data=test_1.values\n\ny=y.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.shape\ntest_data.shape\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Time to train the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# x_train,x_valid,y_train,y_valid=train_test_split(x,y,test_size=0.35)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# x_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training the model with xgboost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\nfrom xgboost import plot_importance\nimport xgboost as xgb\n\n# function to plot all features based out of its importance.\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nimport time","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating RMSLE function ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I am not using this ```RMSLE``` function in my model because of error. If anyone able to figure out let me know in the comment section. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmsle(y_true, y_pred):\n    return np.sqrt(np.mean(np.power(np.log1p(y_true)-np.log1p(y_pred), 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Perform cross-validation\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hint2:\n\nIn this notebook i use ```XGboost( a gradient boosting algorithm )```. Use other available gradient boosting algorithms,gonna get better results. \n\n- I am also using other one.\n- Don't waste you time with ```xgboost``` as this algo is not generalzing well for this data(based on my experience).\n- I though that much hint is very much sufficient and probably you got me what i am trying to say.\n\n- See you on Leaderboard.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = XGBRegressor(\n    max_depth=12,\n    booster = \"gbtree\",\n    n_estimators=200,\n    eval_metric = 'rmse',\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,\n    seed=42,\n    objective='reg:linear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kfold, scores = KFold(n_splits=5, shuffle=True, random_state=0), list()\n\nfor train, test in kfold.split(x):\n    x_train, x_test =x[train], x[test]\n    y_train, y_test = y[train], y[test]\n    model.fit(x_train, y_train,verbose=True,\n              eval_set=[(x_train, y_train), (x_test, y_test)],\n              early_stopping_rounds = 50)\n#     preds = model.predict(x_test)\n#     score = rmsle(y_test, preds)\n#     scores.append(score)\n#     print(score)\n    \n    \n# print(\"Average: \", sum(scores)/len(scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # defint the model parameters\n\n# ts = time.time()\n\n# model = XGBRegressor(\n#     max_depth=12,\n#     booster = \"gbtree\",\n#     n_estimators=500,\n#     min_child_weight=350, \n#     colsample_bytree=0.8, \n#     subsample=0.8, \n#     eta=0.3,\n#     seed=42,\n#     objective='reg:linear')\n\n# model.fit(\n#     x_train, \n#     y_train, \n#     eval_metric=\"rmse\", \n#     eval_set=[(x_train, y_train), (x_valid, y_valid)], \n#     verbose=True, \n#     early_stopping_rounds = 100)\n\n# time.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create prediction on test data\n\npred=model.predict(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample['units_sold']=pred.round()\n\nsample['units_sold']=pred\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample['units_sold'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Treat the -ve predicted values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sample['units_sold']=abs(sample['units_sold']).astype('int')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create the final submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sample.to_csv('submission_xgb.csv',index=False,encoding='utf-8')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Note: \n- With this kernel I have no intention to ruin the competition spirit, it is created to just help you to get started.\n\nIf you like my work then do \n\n    - Do follow\n    - Do upvote\n    - Have doubts regarding this kernel use comment section.\n    \n\nAs this competition is still going on .So I will upload my final kernel once competition got over.\n\n- Stay tuned for new and improved version.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Note1:\n\nMy current ranking in the table is ```29```.\n- If any one want some hint let me know in the comment section. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> Edit-23/07/2020","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Trying some thing new\n\nAs this competition is over but resently i amazed with new ensemble technique ```MinMaxBestBaseStacking```. So to just check is it really effective or not. \n\n[Original Source Repo](https://github.com/QuantScientist/Deep-Learning-Boot-Camp/blob/master/Kaggle-PyTorch/PyTorch-Ensembler/utils.py)\n\n- For the sake of learning.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## MinMaxBestBaseStacking","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_path = \"../input/testing-minmaxbestbasestacking\"\nall_files = os.listdir(sub_path)\nprint(all_files)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for f in all_files:\n    print(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d=pd.read_csv('../input/testing-minmaxbestbasestacking/866_126527_us_submission_lgbm_22.csv')\nd.head()\nd.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Read and concatenate submissions\n\nouts = [pd.read_csv(os.path.join(sub_path, f), index_col=0) for f in all_files ]\nconcat_sub = pd.concat(outs, axis=1)\nconcat_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = list(map(lambda x: \"target\" + str(x), range(len(concat_sub.columns))))\n\ncols = list(map(lambda x: \"target\" + str(x), range(len(concat_sub.columns))))\nconcat_sub.columns = cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"concat_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"concat_sub.reset_index(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"concat_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ncol = concat_sub.shape[1]\nncol","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the data fields ready for stacking\nconcat_sub['target_mean'] = concat_sub.iloc[:, 1:ncol].mean(axis=1)\nconcat_sub['target_median'] = concat_sub.iloc[:, 1:ncol].median(axis=1)\nconcat_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"concat_sub.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create 1st submission for the mean with round\n# concat_sub['target_mean'].round()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col2=['record_ID','units_sold']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# concat_sub['target'] = concat_sub['target_mean']\n\nconcat_sub['target'] = concat_sub['target_mean'].round()\ndata=concat_sub[['record_ID', 'target']]\ndata.columns=col2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data.to_csv('submission_mean.csv', index=False, float_format='%.6f')\n\ndata.to_csv('submission_mean_round.csv', index=False, float_format='%.6f')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Got this \n\n> Your private score for this submission is : 449.7631659776047, \n\n> Had it been a live contest, your rank would be : 24\n\n\nMy actual ranking in this competion is 44.. Got a improvemnt of 20. Really impressed with the results.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create 1st submission for the median","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_med=data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_med['units_sold']=concat_sub['target_median']\ndata_med","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_med.to_csv('submission_median.csv', index=False, float_format='%.6f')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Got this \n\n> Your private score for this submission is : 453.80995302601514, \n> Had it been a live contest, your rank would be : 33\n\nRemember my final standing in the competion is 44. So in both cases my ranking is improve.\n\n- So Ensemble techniqe is working.\n\n- Be ready to use it in upcoming hackathons.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Check the prediction with the Blend of mean and median ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['units_sold']= 1.85/3 * data['units_sold'] + 1.15/3 * data_med['units_sold']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.to_csv('submission_mean_median_blend_2.csv', index=False, float_format='%.6f')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No improvement ... ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}