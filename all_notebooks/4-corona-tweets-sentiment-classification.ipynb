{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom keras.preprocessing.sequence import pad_sequences\nimport keras\nfrom keras.layers import Input,Embedding,Bidirectional,LSTM,Dense,Dropout,TimeDistributed,GlobalAveragePooling1D,BatchNormalization,GlobalMaxPool1D\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\nfrom sklearn.naive_bayes import GaussianNB,BernoulliNB,MultinomialNB\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nimport re\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/covid-19-nlp-text-classification/Corona_NLP_train.csv\",encoding=\"L1\")\ntest_df = pd.read_csv(\"../input/covid-19-nlp-text-classification/Corona_NLP_test.csv\",encoding=\"L1\")\n\nprint(f\"train dataset shape >> {train_df.shape}\")\nprint(f\"test dataset shape >> {test_df.shape}\")\n\ndef data_label_split(dataset):\n    data = dataset['OriginalTweet']\n    label = dataset['Sentiment']\n    return data,label\n\ntrain_data,train_label = data_label_split(train_df)\ntest_data,test_label = data_label_split(test_df)\n\ntrain = pd.DataFrame({\n    'label':train_label,\n    'data':train_data\n})\n\ntest = pd.DataFrame({\n    'label':test_label,\n    'data':test_data\n})\n\ndef reassign_label(x):\n    if x == \"Extremely Positive\" or x == \"Positive\":\n        return 1\n    elif x ==\"Extremely Negative\" or x ==\"Negative\":\n        return -1\n    elif x ==\"Neutral\":\n        return 0\n\ntrain.label = train.label.apply(lambda x:reassign_label(x))\ntest.label = test.label.apply(lambda x:reassign_label(x))\n\n\ntrain_data = train.data\ntest_data = test.data\ntrain_label = train.label\ntest_label = test.label\n\ntrain.sample(15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<1-1> 2글자 이하 단어 제거, @덩어리 제거, #제거, url 주소 제거"},{"metadata":{"trusted":true},"cell_type":"code","source":"shortword = re.compile(r\"\\b\\w{1,2}\\b\")\nhashtag = re.compile(r\"@[a-zA-Z0-9_]*\")\nwebsite = re.compile(r\"(http|https):*/+[a-zA-Z0-9./]*\")\n\n\ndef remove_short(data):\n    removed=[]\n    for s in data:\n        removed_sentence = shortword.sub('',s)\n        removed_sentence = hashtag.sub('',removed_sentence)\n        removed_sentence = website.sub('',removed_sentence)\n        removed_sentence = removed_sentence.replace(\"#\",\"\")\n        removed.append(removed_sentence.strip())\n    return removed\n\ntrain_data = remove_short(train_data)\ntest_data = remove_short(test_data)\n\nprint(len(train_data))\nprint(len(test_data))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<1-2> tokenize & stopwords 제거 (nltk.corpus)"},{"metadata":{"trusted":true},"cell_type":"code","source":"swords = stopwords\nstop_words = set(swords.words('english'))\nprint(len(stop_words))\nprint(\"stopwords samples >> \",stopwords.words('english')[:10])\n\ndef tokenize(data):\n    ret = []\n    for sentence in data:\n        result = word_tokenize(sentence)\n        ret.append(result)\n        \n    return ret\n\ndef remove_stopwords(data):\n    ret = []\n    for sentence in data:\n        result=[]\n        for tok in sentence:\n            if tok not in stop_words:\n                result.append(tok)\n                \n        ret.append(result)\n        \n    return ret\n        \ntrain_data = tokenize(train_data)\ntest_data = tokenize(test_data)\n\ntrain_data = remove_stopwords(train_data)\ntest_data = remove_stopwords(test_data)\n\nprint(len(train_data))\nprint(len(test_data))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"+) Remove empty rows"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.DataFrame({\n    'label':train_label,\n    'data':train_data\n})\n\ntest = pd.DataFrame({\n    'label':test_label,\n    'data':test_data\n})\n    \ntrain['data'] = train['data'].apply(lambda x:np.nan if (len(x) == 0) else (x))\ntest['data'] = test['data'].apply(lambda x:np.nan if len(x)==0 else x)\n\ntrain.dropna(inplace=True)\ntest.dropna(inplace=True)\n\ntrain_data = train.data\ntrain_label = train.label\ntest_data = test.data\ntest_label = test.label\n\nprint(len(train_data))\nprint(len(test_data))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<1-3> Tokenize again to select only words used more than twice & text-integer mapping"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_data)\nprint(f\"Size of vocabs >> {len(tokenizer.word_index)}\")\n\nword_counts = tokenizer.word_counts\nk=0\nfreq=0\ntotal_freq=0\nfor key,value in word_counts.items():\n    total_freq = total_freq + value\n    if value<2:\n        k = k+1\n        freq = freq + value\nprint(f\"freq/total_freq >> {(freq/total_freq)*100}\")\nprint(f\"{k} words are used only for once\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_size = 25000\nvocab_size = word_size+1\n\ntokenizer = Tokenizer(num_words=word_size)\ntokenizer.fit_on_texts(train_data)\n\nword_to_index = tokenizer.word_index\nindex_to_word = tokenizer.index_word\n\ntrain_data = tokenizer.texts_to_sequences(train_data)\ntest_data = tokenizer.texts_to_sequences(test_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<1-4> Padding"},{"metadata":{"trusted":true},"cell_type":"code","source":"lens =  [len(s) for s in train_data]\n\nplt.hist(lens,bins=200)\nplt.show()\n\nsequence_size = 50\n\ntrain_data =pad_sequences(train_data,maxlen=sequence_size,padding='post',truncating='post')\ntest_data = pad_sequences(test_data,maxlen=sequence_size,padding='post',truncating='post')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<2> one-hot encode label"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=train_label)\nplt.tight_layout()\nplt.show()\n\ntrain_label = pd.get_dummies(train_label)\ntest_label = pd.get_dummies(test_label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_label.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<3> Modeling"},{"metadata":{},"cell_type":"markdown","source":"<3-1> LSTM model"},{"metadata":{"trusted":true},"cell_type":"code","source":"word_vec_size = 32\nhidden_size = 256\n\ndef create_LSTM():\n    X = Input(shape=[sequence_size])\n    \n    H = Embedding(vocab_size,word_vec_size,input_length=sequence_size)(X)\n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    H = GlobalMaxPool1D()(H)\n    H = Dropout(0.4)(H)\n    \n    H = Dense(64,activation='relu')(H)\n    H = Dropout(0.4)(H)\n    Y = Dense(3,activation='softmax')(H)\n    \n    model = keras.models.Model(X,Y)\n    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n        \n    return model\n\nlstm = create_LSTM()\nhist_lstm = lstm.fit(train_data,train_label,epochs=5,validation_split=0.2,batch_size=128)\nprint(\"\\nEvaluation on test dataset >>\\n\")\nlstm.evaluate(test_data,test_label)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<3-2> Multi Kernel Conv1D"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping\nfrom keras.layers import Concatenate,Conv1D,GlobalMaxPooling1D,Flatten\n\nes = EarlyStopping(monitor='val_loss',mode='min',patience=3,verbose=1)\n\n\nword_vec_size = 128\nnum_filters=128\n\ndef create_Conv1D(kernel_sizes=[3,4,5]):\n    X = Input(shape=[sequence_size],name=\"Input\")\n    \n    H = Embedding(vocab_size,word_vec_size,input_length=sequence_size)(X)\n    D = Dropout(0.6)(H)\n    \n    \n    conv_blocks = []\n    for k_size in kernel_sizes:\n        H = Conv1D(filters=num_filters,kernel_size=k_size,padding='valid')(D)\n        H = GlobalMaxPooling1D()(H)\n        H= Flatten()(H)\n        conv_blocks.append(H)\n        \n    H = Concatenate()(conv_blocks) if len(conv_blocks) >1 else conv_blocks[0]\n    H = Dropout(0.8)(H)\n    H = Dense(128,activation='relu')(H)\n    Y = Dense(3,activation='softmax')(H)\n    \n    model = keras.models.Model(X,Y)\n    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model\n\nconv1D  = create_Conv1D()\nhist = conv1D.fit(train_data,train_label,epochs=3,validation_split=0.2,batch_size=128)\nprint(\"\\n\\nEvaluation on test dataset >>\\n\\n\")\nconv1D.evaluate(test_data,test_label)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<3-3> Naive Bayes Models"},{"metadata":{},"cell_type":"markdown","source":"(1) These models require additional text preprocessing (token to text)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# def token_to_text(data):\n#     ret =[]\n#     for sentence in data:\n#         result = []\n#         for tok in sentence:\n#             if tok != 0:\n#                 result.append(index_to_word[tok])\n#         if len(result) != 0:\n#             result = \" \".join(result)\n#             ret.append(result)\n#     return ret\n\n# train_x = token_to_text(train_data)\n# test_x  = token_to_text(test_data)\n\ntrain_x = train_df.OriginalTweet\ntest_x = test_df.OriginalTweet\n\ntrain_y = train_df.Sentiment\ntest_y  =test_df.Sentiment","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer()\ntransformer = TfidfTransformer()\n\ntrain_x = vectorizer.fit_transform(train_x)\ntest_x = vectorizer.transform(test_x)\n\ntrain_x = transformer.fit_transform(train_x)\ntest_x = transformer.transform(test_x)\n\nprint(train_x.shape)\nprint(test_x.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multiNB = MultinomialNB()\nmultiNB.fit(train_x,train_y)\ntest_pred = multiNB.predict(test_x)\nacc = accuracy_score(test_y,test_pred)\nacc = np.round(acc*100,2)\nprint(f\"test acc >> {acc}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"preprocessing 안하고 그냥 넣으니까 결과 처참"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/covid-19-nlp-text-classification/Corona_NLP_train.csv\",encoding=\"L1\")\ntest_df = pd.read_csv(\"../input/covid-19-nlp-text-classification/Corona_NLP_test.csv\",encoding=\"L1\")\n\ndf1 = train_df[['Sentiment','OriginalTweet']].copy()\ndf2 = test_df[['Sentiment','OriginalTweet']].copy()\n\nspace = re.compile(r\"\\s{2,}\")\n\ntrain_df.OriginalTweet = train_df.OriginalTweet.str.replace(hashtag,'')\ntrain_df.OriginalTweet = train_df.OriginalTweet.str.replace(website,'')\ntrain_df.OriginalTweet = train_df.OriginalTweet.str.replace(\"#\",\" \")\ntrain_df.OriginalTweet = train_df.OriginalTweet.str.replace(space,\" \")\ntrain_df.OriginalTweet = train_df.OriginalTweet.str.strip()\ntest_df.OriginalTweet = test_df.OriginalTweet.str.replace(hashtag,'')\ntest_df.OriginalTweet = test_df.OriginalTweet.str.replace(website,'')\ntest_df.OriginalTweet = test_df.OriginalTweet.str.replace(\"#\",'')\ntest_df.OriginalTweet = test_df.OriginalTweet.str.replace(space,\" \")\ntest_df.OriginalTweet = test_df.OriginalTweet.str.strip()\n\n\ntrain_df.dropna(inplace=True)\ntest_df.dropna(inplace=True)\n\ntrain_x = train_df.OriginalTweet\ntrain_y = train_df.Sentiment\ntest_x = test_df.OriginalTweet\ntest_y = test_df.Sentiment","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer()\ntransformer = TfidfTransformer()\n\ntrain_x = vectorizer.fit_transform(train_x)\ntest_x = vectorizer.transform(test_x)\n\ntrain_x = transformer.fit_transform(train_x)\ntest_x = transformer.transform(test_x)\n\nprint(train_x.shape)\nprint(test_x.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multiNB = MultinomialNB()\nmultiNB.fit(train_x,train_y)\n\ntrain_pred = multiNB.predict(train_x)\ntrain_acc = accuracy_score(train_y,train_pred)\ntrain_acc = np.round(train_acc*100,2)\nprint(f\"test acc >> {train_acc}%\")\n\ntest_pred = multiNB.predict(test_x)\ntest_acc = accuracy_score(test_y,test_pred)\ntest_acc = np.round(test_acc*100,2)\nprint(f\"test acc >> {test_acc}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"@, 웹사이트, # 제거 했는데도 결과 처참"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}