{"cells":[{"metadata":{"_uuid":"fb022aa1d15f7b42157c66ae23acf5ca17a95ece"},"cell_type":"markdown","source":"## Settings"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"f757ac7b6b2f23a27eb0694106deceaaaafd6c52","_kg_hide-input":true},"cell_type":"code","source":"# Set number of topics for LSI/LDA\nnTopics = 8\n\n# Number of maximal points to plot\npoints = 1000\n\n# Set subject of the analysis\nsubject = 'AI/L/DL Articles'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9fa96ace65e71ec4fcdcdd2ab19329f0b75a142"},"cell_type":"markdown","source":"## Import Libraries"},{"metadata":{"trusted":true,"_uuid":"84ac8ec899b667aa6e99863fb6351e2197a0bab2","_kg_hide-input":true},"cell_type":"code","source":"# To store data\nimport pandas as pd\n\n# To do linear algebra\nimport numpy as np\n\n# To plot graphs\nimport matplotlib.pyplot as plt\nfrom matplotlib.cm import get_cmap\nfrom matplotlib.colors import rgb2hex\n\n# To create nicer graphs\nimport seaborn as sns\n\n# To create interactive graphs\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\n# To vectorize texts\nfrom sklearn.feature_extraction.text import CountVectorizer\n# To decompose texts\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import SparsePCA\n# To visualize high dimensional dataset\nfrom sklearn.manifold import TSNE\n\n# To tag words\nfrom textblob import TextBlob\n\n# To use new datatypes\nfrom collections import Counter\n\n# To stop words\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62a53f652798fbb82ff53bc15540978ca9b58da7"},"cell_type":"markdown","source":"## Load Data"},{"metadata":{"trusted":true,"_uuid":"49c12503b60c6ad96e0d28bc45155f06739d7b2d","_kg_hide-input":true},"cell_type":"code","source":"df = pd.read_csv('../input/medium.csv').rename(columns={'4.Body':'text'}).dropna(subset=['text'], axis=0)\nprint('DataFrame Shape: {}'.format(df.shape))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d38b9af2110f68b535a2804cf93c67a25fc4c40"},"cell_type":"markdown","source":"## Vectorize Texts"},{"metadata":{"trusted":true,"_uuid":"0375a6bdd526275ea2db717afe1853821c337969","_kg_hide-input":true},"cell_type":"code","source":"# Create vectorizer\ncountVectorizer = CountVectorizer(stop_words=stop)\n\n# Vectorize text\nvectorizedText = countVectorizer.fit_transform(df['text'].str.replace(\"'\", '').values)\nprint('Shape Vectorized Text: {}'.format(vectorizedText.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fcf3f521eeff1f9b31aa23bf36aba0478fc8f96b"},"cell_type":"markdown","source":"## Plot n Most Frequent Words"},{"metadata":{"trusted":true,"_uuid":"0bfa9e10f91f37bef530e79bdcf9002e88d23788","_kg_hide-input":true},"cell_type":"code","source":"# Plot n most frequent words\nn = 20\n\n\ndef nMostFrequentWords(n, countVectorizer, vectorizedText):    \n    # Count word appearences in text\n    vectorizedCount = np.sum(vectorizedText, axis=0)\n    \n    # Get word indices and counts\n    wordIndices = np.flip(np.argsort(vectorizedCount), 1)\n    wordCounts = np.flip(np.sort(vectorizedCount),1)\n\n    # Create wordvectors to inverse-transform them\n    wordVectors = np.zeros((n, vectorizedText.shape[1]))\n    for i in range(n):\n        wordVectors[i, wordIndices[0,i]] = 1\n\n    # Inverse-transfrom the wordvectors\n    words = [word[0].encode('ascii').decode('utf-8') for word in countVectorizer.inverse_transform(wordVectors)]\n\n    # Return word and word-counts\n    return (words, wordCounts[0, :n].tolist()[0])\n\n\n\n# Get most frequent words with wordcounts\nwords, wordCounts = nMostFrequentWords(n=n, countVectorizer=countVectorizer, vectorizedText=vectorizedText)\n\n# Create colormap\ncmap = get_cmap('viridis')\ncolors = [rgb2hex(cmap(color)) for color in np.arange(0, 1.000001, 1/(n-1))]\n\n# Create plot\ndata = go.Bar(x = words,\n              y = wordCounts,\n              marker = dict(color = colors))\n\nlayout = go.Layout(title = 'Most Frequent {} Words In {}'.format(n, subject),\n                   xaxis = dict(title = 'Words'),\n                   yaxis = dict(title = 'Count'))\n\nfig = go.Figure(data=[data], layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5da8fbc9b7f2cea193a3e403bb07a7e3560ae0fc"},"cell_type":"markdown","source":"## Word-Tags"},{"metadata":{"trusted":true,"_uuid":"e68b4986de2a2e751485a4fe83b3aa79fda34a94","_kg_hide-input":true},"cell_type":"code","source":"# Tags and descriptions\ntag_dict = {\"CC\":\"conjunction, coordinating; and, or, but\",\n                \"CD\":\"cardinal number; five, three, 13%\",\n                \"DT\":\"determiner; the, a, these\",\n                \"EX\":\"existential there; there were six boys\",\n                \"FW\":\"foreign word; mais\",\n                \"IN\":\"conjunction, subordinating or preposition; of, on, before, unless\",\n                \"JJ\":\"adjective; nice, easy\",\n                \"JJR\":\"adjective, comparative; nicer, easier\",\n                \"JJS\":\"adjective, superlative; nicest, easiest\",\n                \"LS\":\"list item marker; \",\n                \"MD\":\"verb, modal auxillary; may, should\",\n                \"NN\":\"noun, singular or mass; tiger, chair, laughter\",\n                \"NNS\":\"noun, plural; tigers, chairs, insects\",\n                \"NNP\":\"noun, proper singular; Germany, God, Alice\",\n                \"NNPS\":\"noun, proper plural; we met two Christmases ago\",\n                \"PDT\":\"predeterminer; both his children\",\n                \"POS\":\"possessive ending; 's\",\n                \"PRP\":\"pronoun, personal; me, you, it\",\n                \"PRP$\":\"pronoun, possessive; my, your, our\",\n                \"RB\":\"adverb; extremely, loudly, hard\",\n                \"RBR\":\"adverb, comparative; better\",\n                \"RBS\":\"adverb, superlative; best\",\n                \"RP\":\"adverb, particle; about, off, up\",\n                \"SYM\":\"symbol; %\",\n                \"TO\":\"infinitival to; what to do?\",\n                \"UH\":\"interjection; oh, oops, gosh\",\n                \"VB\":\"verb, base form; think\",\n                \"VBZ\":\"verb, 3rd person singular present; she thinks\",\n                \"VBP\":\"verb, non-3rd person singular present; I think\",\n                \"VBD\":\"verb, past tense; they thought\",\n                \"VBN\":\"verb, past participle; a sunken ship\",\n                \"VBG\":\"verb, gerund or present participle; thinking is fun\",\n                \"WDT\":\"wh-determiner; which, whatever, whichever\",\n                \"WP\":\"wh-pronoun, personal; what, who, whom\",\n                \"WP$\":\"wh-pronoun, possessive; whose, whosever\",\n                \"WRB\":\"wh-adverb; where, when\"}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10f69b98f93cf1a48098b3d19dbbfc0e2475db44","_kg_hide-input":true},"cell_type":"code","source":"# Apply tag-function to DataFrame, stack tags and count them\ntag_df = pd.DataFrame.from_records(df['text'].apply(lambda x: [tag for word, tag in TextBlob(x).pos_tags]).tolist()).stack().value_counts().reset_index().rename(columns={'index':'tag', 0:'count'})\n\n\n# Create colormap\nn = tag_df.shape[0]\ncmap = get_cmap('viridis')\ncolors = [rgb2hex(cmap(color)) for color in np.arange(0, 1.000001, 1/(n-1))]\n\n# Create plot\ndata = go.Bar(x = tag_df['tag'],\n              y = tag_df['count'],\n              text = tag_df['tag'].apply(lambda x: tag_dict[x] if x in tag_dict.keys() else x),\n              marker = dict(color = colors))\n\nlayout = go.Layout(title = 'Most Frequent Tags In {}'.format(subject),\n                   xaxis = dict(title = 'Type Of Word'),\n                   yaxis = dict(title = 'Count'))\n\nfig = go.Figure(data=[data], layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aad34079df8ad217f5124559f0573ee49ebfd390"},"cell_type":"markdown","source":"## Latent Semantic Indexing/Analysis LSI/LSA"},{"metadata":{"trusted":true,"_uuid":"f6acf096975d30e2171b44cea32b6e37fc4a9fc3","_kg_hide-input":true},"cell_type":"code","source":"# Create LSI and fit\nlsiModel = TruncatedSVD(n_components=nTopics)\nlsiTopicMatrix = lsiModel.fit_transform(vectorizedText)\nprint('Shape LSI Topic Matrix: {}'.format(lsiTopicMatrix.shape))\n\n# Get most probable keys and all categories with counts\nlsiKeys = lsiTopicMatrix.argmax(axis=1)\nlsiCategories, lsiCounts = zip(*Counter(lsiKeys).items())","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"b5814f69e3b5a24b96b2129d0716297939917923","_kg_hide-input":true},"cell_type":"code","source":"def getTopWords(n, lsiKeys, vectorizedText, countVectorizer):\n    # Create empty array for mean\n    wordMean = np.zeros((nTopics, vectorizedText.shape[1]))\n    # Iterate over each topic\n    for i in np.unique(lsiKeys):\n        wordMean[i] += vectorizedText.toarray()[lsiKeys==i].mean(axis=0)\n        \n    # Sort and get the most frequent n words for each topic\n    topWordsIndices = np.flip(np.argsort(wordMean, axis=1)[:, -n:], axis=1)\n    topWordsPercentage = (np.divide(np.flip(np.sort(wordMean, axis=1)[:, -n:], axis=1), (np.sum(wordMean, axis=1)+0.0000001)[:, None])*100).astype(int)\n\n\n    # Store all words for all topics\n    topWords = []\n\n    # Iterate over the topics with its indices\n    for i, (topic, percentage) in enumerate(zip(topWordsIndices, topWordsPercentage)):\n        # Store all words for one topic\n        topicWords = []\n\n        if i in np.unique(lsiKeys):\n            # Iterate over the indices for the topic\n            for index, percent in zip(topic, percentage):\n                # Create a wordvector for the index\n                wordVector = np.zeros((vectorizedText.shape[1]))\n                wordVector[index] = 1\n                # Inverse-transfor the wordvector\n                word = countVectorizer.inverse_transform(wordVector)[0][0]\n                # Store the word\n                topicWords.append('{}% '.format(percent) + word.encode('ascii').decode('utf-8'))\n        # Store all words for the topic\n        topWords.append(', '.join(topicWords))\n\n    return topWords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"095a1f6052ff68d3a96f0ac49beabd3b09211822","_kg_hide-input":true},"cell_type":"code","source":"# Get top n words\ntopWords = getTopWords(5, lsiKeys, vectorizedText, countVectorizer)\n\n# Print the topics and its words\nfor i, words in enumerate(topWords):\n    print('Topic {}: {}'.format(i, words))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40dea069c0e68fd8b3fca74e98a769a4b00cb24d","_kg_hide-input":true},"cell_type":"code","source":"# Sort data\nlsiCategoriesSorted, lsiCountsSorted = zip(*sorted(zip(lsiCategories, lsiCounts)))\n\n# Create labels\ntopWords = getTopWords(5, lsiKeys, vectorizedText, countVectorizer)\nlabels = ['Topic {}'.format(i) for i in lsiCategoriesSorted]\n\n# Create colormap\nn = nTopics\ncmap = get_cmap('viridis')\ncolors = [rgb2hex(cmap(color)) for color in np.arange(0, 1.000001, 1/(n-1))]\n\n# Create plot\ndata = go.Bar(x = labels,\n              y = lsiCountsSorted,\n              text = [word for word in topWords if word],\n              marker = dict(color = colors))\n\nlayout = go.Layout(title = 'Most Frequent LSI Topics In {}'.format(subject),\n                   xaxis = dict(title = 'Topic'),\n                   yaxis = dict(title = 'Count'))\n\nfig = go.Figure(data=[data], layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7bc13112b9026e0acd66b5bdb6dfa6c5370ec90","_kg_hide-input":true},"cell_type":"code","source":"# Transform high dimensional dataset to visualize in 2D\ntsneModel = TSNE(n_components=2, perplexity=50, learning_rate=100, n_iter=2000, verbose=1, random_state=0, angle=0.75)\ntsneModelVectors = tsneModel.fit_transform(lsiTopicMatrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"027c2fe271069d3ed43f660261fed1b6ccfeba0c","_kg_hide-input":true},"cell_type":"code","source":"# Create colormap\nn = nTopics\ncmap = get_cmap('tab10')\ncolors = [rgb2hex(cmap(color)) for color in np.arange(0, 1.000001, 1/(n-1))]\n\n# Get n top words\ntopWords = getTopWords(3, lsiKeys, vectorizedText, countVectorizer)\n\n\n# Create plot\ndata = []\n# Iterate over each topic\nfor topic in range(nTopics):\n    # Mask for a single topic\n    mask = lsiKeys==topic\n    # Mask for sampling\n    sample_mask = np.zeros(mask.sum()).astype(bool)\n    sample_mask[:int(points/nTopics)] = True\n    np.random.shuffle(sample_mask)\n    \n    scatter = go.Scatter(x = tsneModelVectors[mask,0][sample_mask],\n                         y = tsneModelVectors[mask,1][sample_mask],\n                         name = 'Topic {}: {}'.format(topic, topWords[topic]),\n                         mode = 'markers',\n                         text = df[mask]['text'][sample_mask],\n                         marker = dict(color = colors[topic]))\n    data.append(scatter)\n\nlayout = go.Layout(title = 't-SNE Clustering of {} LSI Topics'.format(nTopics),\n                   showlegend=True,\n                   hovermode = 'closest')\n\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e504c37624fcc946ee363e7958041c04d7a4382c"},"cell_type":"markdown","source":"## Latent Dirichlet Allocation"},{"metadata":{"trusted":true,"_uuid":"696a1a9fc1ca8587c0a62dd9a10e8fb089e8e094","_kg_hide-input":true},"cell_type":"code","source":"# Create LDA and fit\nldaModel = LatentDirichletAllocation(n_components=nTopics, learning_method='online', random_state=0, verbose=0)\nldaTopicMatrix = ldaModel.fit_transform(vectorizedText)\nprint('Shape LSI Topic Matrix: {}'.format(ldaTopicMatrix.shape))\n\n# Get most probable keys and all categories with counts\nldaKeys = ldaTopicMatrix.argmax(axis=1)\nldaCategories, ldaCounts = zip(*Counter(ldaKeys).items())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"017db48cf027803fe1e151eac78efe59654b8838","_kg_hide-input":true},"cell_type":"code","source":"# Get top n words\ntopWords = getTopWords(5, ldaKeys, vectorizedText, countVectorizer)\n\n# Print the topics and its words\nfor i, words in enumerate(topWords):\n    print('Topic {}: {}'.format(i, words))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2cff043acca34e9813790a20ba5b5b0928d50448","_kg_hide-input":true},"cell_type":"code","source":"# Sort data\nldaCategoriesSorted, ldaCountsSorted = zip(*sorted(zip(ldaCategories, ldaCounts)))\n\n# Create labels\ntopWords = getTopWords(5, ldaKeys, vectorizedText, countVectorizer)\nlabels = ['Topic {}'.format(i) for i in ldaCategoriesSorted]\n\n# Create colormap\nn = nTopics\ncmap = get_cmap('viridis')\ncolors = [rgb2hex(cmap(color)) for color in np.arange(0, 1.000001, 1/(n-1))]\n\n# Create plot\ndata = go.Bar(x = labels,\n              y = ldaCountsSorted,\n              text = [word for word in topWords if word],\n              marker = dict(color = colors))\n\nlayout = go.Layout(title = 'Most Frequent LDA Topics In {}'.format(subject),\n                   xaxis = dict(title = 'Topic'),\n                   yaxis = dict(title = 'Count'))\n\nfig = go.Figure(data=[data], layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"661476376a664c881862a199f125a28eb5c52bf4","_kg_hide-input":true},"cell_type":"code","source":"# Transform high dimensional dataset to visualize in 2D\ntsneModel = TSNE(n_components=2, perplexity=50, learning_rate=100, n_iter=2000, verbose=1, random_state=0, angle=0.75)\ntsneModelVectors = tsneModel.fit_transform(ldaTopicMatrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc224dfd473f82dc68f6020c5e67c4ef6def7325","_kg_hide-input":true},"cell_type":"code","source":"# Create colormap\nn = nTopics\ncmap = get_cmap('tab10')\ncolors = [rgb2hex(cmap(color)) for color in np.arange(0, 1.000001, 1/(n-1))]\n\n# Get n top words\ntopWords = getTopWords(3, ldaKeys, vectorizedText, countVectorizer)\n\n\n# Create plot\ndata = []\n# Iterate over each topic\nfor topic in range(nTopics):\n    # Mask for a single topic\n    mask = ldaKeys==topic\n    # Mask for sampling\n    sample_mask = np.zeros(mask.sum()).astype(bool)\n    sample_mask[:int(points/nTopics)] = True\n    np.random.shuffle(sample_mask)\n    \n    scatter = go.Scatter(x = tsneModelVectors[mask,0][sample_mask],\n                         y = tsneModelVectors[mask,1][sample_mask],\n                         name = 'Topic {}: {}'.format(topic, topWords[topic]),\n                         mode = 'markers',\n                         text = df[mask]['text'][sample_mask],\n                         marker = dict(color = colors[topic]))\n    data.append(scatter)\n\nlayout = go.Layout(title = 't-SNE Clustering of {} LDA Topics'.format(nTopics),\n                   showlegend=True,\n                   hovermode = 'closest')\n\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a6998be89d2ad456a4f125a38114119be260fac"},"cell_type":"markdown","source":"## Principal Component Analysis PCA"},{"metadata":{"trusted":true,"_uuid":"db8cbe9b603f601a2a094767ed5b223bbed90883","_kg_hide-input":true},"cell_type":"code","source":"# Create LDA and fit\npcaModel = PCA(n_components=nTopics, random_state=0)\npcaTopicMatrix = pcaModel.fit_transform(vectorizedText.toarray())\nprint('Shape PCA Topic Matrix: {}'.format(pcaTopicMatrix.shape))\n\n# Get most probable keys and all categories with counts\npcaKeys = pcaTopicMatrix.argmax(axis=1)\npcaCategories, pcaCounts = zip(*Counter(pcaKeys).items())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e00db9a13722b4047921256ee2fb5599d00dc0f0","_kg_hide-input":true},"cell_type":"code","source":"# Get top n words\ntopWords = getTopWords(5, pcaKeys, vectorizedText, countVectorizer)\n\n# Print the topics and its words\nfor i, words in enumerate(topWords):\n    print('Topic {}: {}'.format(i, words))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4b66d20a061a0385ceea4c954da6d9242a92686","_kg_hide-input":true},"cell_type":"code","source":"# Sort data\npcaCategoriesSorted, pcaCountsSorted = zip(*sorted(zip(pcaCategories, pcaCounts)))\n\n# Create labels\ntopWords = getTopWords(5, pcaKeys, vectorizedText, countVectorizer)\nlabels = ['Topic {}'.format(i) for i in pcaCategoriesSorted]\n\n# Create colormap\nn = nTopics\ncmap = get_cmap('viridis')\ncolors = [rgb2hex(cmap(color)) for color in np.arange(0, 1.000001, 1/(n-1))]\n\n# Create plot\ndata = go.Bar(x = labels,\n              y = pcaCountsSorted,\n              text = [word for word in topWords if word],\n              marker = dict(color = colors))\n\nlayout = go.Layout(title = 'Most Frequent PCA Topics In {}'.format(subject),\n                   xaxis = dict(title = 'Topic'),\n                   yaxis = dict(title = 'Count'))\n\nfig = go.Figure(data=[data], layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7eab875c166a36cef67c1cf40b60a7dc8b43703e","_kg_hide-input":true},"cell_type":"code","source":"# Transform high dimensional dataset to visualize in 2D\ntsneModel = TSNE(n_components=2, perplexity=50, learning_rate=100, n_iter=2000, verbose=1, random_state=0, angle=0.75)\ntsneModelVectors = tsneModel.fit_transform(pcaTopicMatrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3884b4256179d4d67c3ef3a6c70fb2082a62a2f","_kg_hide-input":true},"cell_type":"code","source":"# Create colormap\nn = nTopics\ncmap = get_cmap('tab10')\ncolors = [rgb2hex(cmap(color)) for color in np.arange(0, 1.000001, 1/(n-1))]\n\n# Get n top words\ntopWords = getTopWords(3, pcaKeys, vectorizedText, countVectorizer)\n\n\n# Create plot\ndata = []\n# Iterate over each topic\nfor topic in range(nTopics):\n    # Mask for a single topic\n    mask = pcaKeys==topic\n    # Mask for sampling\n    sample_mask = np.zeros(mask.sum()).astype(bool)\n    sample_mask[:int(points/nTopics)] = True\n    np.random.shuffle(sample_mask)\n    \n    scatter = go.Scatter(x = tsneModelVectors[mask,0][sample_mask],\n                         y = tsneModelVectors[mask,1][sample_mask],\n                         name = 'Topic {}: {}'.format(topic, topWords[topic]),\n                         mode = 'markers',\n                         text = df[mask]['text'][sample_mask],\n                         marker = dict(color = colors[topic]))\n    data.append(scatter)\n\nlayout = go.Layout(title = 't-SNE Clustering of {} PCA Topics'.format(nTopics),\n                   showlegend=True,\n                   hovermode = 'closest')\n\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b543a0813da130663a6d5bf48a4a6b7309790ff"},"cell_type":"markdown","source":"## Spare Principal Component Analysis SPCA"},{"metadata":{"trusted":true,"_uuid":"bb7e9a996d4954128a9a9527b0a9f901aaed75e3","_kg_hide-input":true},"cell_type":"code","source":"# Create LDA and fit\nspcaModel = SparsePCA(n_components=nTopics, random_state=0)\nspcaTopicMatrix = spcaModel.fit_transform(vectorizedText.toarray())\nprint('Shape SPCA Topic Matrix: {}'.format(spcaTopicMatrix.shape))\n\n# Get most probable keys and all categories with counts\nspcaKeys = spcaTopicMatrix.argmax(axis=1)\nspcaCategories, spcaCounts = zip(*Counter(spcaKeys).items())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43b24802240f3b72523ca3b191abce6984e19172","_kg_hide-input":true},"cell_type":"code","source":"# Get top n words\ntopWords = getTopWords(5, spcaKeys, vectorizedText, countVectorizer)\n\n# Print the topics and its words\nfor i, words in enumerate(topWords):\n    print('Topic {}: {}'.format(i, words))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6ac21daf178c00ea8726c8df201b035d0105877","_kg_hide-input":true},"cell_type":"code","source":"# Sort data\nspcaCategoriesSorted, spcaCountsSorted = zip(*sorted(zip(spcaCategories, spcaCounts)))\n\n# Create labels\ntopWords = getTopWords(5, spcaKeys, vectorizedText, countVectorizer)\nlabels = ['Topic {}'.format(i) for i in spcaCategoriesSorted]\n\n# Create colormap\nn = nTopics\ncmap = get_cmap('viridis')\ncolors = [rgb2hex(cmap(color)) for color in np.arange(0, 1.000001, 1/(n-1))]\n\n# Create plot\ndata = go.Bar(x = labels,\n              y = spcaCountsSorted,\n              text = [word for word in topWords if word],\n              marker = dict(color = colors))\n\nlayout = go.Layout(title = 'Most Frequent SPCA Topics In {}'.format(subject),\n                   xaxis = dict(title = 'Topic'),\n                   yaxis = dict(title = 'Count'))\n\nfig = go.Figure(data=[data], layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e5876a25b07c337ad19aebecb9b453932403ade","_kg_hide-input":true},"cell_type":"code","source":"# Transform high dimensional dataset to visualize in 2D\ntsneModel = TSNE(n_components=2, perplexity=50, learning_rate=100, n_iter=2000, verbose=1, random_state=0, angle=0.75)\ntsneModelVectors = tsneModel.fit_transform(spcaTopicMatrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62fb2789d7ed46b5ba88fec2dd569e3b68aa2524","_kg_hide-input":true},"cell_type":"code","source":"# Create colormap\nn = nTopics\ncmap = get_cmap('tab10')\ncolors = [rgb2hex(cmap(color)) for color in np.arange(0, 1.000001, 1/(n-1))]\n\n# Get n top words\ntopWords = getTopWords(3, spcaKeys, vectorizedText, countVectorizer)\n\n\n# Create plot\ndata = []\n# Iterate over each topic\nfor topic in range(nTopics):\n    # Mask for a single topic\n    mask = spcaKeys==topic\n    # Mask for sampling\n    sample_mask = np.zeros(mask.sum()).astype(bool)\n    sample_mask[:int(points/nTopics)] = True\n    np.random.shuffle(sample_mask)\n    \n    scatter = go.Scatter(x = tsneModelVectors[mask,0][sample_mask],\n                         y = tsneModelVectors[mask,1][sample_mask],\n                         name = 'Topic {}: {}'.format(topic, topWords[topic]),\n                         mode = 'markers',\n                         text = df[mask]['text'][sample_mask],\n                         marker = dict(color = colors[topic]))\n    data.append(scatter)\n\nlayout = go.Layout(title = 't-SNE Clustering of {} SPCA Topics'.format(nTopics),\n                   showlegend=True,\n                   hovermode = 'closest')\n\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51662dd639c92a8e2976932a590c7133ba298e55"},"cell_type":"markdown","source":"## Categorize New Text"},{"metadata":{"trusted":true,"_uuid":"67f4dfb9d732df4632f8d20828a5e3826bbf66cd","_kg_hide-input":true},"cell_type":"code","source":"text = \"Hey, Han Solo what's up?\"\n\ntextVector = countVectorizer.transform([text])\nnewTransformedVector = spcaModel.transform(textVector.toarray())\ntopic = np.argmax(newTransformedVector)\nprint('Topic {}: {} '.format(topic, text))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"e1f085c04a90de4edb463afef24fb2d0ae927f11"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}