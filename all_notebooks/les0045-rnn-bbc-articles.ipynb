{"cells":[{"metadata":{},"cell_type":"markdown","source":"# ARD - project: RNN\n\nAuthor: Brenda Lesniczakova, LES0045 <br>\nDataset: BBC articles"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re # regular expression\nfrom nltk.corpus import stopwords\nfrom nltk.stem import wordnet\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n\nfrom tensorflow import string as tf_string\nfrom tensorflow import keras\nfrom keras.models import Model\nfrom keras.layers.experimental.preprocessing import TextVectorization\nfrom keras.layers import LSTM, Bidirectional, Input, Embedding, Dropout, Dense\nfrom keras.callbacks import EarlyStopping\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BBC articles"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/bbc-fulltext-and-category/bbc-text.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Categories"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"data['category'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (12,5))\nax = fig.add_subplot(111)\nsns.countplot(data.category)\nplt.xlabel('Category', size = 15)\nplt.ylabel('Count', size= 15)\nplt.xticks(size = 12)\nplt.title(\"Count of articles by categories\" , size = 18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_encoder = LabelEncoder()\ndata['label'] = label_encoder.fit_transform(data.category)\nclass_names = data.groupby(['category', 'label']).count().reset_index().loc[:,['category', 'label']]\nclass_names","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cleaning data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.text[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"data['clean_txt'] = data['text'].apply(lambda x: re.sub(r'[^A-Za-z]+', ' ', x))\ndata['clean_txt'] = data['clean_txt'].apply(lambda x: x.lower())\ndata['clean_txt'] = data['clean_txt'].apply(lambda x: x.strip())\n\nstop_words = stopwords.words('english')\ndata['clean_txt'] = data['clean_txt'].apply(lambda x: ' '.join([\n    words for words in x.split() if words not in stop_words]))\nlem = wordnet.WordNetLemmatizer()\ndata['clean_txt'] = data['clean_txt'].apply(lambda x: ' '.join([\n    lem.lemmatize(item, pos='v') for item in x.split()]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.clean_txt[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Vocabulary"},{"metadata":{"trusted":true},"cell_type":"code","source":"word_freq = {}\nfor txt in data.clean_txt:\n    words = pd.Series(txt.split(' ')).value_counts()\n    for word in words.index:\n        if word.index in word_freq:\n            word_freq[word.index] += words[word]\n        else: word_freq[word.index] = words[word]\nprint('Count of unique words:', len(word_freq))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dim = 128 \nvocab_size = len(word_freq)\nsequence_length = 64 \nvect_layer = TextVectorization(max_tokens=vocab_size, output_mode='int',\n                               output_sequence_length=sequence_length)\nvect_layer.adapt(data.clean_txt.values)\n\nprint('Vocabulary example: ', vect_layer.get_vocabulary()[:10])\nprint('Vocabulary shape: ', len(vect_layer.get_vocabulary()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting data to training, validation and testing part"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"X = data.clean_txt\ny = data.label\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1,\n                                                      random_state=13, stratify=y_train)\nprint('Train:', X_train.shape, y_train.shape)\nprint('Test:', X_test.shape, y_test.shape)\nprint('Validation:', X_valid.shape, y_valid.shape)\n\ny_train_vect = to_categorical(y_train)\ny_valid_vect = to_categorical(y_valid)\nprint('\\nEncoding labels example:')\nfor i in range(5):\n    print('  ', list(y_train)[i], '  ', y_train_vect[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_history(history):\n    plt.figure()\n    for key in history.history.keys():\n        plt.plot(history.epoch, history.history[key], label=key)\n    plt.legend()\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"input_layer = Input(shape=(1,), dtype=tf_string)\nx_v = vect_layer(input_layer)\nemb = Embedding(vocab_size, embedding_dim)(x_v)\nx = Bidirectional(LSTM(128, return_sequences=True))(emb)\nx = Dropout(0.5)(x)\nx = Bidirectional(LSTM(64))(x)\nx = Dropout(0.5)(x)\nx = Dense(64, 'relu')(x)\noutput_layer = Dense(5, 'softmax')(x)\n\nmodel = Model(input_layer, output_layer)\nmodel.summary()\nmodel.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"es = EarlyStopping(monitor='val_loss', min_delta=0, patience=70, restore_best_weights=True)\nbatch_size = 128\nepochs = 50\nhistory = model.fit(X_train.values, y_train_vect, validation_data=(X_valid.values, y_valid_vect), \n                    callbacks=[es], epochs=epochs, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_history(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classification report"},{"metadata":{"trusted":true},"cell_type":"code","source":"def class_report(y_test, y_pred_vect):\n    y_pred = np.argmax(y_pred_vect, axis=1)\n    test_accuracy = np.sum(y_pred == y_test.values) / y_test.size\n    print('Test accuracy:', test_accuracy)\n    print('Accuracy score: ', accuracy_score(y_test, y_pred))\n    print('F1 score: ', f1_score(y_test, y_pred, average='macro'), '\\n')\n    print(classification_report(y_true=y_test, y_pred=y_pred))\n\n    conf_mtx = confusion_matrix(y_test, y_pred)\n    df_conf_mtx = pd.DataFrame(conf_mtx, index=class_names.category, columns=class_names.category)\n    plt.figure(figsize=(12,5))\n    sns.heatmap(df_conf_mtx, fmt='d', annot=True, cmap='Blues')\n    plt.xlabel('Predicted label', size = 15)\n    plt.ylabel('True label', size= 15)\n    plt.title('Confusion matrix', size=15)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_report(y_test, model.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Embedding file: GloVe Dictionary\nFile **glove.840B.300d.pkl** was imported from https://www.kaggle.com/authman/pickled-glove840b300d-for-10sec-loading"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"glove_embeddings = np.load('../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl',\n                           allow_pickle=True)\nembedding_dim = len(glove_embeddings['the'])\nprint(\"There are\", len(glove_embeddings), \"words and\", embedding_dim, \"dimensions in Glove Dictionary.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer_keras = Tokenizer(oov_token = \"<OOV>\")\ntokenizer_keras.fit_on_texts(data.clean_txt)\nword_index = tokenizer_keras.word_index\nvocab_size_token = len(word_index)\nprint('Vocabulary shape:', vocab_size_token)\nlist(word_index.items())[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"embedding_mtx = np.zeros((vocab_size_token+1, embedding_dim))\nfor word, idx in word_index.items():\n    if word in glove_embeddings:\n        embedding_mtx[idx] = glove_embeddings[word]\n        \ntokenized = pd.DataFrame([word_index]).T.reset_index()\ntokenized.columns = ['words','index']\ntemp_mtx = pd.DataFrame(embedding_mtx).reset_index()\ntemp_mtx = temp_mtx.drop(0, axis = 0)\ndf_embedding_mtx = pd.merge(tokenized, temp_mtx, on = 'index')\ndf_embedding_mtx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def prepare_data(X, tokenizer, max_len):\n    sequences = tokenizer.texts_to_sequences(X)\n    padded = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n    return padded","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"max_len = 512\nX_train_vect = prepare_data(X_train, tokenizer_keras, max_len)\nX_valid_vect = prepare_data(X_valid, tokenizer_keras, max_len)\nX_test_vect = prepare_data(X_test, tokenizer_keras, max_len)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model with embedding file"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_layer = Input(shape=(max_len,))\nemb = Embedding(vocab_size_token+1, embedding_dim, weights=[embedding_mtx], trainable=False)(input_layer)\nx = Bidirectional(LSTM(128, return_sequences=True))(emb)\nx = Dropout(0.5)(x)\nx = Bidirectional(LSTM(64))(x)\nx = Dropout(0.5)(x)\nx = Dense(64, 'relu')(x)\noutput_layer = Dense(5, 'softmax')(x)\n\nmodel_glove = Model(input_layer, output_layer)\nmodel_glove.summary()\nmodel_glove.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"es = EarlyStopping(monitor='val_loss', min_delta=0, patience=70, restore_best_weights=True)\nbatch_size = 128\nepochs = 50\nhistory = model_glove.fit(X_train_vect, y_train_vect, validation_data = (X_valid_vect, y_valid_vect),\n                          callbacks=[es], epochs=epochs, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_history(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_report(y_test, model_glove.predict(X_test_vect))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}