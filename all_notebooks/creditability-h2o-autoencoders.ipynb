{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRRI9EVsRlf_k35o-qloKvsvMWpdhMJ4aFTjA&usqp=CAU)pt.slideshare.net"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Code by Sheik Mohamed Imran https://www.kaggle.com/imrandude/h2o-autoencoders-and-anomaly-detection-python/notebook\n\nAnomaly detection with H2O in Python"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport h2o\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler,RobustScaler,Normalizer\nfrom h2o.estimators.deeplearning import H2OAutoEncoderEstimator\nfrom pylab import rcParams\nrcParams['figure.figsize']=15,10","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"nRowsRead = 1000 # specify 'None' if want to read whole file\ndf = pd.read_csv('../input/cusersmarildownloadsgermancsv/german.csv', delimiter=';', encoding = \"ISO-8859-2\", nrows = nRowsRead)\ndf.dataframeName = 'german.csv'\nnRow, nCol = df.shape\nprint(f'There are {nRow} rows and {nCol} columns')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#List column\n\nH2O cannot use columns with character datatype. Creating Dummy variables instead. All data is numerical. No need to encode here."},{"metadata":{"trusted":true},"cell_type":"code","source":"#cols_to_transform = [ 'continue_drop','gender','caste','guardian','internet' ]\n#df = pd.get_dummies( df,columns = cols_to_transform )\n#df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors=list(range(0,15))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Standardize input data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copy the original dataset\nscaled_features = df.copy()\n\n# Extract column names to be standardized\ncol_names = ['Creditability',\n 'Account_Balance',\n 'Duration_of_Credit_monthly',\n 'Payment_Status_of_Previous_Credit',\n 'Purpose',\n 'Credit_Amount',\n 'Value_Savings_Stocks',\n 'Length_of_current_employment',\n 'Instalment_per_cent',\n 'Sex_Marital_Status',\n 'Guarantors',\n 'Duration_in_Current_address',\n 'Most_valuable_available_asset',\n 'Age_years',\n 'Concurrent_Credits',\n 'Type_of_apartment',\n 'No_of_Credits_at_this_Bank',\n 'Occupation', 'No_of_dependents',\n 'Telephone',\n 'Foreign_Worker']\n\n# Standardize the columns and re-assingn to original dataframe\nfeatures = scaled_features[col_names]\nscaler = RobustScaler().fit_transform(features.values)\nfeatures = pd.DataFrame(scaler, index=df.index, columns=col_names)\nscaled_features [col_names] = features\nscaled_features.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Split dataset - Duration in Current address as 'test' and Type of apartment as 'train'"},{"metadata":{"trusted":true},"cell_type":"code","source":"#df = df.astype(object)\n\ntrain=scaled_features.loc[scaled_features['Type_of_apartment'] == 1]\ntest=scaled_features.loc[scaled_features['Duration_in_Current_address'] == 1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#H2O Autoencoding and Anomaly detection\n\nStarting H2O cluster"},{"metadata":{"trusted":true},"cell_type":"code","source":"h2o.init(nthreads=-1, enable_assertions = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Convert panda dataframe to H2O dataframe"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"train.hex=h2o.H2OFrame(train)\ntest.hex=h2o.H2OFrame(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Create AutoEncoder Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model=H2OAutoEncoderEstimator(activation=\"Tanh\",\n                              hidden=[120],\n                              ignore_const_cols=False,\n                              epochs=100\n                             )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Train the model with training dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.train(x=predictors,training_frame=train.hex)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Print the output in JSON format"},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"model._model_json['output']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Get anomalous values"},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_rec_error=model.anomaly(test.hex)\ntrain_rec_error=model.anomaly(train.hex)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Convert output to dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_rec_error_df=test_rec_error.as_data_frame()\ntrain_rec_error_df=train_rec_error.as_data_frame()\nfinal = pd.concat([train_rec_error_df, train_rec_error_df])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Calculate top whisker value"},{"metadata":{"trusted":true},"cell_type":"code","source":"boxplotEdges=final.quantile(.75)\niqr = np.subtract(*np.percentile(final, [75, 25]))\ntop_whisker=boxplotEdges[0]+(1.5*iqr)\ntop_whisker","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Add id column to dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_rec_error_df['Telephone']=train_rec_error_df.index\n#test_rec_error_df['']=test_rec_error_df.index + 18200 #Count of train data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Scatter plot with top whisker"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(train_rec_error_df['Telephone'],train_rec_error_df['Reconstruction.MSE'],label='Continued df',s=1)\nplt.axvline(x=18200,linewidth=1)\n#plt.scatter(test_rec_error_df['Telephone'],test_rec_error_df['Reconstruction.MSE'],label='Dropped df',s=1)\nplt.axhline(y=top_whisker,linewidth=1, color='r')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Output:\n\nWe trained the model. From the graph we see all that have been classififed as Outliers?\nCan we?"},{"metadata":{"trusted":true},"cell_type":"code","source":"h2o.cluster().shutdown()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}