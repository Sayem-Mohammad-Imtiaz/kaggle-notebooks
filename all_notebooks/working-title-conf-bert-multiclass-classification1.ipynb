{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport re\nimport gc\nimport os\nprint(os.listdir(\"../input\"))\nimport fileinput\nimport string\nimport tensorflow as tf\nimport zipfile\nimport datetime\nimport sys\nfrom tqdm  import tqdm\ntqdm.pandas()\nfrom nltk.tokenize import wordpunct_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score, roc_auc_score\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\nfrom keras.layers.embeddings import Embedding\nfrom sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a0600af701d2d03c2d3e7f952dbeac1cac9a6b13"},"cell_type":"code","source":"valid_forms = ['am','are','were','was','is','been','being','be']\nblank = '----'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34b8a2b0038c21b865256563d376b0b5faaa7e4c"},"cell_type":"code","source":"def detect(tokens):\n    return [t for t in tokens if t in valid_forms]\n    \ndef replace_blank(tokens):\n    return [blank if t in valid_forms else t for t in tokens]\n\ndef create_windows(tokens, window_size=3):\n    X = []\n    for i, word in enumerate(tokens):\n        if word == blank:\n            window = tokens[i-window_size:i] + tokens[i+1:i+window_size+1]\n            window = ' '.join(window)\n            X.append(window)    \n    return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcf71adc4358539a3d158ee5f97daf4683d90365"},"cell_type":"code","source":"f1 = open(\"../input/title-conference/title_conference.csv\",\"r\")\na = f1.read()\na = re.sub('[\\n]', '', a)\ntokens = wordpunct_tokenize(a)\ny = detect(tokens)\ntokens = replace_blank(tokens)\nX = create_windows(tokens)\nf1.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eaa001a351805a031cfc44928333fab9304eae5b"},"cell_type":"code","source":"df = pd.DataFrame()\ndf[\"Text\"] = X\ndf[\"Conference\"] = y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/title-conference/title_conference.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d39979b41d504a25bff4303e41aba5de29a9456"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba9e53d73830a70df82bda50ce6339ca8f7d205c"},"cell_type":"code","source":"one_hot = pd.get_dummies(df[\"Conference\"])\ndf.drop(['Conference'],axis=1,inplace=True)\ndf = pd.concat([df,one_hot],axis=1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba3ca115ffafaf35f1eeedc1449b836ac1fa8a60"},"cell_type":"code","source":"df1 = df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d92865168b510a57a218078f18c85d71883a903"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df1[\"Title\"].values, df1.drop(['Title'],axis=1).values, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90b59bf2fe0b277e3e6bd4961e80fdb460132675"},"cell_type":"code","source":"vocabulary_size = 20000\ntokenizer = Tokenizer(num_words= vocabulary_size)\ntokenizer.fit_on_texts(X_train)\nsequences = tokenizer.texts_to_sequences(X_train)\nX_train = pad_sequences(sequences, maxlen=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a66ac1534c360c1443d9b41e6377555e0d810fda"},"cell_type":"code","source":"sequences = tokenizer.texts_to_sequences(X_test)\nX_test = pad_sequences(sequences, maxlen=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69419594e347fb579921b93b2657f00b585ad094"},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(20000, 100, input_length=50))\nmodel.add(Dropout(0.2))\nmodel.add(Conv1D(64, 5, activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(LSTM(100))\nmodel.add(Dense(5, activation='softmax'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4321b53974828dde8364d4867978a309f77d4eab"},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df58cce943e21007beb4a3fb9715691e38f26408"},"cell_type":"code","source":"model.fit(X_train, y_train,\n                    batch_size=1024,\n                    epochs=10,\n                    verbose=1,\n                    validation_split=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ff81a21315b716a94d0d7d5966371dbc05526a1"},"cell_type":"code","source":"score = model.evaluate(X_test, y_test,\n                       batch_size=256, verbose=1)\nprint('Test accuracy:', score[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"658dc1479c287153b7f6a58487eeaf2692c88ada"},"cell_type":"code","source":"preds = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4958e35b19fff351c2736be1c147b3cf3ba2b9c9"},"cell_type":"code","source":"print(classification_report(np.argmax(y_test,axis=1),np.argmax(preds,axis=1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f06e533fa0af9d8eccb9c0efb06813a9aff0e494"},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64f2c3a0c296d5bb724b405d18a8df438db444da"},"cell_type":"markdown","source":"# BERT implementation"},{"metadata":{"trusted":true,"_uuid":"02a6443556928bda3f2527b7d803361de3e8e518"},"cell_type":"code","source":"!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n!wget https://raw.githubusercontent.com/google-research/bert/master/modeling.py \n!wget https://raw.githubusercontent.com/google-research/bert/master/optimization.py \n!wget https://raw.githubusercontent.com/google-research/bert/master/run_classifier.py \n!wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b1a537730f8d6a59633a18e18443e422a3c7281"},"cell_type":"code","source":"import modeling\nimport optimization\nimport run_classifier\nimport tokenization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1aa95f622d1b59645a9e22c2e6d5d2a5f29a2a50"},"cell_type":"code","source":"folder = 'model_folder'\nwith zipfile.ZipFile(\"uncased_L-12_H-768_A-12.zip\",\"r\") as zip_ref:\n    zip_ref.extractall(folder)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20541f718c86fed1131409bcc9ee7736d3ec88ac"},"cell_type":"code","source":"BERT_MODEL = 'uncased_L-12_H-768_A-12'\nBERT_PRETRAINED_DIR = f'{folder}/uncased_L-12_H-768_A-12'\nOUTPUT_DIR = f'{folder}/outputs'\nprint(f'>> Model output directory: {OUTPUT_DIR}')\nprint(f'>>  BERT pretrained directory: {BERT_PRETRAINED_DIR}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = pd.read_csv(\"../input/title-conference/title_conference.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2['Conference'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"possible_labels = df2.Conference.unique()\n\nlabel_dict = {}\nfor index, possible_label in enumerate(possible_labels):\n    label_dict[possible_label] = index\nlabel_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2['label'] = df2.Conference.replace(label_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.drop(['Conference'], axis=1 , inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7990d9f6cdd2960ebfd61c40c58d3c1acb7d18b6"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df2[\"Title\"].values, df2[\"label\"].values, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a128c3f38ab69351e34f87977cec04d9d6c9189"},"cell_type":"code","source":"def create_examples(lines, set_type, labels=None):\n#Generate data for the BERT model\n    guid = f'{set_type}'\n    examples = []\n    if guid == 'train':\n        for line, label in zip(lines, labels):\n            text_a = line\n            label = str(label)\n            examples.append(\n              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n    else:\n        for line in lines:\n            text_a = line\n            label = '0'\n            examples.append(\n              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n    return examples\n\n# Model Hyper Parameters\nTRAIN_BATCH_SIZE = 32\nEVAL_BATCH_SIZE = 8\nLEARNING_RATE = 1e-5\nNUM_TRAIN_EPOCHS = 3.0\nWARMUP_PROPORTION = 0.1\nMAX_SEQ_LENGTH = 50\n# Model configs\nSAVE_CHECKPOINTS_STEPS = 100000 #if you wish to finetune a model on a larger dataset, use larger interval\n# each checpoint weights about 1,5gb\nITERATIONS_PER_LOOP = 100000\nNUM_TPU_CORES = 8\nVOCAB_FILE = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\nCONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\nINIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\nDO_LOWER_CASE = BERT_MODEL.startswith('uncased')\n\nlabel_list = [str(num) for num in range(8)]\ntokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=DO_LOWER_CASE)\ntrain_examples = create_examples(X_train, 'train', labels=y_train)\n\ntpu_cluster_resolver = None #Since training will happen on GPU, we won't need a cluster resolver\n#TPUEstimator also supports training on CPU and GPU. You don't need to define a separate tf.estimator.Estimator.\nrun_config = tf.contrib.tpu.RunConfig(\n    cluster=tpu_cluster_resolver,\n    model_dir=OUTPUT_DIR,\n    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n    tpu_config=tf.contrib.tpu.TPUConfig(\n        iterations_per_loop=ITERATIONS_PER_LOOP,\n        num_shards=NUM_TPU_CORES,\n        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n\nnum_train_steps = int(\n    len(train_examples) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\nnum_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n\nmodel_fn = run_classifier.model_fn_builder(\n    bert_config=modeling.BertConfig.from_json_file(CONFIG_FILE),\n    num_labels=len(label_list),\n    init_checkpoint=INIT_CHECKPOINT,\n    learning_rate=LEARNING_RATE,\n    num_train_steps=num_train_steps,\n    num_warmup_steps=num_warmup_steps,\n    use_tpu=False, #If False training will fall on CPU or GPU, depending on what is available  \n    use_one_hot_embeddings=True)\n\nestimator = tf.contrib.tpu.TPUEstimator(\n    use_tpu=False, #If False training will fall on CPU or GPU, depending on what is available \n    model_fn=model_fn,\n    config=run_config,\n    train_batch_size=TRAIN_BATCH_SIZE,\n    eval_batch_size=EVAL_BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_train_steps","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3cb95e4f486c7743add1039f996f501d07109fe"},"cell_type":"code","source":"print('Please wait...')\ntrain_features = run_classifier.convert_examples_to_features(\n    train_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\nprint('>> Started training at {} '.format(datetime.datetime.now()))\nprint('  Num examples = {}'.format(len(train_examples)))\nprint('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\ntf.logging.info(\"  Num steps = %d\", num_train_steps+1)\ntrain_input_fn = run_classifier.input_fn_builder(\n    features=train_features,\n    seq_length=MAX_SEQ_LENGTH,\n    is_training=True,\n    drop_remainder=True)\nestimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\nprint('>> Finished training at {}'.format(datetime.datetime.now()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ed42f193d4586b10b58a65ea06c3d51385160bb"},"cell_type":"code","source":"def input_fn_builder(features, seq_length, is_training, drop_remainder):\n  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n\n  all_input_ids = []\n  all_input_mask = []\n  all_segment_ids = []\n  all_label_ids = []\n\n  for feature in features:\n    all_input_ids.append(feature.input_ids)\n    all_input_mask.append(feature.input_mask)\n    all_segment_ids.append(feature.segment_ids)\n    all_label_ids.append(feature.label_id)\n\n  def input_fn(params):\n    \"\"\"The actual input function.\"\"\"\n    print(params)\n    batch_size = 500\n\n    num_examples = len(features)\n\n    d = tf.data.Dataset.from_tensor_slices({\n        \"input_ids\":\n            tf.constant(\n                all_input_ids, shape=[num_examples, seq_length],\n                dtype=tf.int32),\n        \"input_mask\":\n            tf.constant(\n                all_input_mask,\n                shape=[num_examples, seq_length],\n                dtype=tf.int32),\n        \"segment_ids\":\n            tf.constant(\n                all_segment_ids,\n                shape=[num_examples, seq_length],\n                dtype=tf.int32),\n        \"label_ids\":\n            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n    })\n\n    if is_training:\n      d = d.repeat()\n      d = d.shuffle(buffer_size=100)\n\n    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n    return d\n\n  return input_fn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ddf519b690697635cbd3b47d2ee875feb94fd822"},"cell_type":"code","source":"predict_examples = create_examples(X_test, 'test')\n\npredict_features = run_classifier.convert_examples_to_features(\n    predict_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n\npredict_input_fn = input_fn_builder(\n    features=predict_features,\n    seq_length=MAX_SEQ_LENGTH,\n    is_training=False,\n    drop_remainder=False)\n\nresult = estimator.predict(input_fn=predict_input_fn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb9635b3771ecc148b7dc7a2bf4f5de6a8173445"},"cell_type":"code","source":"preds = []\nfor prediction in result:\n      preds.append(np.argmax(prediction['probabilities']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5cc9b9374f73ce94a04b9448e6807976cd89ec0"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de003a2788e27d334232bd515572d73304192fe5"},"cell_type":"code","source":"print(\"Accuracy of BERT is:\",accuracy_score(y_test,preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1586642280c06b07158651391f2e80b7756a7a3c"},"cell_type":"code","source":"\n\n\nprint(classification_report(y_test,preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d77113dca6782b19458f391d1d48769ad1e487a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}