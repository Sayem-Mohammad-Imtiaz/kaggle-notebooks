{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Natural Language Processing\nNatural language processing (NLP) is the ability of a computer program to understand human language as it is spoken. NLP is a component of artificial intelligence (AI).\n![NLP](https://media.geeksforgeeks.org/wp-content/cdn-uploads/machineLearning3.png)\n\nThe development of NLP applications is challenging because computers traditionally require humans to \"speak\" to them in a programming language that is precise, unambiguous and highly structured, or through a limited number of clearly enunciated voice commands. Human speech, however, is not always precise -- it is often ambiguous and the linguistic structure can depend on many complex variables, including slang, regional dialects and social context.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. some examples \n\n2. removing punctuation\n\n3. tokenization\n\n4. remove the stopwords\n\n5. stemming \n\n6. lemmitization\n\n7. vectorization of data\n\n8. Randomforest\n\n        NOTE . I have used diffrent text cleaning methods using same dataset individually ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#import the essential libaries\nimport nltk #Natural Language Toolkit,\n#import regular expression\nimport re \n#lets start by taking an example\nre_test = \" This is made string to text 2 deffrent regex expression methodes\" #simple sentence\nre_many_messay = \" This is made     string to text 2 deffrent     regex expression   methodes\" #sentence having more than when whitespace\nre_test_messay = \"This-is-made/up.string*to>>>>text----2***** deffrent-regex-expression-methodes\" #sentence having many characters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data by white spaces beetween tham\n#regular expression split \nre.split('\\s',re_test)\n#fro spliting in single white space","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"re.split('\\s',re_many_messay)\n#but what if that data have more than one white spaces between tham? simply use + while splitig\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#+ when there are more than one white spaces\nre.split('\\s+',re_many_messay)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"re.split('\\s+',re_test_messay)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#\\W Matches nonword characters.\n#and re.split splittes nonwords char\nre.split('\\W+',re_test_messay)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#\\S Matches nonwhitespace.\nre.findall('\\S+',re_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#finds \\S  nonwhitespace\nre.findall('\\S+',re_many_messay)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"re.findall('\\w+',re_test_messay)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets take another example\npep8_test = \"I try to follow the PEP8 guidlines\"\npep7_test = \"I try to follow the PEP7 guidlines\"\npeep8_test = \"I try to follow the PEEP8 uidlines\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"re.findall('[a-z]+',pep8_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"re.findall('[A-Z0-9]+',pep8_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"re.sub('[A-Z]+[0-9]+','PEEP8 python style guid',pep8_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"re.sub('[A-Z]+[0-9]+','PEEP8 python style guid',pep7_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"re.sub('[A-Z]+[0-9]+','PEEP8 python style guid',peep8_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\npd.set_option(\"display.max_colwidth\",100)\n\n#okaay lets work with dataset SMSspamcollection dataset\ndata = pd.read_csv(\"../input/sms-spam-collection-dataset/spam.csv\", encoding='latin-1')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.filter(['v1', 'v2'])\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns = (\"label\",\"message\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# REmoving PUnctuation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\n#list of punctuation\nstring.punctuation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let defined the text for removing punctuation from the data\n\ndef remove(text):\n    text_punct = [char for char in text if char not in string.punctuation]\n    return text_punct\n#okayy we have the function we can aplly on the data set\ndata[\"data_message\"] = data[\"message\"].apply(lambda x: remove(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#wwe  can clearly see that it will  returning the individial char in the data \n#for for joining we g=haveto use the space and join as \"\".join for joping words\n\ndef remove(text):\n    text_punct = \"\".join([char for char in text if char not in string.punctuation])\n    return text_punct\n#okayy we have the function we can aplly on the data set\ndata[\"data_message\"] = data[\"message\"].apply(lambda x: remove(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TOkenization\nTokenization is the process by which big quantity of text is divided into smaller parts called tokens.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we than again have to define and function \ndef token(text):\n    tokenz = re.split('\\W+',text)\n    return tokenz\ndata[\"data_token\"] = data[\"data_message\"].apply(lambda x: token(x.lower()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()\n#nlp is casesensative we have to make caracter \n#in same lower case as to study the given data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# remove the stopwords","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstopword  = stopwords.words(\"english\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopword[:20]\n#there are many stopwords avalilable in nltk ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define a simple function for removing the stopwords from the data\ndef remove_corpus(data_token):\n    stop = [word for word in data_token if word not in stopword]\n    return stop\n#making an column to sav the removed stopword data\ndata[\"data_stopword\"] = data[\"data_token\"].apply(lambda x: remove_corpus(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" re.split('\\s',\"This is an interesting te+st\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form. ... A computer program or subroutine that stems word may be called a stemming program, stemming algorithm, or stemmer.\nthere types of steamer\n1. porter steamer\n2. snowball steammer \n3. Lancaster steammer\n4. Regex based steammer","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# STemming\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ps = nltk.PorterStemmer()\ndir(ps)\n#functional attribute presented in porterSteammer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(ps.stem(\"grows\"))\nprint(ps.stem(\"growing\"))\nprint(ps.stem(\"grow\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(ps.stem(\"run\"))\nprint(ps.stem(\"running\"))\nprint(ps.stem(\"runner\"))\n#they are not perfect even","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets take that dataset again now we working on stemming\ndf = pd.read_csv(\"../input/sms-spam-collection-dataset/spam.csv\", encoding='latin-1')\ndf = df.filter(['v1', 'v2'])\ndf.columns = (\"label\",\"message\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe = df.copy()\n#copy so that i can use further","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# sTemming the text\n\nWe already know that a word has one root-base form but having different variations, for example, “play” is a root-base word and playing, played, plays are the different forms of a single word. So, these words get stripped out, they might get the incorrect meanings or some other sort of errors.   \n\nThe process of reducing inflection towards their root forms are called Stemming, this occurs in such a way that depicting a group of relatable words under the same stem, even if the root has no appropriate meaning. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#wee can do every thing we understand previwes in one function\ndef clean_text(unknown):\n    text = \"\".join([word.lower() for word in unknown if word not in string.punctuation])\n    token = re.split('\\W+',unknown)\n    text = [word for word in token if word not in stopword]\n    return text\ndf[\"body_text\"] = df[\"message\"].apply(lambda x: clean_text(x.lower()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def stemming(token_text):\n    text = [ps.stem(word) for word in token_text]\n    return text\ndf[\"stem_text\"] = df[\"body_text\"].apply(lambda x : stemming(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lemmitization\nIn other words, Lemmatization is a method responsible for grouping different inflected forms of words into the root form, having the same meaning. It is similar to stemming, in turn, it gives the stripped word that has some dictionary meaning. The Morphological analysis would require the extraction of the correct lemma of each word. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"wn = nltk.WordNetLemmatizer()\nps = nltk.PorterStemmer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dir(wn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#OKKAY so lets take an ex\nprint(ps.stem(\"meanness\"))\nprint(ps.stem(\"meaning\"))\n#the stem return the stemmed word with no exact dicctonoary relations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(wn.lemmatize(\"meanness\"))\nprint(wn.lemmatize(\"meaning\"))\n#so we can see that lemitization is bit diffrent from stemmer as its\n#returns the meaning full dict words having synonims","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(ps.stem(\"gooes\"))\nprint(ps.stem(\"geese\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(wn.lemmatize(\"gooes\"))\nprint(wn.lemmatize(\"gooes\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#using the simple copy of orignal dataset\ndataframe =  pd.read_csv(\"../input/sms-spam-collection-dataset/spam.csv\", encoding='latin-1')\ndataframe = dataframe.filter(['v1', 'v2'])\ndataframe.columns = (\"label\",\"message\")\ndataframe.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(unknown):\n    text = \"\".join([word for word in unknown if word not in string.punctuation])\n    token = re.split('\\W+',unknown)\n    text = [word for word in token if word not in stopword]\n    return text\ndataframe[\"body_text\"] = dataframe[\"message\"].apply(lambda x: clean_text(x.lower()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemitizer(body_text):\n    text = [wn.lemmatize(word) for word in body_text]\n    return text\ndataframe[\"lemitized\"] = dataframe[\"body_text\"].apply(lambda x: lemitizer(x))    ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"dataframe.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Vectorizing data\nThe CountVectorizer provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary. You can use it as follows: Create an instance of the CountVectorizer class.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vector = pd.read_csv(\"../input/sms-spam-collection-dataset/spam.csv\", encoding='latin-1')\nvector = vector.filter(['v1', 'v2'])\nvector.columns = (\"label\",\"message\")\nvector.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n    token = re.split('\\W+',text)\n    text = [word for word in token if word not in stopword]\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vector[\"tokenized\"] = vector[\"message\"].apply(lambda x: clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vector[\"label\"]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ncount = CountVectorizer()\ncount_x = count.fit_transform(vector[\"message\"])\nprint(count_x.shape)\nprint(count.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = pd.DataFrame(count_x.toarray())\ncount","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" N Gram","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gram = value.copy()\n#im lazy you can read the dataset agin using pandas im using copyied one","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gram.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#wee can do every thing we understand previwes in one function\ndef clean_text(unknown):\n    text = \"\".join([word.lower() for word in unknown if word not in string.punctuation])\n    token = re.split('\\W+',unknown)\n    text = \" \".join([ps.stem(word) for word in token if word not in stopword])\n    return text\ngram[\"body_text\"] = gram[\"message\"].apply(lambda x: clean_text(x.lower()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n#ranfge of n_grams to choose like unigrams,bygrams,trigrams\nn_gram_vec = CountVectorizer(ngram_range=(2, 2))\nn_grams_count = n_gram_vec.fit_transform(gram[\"body_text\"])\nn_grams_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(n_grams_count.shape)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"print(n_gram_vec.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\ntree = RandomForestClassifier(n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dir(tree)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold,cross_val_score\nfold = KFold(n_splits=5)\ncross_val_score(tree,count,vector[\"label\"],cv =fold,scoring=\"accuracy\",n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_fscore_support as score\nX_train,X_test,y_train,y_test = train_test_split(count,data[\"label\"],test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(n_est,depth):\n    rf = RandomForestClassifier(n_estimators=n_est,max_depth=depth,n_jobs=-1)\n    rf_model = rf.fit(X_train,y_train)\n    y_pred = rf_model.predict(X_test)\n    precision,recall,fscore,support = score(y_test,y_pred,pos_label=\"spam\",average=\"binary\")\n    print(\"est:{}/depth:{}----precision:{}/Recall:{}/accuracy:{}\".format(\n        n_est,depth,round(precision,3),round(recall,3),\n        round((y_pred == y_test).sum()/len(y_pred),3)))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for n_est in [10,50,100]:\n    for depth in [10,20,30,None]:\n        train(n_est,depth)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}