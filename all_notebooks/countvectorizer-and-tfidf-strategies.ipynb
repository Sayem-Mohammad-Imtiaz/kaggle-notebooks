{"cells":[{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{},"source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\nspam_data = pd.read_csv('../input/spam.csv', encoding='latin-1')\nspam_data.head()"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{},"source":"spam_data = spam_data.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'], axis=1)\nspam_data = spam_data.rename(columns = {'v1': 'target','v2': 'text'})\n\nspam_data.head()"},{"metadata":{},"cell_type":"markdown","source":"### Data Exploration"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{},"source":"s = spam_data['target'].value_counts()\nsns.barplot(x=s.values, y=s.index)\nplt.title('Data Distribution')"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{},"source":"s1 = spam_data[spam_data['target'] == 'ham']['text'].str.len()\nsns.distplot(s1, label='Ham')\ns2 = spam_data[spam_data['target'] == 'spam']['text'].str.len()\nsns.distplot(s2, label='Spam')\nplt.title('Lenght Distribution')\nplt.legend()"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{},"source":"print(s1.mean(), s2.mean())"},{"metadata":{},"cell_type":"markdown","source":"We can notice that spams messages are often longer than ham messages."},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{},"source":"s1 = spam_data[spam_data['target'] == 'ham']['text'].str.replace(r'\\D+', '').str.len()\nsns.distplot(s1, label='Ham')\ns2 = spam_data[spam_data['target'] == 'spam']['text'].str.replace(r'\\D+', '').str.len()\nsns.distplot(s2, label='Spam')\nplt.title('Digits Distribution')\nplt.legend()"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{},"source":"print(s1.mean(), s2.mean())"},{"metadata":{},"cell_type":"markdown","source":"From this plot, it's clear that the digits distribution in ham messages are rigth skewed, presenting lower mean of digits than spam messages."},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{},"source":"s1 = spam_data[spam_data['target'] == 'ham']['text'].str.replace(r'\\w+', '').str.len()\nsns.distplot(s1, label='Ham')\ns2 = spam_data[spam_data['target'] == 'spam']['text'].str.replace(r'\\w+', '').str.len()\nsns.distplot(s2, label='Spam')\nplt.title('Non-Digits Distribution')\nplt.legend()"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{},"source":"print(s1.mean(), s2.mean())"},{"metadata":{},"cell_type":"markdown","source":"These distributions resembles the ones regarding the text messages. Here, the values are smaller, though. Hams presents less non-digits than spams."},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{},"source":"spam_data.groupby('target').describe()"},{"metadata":{},"cell_type":"markdown","source":"### Count Vectorizer vs. Tfidf"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"collapsed":true},"source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(spam_data['text'], \n                                                    spam_data['target'], \n                                                    random_state=0)"},{"metadata":{},"cell_type":"markdown","source":"- ** Count Vectorizer **"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{},"source":"from sklearn.feature_extraction.text import CountVectorizer\n\nvect = CountVectorizer().fit(X_train)\nprint('Vocabulary len:', len(vect.get_feature_names()))\nprint('Longest word:', max(vect.vocabulary_, key=len))\n\nX_train_vectorized = vect.transform(X_train)"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{},"source":"from sklearn.naive_bayes import MultinomialNB\n\nmodel = MultinomialNB(alpha=0.1)\nmodel.fit(X_train_vectorized, y_train)"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{},"source":"# get the feature names as numpy array\nfeature_names = np.array(vect.get_feature_names())\n\n# Sort the coefficients from the model\nsorted_coef_index = model.coef_[0].argsort()\n\n# Find the 10 smallest and 10 largest coefficients\n# The 10 largest coefficients are being indexed using [:-11:-1] \n# so the list returned is in order of largest to smallest\nprint('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{},"source":"from sklearn.metrics import accuracy_score\n\ny_pred = model.predict(vect.transform(X_test))\nprint('Accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100))"},{"metadata":{},"cell_type":"markdown","source":"- **Tfidf**\n\n Let's ignore terms that have a document frequency strictly lower than 3."},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{},"source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvect = TfidfVectorizer(min_df=3).fit(X_train)\nprint('Vocabulary len:', len(vect.get_feature_names()))\nprint('Longest word:', max(vect.vocabulary_, key=len))\n\nX_train_vectorized = vect.transform(X_train)"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{},"source":"model = MultinomialNB(alpha=0.1)\nmodel.fit(X_train_vectorized, y_train)"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{},"source":"feature_names = np.array(vect.get_feature_names())\n\nsorted_coef_index = model.coef_[0].argsort()\n\nprint('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{},"source":"from sklearn.metrics import accuracy_score\n\ny_pred = model.predict(vect.transform(X_test))\nprint('Accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100))"},{"metadata":{},"cell_type":"markdown","source":"### Feature Engineering"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"collapsed":true},"source":"def add_feature(X, feature_to_add):\n    \"\"\"\n    Returns sparse feature matrix with added feature.\n    feature_to_add can also be a list of features.\n    \"\"\"\n    from scipy.sparse import csr_matrix, hstack\n    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"},{"metadata":{},"cell_type":"markdown","source":"---\n#### First Model\n\nFirst, let's ignore terms that have a document frequency strictly lower than 3.\nUsing this document-term matrix and an additional feature, the length of document (number of characters), we will test how our Tfidf performs."},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{},"source":"vect = TfidfVectorizer(min_df=5).fit(X_train)\nprint('Vocabulary len:', len(vect.get_feature_names()))\nprint('Longest word:', max(vect.vocabulary_, key=len))\n\nX_train_vectorized = vect.transform(X_train)\n\nX_train_vectorized = add_feature(X_train_vectorized, X_train.str.len())"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{},"source":"model = MultinomialNB(alpha=0.1)\nmodel.fit(X_train_vectorized, y_train)"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{},"source":"index = np.array(vect.get_feature_names() + ['length_of_doc'])\nvalues  = model.coef_[0]\nfeatures_series = pd.Series(data=values,index=index)\n\nprint('Smallest Coefs:\\n{}\\n'.format(features_series.nsmallest(10).index.values.tolist()))\nprint('Largest Coefs: \\n{}'.format(features_series.nlargest(10).index.values.tolist()))"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{},"source":"X_test_vectorized = vect.transform(X_test)\nX_test_vectorized = add_feature(X_test_vectorized, X_test.str.len())\n    \ny_pred = model.predict(X_test_vectorized)\nprint('Accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100))"},{"metadata":{},"cell_type":"markdown","source":"---\n#### Second Model\n\nNow let's use a Tfidf ignoring terms that have a document frequency strictly lower than **5** and using **word n-grams from n=1 to n=3** (unigrams, bigrams and trigrams).\n\nWe will also make use of the following additional features:\n* the length of document (number of characters)\n* number of digits per document"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{},"source":"vect = TfidfVectorizer(min_df=5, ngram_range=(1, 3)).fit(X_train)\nprint('Vocabulary len:', len(vect.get_feature_names()))\nprint('Longest word:', max(vect.vocabulary_, key=len))\n\nX_train_vectorized = vect.transform(X_train)\n\nX_train_vectorized = add_feature(X_train_vectorized, X_train.str.len())\nX_train_vectorized = add_feature(X_train_vectorized, X_train.str.replace(r'\\D+', '').str.len())"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{},"source":"model = MultinomialNB(alpha=0.1)\nmodel.fit(X_train_vectorized, y_train)"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{},"source":"index = np.array(vect.get_feature_names() + ['length_of_doc', 'digit_count'])\nvalues  = model.coef_[0]\nfeatures_series = pd.Series(data=values,index=index)\n\nprint('Smallest Coefs:\\n{}\\n'.format(features_series.nsmallest(10).index.values.tolist()))\nprint('Largest Coefs: \\n{}'.format(features_series.nlargest(10).index.values.tolist()))"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{},"source":"X_test_vectorized = vect.transform(X_test)\nX_test_vectorized = add_feature(X_test_vectorized, X_test.str.len())\nX_test_vectorized = add_feature(X_test_vectorized, X_test.str.replace(r'\\D+', '').str.len())\n    \ny_pred = model.predict(X_test_vectorized)\nprint('Accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100))"},{"metadata":{},"cell_type":"markdown","source":"Ops, accuracy dropped a bit."},{"metadata":{},"cell_type":"markdown","source":"---\nThird Model\n\nFinally, let's use a Count Vectorizer ignoring terms that have a document frequency strictly lower than **5** and using **character n-grams from n=2 to n=5.**\n\nTo tell Count Vectorizer to use character n-grams we pass in `analyzer='char_wb'` which creates character n-grams only from text inside word boundaries. This should make the model more robust to spelling mistakes.\n\nAt this time we are goint to use these additional features:\n* the length of document (number of characters)\n* number of digits per document\n* number of non-word characters (anything other than a letter, digit or underscore.)"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{},"source":"vect = CountVectorizer(min_df=5, ngram_range=(2, 5), analyzer='char_wb').fit(X_train)\nprint('Vocabulary len:', len(vect.get_feature_names()))\nprint('Longest word:', max(vect.vocabulary_, key=len))\n\nX_train_vectorized = vect.transform(X_train)\n\nX_train_vectorized = add_feature(X_train_vectorized, X_train.str.len())\nX_train_vectorized = add_feature(X_train_vectorized, X_train.str.replace(r'\\D+', '').str.len())\nX_train_vectorized = add_feature(X_train_vectorized, X_train.str.replace(r'\\w+', '').str.len())"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{},"source":"model = MultinomialNB(alpha=0.1)\nmodel.fit(X_train_vectorized, y_train)"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{},"source":"index = np.array(vect.get_feature_names() + ['length_of_doc', 'digit_count', 'non_word_char_count'])\nvalues = model.coef_[0]\nfeatures_series = pd.Series(data=values,index=index)\n\nprint('Smallest Coefs:\\n{}\\n'.format(features_series.nsmallest(10).index.values.tolist()))\nprint('Largest Coefs: \\n{}'.format(features_series.nlargest(10).index.values.tolist()))"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{},"source":"X_test_vectorized = vect.transform(X_test)\nX_test_vectorized = add_feature(X_test_vectorized, X_test.str.len())\nX_test_vectorized = add_feature(X_test_vectorized, X_test.str.replace(r'\\D+', '').str.len())\nX_test_vectorized = add_feature(X_test_vectorized, X_test.str.replace(r'\\w+', '').str.len())\n    \ny_pred = model.predict(X_test_vectorized)\nprint('Accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100))"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"collapsed":true},"source":""}],"nbformat":4,"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"mimetype":"text/x-python","version":"3.6.1","pygments_lexer":"ipython3","name":"python","nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py"}},"nbformat_minor":1}